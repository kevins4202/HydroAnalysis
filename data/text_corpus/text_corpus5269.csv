index,text
26345,human use of land increasingly alters the structure and the functioning of the environment to ex ante understand and anticipate these changes there is an increased need for readily available and operational land use change models one of these models is clue which has been used in many studies all over the world these studies brought forward operational hurdles that hamper model application the overall objective of this paper is to present a new version of the clue model iclue that helps to overcome these hurdles we describe the technical redevelopment conceptual innovations several applications and success factors and critical reflections iclue minimizes manual error prone actions enhances ease of use speeds up the operational modelling process and provides data visualisations to empower users to analyse and interpret results keywords land use change model transparency separation of concerns user centered design usability software availability name of the model iclue developers peter verweij peter verweij wur nl ph 31 317 481601 johnny te roller johnny teroller wur nl wim de winter wim dewinter iconoclastica nl software required quickscan freely available at www quickscan pro if regression is used then java numerical library jmsl commercially available at https www roguewave com program language java version 8 program size 335 kb excluding software required year first available 2014 code availability https doi org 10 5281 zenodo 1100980 license open access under eupl data requirements geotiff raster files 1 introduction 1 1 history of land use modelling practice humanity and its socio economical system are maintained by and depend upon the natural environment peer 2010 human use of land alters structure and functioning of this natural environment vitousek et al 1997 changes in land cover through agriculture forestry and urbanisation represent the most substantial alteration through their interaction with most components of global environmental change ojima et al 1994 turner et al 1994 particularly related to climate change since the last decade land use and land use change have been gaining importance e g pielke 2005 because of their role in both climate adaptation and climate mitigation biofuels are needed to reduce fossil fuel consumption and mitigate climate change while land also needs to be altered to increase food production and adapt to droughts and floods overall there is an increased need for readily available and operational land use change models this also calls for a re evaluation and reconstruction of existing models land use models have a rather long history starting with a number of seminal papers in the 1970s e g wilson 1971 particularly important for the current generation of land use models were cellular automata models batty et al 1997 boosted by increasing remote sensed data availability the number of land use models started increasing and diversifying in the last decades including specialised urban growth models e g clarke et al 2007 liao et al 2016 forest landscape thompson et al 2016 and deforestation models soares filho et al 2006 and agricultural land use models the latter category has spawned a rather large number of agricultural spatially explicit integrated land use models see for an overview national research council 2014 verburg et al 2004 in this paper we focus on an empirical data driven spatially explicit model there are two categories of these models that are of importance models with a predominantly top down logic of first determining narrative scenario s with overall demand mallampalli et al 2016 which is subsequently allocated and models with a predominantly bottom up logic of determining which spatially explicit land use changes are likely to happen the topic of study the clue modelling framework is an example of this first category 1 2 the iclue model and its history the iclue land use change model originates from the clue model family veldkamp and fresco 1996 kok et al 2001 verburg et al 2002 verburg and overmars 2009 the clue model has been used in many studies all over the world for land use planning environmental impact assessment and ex ante policy assessments lesschen et al 2007 luo et al 2010 britz et al 2011 gibreel et al 2014 clue allocates land use based on areal land use demands allocation takes place based on 1 land use suitability e g no agriculture on steep slopes with dry unfertile soils 2 conversion rules e g urban cannot change into pasture or new production forest can be harvested only after 20 years and turned into fallow land 3 neighbouring land use e g built up area is likely to expand next to existing built up area and 4 the areal demand for specific land use types e g 250 ha of agriculture in my study area in 2030 fig 1 the land use suitability describes how well a certain land use type fits on a specific location based on the characteristics of that location such as soil type slope climate and accessibility of markets the suitability can be determined by statistical methods e g empirically using logistic regression analysis expert knowledge or sequential assimilation of observations using bayesian belief networks kjaerulff and madson 2013 verstegen et al 2014 conversion rules determine if and under which conditions a conversion from one land use type to another is allowed typically these rules are defined by scientific experts but may also be gathered from stakeholders rules include conversion possible conversion impossible conversion possible after a specified amount of time and conversion im possible within specified areas in addition the ease of change indicates the reluctance of a land use type to change neighbouring land use influences the spatial allocation of a specific land use type at a specific spot nearby land use is more determinant than areas further away the land use demand can be determined on simple trend extrapolations or complex economic models typically demands are defined per administrative unit due to the political and or institutional target setting e g to determine land use changes for europe in 2050 each country implements its own policies finally determining land use demands the future land use demands need to specify at least for the final year of model simulation the area covered by the different land use types the sum of these four determinants specify the probability for each potential land use type per location this results in a location specific probability distribution across the potential future land use types each location is allocated to a single specific land use type using the probability distribution in this way the allocated land use does not necessarily match the predetermined demands neither when a tolerance for deviation from the demands is indicated hence the model iterates until the demands are met within the pre defined tolerances in each iteration the probabilities are adapted to reduce the mismatch in demand and allocation 1 3 problem definition and objective in a great number of studies in different parts of the world on different scales and different domains clue has been used as land use change model the need to understand processes involving past and future land use changes will always remain highly relevant however these studies brought forward operational hurdles that made us reluctant to apply the model and that we seek to solve there is a dependency on a small amount of experts to apply the model implicating that the workforce in projects is limited by the availability and preferences of individual experts results are not self explanatory in relation to the underlying land use change processes data preparation for and post processing of clue runs are time consuming as also identified by mas et al 2014 error prone due to the many manual actions required for data conversion scripts and tools the many coding file naming and file structuring conventions and manual guarantee of coding consistencies across files lacking clear warnings and error messages hampering efficient handling of causes the overall objective of the paper is therefore to present a new version of the clue model iclue which was constructed to provide solutions for the issues above and evaluate its ease of application in the paper we describe the development process the technical architecture several applications and the way we solved the issues above 2 method 2 1 scoping during half a day workshop with 5 modelling experts and 3 software engineers we exchanged experiences from clue modellers and identified common bottlenecks and formulated joint visions next we did an in depth literature review of peer reviewed published clue papers kok et al 2001 veldkamp and fresco 1996 verburg et al 2002 verburg and overmars 2009 the manual verburg 2010 overmars et al unpubl scrutinizing the code of the various software tools and listing the file naming coding and file structuring conventions despite differences in detail and realization between the sources we derived a single scheme describing the process of doing a clue run in which all manual data conversion interventions are identified see fig 2 while scrutinizing the code we ran into conceptual model imperfections for which model innovations were sought we jointly envisioned the model to use a minimum of manual error prone data conversion actions and include integrated result visualisations and transparent analysis tools in addition the statistical method and conversion rule base of the model should be adaptable and extensible several existing software solutions were considered as implementation platform dinamica soares filho et al 2002 r r core team 2015 quickscan verweij et al 2016 idrisi 1 1 https clarklabs org products and dataflow management systems the visualisation data storage and readily available algorithms of these platforms were seen as an important added value in contrast to developing the model fully from scratch quickscan was chosen as the implementation platform since it offers many visualisations data storage mechanisms application programming interfaces api s for result analysis and back tracing into modelled results and focus though not limited on categorical data like land use data moreover the involved software engineers are familiar with its technical architecture see fig 3 for the envisioned model setup 2 2 vision definition through wire frames a functional design is a formal documentation of what an application should do and how an application should function in interaction with a user it is a reference for the implementation within the user centered design approach raskin 2000 usability requirements drive the features and technical development by studying the usefulness with the intended users prototypes of interface design can be used to test usability with users prototypes can be incomplete versions of the software product but may as well be screen designs in a software presentation tool or even hand drawn sketches on paper sefelin et al 2003 they allow users to evaluate developers proposals for the interface construction of the product by actual testing rather than having to interpret and valuate the design based on descriptions the main objective of a prototype is to find out if the developers are on the right track and to further feed requirement discussion verweij et al 2014 in general a prototype is an inexpensive way to try out ideas so that as many issues as possible are understood before the real implementation is made tate 2005 wire frames are prototypes addressing the layout of a screen and deal with information structure relationships between information and flow between screens they are a graphical means of communication to further feed discussion on structure and information wire frames do not address aesthetics during the user feedback design phase in an agile method new wire frames might be added for new functionality or existing ones might be revised verweij et al 2014 during three two day sessions with land use modellers and software engineers many versions of screen designs were jointly sketched on paper each version was tested against modelling experience on the logical sequence of modelling processes and options and iteratively resulting in an improved sketch at the end of each session the paper sketches were transformed into a set of digital wire frames fig 4 shows an example of a developed wire frame 2 3 result visualisation and analysis the most important outcome of a land use model are the spatially projected allocations of land use classes through time in interviews the land use modellers wish listed the following result visualisations to get understanding on the occurring changes what where and how map of land use at any modelled time possibly dynamically displayed like a movie through time summary statistics of the area of each land use class at any modelled time possibly dynamically displayed like a movie through time hotspot maps i e land use transition frequency per location over the modelled time type of land use transitions between two modelled times i e the area gained and lost per land use class e g pastures may have disappeared in some locations while appeared at other locations this provides an immediate overview of the areal changes the area gained and lost per land use class defined by its originating and target land use class e g pastures that have disappeared have been replaced partly by forest and partly by urban areas where pasture appeared this was at the expense of arable land and forest these types of relocation are a result of the interplay between demands and local suitabilities in this case the relocation of pasture is enforced by the high demand for forest and urban areas this provides details on areal transitions see fig 5 for screenshots of these visualisations 3 results 3 1 technical architecture and model setup an architecture is a common abstraction of a system that manifests early design decisions through which the system to be build can be analysed as such an architecture helps communication among stakeholders as a basis for mutual understanding negotiation and consensus by documenting system qualities like modularity adaptability extensibility maintainability and portability bass et al 2003 verweij et al 2010 the iclue model was developed as an independent piece of java 2 2 https www oracle com java index html software equipped with a plugin api for using it within the quickscan framework application see fig 6 quickscan 3 3 http www quickscan pro is a java based spatial modelling environment developed using the openmi ogc standard 4 4 http www opengeospatial org standards openmi model integration framework gijsbers et al 2002 knapen et al 2013 quickscan offers support for scenario and indicator concepts and provides many built in visualisations analysis tools possibilities to trace back into modelled results and user interface templates for model parameter editors these tools have been designed according to the separation of concerns principle i e separation of software logic user interface and data for iclue two parameter editors were developed a wizard similar to the wire frames and a parameter file the model was developed incrementally given the short development iterations in which new functionality was added we chose to start with the parameter file editor which was easy and quick to adapt during the iterations in contrast to a fully functioning wizard user interface see annex i for the template of the parameter file format during the scoping phase the modellers explained the need for flexibility for the suitability method and the conversion rules these requirements have a central role in the architecture of the model as extension hooks for which several implementations were made based on the requirements of different modelling cases see fig 7 3 2 model implementation the logical calculation process of the iclue model as described above is illustrated in fig 8 conceptual model innovations include demand deviation validation per land use type sampling from land use probability distribution probability calculation using sigmoid function to calculate demand weight using shocks on the demand weight in iterative matching of allocation and demand these innovations are described below the allocated land use area does not necessarily match the demand exactly the user can specify the allowable deviation per land use class and indicate whether it is an absolute areal deviation in number of cells or hectares or a relative areal deviation in percentage of the total demand typically land use types covering large areas are given a relative deviation while land use types covering small areas use an absolute deviation during allocation each cell is appointed a single specific land use type in iclue this is based on a sample drawn from this location specific probability distribution for including stochastic behaviour mas et al 2014 and a fair representation of the probability distribution land use probabilities are calculated by summing the suitability neighbourhood and demand weight the probability for the current land use is calculated by also adding the ease of change each determinant is represented by a number between 0 low and 1 high with the exception of demand weight fig 9 a probability i t lu suitability i t lu neighbourhood i t lu demand weight i t lu ease of change i t lu probability fraction probability i t lu lu probability i t 100 the difference between the future demand and the actual area covered by a land use type is used to derive the demand weight the bigger the difference the bigger the weight derived from a sigmoid function between 1 the actual area has to decrease strongly and 1 the actual areas has to increase strongly fig 9 b the model uses iterations to find a solution fitting within all tolerance limits in each iteration the probabilities are adapted towards reducing the mismatch in demand and allocation the bigger the mismatch the larger the adaptation per iteration a maximum of 5 is added or removed from the probabilities for land use classes that do not pass the tolerance limit validation if the moving average of the deviation between iterations becomes too small 5 over 10 iterations the pseudorandom number generator mersenne twister matsumoto and nishimura 1998 redefines the probability of land use classes with unmatched demands within the range of 95 105 of the original probability value mersenne twister is also used to break flip flopping or continuous switching of allocated land use again by redefining the probability within the 95 105 range 3 3 approach validation sofar iclue has been applied in 4 independent projects by 8 individuals from various organisations across the world the 8 model operators have experience with using other clue versions these applications vary in resolution size of the study area number of projected years number of land use classes number of drivers and complexity of the conversion rules detailed descriptions of these applications and the observations on the use of iclue are given below 3 3 1 sustainable mineral exploitation for sustainable exploitation of minerals in europe the current mineral requirements of society must be met without compromising the ability of future generations to meet their own needs accordingly within the framework of the eu horizon2020 minatura2020 project potentially exploitable mineral deposits and resources have been assessed against other both current and future land uses taking into account criteria such as priorities for settlements natural habitats etc the iclue model was used to project future land use in eight case study countries hungary italy poland portugal slovenia sweden and the united kingdom eleven land use classes were dynamically modelled into the future from 2012 till 2050 forest arable land with annual crops arable land with permanent crops grassland non grazed grassland shrubland non grazed shrubland sparsely vegetated areas non grazed sparsely vegetated areas built up area and open pit mineral extraction sites or dump sites overall the most prominent changes are the increase in built up area urban sprawl and the decrease in cropped land for several case study countries we see a major increase in shrubland which can be seen as land abandonment these patterns are mainly driven by distance to the coast population density or global night lights index as a proxy impervious area and cropping frequency below the case study of portugal is exemplified portugal the southwesternmost country of mainland europe has a total area of about 90 000 km2 portugal s northern inland is mountainous with several plateaus indented by river valleys whereas the south is characterized by rolling plains portugal is a significant european minerals producer especially of copper tin tungsten and uranium for portugal a clear segregation in land use change was projected extensive urban sprawl along the coast and a succession of inland annual crop areas into grassland and subsequently shrubland reflecting the process of agricultural land abandonment these patterns are mainly driven by distance to the coast population density or global night lights index as a proxy impervious area and cropping frequency gis data used has a resolution of 100 m2 with a total of 3470 rows and 5770 columns 34 drivers were used cormont et al 2016 we coded the driver accessibility as quantitative with values 1 2 and 3 which we later changed to ordinal with values near far and very far as advised by a colleague statistician since the value 2 is not necessarily two times as far as 1 instead accessibility could also have been expressed as travel time in hours or distance to in kilometres a similar case is related to the driver aspect which runs from 0 to 360 where 0 equals 360 the solution used was to qualitatively classify it in north northwest west etc since all driver files are required to have the same spatial extent resolution and projection we developed a small script to process all gis data to a given template initially we used a coarse spatial resolution to be able to do quick model runs to test effects of demand tolerances and conversion rules which we later replaced by the targeted spatial resolution we processed multiple countries using the same drivers conveniently allowing us to copy the parameters from one country to another iclue runs varied in processing time depending on the size of the country from roughly 10 min to a bit more than 1 h 3 3 2 biodiversity to mitigate climate change tropical forests provide us with foods fibres and medicines they filter water and control its flow they also soak up carbon dioxide from the air mitigating climate change within the eu seventh framework programme project robin we analysed the impacts of three alternative land use policy scenarios aimed at maximising climate mitigation potential the projection of these policies was executed with both clue and iclue this was done at continental level covering all 21 countries of south and meso america eight land use classes were dynamically modelled into the future from 2005 till 2050 forest shrubland grazed shrubland grassland grazed grassland cropland for food feed and fodder cropland for food with perennial trees or shrubs and cropland with energy crops van eupen et al 2014 the cell based sampling of the probability distribution results in a speckled image of allocated land use in the applications this undesirable effect was prevented by including distance to drivers e g distance to roads or distance to edge of forest this effect could also have been prevented by using neighbourhood functions to understand the transitions the trace back tool offered the possibility to see the history of land use changes at a certain location even more insight would be gained by showing the probability distribution change through time by location de composed to its determinants 3 3 3 water scarcity in europe and northern africa water scarcity affects ecosystems and the services they provide to society in five river basins in europe and one moroccan basin the prevalence of and interaction between stressors were identified in the context of the eu funded project globaqua climate change is expected to worsen the situation iclue was used in all six river basins to understand the complex relations between ecosystems and their stressors and to analyse the effects under changing conditions one of them the ebro river basin in spain is exemplified below the ebro catchment over 85 000 km2 is located in north eastern spain and includes andorra and a small part of france the basin experiences a mediterranean climate grasslands and coniferous forests dominate in the pyrenees broadleaved forests are prevalent in the western mountain regions the mean discharge has been decreasing in the last decades due to infrastructural works and regrowth of forest climate change is expected to worsen the water scarcity problems thereby impacting ecosystems and their services to society gis data of a 1 km2 resolution included 21 drivers such as employment in different sectors distances to settlements river and roads hydrogeology elevation aspect slope erosion levels population gross domestic product and water use the corine land use map of 2000 was used as a baseline for a projection to 2030 the following land use classes have been modelled non irrigated arable land permanently irrigated land vineyards fruit trees and olives grasslands pastures complex cultivation patterns agriculture with natural vegetation broad leaved forest coniferous and mixed forest sealed area transitional woodland incl shrub and sclerophyllous vegetation open spaces with little or no vegetation and water huber garcia et al 2018 gis data used has a resolution of 1 km2 with a total of 313 rows and 527 columns learning to work with iclue is easy after half a day training users could independently parametrize run and validate the model for other study areas the possibilities to introduce errors are next to nil gis data preparation still requires to use the same spatial extent resolution and projection and this quite a lot of time iclue runs in less than 10 min 3 3 4 bangladesh bangladesh is one of the least developed countries of the world dominated by rural areas with a total area of almost 150 000 km2 in the south of asia bangladesh is experiencing an increasing rate of land use change as a result of population dynamics economic development climate change improved accessibility and technological developments in agriculture for strategic planning and informed decision making iclue was used to project future land use under different scenarios gis data used has a resolution of 1 km2 with a total of 520 rows and 718 columns drivers include administrative units elevation distance to the main road network gross domestic product population density soil and various climate variables and their projections the land use map of 2000 was used as baseline for a projection over 30 years distinguishing between cultivated land forest grassland water bodies built up area and unused land hasan et al submitted for this study we wanted to use a well known and well accepted model with as little effort and capacity as possible with a download from the internet and some email assistance we were able to run the model for three scenarios the evident error messages allowed us to progress without further technical assistance initially the model did not find a solution within the tolerances we defined in order to successfully allocate the demands we set very loose tolerances which we then increasingly tightened in a trial and error manner for our study area and model parametrization iclue runs in a few minutes 4 discussion with the new iclue model we minimized operational hurdles that previously made us reluctant to apply the model we solved these issues in the following manner there is a dependency on a small amount of experts to apply the model implicating that the workforce in projects is limited by the availability and preferences of individual experts to concentrate on the complex endeavour of land use modelling the operational part of the modelling must be as simple as possible through the separation of concerns the model can be fully run with a self explanatory single set of parameters and enables users to independently run the model after a very short about half a day training results are not self explanatory in relation to the underlying land use change processes land use modelling brings together many different mutually dependent factors the many built in data visualisations empower users to disentangle the underlying land use transitions that take place both at cell level and for the study area as a whole although the visualisations facilitate analysis the user must have good understanding of land use change processes to correctly interpret the results data preparation for and post processing of clue runs are time consuming since iclue only uses a single parameters file there is no longer a need to manually harmonise a set of parameter files as input post processing into the analysis and visualisation tools is automatically handled by the integrated quickscan environment less input files and automatic post processing speed up the cycle of model execution and result analysis it stimulates to do iterations to improve the modelling results just like in clue gis data is required to all be in the same spatial extent resolution and projection error prone due to the many manual actions required for data conversion scripts and tools the many coding file naming and file structuring conventions and manual guarantee of coding consistencies across files the automation of the many manual steps in iclue makes the whole procedure of data preparation and model run less error prone the user no longer needs to know how data is transformed and coded and how the separate processes make use of that data however the automation also implies that the user does not necessarily has to look into the results of the statistical analysis and thereby might miss out on algebraic insights lacking clear warnings and error messages hampering efficient handling of causes a syntax and semantic check takes place over the parameters file in case of incorrectness or incompleteness of the parameters file an error report with suggestions for improvement is shown in addition to the issues forming the rationale behind the development of iclue we observed new forms of use emerging firstly the accelerated operation and introduced transparency facilitates the understanding of the underlying processes and stimulates and enables the adaptation of model parameters for subsequent runs thereby creating opportunities for interactive use during participatory spatial planning and participatory modelling voinov and brown gaddis 2008 hewitt et al 2014 verweij et al 2016 secondly the introduced transparency enhances understanding of modelled land use change processes opening up new possibilities for methodological innovations for land use modelling thirdly the embedding into a modelling framework enhances flexibility in terms of integrated linkages to other models e g wagner et al 2017 connor et al 2015 knapen et al 2013 and it allows to do further post processing e g into land use dependant indicators such as ecosystem services or sustainability indicators or impact assessment and scenario analysis although we argue that iclue is a technical and conceptual improvement over previous versions of the clue model there are also some critical issues each member of the clue model family produces slightly different results so does iclue because of the model innovations e g demand deviation validation per land use type and the sampling from the land use probability distribution changes are relatively small and do not influence the overall interpretation iclue allocates demands for each time step year if demands are not met in a time step the model stops execution and displays a warning explaining why the model halts when demands are met within tolerance limits the model does not automatically show how far off the results are this deviation could be helpful in fine tuning the tolerances the deviation can currently be analysed by performing manual diagnostics including a random number generator inhibits reproducibility however various model runs using the same input parameters result in similar output patterns this type of model is targeted at scenario studies in which the focus is on patterns of change rather than exact predictions for each location so far all applications used regressions to determine suitabilities try outs with bayesian belief networks and especially the interactive visualisation of belief bars show that these are likely to improve the transparency of the suitability statistics bayesian belief networks are also a useful tool to include stakeholder knowledge non spatial information e g policies and management options and decision hierarchy celio et al 2014 hewitt et al 2014 mahamane et al 2017 however further developments are required to fully automatically derive bayesian models with probability tables from driver samples as bayesian evidence currently the trace back tool displays land use through time for a given location see fig 5 b the trace back tool would profit from additionally showing graphical overviews of probability bars for each land use type through time and the contribution of each determinant in the total probability iclue does not and cannot address all issues related to land use modelling verstegen et al 2014 celio et al 2014 and van vliet et al 2016 provide methods for calibrating and validating land use projection models to improve the allocation future improvements could seek links with other types of models to improve e g agency grashof bokdam et al 2017 murray rust et al 2014 ralha et al 2013 or representation of process based information during the development of iclue the authors worked intensively together in building the software amongst the team this co creation process built trust in the model software and its usage as each member reviewed the work done by the other team members the iclue source code is freely available https doi org 10 5281 zenodo 1100980 under eupl 5 5 https joinup ec europa eu sites default files custom page attachment eupl v1 2 en pdf and as part of the quickscan modeling environment quickscan can be obtained via www quickscan pro iclue can directly be included in java based software or wrapped in scripting languages like r 6 6 https www r project org or python 7 7 https www python org for further analysis author contributions pv kk mve ac sj and jtr designed the research kk mve pv ac wdw and igs designed the model pv jtr and wdw developed the software mve pv and ac ran and tested the model pv ac sj kk wrote the paper and mps raised funds and commented on the paper acknowledgements the authors gratefully acknowledge the funding received for this work the work has been funded through the european union seventh framework programme fp7 2007 2013 under grant agreement no 283093 the role of biodiversity in climate change mitigation robin horizon 2020 research and innovation programme under grant agreement no 642139 minatura2020 and the dutch ministry of economic affairs under the strategic research programs sustainable spatial development of ecosystems landscapes seas and regions and system earth management special thanks go to dennis walvoort from wageningen environmental research and martin karlsen and anders madsen from hugin for their statistical expertise annex i iclue parameter template property file uses key value notation the symbol cannot be used for other purposes key cannot contain any white spaces use camel casing instead key uses namespace notation a between key parts to denote a hierarchical relation a value can contain white spaces in value the symbol is used to separate list elements it can therefore not be used for other purposes baseline landuse map and year that the map represents example baseline filename d clue mexico rob lu 16a example baseline year 2005 landuse classes code in map file colour code in hex rgb ease of change initial age in years demand deviation type demand deviation amount colour examples red ff0000 green 00ff00 blue 0000ff yellow ffff00 white ffffff black 000000 grey aaaaaa orange ffaa00 purple aa00ff see also http www color hex com color names html ease of change very easy easy hard very hard cannot change demand deviation type absolutedeviation cell count percentagedeviation 0 100 example 1 landuseclass forest 10001 38a800 hard 100 absolutedeviation 2047 example 2 landuseclass urban 10002 38a800 very easy 22 percentagedeviation 15 administrative units map and list of unit name and unit code example administrativeunits filename d clue europe masker example administrativeunit netherlands 1 example administrativeunit belgium 2 demands line with sequence of landuse classes line with same sequence of landuse demands per year example landusedemands sequence forest urban example landusedemand netherlands 2025 430787 232460 example landusedemand netherlands 2050 530787 132460 example landusedemand belgium 2010 300 200 example landusedemand belgium 2050 400 100 drivers can be constant or dynamic driver dynamic drivers change over time for every driver line 1 datatype qualitative quantitative line 2 filename full path line 3 etc class classname class code in map file class colour in hex rgb the following 4 examples illustrate 1 qualitative constant driver 2 quantitative constant driver 3 qualitative dynamic driver 4 quantitative drynamic driver example 1 parametermap constant ecoregions datatype qualitative example 1 parametermap constant ecoregions filename d clue mexico wwf ecoregion example 1 parametermap constant ecoregions class boreal 204 ffaa5b example 1 parametermap constant ecoregions class pannonioal 205 22e4ff example 1 parametermap constant ecoregions class tundra 206 ffff00 example 2 parametermap constant energycrophectare datatype quantitative example 2 parametermap constant energycrophectare filename d clue mexico rk encrop ha example 3 parametermap dynamic temperature datatype qualitative example 3 parametermap dynamic temperature class cool 1 0000ff example 3 parametermap dynamic temperature class moderate 2 ffaa00 example 3 parametermap dynamic temperature class hot 3 ff0000 example 3 parametermap dynamic temperature filename 2005 d samplepath filename 2005 example 3 parametermap dynamic temperature filename 2012 d samplepath filename 2012 example 3 parametermap dynamic temperature filename 2020 d samplepath filename 2020 example 4 parametermap dynamic populationdensity datatype quantitative example 4 parametermap dynamic populationdensity filename 2005 d samplepath filename 2005 example 4 parametermap dynamic populationdensity filename 2010 d samplepath filename 2010 example 4 parametermap dynamic populationdensity filename 2020 d samplepath filename 2020 suitability calculation line 1 method stepwiseregression functiondictionary line 2 depending the method line 2 stepwiseregression samplesizepercentage decimal number between 0 100 percentage of the number of cells for each land use class that ll be used to do the regression upon line 3 stepwiseregression correlationthreshold decimal number between 0 1 drivers are being correlated for each landuse if drivers are highly correlated above threshold the driver with the lowest correlation with the landuse class is omitted line 4 stepwiseregression exportfilename d path filename prop example suitability method stepwiseregression example suitability stepwiseregression samplesizepercentage 7 5 example suitability stepwiseregression correlationthreshold 0 85 line 2 functiondictionary adminunit landuseclass functionconstant decimal number between 1 1 constant value in function line 3 functiondictionary adminunit landuseclass functioncoefficient driver decimal number between 1 1 coefficient value in function for quantitative driver line 4 functiondictionary adminunit landuseclass functioncoefficient driver class classname decimal number between 1 1 coefficient value in function for qualitative driver line 5 etc for driver and landuse class conversion choose from the options always never years 7 location d samplepath conservationareas tif default is always no need to include a land use conversion that can take place always for location areas with data are not allowed to be converted areas without data nodata can be converted example 1 conversion urban forest never example 2 conversion forest urban years 15 example 3 conversion forest arable location d samplepath conservationareas tif target time define until what time land use allocation calculations take place example targettime 2050 
26345,human use of land increasingly alters the structure and the functioning of the environment to ex ante understand and anticipate these changes there is an increased need for readily available and operational land use change models one of these models is clue which has been used in many studies all over the world these studies brought forward operational hurdles that hamper model application the overall objective of this paper is to present a new version of the clue model iclue that helps to overcome these hurdles we describe the technical redevelopment conceptual innovations several applications and success factors and critical reflections iclue minimizes manual error prone actions enhances ease of use speeds up the operational modelling process and provides data visualisations to empower users to analyse and interpret results keywords land use change model transparency separation of concerns user centered design usability software availability name of the model iclue developers peter verweij peter verweij wur nl ph 31 317 481601 johnny te roller johnny teroller wur nl wim de winter wim dewinter iconoclastica nl software required quickscan freely available at www quickscan pro if regression is used then java numerical library jmsl commercially available at https www roguewave com program language java version 8 program size 335 kb excluding software required year first available 2014 code availability https doi org 10 5281 zenodo 1100980 license open access under eupl data requirements geotiff raster files 1 introduction 1 1 history of land use modelling practice humanity and its socio economical system are maintained by and depend upon the natural environment peer 2010 human use of land alters structure and functioning of this natural environment vitousek et al 1997 changes in land cover through agriculture forestry and urbanisation represent the most substantial alteration through their interaction with most components of global environmental change ojima et al 1994 turner et al 1994 particularly related to climate change since the last decade land use and land use change have been gaining importance e g pielke 2005 because of their role in both climate adaptation and climate mitigation biofuels are needed to reduce fossil fuel consumption and mitigate climate change while land also needs to be altered to increase food production and adapt to droughts and floods overall there is an increased need for readily available and operational land use change models this also calls for a re evaluation and reconstruction of existing models land use models have a rather long history starting with a number of seminal papers in the 1970s e g wilson 1971 particularly important for the current generation of land use models were cellular automata models batty et al 1997 boosted by increasing remote sensed data availability the number of land use models started increasing and diversifying in the last decades including specialised urban growth models e g clarke et al 2007 liao et al 2016 forest landscape thompson et al 2016 and deforestation models soares filho et al 2006 and agricultural land use models the latter category has spawned a rather large number of agricultural spatially explicit integrated land use models see for an overview national research council 2014 verburg et al 2004 in this paper we focus on an empirical data driven spatially explicit model there are two categories of these models that are of importance models with a predominantly top down logic of first determining narrative scenario s with overall demand mallampalli et al 2016 which is subsequently allocated and models with a predominantly bottom up logic of determining which spatially explicit land use changes are likely to happen the topic of study the clue modelling framework is an example of this first category 1 2 the iclue model and its history the iclue land use change model originates from the clue model family veldkamp and fresco 1996 kok et al 2001 verburg et al 2002 verburg and overmars 2009 the clue model has been used in many studies all over the world for land use planning environmental impact assessment and ex ante policy assessments lesschen et al 2007 luo et al 2010 britz et al 2011 gibreel et al 2014 clue allocates land use based on areal land use demands allocation takes place based on 1 land use suitability e g no agriculture on steep slopes with dry unfertile soils 2 conversion rules e g urban cannot change into pasture or new production forest can be harvested only after 20 years and turned into fallow land 3 neighbouring land use e g built up area is likely to expand next to existing built up area and 4 the areal demand for specific land use types e g 250 ha of agriculture in my study area in 2030 fig 1 the land use suitability describes how well a certain land use type fits on a specific location based on the characteristics of that location such as soil type slope climate and accessibility of markets the suitability can be determined by statistical methods e g empirically using logistic regression analysis expert knowledge or sequential assimilation of observations using bayesian belief networks kjaerulff and madson 2013 verstegen et al 2014 conversion rules determine if and under which conditions a conversion from one land use type to another is allowed typically these rules are defined by scientific experts but may also be gathered from stakeholders rules include conversion possible conversion impossible conversion possible after a specified amount of time and conversion im possible within specified areas in addition the ease of change indicates the reluctance of a land use type to change neighbouring land use influences the spatial allocation of a specific land use type at a specific spot nearby land use is more determinant than areas further away the land use demand can be determined on simple trend extrapolations or complex economic models typically demands are defined per administrative unit due to the political and or institutional target setting e g to determine land use changes for europe in 2050 each country implements its own policies finally determining land use demands the future land use demands need to specify at least for the final year of model simulation the area covered by the different land use types the sum of these four determinants specify the probability for each potential land use type per location this results in a location specific probability distribution across the potential future land use types each location is allocated to a single specific land use type using the probability distribution in this way the allocated land use does not necessarily match the predetermined demands neither when a tolerance for deviation from the demands is indicated hence the model iterates until the demands are met within the pre defined tolerances in each iteration the probabilities are adapted to reduce the mismatch in demand and allocation 1 3 problem definition and objective in a great number of studies in different parts of the world on different scales and different domains clue has been used as land use change model the need to understand processes involving past and future land use changes will always remain highly relevant however these studies brought forward operational hurdles that made us reluctant to apply the model and that we seek to solve there is a dependency on a small amount of experts to apply the model implicating that the workforce in projects is limited by the availability and preferences of individual experts results are not self explanatory in relation to the underlying land use change processes data preparation for and post processing of clue runs are time consuming as also identified by mas et al 2014 error prone due to the many manual actions required for data conversion scripts and tools the many coding file naming and file structuring conventions and manual guarantee of coding consistencies across files lacking clear warnings and error messages hampering efficient handling of causes the overall objective of the paper is therefore to present a new version of the clue model iclue which was constructed to provide solutions for the issues above and evaluate its ease of application in the paper we describe the development process the technical architecture several applications and the way we solved the issues above 2 method 2 1 scoping during half a day workshop with 5 modelling experts and 3 software engineers we exchanged experiences from clue modellers and identified common bottlenecks and formulated joint visions next we did an in depth literature review of peer reviewed published clue papers kok et al 2001 veldkamp and fresco 1996 verburg et al 2002 verburg and overmars 2009 the manual verburg 2010 overmars et al unpubl scrutinizing the code of the various software tools and listing the file naming coding and file structuring conventions despite differences in detail and realization between the sources we derived a single scheme describing the process of doing a clue run in which all manual data conversion interventions are identified see fig 2 while scrutinizing the code we ran into conceptual model imperfections for which model innovations were sought we jointly envisioned the model to use a minimum of manual error prone data conversion actions and include integrated result visualisations and transparent analysis tools in addition the statistical method and conversion rule base of the model should be adaptable and extensible several existing software solutions were considered as implementation platform dinamica soares filho et al 2002 r r core team 2015 quickscan verweij et al 2016 idrisi 1 1 https clarklabs org products and dataflow management systems the visualisation data storage and readily available algorithms of these platforms were seen as an important added value in contrast to developing the model fully from scratch quickscan was chosen as the implementation platform since it offers many visualisations data storage mechanisms application programming interfaces api s for result analysis and back tracing into modelled results and focus though not limited on categorical data like land use data moreover the involved software engineers are familiar with its technical architecture see fig 3 for the envisioned model setup 2 2 vision definition through wire frames a functional design is a formal documentation of what an application should do and how an application should function in interaction with a user it is a reference for the implementation within the user centered design approach raskin 2000 usability requirements drive the features and technical development by studying the usefulness with the intended users prototypes of interface design can be used to test usability with users prototypes can be incomplete versions of the software product but may as well be screen designs in a software presentation tool or even hand drawn sketches on paper sefelin et al 2003 they allow users to evaluate developers proposals for the interface construction of the product by actual testing rather than having to interpret and valuate the design based on descriptions the main objective of a prototype is to find out if the developers are on the right track and to further feed requirement discussion verweij et al 2014 in general a prototype is an inexpensive way to try out ideas so that as many issues as possible are understood before the real implementation is made tate 2005 wire frames are prototypes addressing the layout of a screen and deal with information structure relationships between information and flow between screens they are a graphical means of communication to further feed discussion on structure and information wire frames do not address aesthetics during the user feedback design phase in an agile method new wire frames might be added for new functionality or existing ones might be revised verweij et al 2014 during three two day sessions with land use modellers and software engineers many versions of screen designs were jointly sketched on paper each version was tested against modelling experience on the logical sequence of modelling processes and options and iteratively resulting in an improved sketch at the end of each session the paper sketches were transformed into a set of digital wire frames fig 4 shows an example of a developed wire frame 2 3 result visualisation and analysis the most important outcome of a land use model are the spatially projected allocations of land use classes through time in interviews the land use modellers wish listed the following result visualisations to get understanding on the occurring changes what where and how map of land use at any modelled time possibly dynamically displayed like a movie through time summary statistics of the area of each land use class at any modelled time possibly dynamically displayed like a movie through time hotspot maps i e land use transition frequency per location over the modelled time type of land use transitions between two modelled times i e the area gained and lost per land use class e g pastures may have disappeared in some locations while appeared at other locations this provides an immediate overview of the areal changes the area gained and lost per land use class defined by its originating and target land use class e g pastures that have disappeared have been replaced partly by forest and partly by urban areas where pasture appeared this was at the expense of arable land and forest these types of relocation are a result of the interplay between demands and local suitabilities in this case the relocation of pasture is enforced by the high demand for forest and urban areas this provides details on areal transitions see fig 5 for screenshots of these visualisations 3 results 3 1 technical architecture and model setup an architecture is a common abstraction of a system that manifests early design decisions through which the system to be build can be analysed as such an architecture helps communication among stakeholders as a basis for mutual understanding negotiation and consensus by documenting system qualities like modularity adaptability extensibility maintainability and portability bass et al 2003 verweij et al 2010 the iclue model was developed as an independent piece of java 2 2 https www oracle com java index html software equipped with a plugin api for using it within the quickscan framework application see fig 6 quickscan 3 3 http www quickscan pro is a java based spatial modelling environment developed using the openmi ogc standard 4 4 http www opengeospatial org standards openmi model integration framework gijsbers et al 2002 knapen et al 2013 quickscan offers support for scenario and indicator concepts and provides many built in visualisations analysis tools possibilities to trace back into modelled results and user interface templates for model parameter editors these tools have been designed according to the separation of concerns principle i e separation of software logic user interface and data for iclue two parameter editors were developed a wizard similar to the wire frames and a parameter file the model was developed incrementally given the short development iterations in which new functionality was added we chose to start with the parameter file editor which was easy and quick to adapt during the iterations in contrast to a fully functioning wizard user interface see annex i for the template of the parameter file format during the scoping phase the modellers explained the need for flexibility for the suitability method and the conversion rules these requirements have a central role in the architecture of the model as extension hooks for which several implementations were made based on the requirements of different modelling cases see fig 7 3 2 model implementation the logical calculation process of the iclue model as described above is illustrated in fig 8 conceptual model innovations include demand deviation validation per land use type sampling from land use probability distribution probability calculation using sigmoid function to calculate demand weight using shocks on the demand weight in iterative matching of allocation and demand these innovations are described below the allocated land use area does not necessarily match the demand exactly the user can specify the allowable deviation per land use class and indicate whether it is an absolute areal deviation in number of cells or hectares or a relative areal deviation in percentage of the total demand typically land use types covering large areas are given a relative deviation while land use types covering small areas use an absolute deviation during allocation each cell is appointed a single specific land use type in iclue this is based on a sample drawn from this location specific probability distribution for including stochastic behaviour mas et al 2014 and a fair representation of the probability distribution land use probabilities are calculated by summing the suitability neighbourhood and demand weight the probability for the current land use is calculated by also adding the ease of change each determinant is represented by a number between 0 low and 1 high with the exception of demand weight fig 9 a probability i t lu suitability i t lu neighbourhood i t lu demand weight i t lu ease of change i t lu probability fraction probability i t lu lu probability i t 100 the difference between the future demand and the actual area covered by a land use type is used to derive the demand weight the bigger the difference the bigger the weight derived from a sigmoid function between 1 the actual area has to decrease strongly and 1 the actual areas has to increase strongly fig 9 b the model uses iterations to find a solution fitting within all tolerance limits in each iteration the probabilities are adapted towards reducing the mismatch in demand and allocation the bigger the mismatch the larger the adaptation per iteration a maximum of 5 is added or removed from the probabilities for land use classes that do not pass the tolerance limit validation if the moving average of the deviation between iterations becomes too small 5 over 10 iterations the pseudorandom number generator mersenne twister matsumoto and nishimura 1998 redefines the probability of land use classes with unmatched demands within the range of 95 105 of the original probability value mersenne twister is also used to break flip flopping or continuous switching of allocated land use again by redefining the probability within the 95 105 range 3 3 approach validation sofar iclue has been applied in 4 independent projects by 8 individuals from various organisations across the world the 8 model operators have experience with using other clue versions these applications vary in resolution size of the study area number of projected years number of land use classes number of drivers and complexity of the conversion rules detailed descriptions of these applications and the observations on the use of iclue are given below 3 3 1 sustainable mineral exploitation for sustainable exploitation of minerals in europe the current mineral requirements of society must be met without compromising the ability of future generations to meet their own needs accordingly within the framework of the eu horizon2020 minatura2020 project potentially exploitable mineral deposits and resources have been assessed against other both current and future land uses taking into account criteria such as priorities for settlements natural habitats etc the iclue model was used to project future land use in eight case study countries hungary italy poland portugal slovenia sweden and the united kingdom eleven land use classes were dynamically modelled into the future from 2012 till 2050 forest arable land with annual crops arable land with permanent crops grassland non grazed grassland shrubland non grazed shrubland sparsely vegetated areas non grazed sparsely vegetated areas built up area and open pit mineral extraction sites or dump sites overall the most prominent changes are the increase in built up area urban sprawl and the decrease in cropped land for several case study countries we see a major increase in shrubland which can be seen as land abandonment these patterns are mainly driven by distance to the coast population density or global night lights index as a proxy impervious area and cropping frequency below the case study of portugal is exemplified portugal the southwesternmost country of mainland europe has a total area of about 90 000 km2 portugal s northern inland is mountainous with several plateaus indented by river valleys whereas the south is characterized by rolling plains portugal is a significant european minerals producer especially of copper tin tungsten and uranium for portugal a clear segregation in land use change was projected extensive urban sprawl along the coast and a succession of inland annual crop areas into grassland and subsequently shrubland reflecting the process of agricultural land abandonment these patterns are mainly driven by distance to the coast population density or global night lights index as a proxy impervious area and cropping frequency gis data used has a resolution of 100 m2 with a total of 3470 rows and 5770 columns 34 drivers were used cormont et al 2016 we coded the driver accessibility as quantitative with values 1 2 and 3 which we later changed to ordinal with values near far and very far as advised by a colleague statistician since the value 2 is not necessarily two times as far as 1 instead accessibility could also have been expressed as travel time in hours or distance to in kilometres a similar case is related to the driver aspect which runs from 0 to 360 where 0 equals 360 the solution used was to qualitatively classify it in north northwest west etc since all driver files are required to have the same spatial extent resolution and projection we developed a small script to process all gis data to a given template initially we used a coarse spatial resolution to be able to do quick model runs to test effects of demand tolerances and conversion rules which we later replaced by the targeted spatial resolution we processed multiple countries using the same drivers conveniently allowing us to copy the parameters from one country to another iclue runs varied in processing time depending on the size of the country from roughly 10 min to a bit more than 1 h 3 3 2 biodiversity to mitigate climate change tropical forests provide us with foods fibres and medicines they filter water and control its flow they also soak up carbon dioxide from the air mitigating climate change within the eu seventh framework programme project robin we analysed the impacts of three alternative land use policy scenarios aimed at maximising climate mitigation potential the projection of these policies was executed with both clue and iclue this was done at continental level covering all 21 countries of south and meso america eight land use classes were dynamically modelled into the future from 2005 till 2050 forest shrubland grazed shrubland grassland grazed grassland cropland for food feed and fodder cropland for food with perennial trees or shrubs and cropland with energy crops van eupen et al 2014 the cell based sampling of the probability distribution results in a speckled image of allocated land use in the applications this undesirable effect was prevented by including distance to drivers e g distance to roads or distance to edge of forest this effect could also have been prevented by using neighbourhood functions to understand the transitions the trace back tool offered the possibility to see the history of land use changes at a certain location even more insight would be gained by showing the probability distribution change through time by location de composed to its determinants 3 3 3 water scarcity in europe and northern africa water scarcity affects ecosystems and the services they provide to society in five river basins in europe and one moroccan basin the prevalence of and interaction between stressors were identified in the context of the eu funded project globaqua climate change is expected to worsen the situation iclue was used in all six river basins to understand the complex relations between ecosystems and their stressors and to analyse the effects under changing conditions one of them the ebro river basin in spain is exemplified below the ebro catchment over 85 000 km2 is located in north eastern spain and includes andorra and a small part of france the basin experiences a mediterranean climate grasslands and coniferous forests dominate in the pyrenees broadleaved forests are prevalent in the western mountain regions the mean discharge has been decreasing in the last decades due to infrastructural works and regrowth of forest climate change is expected to worsen the water scarcity problems thereby impacting ecosystems and their services to society gis data of a 1 km2 resolution included 21 drivers such as employment in different sectors distances to settlements river and roads hydrogeology elevation aspect slope erosion levels population gross domestic product and water use the corine land use map of 2000 was used as a baseline for a projection to 2030 the following land use classes have been modelled non irrigated arable land permanently irrigated land vineyards fruit trees and olives grasslands pastures complex cultivation patterns agriculture with natural vegetation broad leaved forest coniferous and mixed forest sealed area transitional woodland incl shrub and sclerophyllous vegetation open spaces with little or no vegetation and water huber garcia et al 2018 gis data used has a resolution of 1 km2 with a total of 313 rows and 527 columns learning to work with iclue is easy after half a day training users could independently parametrize run and validate the model for other study areas the possibilities to introduce errors are next to nil gis data preparation still requires to use the same spatial extent resolution and projection and this quite a lot of time iclue runs in less than 10 min 3 3 4 bangladesh bangladesh is one of the least developed countries of the world dominated by rural areas with a total area of almost 150 000 km2 in the south of asia bangladesh is experiencing an increasing rate of land use change as a result of population dynamics economic development climate change improved accessibility and technological developments in agriculture for strategic planning and informed decision making iclue was used to project future land use under different scenarios gis data used has a resolution of 1 km2 with a total of 520 rows and 718 columns drivers include administrative units elevation distance to the main road network gross domestic product population density soil and various climate variables and their projections the land use map of 2000 was used as baseline for a projection over 30 years distinguishing between cultivated land forest grassland water bodies built up area and unused land hasan et al submitted for this study we wanted to use a well known and well accepted model with as little effort and capacity as possible with a download from the internet and some email assistance we were able to run the model for three scenarios the evident error messages allowed us to progress without further technical assistance initially the model did not find a solution within the tolerances we defined in order to successfully allocate the demands we set very loose tolerances which we then increasingly tightened in a trial and error manner for our study area and model parametrization iclue runs in a few minutes 4 discussion with the new iclue model we minimized operational hurdles that previously made us reluctant to apply the model we solved these issues in the following manner there is a dependency on a small amount of experts to apply the model implicating that the workforce in projects is limited by the availability and preferences of individual experts to concentrate on the complex endeavour of land use modelling the operational part of the modelling must be as simple as possible through the separation of concerns the model can be fully run with a self explanatory single set of parameters and enables users to independently run the model after a very short about half a day training results are not self explanatory in relation to the underlying land use change processes land use modelling brings together many different mutually dependent factors the many built in data visualisations empower users to disentangle the underlying land use transitions that take place both at cell level and for the study area as a whole although the visualisations facilitate analysis the user must have good understanding of land use change processes to correctly interpret the results data preparation for and post processing of clue runs are time consuming since iclue only uses a single parameters file there is no longer a need to manually harmonise a set of parameter files as input post processing into the analysis and visualisation tools is automatically handled by the integrated quickscan environment less input files and automatic post processing speed up the cycle of model execution and result analysis it stimulates to do iterations to improve the modelling results just like in clue gis data is required to all be in the same spatial extent resolution and projection error prone due to the many manual actions required for data conversion scripts and tools the many coding file naming and file structuring conventions and manual guarantee of coding consistencies across files the automation of the many manual steps in iclue makes the whole procedure of data preparation and model run less error prone the user no longer needs to know how data is transformed and coded and how the separate processes make use of that data however the automation also implies that the user does not necessarily has to look into the results of the statistical analysis and thereby might miss out on algebraic insights lacking clear warnings and error messages hampering efficient handling of causes a syntax and semantic check takes place over the parameters file in case of incorrectness or incompleteness of the parameters file an error report with suggestions for improvement is shown in addition to the issues forming the rationale behind the development of iclue we observed new forms of use emerging firstly the accelerated operation and introduced transparency facilitates the understanding of the underlying processes and stimulates and enables the adaptation of model parameters for subsequent runs thereby creating opportunities for interactive use during participatory spatial planning and participatory modelling voinov and brown gaddis 2008 hewitt et al 2014 verweij et al 2016 secondly the introduced transparency enhances understanding of modelled land use change processes opening up new possibilities for methodological innovations for land use modelling thirdly the embedding into a modelling framework enhances flexibility in terms of integrated linkages to other models e g wagner et al 2017 connor et al 2015 knapen et al 2013 and it allows to do further post processing e g into land use dependant indicators such as ecosystem services or sustainability indicators or impact assessment and scenario analysis although we argue that iclue is a technical and conceptual improvement over previous versions of the clue model there are also some critical issues each member of the clue model family produces slightly different results so does iclue because of the model innovations e g demand deviation validation per land use type and the sampling from the land use probability distribution changes are relatively small and do not influence the overall interpretation iclue allocates demands for each time step year if demands are not met in a time step the model stops execution and displays a warning explaining why the model halts when demands are met within tolerance limits the model does not automatically show how far off the results are this deviation could be helpful in fine tuning the tolerances the deviation can currently be analysed by performing manual diagnostics including a random number generator inhibits reproducibility however various model runs using the same input parameters result in similar output patterns this type of model is targeted at scenario studies in which the focus is on patterns of change rather than exact predictions for each location so far all applications used regressions to determine suitabilities try outs with bayesian belief networks and especially the interactive visualisation of belief bars show that these are likely to improve the transparency of the suitability statistics bayesian belief networks are also a useful tool to include stakeholder knowledge non spatial information e g policies and management options and decision hierarchy celio et al 2014 hewitt et al 2014 mahamane et al 2017 however further developments are required to fully automatically derive bayesian models with probability tables from driver samples as bayesian evidence currently the trace back tool displays land use through time for a given location see fig 5 b the trace back tool would profit from additionally showing graphical overviews of probability bars for each land use type through time and the contribution of each determinant in the total probability iclue does not and cannot address all issues related to land use modelling verstegen et al 2014 celio et al 2014 and van vliet et al 2016 provide methods for calibrating and validating land use projection models to improve the allocation future improvements could seek links with other types of models to improve e g agency grashof bokdam et al 2017 murray rust et al 2014 ralha et al 2013 or representation of process based information during the development of iclue the authors worked intensively together in building the software amongst the team this co creation process built trust in the model software and its usage as each member reviewed the work done by the other team members the iclue source code is freely available https doi org 10 5281 zenodo 1100980 under eupl 5 5 https joinup ec europa eu sites default files custom page attachment eupl v1 2 en pdf and as part of the quickscan modeling environment quickscan can be obtained via www quickscan pro iclue can directly be included in java based software or wrapped in scripting languages like r 6 6 https www r project org or python 7 7 https www python org for further analysis author contributions pv kk mve ac sj and jtr designed the research kk mve pv ac wdw and igs designed the model pv jtr and wdw developed the software mve pv and ac ran and tested the model pv ac sj kk wrote the paper and mps raised funds and commented on the paper acknowledgements the authors gratefully acknowledge the funding received for this work the work has been funded through the european union seventh framework programme fp7 2007 2013 under grant agreement no 283093 the role of biodiversity in climate change mitigation robin horizon 2020 research and innovation programme under grant agreement no 642139 minatura2020 and the dutch ministry of economic affairs under the strategic research programs sustainable spatial development of ecosystems landscapes seas and regions and system earth management special thanks go to dennis walvoort from wageningen environmental research and martin karlsen and anders madsen from hugin for their statistical expertise annex i iclue parameter template property file uses key value notation the symbol cannot be used for other purposes key cannot contain any white spaces use camel casing instead key uses namespace notation a between key parts to denote a hierarchical relation a value can contain white spaces in value the symbol is used to separate list elements it can therefore not be used for other purposes baseline landuse map and year that the map represents example baseline filename d clue mexico rob lu 16a example baseline year 2005 landuse classes code in map file colour code in hex rgb ease of change initial age in years demand deviation type demand deviation amount colour examples red ff0000 green 00ff00 blue 0000ff yellow ffff00 white ffffff black 000000 grey aaaaaa orange ffaa00 purple aa00ff see also http www color hex com color names html ease of change very easy easy hard very hard cannot change demand deviation type absolutedeviation cell count percentagedeviation 0 100 example 1 landuseclass forest 10001 38a800 hard 100 absolutedeviation 2047 example 2 landuseclass urban 10002 38a800 very easy 22 percentagedeviation 15 administrative units map and list of unit name and unit code example administrativeunits filename d clue europe masker example administrativeunit netherlands 1 example administrativeunit belgium 2 demands line with sequence of landuse classes line with same sequence of landuse demands per year example landusedemands sequence forest urban example landusedemand netherlands 2025 430787 232460 example landusedemand netherlands 2050 530787 132460 example landusedemand belgium 2010 300 200 example landusedemand belgium 2050 400 100 drivers can be constant or dynamic driver dynamic drivers change over time for every driver line 1 datatype qualitative quantitative line 2 filename full path line 3 etc class classname class code in map file class colour in hex rgb the following 4 examples illustrate 1 qualitative constant driver 2 quantitative constant driver 3 qualitative dynamic driver 4 quantitative drynamic driver example 1 parametermap constant ecoregions datatype qualitative example 1 parametermap constant ecoregions filename d clue mexico wwf ecoregion example 1 parametermap constant ecoregions class boreal 204 ffaa5b example 1 parametermap constant ecoregions class pannonioal 205 22e4ff example 1 parametermap constant ecoregions class tundra 206 ffff00 example 2 parametermap constant energycrophectare datatype quantitative example 2 parametermap constant energycrophectare filename d clue mexico rk encrop ha example 3 parametermap dynamic temperature datatype qualitative example 3 parametermap dynamic temperature class cool 1 0000ff example 3 parametermap dynamic temperature class moderate 2 ffaa00 example 3 parametermap dynamic temperature class hot 3 ff0000 example 3 parametermap dynamic temperature filename 2005 d samplepath filename 2005 example 3 parametermap dynamic temperature filename 2012 d samplepath filename 2012 example 3 parametermap dynamic temperature filename 2020 d samplepath filename 2020 example 4 parametermap dynamic populationdensity datatype quantitative example 4 parametermap dynamic populationdensity filename 2005 d samplepath filename 2005 example 4 parametermap dynamic populationdensity filename 2010 d samplepath filename 2010 example 4 parametermap dynamic populationdensity filename 2020 d samplepath filename 2020 suitability calculation line 1 method stepwiseregression functiondictionary line 2 depending the method line 2 stepwiseregression samplesizepercentage decimal number between 0 100 percentage of the number of cells for each land use class that ll be used to do the regression upon line 3 stepwiseregression correlationthreshold decimal number between 0 1 drivers are being correlated for each landuse if drivers are highly correlated above threshold the driver with the lowest correlation with the landuse class is omitted line 4 stepwiseregression exportfilename d path filename prop example suitability method stepwiseregression example suitability stepwiseregression samplesizepercentage 7 5 example suitability stepwiseregression correlationthreshold 0 85 line 2 functiondictionary adminunit landuseclass functionconstant decimal number between 1 1 constant value in function line 3 functiondictionary adminunit landuseclass functioncoefficient driver decimal number between 1 1 coefficient value in function for quantitative driver line 4 functiondictionary adminunit landuseclass functioncoefficient driver class classname decimal number between 1 1 coefficient value in function for qualitative driver line 5 etc for driver and landuse class conversion choose from the options always never years 7 location d samplepath conservationareas tif default is always no need to include a land use conversion that can take place always for location areas with data are not allowed to be converted areas without data nodata can be converted example 1 conversion urban forest never example 2 conversion forest urban years 15 example 3 conversion forest arable location d samplepath conservationareas tif target time define until what time land use allocation calculations take place example targettime 2050 
26346,predicting the rate of escherischia coli e coli loss in a river network is one of the key conditions required in the management of bathing waters with well verified numerical models being effective tools used to predict bathing water quality in regions with limited field data in this study a unique finite volume method fvm one dimensional model is firstly developed to solve the mass transport process in river networks with multiple moving stagnation points the model is then applied to predict the concentration distribution of e coli in the river ribble network uk where the phenomena of multiple stagnation points and different flow directions appear extensively in a tidal sub channel network validation of the model demonstrates that the proposed method gives reasonably accurate solution the verification results show that the model predictions generally agree well with measured discharges water levels and e coli concentration values with mass conservation of the solution reaching 99 0 within 12 days for the ribble case an analysis of 16 one year scenario runs for the ribble network shows that the main reduction in e coli concentrations occurs in the riverine and estuarine regions due to the relatively large decay rate in the brackish riverine waters and the long retention time due to the complex river discharge patterns and the tidal flows in the regions keywords mass transport one dimensional model river networks multiple stagnations e coli loss 1 introduction escherischia coli e coli loss at the river estuary transition zone is a complex process where decay and production through various sources coexist the pattern of e coli loss varies from case to case and is governed by their biotic intrinsic parameters abiotic environmental conditions and episodic sources field sampled data are important in the evaluation of the fate of e coli but they are usually limited therefore numerical models are often used together with limited field measurements and laboratory analysis to evaluate quantitatively the e coli losses in riverine and coastal waters servais et al 2007 however the accuracy of the models used needs to be verified to ensure that the solutions are stable and mass conservative as well as including appropriate values for key parameters such as bed roughness dispersion and decay rates steets and holden 2003 a mass conserved stable accurate and computationally manageable model is therefore a prerequisite for e coli concentration evaluation since rainfall runoff intensities enter river channels in pulses often at minute scales creating large gradients in pollutant concentrations sanders et al 2001 this is especially important in complex river networks with relatively steep gradients and also where highly unsteady tidal currents exist in the estuarine and coastal zones a small mass conservation error in the hydrodynamic solution may cause a large error in the matter transport solution bousso et al 2012 however it is often difficult to obtain highly conservative solutions in a natural river system for a number of reasons including the use of non consistent governing equations aral et al 2000 partial or full linearization of the governing equations different discretized formats between the hydrodynamic and mass transport model equations etc in order to improve on the mass conservation properties of such solutions the finite volume method fvm murillo and navas montilla 2016 wu and wang 2008 zhang et al 2011 is increasingly used in water quality modelling studies together with an unstructured grid however when an explicit fvm model is used two key shortcomings remain one being the smaller time step imposed by the courant friedrichs lewy cfl limiting condition delis et al 2000 stelling 2003 and the difficulty in maintaining robustness for complex looped and dendritic river networks jin et al 2002 for long term simulations e g for up to 100 years and for a series of scenario runs of the hydrodynamic sediment and mass transport processes 1d models are extensively used because of their higher efficiency and even higher accuracy than 2d and 3d models when dealing with large and complex river networks lauer et al 2016 wu et al 2004 zhou and lin 1998 usually a 1 d model is used to link a catchment hydrological model merkhali et al 2015 paiva et al 2011 and a 2d or 3d estuarine and or coastal model blad et al 2012 twigt et al 2009 therefore 1d models are generally invaluable tools in an integrated modelling system for simulating hydrological hydrodynamic and mass transport processes from the catchment cells to river networks and then to the receiving estuarine and coastal waters nana et al 2014 salvadore et al 2015 in general different flow directions often exist in estuaries caused by the river flow and tidal waves and there four basic flow directions can exist in a sub channel including i down flow ii up flow iii inward flow and iv outward flow zhang et al 2014 for the case of iii or iv a positive stagnation point or a negative stagnation point will occur see fig 1 c however in a sub channel there may be more than one stagnation point and the number of stagnation points and their locations can change continuously due to the interaction between the tides and river flows stagnation can also occur at more than a single point in an estuary and or river reach therefore an existing algorithm for dealing with only one stagnation developed by hu et al 2010 and zhang et al 2014 has been refined in the current study to enable the physical processes of multiple stagnation zones to be predicted the main objective of this study is therefore to improve on the accuracy of numerical model predictions of e coli losses in river networks and to reduce the error level in mass conservation details are given of the development of a fvm based model to simulate the mass transport processes in river and estuarine networks particularly where multiple stagnation zones and different flow directions may occur firstly in this model a new algorithm is developed to predict the formation of multiple stagnation zones and the mass transport processes in these zones secondly a dynamic decay rate is formulated for different salinity and radiation levels based on data obtained from laboratory studies and field investigation thirdly field measured hydrodynamic and e coli data acquired for the river ribble network and fylde coast in 2012 are used to calibrate and validate the hydro epidemiological model finally the loss of e coli in the river ribble network is evaluated using the refined 1d model a series of scenario simulations are also reported using the refined 1d modelling system and the e coli losses in the middle and lower regions of the river ribble including different sources from 47 sub catchments are quantitatively predicted the results show the importance of the need for model mass conservation especially in the lower reaches of the river basin where the reversing current and the multiple stagnation zones appear extensively driven by tidal and river flow interactions 2 theory model framework 2 1 hydrodynamic model the st venant equations are widely used as the governing equations to predict the hydrodynamic processes in river networks as given by 1 b z t q x q 2 q t x q 2 a g a z x s f s e l 0 where b wetted cross sectional width m z elevation of water surface above datum m q river discharge m3 s q lateral discharge per unit channel width m2 s x curvilinear distance of river channel m t time s a wetted cross sectional area m2 g gravitational acceleration m s2 s f friction slope expressed as s f n 2 q q a a b 4 3 in which n manning s coefficient s e local longitudinal slope of water surface due to localised head losses and l momentum of lateral discharge inputs 2 2 mass transport model the mass transport equation given as 3 a c t q c x x a e x c x s c w c where e x longitudinal dispersion coefficient which is based on a formula derived by fisher fischer 1973 s c a source term due to bacterial decay s c i n s k c k decay rate hr 1 ins constant source term for e coli this term is zero in engineering studies the t90 which is the time needed for 90 of the bacteria to die off t90 ln10 k unit is hr is usually used the value of t90 is related to radiation salinity and organic matter etc yang et al 2008 the limited measured data is given in table 1 huang et al 2017 based on radiation and salinity condition then intepolated t90 is used in the model based on the given radiation and modelled salinity w c external sources from point and diffuse source inputs which is decided by the e coli flux from lateral sub catchments the fvm is used to improve on the mass conservation of eq 3 however the consistency between appendix s1 eqs 2 3 and eq 3 may not be entirely satisfactory because of the additional errors introduced by the linearization in deriving appendix s1 eqs 2 3 therefore a small time step approach is used i e several inner iterations are carried out within a time step to reduce the errors in the solution of the hydrodynamic equations in this way the mass conservation level is improved in the solution of eq 3 the staggered grids where the hydrodynamic and water quality variables are located at the cross sections and the centre of a control volume respectively see fig 1 are used to further reduce the mass conservation error after obtaining the solution of concentrations at junctions an explicit method is used to determine the e coli concentration value for each control volume for the case of a positive flow the mass transport equation eq 3 is discretized using the implicit upwind scheme as given in eq 4 4a a c t a c j k 1 a c j k  t 4b q c x q c j k 1 q c j 1 k 1  x j 1 4c x a e x c x 1  x j 1 a e x j 1 k 1 c j k 1 c j 1 k 1  x j 2  x j 1 2 a e x j k 1 c j 1 k 1 c j k 1  x j  x j 1 2 4d s c w c a k c j k 1 i n s w c k 1 where a j k b j k z j k average river area width and water elevation in the jth control volume respectively at the kth time step the linear equations for the krth sub channel can be written as 5 a j c j 1 k 1 b j c j k 1 c j c j 1 k 1 z z j j j s j s 1 j e where a j b j c j coefficients z z j explicit term c j k 1 mass concentration in control volume j at the k 1th time step js je the start and end cross section number for the krth sub channel respectively the other three flow directions types are discretized in a similar manner for the flow pattern shown in fig 1b the coefficients a j b j c j z z j in the inner cross sections j js js 1 je are derived in a similar manner as for the euquations 1 and 2 see appendix s2 eq 1 in order to predict the flow patterns in tidal river reaches with multiple stagnation points a refined algorithm has been developed based on existing work zhang et al 2014 in this model i a search was carried for each cross section to identify the existence of a stagnation in a sub channel and possible position using eq 6 and eq 7 ii the flow and stagnation type is identified based on theflow direction distributions see kr1 6 in fig 1c and solute concentrations within the inner junctions where multiple stagnation points existed using eq 8 and iii a set of linear equations and related parameters were reconstructed for a junction with multiple stagnations in the mass transport solution see eq 9 once the values of the concentration at the kth time step are known the concentration values at the k 1th time step for the krth sub channel can then be calculated using appendix s3 eq 1 together with the concentration values at the starting and ending junctions moreover because the lower e coli concentration boundary is required during a flood tide the simplified lower boundary is given using eq 10 and eq 11 based on the refinement steps above the main procedure used to solve the hydrodynamic and mass transport equations are shown in a flowchart in fig 2 identification of a stagnation point in a sub channel based on the flow directions at two adjacent cross sections of a control volume 6 q j 1 q j 0 a moving stagnation zone s position x s t g y s t g is identified by eq 7 7 x s t g x t j 1 q j 1 q j 1 q j x t j x t j 1 y s t g y t j 1 q j 1 q j 1 q j y t j y t j 1 where x t j y t j coordinates at the jth cross section of a sub channel if there are m stagnation points m 2 in a sub channel there will be m 1 reaches with each having a single flow direction or the e coli concentrations of each control volume can be predicted using the formulae for the and flow patterns if the concentration values for each stagnation zone are given the e coli concentration for a control volume with a stagnation point is solved for in two steps step 1 solving the c j k 1 at the control volume with the stagnation there are two types of stagnations one is the positive stagnation where water flows inwards and the other is the negative stagnation where water flows outwards an explicit upwind scheme is used to determine the e coli concentration value at the control volume with a stagnation point for example for a positive stagnation case then c j k 1 is determined using eq 8 8 c j k 1 a m j k a m j k 1 c j k  t a m j k 1 q j 1 k 1 c j k q j k 1 c j k  x j 1  t  x i 1 a m j k 1 a e x i 1 k 1 c i 1 k c i k  x i 2  x i 1 2 a e x i k 1 c i k c i 1 k  x i  x i 1 2  t a m j k 1 s c w c j k 1 step 2 re construction of linear systems with multiple stagnations here we use fig 1 c to illustrate the construction of the equations for the four flow directions and multiple moving stagnation zones assuming that the mass is well mixed at the junction and omitting the junction s storage variation the mass conservation equation for j7 see fig 1c can be written as 9 q k r 1 j e c k r 1 j e q k r 4 j e c k r 4 j e q k r 5 j e c k r 5 j e q k r 2 j e q k r 3 j e q k r 6 j e c j 7 0 the parameters of implicit algebraic equations eq 5 can be solved using appendix s2 eqs 1 and 2 during the construction of the concentration equation at a junction linked with boundaries the upper concentration boundary value is required for a positive flow or the low concentration boundary value is required only for a negative flow for a sub channel with multiple stagnation points only the concentration values at the first and last negative stagnation points are needed the number and position of the stagnation point s may change but the matrix structure remains the same after solving the concentration at the internal junctions using eq 9 then the concentration at the control volume centre for every sub channel can be solved using appendix s3 eq 1 during a flood tide a lower boundary condition is required and water elevation and e coli concentration values need to be specified however it is often difficult to obtain the measured e coli concentration data herein we follow the returned coefficient concept proposed by falconer 1984 which is expressed in eq 10 10 c t  c where c t e coli concentration input from the sea boundary at time t  e coli loss coefficient which ranges from 0 0 to 1 0 c mean e coli concentration across the sea boundary defined as 11 c t 1 t 2 q t c t d t t 1 t 2 q t d t in order to reduce the uncertainty level a series of the virtual volumes have been added to the seaward boundary to store the outflow and e coli during the ebb tides and a proportion of the integrated e coli efflux will return to the riverine networks through the sea boundary during the subsequent flood tides the e coli losses are mainly caused by two mechanisms one is related to the tidal and river flow characteristics and the other is by the natural decay of e coli in the virtual volumes 3 application 3 1 model setup for the ribble case uk the ribble river basin is located in the north west of england it originates from the rural hills of the yorkshire dales and the source of the river ribble to major urban areas of lancashire including blackburn burnley and preston with an area of 1583 km2 it has 4 key tributaries including the rivers hodder calder darwen and douglas as well as the crossens drainage system which flows into the ribble estuary see fig 3 it is the only uk research catchment for studies linked to the eu water framework directive wfd implementation kay et al 2005 and a significant amount of historical data on the river topography hydrology water quality and e coli measurements have been collected over the past 20 years the following data were used in the model i bathymetric data in the estuary and riverine regions which were converted into 1 d cross section data by interpolation huang et al 2014 ii 15 min sampled discharges and hourly e coli concentration data predicted at 5 main upper river boundaries namely the ribble no 710305 hodder no 711610 calder no 712615 darwen no 713122 and douglas no 700306 and 7 sub catchments as shown in fig 3 iii the inflow and e coli boundaries from 5 main upper rivers and 47 minor branches provided by the uk environment agency ea and sheffield university using the hydrological simulation program fortran hspf bicknell et al 1997 and infoworks wallingford software ltd 1995 models and iv half hour tidal elevation data at the lower boundary hourly meteorological data were acquired at 9 stations around the 1 d model domain and interpolated into the cross sections of the 1 d model these data included air and earth temperatures radiation levels relative humidity rainfall etc with the data being provided by the british atmospheric data centre uk http badc nerc ac uk home the main parameters used in the model are listed in table 1 3 2 mass conservation test in order to test the refined model two objective functions were used to check the mass conservation level i temporal difference between the net input flow volume and water storage across the whole model domain  v w o b j k 1 ii accumulated water conservation error s u m v w o b j k 1 they are expressed as 12a  v w o b j k 1 v w i o k 1 d v w k 1 12b s u m v w o b j k 1 s u m v w o b j k  v w o b j k 1 where v w i o k 1 is the net increase in the water volume from the inflow and outflow boundaries and point sources after each time step v w i o k 1 n b 1 n b d f n b q n b  t n p 1 n p s q n p  t f n b 1 and 1 for inflow and outflow boundaries respectively d v w k 1 v w k 1 v w k v w k and v w k 1 are the water volume in the model domain at k and k 1 time step respectively the model predicted results are presented in fig 4 the water storage values predicted by the refined method are very close to the net input water volume values see fig 4a the accumulated water mass conservation error is also small see fig 4b with the relative error being about 0 3 in 12 days where v e c i o k 1 n b 1 n b d f n b q n b c n b  t n p 1 n p s q n p c n p  t and d v e c k 1 v e c k 1 v e c k v e c k and v e c k 1 are the total e coli counts in the model domain at n and n 1 time steps respectively in which v e c k 1 j 1 n s c a j k 1 c j k 1  x j for this case the decay rate for the e coli solution was set to zero the predicted values of the objective variables are shown in fig 5 these results indicate that the level of agreement between v e c i o and d v e c is generally very close see fig 5a nevertheless there is also a relatively small accumulated mass loss see fig 5b with the average error being around 1 0 of the total e coli number over the simulation period 3 3 model verification in verifying the model the inflow discharges were measured at the 5 main upper boundaries to drive the model in addition discharges from the sub catchments were calculated using the hspf model where the calculated discharges were used to compensate for the shortage of measured discharge data in these catchments flow discharges and stage data at 3 main control stations no 700306 713056 and 713019 see fig 3 were used to verify the enhanced river network model it can be seen from fig 6 a and b that the model predicted and measured flow discharge values at the two control stations no 713056 and 713019 in the main channel generally agreed very well for the cases of low and medium flows with the statistical parameters rmse mae and nsce being presented in table 2 the model under predicted the maximum flood discharges at the two stations on 23rd june 2012 but predicted other peak flows quite well the errors are thought to be mainly caused by the local rainfall measurements and spatial interpolation errors based on the limited rainfall stations the comparison between the predicted water elevations and measured data at stations along the rivers douglas no 700306 and ribble no 713019 respectively agreed satisfactorily fig 6c and d and the statistical value of the nsce parameter at these two stations were 0 85 and 0 64 respectively other statistics are presented in table 2 the under estimated water elevation at flood peak on 26th september 2012 for the river douglas no 700306 is thought to be caused by the spatial interpolation error of the intense rainfall based on limited rain gauge stations and flow discharge under estimation by the hspf model further verification will be carried out when new and continuous measured water level data are available the e coli concentrations were measured at more than 10 stations in the ribble catchment by the centre for research into environment and health creh aberystwyth university as a part of the cloud to coast c2c project the results for 2 stations are presented herein i e fig 6e and f with comparisons being given between model predicted and measured e coli concentrations at two sites along the main channel of the river ribble it can be seen that the model predicted e coli concentrations generally agree well with the limited measured data but the model over predicted the concentration during the period from 8th to 28th august 2012 the error analysis results for the parameters rmse mae and nse are presented and listed in table 2 the rmse values are about 1 2 and 0 9 times of the average values of the measurements at these two stations while the mae and nsce values range from 9893 5 cfu 100 ml to 654 9 cfu 100 ml and 0 32 to 0 56 respectively since the e coli concentration includes certain uncertainty factors the predicted e coli concentration values are considered acceptable although there are some large errors during the high flow period the errors in predicting the peak values may be partly attributed to the sparse sampling rate relative to that predicted at each time step in the numerical and thereby potentially resulting in some high concentration points being missed in the measured data 3 4 evolution of stagnation zones in the river ribble networks based on the definition given by clancy 1975 a stagnation zone is a flow field where the local velocity of the fluid is zero stagnation exists extensively at the transition zone between the river and sea driven mainly by the river flows and tidal currents and occasionally driven by unsteady flows in the river networks in the current study the existence of stagnation zones was checked and their dynamic positions were simulated using eqs 12 and 13 and the 1 d hydrodynamic model has been extended to include a module on multiple stagnation zones the generation movement and extinction of stagnation zones are shown in fig 7 based on the model results the key processes in the stagnation zones can be summarised as follows i during low ebb phase there is a strong downward flow and thus stagnation does not exist ii during the flood phase the upward flow from the estuary meets the river flow then a positive stagnation is generated in the lower estuary fig 7a it then moves upwards with the tidal currents with more stagnation zones potentially being formed in river branches of the river network fig 7b iii during the flood to ebb phase a negative stagnation forms in the lower estuary fig 7c and it then moves towards the upper reaches with the previous positive stagnation zones moving downwards if a sub channel is long enough then the positive and negative stagnation zones can coexist fig 7d until the moving stagnation zones merge and then disappear finally the flow returns to a single downward direction and the processes of phase i to iii will be repeated the processes of generation movement and extinction of the multiple stagnations driven by the tidal and river flow interactions in the ribble river networks were predicted using the numerical model the results indicate that the multiple stagnation zones may appear in the river networks in the flood to ebb stage 3 5 source apportionment and its impacts in order to evaluate the rural and urban source apportionments and its impacts on the lower reach of the river ribble 16 one year scenarios see table 3 were simulated in which the inputs from the 7 sub catchments were combined see fig 3 to simplify the calculation procedure the simulation results are shown in table 4 it can be seen from table 4 that in the river ribble networks the total e coli loss is between 31 and 53 which varied with different source locations and dynamic weather and hydrodynamic conditions before the e coli flux arrives at the tidal limit station i e bullnose from the upper reaches approximately 8 of the e coli died off in the long narrow middle and upper reaches of the river ribble over 40 of the e coli then died off in the middle and lower reaches in total about 80 of the local e coli losses occur in the riverine and estuarine regions from bullnose to 11 mp for the following reasons i a large retention time caused by the tidal reciprocating flows in the wide and shallow channels ii a higher decay rate due to the increasing salinity levels in the shallow salt marshes mancini 1978 particularly when compared with the fresh water in the upper and middle reaches in general the rural and urban e coli sources have different characteristics the rural region with a large proportion of e coli sources from livestock is generally located in the upper and middle reaches of the river basin while the urbanised communities with an important portion of e coli from domestic sewage industrial waste water etc are located in the lower reaches of the river basin close to the receiving waters i e estuaries and coastal zones the urban e coli sources are controlled more frequently by man made devices such as waste water treatment plants wwtps and combined sewer overflows csos with the retention and transportation times for e coli general being increased and decreased respectively in these devices thus they may cause a non consistent phase difference between the flow discharge and the e coli fluxes the transport time and related loss rate for urban source usually varies considerably especially for extreme flow events the general transport time for rural e coli sources can therefore be shorter than the corresponding urban sources although the distance between the rural e coli source and the receiving waters is usually longer in the ribble river networks the overall e coli decay rate from the rural source is about 3 higher than that from urban sources meanwhile there is some exceptional variations in the darwen sub catchment where the urban e coli source is dominant due to the highly urbanised level and population density in the basin 4 discussion 4 1 validity and effectiveness of the proposed methods the accuracy of the solution of the 1 d mass transport equation depends partly on the representation of the hydrodynamic equations and partly on the accuracy of the discretization method of these equations moreover the errors from the hydrodynamic solutions may be transferred to the mass transport solutions and may even cause fluctuations in the solutions the 1 d st venant equations with z and q being main variables frequently used in the engineering community are not strictly conservative cunge et al 1980 because of the approximation a t b z t and the error from the approximation during linearization will increase when the width b varies significantly within a time step in a river particularly where a shallow and wide river has a narrow deep main channel in order to enhance a consistent solution between the hydrodynamic and mass transport equations limited inner iterations in eq 1 are carried out to reduce the error from the flow solution the preissmann scheme is based on a bi diagonal implicit finite difference method for solving the 1 d st venant equations and is unconditionally stable and robust however the mass and momentum equations i e eqs 1 and 2 are only equivalent to the discretized equation appendix s1 eqs 2 and 3 when the conditions  a a and  q q are satisfied for some special conditions e g near bank full discharge low tide or stagnant flows  a or  q may be of a similar order of magnitude or even larger than a and q then the assumed conditions cannot be satisfied and some large errors may occur in the hydrodynamic solutions in order to enhance the preissmann scheme the method of limited inner iterations with a smaller time step is used hu et al 2010 together with the mass conservation check in the hydrodynamic solutions also the transformation from the finite difference method fdm to fvm based on the staggered variables distribution improves the mass conservation level of the solution furthermore the solution for multiple stagnation zones makes the model predictions closer to the real physical process for tidal wave propagation and the interaction with the flow in river networks 4 2 e coli concentration difference at stations in order to evaluate the model results a comparison was made of the predictions fig 8 made using the three methods including i the finite difference method ii the finite volume method with a single stagnation zone fvm s and iii the finite volume method with multiple stagnation zones fvm m at 2 stations no 9 mp and no dgs995 in fig 3 during the calculation the main parameters such as decay rate and returning coefficient 0 1 were kept the same the main findings can be summarised as follows i in the upper and middle reaches the e coli concentration differences between the fdm and fvm algorithms is small because the flow direction is identical ii in the lower region because of the existence of a reversing current the mass loss is relatively large when the fdm algorithm is used and there is a relatively large e coli concentration difference between the fdm and fvm algorithms at the 9 mp station fig 8a and b and dgs 995 station fig 8c and d and iii there may be more than one stagnation zone in a sub channel driven by the tidal and river flows especially during the second half of the spring to ebb tidal period since the duration of multiple stagnation zones in the ribble river is relatively short the impact of multiple stagnation zones is minor on the e coli processes and the predicted concentration difference between a single and multiple stagnation zones is small in the main river and the estuarine region however in the river douglas the occurrence of multiple stagnation zones is more common because of the weak river flow the strong tidal currents and the long branched channel with a small bed slope therefore in this river the predicted difference in the e coli concentrations between the two fvm methods is much larger than that in the river ribble fig 8d the refinement of the solution method makes the predictions closer to the physical process and increases the model s generality when compared to the original method moreover the solution is more stable when the fvm algorithm is used 4 3 e coli loss rate by different methods it can be seen from table 4 that the fdm s algorithm can predict larger e coli losses due to its non conservation property and the fvm algorithm can enhance the mass conservation level by up to 10 with the same decay rate and returning coefficients at the lower boundary there are no obvious difference in the e coli predictions with single and multiple stagnations zones when the two fvm algorithms are compared as confirmed by the percentage losses shown in fig 8 and table 5 the decay rate in lower river reaches and the estuarine waters is larger than that in the upper riverine reaches and about 16 48 of the e coli will die off in the lower river reaches and the estuarine waters meanwhile because of heterogeneity in the bed sediments and vegetation in the region the transport processes and the fate of e coli in the lower river reaches and the estuary are complex and further study is needed in order to reduce the level of uncertainty 5 conclusions a refined one dimensional model has been developed for improving the mass conservation solution properties for solute mass fluxes particularly for e coli using consistent equations a staggered grid and transformation from a fdm to a fvm algorithm moreover an enhanced approach is proposed to solve the mass transport equation in a sub channel where there may be multiple stagnation zones which may be a general phenomenon in the lower reaches of a tidal river the test case for the ribble river basin and estuary shows that the mass conservation level reaches 99 0 after 12 days of simulation using the refined model and for an extremely complex flow and tidal dynamics scenario with the model generally predicting the discharge water elevations and e coli concentrations to a high degree of accuracy for the highly unsteady field measurements acquired in 2012 the refined and verified 1 d model has been applied to 16 one year scenarios for different e coli source apportionments based on the results obtained using the hspf and infoworks models the results from these model scenarios indicate the following i the degree of mass conservation in the numerical model solution is a prerequisite condition for the evaluation of the source transport and fate of e coli bacteria ii in the ribble catchment the e coli inputs are mainly from the darwen calder and douglas rivers and the middle and lower reaches of the river ribble with highly urbanised and high population density areas contributing a large proportion of these inputs the transport time and related loss rate for the urban sources usually varies considerably typically 16 48 of the e coli died off during the transport processes from the input sources to the river ribble outlet with these findings being attributed to the complex hydrodynamic and tidal conditions predicted in the modelling system iii the fate of e coli concentrations was found to be closely linked to the source positions and the solute and mass transport processes associated with the local hydrodynamic and salinity conditions acknowledgements the research reported herein was funded by the natural environment research council the medical research council the department for environment food and rural affairs and the economic and social research council gr ne i008306 1 uk the authors are also grateful to the environment agency north west for their provision of data and to all colleagues from the universities of aberystwyth and sheffield working on the nerc c2c project and in particular to professor david kay and dr carl stapleton of the centre for research into environment and health creh who provided the concentration data for source apportionment the detailed and rigorous comments made by the three anonymous reviewers are also much appreciated appendix a supplementary data the following are the supplementary data related to this article data profile data profile emands final manuscript emands final manuscript appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 07 009 
26346,predicting the rate of escherischia coli e coli loss in a river network is one of the key conditions required in the management of bathing waters with well verified numerical models being effective tools used to predict bathing water quality in regions with limited field data in this study a unique finite volume method fvm one dimensional model is firstly developed to solve the mass transport process in river networks with multiple moving stagnation points the model is then applied to predict the concentration distribution of e coli in the river ribble network uk where the phenomena of multiple stagnation points and different flow directions appear extensively in a tidal sub channel network validation of the model demonstrates that the proposed method gives reasonably accurate solution the verification results show that the model predictions generally agree well with measured discharges water levels and e coli concentration values with mass conservation of the solution reaching 99 0 within 12 days for the ribble case an analysis of 16 one year scenario runs for the ribble network shows that the main reduction in e coli concentrations occurs in the riverine and estuarine regions due to the relatively large decay rate in the brackish riverine waters and the long retention time due to the complex river discharge patterns and the tidal flows in the regions keywords mass transport one dimensional model river networks multiple stagnations e coli loss 1 introduction escherischia coli e coli loss at the river estuary transition zone is a complex process where decay and production through various sources coexist the pattern of e coli loss varies from case to case and is governed by their biotic intrinsic parameters abiotic environmental conditions and episodic sources field sampled data are important in the evaluation of the fate of e coli but they are usually limited therefore numerical models are often used together with limited field measurements and laboratory analysis to evaluate quantitatively the e coli losses in riverine and coastal waters servais et al 2007 however the accuracy of the models used needs to be verified to ensure that the solutions are stable and mass conservative as well as including appropriate values for key parameters such as bed roughness dispersion and decay rates steets and holden 2003 a mass conserved stable accurate and computationally manageable model is therefore a prerequisite for e coli concentration evaluation since rainfall runoff intensities enter river channels in pulses often at minute scales creating large gradients in pollutant concentrations sanders et al 2001 this is especially important in complex river networks with relatively steep gradients and also where highly unsteady tidal currents exist in the estuarine and coastal zones a small mass conservation error in the hydrodynamic solution may cause a large error in the matter transport solution bousso et al 2012 however it is often difficult to obtain highly conservative solutions in a natural river system for a number of reasons including the use of non consistent governing equations aral et al 2000 partial or full linearization of the governing equations different discretized formats between the hydrodynamic and mass transport model equations etc in order to improve on the mass conservation properties of such solutions the finite volume method fvm murillo and navas montilla 2016 wu and wang 2008 zhang et al 2011 is increasingly used in water quality modelling studies together with an unstructured grid however when an explicit fvm model is used two key shortcomings remain one being the smaller time step imposed by the courant friedrichs lewy cfl limiting condition delis et al 2000 stelling 2003 and the difficulty in maintaining robustness for complex looped and dendritic river networks jin et al 2002 for long term simulations e g for up to 100 years and for a series of scenario runs of the hydrodynamic sediment and mass transport processes 1d models are extensively used because of their higher efficiency and even higher accuracy than 2d and 3d models when dealing with large and complex river networks lauer et al 2016 wu et al 2004 zhou and lin 1998 usually a 1 d model is used to link a catchment hydrological model merkhali et al 2015 paiva et al 2011 and a 2d or 3d estuarine and or coastal model blad et al 2012 twigt et al 2009 therefore 1d models are generally invaluable tools in an integrated modelling system for simulating hydrological hydrodynamic and mass transport processes from the catchment cells to river networks and then to the receiving estuarine and coastal waters nana et al 2014 salvadore et al 2015 in general different flow directions often exist in estuaries caused by the river flow and tidal waves and there four basic flow directions can exist in a sub channel including i down flow ii up flow iii inward flow and iv outward flow zhang et al 2014 for the case of iii or iv a positive stagnation point or a negative stagnation point will occur see fig 1 c however in a sub channel there may be more than one stagnation point and the number of stagnation points and their locations can change continuously due to the interaction between the tides and river flows stagnation can also occur at more than a single point in an estuary and or river reach therefore an existing algorithm for dealing with only one stagnation developed by hu et al 2010 and zhang et al 2014 has been refined in the current study to enable the physical processes of multiple stagnation zones to be predicted the main objective of this study is therefore to improve on the accuracy of numerical model predictions of e coli losses in river networks and to reduce the error level in mass conservation details are given of the development of a fvm based model to simulate the mass transport processes in river and estuarine networks particularly where multiple stagnation zones and different flow directions may occur firstly in this model a new algorithm is developed to predict the formation of multiple stagnation zones and the mass transport processes in these zones secondly a dynamic decay rate is formulated for different salinity and radiation levels based on data obtained from laboratory studies and field investigation thirdly field measured hydrodynamic and e coli data acquired for the river ribble network and fylde coast in 2012 are used to calibrate and validate the hydro epidemiological model finally the loss of e coli in the river ribble network is evaluated using the refined 1d model a series of scenario simulations are also reported using the refined 1d modelling system and the e coli losses in the middle and lower regions of the river ribble including different sources from 47 sub catchments are quantitatively predicted the results show the importance of the need for model mass conservation especially in the lower reaches of the river basin where the reversing current and the multiple stagnation zones appear extensively driven by tidal and river flow interactions 2 theory model framework 2 1 hydrodynamic model the st venant equations are widely used as the governing equations to predict the hydrodynamic processes in river networks as given by 1 b z t q x q 2 q t x q 2 a g a z x s f s e l 0 where b wetted cross sectional width m z elevation of water surface above datum m q river discharge m3 s q lateral discharge per unit channel width m2 s x curvilinear distance of river channel m t time s a wetted cross sectional area m2 g gravitational acceleration m s2 s f friction slope expressed as s f n 2 q q a a b 4 3 in which n manning s coefficient s e local longitudinal slope of water surface due to localised head losses and l momentum of lateral discharge inputs 2 2 mass transport model the mass transport equation given as 3 a c t q c x x a e x c x s c w c where e x longitudinal dispersion coefficient which is based on a formula derived by fisher fischer 1973 s c a source term due to bacterial decay s c i n s k c k decay rate hr 1 ins constant source term for e coli this term is zero in engineering studies the t90 which is the time needed for 90 of the bacteria to die off t90 ln10 k unit is hr is usually used the value of t90 is related to radiation salinity and organic matter etc yang et al 2008 the limited measured data is given in table 1 huang et al 2017 based on radiation and salinity condition then intepolated t90 is used in the model based on the given radiation and modelled salinity w c external sources from point and diffuse source inputs which is decided by the e coli flux from lateral sub catchments the fvm is used to improve on the mass conservation of eq 3 however the consistency between appendix s1 eqs 2 3 and eq 3 may not be entirely satisfactory because of the additional errors introduced by the linearization in deriving appendix s1 eqs 2 3 therefore a small time step approach is used i e several inner iterations are carried out within a time step to reduce the errors in the solution of the hydrodynamic equations in this way the mass conservation level is improved in the solution of eq 3 the staggered grids where the hydrodynamic and water quality variables are located at the cross sections and the centre of a control volume respectively see fig 1 are used to further reduce the mass conservation error after obtaining the solution of concentrations at junctions an explicit method is used to determine the e coli concentration value for each control volume for the case of a positive flow the mass transport equation eq 3 is discretized using the implicit upwind scheme as given in eq 4 4a a c t a c j k 1 a c j k  t 4b q c x q c j k 1 q c j 1 k 1  x j 1 4c x a e x c x 1  x j 1 a e x j 1 k 1 c j k 1 c j 1 k 1  x j 2  x j 1 2 a e x j k 1 c j 1 k 1 c j k 1  x j  x j 1 2 4d s c w c a k c j k 1 i n s w c k 1 where a j k b j k z j k average river area width and water elevation in the jth control volume respectively at the kth time step the linear equations for the krth sub channel can be written as 5 a j c j 1 k 1 b j c j k 1 c j c j 1 k 1 z z j j j s j s 1 j e where a j b j c j coefficients z z j explicit term c j k 1 mass concentration in control volume j at the k 1th time step js je the start and end cross section number for the krth sub channel respectively the other three flow directions types are discretized in a similar manner for the flow pattern shown in fig 1b the coefficients a j b j c j z z j in the inner cross sections j js js 1 je are derived in a similar manner as for the euquations 1 and 2 see appendix s2 eq 1 in order to predict the flow patterns in tidal river reaches with multiple stagnation points a refined algorithm has been developed based on existing work zhang et al 2014 in this model i a search was carried for each cross section to identify the existence of a stagnation in a sub channel and possible position using eq 6 and eq 7 ii the flow and stagnation type is identified based on theflow direction distributions see kr1 6 in fig 1c and solute concentrations within the inner junctions where multiple stagnation points existed using eq 8 and iii a set of linear equations and related parameters were reconstructed for a junction with multiple stagnations in the mass transport solution see eq 9 once the values of the concentration at the kth time step are known the concentration values at the k 1th time step for the krth sub channel can then be calculated using appendix s3 eq 1 together with the concentration values at the starting and ending junctions moreover because the lower e coli concentration boundary is required during a flood tide the simplified lower boundary is given using eq 10 and eq 11 based on the refinement steps above the main procedure used to solve the hydrodynamic and mass transport equations are shown in a flowchart in fig 2 identification of a stagnation point in a sub channel based on the flow directions at two adjacent cross sections of a control volume 6 q j 1 q j 0 a moving stagnation zone s position x s t g y s t g is identified by eq 7 7 x s t g x t j 1 q j 1 q j 1 q j x t j x t j 1 y s t g y t j 1 q j 1 q j 1 q j y t j y t j 1 where x t j y t j coordinates at the jth cross section of a sub channel if there are m stagnation points m 2 in a sub channel there will be m 1 reaches with each having a single flow direction or the e coli concentrations of each control volume can be predicted using the formulae for the and flow patterns if the concentration values for each stagnation zone are given the e coli concentration for a control volume with a stagnation point is solved for in two steps step 1 solving the c j k 1 at the control volume with the stagnation there are two types of stagnations one is the positive stagnation where water flows inwards and the other is the negative stagnation where water flows outwards an explicit upwind scheme is used to determine the e coli concentration value at the control volume with a stagnation point for example for a positive stagnation case then c j k 1 is determined using eq 8 8 c j k 1 a m j k a m j k 1 c j k  t a m j k 1 q j 1 k 1 c j k q j k 1 c j k  x j 1  t  x i 1 a m j k 1 a e x i 1 k 1 c i 1 k c i k  x i 2  x i 1 2 a e x i k 1 c i k c i 1 k  x i  x i 1 2  t a m j k 1 s c w c j k 1 step 2 re construction of linear systems with multiple stagnations here we use fig 1 c to illustrate the construction of the equations for the four flow directions and multiple moving stagnation zones assuming that the mass is well mixed at the junction and omitting the junction s storage variation the mass conservation equation for j7 see fig 1c can be written as 9 q k r 1 j e c k r 1 j e q k r 4 j e c k r 4 j e q k r 5 j e c k r 5 j e q k r 2 j e q k r 3 j e q k r 6 j e c j 7 0 the parameters of implicit algebraic equations eq 5 can be solved using appendix s2 eqs 1 and 2 during the construction of the concentration equation at a junction linked with boundaries the upper concentration boundary value is required for a positive flow or the low concentration boundary value is required only for a negative flow for a sub channel with multiple stagnation points only the concentration values at the first and last negative stagnation points are needed the number and position of the stagnation point s may change but the matrix structure remains the same after solving the concentration at the internal junctions using eq 9 then the concentration at the control volume centre for every sub channel can be solved using appendix s3 eq 1 during a flood tide a lower boundary condition is required and water elevation and e coli concentration values need to be specified however it is often difficult to obtain the measured e coli concentration data herein we follow the returned coefficient concept proposed by falconer 1984 which is expressed in eq 10 10 c t  c where c t e coli concentration input from the sea boundary at time t  e coli loss coefficient which ranges from 0 0 to 1 0 c mean e coli concentration across the sea boundary defined as 11 c t 1 t 2 q t c t d t t 1 t 2 q t d t in order to reduce the uncertainty level a series of the virtual volumes have been added to the seaward boundary to store the outflow and e coli during the ebb tides and a proportion of the integrated e coli efflux will return to the riverine networks through the sea boundary during the subsequent flood tides the e coli losses are mainly caused by two mechanisms one is related to the tidal and river flow characteristics and the other is by the natural decay of e coli in the virtual volumes 3 application 3 1 model setup for the ribble case uk the ribble river basin is located in the north west of england it originates from the rural hills of the yorkshire dales and the source of the river ribble to major urban areas of lancashire including blackburn burnley and preston with an area of 1583 km2 it has 4 key tributaries including the rivers hodder calder darwen and douglas as well as the crossens drainage system which flows into the ribble estuary see fig 3 it is the only uk research catchment for studies linked to the eu water framework directive wfd implementation kay et al 2005 and a significant amount of historical data on the river topography hydrology water quality and e coli measurements have been collected over the past 20 years the following data were used in the model i bathymetric data in the estuary and riverine regions which were converted into 1 d cross section data by interpolation huang et al 2014 ii 15 min sampled discharges and hourly e coli concentration data predicted at 5 main upper river boundaries namely the ribble no 710305 hodder no 711610 calder no 712615 darwen no 713122 and douglas no 700306 and 7 sub catchments as shown in fig 3 iii the inflow and e coli boundaries from 5 main upper rivers and 47 minor branches provided by the uk environment agency ea and sheffield university using the hydrological simulation program fortran hspf bicknell et al 1997 and infoworks wallingford software ltd 1995 models and iv half hour tidal elevation data at the lower boundary hourly meteorological data were acquired at 9 stations around the 1 d model domain and interpolated into the cross sections of the 1 d model these data included air and earth temperatures radiation levels relative humidity rainfall etc with the data being provided by the british atmospheric data centre uk http badc nerc ac uk home the main parameters used in the model are listed in table 1 3 2 mass conservation test in order to test the refined model two objective functions were used to check the mass conservation level i temporal difference between the net input flow volume and water storage across the whole model domain  v w o b j k 1 ii accumulated water conservation error s u m v w o b j k 1 they are expressed as 12a  v w o b j k 1 v w i o k 1 d v w k 1 12b s u m v w o b j k 1 s u m v w o b j k  v w o b j k 1 where v w i o k 1 is the net increase in the water volume from the inflow and outflow boundaries and point sources after each time step v w i o k 1 n b 1 n b d f n b q n b  t n p 1 n p s q n p  t f n b 1 and 1 for inflow and outflow boundaries respectively d v w k 1 v w k 1 v w k v w k and v w k 1 are the water volume in the model domain at k and k 1 time step respectively the model predicted results are presented in fig 4 the water storage values predicted by the refined method are very close to the net input water volume values see fig 4a the accumulated water mass conservation error is also small see fig 4b with the relative error being about 0 3 in 12 days where v e c i o k 1 n b 1 n b d f n b q n b c n b  t n p 1 n p s q n p c n p  t and d v e c k 1 v e c k 1 v e c k v e c k and v e c k 1 are the total e coli counts in the model domain at n and n 1 time steps respectively in which v e c k 1 j 1 n s c a j k 1 c j k 1  x j for this case the decay rate for the e coli solution was set to zero the predicted values of the objective variables are shown in fig 5 these results indicate that the level of agreement between v e c i o and d v e c is generally very close see fig 5a nevertheless there is also a relatively small accumulated mass loss see fig 5b with the average error being around 1 0 of the total e coli number over the simulation period 3 3 model verification in verifying the model the inflow discharges were measured at the 5 main upper boundaries to drive the model in addition discharges from the sub catchments were calculated using the hspf model where the calculated discharges were used to compensate for the shortage of measured discharge data in these catchments flow discharges and stage data at 3 main control stations no 700306 713056 and 713019 see fig 3 were used to verify the enhanced river network model it can be seen from fig 6 a and b that the model predicted and measured flow discharge values at the two control stations no 713056 and 713019 in the main channel generally agreed very well for the cases of low and medium flows with the statistical parameters rmse mae and nsce being presented in table 2 the model under predicted the maximum flood discharges at the two stations on 23rd june 2012 but predicted other peak flows quite well the errors are thought to be mainly caused by the local rainfall measurements and spatial interpolation errors based on the limited rainfall stations the comparison between the predicted water elevations and measured data at stations along the rivers douglas no 700306 and ribble no 713019 respectively agreed satisfactorily fig 6c and d and the statistical value of the nsce parameter at these two stations were 0 85 and 0 64 respectively other statistics are presented in table 2 the under estimated water elevation at flood peak on 26th september 2012 for the river douglas no 700306 is thought to be caused by the spatial interpolation error of the intense rainfall based on limited rain gauge stations and flow discharge under estimation by the hspf model further verification will be carried out when new and continuous measured water level data are available the e coli concentrations were measured at more than 10 stations in the ribble catchment by the centre for research into environment and health creh aberystwyth university as a part of the cloud to coast c2c project the results for 2 stations are presented herein i e fig 6e and f with comparisons being given between model predicted and measured e coli concentrations at two sites along the main channel of the river ribble it can be seen that the model predicted e coli concentrations generally agree well with the limited measured data but the model over predicted the concentration during the period from 8th to 28th august 2012 the error analysis results for the parameters rmse mae and nse are presented and listed in table 2 the rmse values are about 1 2 and 0 9 times of the average values of the measurements at these two stations while the mae and nsce values range from 9893 5 cfu 100 ml to 654 9 cfu 100 ml and 0 32 to 0 56 respectively since the e coli concentration includes certain uncertainty factors the predicted e coli concentration values are considered acceptable although there are some large errors during the high flow period the errors in predicting the peak values may be partly attributed to the sparse sampling rate relative to that predicted at each time step in the numerical and thereby potentially resulting in some high concentration points being missed in the measured data 3 4 evolution of stagnation zones in the river ribble networks based on the definition given by clancy 1975 a stagnation zone is a flow field where the local velocity of the fluid is zero stagnation exists extensively at the transition zone between the river and sea driven mainly by the river flows and tidal currents and occasionally driven by unsteady flows in the river networks in the current study the existence of stagnation zones was checked and their dynamic positions were simulated using eqs 12 and 13 and the 1 d hydrodynamic model has been extended to include a module on multiple stagnation zones the generation movement and extinction of stagnation zones are shown in fig 7 based on the model results the key processes in the stagnation zones can be summarised as follows i during low ebb phase there is a strong downward flow and thus stagnation does not exist ii during the flood phase the upward flow from the estuary meets the river flow then a positive stagnation is generated in the lower estuary fig 7a it then moves upwards with the tidal currents with more stagnation zones potentially being formed in river branches of the river network fig 7b iii during the flood to ebb phase a negative stagnation forms in the lower estuary fig 7c and it then moves towards the upper reaches with the previous positive stagnation zones moving downwards if a sub channel is long enough then the positive and negative stagnation zones can coexist fig 7d until the moving stagnation zones merge and then disappear finally the flow returns to a single downward direction and the processes of phase i to iii will be repeated the processes of generation movement and extinction of the multiple stagnations driven by the tidal and river flow interactions in the ribble river networks were predicted using the numerical model the results indicate that the multiple stagnation zones may appear in the river networks in the flood to ebb stage 3 5 source apportionment and its impacts in order to evaluate the rural and urban source apportionments and its impacts on the lower reach of the river ribble 16 one year scenarios see table 3 were simulated in which the inputs from the 7 sub catchments were combined see fig 3 to simplify the calculation procedure the simulation results are shown in table 4 it can be seen from table 4 that in the river ribble networks the total e coli loss is between 31 and 53 which varied with different source locations and dynamic weather and hydrodynamic conditions before the e coli flux arrives at the tidal limit station i e bullnose from the upper reaches approximately 8 of the e coli died off in the long narrow middle and upper reaches of the river ribble over 40 of the e coli then died off in the middle and lower reaches in total about 80 of the local e coli losses occur in the riverine and estuarine regions from bullnose to 11 mp for the following reasons i a large retention time caused by the tidal reciprocating flows in the wide and shallow channels ii a higher decay rate due to the increasing salinity levels in the shallow salt marshes mancini 1978 particularly when compared with the fresh water in the upper and middle reaches in general the rural and urban e coli sources have different characteristics the rural region with a large proportion of e coli sources from livestock is generally located in the upper and middle reaches of the river basin while the urbanised communities with an important portion of e coli from domestic sewage industrial waste water etc are located in the lower reaches of the river basin close to the receiving waters i e estuaries and coastal zones the urban e coli sources are controlled more frequently by man made devices such as waste water treatment plants wwtps and combined sewer overflows csos with the retention and transportation times for e coli general being increased and decreased respectively in these devices thus they may cause a non consistent phase difference between the flow discharge and the e coli fluxes the transport time and related loss rate for urban source usually varies considerably especially for extreme flow events the general transport time for rural e coli sources can therefore be shorter than the corresponding urban sources although the distance between the rural e coli source and the receiving waters is usually longer in the ribble river networks the overall e coli decay rate from the rural source is about 3 higher than that from urban sources meanwhile there is some exceptional variations in the darwen sub catchment where the urban e coli source is dominant due to the highly urbanised level and population density in the basin 4 discussion 4 1 validity and effectiveness of the proposed methods the accuracy of the solution of the 1 d mass transport equation depends partly on the representation of the hydrodynamic equations and partly on the accuracy of the discretization method of these equations moreover the errors from the hydrodynamic solutions may be transferred to the mass transport solutions and may even cause fluctuations in the solutions the 1 d st venant equations with z and q being main variables frequently used in the engineering community are not strictly conservative cunge et al 1980 because of the approximation a t b z t and the error from the approximation during linearization will increase when the width b varies significantly within a time step in a river particularly where a shallow and wide river has a narrow deep main channel in order to enhance a consistent solution between the hydrodynamic and mass transport equations limited inner iterations in eq 1 are carried out to reduce the error from the flow solution the preissmann scheme is based on a bi diagonal implicit finite difference method for solving the 1 d st venant equations and is unconditionally stable and robust however the mass and momentum equations i e eqs 1 and 2 are only equivalent to the discretized equation appendix s1 eqs 2 and 3 when the conditions  a a and  q q are satisfied for some special conditions e g near bank full discharge low tide or stagnant flows  a or  q may be of a similar order of magnitude or even larger than a and q then the assumed conditions cannot be satisfied and some large errors may occur in the hydrodynamic solutions in order to enhance the preissmann scheme the method of limited inner iterations with a smaller time step is used hu et al 2010 together with the mass conservation check in the hydrodynamic solutions also the transformation from the finite difference method fdm to fvm based on the staggered variables distribution improves the mass conservation level of the solution furthermore the solution for multiple stagnation zones makes the model predictions closer to the real physical process for tidal wave propagation and the interaction with the flow in river networks 4 2 e coli concentration difference at stations in order to evaluate the model results a comparison was made of the predictions fig 8 made using the three methods including i the finite difference method ii the finite volume method with a single stagnation zone fvm s and iii the finite volume method with multiple stagnation zones fvm m at 2 stations no 9 mp and no dgs995 in fig 3 during the calculation the main parameters such as decay rate and returning coefficient 0 1 were kept the same the main findings can be summarised as follows i in the upper and middle reaches the e coli concentration differences between the fdm and fvm algorithms is small because the flow direction is identical ii in the lower region because of the existence of a reversing current the mass loss is relatively large when the fdm algorithm is used and there is a relatively large e coli concentration difference between the fdm and fvm algorithms at the 9 mp station fig 8a and b and dgs 995 station fig 8c and d and iii there may be more than one stagnation zone in a sub channel driven by the tidal and river flows especially during the second half of the spring to ebb tidal period since the duration of multiple stagnation zones in the ribble river is relatively short the impact of multiple stagnation zones is minor on the e coli processes and the predicted concentration difference between a single and multiple stagnation zones is small in the main river and the estuarine region however in the river douglas the occurrence of multiple stagnation zones is more common because of the weak river flow the strong tidal currents and the long branched channel with a small bed slope therefore in this river the predicted difference in the e coli concentrations between the two fvm methods is much larger than that in the river ribble fig 8d the refinement of the solution method makes the predictions closer to the physical process and increases the model s generality when compared to the original method moreover the solution is more stable when the fvm algorithm is used 4 3 e coli loss rate by different methods it can be seen from table 4 that the fdm s algorithm can predict larger e coli losses due to its non conservation property and the fvm algorithm can enhance the mass conservation level by up to 10 with the same decay rate and returning coefficients at the lower boundary there are no obvious difference in the e coli predictions with single and multiple stagnations zones when the two fvm algorithms are compared as confirmed by the percentage losses shown in fig 8 and table 5 the decay rate in lower river reaches and the estuarine waters is larger than that in the upper riverine reaches and about 16 48 of the e coli will die off in the lower river reaches and the estuarine waters meanwhile because of heterogeneity in the bed sediments and vegetation in the region the transport processes and the fate of e coli in the lower river reaches and the estuary are complex and further study is needed in order to reduce the level of uncertainty 5 conclusions a refined one dimensional model has been developed for improving the mass conservation solution properties for solute mass fluxes particularly for e coli using consistent equations a staggered grid and transformation from a fdm to a fvm algorithm moreover an enhanced approach is proposed to solve the mass transport equation in a sub channel where there may be multiple stagnation zones which may be a general phenomenon in the lower reaches of a tidal river the test case for the ribble river basin and estuary shows that the mass conservation level reaches 99 0 after 12 days of simulation using the refined model and for an extremely complex flow and tidal dynamics scenario with the model generally predicting the discharge water elevations and e coli concentrations to a high degree of accuracy for the highly unsteady field measurements acquired in 2012 the refined and verified 1 d model has been applied to 16 one year scenarios for different e coli source apportionments based on the results obtained using the hspf and infoworks models the results from these model scenarios indicate the following i the degree of mass conservation in the numerical model solution is a prerequisite condition for the evaluation of the source transport and fate of e coli bacteria ii in the ribble catchment the e coli inputs are mainly from the darwen calder and douglas rivers and the middle and lower reaches of the river ribble with highly urbanised and high population density areas contributing a large proportion of these inputs the transport time and related loss rate for the urban sources usually varies considerably typically 16 48 of the e coli died off during the transport processes from the input sources to the river ribble outlet with these findings being attributed to the complex hydrodynamic and tidal conditions predicted in the modelling system iii the fate of e coli concentrations was found to be closely linked to the source positions and the solute and mass transport processes associated with the local hydrodynamic and salinity conditions acknowledgements the research reported herein was funded by the natural environment research council the medical research council the department for environment food and rural affairs and the economic and social research council gr ne i008306 1 uk the authors are also grateful to the environment agency north west for their provision of data and to all colleagues from the universities of aberystwyth and sheffield working on the nerc c2c project and in particular to professor david kay and dr carl stapleton of the centre for research into environment and health creh who provided the concentration data for source apportionment the detailed and rigorous comments made by the three anonymous reviewers are also much appreciated appendix a supplementary data the following are the supplementary data related to this article data profile data profile emands final manuscript emands final manuscript appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 07 009 
26347,the use of physically based spatially distributed models to solve problems in hydrology has been limited by their intensive data and setup time requirements we have therefore created a system that enables the automatic setup of a robust physically based spatially distributed shetran model for any catchment gauged or ungauged in great britain national scale datasets for topography soil landuse geology and climate have been collated processed and stored to allow rapid retrieval and configuration of catchment models with minimal user intervention these maps can be easily replaced by national datasets of other countries or global datasets ensuring the system s international transferability a graphical user interface has been developed to facilitate the model setup process the resultant system shetran gb has the potential to significantly aid the deployment of shetran for addressing important issues relating to water resources hydrological extremes and climate change either for individual or multiple catchments keywords shetran physically based hydrological model data gui 1 introduction robust numerical models are an essential tool for research in hydrological processes and for informing flood and water management around the world these models can be classified differently depending on whether they represent processes in conceptual or physically based ways e g o connell 1991 wheater et al 1993 singh 1995 refsgaard 1996 conceptual models use a series of connected stores or reservoirs to emulate catchment responses these models parameterise key hydrological processes and often require calibration of parameters with limited physical meaning pechlivanidis et al 2011 this forms a potential issue in applying conceptual models to particular types of problem such as prediction outside of calibration limits and under changing conditions beven 2012 as more complex physically based models are potentially useful for investigating a number of prominent hydrological problems such as predictions under non stationary land use or climate it is logical to take advantage of recent advances in computing power and data availability to facilitate their wider use this helps to overcome the common criticism that physically based models require prohibitively large data time and computational resource requirements beven 2012 as such this study has the purpose of creating for the first time a physically based hydrological modelling system for great britain using the shetran model abbott et al 1986 ewen et al 2000 and national datasets that are freely available for academic research shetran is a physically based spatially distributed hydrological model which has its origins in the systme hydrologique europen she model developed by the british institute of hydrology the danish hydraulic institute and the french company sogreah abbott et al 1986 ewen et al 2000 the foundations of she were strongly influenced by freeze and harlan 1969 who proposed a blueprint for a physically based hydrological model with these principles taken further and additional processes incorporated during the development of shetran the model is based on the solution of finite difference approximations to equations describing fully three dimensional coupled surface subsurface water flow and transport of sediments and reactive solutes as described in ewen et al 2000 since its inception shetran has been used in a wide variety of applications including assessment of the impacts of groundwater abstraction impacts on streamflows parkin et al 2007 examination of deforestation impacts on peak flows and sediment yields birkinshaw et al 2011 and nitrate transport koo and o connell 2006 to name but a few a graphical user interface gui was previously developed to improve the usability of shetran birkinshaw et al 2010 but it did not provide data or a standard parameter set to aid model setup as bierkens 2015 notes national continental and global scale simulations of hydrology have been carried out previously with land surface models lsms macroscale hydrological models mhms and increasingly with physically based hydrological models solving the partial differential equations representing coupled surface subsurface flows a recent example of the latter approach is high resolution 1 km modelling of the continental us with parflow maxwell et al 2015 this follows on from other national scale modelling efforts with process oriented models of differing origins and complexities for example henriksen et al 2003a describe the application of the physically based model mike she for integrated groundwater surface water simulations across denmark habets et al 2008 use the land surface scheme isba coupled with the meteorological analysis system safran and a groundwater model modcou to produce a national hydrological modelling system for france however there have not been any large scale applications of shetran to date or indeed any nationwide physically based modelling studies for gb rather the current standard national hydrological model for gb is the conceptual 1 km grid to grid model g2g bell et al 2009 which is in turn based on the probability distributed model pdm moore 2007 in order to facilitate their application at large scales a number of studies have developed ways of improving the setup and management of data intensive hydrological models some studies have articulated or developed data models using geodatabase structures to support many types of hydrological modelling for example maidment 2002 developed the widely used and flexible archydro system while goodall and maidment 2009 proposed an alternative spatiotemporal data model that is conceptualised around control volumes fluxes and flux couplers olivera et al 2006 drew on elements from archydro but added additional elements to integrate the arcgis geographical information system gis with the soil and water assessment tool swat model more recently bhatt et al 2014 developed an integrated user interface with an underpinning shared geodata model to couple the open source gis software quantum gis with the penn state integrated hydrological model pihm and demonstrated its power and effectiveness throughout the modelling chain other important advances that have taken place are in enabling rapid access to very large datasets and transforming data in line with the requirements of modelling leonard and duffy 2013 2014 developed the essential terrestrial variable etv web services and data model workflows to transform etv data ready for use in hydrological models anywhere in the continental us with particular reference to examples using pihm acknowledging the challenges in centralising and standardising data sources billah et al 2016 use the variable infiltration capacity vic model to demonstrate that data preparation pipelines based on a rule oriented data management system can be used to provide views on underlying data suitable for modelling yet while there are now some advanced solutions for efficiently collating and transforming the information needed for physically based hydrological models solutions are not currently available for many regions including great britain gb this paper attempts to help address this issue by providing an interface to set up the shetran model for any catchment in gb with national and international datasets prepared and transformed for physically based hydrological modelling shetran gb the integrated system achieves the automated application without calibration of a physically based rather than lumped calibrated hydrological modelling system using standard freely available data the system has been designed so that the datasets for gb could be easily substituted for national datasets of other countries or global datasets shetran has been shown to work well in a variety of climates including semi arid basins in iberia guerreiro et al 2017 tropical basins in west africa op de hipt et al 2017 monsoonal climates in india naseela et al 2015 and mountainous catchments influenced by snowmelt bathurst et al 2011 amongst others giving us confidence that an internatonal version of shetran gb would be a useful tool for physically based hydrological modellers the paper is structured as follows the input data required for the modelling system are discussed in section 2 followed by a description of the modifications made to shetran in section 3 section 4 outlines the software and user interface and section 5 describes in more detail the software design and implementation an example simulation from shetran gb is given in section 6 the work is discussed in section 7 and summarised in the conclusion 2 input data physically based hydrological models are very data intensive shetran requires a digital elevation model dem a map describing the subsurface properties of a catchment a land cover map a time series of rainfall a time series of potential evapotranspiration pet and a mask delineating the watershed of the catchment these inputs are typically gridded at a resolution between 50 m and 5km and time series data is supplied at a daily or hourly time step the datasets chosen for input to the modelling system see tables 1 and 2 each cover the whole of great britain which means that the information supplied for each catchment is consistent this has the advantage that interpreting variations in model performance will not be additionally complicated by the confounding effects of using multiple datasets for different catchments with the exception of the ukcp09 rainfall inputs which can be substituted by the ceh gear rainfall inputs see below the datasets are all also freely available to download under an academic licence making the models suitable for use by any research group it has been presumed that the quality control processes undertaken in the construction of each dataset make them sufficient for direct application in the modelling system without any further quality review or refinement the fully distributed nature of shetran means that model structure and properties need to be specified for each grid cell all maps required as part of this were resampled to a 100 m 500 m and 1 km resolution and aligned with the british national grid bng for consistency and ease of use maps at these three resolutions can be selected from the gui for small catchments 10 km2 it is expected that users will select a 100 or 500 m resolution and for medium large catchments use a 1 km resolution indeed the 1 km resolution maps form the standard shetran gb setup this standard 1 km resolution is partly dictated by the scales of the available soil and land cover datasets but it could be argued that higher spatial resolution models are desirable in order to more realistically capture the significant heterogeneity that can occur at sub kilometre scales in catchments paniconi and putti 2015 for example with respect to the representation of soils it is recognised that effective parameters are required at this comparatively coarse resolution to implicitly account for the many soil types and complex variations in structure and properties that could be present within a single grid square some authors such as beven 2006 argue that this compensation by effective parameterisation could undermine the physical basis of the model to some degree however the extent to which this issue represents a problem can be evaluated with respect to model performance if the dynamics of catchment models appear to be conceptually plausible and consistent with available evaluation data the selected spatial resolution can be effectively justified for the purpose of national scale modelling of water resources climate change impacts and land use change impacts the 1 km spatial resolution therefore represents a balance between the information content of available data and pragmatic considerations with respect to computing resources and run times in particular for example a 1 km resolution shetran model for a 500 km2 catchment over 10 years takes approximately 35 min to run on a desktop computer as well as perceived priorities for investigation and evaluation this was also guided by the recent work of zhang 2012 who investigated the influence of spatial and temporal resolution in shetran zhang 2012 found that simulations can be significantly improved by increasing the temporal resolution of forcing inputs particularly rainfall from daily to hourly intervals in contrast improvements due to increasing spatial resolution were found to be more varied and to depend upon the level of catchment homogeneity this highlights the general importance of improving the temporal resolution of inputs relative to the spatial resolution although this will vary depending on specific model use furthermore the 1 km resolution is consistent with other large scale national modelling studies such as the uk grid to grid model bell et al 2007 and the national mike she model for denmark henriksen et al 2003b which also use a 1 km grid resolution recent versions of the national mike she model for denmark have moved to a 500 m grid resolution hjberg et al 2013 olsen et al 2013 the digital elevation model dem used in this study was based on the 50 m ordnance survey os land form panorama data ordnance survey 2013a this raster was then resampled to a 1 km resolution using bilinear resampling to determine the average elevation for each grid square one of the applications of the dem in shetran is in generation of the location of river channels for routing within the model these are referred to in shetran as river links tests suggested that these links more closely follow actual river paths when the minimum elevations in each grid square taken from the original resolution of the panorama dem are accounted for a dem based on the minimum elevation values in each 1 km grid square was therefore created with which the river locations were then calculated shetran represents the catchment sub surface for each grid cell as a column containing multiple layers of soil or rock the european soil database esdb v2 0 liedekerke et al 2006 was identified as the most suitable dataset for meeting the requirements of this modelling system it is a europe wide 1 km resolution database in which hydraulic properties were assigned by a collaboration of 12 european countries liedekerke et al 2006 standardized both the particle size and the hydraulic data across europe by fitting the mualem van genuchten model parameters van genuchten 1980 to the individual hydraulic properties stored in the esdb the esdb layers containing the information that shetran uses dominant topsoil texture depth to textural change dominant subsoil texture and depth to rock were selected and combined into one raster file of unique soil classes for shetran bedrock depths were not available as a national dataset therefore as an initial representation of geology a 20 m thick bedrock layer was added to the bottom of each soil column the data for this were taken from the british geological survey bgs 1 625 000 scale digital hydrogeological map british geological survey 2014 236 unique subsurface column types were identified on this basis and coded for use in shetran the centre for ecology and hydrology ceh land cover map lcm 2007 morton et al 2011a is derived from satellite images and digital cartography using land cover classifications based on the uk biodiversity action plan broad habitats that lead to the definition of 23 land cover types the map was simplified into the 7 basic land cover types typically used with shetran arable bare ground grass deciduous forest evergreen forest shrub and urban birkinshaw 2011 most previous work with shetran has used point rainfall data bathurst et al 2011 elliott et al 2012 birkinshaw et al 2014 this approach is adequate for modelling individual catchments under certain conditions but a more coherent dataset is necessary for setting up a nationwide system so that a collective approach to catchment modelling may be followed minimising errors from non standardisation or bias shetran was therefore updated to take gridded rainfall as input and the ukcp09 5 km gridded precipitation dataset was initially selected as the fundamental rainfall input for the modelling system perry et al 2009 this dataset created by the met office is based on a large amount of data from the uk s comparatively dense gauge network and provides full coverage of the uk at a daily resolution for 1958 2007 in addition ceh have recently developed a new 1 km gridded daily rainfall product ceh gear gridded estimates of daily and monthly areal rainfall for the united kingdom 1890 2012 tanguy et al 2014 unlike the ukcp09 5 km gridded dataset which uses inverse distance weighting to interpolate station data into a gridded format ceh gear uses natural neighbour interpolation the gridded daily values are also adjusted by a monthly correction factor to ensure that they are consistent with gridded monthly rainfall totals calculated from a denser station network unlike the ukcp09 dataset the ceh gear rainfall is freely available either dataset can be used for model setup and can be selected in the python code rather than through the gui the met office rainfall and evaporation calculation system morecs is often used as an evaporation data source for hydrological modelling in the uk hough and jones 1997 for example by the environment agency regional groundwater models shepley et al 2012 it provides nationwide real time assessments of rainfall potential evapotranspiration pet and soil moisture hough and jones 1997 but the data are not freely available other distributed estimates of pet are very limited such that the approach taken in this work was to calculate pet directly from the gridded variables available within the ukcp09 dataset using the fao penman monteith method allen et al 1998 to produce a uk wide 5 km 5 km grid of time varying pet this method also allows the shetran gb system to be more directly compatible with ukcp09 weather generator outputs which calculates pet in the same way kilsby et al 2007 daily maximum and minimum temperature data are used to provide input to the snowmelt module of shetran there are options for both temperature index and energy balance approaches to modelling snowpacks within shetran but the former simpler method is used here as it has lower input data requirements given the climatology of the uk this modelling decision is likely to have most bearing on upland and mountainous regions for which estimation of all the inputs required for energy balance is particularly complicated due to topographic complexity shetran was modified to allow for input of a map showing the position of lakes in a catchment in order to improve their representation in catchment simulations the data layer used as input is the os meridian 2 lakes layer ordnance survey 2013b this dataset is available as a vector file which was converted to a 1 km raster 3 existing functionality of shetran and new modifications currently the subsurface in shetran is treated as a variably saturated heterogeneous porous medium and fully 3d flow and transport can be simulated for combinations of confined unconfined and perched systems the unsaturated zone is modelled as an integral part of the subsurface overland flow is produced as a result of both infiltration excess and saturation excess and is simulated using the diffusion approximation of the saint venant equations a network of 1d channels flows around the edge of the grid squares and the flow in these is also modelled using the diffusion approximation of the saint venant equations the surface and subsurface are fully coupled there is no explicit modelling of pipes as these are not provided in any national dataset lakes are not explicitly modelled but occur as a result of the physical characteristics of the catchment the catchments are assumed to have no flow boundary conditions apart from the outlet where there is assumed to be a weir the catchments are set to be saturated initially but the results from the first two years of simulation a 2 year spin up period are not used in the analysis shetran has been modified in several ways in order to improve its performance using national datasets the modifications include the following shetran now accommodates spatially varying rainfall and potential evapotranspiration data as opposed to only point rainfall data as used in most prior work this allows for use of recently developed gridded products and provides more realistic representation of important variability within catchments each model grid square is assigned a code in an ascii map that corresponds to a time series in a separate input file there is now also a better process within shetran for removing sinks in the dem i e grid squares at a lower elevation than all neighbouring grid squares to prevent unrealistic levels of ponding and surface storage which would act to reduce flows in an unrealistic way this is achieved in an iterative process the elevation of all the grid squares where there is a sink is gradually increased until it is no longer at a lower elevation than its neighbours this is then repeated until there are no sinks throughout the catchment a minimum dem describing the minimum elevation in a grid square is now used in combination with an average dem describing the average elevation in a grid square to more accurately route the river links calculated within shetran the position of the river links is calculated from the minimum dem by analysing the number of upstream grid squares that flow into a particular grid square when these reach a certain number the default is 20 a river channel is produced birkinshaw 2010 the channel elevations are based on the 2 adjacent grid squares the elevations are then modified so that there is always a downward flow path the average dem is used for all other processes shetran has been modified to accept a map of lake locations so that they can be accurately represented within catchments if a lake grid cell intersects a river link it is treated as an open water body by reducing the default strickler coefficient controlling surface roughness from 20 to 3 the strickler coefficient is the inverse of mannings roughness coefficient gauckler 1867 manning 1891 this acts to effectively slow flow and increase storage of water in the channel all river channels have an individual stickler parameter associated with them shetran now adjusts this parameter so that it corresponds with the lake value as a results of these physical characteristics and the low strickler coefficient in the channel when there is heavy rain and large river inflows the water level in these channels increase and overflow into the nearby grid squares occurs this surface water which corresponds to lake water storage gradually builds up and then reduces once the rain stops changes have also been made so that it is possible to assign strickler coefficients as a function of land cover rather than applying one parameter value for the whole catchment as has generally been the case in the past the option was always available within shetran although not normally used this allows roughness to vary with vegetation as would be expected e g concrete surfaces have a low roughness and thus high strickler coefficient whereas vegetated surfaces are rougher giving them a lower strickler coefficient 4 automatic set up of shetran in order to develop a national modelling system based on shetran a large array of data for the whole of great britain and the period 1960 2006 as described above was integrated into a framework that features a new user friendly graphical interface which extracts and prepares the data required for a shetran simulation of any catchment in great britain previously models would be set up manually this involved finding and downloading the appropriate datasets importing them into gis to fill sinks delineate catchment boundaries and extract the relevant area from each data layer then convert these to shetran input files and assign the parameters individually this new gui has vastly reduced the time it takes to set up and run a model from months birkinshaw 2010 to seconds the resultant model is an uncalibrated model based on the fixed set of parameters from national datasets the system represents substantial progress in the ability to deploy shetran for individual or very large numbers of catchments the python scripts underlying the system take input from a user interface scripted in html and javascript to automatically set up a model for a catchment see fig 1 users define the catchment resolution and start and end date of the simulation for gauged catchments an existing catchment boundary can be selected from a map while for ungauged catchments a shapefile or ascii map of the catchment boundary can be uploaded to extract an appropriate model the gauged catchment boundaries used in this work were determined on the basis of the records for all of the gauged catchments in the uk held by the national river flow archive nrfa national river flow archive 2014b the algorithm underpinning catchment setup takes a boundary shapefile or gauge number as input to delineate the catchment along with the start and end date of the simulation as input it then creates a project directory creates or selects the catchment mask and uses that to extract the other data and write it in the appropriate format for input to shetran the shetran data pre processor is run generating the input files for shetran the automated setup of shetran gb catchments is based on a single standard conceptual model derived from the available datasets the sub surface can have 2 3 layers defined by the soil and geology datasets these are further broken down by shetran into a total of 35 computational layers other key universal parameters are set to fixed values this standard conceptual model cannot be changed in an automated way through the gui but could be changed easily by the underlying model set up code individual models can also be altered by hand after set up with the gui 5 software design and implementation the software is designed for those wishing to use a physically based hydrological model but do not have access to detailed datasets for their catchment of interest new users of shetran often wish to begin modelling with a default set of parameters and subsequently refine the model when they become comfortable with the program the gui makes it much easier for someone to start using shetran from scratch but it is of little benefit to those with a full set of detailed data about their catchment as this will surpass the information provided by national datasets experienced users of shetran will also benefit from the software as they will be able to easily set up a catchment quickly the software requires a regular desktop computer with windows and python 2 7 with osgeo image imagedraw and tornado modules and their dependencies shetran gb is 245 1 gb as a bundle of python scripts exe files and an ascii data archive no preprocessing is required before use the gui only takes a few seconds to produce an average catchment model it takes 170 s to extract the full time series of data for the largest gauged catchment in great britain 9948 km2 the software has been designed so that the underlying code can be run with or without the gui the gui is a clean and simple tool for setting up one catchment model at a time but the underpinning python code can be run in parallel to set up many catchments at once the python model setup code interacts with the javascript interface through the tornado module receiving information about the required start and end dates of the simulation and the catchment boundary if the catchment exists in the boundary catalogue the existing catchment ascii files are retrieved if the catchment does not exist i e a shapefile has been uploaded instead an additional module is called that converts the uploaded shapefile into an ascii mask using the osgeo module once the ascii mask has been generated it is used to extract the other spatial information required from national maps the national maps are not read into memory rather the lines of the files are skipped over until the required point is reached the relevant lines are read into memory and then the file is closed this is to make the system adaptable to much higher resolution national maps when they become available all maps are stored using the british national grid coordinate system which provides the regular array required for shetran the values in the extracted maps are used to retrieve the corresponding time series data parameters are preassigned based on extensive experience from previous studies although parameter values can be adjusted later the automated setup of shetran models means that common errors can be avoided the shetran input files are very detailed and it is easy to make a typing error or assign the wrong value to a parameter the automated set up ensures that every map is the same size resolution and location and that each parameter is assigned correctly this adds a level of credibility and reduces the probability of errors in the model especially for large heterogeneous catchments 6 example simulation the eden catchment was simulated to demonstrate the suitability of the datasets for use in a national physically based modelling system the eden is a commonly studied area in the uk and so we are able to compare an uncalibrated version of shetran set up by the system described in this paper to a calibrated version set up for another study janes et al 2018 calibrate shetran to river flows for the eden at kirkby stephen 70 km2 in area and at temple sowerby 616 km2 over the period 1991 2001 the two catchments achieve nash sutcliffe efficiencies nse nash and sutcliffe 1970 of 0 85 and 0 86 respectively r2 moriasi et al 2007 of 0 88 and 0 7 and pbias moriasi et al 2007 of 14 and 8 for the validation period 2001 2007 uncalibrated shetran gb simulations produce nse values of 0 78 and 0 80 see fig 2 r2 of 0 79 and 0 81 and percentage bias in the water balance pbias of 8 6 and 1 0 for the same catchments for the period 1992 2002 this shows that the data used for the automated setup of shetran gb is appropriate but that model performance can be improved with calibration to provide an indication of the robustness of the system some key potential uncertainties for the eden catchment are now explored fig 3 compares the river channel network automatically derived in shetran from the digital elevation model at 1 km and 500 m resolution to the existing river network map ordnance survey 2018 the derived river network at both resolutions corresponds well to the main river channel through the catchment at 500 m resolution the secondary river channels are also well represented with only a few errors this illustrates the importance of resolution rather than the underlying data in the uncertainty associated with the dem rainfall data has many uncertainties associated with it including mechanical errors recording errors evaporation from partly filled buckets wind induced under catch and snow effects that can reduce rainfall totals by over 40 mcmillan et al 2012 pollock et al 2018 uncertainty is also introduced by gridding gauge data due to the interpolation method used the ukcp09 and ceh gear daily rainfall datasets use different interpolation methods but are still highly correlated indeed when the 24 h accumulation for each ceh gear 1 km time series is compared with its corresponding 5 km grid square the spearman s rank and pearson correlation coefficients range from 0 9 to 1 pet is another very uncertain variable as it can be calculated in approximately 50 ways lu et al 2007 and cannot be directly measured lu et al 2007 test six of these methods including penman monteith and find that pet can vary by up to 500 mm year in their study area of the southeastern united states depending on the pet method used other uncertainties in the land cover soil and geological data are very difficult to quantify without collecting field data from the catchment itself but there are undoubtedly large uncertainties associated with the significant heterogeneity that can occur at sub kilometre scales in catchments more extensive analysis of the system s performance for a large number of catchments including the uncertainties associated with the input datasets will follow in subsequent work such analyses are facilitated by the flexibility of shetran gb which enables substitution of alternative datasets to assess their impacts on simulations 7 discussion this paper outlines the automated set up of a physically based model for catchments in gb the aim of this was to increase the usability of shetran and to provide a uniform framework for exploring the possibility of a national uncalibrated model based on parameters from established datasets so that ungauged catchments can be effectively modelled with some certainty additional experiments will test this theory whereas this paper focuses on describing the modelling framework itself there are many positives to using such a framework the time taken to set up a model has been vastly reduced allowing the modeller to focus on other aspects of the modelling process calibration uncertainty analysis exploration of a wider range of model outputs the data used are currently the best available and they can easily be replaced by new or improved data sets when they become available the models are based on a standard set of parameters which provides consistency between models the automated process reduces errors and uncertainties by removing the possibility of errors when manually setting up the complex model input files however such a strict framework does restrict the modeller from developing their own conceptual model of the catchment and does nothing to help incorporate other more detailed local datasets there is also the danger that these models will be used without due consideration of catchment processes whilst increased automation of both model setup and automatic calibration can improve efficiency and reliability in hydrological modelling it is important not to overlook the role of detailed local knowledge of an area and the role of a modeller s expertise the gui has been designed to be simple and easy to use whilst the underlying code has been designed to be flexible there is much scope to use the code for other purposes for example building different conceptual models e g altering the boundary conditions the universal parameters etc generating input files for models other than shetran or even applying the framework to other countries many countries already have national datasets which could easily replace those presented here indeed as a starting point for any country a series of global datasets could be used including the shuttle radar topography mission srtm dem jarvis et al 2008 the fao harmonised world soil database nachtergaele and batjes 2012 the nasausgs global land survey gls datasets gutman et al 2013 and a global 50 yr 3 hourly 1 0 dataset of meteorological forcings sheffield et al 2006 the global maps would need to be projected onto an equal area grid and have shetran compatible properties assigned to them as described here before being incorporated into the system as discussed in the introduction shetran has been used to model catchments across the world and it is likely that the main limiting factor for international modelling would be the quality of the input datasets used not the physical processes being represented in shetran itself the most difficult data to obtain would be a parameterized hydrogeological dataset and this would be a major limitation when trying to model groundwater dominated catchments 8 conclusion this paper has detailed the datasets processing and software development involved in setting up a national shetran modelling system for great britain freely available data were collated and processed into 1 km rasters aligned with the british national grid the datasets together describe the country s physical hydrological and climatic characteristics modifications were made to shetran itself to permit more realistic input data such as by allowing distributed precipitation inputs as well as to enhance representation of some hydrological processes for example channel delineation was improved by using a minimum dem with modifications also made to handle dem sinks and improve the treatment of lakes in shetran a graphical user interface was developed so that modellers can rapidly set up a shetran catchment model the system could be straightforwardly transferred to other regions of the world using global or national datasets forthcoming publications will examine the robustness of the system assess the performance of simulations and outline ways in which it could be used for impact case studies whilst the datasets have been set up and used in shetran it would be very simple to use exactly the same data in any other model code the xml file precipitation data and ascii grids produced could be easily adapted to be used by any other distributed hydrological model 9 software availability shetran gb was developed by elizabeth lewis and stephen birkinshaw the software is available from elizabeth lewis elizabeth lewis2 ncl ac uk cassie building newcastle university newcastle upon tyne ne1 7ru uk software first available from january 2016 for free under academic licence software requires a regular desktop computer with windows and python 2 7 with osgeo image imagedraw and tornado modules shetran gb is 245 1 gb as a bundle of python scripts exe files and an ascii data archive technical documentation is embedded directly in the code through comments user documentation is provided separately which provides a worked example of how to use the software acknowledgements funding this work was supported by the natural environment research council ne j500239 1 hayley j fowler is funded by the wolfson foundation and the royal society as a royal society wolfson research merit award wm140025 holder contains data from the uk national river flow archive hosted by the centre for ecology hydrology and operated in partnership with uk hydrometric measuring authorities crown copyright 2009 the uk climate projections data have been made available by the department of the environment food and rural affairs defra and department of energy and climate change decc under licence from the met office newcastle university university of east anglia and proudman oceanographic laboratory these organisations accept no responsibility for any inaccuracies or omissions in the data nor for any loss or damage directly or indirectly caused to any person or body by reason of or arising out of any use of this data lcm 2007 and database right nerc ceh 2011 all rights reserved contains ordnance survey data crown copyright and database right 2007 
26347,the use of physically based spatially distributed models to solve problems in hydrology has been limited by their intensive data and setup time requirements we have therefore created a system that enables the automatic setup of a robust physically based spatially distributed shetran model for any catchment gauged or ungauged in great britain national scale datasets for topography soil landuse geology and climate have been collated processed and stored to allow rapid retrieval and configuration of catchment models with minimal user intervention these maps can be easily replaced by national datasets of other countries or global datasets ensuring the system s international transferability a graphical user interface has been developed to facilitate the model setup process the resultant system shetran gb has the potential to significantly aid the deployment of shetran for addressing important issues relating to water resources hydrological extremes and climate change either for individual or multiple catchments keywords shetran physically based hydrological model data gui 1 introduction robust numerical models are an essential tool for research in hydrological processes and for informing flood and water management around the world these models can be classified differently depending on whether they represent processes in conceptual or physically based ways e g o connell 1991 wheater et al 1993 singh 1995 refsgaard 1996 conceptual models use a series of connected stores or reservoirs to emulate catchment responses these models parameterise key hydrological processes and often require calibration of parameters with limited physical meaning pechlivanidis et al 2011 this forms a potential issue in applying conceptual models to particular types of problem such as prediction outside of calibration limits and under changing conditions beven 2012 as more complex physically based models are potentially useful for investigating a number of prominent hydrological problems such as predictions under non stationary land use or climate it is logical to take advantage of recent advances in computing power and data availability to facilitate their wider use this helps to overcome the common criticism that physically based models require prohibitively large data time and computational resource requirements beven 2012 as such this study has the purpose of creating for the first time a physically based hydrological modelling system for great britain using the shetran model abbott et al 1986 ewen et al 2000 and national datasets that are freely available for academic research shetran is a physically based spatially distributed hydrological model which has its origins in the systme hydrologique europen she model developed by the british institute of hydrology the danish hydraulic institute and the french company sogreah abbott et al 1986 ewen et al 2000 the foundations of she were strongly influenced by freeze and harlan 1969 who proposed a blueprint for a physically based hydrological model with these principles taken further and additional processes incorporated during the development of shetran the model is based on the solution of finite difference approximations to equations describing fully three dimensional coupled surface subsurface water flow and transport of sediments and reactive solutes as described in ewen et al 2000 since its inception shetran has been used in a wide variety of applications including assessment of the impacts of groundwater abstraction impacts on streamflows parkin et al 2007 examination of deforestation impacts on peak flows and sediment yields birkinshaw et al 2011 and nitrate transport koo and o connell 2006 to name but a few a graphical user interface gui was previously developed to improve the usability of shetran birkinshaw et al 2010 but it did not provide data or a standard parameter set to aid model setup as bierkens 2015 notes national continental and global scale simulations of hydrology have been carried out previously with land surface models lsms macroscale hydrological models mhms and increasingly with physically based hydrological models solving the partial differential equations representing coupled surface subsurface flows a recent example of the latter approach is high resolution 1 km modelling of the continental us with parflow maxwell et al 2015 this follows on from other national scale modelling efforts with process oriented models of differing origins and complexities for example henriksen et al 2003a describe the application of the physically based model mike she for integrated groundwater surface water simulations across denmark habets et al 2008 use the land surface scheme isba coupled with the meteorological analysis system safran and a groundwater model modcou to produce a national hydrological modelling system for france however there have not been any large scale applications of shetran to date or indeed any nationwide physically based modelling studies for gb rather the current standard national hydrological model for gb is the conceptual 1 km grid to grid model g2g bell et al 2009 which is in turn based on the probability distributed model pdm moore 2007 in order to facilitate their application at large scales a number of studies have developed ways of improving the setup and management of data intensive hydrological models some studies have articulated or developed data models using geodatabase structures to support many types of hydrological modelling for example maidment 2002 developed the widely used and flexible archydro system while goodall and maidment 2009 proposed an alternative spatiotemporal data model that is conceptualised around control volumes fluxes and flux couplers olivera et al 2006 drew on elements from archydro but added additional elements to integrate the arcgis geographical information system gis with the soil and water assessment tool swat model more recently bhatt et al 2014 developed an integrated user interface with an underpinning shared geodata model to couple the open source gis software quantum gis with the penn state integrated hydrological model pihm and demonstrated its power and effectiveness throughout the modelling chain other important advances that have taken place are in enabling rapid access to very large datasets and transforming data in line with the requirements of modelling leonard and duffy 2013 2014 developed the essential terrestrial variable etv web services and data model workflows to transform etv data ready for use in hydrological models anywhere in the continental us with particular reference to examples using pihm acknowledging the challenges in centralising and standardising data sources billah et al 2016 use the variable infiltration capacity vic model to demonstrate that data preparation pipelines based on a rule oriented data management system can be used to provide views on underlying data suitable for modelling yet while there are now some advanced solutions for efficiently collating and transforming the information needed for physically based hydrological models solutions are not currently available for many regions including great britain gb this paper attempts to help address this issue by providing an interface to set up the shetran model for any catchment in gb with national and international datasets prepared and transformed for physically based hydrological modelling shetran gb the integrated system achieves the automated application without calibration of a physically based rather than lumped calibrated hydrological modelling system using standard freely available data the system has been designed so that the datasets for gb could be easily substituted for national datasets of other countries or global datasets shetran has been shown to work well in a variety of climates including semi arid basins in iberia guerreiro et al 2017 tropical basins in west africa op de hipt et al 2017 monsoonal climates in india naseela et al 2015 and mountainous catchments influenced by snowmelt bathurst et al 2011 amongst others giving us confidence that an internatonal version of shetran gb would be a useful tool for physically based hydrological modellers the paper is structured as follows the input data required for the modelling system are discussed in section 2 followed by a description of the modifications made to shetran in section 3 section 4 outlines the software and user interface and section 5 describes in more detail the software design and implementation an example simulation from shetran gb is given in section 6 the work is discussed in section 7 and summarised in the conclusion 2 input data physically based hydrological models are very data intensive shetran requires a digital elevation model dem a map describing the subsurface properties of a catchment a land cover map a time series of rainfall a time series of potential evapotranspiration pet and a mask delineating the watershed of the catchment these inputs are typically gridded at a resolution between 50 m and 5km and time series data is supplied at a daily or hourly time step the datasets chosen for input to the modelling system see tables 1 and 2 each cover the whole of great britain which means that the information supplied for each catchment is consistent this has the advantage that interpreting variations in model performance will not be additionally complicated by the confounding effects of using multiple datasets for different catchments with the exception of the ukcp09 rainfall inputs which can be substituted by the ceh gear rainfall inputs see below the datasets are all also freely available to download under an academic licence making the models suitable for use by any research group it has been presumed that the quality control processes undertaken in the construction of each dataset make them sufficient for direct application in the modelling system without any further quality review or refinement the fully distributed nature of shetran means that model structure and properties need to be specified for each grid cell all maps required as part of this were resampled to a 100 m 500 m and 1 km resolution and aligned with the british national grid bng for consistency and ease of use maps at these three resolutions can be selected from the gui for small catchments 10 km2 it is expected that users will select a 100 or 500 m resolution and for medium large catchments use a 1 km resolution indeed the 1 km resolution maps form the standard shetran gb setup this standard 1 km resolution is partly dictated by the scales of the available soil and land cover datasets but it could be argued that higher spatial resolution models are desirable in order to more realistically capture the significant heterogeneity that can occur at sub kilometre scales in catchments paniconi and putti 2015 for example with respect to the representation of soils it is recognised that effective parameters are required at this comparatively coarse resolution to implicitly account for the many soil types and complex variations in structure and properties that could be present within a single grid square some authors such as beven 2006 argue that this compensation by effective parameterisation could undermine the physical basis of the model to some degree however the extent to which this issue represents a problem can be evaluated with respect to model performance if the dynamics of catchment models appear to be conceptually plausible and consistent with available evaluation data the selected spatial resolution can be effectively justified for the purpose of national scale modelling of water resources climate change impacts and land use change impacts the 1 km spatial resolution therefore represents a balance between the information content of available data and pragmatic considerations with respect to computing resources and run times in particular for example a 1 km resolution shetran model for a 500 km2 catchment over 10 years takes approximately 35 min to run on a desktop computer as well as perceived priorities for investigation and evaluation this was also guided by the recent work of zhang 2012 who investigated the influence of spatial and temporal resolution in shetran zhang 2012 found that simulations can be significantly improved by increasing the temporal resolution of forcing inputs particularly rainfall from daily to hourly intervals in contrast improvements due to increasing spatial resolution were found to be more varied and to depend upon the level of catchment homogeneity this highlights the general importance of improving the temporal resolution of inputs relative to the spatial resolution although this will vary depending on specific model use furthermore the 1 km resolution is consistent with other large scale national modelling studies such as the uk grid to grid model bell et al 2007 and the national mike she model for denmark henriksen et al 2003b which also use a 1 km grid resolution recent versions of the national mike she model for denmark have moved to a 500 m grid resolution hjberg et al 2013 olsen et al 2013 the digital elevation model dem used in this study was based on the 50 m ordnance survey os land form panorama data ordnance survey 2013a this raster was then resampled to a 1 km resolution using bilinear resampling to determine the average elevation for each grid square one of the applications of the dem in shetran is in generation of the location of river channels for routing within the model these are referred to in shetran as river links tests suggested that these links more closely follow actual river paths when the minimum elevations in each grid square taken from the original resolution of the panorama dem are accounted for a dem based on the minimum elevation values in each 1 km grid square was therefore created with which the river locations were then calculated shetran represents the catchment sub surface for each grid cell as a column containing multiple layers of soil or rock the european soil database esdb v2 0 liedekerke et al 2006 was identified as the most suitable dataset for meeting the requirements of this modelling system it is a europe wide 1 km resolution database in which hydraulic properties were assigned by a collaboration of 12 european countries liedekerke et al 2006 standardized both the particle size and the hydraulic data across europe by fitting the mualem van genuchten model parameters van genuchten 1980 to the individual hydraulic properties stored in the esdb the esdb layers containing the information that shetran uses dominant topsoil texture depth to textural change dominant subsoil texture and depth to rock were selected and combined into one raster file of unique soil classes for shetran bedrock depths were not available as a national dataset therefore as an initial representation of geology a 20 m thick bedrock layer was added to the bottom of each soil column the data for this were taken from the british geological survey bgs 1 625 000 scale digital hydrogeological map british geological survey 2014 236 unique subsurface column types were identified on this basis and coded for use in shetran the centre for ecology and hydrology ceh land cover map lcm 2007 morton et al 2011a is derived from satellite images and digital cartography using land cover classifications based on the uk biodiversity action plan broad habitats that lead to the definition of 23 land cover types the map was simplified into the 7 basic land cover types typically used with shetran arable bare ground grass deciduous forest evergreen forest shrub and urban birkinshaw 2011 most previous work with shetran has used point rainfall data bathurst et al 2011 elliott et al 2012 birkinshaw et al 2014 this approach is adequate for modelling individual catchments under certain conditions but a more coherent dataset is necessary for setting up a nationwide system so that a collective approach to catchment modelling may be followed minimising errors from non standardisation or bias shetran was therefore updated to take gridded rainfall as input and the ukcp09 5 km gridded precipitation dataset was initially selected as the fundamental rainfall input for the modelling system perry et al 2009 this dataset created by the met office is based on a large amount of data from the uk s comparatively dense gauge network and provides full coverage of the uk at a daily resolution for 1958 2007 in addition ceh have recently developed a new 1 km gridded daily rainfall product ceh gear gridded estimates of daily and monthly areal rainfall for the united kingdom 1890 2012 tanguy et al 2014 unlike the ukcp09 5 km gridded dataset which uses inverse distance weighting to interpolate station data into a gridded format ceh gear uses natural neighbour interpolation the gridded daily values are also adjusted by a monthly correction factor to ensure that they are consistent with gridded monthly rainfall totals calculated from a denser station network unlike the ukcp09 dataset the ceh gear rainfall is freely available either dataset can be used for model setup and can be selected in the python code rather than through the gui the met office rainfall and evaporation calculation system morecs is often used as an evaporation data source for hydrological modelling in the uk hough and jones 1997 for example by the environment agency regional groundwater models shepley et al 2012 it provides nationwide real time assessments of rainfall potential evapotranspiration pet and soil moisture hough and jones 1997 but the data are not freely available other distributed estimates of pet are very limited such that the approach taken in this work was to calculate pet directly from the gridded variables available within the ukcp09 dataset using the fao penman monteith method allen et al 1998 to produce a uk wide 5 km 5 km grid of time varying pet this method also allows the shetran gb system to be more directly compatible with ukcp09 weather generator outputs which calculates pet in the same way kilsby et al 2007 daily maximum and minimum temperature data are used to provide input to the snowmelt module of shetran there are options for both temperature index and energy balance approaches to modelling snowpacks within shetran but the former simpler method is used here as it has lower input data requirements given the climatology of the uk this modelling decision is likely to have most bearing on upland and mountainous regions for which estimation of all the inputs required for energy balance is particularly complicated due to topographic complexity shetran was modified to allow for input of a map showing the position of lakes in a catchment in order to improve their representation in catchment simulations the data layer used as input is the os meridian 2 lakes layer ordnance survey 2013b this dataset is available as a vector file which was converted to a 1 km raster 3 existing functionality of shetran and new modifications currently the subsurface in shetran is treated as a variably saturated heterogeneous porous medium and fully 3d flow and transport can be simulated for combinations of confined unconfined and perched systems the unsaturated zone is modelled as an integral part of the subsurface overland flow is produced as a result of both infiltration excess and saturation excess and is simulated using the diffusion approximation of the saint venant equations a network of 1d channels flows around the edge of the grid squares and the flow in these is also modelled using the diffusion approximation of the saint venant equations the surface and subsurface are fully coupled there is no explicit modelling of pipes as these are not provided in any national dataset lakes are not explicitly modelled but occur as a result of the physical characteristics of the catchment the catchments are assumed to have no flow boundary conditions apart from the outlet where there is assumed to be a weir the catchments are set to be saturated initially but the results from the first two years of simulation a 2 year spin up period are not used in the analysis shetran has been modified in several ways in order to improve its performance using national datasets the modifications include the following shetran now accommodates spatially varying rainfall and potential evapotranspiration data as opposed to only point rainfall data as used in most prior work this allows for use of recently developed gridded products and provides more realistic representation of important variability within catchments each model grid square is assigned a code in an ascii map that corresponds to a time series in a separate input file there is now also a better process within shetran for removing sinks in the dem i e grid squares at a lower elevation than all neighbouring grid squares to prevent unrealistic levels of ponding and surface storage which would act to reduce flows in an unrealistic way this is achieved in an iterative process the elevation of all the grid squares where there is a sink is gradually increased until it is no longer at a lower elevation than its neighbours this is then repeated until there are no sinks throughout the catchment a minimum dem describing the minimum elevation in a grid square is now used in combination with an average dem describing the average elevation in a grid square to more accurately route the river links calculated within shetran the position of the river links is calculated from the minimum dem by analysing the number of upstream grid squares that flow into a particular grid square when these reach a certain number the default is 20 a river channel is produced birkinshaw 2010 the channel elevations are based on the 2 adjacent grid squares the elevations are then modified so that there is always a downward flow path the average dem is used for all other processes shetran has been modified to accept a map of lake locations so that they can be accurately represented within catchments if a lake grid cell intersects a river link it is treated as an open water body by reducing the default strickler coefficient controlling surface roughness from 20 to 3 the strickler coefficient is the inverse of mannings roughness coefficient gauckler 1867 manning 1891 this acts to effectively slow flow and increase storage of water in the channel all river channels have an individual stickler parameter associated with them shetran now adjusts this parameter so that it corresponds with the lake value as a results of these physical characteristics and the low strickler coefficient in the channel when there is heavy rain and large river inflows the water level in these channels increase and overflow into the nearby grid squares occurs this surface water which corresponds to lake water storage gradually builds up and then reduces once the rain stops changes have also been made so that it is possible to assign strickler coefficients as a function of land cover rather than applying one parameter value for the whole catchment as has generally been the case in the past the option was always available within shetran although not normally used this allows roughness to vary with vegetation as would be expected e g concrete surfaces have a low roughness and thus high strickler coefficient whereas vegetated surfaces are rougher giving them a lower strickler coefficient 4 automatic set up of shetran in order to develop a national modelling system based on shetran a large array of data for the whole of great britain and the period 1960 2006 as described above was integrated into a framework that features a new user friendly graphical interface which extracts and prepares the data required for a shetran simulation of any catchment in great britain previously models would be set up manually this involved finding and downloading the appropriate datasets importing them into gis to fill sinks delineate catchment boundaries and extract the relevant area from each data layer then convert these to shetran input files and assign the parameters individually this new gui has vastly reduced the time it takes to set up and run a model from months birkinshaw 2010 to seconds the resultant model is an uncalibrated model based on the fixed set of parameters from national datasets the system represents substantial progress in the ability to deploy shetran for individual or very large numbers of catchments the python scripts underlying the system take input from a user interface scripted in html and javascript to automatically set up a model for a catchment see fig 1 users define the catchment resolution and start and end date of the simulation for gauged catchments an existing catchment boundary can be selected from a map while for ungauged catchments a shapefile or ascii map of the catchment boundary can be uploaded to extract an appropriate model the gauged catchment boundaries used in this work were determined on the basis of the records for all of the gauged catchments in the uk held by the national river flow archive nrfa national river flow archive 2014b the algorithm underpinning catchment setup takes a boundary shapefile or gauge number as input to delineate the catchment along with the start and end date of the simulation as input it then creates a project directory creates or selects the catchment mask and uses that to extract the other data and write it in the appropriate format for input to shetran the shetran data pre processor is run generating the input files for shetran the automated setup of shetran gb catchments is based on a single standard conceptual model derived from the available datasets the sub surface can have 2 3 layers defined by the soil and geology datasets these are further broken down by shetran into a total of 35 computational layers other key universal parameters are set to fixed values this standard conceptual model cannot be changed in an automated way through the gui but could be changed easily by the underlying model set up code individual models can also be altered by hand after set up with the gui 5 software design and implementation the software is designed for those wishing to use a physically based hydrological model but do not have access to detailed datasets for their catchment of interest new users of shetran often wish to begin modelling with a default set of parameters and subsequently refine the model when they become comfortable with the program the gui makes it much easier for someone to start using shetran from scratch but it is of little benefit to those with a full set of detailed data about their catchment as this will surpass the information provided by national datasets experienced users of shetran will also benefit from the software as they will be able to easily set up a catchment quickly the software requires a regular desktop computer with windows and python 2 7 with osgeo image imagedraw and tornado modules and their dependencies shetran gb is 245 1 gb as a bundle of python scripts exe files and an ascii data archive no preprocessing is required before use the gui only takes a few seconds to produce an average catchment model it takes 170 s to extract the full time series of data for the largest gauged catchment in great britain 9948 km2 the software has been designed so that the underlying code can be run with or without the gui the gui is a clean and simple tool for setting up one catchment model at a time but the underpinning python code can be run in parallel to set up many catchments at once the python model setup code interacts with the javascript interface through the tornado module receiving information about the required start and end dates of the simulation and the catchment boundary if the catchment exists in the boundary catalogue the existing catchment ascii files are retrieved if the catchment does not exist i e a shapefile has been uploaded instead an additional module is called that converts the uploaded shapefile into an ascii mask using the osgeo module once the ascii mask has been generated it is used to extract the other spatial information required from national maps the national maps are not read into memory rather the lines of the files are skipped over until the required point is reached the relevant lines are read into memory and then the file is closed this is to make the system adaptable to much higher resolution national maps when they become available all maps are stored using the british national grid coordinate system which provides the regular array required for shetran the values in the extracted maps are used to retrieve the corresponding time series data parameters are preassigned based on extensive experience from previous studies although parameter values can be adjusted later the automated setup of shetran models means that common errors can be avoided the shetran input files are very detailed and it is easy to make a typing error or assign the wrong value to a parameter the automated set up ensures that every map is the same size resolution and location and that each parameter is assigned correctly this adds a level of credibility and reduces the probability of errors in the model especially for large heterogeneous catchments 6 example simulation the eden catchment was simulated to demonstrate the suitability of the datasets for use in a national physically based modelling system the eden is a commonly studied area in the uk and so we are able to compare an uncalibrated version of shetran set up by the system described in this paper to a calibrated version set up for another study janes et al 2018 calibrate shetran to river flows for the eden at kirkby stephen 70 km2 in area and at temple sowerby 616 km2 over the period 1991 2001 the two catchments achieve nash sutcliffe efficiencies nse nash and sutcliffe 1970 of 0 85 and 0 86 respectively r2 moriasi et al 2007 of 0 88 and 0 7 and pbias moriasi et al 2007 of 14 and 8 for the validation period 2001 2007 uncalibrated shetran gb simulations produce nse values of 0 78 and 0 80 see fig 2 r2 of 0 79 and 0 81 and percentage bias in the water balance pbias of 8 6 and 1 0 for the same catchments for the period 1992 2002 this shows that the data used for the automated setup of shetran gb is appropriate but that model performance can be improved with calibration to provide an indication of the robustness of the system some key potential uncertainties for the eden catchment are now explored fig 3 compares the river channel network automatically derived in shetran from the digital elevation model at 1 km and 500 m resolution to the existing river network map ordnance survey 2018 the derived river network at both resolutions corresponds well to the main river channel through the catchment at 500 m resolution the secondary river channels are also well represented with only a few errors this illustrates the importance of resolution rather than the underlying data in the uncertainty associated with the dem rainfall data has many uncertainties associated with it including mechanical errors recording errors evaporation from partly filled buckets wind induced under catch and snow effects that can reduce rainfall totals by over 40 mcmillan et al 2012 pollock et al 2018 uncertainty is also introduced by gridding gauge data due to the interpolation method used the ukcp09 and ceh gear daily rainfall datasets use different interpolation methods but are still highly correlated indeed when the 24 h accumulation for each ceh gear 1 km time series is compared with its corresponding 5 km grid square the spearman s rank and pearson correlation coefficients range from 0 9 to 1 pet is another very uncertain variable as it can be calculated in approximately 50 ways lu et al 2007 and cannot be directly measured lu et al 2007 test six of these methods including penman monteith and find that pet can vary by up to 500 mm year in their study area of the southeastern united states depending on the pet method used other uncertainties in the land cover soil and geological data are very difficult to quantify without collecting field data from the catchment itself but there are undoubtedly large uncertainties associated with the significant heterogeneity that can occur at sub kilometre scales in catchments more extensive analysis of the system s performance for a large number of catchments including the uncertainties associated with the input datasets will follow in subsequent work such analyses are facilitated by the flexibility of shetran gb which enables substitution of alternative datasets to assess their impacts on simulations 7 discussion this paper outlines the automated set up of a physically based model for catchments in gb the aim of this was to increase the usability of shetran and to provide a uniform framework for exploring the possibility of a national uncalibrated model based on parameters from established datasets so that ungauged catchments can be effectively modelled with some certainty additional experiments will test this theory whereas this paper focuses on describing the modelling framework itself there are many positives to using such a framework the time taken to set up a model has been vastly reduced allowing the modeller to focus on other aspects of the modelling process calibration uncertainty analysis exploration of a wider range of model outputs the data used are currently the best available and they can easily be replaced by new or improved data sets when they become available the models are based on a standard set of parameters which provides consistency between models the automated process reduces errors and uncertainties by removing the possibility of errors when manually setting up the complex model input files however such a strict framework does restrict the modeller from developing their own conceptual model of the catchment and does nothing to help incorporate other more detailed local datasets there is also the danger that these models will be used without due consideration of catchment processes whilst increased automation of both model setup and automatic calibration can improve efficiency and reliability in hydrological modelling it is important not to overlook the role of detailed local knowledge of an area and the role of a modeller s expertise the gui has been designed to be simple and easy to use whilst the underlying code has been designed to be flexible there is much scope to use the code for other purposes for example building different conceptual models e g altering the boundary conditions the universal parameters etc generating input files for models other than shetran or even applying the framework to other countries many countries already have national datasets which could easily replace those presented here indeed as a starting point for any country a series of global datasets could be used including the shuttle radar topography mission srtm dem jarvis et al 2008 the fao harmonised world soil database nachtergaele and batjes 2012 the nasausgs global land survey gls datasets gutman et al 2013 and a global 50 yr 3 hourly 1 0 dataset of meteorological forcings sheffield et al 2006 the global maps would need to be projected onto an equal area grid and have shetran compatible properties assigned to them as described here before being incorporated into the system as discussed in the introduction shetran has been used to model catchments across the world and it is likely that the main limiting factor for international modelling would be the quality of the input datasets used not the physical processes being represented in shetran itself the most difficult data to obtain would be a parameterized hydrogeological dataset and this would be a major limitation when trying to model groundwater dominated catchments 8 conclusion this paper has detailed the datasets processing and software development involved in setting up a national shetran modelling system for great britain freely available data were collated and processed into 1 km rasters aligned with the british national grid the datasets together describe the country s physical hydrological and climatic characteristics modifications were made to shetran itself to permit more realistic input data such as by allowing distributed precipitation inputs as well as to enhance representation of some hydrological processes for example channel delineation was improved by using a minimum dem with modifications also made to handle dem sinks and improve the treatment of lakes in shetran a graphical user interface was developed so that modellers can rapidly set up a shetran catchment model the system could be straightforwardly transferred to other regions of the world using global or national datasets forthcoming publications will examine the robustness of the system assess the performance of simulations and outline ways in which it could be used for impact case studies whilst the datasets have been set up and used in shetran it would be very simple to use exactly the same data in any other model code the xml file precipitation data and ascii grids produced could be easily adapted to be used by any other distributed hydrological model 9 software availability shetran gb was developed by elizabeth lewis and stephen birkinshaw the software is available from elizabeth lewis elizabeth lewis2 ncl ac uk cassie building newcastle university newcastle upon tyne ne1 7ru uk software first available from january 2016 for free under academic licence software requires a regular desktop computer with windows and python 2 7 with osgeo image imagedraw and tornado modules shetran gb is 245 1 gb as a bundle of python scripts exe files and an ascii data archive technical documentation is embedded directly in the code through comments user documentation is provided separately which provides a worked example of how to use the software acknowledgements funding this work was supported by the natural environment research council ne j500239 1 hayley j fowler is funded by the wolfson foundation and the royal society as a royal society wolfson research merit award wm140025 holder contains data from the uk national river flow archive hosted by the centre for ecology hydrology and operated in partnership with uk hydrometric measuring authorities crown copyright 2009 the uk climate projections data have been made available by the department of the environment food and rural affairs defra and department of energy and climate change decc under licence from the met office newcastle university university of east anglia and proudman oceanographic laboratory these organisations accept no responsibility for any inaccuracies or omissions in the data nor for any loss or damage directly or indirectly caused to any person or body by reason of or arising out of any use of this data lcm 2007 and database right nerc ceh 2011 all rights reserved contains ordnance survey data crown copyright and database right 2007 
26348,thematic maps are important for a range of disciplines including spatial planning and ecosystem status assessments despite an increasing focus on accuracy assessment methods to ensure maps are fit for purpose the adoption of these recommendations has not been widespread we present a methodology which utilises bootstrap aggregation and adheres to recommended practices for accuracy assessments furthermore additional information is extracted from the model outputs to produce spatial maps of confidence also supporting map interpretation the methodology has been applied to two study sites using both pixel based and object based units of analyses accuracy assessments for both study sites identified the classes that were responsible for most of the map error in addition spatially explicit confidence maps supported our understanding of the sources of error this paper provides a useful methodology to improve accuracy assessment and reporting and is well suited to studies where groundtruth data are limited keywords accuracy bagging confidence image object pixel remote sensing 1 introduction thematic mapping from remotely sensed data is typically based on image classification via visual or more recently computer aided analysis this can be attempted by finding patterns in the predictive data unsupervised classification alternatively groundtruth observations are used to guide the classification of the predictive data supervised classification in both cases the resulting classified image may be treated as a thematic map depicting the spatial distribution of the mapped classes foody 2002 such classes might relate to land cover or habitat for example land cover and habitat maps are an important tool for a range of disciplines including spatial planning monitoring of change ecosystem status assessments and economic valuing of ecosystem goods and services however any map will be a simplification of reality which will invariably introduce an element of map error smits et al 1999 foody 2002 furthermore errors are likely to be accumulated at each step of the mapping process i e data acquisition processing analysis and modelling which are then propagated into the final map lunetta et al 1991 it is important to quantify and communicate such errors because maps without associated information on the accuracy of the predictions essentially remain untested hypotheses strahler et al 2006 accuracy acc is defined as the closeness of agreement between assigned classes and the true ground conditions in a map scene it is commonly reported as some measure of correctly classified mapping units compared to the original ground conditions assuming that the observed conditions are a representative sample of reality then we can say that accuracy is the degree to which a derived map agrees with reality error e is the discrepancy between the assigned class and observed ground conditions in general acc e 1 100 or e 1 acc without an accuracy assessment it will not be possible for the map user to decide whether a map is fit for the intended purpose missing information on map accuracy might have dramatic consequences e g when valuing ecosystem services based on land cover maps foody 2015 beyond the use of map accuracy as a quality statement relating to a map product such assessments are of importance when evaluating the performance of different classifiers and for understanding sources of map errors uncertainty is the absence of exact knowledge and uncertainty may be present throughout the modelling process refsgaard et al 2007 for example when applying a predictive model the operator will determine the type of model used resolution of prediction variables groundtruth method and sample size all of which may influence the final map stephens and diesing 2014 however where uncertainty is present it may not necessarily result in map error foody 2005 and therefore understanding the relationship between uncertainty and error is important for interpreting maps barry and elith 2006 separate the sources of map error into two broad categories errors arising from deficiencies in the data such as missing covariates small sample size sampling bias and errors in the variables and those introduced by the model specification here we investigate the issue of prediction errors introduced through the model misspecification to quantify and understand what errors are present it is necessary to perform a thorough accuracy assessment map accuracy can be analysed by comparing the predictive map to a reference dataset typically groundtruth data this is done to determine whether the map is as close to reality as possible given the available resources and whether it is fit for purpose bennett et al 2013 foody 2002 outlined the four major aspects of map error that may need to be addressed to determine whether a map is fit for purpose first it is important to determine the frequency of errors second the magnitude of errors needs to be determined as some errors are more important than others for example where two classes are very similar the consequence of misclassification may be minor however other classification errors may be more important third the type or source of error needs to be identified for example was the error caused by inaccuracies in the reference data issues of class definition or errors in the predictive model finally it is essential to determine the spatial distribution of error because error is rarely evenly distributed across the map steele et al 1998 kyriakidis and dungan 2001 rather it is typically concentrated near class boundaries or in areas of high complexity steele et al 1998 foody 2005 lucieer and lucieer 2009 in practice addressing all these points can be complex thus if the end users wish to apply maps to a variety of purposes then understanding each of these factors may influence the suitability of a map to a particular purpose stehman 1997 stehman and czaplewski 1998 riemann et al 2010 to sufficiently evaluate these types of map errors a number of studies have sought to standardise the accuracy assessment methodology for determining the value of a map and understanding the sources of map error stehman 1997 foody 2002 strahler et al 2006 olofsson et al 2013 2014 bradley et al 2016 the basis of most accuracy assessments is the error matrix also known as confusion matrix or contingency table and this should be provided in its raw form with any map foody 2002 stehman 2004 strahler et al 2006 there is ongoing debate over which measures best summarise the error matrix liu et al 2007 pontius and millones 2011 commonly recommended measures include overall accuracy user s and producer s accuracy sensitivity and specificity and balanced accuracy congalton 1991 fielding and bell 1997 brodersen et al 2010 however as each metric assesses a different component of map error and the suitability of different metrics will often depend on the specific research question being asked therefore it is generally agreed that multiple accuracy measures provide a clearer summary of map accuracy and these should be accompanied with confidence limits foody 2002 liu et al 2007 despite these recommendations accuracy assessments are still often oversimplified looking for the map with the highest accuracy rather than considering the importance of the error furthermore the issue of providing spatially explicit error assessments is all too commonly ignored whilst values measuring accuracy can enable comparison between maps they represent the global average accuracy of the map or a class revealing nothing about the spatial distribution of error if errors are concentrated in certain areas then the global averages may not be appropriate for large sections of the study site liu et al 2004 comber et al 2012 for example transitional zones between habitats mean that map error or uncertainty may be concentrated near class boundaries particularly when using hard classifications foody et al 1992 lucieer and lucieer 2009 rocchini et al 2013 several approaches for providing spatially explicit accuracy assessments have been developed which convey this information in an informative way kyriakidis and dungan 2001 applied geostatistics to model variation in accuracy across the study site foody 2005 used locally constrained error matrices which were then spatially interpolated to produce a map of accuracy for the study site this concept was then developed further by comber et al 2012 who applied geographically weighted regression so that points closer to the area of interest were weighted higher in addition to interpolating accuracy across the spatial domain khatami et al 2017 showed that the spectral domain or the predictive data domain for models that do not utilise multiple imagery bands can also produce maps of accuracy which may support map interpretation however these methods are data intensive so in many cases the collection of sufficient samples are not feasible strahler et al 2006 for example these four studies used a minimum of 150 validation points to develop a spatially constrained accuracy measure this far exceeds what is available for many studies particularly where time and budgetary constraints prevent post classification sampling steele 2005 for instance when collecting data in the marine environment to bridge the gap between current recommendations for accuracy assessment and what is often currently applied we present a tool for supervised thematic mapping in the form of an r script bootstrap aggregation also known as bagging breiman 1996 is incorporated into the methodology to produce more robust predictions and assessments of map accuracy whilst bagging has been shown to improve the robustness of predictions steele et al 2003 carreiras et al 2006 additional information can also be extracted from the individual models to spatially map the confidence of the prediction and produce more robust accuracy assessments by bringing these methods together in a simple to apply methodology we improve the informative value of habitat maps the code to apply these methods in r is provided in appendix s1 to provide a simple means of applying these methods more widely to improve how accuracy assessments are performed in other studies to illustrate the application and versatility of the proposed r tool and show the informative value of the model outputs two case studies using different data types are presented 2 material and methods 2 1 overview of modelling process a generalised overview of the proposed methodology is presented in fig 1 the methodology is semi automated with several user defined components that may vary depending on the data and objectives of the study see appendix s1 for accompanying r script the methodology starts with the user selecting the most appropriate unit of analysis i e pixel based or image object based step 1 the groundtruth observations are then divided multiple times with replacement to produce a user defined number of randomly selected testing and training data sets from the groundtruth observations step 2 each individual dataset will form one bootstrap iteration that will later be aggregated to produce the final map and accompanying accuracy assessment a classification model is then created for each randomly selected training dataset step 3 predictions from each classification model are cross validated against the corresponding independent testing dataset from this an error matrix is populated for each model and accuracy statistics can be calculated step 4 average and standard deviations of the accuracy statistics are calculated from the iterations each classification model is used to generate a spatial prediction for the area of interest these predictions are then aggregated with the final class for each unit of analysis predicted based on a plurality vote step 5 the spatial measures of predictive confidence are calculated by aggregating mapped outputs of each iteration step 1 unit of analysis pixels continue to be the predominant unit of analysis for many researchers however geographic object based image analysis geobia has several advantages over traditional pixel based thematic mapping approaches for instance i partitioning an image into objects is akin to the way humans conceptually organise the landscape seascape to comprehend it ii using image objects instead of pixels as basic units is less computationally intensive iii image objects exhibit useful features e g shape texture contextual relationships with neighbouring objects that pixels lack iv image objects are easily integrated into vector gis hay and castilla 2006 2008 geobia is widely used in terrestrial remote sensing applications blaschke 2010 blaschke et al 2014 but its use for benthic habitat mapping is relatively new lucieer 2008 lucieer and lamarche 2011 zhang et al 2013 diesing et al 2014 wahidin et al 2015 a segmentation algorithm is used to subdivide the image into image objects discrete regions of a digital image that are internally coherent and different from their surroundings castilla and hay 2008 as both units of analysis are regularly used the methodology has been developed to implement either an object based or pixel based approach step 2 bootstrap aggregation due to financial and logistic constraints it is rare to have truly independent test data particularly in the marine environment where a suitably large and independent reference dataset cannot be collected data partitioning is regularly used to create training and testing datasets as subsets from the original dataset however a single split of data into training and test sets might result in a large variance in estimates of accuracy lyons et al 2018 using less training observations to inform the model can influence model fit or create misleading accuracy measures where test observations are limited fielding and bell 1997 steele 2005 the bootstrap aggregation or bagging method is increasingly used to improve classification accuracy and provide more reliable accuracy assessments this is of increased benefit where predictions are unstable breiman 1996 steele et al 2003 carreiras et al 2006 bagging involves creating multiple training and corresponding testing datasets by randomly resampling the reference data with replacement a separate predictive map is then generated from each dataset with the remaining data used to perform an accuracy assessment the proportion of groundtruth observations used for model training and testing and the number of cross validation datasets created is dependent on a range of factors including total number of observations per class complexity of the system variations observed between individual models and computational power available in addition class frequency among groundtruth observations is rarely balanced therefore the script has been developed to subsample into testing and training data using stratified random sampling with proportional allocation based on class type olofsson et al 2014 this produces training and testing samples with approximately the same class frequency as the original dataset in both case studies the groundtruth data are split with 70 used for model training and 30 used to test the prediction although this can be adjusted by the operator this is repeated for an operator defined number of bootstrap replicates twenty five replicates are recommended to optimise performance breiman 1996 while 10 replicates would generally be the minimum step 3 machine learning algorithm this study presents the use of the predictive model random forest rf breiman 2001 while rf models are presented in this study it should be noted that the mapping tool presented here applies the predictive models using the r package caret this allows flexibility of model selection from several classifier based algorithms which produce predictions as both a class and probability format some basic classification methods are presented in the accompanying code e g generalised linear models k nearest neighbours or conditional inference trees but we stress that model selection is ultimately the decision of the operator rf models were selected for the case studies as they are a commonly used classification model which can be applied without extensive tuning and has shown high predictive accuracy in a number of domains prasad et al 2006 mutanga et al 2012 oliveira et al 2012 rodriguez galiano et al 2012 zhi et al 2014 turner et al 2018 step 4 accuracy assessment map validation is performed using the test dataset that has been withheld from model training for each bootstrapped sample using these test datasets an error matrix is populated for each individual map performance metrics and the final error matrix present the mean accuracy across the bootstrapped samples with confidence intervals representing the variation between samples in this study specific and global accuracy statistics for sensitivity specificity and balanced accuracy plus global accuracy statistics of overall accuracy and cohen s kappa coefficient cohen 1960 and class specific accuracy measures of user s and producer s accuracy are automatically calculated step 5 prediction and spatially explicit confidence maps the final prediction is determined as an ensemble map based on plurality vote from the individual maps i e first past the post this method is known as monte carlo cross validation it is also possible to extract the proportional frequency of the most common class for each mapping unit to improve our understanding of how stable the prediction is based on different groundtruth observation data saatchi et al 2007 for example where all models predicted one habitat this would have a maximum confidence and indicate high stability between models in addition along with assigning each individual mapping unit to a class for several commonly used models within the caret package such as generalised linear and additive models rf and conditional inference trees it is also possible to extract the probability of membership to that class this value is an estimate of the probability of belonging to a class and is a feature of these models the probability of each class within the individual models can be used as an indication of the fit of the classification algorithm in separating the classes carreiras et al 2006 by extracting the average probability from the bootstrapped replicates for each unit of analysis a map of probability of class membership can be generated indicating the quality of model fit across the study site finally combined confidence is computed by multiplying the frequency of the most common class and the average probability of the most common class for each mapping unit this final confidence map presents a simple representation of confidence for the study site that incorporates an indication of model fit and model stability in one graphic allowing the identification of high and low confidence across the study site while not directly mapping accuracy these measures have been used to map the relative confidence of the methodology indicating regions of uncertainty and can provide valuable information to support map interpretation carpenter et al 1999 saatchi et al 2007 ahsan et al 2010 diesing and stephens 2015 2 2 case study 1 pixel based 2 2 1 study area the runswick bay marine conservation zone mcz is in the north sea on the yorkshire coast united kingdom uk fig 2 the site extends approximately 7 km to its seaward boundary covering a total area of 68 km2 and reaching water depths of up to 50 m the site comprises of areas of exposed bedrock with a mosaic of different sediment types increasingly widespread with depth 2 2 2 groundtruth observation data due to the nature of the substrate type benthic observation data used for this study were collected using a combination of drop camera surveys and sediment grab sampling drop camera sampling was performed at all sample stations to capture still images every 10 15 m over a distance 150 m where the drop camera indicated the presence of sediment a grab sample was performed still images and grab samples were classified to level three of the european nature information system eunis habitat types five benthic habitat classes were observed in the underwater still images namely sublittoral coarse sediment sublittoral sand sublittoral mud and sublittoral mixed sediments and moderate energy circalittoral rock continuous video transect data were reduced to observations at point locations by randomly selecting positions along the transect with a minimum distance of 100 m between samples or to the nearest grab sample this provided a groundtruth observation dataset of 162 observations of which 51 were sediment grabs and 112 were underwater still images table 1 and fig 2 2 2 3 acoustic data full coverage of bathymetry and backscatter intensity of the acoustic return data was acquired using a kongsberg em3002d multibeam echosounder system in 2013 bathymetry data were collected and processed in accordance with the international hydrographic organisation standards for hydrographic surveys order 1 special publication 44 edition 4 iho 2008 using the software caris hips and sips version 7 1 1 the software package qps fm geocoder toolkit was used to produce fully compensated and corrected backscatter mosaics the bathymetry and backscatter data were initially resampled to the same grid at 5 m pixel resolution to simplify the processing requirements for analysis a 2d fourier filter wilken et al 2012 was applied to the backscatter to reduce apparent stripe noise to further characterise the various topographic attributes of the seafloor six secondary derived features recommended in lecours et al 2017 were produced from the bathymetry data the mean bathymetry and standard deviation of the bathymetry were subsequently discarded due to having a high correlation 0 7 with other variables appendix s2 no derivatives of backscatter were assessed in this study 2 2 4 model algorithm parameters the number of cross validation runs was set to 25 each with a split of 70 training 30 testing data 2 3 case study 2 image object based 2 3 1 study area two tree island study site is in the outer thames estuary on the coast of the uk fig 3 the upper shore is covered by a wide belt of saltmarsh which borders an intertidal mudflat the mid and upper intertidal sections of the mudflat are covered by an extensive 100 ha intertidal seagrass meadow composed of a patchwork of zostera noltii and zostera angustifolia beds 2 3 2 remotely piloted aircraft imagery a quest q pod remotely piloted aircraft rpa was flown over the site during periods of low water in august and september of 2015 the rpa carried a twin payload of two 12 megapixel cameras one acquiring red green blue imagery and the other fitted with a filter to acquire imagery in the near infrared nir images were georectified using the software agisoft photoscan through a combination of aircraft position and altitude data and ground control markers images were aligned and mosaiced based on common points on the photographs to produce a fully georeferenced orthomosaic with six bands red green blue nir1 nir2 nir3 at 3 cm pixel resolution and a 10 cm resolution digital surface model dsm fig 4 additional raster layers were calculated for a selection of band ratios red green nir1 red nir2 red nir1 green nir2 green we acknowledge that other band ratios are available that may have improved the prediction xue and su 2017 however for the purpose of exploring map predictions and accuracy these band ratios were considered to be sufficient and our predicted map achieved high accuracies the 10 cm cell size dsm was used to calculate slope on a five pixel radius and a topographic position index tpi weiss 2001 with a 100 pixel radius 2 3 3 geobia segmentation the predictor layers were imported into ecognition v9 2 for image segmentation the multi resolution segmentation algorithm used in this study partitions an image into regions called objects with homogenous attributes across a user defined set of layers based on a level of allowed variability defined by the scale parameter and effect of object shape compactness and shape parameters segmentation was carried out on a combination of the red green and blue bands using the multi resolution segmentation algorithm with a scale parameter of 25 shape parameter of 0 1 and compactness of 0 5 the parameters were tuned based on visual inspection of the resulting segmentations fig 4 this provided a segmentation of the study site with approximately 1 6 million objects a selection of layer attributes including summary and textural statistics were calculated for the objects object mean 75th percentile and skewness were calculated for all input layers additionally object grey level co occurrence matrix contrast and entropy were calculated for the spectral reflectance and band ratio layers the objects with their accompanying attributes were exported from ecognition as a polygon shapefile for further analysis 2 3 4 observation data a random selection of 1000 objects were visually inspected and classified into one of four habitats mud saltmarsh 100 seagrass cover sg 100 and 100 seagrass cover sg s from these 100 observations of each class were randomly subsampled to produce a groundtruth dataset of 400 objects fig 3 2 3 5 model algorithm parameters the number of cross validation runs was set to 10 each with a split of 70 training 30 testing data 3 results 3 1 case study 1 pixel based the habitat map for runswick bay mcz fig 5 a depicts moderate energy circalittoral rock nearshore with a channel of predominantly sublittoral sand and sublittoral mud to the north these finer sediments grade to a complex matrix of sublittoral coarse sediment and sublittoral mixed sediments further from shore in deeper waters generally this is consistent with what was observed in the groundtruth observations however the detailed accuracy assessment reveals the prediction s limitations overall accuracy whilst above the no information rate was approximately 43 fig 6 and fig 7 a more detailed examination of the error matrix to determine the predominant sources of error from the prediction suggests that sublittoral mixed sediments was the least accurately predicted class with a mean user s and producer s accuracy of 32 fig 6 this class was frequently incorrectly classed by the models as either subtidal coarse sediment or subtidal mud of the subtidal mixed sediments samples an average of 5 5 samples were incorrectly classed as subtidal coarse sediment and 2 8 as subtidal coarse sediment compared to only 4 8 that were correctly classified the best performing class was sublittoral sand which had a user s and producer s accuracy of 64 and 55 respectively fig 6 moderate energy circalittoral rock had a high user s accuracy of 62 but the producer s accuracy was the second worst of the classes at only 41 inspection of the spatially explicit confidence maps indicates that the proportional frequency of the most common class was generally high across the whole study site fig 5b however there were areas of reduced confidence visible in the average probability of the most common class map fig 5c this map showed that areas with the lowest average probability of the most common class were in the regions classified as sublittoral mud and those pixels positioned near the boundaries between two different classes such as between sublittoral coarse sediment and sublittoral mixed sediments or between sublittoral mud and sublittoral mixed sediments due to the magnitude of variation present in the average probability of the most common class fig 5c relative to the proportional frequency of the most common class fig 5b variation in the combined confidence map fig 5d was predominantly driven by the average probability of the most common class in this map we see the same general pattern observed in the average probability of most common class map with the trend augmented by the subtler differences in proportional frequency of most common class 3 2 case study 2 object based the habitat map for the two tree island fig 8 a depicts saltmarsh to the north with several drainage channels cutting through the saltmarsh which have a mud substrate these habitats transition to a region of high density seagrass coverage sg 100 in the centre of the site to the south there is a complex matrix of sg 100 sg s and mud the accuracy assessment indicated that the model was high performing with an overall accuracy of 90 fig 9 and fig 10 as the number of observations for each class were equal this is also true for the balanced accuracy which is the average of the accuracy of each class brodersen et al 2010 class specific accuracy suggests that the greatest source of error related to sg s fig 9 which had a mean user s accuracy of 85 and producer s accuracy of 91 equivalent to a sensitivity of 0 85 fig 10 the error matrix showed that of the 10 sg s test observations for each replicate model an average of 1 7 observations were falsely classified as mud and 2 8 falsely classified as sg 100 by the models comparison of the spatially explicit confidence maps fig 8b d was consistent with the error matrix the proportional frequency of the most common class was high across the study site indicating that there was little variability in predictions from the individual bootstrapped models fig 8b however confidence of the average probability of the most common class was more variable across the site fig 8c for example confidence was lowest in areas of transitional habitat at the border of the saltmarsh where mud sg s and sg 100 are all present there was also reduced confidence in the two tidal channels to the south of the site which were classified as mud and sg s fig 8c 4 discussion assessment of thematic map accuracy is regularly oversimplified failing to address the four types of error that may be present within a map here we have presented a robust methodology which follows recommended practices to assess the suitability of a map foody 2002 liu et al 2007 diesing et al 2016 in addition by incorporating bootstrap aggregation into the modelling process this methodology is well suited to data limited environments where withholding groundtruth observations for the accuracy assessment would sacrifice model precision steele 2005 for example a total of 162 groundtruth observations were available for the runswick bay mcz with which to inform the habitat model and assess the model accuracy by repeatedly subsetting the data with replacement each replicate model is validated against an independent dataset thus the random bias introduced by any one split of the data is reduced by creating an ensemble of all the models carreiras et al 2006 falkowski et al 2009 a detailed understanding of accuracy supports the use of one map over another this is important as maps created using different methods or based on data from different sources will vary environmental managers and policy makers are then faced with the challenge of trying to determine which map to use for planning and management therefore the intention of an accuracy assessment is to determine whether a map is suitable for a particular purpose bennett et al 2013 van vliet et al 2016 meaning comparing different maps overall accuracies may be of limited value fielding and bell 1997 for example a habitat map may still be suitable for a particular application despite a low overall accuracy in the case of the runswick bay mcz the overall accuracy of approximately 43 would generally be considered low foody 2002 resulting in the model being discarded however the error matrix and class specific accuracy measures reveal that the largest source of error relates to the models inability to separate the sublittoral mixed sediments and sublittoral coarse sediment classes so if the difference between sublittoral mixed sediments and sublittoral coarse sediment is of little concern for management then the operator may choose to merge these into a single class thereby increasing the overall accuracy similarly if the operator were predominantly concerned with identifying areas that are likely to be moderate energy circalittoral rock habitat as might be the case when managing creel fishing in the region then despite the models low overall accuracy the delineation of this habitat may allow management to be targeted in these specific areas another common application of habitat maps is to understand change through time which may be to understand the effects of a pressure to the environment or to assess the effects of management measures imposed however if the magnitude of the spatial change is of a similar magnitude to the map error then it may not be possible to determine whether any change has occurred in the two tree island example the model would generally be considered high performing 90 overall map accuracy yet it is notable that the mean producer s accuracy of the sg s class is less than the other three classes 85 compared to 95 for the other classes this makes sense as sg s is the intermediate habitat between complete seagrass coverage sg 100 and mud therefore errors are likely to occur where these classes are most similar so if the intention were to monitor changes to the extent of seagrass it may require a substantial change in sg 100 and sg s to conclude that an observed change was not just model variation however observed changes to the saltmarsh class could be interpreted with greater confidence due to this class s high accuracy these examples highlight how thematic map use is supported by understanding the sources of error which can be achieved through a detailed assessment of accuracy by calculating multiple metrics and class specific measures in an accuracy assessment the proposed method provides the necessary information for map users to make an informed decision regarding map interpretation the spatially explicit confidence maps are valuable for understanding both the sources of error and considering the suitability of a map in addition understanding spatial confidence allows targeted management measures to be implemented within the study site such as applying different levels of protection based on the confidence that a feature is present or identifying areas of low confidence which need to be targeted during additional surveys in the two tree case study there is a distinct band of reduced confidence in the transition zone between sg s and saltmarsh visible in the average probability of the most common class fig 8c this is unsurprising as map error is commonly concentrated at the boundaries between classes which have similar environmental characteristics steele et al 1998 foody 2002 nevertheless this map provides a clear representation of areas where the model is uncertain indicating that managers should use caution when interpreting the exact boundaries of classes in this region reduced confidence is also present in the tidal channels in the south east corner of the study site this may relate to the presence of a thin film of diatoms covering the mud that were observed in this area causing a similar spectral response to the sparsest seagrass cover lack of in situ groundtruth locations for the diatom film and the inability to identify it in images with acceptable confidence led to its inclusion in the mud class in this analysis however this could potentially be improved with groundtruth observations for a diatom film class spatial confidence in the runswick bay mcz case study is also predominantly driven by the average probability not frequency of the most common class fig 5c this supports what was observed in the error matrix i e that it is likely the classes used were unsuitable to be mapped using the available predictive layers as map errors are predominantly associated with the classes of sublittoral mud sublittoral coarse and sublittoral mixed sediments even though this would be expected based on the error matrix on its own the map makes this clearer for the end user who may make decisions based on the distribution of certain classes 4 1 limitations the maps of spatial confidence provide an indication of uncertainty within the model and not necessarily the distribution of map accuracy the confidence measures shown in the map outputs relate to the model fit and stability bootstrap repeatability for the specific predictor variable values occurring at each pixel or object independently of the neighbouring pixels and objects consequently issues relating to spatial bias are not addressed for methods of classifying map accuracy across a study site we recommend those discussed in khatami et al 2017 however accuracy interpolation methods for the spatial and spectral domains have so far only been applied to large datasets for example 548 test pixels were used in khatami et al 2017 as this is beyond the number of samples available for many studies the application of these methods is currently limited this paper presents a range of accuracy statistics while many accuracy statistics have been criticised liu et al 2007 pontius and millones 2011 bradley et al 2016 we provide a range of commonly used statistics to give researchers the ability to interpret a specific problem using the method that is most appropriate for the specific question at hand further developments of the tool may include new accuracy metrics such as those described in pontius and millones 2011 4 2 implications and suggestions for future research this paper presents the use of a semi automated mapping methodology utilising bootstrap aggregation in a simple to apply r script appendix s1 central to this methodology is that it adheres to current recommended practices for accuracy assessments and makes full use of available data especially where groundtruth observations may be limited this methodology is applied to both pixels and image objects as units of analysis to display the flexibility and suitability of this approach to understand the sources of map error and the suitability of the final prediction for a range of applications while the methods presented in this paper are not new the script allows an easy application to other studies generating a map is often an iterative process examining the various sources of uncertainty within the data and methodologies applied to optimise the use of the available data therefore even where this is not used to produce the final prediction it has the potential to provide valuable insights into potential sources of error within the model predictive mapping such as these have been applied to a wide range of applications such as cropland mapping vogels et al 2017 predicting wildfire susceptibility leuenberger et al 2018 and economic valuations of ecosystem services foody 2015 and understanding the uncertainty in these models can support map interpretation in addition due to the rigour of the accuracy assessment this tool could provide an excellent base with which to examine other sources of uncertainty such as positioning error choice of classification method and others software availability the complete and well commented code for cross validation and spatial variability is applied in the software r and is available at https github com pmitchell cefas mitchell et al ems appendix s1 model scripts acknowledgements the 2015 rpa data collection at two tree island was funded by cefas as part of the cefas environment agency partnership agreement we would like to thank the cefas staff members who collected and processed data within the two tree island study site we thank defra and the mpa programme partner organisations for allowing the data from the runswick bay mcz to be used in this study and crew and scientific staff who were involved in the collection and processing of the data the funding for writing this paper also comes from the defra funded mpa programme we thank dr alexander callaway cefas for valuable comments on an earlier draft of this manuscript appendix a supplementary data the following are the supplementary data related to this article supplementary data 1 supplementary data 1 supplementary data 2 supplementary data 2 appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 07 014 
26348,thematic maps are important for a range of disciplines including spatial planning and ecosystem status assessments despite an increasing focus on accuracy assessment methods to ensure maps are fit for purpose the adoption of these recommendations has not been widespread we present a methodology which utilises bootstrap aggregation and adheres to recommended practices for accuracy assessments furthermore additional information is extracted from the model outputs to produce spatial maps of confidence also supporting map interpretation the methodology has been applied to two study sites using both pixel based and object based units of analyses accuracy assessments for both study sites identified the classes that were responsible for most of the map error in addition spatially explicit confidence maps supported our understanding of the sources of error this paper provides a useful methodology to improve accuracy assessment and reporting and is well suited to studies where groundtruth data are limited keywords accuracy bagging confidence image object pixel remote sensing 1 introduction thematic mapping from remotely sensed data is typically based on image classification via visual or more recently computer aided analysis this can be attempted by finding patterns in the predictive data unsupervised classification alternatively groundtruth observations are used to guide the classification of the predictive data supervised classification in both cases the resulting classified image may be treated as a thematic map depicting the spatial distribution of the mapped classes foody 2002 such classes might relate to land cover or habitat for example land cover and habitat maps are an important tool for a range of disciplines including spatial planning monitoring of change ecosystem status assessments and economic valuing of ecosystem goods and services however any map will be a simplification of reality which will invariably introduce an element of map error smits et al 1999 foody 2002 furthermore errors are likely to be accumulated at each step of the mapping process i e data acquisition processing analysis and modelling which are then propagated into the final map lunetta et al 1991 it is important to quantify and communicate such errors because maps without associated information on the accuracy of the predictions essentially remain untested hypotheses strahler et al 2006 accuracy acc is defined as the closeness of agreement between assigned classes and the true ground conditions in a map scene it is commonly reported as some measure of correctly classified mapping units compared to the original ground conditions assuming that the observed conditions are a representative sample of reality then we can say that accuracy is the degree to which a derived map agrees with reality error e is the discrepancy between the assigned class and observed ground conditions in general acc e 1 100 or e 1 acc without an accuracy assessment it will not be possible for the map user to decide whether a map is fit for the intended purpose missing information on map accuracy might have dramatic consequences e g when valuing ecosystem services based on land cover maps foody 2015 beyond the use of map accuracy as a quality statement relating to a map product such assessments are of importance when evaluating the performance of different classifiers and for understanding sources of map errors uncertainty is the absence of exact knowledge and uncertainty may be present throughout the modelling process refsgaard et al 2007 for example when applying a predictive model the operator will determine the type of model used resolution of prediction variables groundtruth method and sample size all of which may influence the final map stephens and diesing 2014 however where uncertainty is present it may not necessarily result in map error foody 2005 and therefore understanding the relationship between uncertainty and error is important for interpreting maps barry and elith 2006 separate the sources of map error into two broad categories errors arising from deficiencies in the data such as missing covariates small sample size sampling bias and errors in the variables and those introduced by the model specification here we investigate the issue of prediction errors introduced through the model misspecification to quantify and understand what errors are present it is necessary to perform a thorough accuracy assessment map accuracy can be analysed by comparing the predictive map to a reference dataset typically groundtruth data this is done to determine whether the map is as close to reality as possible given the available resources and whether it is fit for purpose bennett et al 2013 foody 2002 outlined the four major aspects of map error that may need to be addressed to determine whether a map is fit for purpose first it is important to determine the frequency of errors second the magnitude of errors needs to be determined as some errors are more important than others for example where two classes are very similar the consequence of misclassification may be minor however other classification errors may be more important third the type or source of error needs to be identified for example was the error caused by inaccuracies in the reference data issues of class definition or errors in the predictive model finally it is essential to determine the spatial distribution of error because error is rarely evenly distributed across the map steele et al 1998 kyriakidis and dungan 2001 rather it is typically concentrated near class boundaries or in areas of high complexity steele et al 1998 foody 2005 lucieer and lucieer 2009 in practice addressing all these points can be complex thus if the end users wish to apply maps to a variety of purposes then understanding each of these factors may influence the suitability of a map to a particular purpose stehman 1997 stehman and czaplewski 1998 riemann et al 2010 to sufficiently evaluate these types of map errors a number of studies have sought to standardise the accuracy assessment methodology for determining the value of a map and understanding the sources of map error stehman 1997 foody 2002 strahler et al 2006 olofsson et al 2013 2014 bradley et al 2016 the basis of most accuracy assessments is the error matrix also known as confusion matrix or contingency table and this should be provided in its raw form with any map foody 2002 stehman 2004 strahler et al 2006 there is ongoing debate over which measures best summarise the error matrix liu et al 2007 pontius and millones 2011 commonly recommended measures include overall accuracy user s and producer s accuracy sensitivity and specificity and balanced accuracy congalton 1991 fielding and bell 1997 brodersen et al 2010 however as each metric assesses a different component of map error and the suitability of different metrics will often depend on the specific research question being asked therefore it is generally agreed that multiple accuracy measures provide a clearer summary of map accuracy and these should be accompanied with confidence limits foody 2002 liu et al 2007 despite these recommendations accuracy assessments are still often oversimplified looking for the map with the highest accuracy rather than considering the importance of the error furthermore the issue of providing spatially explicit error assessments is all too commonly ignored whilst values measuring accuracy can enable comparison between maps they represent the global average accuracy of the map or a class revealing nothing about the spatial distribution of error if errors are concentrated in certain areas then the global averages may not be appropriate for large sections of the study site liu et al 2004 comber et al 2012 for example transitional zones between habitats mean that map error or uncertainty may be concentrated near class boundaries particularly when using hard classifications foody et al 1992 lucieer and lucieer 2009 rocchini et al 2013 several approaches for providing spatially explicit accuracy assessments have been developed which convey this information in an informative way kyriakidis and dungan 2001 applied geostatistics to model variation in accuracy across the study site foody 2005 used locally constrained error matrices which were then spatially interpolated to produce a map of accuracy for the study site this concept was then developed further by comber et al 2012 who applied geographically weighted regression so that points closer to the area of interest were weighted higher in addition to interpolating accuracy across the spatial domain khatami et al 2017 showed that the spectral domain or the predictive data domain for models that do not utilise multiple imagery bands can also produce maps of accuracy which may support map interpretation however these methods are data intensive so in many cases the collection of sufficient samples are not feasible strahler et al 2006 for example these four studies used a minimum of 150 validation points to develop a spatially constrained accuracy measure this far exceeds what is available for many studies particularly where time and budgetary constraints prevent post classification sampling steele 2005 for instance when collecting data in the marine environment to bridge the gap between current recommendations for accuracy assessment and what is often currently applied we present a tool for supervised thematic mapping in the form of an r script bootstrap aggregation also known as bagging breiman 1996 is incorporated into the methodology to produce more robust predictions and assessments of map accuracy whilst bagging has been shown to improve the robustness of predictions steele et al 2003 carreiras et al 2006 additional information can also be extracted from the individual models to spatially map the confidence of the prediction and produce more robust accuracy assessments by bringing these methods together in a simple to apply methodology we improve the informative value of habitat maps the code to apply these methods in r is provided in appendix s1 to provide a simple means of applying these methods more widely to improve how accuracy assessments are performed in other studies to illustrate the application and versatility of the proposed r tool and show the informative value of the model outputs two case studies using different data types are presented 2 material and methods 2 1 overview of modelling process a generalised overview of the proposed methodology is presented in fig 1 the methodology is semi automated with several user defined components that may vary depending on the data and objectives of the study see appendix s1 for accompanying r script the methodology starts with the user selecting the most appropriate unit of analysis i e pixel based or image object based step 1 the groundtruth observations are then divided multiple times with replacement to produce a user defined number of randomly selected testing and training data sets from the groundtruth observations step 2 each individual dataset will form one bootstrap iteration that will later be aggregated to produce the final map and accompanying accuracy assessment a classification model is then created for each randomly selected training dataset step 3 predictions from each classification model are cross validated against the corresponding independent testing dataset from this an error matrix is populated for each model and accuracy statistics can be calculated step 4 average and standard deviations of the accuracy statistics are calculated from the iterations each classification model is used to generate a spatial prediction for the area of interest these predictions are then aggregated with the final class for each unit of analysis predicted based on a plurality vote step 5 the spatial measures of predictive confidence are calculated by aggregating mapped outputs of each iteration step 1 unit of analysis pixels continue to be the predominant unit of analysis for many researchers however geographic object based image analysis geobia has several advantages over traditional pixel based thematic mapping approaches for instance i partitioning an image into objects is akin to the way humans conceptually organise the landscape seascape to comprehend it ii using image objects instead of pixels as basic units is less computationally intensive iii image objects exhibit useful features e g shape texture contextual relationships with neighbouring objects that pixels lack iv image objects are easily integrated into vector gis hay and castilla 2006 2008 geobia is widely used in terrestrial remote sensing applications blaschke 2010 blaschke et al 2014 but its use for benthic habitat mapping is relatively new lucieer 2008 lucieer and lamarche 2011 zhang et al 2013 diesing et al 2014 wahidin et al 2015 a segmentation algorithm is used to subdivide the image into image objects discrete regions of a digital image that are internally coherent and different from their surroundings castilla and hay 2008 as both units of analysis are regularly used the methodology has been developed to implement either an object based or pixel based approach step 2 bootstrap aggregation due to financial and logistic constraints it is rare to have truly independent test data particularly in the marine environment where a suitably large and independent reference dataset cannot be collected data partitioning is regularly used to create training and testing datasets as subsets from the original dataset however a single split of data into training and test sets might result in a large variance in estimates of accuracy lyons et al 2018 using less training observations to inform the model can influence model fit or create misleading accuracy measures where test observations are limited fielding and bell 1997 steele 2005 the bootstrap aggregation or bagging method is increasingly used to improve classification accuracy and provide more reliable accuracy assessments this is of increased benefit where predictions are unstable breiman 1996 steele et al 2003 carreiras et al 2006 bagging involves creating multiple training and corresponding testing datasets by randomly resampling the reference data with replacement a separate predictive map is then generated from each dataset with the remaining data used to perform an accuracy assessment the proportion of groundtruth observations used for model training and testing and the number of cross validation datasets created is dependent on a range of factors including total number of observations per class complexity of the system variations observed between individual models and computational power available in addition class frequency among groundtruth observations is rarely balanced therefore the script has been developed to subsample into testing and training data using stratified random sampling with proportional allocation based on class type olofsson et al 2014 this produces training and testing samples with approximately the same class frequency as the original dataset in both case studies the groundtruth data are split with 70 used for model training and 30 used to test the prediction although this can be adjusted by the operator this is repeated for an operator defined number of bootstrap replicates twenty five replicates are recommended to optimise performance breiman 1996 while 10 replicates would generally be the minimum step 3 machine learning algorithm this study presents the use of the predictive model random forest rf breiman 2001 while rf models are presented in this study it should be noted that the mapping tool presented here applies the predictive models using the r package caret this allows flexibility of model selection from several classifier based algorithms which produce predictions as both a class and probability format some basic classification methods are presented in the accompanying code e g generalised linear models k nearest neighbours or conditional inference trees but we stress that model selection is ultimately the decision of the operator rf models were selected for the case studies as they are a commonly used classification model which can be applied without extensive tuning and has shown high predictive accuracy in a number of domains prasad et al 2006 mutanga et al 2012 oliveira et al 2012 rodriguez galiano et al 2012 zhi et al 2014 turner et al 2018 step 4 accuracy assessment map validation is performed using the test dataset that has been withheld from model training for each bootstrapped sample using these test datasets an error matrix is populated for each individual map performance metrics and the final error matrix present the mean accuracy across the bootstrapped samples with confidence intervals representing the variation between samples in this study specific and global accuracy statistics for sensitivity specificity and balanced accuracy plus global accuracy statistics of overall accuracy and cohen s kappa coefficient cohen 1960 and class specific accuracy measures of user s and producer s accuracy are automatically calculated step 5 prediction and spatially explicit confidence maps the final prediction is determined as an ensemble map based on plurality vote from the individual maps i e first past the post this method is known as monte carlo cross validation it is also possible to extract the proportional frequency of the most common class for each mapping unit to improve our understanding of how stable the prediction is based on different groundtruth observation data saatchi et al 2007 for example where all models predicted one habitat this would have a maximum confidence and indicate high stability between models in addition along with assigning each individual mapping unit to a class for several commonly used models within the caret package such as generalised linear and additive models rf and conditional inference trees it is also possible to extract the probability of membership to that class this value is an estimate of the probability of belonging to a class and is a feature of these models the probability of each class within the individual models can be used as an indication of the fit of the classification algorithm in separating the classes carreiras et al 2006 by extracting the average probability from the bootstrapped replicates for each unit of analysis a map of probability of class membership can be generated indicating the quality of model fit across the study site finally combined confidence is computed by multiplying the frequency of the most common class and the average probability of the most common class for each mapping unit this final confidence map presents a simple representation of confidence for the study site that incorporates an indication of model fit and model stability in one graphic allowing the identification of high and low confidence across the study site while not directly mapping accuracy these measures have been used to map the relative confidence of the methodology indicating regions of uncertainty and can provide valuable information to support map interpretation carpenter et al 1999 saatchi et al 2007 ahsan et al 2010 diesing and stephens 2015 2 2 case study 1 pixel based 2 2 1 study area the runswick bay marine conservation zone mcz is in the north sea on the yorkshire coast united kingdom uk fig 2 the site extends approximately 7 km to its seaward boundary covering a total area of 68 km2 and reaching water depths of up to 50 m the site comprises of areas of exposed bedrock with a mosaic of different sediment types increasingly widespread with depth 2 2 2 groundtruth observation data due to the nature of the substrate type benthic observation data used for this study were collected using a combination of drop camera surveys and sediment grab sampling drop camera sampling was performed at all sample stations to capture still images every 10 15 m over a distance 150 m where the drop camera indicated the presence of sediment a grab sample was performed still images and grab samples were classified to level three of the european nature information system eunis habitat types five benthic habitat classes were observed in the underwater still images namely sublittoral coarse sediment sublittoral sand sublittoral mud and sublittoral mixed sediments and moderate energy circalittoral rock continuous video transect data were reduced to observations at point locations by randomly selecting positions along the transect with a minimum distance of 100 m between samples or to the nearest grab sample this provided a groundtruth observation dataset of 162 observations of which 51 were sediment grabs and 112 were underwater still images table 1 and fig 2 2 2 3 acoustic data full coverage of bathymetry and backscatter intensity of the acoustic return data was acquired using a kongsberg em3002d multibeam echosounder system in 2013 bathymetry data were collected and processed in accordance with the international hydrographic organisation standards for hydrographic surveys order 1 special publication 44 edition 4 iho 2008 using the software caris hips and sips version 7 1 1 the software package qps fm geocoder toolkit was used to produce fully compensated and corrected backscatter mosaics the bathymetry and backscatter data were initially resampled to the same grid at 5 m pixel resolution to simplify the processing requirements for analysis a 2d fourier filter wilken et al 2012 was applied to the backscatter to reduce apparent stripe noise to further characterise the various topographic attributes of the seafloor six secondary derived features recommended in lecours et al 2017 were produced from the bathymetry data the mean bathymetry and standard deviation of the bathymetry were subsequently discarded due to having a high correlation 0 7 with other variables appendix s2 no derivatives of backscatter were assessed in this study 2 2 4 model algorithm parameters the number of cross validation runs was set to 25 each with a split of 70 training 30 testing data 2 3 case study 2 image object based 2 3 1 study area two tree island study site is in the outer thames estuary on the coast of the uk fig 3 the upper shore is covered by a wide belt of saltmarsh which borders an intertidal mudflat the mid and upper intertidal sections of the mudflat are covered by an extensive 100 ha intertidal seagrass meadow composed of a patchwork of zostera noltii and zostera angustifolia beds 2 3 2 remotely piloted aircraft imagery a quest q pod remotely piloted aircraft rpa was flown over the site during periods of low water in august and september of 2015 the rpa carried a twin payload of two 12 megapixel cameras one acquiring red green blue imagery and the other fitted with a filter to acquire imagery in the near infrared nir images were georectified using the software agisoft photoscan through a combination of aircraft position and altitude data and ground control markers images were aligned and mosaiced based on common points on the photographs to produce a fully georeferenced orthomosaic with six bands red green blue nir1 nir2 nir3 at 3 cm pixel resolution and a 10 cm resolution digital surface model dsm fig 4 additional raster layers were calculated for a selection of band ratios red green nir1 red nir2 red nir1 green nir2 green we acknowledge that other band ratios are available that may have improved the prediction xue and su 2017 however for the purpose of exploring map predictions and accuracy these band ratios were considered to be sufficient and our predicted map achieved high accuracies the 10 cm cell size dsm was used to calculate slope on a five pixel radius and a topographic position index tpi weiss 2001 with a 100 pixel radius 2 3 3 geobia segmentation the predictor layers were imported into ecognition v9 2 for image segmentation the multi resolution segmentation algorithm used in this study partitions an image into regions called objects with homogenous attributes across a user defined set of layers based on a level of allowed variability defined by the scale parameter and effect of object shape compactness and shape parameters segmentation was carried out on a combination of the red green and blue bands using the multi resolution segmentation algorithm with a scale parameter of 25 shape parameter of 0 1 and compactness of 0 5 the parameters were tuned based on visual inspection of the resulting segmentations fig 4 this provided a segmentation of the study site with approximately 1 6 million objects a selection of layer attributes including summary and textural statistics were calculated for the objects object mean 75th percentile and skewness were calculated for all input layers additionally object grey level co occurrence matrix contrast and entropy were calculated for the spectral reflectance and band ratio layers the objects with their accompanying attributes were exported from ecognition as a polygon shapefile for further analysis 2 3 4 observation data a random selection of 1000 objects were visually inspected and classified into one of four habitats mud saltmarsh 100 seagrass cover sg 100 and 100 seagrass cover sg s from these 100 observations of each class were randomly subsampled to produce a groundtruth dataset of 400 objects fig 3 2 3 5 model algorithm parameters the number of cross validation runs was set to 10 each with a split of 70 training 30 testing data 3 results 3 1 case study 1 pixel based the habitat map for runswick bay mcz fig 5 a depicts moderate energy circalittoral rock nearshore with a channel of predominantly sublittoral sand and sublittoral mud to the north these finer sediments grade to a complex matrix of sublittoral coarse sediment and sublittoral mixed sediments further from shore in deeper waters generally this is consistent with what was observed in the groundtruth observations however the detailed accuracy assessment reveals the prediction s limitations overall accuracy whilst above the no information rate was approximately 43 fig 6 and fig 7 a more detailed examination of the error matrix to determine the predominant sources of error from the prediction suggests that sublittoral mixed sediments was the least accurately predicted class with a mean user s and producer s accuracy of 32 fig 6 this class was frequently incorrectly classed by the models as either subtidal coarse sediment or subtidal mud of the subtidal mixed sediments samples an average of 5 5 samples were incorrectly classed as subtidal coarse sediment and 2 8 as subtidal coarse sediment compared to only 4 8 that were correctly classified the best performing class was sublittoral sand which had a user s and producer s accuracy of 64 and 55 respectively fig 6 moderate energy circalittoral rock had a high user s accuracy of 62 but the producer s accuracy was the second worst of the classes at only 41 inspection of the spatially explicit confidence maps indicates that the proportional frequency of the most common class was generally high across the whole study site fig 5b however there were areas of reduced confidence visible in the average probability of the most common class map fig 5c this map showed that areas with the lowest average probability of the most common class were in the regions classified as sublittoral mud and those pixels positioned near the boundaries between two different classes such as between sublittoral coarse sediment and sublittoral mixed sediments or between sublittoral mud and sublittoral mixed sediments due to the magnitude of variation present in the average probability of the most common class fig 5c relative to the proportional frequency of the most common class fig 5b variation in the combined confidence map fig 5d was predominantly driven by the average probability of the most common class in this map we see the same general pattern observed in the average probability of most common class map with the trend augmented by the subtler differences in proportional frequency of most common class 3 2 case study 2 object based the habitat map for the two tree island fig 8 a depicts saltmarsh to the north with several drainage channels cutting through the saltmarsh which have a mud substrate these habitats transition to a region of high density seagrass coverage sg 100 in the centre of the site to the south there is a complex matrix of sg 100 sg s and mud the accuracy assessment indicated that the model was high performing with an overall accuracy of 90 fig 9 and fig 10 as the number of observations for each class were equal this is also true for the balanced accuracy which is the average of the accuracy of each class brodersen et al 2010 class specific accuracy suggests that the greatest source of error related to sg s fig 9 which had a mean user s accuracy of 85 and producer s accuracy of 91 equivalent to a sensitivity of 0 85 fig 10 the error matrix showed that of the 10 sg s test observations for each replicate model an average of 1 7 observations were falsely classified as mud and 2 8 falsely classified as sg 100 by the models comparison of the spatially explicit confidence maps fig 8b d was consistent with the error matrix the proportional frequency of the most common class was high across the study site indicating that there was little variability in predictions from the individual bootstrapped models fig 8b however confidence of the average probability of the most common class was more variable across the site fig 8c for example confidence was lowest in areas of transitional habitat at the border of the saltmarsh where mud sg s and sg 100 are all present there was also reduced confidence in the two tidal channels to the south of the site which were classified as mud and sg s fig 8c 4 discussion assessment of thematic map accuracy is regularly oversimplified failing to address the four types of error that may be present within a map here we have presented a robust methodology which follows recommended practices to assess the suitability of a map foody 2002 liu et al 2007 diesing et al 2016 in addition by incorporating bootstrap aggregation into the modelling process this methodology is well suited to data limited environments where withholding groundtruth observations for the accuracy assessment would sacrifice model precision steele 2005 for example a total of 162 groundtruth observations were available for the runswick bay mcz with which to inform the habitat model and assess the model accuracy by repeatedly subsetting the data with replacement each replicate model is validated against an independent dataset thus the random bias introduced by any one split of the data is reduced by creating an ensemble of all the models carreiras et al 2006 falkowski et al 2009 a detailed understanding of accuracy supports the use of one map over another this is important as maps created using different methods or based on data from different sources will vary environmental managers and policy makers are then faced with the challenge of trying to determine which map to use for planning and management therefore the intention of an accuracy assessment is to determine whether a map is suitable for a particular purpose bennett et al 2013 van vliet et al 2016 meaning comparing different maps overall accuracies may be of limited value fielding and bell 1997 for example a habitat map may still be suitable for a particular application despite a low overall accuracy in the case of the runswick bay mcz the overall accuracy of approximately 43 would generally be considered low foody 2002 resulting in the model being discarded however the error matrix and class specific accuracy measures reveal that the largest source of error relates to the models inability to separate the sublittoral mixed sediments and sublittoral coarse sediment classes so if the difference between sublittoral mixed sediments and sublittoral coarse sediment is of little concern for management then the operator may choose to merge these into a single class thereby increasing the overall accuracy similarly if the operator were predominantly concerned with identifying areas that are likely to be moderate energy circalittoral rock habitat as might be the case when managing creel fishing in the region then despite the models low overall accuracy the delineation of this habitat may allow management to be targeted in these specific areas another common application of habitat maps is to understand change through time which may be to understand the effects of a pressure to the environment or to assess the effects of management measures imposed however if the magnitude of the spatial change is of a similar magnitude to the map error then it may not be possible to determine whether any change has occurred in the two tree island example the model would generally be considered high performing 90 overall map accuracy yet it is notable that the mean producer s accuracy of the sg s class is less than the other three classes 85 compared to 95 for the other classes this makes sense as sg s is the intermediate habitat between complete seagrass coverage sg 100 and mud therefore errors are likely to occur where these classes are most similar so if the intention were to monitor changes to the extent of seagrass it may require a substantial change in sg 100 and sg s to conclude that an observed change was not just model variation however observed changes to the saltmarsh class could be interpreted with greater confidence due to this class s high accuracy these examples highlight how thematic map use is supported by understanding the sources of error which can be achieved through a detailed assessment of accuracy by calculating multiple metrics and class specific measures in an accuracy assessment the proposed method provides the necessary information for map users to make an informed decision regarding map interpretation the spatially explicit confidence maps are valuable for understanding both the sources of error and considering the suitability of a map in addition understanding spatial confidence allows targeted management measures to be implemented within the study site such as applying different levels of protection based on the confidence that a feature is present or identifying areas of low confidence which need to be targeted during additional surveys in the two tree case study there is a distinct band of reduced confidence in the transition zone between sg s and saltmarsh visible in the average probability of the most common class fig 8c this is unsurprising as map error is commonly concentrated at the boundaries between classes which have similar environmental characteristics steele et al 1998 foody 2002 nevertheless this map provides a clear representation of areas where the model is uncertain indicating that managers should use caution when interpreting the exact boundaries of classes in this region reduced confidence is also present in the tidal channels in the south east corner of the study site this may relate to the presence of a thin film of diatoms covering the mud that were observed in this area causing a similar spectral response to the sparsest seagrass cover lack of in situ groundtruth locations for the diatom film and the inability to identify it in images with acceptable confidence led to its inclusion in the mud class in this analysis however this could potentially be improved with groundtruth observations for a diatom film class spatial confidence in the runswick bay mcz case study is also predominantly driven by the average probability not frequency of the most common class fig 5c this supports what was observed in the error matrix i e that it is likely the classes used were unsuitable to be mapped using the available predictive layers as map errors are predominantly associated with the classes of sublittoral mud sublittoral coarse and sublittoral mixed sediments even though this would be expected based on the error matrix on its own the map makes this clearer for the end user who may make decisions based on the distribution of certain classes 4 1 limitations the maps of spatial confidence provide an indication of uncertainty within the model and not necessarily the distribution of map accuracy the confidence measures shown in the map outputs relate to the model fit and stability bootstrap repeatability for the specific predictor variable values occurring at each pixel or object independently of the neighbouring pixels and objects consequently issues relating to spatial bias are not addressed for methods of classifying map accuracy across a study site we recommend those discussed in khatami et al 2017 however accuracy interpolation methods for the spatial and spectral domains have so far only been applied to large datasets for example 548 test pixels were used in khatami et al 2017 as this is beyond the number of samples available for many studies the application of these methods is currently limited this paper presents a range of accuracy statistics while many accuracy statistics have been criticised liu et al 2007 pontius and millones 2011 bradley et al 2016 we provide a range of commonly used statistics to give researchers the ability to interpret a specific problem using the method that is most appropriate for the specific question at hand further developments of the tool may include new accuracy metrics such as those described in pontius and millones 2011 4 2 implications and suggestions for future research this paper presents the use of a semi automated mapping methodology utilising bootstrap aggregation in a simple to apply r script appendix s1 central to this methodology is that it adheres to current recommended practices for accuracy assessments and makes full use of available data especially where groundtruth observations may be limited this methodology is applied to both pixels and image objects as units of analysis to display the flexibility and suitability of this approach to understand the sources of map error and the suitability of the final prediction for a range of applications while the methods presented in this paper are not new the script allows an easy application to other studies generating a map is often an iterative process examining the various sources of uncertainty within the data and methodologies applied to optimise the use of the available data therefore even where this is not used to produce the final prediction it has the potential to provide valuable insights into potential sources of error within the model predictive mapping such as these have been applied to a wide range of applications such as cropland mapping vogels et al 2017 predicting wildfire susceptibility leuenberger et al 2018 and economic valuations of ecosystem services foody 2015 and understanding the uncertainty in these models can support map interpretation in addition due to the rigour of the accuracy assessment this tool could provide an excellent base with which to examine other sources of uncertainty such as positioning error choice of classification method and others software availability the complete and well commented code for cross validation and spatial variability is applied in the software r and is available at https github com pmitchell cefas mitchell et al ems appendix s1 model scripts acknowledgements the 2015 rpa data collection at two tree island was funded by cefas as part of the cefas environment agency partnership agreement we would like to thank the cefas staff members who collected and processed data within the two tree island study site we thank defra and the mpa programme partner organisations for allowing the data from the runswick bay mcz to be used in this study and crew and scientific staff who were involved in the collection and processing of the data the funding for writing this paper also comes from the defra funded mpa programme we thank dr alexander callaway cefas for valuable comments on an earlier draft of this manuscript appendix a supplementary data the following are the supplementary data related to this article supplementary data 1 supplementary data 1 supplementary data 2 supplementary data 2 appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 07 014 
26349,the use of physics based wave propagation predictions requires a considerable time commitment a high level of expertise and extensive climate and reef data that are not always available when undertaking planning for management of coasts and coral reef ecosystems bayesian belief networks bbns have at least three attributes that make them an excellent choice to communicate physics based wave model predictions first bbns subsume thousands of predictions to provide probabilistic outcomes second by using prior probabilities a practitioner can still obtain predictions of wave outcomes even when their knowledge of input parameters is incomplete third bbns can propagate evidence from outputs to inputs which can be used to identify input conditions that are most likely to deliver a chosen outcome these three attributes are tested and found to hold for a bbn developed for this purpose graphical abstract image 1 keywords bayesian belief networks swan physics based wave modelling communication end user 1 introduction wind and swell waves have been demonstrated to be physical drivers of coastlines and coral reefs barry and dayton 1991 p289 290 world bank 2016 these reefs in turn provide coastal protection services to coastal communities as they absorb wave energy either through triggering wave breaking or through drag and frictional dissipation managing these protection services under various reef stresses e g fishing tourism sea level rise ocean chemistry changes has led researchers to apply process based wave models such as baldock et al 2015 baldock et al 2014a baldock et al 2014b hoeke et al 2011 saunders et al 2014 storlazzi et al 2011 vitousek et al 2007 to name just a few using process based wave models has been justified for regions in which shallow water propagation effects are significant callaghan et al 2015 with callaghan et al 2010 pointing out that different conclusions were possible between fetch and process based wave models callaghan et al 2010 found process based model results agree with field observations it should be noted that the protection provided by coral reefs from waves is often overstated for example waves were able to propagate across a coral reef at niue and then up a 22 m vertical cliff to remove all the buildings in alofi callaghan et al 2006 this coral shelf at least for everyday waves dissipates ocean waves well before the cliff that is wave breaking operates differently at this site for a significant wave height of 2 m compared to 15 m nevertheless protection services are provided by coral reefs under non extreme and potentially some extreme conditions physics based wave model applications for coastal protection services have the potential to help inform coastal and coral reef management in regions in which detail wave modelling is at this stage unavailable for example baldock et al 2014a took results from one dimensional physics based wave model built with simulating waves nearshore swan booij et al 1999 holthuijsen 2007 ris et al 1999 it demonstrated impacts on wave height near bed velocities and forces onto corals as the sea level rises these impacts were assessed for significant wave heights of 0 5 m average climate and 3 m cyclonic conditions while coastal managers are able to access this journal article and use figures contained therein it remains deterministic and at a particular level of forcing i e average or cyclonic conditions for example if you know reef width and roughness but not depth and offshore wave forcing using it for planning becomes difficult as uncertainties are large while there are various reports containing more extensive sets of predictions than journal articles considerable time commitment a high level of expertise and extensive climate and coral reef data are required to take advantage of them the underlying physics based wave model prediction database could be released however using such a database also requires high levels of expertise in informatics results are in 540 netcdf files totalling 17 gb containing 220 320 simulations with ca 20 billion predictions assuming this expertise is available there remains missing data as previously discussed it is potentially possible to overcome these barriers with bayesian belief networks in addition to generating other results using the bbn model developed bayesian belief networks have three key features for communicating the wave conditions within a database of predictions developed using a physics based wave model brevity data reduction a probabilistic framework and propagating evidence from outputs to inputs these features are discussed further brevity in communication is achieved through the conversion of physics based wave model simulations into probabilistic outcomes that are easily accessible through a simple graphical user interface this allows the user to access the results of potentially hundreds of hours of high performance computing instantaneously missing data are handled by using other evidence judgment or assuming a uniform distribution when nothing is known consequently variables dependent on variables with missing data will include more uncertainty end users can propagate information through a bayesian belief network either from inputs to outputs or outputs to inputs propagating from output to input allows end users to highlight key input or inputs that lead to a particular outcome e g under which circumstances is it most likely to maintain low wave heights given a moderately fast rate of sea level rise while applying bayesian belief networks to outputs of a physics based wave model prediction database may be new their use in coral reefs is extensive for example ban et al 2014 franco et al 2016 and shenton et al 2010 modelled coral reef interactions with tropical cyclones climate forcing and catchment run off and renken and mumby 2009 krug et al 2013 and gilby et al 2016 for non physical stressors of coral reefs prez miana 2016 indicated it is suitable in ecosystem services modelling with specific recommendations by kuhnert and hayes 2009 the research question addressed is can a bayesian belief network be used to estimate probabilities similar to those estimated from probability theory and a physics based wave model prediction database if so then using the bayesian belief networks as a means to communicate these wave predictions may well be helpful to coastal and coral reef planners in developing countries non expert end users this article provides background information on bayesian belief networks key details of the physics based wave prediction database from baldock et al 2014a and details of the bayesian belief network structure built section 2 section 3 assesses training algorithms by comparing probability predictions with those calculated using wave prediction database for beach toe significant wave height distribution predictive inference and diagnostic inference section 4 discusses using bayesian belief network probabilities estimations either predictive inference and diagnostic inference and how these are beneficial for coral reef and coastal planners in developing countries this article finishes with a brief summary of findings section 5 2 methods bayesian belief networks enable users to model the body of knowledge in a given area by mapping cause and effect relations among key variables it has two components qualitative component a directed acyclic graph dag and a quantitative component priors for nodes with no parents and conditional distributions for nodes with parents the dag corresponds to the factorization of the joint probability distribution over the set of all the variables kim and pearl 1983 pearl 1988 pp42 71 116 133 russell and norvig 2003 pp462 519 the graphical structure captures cause and effect knowledge or assumptions and forms a probabilistic description that quantifies relationships between variables in the network in this graphical depiction variables are symbolised by nodes a b g fig 1 and dependence between two variables by an arrow pointing from the cause variable to the effect variable for the probabilistic description each node in a bbn s dag represents the probability distribution of a set of mutually exclusive outcomes variables with no parents are described using prior probabilities the probability distributions of the remaining variables are estimated using conditional probabilities and applying the bayes theorem fig 1 variables within a structure include but are not limited to discrete or categorical continuous discretised continuous or ranked a discrete and discretised continuous variable have an exhaustive and mutually exclusive set of outcomes or states the marginal and conditional probability distributions represented by a graphical structure can be estimated using a range of sources for example expert experience laboratory experimental measurements or field measurements in this article we assign marginal and conditional probabilities using the wave propagation model output from baldock et al 2014a referred to as the wave prediction database baldock et al 2014a applied a physics based wave model for water waves between 2 and 20 s period propagating over an idealised barrier reef lagoon system the physics based wave model that baldock et al 2014a applied was swan standing for simulating waves nearshore swan has previously been applied to simulate wave propagation over a range of coral reefs hamylton et al 2013 ortiz et al 2016 saunders et al 2014 storlazzi et al 2011 vitousek et al 2007 it has been extensively tested by ris et al 1999 in a wide variety of nearshore environments and further extended for use in shallow water environments by van der westhuysen 2010 for a comprehensive treatment on wind and swell wave generation propagation and dissipation physical processes and modelling using swan see holthuijsen 2007 baldock et al 2014a argue that insights on sea level rise impacts on coral reef ecosystem hydrodynamics are possible by modelling an idealised one dimensional system fig 2 and table 1 based on reef geometries that are representative of reefs worldwide including those within the great barrier reef gbr baldock et al 2014a varied the geometry and forcing from wind and swell waves table 1 as follows smooth or rough reefs reef top depths between 0 5 m and 3 m reef top widths between 50 m and 1200 m lagoon depths between 5 m and 20 m and lagoon lengths between 50 m and 2000 m they varied the forcing with mean water level rise between zero and 1 m onshore wind speed between zero and 20 m s gale on the beaufort scale and offshore significant wave height between 0 3 m and 3 m table 1 the physics based wave model swan was run for each combination of these geometry and forcing variables leading to 103 680 runs these swan runs include the physical process of wave shoaling mean water level setup wave setup wave generation by wind and wave dissipation by depth limited wave breaking white capping drag on corals and bottom friction swan outputs waves across the reef ecosystem and we are focused on beach toe significant wave height in this article combining geometry forcing and beach toe significant wave height variables comprise the wave prediction database all variables of the wave prediction database table 1 are for continuous quantities and were discretised table 1 as follows for marginal distributions first eight variables in table 1 discretisation was done around the database values of each variable with one value in each discrete range leading to uniform marginal distributions when probabilities are estimated from this database this approach maximises database usefulness in the bbn training with one database record for every combination of discretised input variables site specific information can be included post training as demonstrated in the discussion section the beach toe significant wave height was discretised into five states suitable for coastal and reef planning and have approximately equal probabilities when trained using the wave prediction database developed by baldock et al 2014a the range for each state was selected centrally between database values additionally for the lower and upper limits of the database variables an additional amount was included to indicate what is reasonable related to the database value for example for the lowest offshore wave height in the database of 0 3 m it is reasonable to suggest this is representative of wave heights from 0 25 m to 0 4 halfway to the next database value of 0 5 m these additional amounts will potentially vary between different wave modellers as it is a subjective assessment a bayesian belief network was structured to link all geometry and forcing variables directly to a single outcome of beach toe significant wave height fig 3 for the eight marginal distributions variables labelled input in table 1 or variables that have no incoming arrows in fig 3 and their adopted discretisation table 1 this lead to 8 2 4 6 6 3 5 3 103 680 entries in the conditional probability table for beach toe significant wave height the wave prediction database has one beach toe significant wave height outcome for each entry of this conditional probability table a case file was prepared with continuous variables except for reef roughness that was discretised manually into smooth or rough consequently the bayesian belief network nodes within netica are type nature and continuous for all variables except reef roughness which is discrete all continuous nodes used discretization and there are no equations or delays or any other changes to netica default property values all marginal and one conditional probability tables were populated using counting expectation maximization and gradient descent learning algorithms russell and norvig 2003 these training algorithms were applied with uniform prior marginal and conditional distributions and the degree weighting used by netica was increased until learnt marginal and condition probability tables were no longer dependent on its value the bayesian belief network estimated probabilities were then assessed by comparing them to probabilities estimated using probability theory and the wave prediction database these probabilities are determined by counting the relevant number of cases contained in the wave prediction database for example the conditional probability of the beach toe significant wave height being between 0 6 m and 0 8 m given offshore significant wave height being between 1 125 m and 1 375 m involves 12 960 database entities of which 2360 cases have beach toe significant wave heights between 0 6 m and 0 8 m leading to a probability of 2360 12960 or 18 2 for the outcome variable distribution each discrete range probability was obtained by counting cases within each range and then dividing through by the total number of cases for example there are 19 587 cases in the discretised range 0 0 4 m for beach toe significant wave height and the total number of cases is 103 680 leading to a probability of 19587 103680 or 18 9 3 results a bayesian belief network that takes reef wave and wind parameters table 1 and links to beach toe significant wave height fig 4 was trained using the wave prediction databased developed by baldock et al 2014a to ensure the beach toe significant wave height conditional probability table was independent of degree weighting degree weighting was increased until each probability within this table was stable to six significant figures the counting learning algorithm required a degree weighting to be greater than or equal to 106 with learning taking less than 2 s training using expectation maximization required degree weighting greater than or equal to 10 and took ca 10 min training using gradient descent learning algorithm required degree weighting greater than or equal to 2 and took ca an hour the conditional probabilities for beach toe significant wave height obtained by counting and expectation maximization learning algorithms are indistinguishable to six significant figures see electronic files and their posterior probabilities is also indistinguishable there are minor differences between either counting or expectation maximization learning algorithms or gradient descent learning algorithm see electronic files and fig 4 the posterior probabilities for beach toe significant wave height table 2 match to three significant figures between counting and expectation maximization and probabilities estimated through probability theory and the wave prediction database while there are minor differences in models resulting from these two learning algorithms and gradient descent learning algorithm a comparison between the probability distributions derived from the results of the bayesian belief network and the wave prediction database was used to assess the effectiveness of the different learning algorithms using the beach toe significant wave height as the output variable of interest this was done as follows i for a particular variable the bayesian belief network was instantiated for each discrete range for the continuous variables or each input for the discrete variables in turn with all other input variables remaining uniformly distributed for example fig 5 left panel shows an instantiation of the offshore significant wave height set to between 0 25 and 0 4 m with all other input variables uniformly distributed this procedure was repeated for all eight input variables leading to 37 different predictions of the beach toe significant wave height distribution from the bayesian belief network with one example shown in the left panel of fig 5 ii these predicted distributions from the bayesian belief network were then compared to those determined directly from the wave prediction database fig 6 fig 6 is constructed as follows from left to right the panels in fig 6 correspond to the distributions derived from the wave prediction database and then the predictions from the bayesian belief network trained using counting expectation maximization and gradient descent learning algorithms respectively the rows of panels show results obtained in step i for all the different input variables which are from top to bottom row offshore significant wave height reef roughness reef top depth reef top width sea level rise onshore wind speed lagoon length and lagoon depth in each of the 32 panels there are five groups of bars with one group for each discrete range of the predicted beach toe significant wave height 0 0 4 m 0 4 0 6 m 0 6 0 8 m 0 8 1 1 m and 1 1 2 4 m within each group of bars the coloured bars show the probabilities for each of the discrete ranges for the variable associated with that row as identified in the panel title the coloured bars are arranged in ascending order for the discrete ranges for that variable for example for the bottom row of panels there are three bars in each group corresponding to the discrete ranges for the lagoon depth of 2 5 7 5 m 7 5 15 m and 15 25 m respectively since the number of discrete ranges changes between variables the number of bars in each group also varies fig 6 shows that the bayesian belief network predicted beach toe significant wave height distributions have nearly identical shapes indicated by the patterns of the bars and magnitudes to those obtained directly from the wave prediction database this demonstrates that the probabilities estimated by the bayesian belief network are qualitatively and quantitatively consistent with those from the wave prediction database a quantitative comparison of the probabilities shown in fig 6 is provided in fig 7 which shows that the predicted probabilities are nearly identical for the counting and expectation maximization learning algorithms standard error of 0 003 and in near perfect agreement with probabilities derived from the wave prediction database while not identical to the probabilities derived from the wave prediction database the gradient descent learning algorithm estimates are still mostly within 5 fig 7 of the true values with a standard error of 1 1 a tornado diagram fig 8 based on the expected beach toe significant wave height indicates reef top depth and offshore significant wave height as the most influential variables and reef roughness being the least influential these sensitivity ranking are invariant between wave prediction database and the bayesian belief network trained with any of the learning algorithms used we now focus on options for end users of the bayesian belief network and in particular testing their instantiation of a bayesian network for diagnostic inference fig 5 right panel which we describe as reversibility or back propagation from beach toe wave height to all other variables e g fig 9 conditions leading to beach toe significant wave heights between 1 1 m and 2 4 m probabilities derived from the wave prediction database fig 10 blue bars calculated for beach toe significant wave heights between 1 1 m and 2 4 m are near identical to bayesian belief network estimates when trained using counting and expectation maximization learning algorithms for gradient descent learning algorithm minor differences exist fig 10 yellow bars has minor differences to all other bars 4 discussion the bayesian belief network tested had marginal and conditional probability tables populated trained using three learning algorithms counting expectation maximization and gradient descent in all cases the training set used corresponds to the baldock et al 2014a physics based wave model prediction database when comparing the bayesian belief network conditional distributions to the wave prediction database derived conditional probabilities the bayesian belief network was best trained using either counting or expectation maximization fig 7 with the counting algorithm taking a few seconds the non expert end users are coastal and coral reef planners in developing countries the communication of technical results via a bayesian belief network is favoured over other alternatives for several reasons first in many cases planners lack the technical capacity and budget to undertake bespoke wave modelling and a simple interface allows practitioners to harness information affordably second much of the information held in the bayesian belief network is available in the primary literature in this case in the publications by baldock et al this introduces several barriers to communication and uptake not only will these articles necessarily include technical jargon they will often be in a user s second or subsequent language thus an alternative that combines a bayesian belief network with clear guidelines on its use and limitations of its application has greater traction further using predictions directly from journal article figures requires manual scaling which while possible becomes difficult when operating with more than the four input variables that baldock et al 2014a plotted inputs being reef depth width sea level rise and smooth rough reefs and their output variable being wave height e g their fig 3 page 160 the wave prediction database baldock et al 2014a developed has eight input variables perhaps more significantly by being able to modify marginal distributions all eight input variables the bayesian belief network allows users to access model results even if input variable information is incomplete unknown variables can be assigned an assumed distribution left as uninformative or instantiated communicating physics based wave model results to non expert end users by means of a bayesian belief network requires those users to have an understanding of the bayesian belief network variables a knowledge of bayesian belief network limitations and coastal process knowledge relating to wave propagation across reef ecosystems but not the technical details of wave model physics for the physics based wave model used here that is approximately one hundred thousand lines of fortran code non expert end users having this system understanding are able to interact with the bayesian belief network dynamically e g through a world wide web based bayesian belief network application and consequently are able to benefit from their three attributes of subsuming large results sets introducing probabilities here in terms of offshore wave height prior distribution fig 4 and reversibility figs 9 and 10 bayesian belief networks are able to manage uncertainties from lack of knowledge consider a situation where coastal management is limited to information from the world wide web aerial images local knowledge e g coral type and reef depth ranges and wave conditions as long as the ranges of the variables fall within the values of the network nodes the bayesian belief network can still provide beach toe significant wave height fig 11 as the sea level rises as a distribution of wave heights the coastal planner then has an indication of how nearshore waves will change under sea level rise if that indication change requires refinement the sensitivity analysis fig 8 provides guidance to end users about input variable influence that the end user can use to prioritise their search for additional information 5 summary coral reef planners in developing countries non expert end users have difficulties in accessing the wave prediction database previously reported in baldock et al 2014a because of knowledge and data limitations bayesian belief networks have been demonstrated to be able to match probabilities derived from a physics based wave model prediction database and consequently constitute a useful tool enabling their communication that is extensive computing and software knowledge required to access result files are replaced with bayesian belief networks built by experts that while requiring training are more accessible to end users three key attributes of bayesian belief network that facilitate this communication were confirmed in that they subsume thousands of physics based wave model simulations to provide probabilistic outcomes overcame limited knowledge of input parameters through providing uniform or assumed marginal distributions e g fig 4 and back propagation from output to input e g figs 9 and 10 as good as that obtained using probability theory and the wave prediction database software physics based wave model predictions using swan 40 85 www swan tudelft nl bayesian belief network calculations used netica 5 18 www norsys com acknowledgments this work was funded by the world bank gef the university of queensland australia project entitled capturing coral reef ecosystems services ccres the global change institute the university of queensland australia and australian research council grant dp dp14010130 the high performance computing was supported by queensland cyber infrastructure foundation australia and the university of queensland australia b s acknowledges the the university of queensland grant rm 2014001465 co funded by global change institute the university of queensland and the university of queensland research and innovation australia appendix a supplementary data the following is the supplementary data related to this article online data online data appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 07 021 
26349,the use of physics based wave propagation predictions requires a considerable time commitment a high level of expertise and extensive climate and reef data that are not always available when undertaking planning for management of coasts and coral reef ecosystems bayesian belief networks bbns have at least three attributes that make them an excellent choice to communicate physics based wave model predictions first bbns subsume thousands of predictions to provide probabilistic outcomes second by using prior probabilities a practitioner can still obtain predictions of wave outcomes even when their knowledge of input parameters is incomplete third bbns can propagate evidence from outputs to inputs which can be used to identify input conditions that are most likely to deliver a chosen outcome these three attributes are tested and found to hold for a bbn developed for this purpose graphical abstract image 1 keywords bayesian belief networks swan physics based wave modelling communication end user 1 introduction wind and swell waves have been demonstrated to be physical drivers of coastlines and coral reefs barry and dayton 1991 p289 290 world bank 2016 these reefs in turn provide coastal protection services to coastal communities as they absorb wave energy either through triggering wave breaking or through drag and frictional dissipation managing these protection services under various reef stresses e g fishing tourism sea level rise ocean chemistry changes has led researchers to apply process based wave models such as baldock et al 2015 baldock et al 2014a baldock et al 2014b hoeke et al 2011 saunders et al 2014 storlazzi et al 2011 vitousek et al 2007 to name just a few using process based wave models has been justified for regions in which shallow water propagation effects are significant callaghan et al 2015 with callaghan et al 2010 pointing out that different conclusions were possible between fetch and process based wave models callaghan et al 2010 found process based model results agree with field observations it should be noted that the protection provided by coral reefs from waves is often overstated for example waves were able to propagate across a coral reef at niue and then up a 22 m vertical cliff to remove all the buildings in alofi callaghan et al 2006 this coral shelf at least for everyday waves dissipates ocean waves well before the cliff that is wave breaking operates differently at this site for a significant wave height of 2 m compared to 15 m nevertheless protection services are provided by coral reefs under non extreme and potentially some extreme conditions physics based wave model applications for coastal protection services have the potential to help inform coastal and coral reef management in regions in which detail wave modelling is at this stage unavailable for example baldock et al 2014a took results from one dimensional physics based wave model built with simulating waves nearshore swan booij et al 1999 holthuijsen 2007 ris et al 1999 it demonstrated impacts on wave height near bed velocities and forces onto corals as the sea level rises these impacts were assessed for significant wave heights of 0 5 m average climate and 3 m cyclonic conditions while coastal managers are able to access this journal article and use figures contained therein it remains deterministic and at a particular level of forcing i e average or cyclonic conditions for example if you know reef width and roughness but not depth and offshore wave forcing using it for planning becomes difficult as uncertainties are large while there are various reports containing more extensive sets of predictions than journal articles considerable time commitment a high level of expertise and extensive climate and coral reef data are required to take advantage of them the underlying physics based wave model prediction database could be released however using such a database also requires high levels of expertise in informatics results are in 540 netcdf files totalling 17 gb containing 220 320 simulations with ca 20 billion predictions assuming this expertise is available there remains missing data as previously discussed it is potentially possible to overcome these barriers with bayesian belief networks in addition to generating other results using the bbn model developed bayesian belief networks have three key features for communicating the wave conditions within a database of predictions developed using a physics based wave model brevity data reduction a probabilistic framework and propagating evidence from outputs to inputs these features are discussed further brevity in communication is achieved through the conversion of physics based wave model simulations into probabilistic outcomes that are easily accessible through a simple graphical user interface this allows the user to access the results of potentially hundreds of hours of high performance computing instantaneously missing data are handled by using other evidence judgment or assuming a uniform distribution when nothing is known consequently variables dependent on variables with missing data will include more uncertainty end users can propagate information through a bayesian belief network either from inputs to outputs or outputs to inputs propagating from output to input allows end users to highlight key input or inputs that lead to a particular outcome e g under which circumstances is it most likely to maintain low wave heights given a moderately fast rate of sea level rise while applying bayesian belief networks to outputs of a physics based wave model prediction database may be new their use in coral reefs is extensive for example ban et al 2014 franco et al 2016 and shenton et al 2010 modelled coral reef interactions with tropical cyclones climate forcing and catchment run off and renken and mumby 2009 krug et al 2013 and gilby et al 2016 for non physical stressors of coral reefs prez miana 2016 indicated it is suitable in ecosystem services modelling with specific recommendations by kuhnert and hayes 2009 the research question addressed is can a bayesian belief network be used to estimate probabilities similar to those estimated from probability theory and a physics based wave model prediction database if so then using the bayesian belief networks as a means to communicate these wave predictions may well be helpful to coastal and coral reef planners in developing countries non expert end users this article provides background information on bayesian belief networks key details of the physics based wave prediction database from baldock et al 2014a and details of the bayesian belief network structure built section 2 section 3 assesses training algorithms by comparing probability predictions with those calculated using wave prediction database for beach toe significant wave height distribution predictive inference and diagnostic inference section 4 discusses using bayesian belief network probabilities estimations either predictive inference and diagnostic inference and how these are beneficial for coral reef and coastal planners in developing countries this article finishes with a brief summary of findings section 5 2 methods bayesian belief networks enable users to model the body of knowledge in a given area by mapping cause and effect relations among key variables it has two components qualitative component a directed acyclic graph dag and a quantitative component priors for nodes with no parents and conditional distributions for nodes with parents the dag corresponds to the factorization of the joint probability distribution over the set of all the variables kim and pearl 1983 pearl 1988 pp42 71 116 133 russell and norvig 2003 pp462 519 the graphical structure captures cause and effect knowledge or assumptions and forms a probabilistic description that quantifies relationships between variables in the network in this graphical depiction variables are symbolised by nodes a b g fig 1 and dependence between two variables by an arrow pointing from the cause variable to the effect variable for the probabilistic description each node in a bbn s dag represents the probability distribution of a set of mutually exclusive outcomes variables with no parents are described using prior probabilities the probability distributions of the remaining variables are estimated using conditional probabilities and applying the bayes theorem fig 1 variables within a structure include but are not limited to discrete or categorical continuous discretised continuous or ranked a discrete and discretised continuous variable have an exhaustive and mutually exclusive set of outcomes or states the marginal and conditional probability distributions represented by a graphical structure can be estimated using a range of sources for example expert experience laboratory experimental measurements or field measurements in this article we assign marginal and conditional probabilities using the wave propagation model output from baldock et al 2014a referred to as the wave prediction database baldock et al 2014a applied a physics based wave model for water waves between 2 and 20 s period propagating over an idealised barrier reef lagoon system the physics based wave model that baldock et al 2014a applied was swan standing for simulating waves nearshore swan has previously been applied to simulate wave propagation over a range of coral reefs hamylton et al 2013 ortiz et al 2016 saunders et al 2014 storlazzi et al 2011 vitousek et al 2007 it has been extensively tested by ris et al 1999 in a wide variety of nearshore environments and further extended for use in shallow water environments by van der westhuysen 2010 for a comprehensive treatment on wind and swell wave generation propagation and dissipation physical processes and modelling using swan see holthuijsen 2007 baldock et al 2014a argue that insights on sea level rise impacts on coral reef ecosystem hydrodynamics are possible by modelling an idealised one dimensional system fig 2 and table 1 based on reef geometries that are representative of reefs worldwide including those within the great barrier reef gbr baldock et al 2014a varied the geometry and forcing from wind and swell waves table 1 as follows smooth or rough reefs reef top depths between 0 5 m and 3 m reef top widths between 50 m and 1200 m lagoon depths between 5 m and 20 m and lagoon lengths between 50 m and 2000 m they varied the forcing with mean water level rise between zero and 1 m onshore wind speed between zero and 20 m s gale on the beaufort scale and offshore significant wave height between 0 3 m and 3 m table 1 the physics based wave model swan was run for each combination of these geometry and forcing variables leading to 103 680 runs these swan runs include the physical process of wave shoaling mean water level setup wave setup wave generation by wind and wave dissipation by depth limited wave breaking white capping drag on corals and bottom friction swan outputs waves across the reef ecosystem and we are focused on beach toe significant wave height in this article combining geometry forcing and beach toe significant wave height variables comprise the wave prediction database all variables of the wave prediction database table 1 are for continuous quantities and were discretised table 1 as follows for marginal distributions first eight variables in table 1 discretisation was done around the database values of each variable with one value in each discrete range leading to uniform marginal distributions when probabilities are estimated from this database this approach maximises database usefulness in the bbn training with one database record for every combination of discretised input variables site specific information can be included post training as demonstrated in the discussion section the beach toe significant wave height was discretised into five states suitable for coastal and reef planning and have approximately equal probabilities when trained using the wave prediction database developed by baldock et al 2014a the range for each state was selected centrally between database values additionally for the lower and upper limits of the database variables an additional amount was included to indicate what is reasonable related to the database value for example for the lowest offshore wave height in the database of 0 3 m it is reasonable to suggest this is representative of wave heights from 0 25 m to 0 4 halfway to the next database value of 0 5 m these additional amounts will potentially vary between different wave modellers as it is a subjective assessment a bayesian belief network was structured to link all geometry and forcing variables directly to a single outcome of beach toe significant wave height fig 3 for the eight marginal distributions variables labelled input in table 1 or variables that have no incoming arrows in fig 3 and their adopted discretisation table 1 this lead to 8 2 4 6 6 3 5 3 103 680 entries in the conditional probability table for beach toe significant wave height the wave prediction database has one beach toe significant wave height outcome for each entry of this conditional probability table a case file was prepared with continuous variables except for reef roughness that was discretised manually into smooth or rough consequently the bayesian belief network nodes within netica are type nature and continuous for all variables except reef roughness which is discrete all continuous nodes used discretization and there are no equations or delays or any other changes to netica default property values all marginal and one conditional probability tables were populated using counting expectation maximization and gradient descent learning algorithms russell and norvig 2003 these training algorithms were applied with uniform prior marginal and conditional distributions and the degree weighting used by netica was increased until learnt marginal and condition probability tables were no longer dependent on its value the bayesian belief network estimated probabilities were then assessed by comparing them to probabilities estimated using probability theory and the wave prediction database these probabilities are determined by counting the relevant number of cases contained in the wave prediction database for example the conditional probability of the beach toe significant wave height being between 0 6 m and 0 8 m given offshore significant wave height being between 1 125 m and 1 375 m involves 12 960 database entities of which 2360 cases have beach toe significant wave heights between 0 6 m and 0 8 m leading to a probability of 2360 12960 or 18 2 for the outcome variable distribution each discrete range probability was obtained by counting cases within each range and then dividing through by the total number of cases for example there are 19 587 cases in the discretised range 0 0 4 m for beach toe significant wave height and the total number of cases is 103 680 leading to a probability of 19587 103680 or 18 9 3 results a bayesian belief network that takes reef wave and wind parameters table 1 and links to beach toe significant wave height fig 4 was trained using the wave prediction databased developed by baldock et al 2014a to ensure the beach toe significant wave height conditional probability table was independent of degree weighting degree weighting was increased until each probability within this table was stable to six significant figures the counting learning algorithm required a degree weighting to be greater than or equal to 106 with learning taking less than 2 s training using expectation maximization required degree weighting greater than or equal to 10 and took ca 10 min training using gradient descent learning algorithm required degree weighting greater than or equal to 2 and took ca an hour the conditional probabilities for beach toe significant wave height obtained by counting and expectation maximization learning algorithms are indistinguishable to six significant figures see electronic files and their posterior probabilities is also indistinguishable there are minor differences between either counting or expectation maximization learning algorithms or gradient descent learning algorithm see electronic files and fig 4 the posterior probabilities for beach toe significant wave height table 2 match to three significant figures between counting and expectation maximization and probabilities estimated through probability theory and the wave prediction database while there are minor differences in models resulting from these two learning algorithms and gradient descent learning algorithm a comparison between the probability distributions derived from the results of the bayesian belief network and the wave prediction database was used to assess the effectiveness of the different learning algorithms using the beach toe significant wave height as the output variable of interest this was done as follows i for a particular variable the bayesian belief network was instantiated for each discrete range for the continuous variables or each input for the discrete variables in turn with all other input variables remaining uniformly distributed for example fig 5 left panel shows an instantiation of the offshore significant wave height set to between 0 25 and 0 4 m with all other input variables uniformly distributed this procedure was repeated for all eight input variables leading to 37 different predictions of the beach toe significant wave height distribution from the bayesian belief network with one example shown in the left panel of fig 5 ii these predicted distributions from the bayesian belief network were then compared to those determined directly from the wave prediction database fig 6 fig 6 is constructed as follows from left to right the panels in fig 6 correspond to the distributions derived from the wave prediction database and then the predictions from the bayesian belief network trained using counting expectation maximization and gradient descent learning algorithms respectively the rows of panels show results obtained in step i for all the different input variables which are from top to bottom row offshore significant wave height reef roughness reef top depth reef top width sea level rise onshore wind speed lagoon length and lagoon depth in each of the 32 panels there are five groups of bars with one group for each discrete range of the predicted beach toe significant wave height 0 0 4 m 0 4 0 6 m 0 6 0 8 m 0 8 1 1 m and 1 1 2 4 m within each group of bars the coloured bars show the probabilities for each of the discrete ranges for the variable associated with that row as identified in the panel title the coloured bars are arranged in ascending order for the discrete ranges for that variable for example for the bottom row of panels there are three bars in each group corresponding to the discrete ranges for the lagoon depth of 2 5 7 5 m 7 5 15 m and 15 25 m respectively since the number of discrete ranges changes between variables the number of bars in each group also varies fig 6 shows that the bayesian belief network predicted beach toe significant wave height distributions have nearly identical shapes indicated by the patterns of the bars and magnitudes to those obtained directly from the wave prediction database this demonstrates that the probabilities estimated by the bayesian belief network are qualitatively and quantitatively consistent with those from the wave prediction database a quantitative comparison of the probabilities shown in fig 6 is provided in fig 7 which shows that the predicted probabilities are nearly identical for the counting and expectation maximization learning algorithms standard error of 0 003 and in near perfect agreement with probabilities derived from the wave prediction database while not identical to the probabilities derived from the wave prediction database the gradient descent learning algorithm estimates are still mostly within 5 fig 7 of the true values with a standard error of 1 1 a tornado diagram fig 8 based on the expected beach toe significant wave height indicates reef top depth and offshore significant wave height as the most influential variables and reef roughness being the least influential these sensitivity ranking are invariant between wave prediction database and the bayesian belief network trained with any of the learning algorithms used we now focus on options for end users of the bayesian belief network and in particular testing their instantiation of a bayesian network for diagnostic inference fig 5 right panel which we describe as reversibility or back propagation from beach toe wave height to all other variables e g fig 9 conditions leading to beach toe significant wave heights between 1 1 m and 2 4 m probabilities derived from the wave prediction database fig 10 blue bars calculated for beach toe significant wave heights between 1 1 m and 2 4 m are near identical to bayesian belief network estimates when trained using counting and expectation maximization learning algorithms for gradient descent learning algorithm minor differences exist fig 10 yellow bars has minor differences to all other bars 4 discussion the bayesian belief network tested had marginal and conditional probability tables populated trained using three learning algorithms counting expectation maximization and gradient descent in all cases the training set used corresponds to the baldock et al 2014a physics based wave model prediction database when comparing the bayesian belief network conditional distributions to the wave prediction database derived conditional probabilities the bayesian belief network was best trained using either counting or expectation maximization fig 7 with the counting algorithm taking a few seconds the non expert end users are coastal and coral reef planners in developing countries the communication of technical results via a bayesian belief network is favoured over other alternatives for several reasons first in many cases planners lack the technical capacity and budget to undertake bespoke wave modelling and a simple interface allows practitioners to harness information affordably second much of the information held in the bayesian belief network is available in the primary literature in this case in the publications by baldock et al this introduces several barriers to communication and uptake not only will these articles necessarily include technical jargon they will often be in a user s second or subsequent language thus an alternative that combines a bayesian belief network with clear guidelines on its use and limitations of its application has greater traction further using predictions directly from journal article figures requires manual scaling which while possible becomes difficult when operating with more than the four input variables that baldock et al 2014a plotted inputs being reef depth width sea level rise and smooth rough reefs and their output variable being wave height e g their fig 3 page 160 the wave prediction database baldock et al 2014a developed has eight input variables perhaps more significantly by being able to modify marginal distributions all eight input variables the bayesian belief network allows users to access model results even if input variable information is incomplete unknown variables can be assigned an assumed distribution left as uninformative or instantiated communicating physics based wave model results to non expert end users by means of a bayesian belief network requires those users to have an understanding of the bayesian belief network variables a knowledge of bayesian belief network limitations and coastal process knowledge relating to wave propagation across reef ecosystems but not the technical details of wave model physics for the physics based wave model used here that is approximately one hundred thousand lines of fortran code non expert end users having this system understanding are able to interact with the bayesian belief network dynamically e g through a world wide web based bayesian belief network application and consequently are able to benefit from their three attributes of subsuming large results sets introducing probabilities here in terms of offshore wave height prior distribution fig 4 and reversibility figs 9 and 10 bayesian belief networks are able to manage uncertainties from lack of knowledge consider a situation where coastal management is limited to information from the world wide web aerial images local knowledge e g coral type and reef depth ranges and wave conditions as long as the ranges of the variables fall within the values of the network nodes the bayesian belief network can still provide beach toe significant wave height fig 11 as the sea level rises as a distribution of wave heights the coastal planner then has an indication of how nearshore waves will change under sea level rise if that indication change requires refinement the sensitivity analysis fig 8 provides guidance to end users about input variable influence that the end user can use to prioritise their search for additional information 5 summary coral reef planners in developing countries non expert end users have difficulties in accessing the wave prediction database previously reported in baldock et al 2014a because of knowledge and data limitations bayesian belief networks have been demonstrated to be able to match probabilities derived from a physics based wave model prediction database and consequently constitute a useful tool enabling their communication that is extensive computing and software knowledge required to access result files are replaced with bayesian belief networks built by experts that while requiring training are more accessible to end users three key attributes of bayesian belief network that facilitate this communication were confirmed in that they subsume thousands of physics based wave model simulations to provide probabilistic outcomes overcame limited knowledge of input parameters through providing uniform or assumed marginal distributions e g fig 4 and back propagation from output to input e g figs 9 and 10 as good as that obtained using probability theory and the wave prediction database software physics based wave model predictions using swan 40 85 www swan tudelft nl bayesian belief network calculations used netica 5 18 www norsys com acknowledgments this work was funded by the world bank gef the university of queensland australia project entitled capturing coral reef ecosystems services ccres the global change institute the university of queensland australia and australian research council grant dp dp14010130 the high performance computing was supported by queensland cyber infrastructure foundation australia and the university of queensland australia b s acknowledges the the university of queensland grant rm 2014001465 co funded by global change institute the university of queensland and the university of queensland research and innovation australia appendix a supplementary data the following is the supplementary data related to this article online data online data appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 07 021 
