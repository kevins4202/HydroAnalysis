index,text
26275,this work introduces epanetcpa an open source matlab toolbox for modelling the hydraulic response of water distribution systems to cyber physical attacks epanetcpa allows users to quickly design various attack scenarios and assess their impact via simulation with epanet a popular public domain model for water network analysis the toolbox offers both demand driven and pressure driven simulations enabling the users to realistically analyze cyber physical attacks and their impacts under both pressure sufficient and pressure deficient conditions epanetcpa is available under the mit license keywords water distribution systems smart water networks epanet cyber security cyber physical systems software availability name of software epanetcpa license mit developers r taormina h c douglas contact email riccardo taormina sutd edu sg riccardo taormina gmail com software required matlab epanet2 programmer s toolkit year first available 2018 available from github https github com rtaormina epanetcpa 1 introduction epanet is a popular public domain model for simulating hydraulic and water quality processes in water distribution systems rossman 2000 the model developed by the united states environmental protection agency is available as a stand alone gui software or as a dynamic link library known as the programmer s toolkit generally used to develop custom applications over the years epanet routines have been extended to simulate a broader range of physical processes or solve specific engineering problems for example epanet msx allows modelling the reaction of multiple interacting chemical species in both bulk flow and pipe walls shang et al 2008 ettar facilitates the optimization of pump operations and overcomes some limitations of epanet in dealing with complex control rules marchi et al 2016 wrappers have also been developed to facilitate the interaction between epanet and various programming languages such as r matlab and python for instance the r package epanetreader developed by eck 2016 reads network and simulation data into the r software environment and provides summary and plotting functionalities more recently arandia and eck 2018 released epanet2toolkit a comprehensive r package that allows running extended period simulations of hydraulics and water quality processes and creating custom interactive applications similarly the epanet matlab toolkit eliades 2010 provides a thorough epanet programming interface for the matlab computing environment python users can adopt the water network tool for resilience klise et al 2017 an epanet based python package designed to simulate and analyze the resilience of water distribution networks particularly under natural disasters while epanet provides a detailed representation of all physical components of a water distribution system it is not designed to account for the interaction between these components and the modern networked devices introduced for high level process supervisory management examples of such devices include smart meters sensors industrial computers and control system architectures all regularly used to monitor and control hydraulic and water quality processes see cominola et al 2015 gong et al 2016 sopasakis et al 2018 and references therein these devices present a security threat by exposing networks to cyber and cyber physical attacks cpa i e cyber penetrations designed to disrupt the physical processes of a water distribution system rasekh et al 2016 impacts are potentially severe as they can range from stopping water supply to even compromising water quality horta 2007 dakin et al 2009 hence there is a growing interest towards understanding the environmental impact of such attacks being able to model the interactions between hydraulic networks and the modern devices that control them is a first necessary step towards securing these critical infrastructures in this work we contribute to the growing field of smart water networks by presenting epanetcpa an open source matlab toolbox that allows users to design custom cyber physical attacks and simulate with epanet the corresponding hydraulic response of water networks an early closed source version of this toolbox was briefly introduced in taormina et al 2017 the version described here presents major improvements and extensive refactoring that further extend the toolbox s capabilities and improve its usability making it more appealing to users from various domains e g environmental modelling cyber security among the new functionalities we note the capability of simulating pressure deficient conditions which enables users to assess a broader range of environmental impacts 2 epanetcpa this section provides an overview of epanetcpa and summarizes how to configure and run a simulation with matlab details are provided on epanetcpa s input and output files and on the main classes constituting the toolbox with particular emphasis on those implementing the attacks 2 1 overview the key functionality of epanetcpa is to model the interaction between the physical and cyber layer of a water distribution system the abstraction of the cyber layer consists of a graph including the supervisory control and data acquisition scada system and multiple programmable logic controllers plcs with attached sensors and or actuators plcs perform control locally and are responsible for the control statements pertaining to each actuator in addition plcs send sensor readings to scada and to other plcs which employ the readings in their control logic scada stores sensor readings and actuator status sent by the plcs but it can also directly operate the actuators epanetcpa features a wide range of cyber physical attacks in particular the threats implemented in the toolbox are based on the attacker model proposed in taormina et al 2017 and include physical attacks to sensors and actuators deception attacks alteration of control signals measurements and transmissions denial of service dos of communication channels replay attacks via eavesdropping of transmissions and alteration of plc and scada control statements the hydraulic response of a water network can be modelled using epanet s default demand driven analysis or using pressure driven analysis pda which determines the amount of water delivered to customers on the basis of the available head this enables users to quantify various environmental impacts such as the demand satisfaction ratio or the time at which consumers demand is not met 2 2 configuring and running a simulation epanetcpa has been developed for running complex simulation scenarios with minimal effort from the user provided the folder containing the epanetcpa library has been added to the matlab search path only three lines of code are needed for initiating running and storing the results of a simulation as shown in the example below listing 1 listing 1 epanetcpa main file image 1 where example inp is the traditional epanet input file and example cpa is an additional input file with information regarding the cyber layer attacks and supplemental options the first line of code instantiates an epanetcpa object while the second and third lines invoke two methods that run the simulation and save the results to files respectively 2 2 1 the cpa input file the additional cpa file is structured in sections similar to an epanet inp input file as shown in the example of listing 2 the cpa file consists of four sections identified by brackets with semicolons denoting comments or section headers to follow epanet conventions the entries of each section are rows containing comma separated fields different entries within the same field are separated by whitespaces the sections are cybernodes each controller i e plcs and scada of the water distribution system is a node in the cyber layer this section lists all nodes along with the sensors and actuators they supervise or control the actuator identifiers must correspond to those listed in the inp file while sensor identifiers are specified by adding a prefix to the monitored hydraulic component prefixes identify the variables read and are separated from the component identifier by an underscore the p prefix identifies pressure at junctions or water level in tanks while f s and se point respectively to the flow status and setting of pumps and valves in listing 2 for example plc1 reads the water level in tank tank p tank line 3 while plc2 controls the operations of pumps pump1 and pump2 and reads their status s pump1 and s pump2 line 4 the fact that a scada entry is missing indicates that in this example the scada does not read or control directly any component but it only stores the information sent by the plcs cyberlinks connections between cyber nodes are specified in this section each line defines a cyber link between a source and a destination node which is used to transfer the sensor readings multiple sensors can be specified in one line for conciseness which results in the creation of multiple cyber links between the same source and destination nodes in listing 2 since plc2 needs p tank to operate the two pumps an incoming connection with plc1 is established line 7 plc1 sends the tank level also to scada for monitoring purposes line 8 on the other hand plc2 reads the status of the two pumps it controls line 4 but it does not send these readings to scada cyberattacks the attacks characterizing the simulated scenario are defined in this section of the cpa file each entry is made of 5 parts identifying the type of attack the targeted component the start and end condition of the attacks as well as arguments that further detail a specific adversary action listing 2 features two overlapping attacks a dos attack on the communication channel carrying tank readings from plc1 to plc2 line 11 and a replay attack that alters the tank readings arriving to the scada from plc1 line 12 the first two arguments of the replay attack specify that the current readings are replaced with those eavesdropped 50 h before the attack started after a uniform random component bounded in the 0 05 m range has been added the last arguments limit the resulting distorted signal between a maximum of 5 and a minimum of 0 m options five types of options can be specified in this section the line verbosity 1 specifies that the echo on screen is given at every hour line 14 while the option what to store is used to define which nodal and link variables to store during the simulation line 15 in particular the first field of what to store contains a list of the tracked components while the remaining fields identify which nodal and link variables to track epanetcpa may store all epanet variables such as pressure and demand at the junctions tank water levels stored as pressures or flows through pipes pumps and valves in the reported cpa example the code stores the water level of the tank pressure as well as the flow through the pumps and their status by commenting out this option the toolbox stores all variables for all nodes and links in the map the remaining options are initial conditions patterns and pda options not appearing in listing 2 which the user can employ to overwrite the initial level of the tanks in the inp file replace the original demand patterns stored in a tabular file and input the details needed to run the simulation using pressure driven analysis the user can choose among four types of head flow relationships hpr for pda namely wagner bhave salgado castro and fujiwara wagner et al 1988 douglas et al 2018 this choice is specified in the pda options along with the hfr parameters to be used for instance the line below specifies that the simulation will be carried out using the wagner hfr with emitter coefficient 0 5 and minimum and desired pressures of 0 and 20 m respectively image 2 omitting this option results in running a demand driven simulation listing 2 the cpa input le fields in each line are tab separated image 3 2 2 2 the inpx input file to fully enable the simulation of cyber physical attacks on the map epanetcpa modifies the inp file to create an augmented inpx file an internal step that does not require any user intervention the inpx file contains some extra dummy controls needed to override the original control logic when attacks are in place as well as some modifications to the map to allow for simulating tank overflows the inpx file also features user specified initial tank levels and demand patterns if the initial conditions and patterns options are specified to overwrite the original values contained in the inp file see section 2 2 1 further additions to the map are needed when performing simulations using pressure driven analysis which is carried out in epanetcpa using the approach described in abdy sayyed et al 2015 this method incorporates hfrs in the epanet demand driven engine by adding a string of artificial components i e a check valve a junction a flow control valve and an emitter node to each demand node 2 2 3 the output files the outputresults method of the epanetcpa class see listing 1 stores the results of the simulation in two comma separated files csv named according to the string argument passed the first file contains the ground truth i e the actual observable values of the variables at the physical layer the file contains as many rows as the simulation time steps plus a header identifying each column the time stamp of each simulation step is stored in the first column which is followed by as many columns as the number of tracked nodal and link variables these are specified via the what to store option of the cpa file described before the file containing ground truth results also stores information on when the attacks are in place in particular the attacks are tracked by additional boolean columns one for each attack which are arranged based on the order of appearance specified in the cyberattacks section of the cpa file if the simulation features attacks that manipulate the readings arriving to plcs and scada the toolbox outputs an additional csv file identified with an altered readings suffix to track the false information sent to these cyber nodes each entry in this tabular file has five attributes that contain the timestamp at which the alteration occurred the layer affected by the alteration a plc identifier scada or the physical layer in case of sensor damage or replacement the sensor being altered the value of the altered reading the epanet variable being altered 2 3 epanetcpa classes and featured attacks epanetcpa is available as a collection of matlab classes that act as a wrapper around the epanet2 programmer s toolkit as shown in listing 1 the user only needs to interact with an object of the epanetcpa main class if the inp and cpa input files are available and error free upon class instantiation the toolbox creates an epanetcpamap object and an epanetcpasimulation object which implement the augmented epanet map and simulation engine respectively here the term augmented refers to the additional capabilities of epanetcpa that allow to model attacks and their effects on the water network the epanetcpamap object interfaces with the enhanced inpx network and incorporates the controllers plcs and scada defining the cyber layer the epanetcpasimulation object contains the methods to run the epanet hydraulic engine execute the attacks and override the original control logic to simulate their effects on the water network the attacks featured in epanetcpa are realized in specific classes which inherit from a cyberphysicalattack superclass where common properties and methods are defined there are four attack classes implemented in epanetcpa which are named with an attackon prefix followed by the type of target these targets can be sensors actuators control statements and communication links thus the relative classes are named attackonsensor attackonactuator attackoncontrol and attackoncommunication attack objects are instantiated based on the instructions in the cyberattacks section of the cpa file see section 2 2 1 regardless of the attack class the instructions follow the same pattern made of five comma separated attributes see lines 10 12 of listing 2 the first attribute identifies the attack class to be instantiated without the attackon prefix for brevity while the second contains a string that specifies the actual target of the attack table 1 provides information on how to define the target for each attack class next the third and fourth fields contain matlab conditional statements that specify when the attack begins and ends the conditions can be set on time variables on the status of sensors and actuators or on the state of other attacks logical operators can be used to create complex statements by concatenating multiple simple conditions table 2 reports some examples of simple and complex statements lastly the fifth field contains an argument or a list of arguments separated by whitespaces that further define the attack for example these arguments specify whether an attackoncommunication is a denial of service or a replay attack table 3 lists all possible arguments for each attack class and describes how they affect the attack specifications 3 example application the toolbox is demonstrated using the demo attack scenario described in the cpa file provided in listing 2 the attacks are launched on a simplified water distribution system consisting of one reservoir two pumps pump1 and pump2 one storage tank tank a few demand nodes and two plcs connected to the scada system fig 1 a right panel plc1 reads the water level in tank line 3 of listing 2 and sends the readings to both scada and plc2 lines 7 8 plc2 controls pumping operations line 4 by comparing the water level in tank against the low and high thresholds set in the control logic in the inp file the trajectory of the tank water level under normal operating conditions for a week long simulation is shown in fig 1a left panel the attack aims to cause the tank to overflow this is achieved by disabling the shut down of the pumps normally occurring when the relative upper threshold in tank is reached to achieve that the attacker perpetrates a dos attack on the communication channel between plc1 and plc2 line 11 preventing plc1 from sending updated readings to plc2 fig 1b right panel if the remaining connection leaving plc1 is not affected personnel at scada may be able to detect the anomalous increase in the tank water level fig 1b left panel to maximize the chances of success the adversary can conceal the attack by manipulating the plc1 scada communication channel with a replay attack line 12 where past transmission of valid p tank data recorded up to 50 h before the attack are maliciously replayed fig 1c 4 outlook epanetcpa requires minimal user effort to simulate the interaction between the physical and cyber layer of a water distribution system allowing both experts and the broader public to reproduce plausible cyber physical attack scenarios and assess their environmental impact we expect that these features along with the availability of the source code will encourage external contributions and user feedback in the long run we intend to enable the study of attack scenarios affecting water quality processes by interfacing epanetcpa with epanet msx shang et al 2008 finally we note that the use of the toolbox goes beyond the evaluation of environmental impacts for example the data generated by epanetcpa can be used to design more secure communication networks or to train attack detection algorithms taormina et al 2018 taormina and galelli 2018 acknowledgments dr taormina dr galelli and dr tippenhauer were supported in part by the national research foundation nrf prime minister s office singapore under its national cybersecurity r d programme award no nrf2014ncr ncr001 40 and administered by the national cybersecurity r d directorate mr douglas is supported by the mit sutd dual masters programme the authors are grateful to the anonymous reviewers and particularly to reviewer 2 for their key suggestions on how to improve epanetcpa and this accompanying manuscript 
26275,this work introduces epanetcpa an open source matlab toolbox for modelling the hydraulic response of water distribution systems to cyber physical attacks epanetcpa allows users to quickly design various attack scenarios and assess their impact via simulation with epanet a popular public domain model for water network analysis the toolbox offers both demand driven and pressure driven simulations enabling the users to realistically analyze cyber physical attacks and their impacts under both pressure sufficient and pressure deficient conditions epanetcpa is available under the mit license keywords water distribution systems smart water networks epanet cyber security cyber physical systems software availability name of software epanetcpa license mit developers r taormina h c douglas contact email riccardo taormina sutd edu sg riccardo taormina gmail com software required matlab epanet2 programmer s toolkit year first available 2018 available from github https github com rtaormina epanetcpa 1 introduction epanet is a popular public domain model for simulating hydraulic and water quality processes in water distribution systems rossman 2000 the model developed by the united states environmental protection agency is available as a stand alone gui software or as a dynamic link library known as the programmer s toolkit generally used to develop custom applications over the years epanet routines have been extended to simulate a broader range of physical processes or solve specific engineering problems for example epanet msx allows modelling the reaction of multiple interacting chemical species in both bulk flow and pipe walls shang et al 2008 ettar facilitates the optimization of pump operations and overcomes some limitations of epanet in dealing with complex control rules marchi et al 2016 wrappers have also been developed to facilitate the interaction between epanet and various programming languages such as r matlab and python for instance the r package epanetreader developed by eck 2016 reads network and simulation data into the r software environment and provides summary and plotting functionalities more recently arandia and eck 2018 released epanet2toolkit a comprehensive r package that allows running extended period simulations of hydraulics and water quality processes and creating custom interactive applications similarly the epanet matlab toolkit eliades 2010 provides a thorough epanet programming interface for the matlab computing environment python users can adopt the water network tool for resilience klise et al 2017 an epanet based python package designed to simulate and analyze the resilience of water distribution networks particularly under natural disasters while epanet provides a detailed representation of all physical components of a water distribution system it is not designed to account for the interaction between these components and the modern networked devices introduced for high level process supervisory management examples of such devices include smart meters sensors industrial computers and control system architectures all regularly used to monitor and control hydraulic and water quality processes see cominola et al 2015 gong et al 2016 sopasakis et al 2018 and references therein these devices present a security threat by exposing networks to cyber and cyber physical attacks cpa i e cyber penetrations designed to disrupt the physical processes of a water distribution system rasekh et al 2016 impacts are potentially severe as they can range from stopping water supply to even compromising water quality horta 2007 dakin et al 2009 hence there is a growing interest towards understanding the environmental impact of such attacks being able to model the interactions between hydraulic networks and the modern devices that control them is a first necessary step towards securing these critical infrastructures in this work we contribute to the growing field of smart water networks by presenting epanetcpa an open source matlab toolbox that allows users to design custom cyber physical attacks and simulate with epanet the corresponding hydraulic response of water networks an early closed source version of this toolbox was briefly introduced in taormina et al 2017 the version described here presents major improvements and extensive refactoring that further extend the toolbox s capabilities and improve its usability making it more appealing to users from various domains e g environmental modelling cyber security among the new functionalities we note the capability of simulating pressure deficient conditions which enables users to assess a broader range of environmental impacts 2 epanetcpa this section provides an overview of epanetcpa and summarizes how to configure and run a simulation with matlab details are provided on epanetcpa s input and output files and on the main classes constituting the toolbox with particular emphasis on those implementing the attacks 2 1 overview the key functionality of epanetcpa is to model the interaction between the physical and cyber layer of a water distribution system the abstraction of the cyber layer consists of a graph including the supervisory control and data acquisition scada system and multiple programmable logic controllers plcs with attached sensors and or actuators plcs perform control locally and are responsible for the control statements pertaining to each actuator in addition plcs send sensor readings to scada and to other plcs which employ the readings in their control logic scada stores sensor readings and actuator status sent by the plcs but it can also directly operate the actuators epanetcpa features a wide range of cyber physical attacks in particular the threats implemented in the toolbox are based on the attacker model proposed in taormina et al 2017 and include physical attacks to sensors and actuators deception attacks alteration of control signals measurements and transmissions denial of service dos of communication channels replay attacks via eavesdropping of transmissions and alteration of plc and scada control statements the hydraulic response of a water network can be modelled using epanet s default demand driven analysis or using pressure driven analysis pda which determines the amount of water delivered to customers on the basis of the available head this enables users to quantify various environmental impacts such as the demand satisfaction ratio or the time at which consumers demand is not met 2 2 configuring and running a simulation epanetcpa has been developed for running complex simulation scenarios with minimal effort from the user provided the folder containing the epanetcpa library has been added to the matlab search path only three lines of code are needed for initiating running and storing the results of a simulation as shown in the example below listing 1 listing 1 epanetcpa main file image 1 where example inp is the traditional epanet input file and example cpa is an additional input file with information regarding the cyber layer attacks and supplemental options the first line of code instantiates an epanetcpa object while the second and third lines invoke two methods that run the simulation and save the results to files respectively 2 2 1 the cpa input file the additional cpa file is structured in sections similar to an epanet inp input file as shown in the example of listing 2 the cpa file consists of four sections identified by brackets with semicolons denoting comments or section headers to follow epanet conventions the entries of each section are rows containing comma separated fields different entries within the same field are separated by whitespaces the sections are cybernodes each controller i e plcs and scada of the water distribution system is a node in the cyber layer this section lists all nodes along with the sensors and actuators they supervise or control the actuator identifiers must correspond to those listed in the inp file while sensor identifiers are specified by adding a prefix to the monitored hydraulic component prefixes identify the variables read and are separated from the component identifier by an underscore the p prefix identifies pressure at junctions or water level in tanks while f s and se point respectively to the flow status and setting of pumps and valves in listing 2 for example plc1 reads the water level in tank tank p tank line 3 while plc2 controls the operations of pumps pump1 and pump2 and reads their status s pump1 and s pump2 line 4 the fact that a scada entry is missing indicates that in this example the scada does not read or control directly any component but it only stores the information sent by the plcs cyberlinks connections between cyber nodes are specified in this section each line defines a cyber link between a source and a destination node which is used to transfer the sensor readings multiple sensors can be specified in one line for conciseness which results in the creation of multiple cyber links between the same source and destination nodes in listing 2 since plc2 needs p tank to operate the two pumps an incoming connection with plc1 is established line 7 plc1 sends the tank level also to scada for monitoring purposes line 8 on the other hand plc2 reads the status of the two pumps it controls line 4 but it does not send these readings to scada cyberattacks the attacks characterizing the simulated scenario are defined in this section of the cpa file each entry is made of 5 parts identifying the type of attack the targeted component the start and end condition of the attacks as well as arguments that further detail a specific adversary action listing 2 features two overlapping attacks a dos attack on the communication channel carrying tank readings from plc1 to plc2 line 11 and a replay attack that alters the tank readings arriving to the scada from plc1 line 12 the first two arguments of the replay attack specify that the current readings are replaced with those eavesdropped 50 h before the attack started after a uniform random component bounded in the 0 05 m range has been added the last arguments limit the resulting distorted signal between a maximum of 5 and a minimum of 0 m options five types of options can be specified in this section the line verbosity 1 specifies that the echo on screen is given at every hour line 14 while the option what to store is used to define which nodal and link variables to store during the simulation line 15 in particular the first field of what to store contains a list of the tracked components while the remaining fields identify which nodal and link variables to track epanetcpa may store all epanet variables such as pressure and demand at the junctions tank water levels stored as pressures or flows through pipes pumps and valves in the reported cpa example the code stores the water level of the tank pressure as well as the flow through the pumps and their status by commenting out this option the toolbox stores all variables for all nodes and links in the map the remaining options are initial conditions patterns and pda options not appearing in listing 2 which the user can employ to overwrite the initial level of the tanks in the inp file replace the original demand patterns stored in a tabular file and input the details needed to run the simulation using pressure driven analysis the user can choose among four types of head flow relationships hpr for pda namely wagner bhave salgado castro and fujiwara wagner et al 1988 douglas et al 2018 this choice is specified in the pda options along with the hfr parameters to be used for instance the line below specifies that the simulation will be carried out using the wagner hfr with emitter coefficient 0 5 and minimum and desired pressures of 0 and 20 m respectively image 2 omitting this option results in running a demand driven simulation listing 2 the cpa input le fields in each line are tab separated image 3 2 2 2 the inpx input file to fully enable the simulation of cyber physical attacks on the map epanetcpa modifies the inp file to create an augmented inpx file an internal step that does not require any user intervention the inpx file contains some extra dummy controls needed to override the original control logic when attacks are in place as well as some modifications to the map to allow for simulating tank overflows the inpx file also features user specified initial tank levels and demand patterns if the initial conditions and patterns options are specified to overwrite the original values contained in the inp file see section 2 2 1 further additions to the map are needed when performing simulations using pressure driven analysis which is carried out in epanetcpa using the approach described in abdy sayyed et al 2015 this method incorporates hfrs in the epanet demand driven engine by adding a string of artificial components i e a check valve a junction a flow control valve and an emitter node to each demand node 2 2 3 the output files the outputresults method of the epanetcpa class see listing 1 stores the results of the simulation in two comma separated files csv named according to the string argument passed the first file contains the ground truth i e the actual observable values of the variables at the physical layer the file contains as many rows as the simulation time steps plus a header identifying each column the time stamp of each simulation step is stored in the first column which is followed by as many columns as the number of tracked nodal and link variables these are specified via the what to store option of the cpa file described before the file containing ground truth results also stores information on when the attacks are in place in particular the attacks are tracked by additional boolean columns one for each attack which are arranged based on the order of appearance specified in the cyberattacks section of the cpa file if the simulation features attacks that manipulate the readings arriving to plcs and scada the toolbox outputs an additional csv file identified with an altered readings suffix to track the false information sent to these cyber nodes each entry in this tabular file has five attributes that contain the timestamp at which the alteration occurred the layer affected by the alteration a plc identifier scada or the physical layer in case of sensor damage or replacement the sensor being altered the value of the altered reading the epanet variable being altered 2 3 epanetcpa classes and featured attacks epanetcpa is available as a collection of matlab classes that act as a wrapper around the epanet2 programmer s toolkit as shown in listing 1 the user only needs to interact with an object of the epanetcpa main class if the inp and cpa input files are available and error free upon class instantiation the toolbox creates an epanetcpamap object and an epanetcpasimulation object which implement the augmented epanet map and simulation engine respectively here the term augmented refers to the additional capabilities of epanetcpa that allow to model attacks and their effects on the water network the epanetcpamap object interfaces with the enhanced inpx network and incorporates the controllers plcs and scada defining the cyber layer the epanetcpasimulation object contains the methods to run the epanet hydraulic engine execute the attacks and override the original control logic to simulate their effects on the water network the attacks featured in epanetcpa are realized in specific classes which inherit from a cyberphysicalattack superclass where common properties and methods are defined there are four attack classes implemented in epanetcpa which are named with an attackon prefix followed by the type of target these targets can be sensors actuators control statements and communication links thus the relative classes are named attackonsensor attackonactuator attackoncontrol and attackoncommunication attack objects are instantiated based on the instructions in the cyberattacks section of the cpa file see section 2 2 1 regardless of the attack class the instructions follow the same pattern made of five comma separated attributes see lines 10 12 of listing 2 the first attribute identifies the attack class to be instantiated without the attackon prefix for brevity while the second contains a string that specifies the actual target of the attack table 1 provides information on how to define the target for each attack class next the third and fourth fields contain matlab conditional statements that specify when the attack begins and ends the conditions can be set on time variables on the status of sensors and actuators or on the state of other attacks logical operators can be used to create complex statements by concatenating multiple simple conditions table 2 reports some examples of simple and complex statements lastly the fifth field contains an argument or a list of arguments separated by whitespaces that further define the attack for example these arguments specify whether an attackoncommunication is a denial of service or a replay attack table 3 lists all possible arguments for each attack class and describes how they affect the attack specifications 3 example application the toolbox is demonstrated using the demo attack scenario described in the cpa file provided in listing 2 the attacks are launched on a simplified water distribution system consisting of one reservoir two pumps pump1 and pump2 one storage tank tank a few demand nodes and two plcs connected to the scada system fig 1 a right panel plc1 reads the water level in tank line 3 of listing 2 and sends the readings to both scada and plc2 lines 7 8 plc2 controls pumping operations line 4 by comparing the water level in tank against the low and high thresholds set in the control logic in the inp file the trajectory of the tank water level under normal operating conditions for a week long simulation is shown in fig 1a left panel the attack aims to cause the tank to overflow this is achieved by disabling the shut down of the pumps normally occurring when the relative upper threshold in tank is reached to achieve that the attacker perpetrates a dos attack on the communication channel between plc1 and plc2 line 11 preventing plc1 from sending updated readings to plc2 fig 1b right panel if the remaining connection leaving plc1 is not affected personnel at scada may be able to detect the anomalous increase in the tank water level fig 1b left panel to maximize the chances of success the adversary can conceal the attack by manipulating the plc1 scada communication channel with a replay attack line 12 where past transmission of valid p tank data recorded up to 50 h before the attack are maliciously replayed fig 1c 4 outlook epanetcpa requires minimal user effort to simulate the interaction between the physical and cyber layer of a water distribution system allowing both experts and the broader public to reproduce plausible cyber physical attack scenarios and assess their environmental impact we expect that these features along with the availability of the source code will encourage external contributions and user feedback in the long run we intend to enable the study of attack scenarios affecting water quality processes by interfacing epanetcpa with epanet msx shang et al 2008 finally we note that the use of the toolbox goes beyond the evaluation of environmental impacts for example the data generated by epanetcpa can be used to design more secure communication networks or to train attack detection algorithms taormina et al 2018 taormina and galelli 2018 acknowledgments dr taormina dr galelli and dr tippenhauer were supported in part by the national research foundation nrf prime minister s office singapore under its national cybersecurity r d programme award no nrf2014ncr ncr001 40 and administered by the national cybersecurity r d directorate mr douglas is supported by the mit sutd dual masters programme the authors are grateful to the anonymous reviewers and particularly to reviewer 2 for their key suggestions on how to improve epanetcpa and this accompanying manuscript 
26276,preparing experiment ready images is a labor intensive but necessary process for almost all remote sensing related studies this short communication introduces a software to efficiently acquire large scale high quality and cloud free landsat images to support subsequent analysis and applications the software fully leverages google earth engine s processing of huge amount of remote sensing images and the visualization on the availability and accessibility of landsat images provided by heatmaps at both regional and global scales the cloud percentage property and quality bands of landsat surface reflectance products are used for masking clouds cloud shadows snow ice and low quality pixels in this study the software can be applied to effectively choose study areas and prepare data in different fields such as hydrology ecology and human activities this functioning open source software can be accessible via github webpages in both javascript and python formats keywords google earth engine landsat image accessibility cloud free experiment ready images 1 introduction optical remote sensing satellites landsat with about 8 million images have been continuously providing earth observation data for 40 years which constitute the longest uninterrupted records of global scale medium spatial resolution earth observation data wulder et al 2016 it has a great potential of landsat data to provide a consistent long term large area data record roy et al 2010 high quality user ready composited mosaics of landsat can support decadal assessments of environmental and land cover change production of reflectance based biophysical products and applications that merge reflectance data from multiple sensors masek et al 2006 applications include modelling deforestation for multiple years mas et al 2004 water quality estimation montzka et al 2008 mouri et al 2012 land surface phenology variability broich et al 2015 etc usually at national continental even global scales typically greater than 1mkm2 in area hansen and loveland 2012 however it is usually labor intensive to acquire long term experiment ready and high quality dataset to support subsequent analysis and applications due to atmospheric impact and sensor systematic error remote sensing images usually require radiometric and geometric corrections hansen and loveland 2012 to eliminate noises such as cloud cover cloud shadows and snow pixels zhu et al 2015 or to correct geometric geolocation errors preparing long term and large scale remote sensing images of high quality can be inefficient especially for individuals since it includes inspecting and downloading and filtering of big data from the united states geological survey s usgs landsat data archive woodcock et al 2008 the computing intensity work of which thus requires high capacity and computing power users only with fast internet speeds and high performance computers could have more efficient data preprocessing the largest file size achievable is also limited by the amount of addressable memory on a user s personal computer usually conservatively considered to be a 32 bit computer with a maximum file size of 2 gb there are a few image composites algorithms that were proposed for landsat images zhu 2017 since roy et al 2010 studied image compositing criteria for landsat images with etm in 2010 some regional roy et al 2010 potapov et al 2011 white et al 2014 potapov et al 2015 and global tucker et al 2004 roy et al 2010 high quality landsat composite images have been available subsequently global high quality 30 m landsat mosaics are usually in decadal geocover tucker et al 2004 annual or monthly roy et al 2010 for the benchmark work a leading project for high quality user ready landsat image mosaics is web enabled landsat data weld roy et al 2010 a landsat composited mosaics project of usgs providing 2009 2010 and 2011 global monthly and annual images and weekly monthly seasonal and annual 30 m continental u s conus and alaska composites generated from landsat 7 etm data are available for 10 years 2003 2012 https landsat usgs gov web enabled landsat data weld projects weld is based on landsat ecosystem disturbance adaptive processing system ledaps schmidt et al 2013 however these data are limited to specific time periods or areas several famous commercial remote sensing image processing software like envi and erdas provides a series of image mosaicking functions however they basically need the users to download images first and process them locally therefore it would be rather helpful to leverage the power of abundant supercomputers high performance computing systems and large scale cloud computing capacity of modern computer techniques ferdman et al 2012 to get user defined spatial and temporal high quality composited images the google earth engine gee gorelick et al 2017 is a dominant cloud based geospatial processing platform coupled to a growing archive of imagery gathered from nasa s earth observing satellites landsat modis etc and esa s sentinel satellites among many other sources which could facilitate remote sensing data processing and information extraction at large scales without heavy data downloading and data intensive processing with the strong hardware and software platform gee can support efficient data processing and preparation and has been used for many global scale studies such as monitoring global forest changes hansen et al 2013 estimating crop yield lobell et al 2015 producing high spatio temporal resolution ndvi products robinson et al 2017 monitoring land cover changes johansen et al 2015 kline 2017 and mapping long term global high resolution surface water pekel et al 2016 bare ground ying et al 2017 and continental crop xiong et al 2017b gee api has also been used in many third party applications e g flood mapping using python coltin et al 2016 however little literature has shown the process to acquire user defined high quality images for scoring landsat pixels by their relative cloudiness gee itself provides a rudimentary cloud scoring algorithm for landsat top of atmosphere toa images this is not a robust cloud detector and is intended mainly to compare multiple looks at the same point for relative cloud likelihood it also does not consider the cloud shadows or snow cover although many gee generated datasets have been shared no paper has systematically introduced the generating process of high quality landsat images so far the new paradigm would be pixel based rather than conventional scene based image compositing white et al 2014 the target of this short communication is to provide a framework and corresponding software for user defined spatio temporal high quality landsat composite mosaics based on gee cloud platform local regional and global wall to wall land surface high quality user ready mosaics can then be easily produced for experiment ready analysis the sample codes in javascript and python for high quality landsat image production are shared and can be further improved by the gee user community on github 2 design and tool capabilities 2 1 software environment both javascript and python versions of the code are available for users which are consistent with the languages used by gee the javascript code can be executed on the gee webpage platform https code earthengine google com while the python environment can be set up after the installment of gee python api https developers google com earth engine python install manual codes under both environments can be modified with a custom time period and boundary parameters among some others like cloud coverage compared to python javascript version provides more comprehensive tutorials with more active community currently it is an instant visualizing client side and can be shared with just a webpage link however python would be a better choice for personal or third party applications involving self defined functions since javascript does not well support the output of large scale and a huge amount of images 2 2 framework for user defined high quality mosaic images this study proposes a framework for multi year high quality landsat mosaic image production under gee cloud environment fig 1 the general steps are described as follows step 1 inspect the available image count and start time of those images with heatmaps in the user defined study area step 2 define a time period for mosaicking with user input start date and end date step 3 select a compositing paradigm which could be either scene based or pixel based step 4 mosaicking images in the study area step 5 produced high quality composited mosaic images can be exported spatial boundary and time period start date to end date should be specified for further inspection table 1 shows the available time period of landsat surface reflectance data for each sensor the spatial boundary could be defined as points lines or polygons which can be a kml file uploaded to google drive as google fusion tables gft vector boundaries in esri shapefile format can be easily converted to kml format in commercial e g arcgis or open source e g qgis gis software the spatial coverage can also be defined by geometric drawing using a mouse on the javascript visualization client side webpage about the mosaicking process if the scene based paradigm is chosen in the last step a1 the image collection will be filtered with the cloud cover threshold defined by users the cloud cover property of each image is computed by the automatic cloud cover assessment acca algorithm during the landsat archive process irish et al 2006 if the pixel based paradigm is selected a2 the quality bands would be used for filtering it includes selecting the non filled pixels non saturated pixels cloud cloud shadow snow and clip the edge with a user defined width the quality bands of surface reflectance images include pixel qa which is computed by cfmask algorithm zhu et al 2015 and radsat qa as a per pixel saturation mask details about quality bands can be seen from https landsat usgs gov landsat surface reflectance quality assessment for landsat surface reflectance data after the quasi high quality images produced with a1 or a2 one of the pixel wise mosaic strategies can be selected to make the composited high quality images median mosaic is used to select a median value of all qualified images processed by previous steps in order to ensure no obvious spectral difference between images of multiple years similarly the mean mosaic is to compute the mean value of all bands for each pixel max ndvi is to select the greenest pixels while greatest brightness temperature strategy ensures the low temperature clouds are not included the user defined quality settings are realized in two ways user input and selection the first type is user input params including the spatial boundary of the study area time period cloud cover threshold and clipping length for image edges the other type is user selection params when spatial and temporal extent defined the compositing paradigm should be chosen if pixel based paradigm selected quality bands should be chosen to filter qualified pixels with user defined conditions after quasi high quality images or pixels produced a mosaic strategy could be defined the outputs are the mosaic images produced with user defined parameters for javascript a mosaic image can be visualized on the webpage and exported to google drive without too much effort in the task sub window for python hundreds of small area images can be automatically exported in a batch mode without clicking run in the task window one by one 2 3 software functions table 2 shows the summary of functions and the related descriptions codes for high quality image production are provided in both javascript and python with the name highquality it mainly realizes the preparation of experiment ready mosaic images without too much effort inspecting tools include imagecount and firstyearobs imagecount is used to inspect the number of available images that can be easily visualized for study time extents and areas of interest in order to efficiently support the selection of landsat images for case studies for example researchers can input the latitude and longitude extent in their study area and visualize the availability map to get a first knowledge of the accessibility of good quality landsat data available in different months similarly firstyearobs is used to inspect to which year the available remote sensing images can be traced back the computing power of gee makes it convenient to directly process image data on the platform without downloading data however there are still some user specific applications that need to be processed locally the images need to be downloaded for further processing batchdownload written in python can efficiently realize this the function clips images with vector boundaries i e features and saves those clipped small images to google drive which can then be downloaded in bulk 3 results and discussions this section introduces our software in details which include showing heatmaps for starting year information image count presenting the experiment ready landsat mosaic images and land use classification from the composited images the yangtze river basin with a total drainage area about 1 8 mkm2 is chosen as the case study area to show the results as the longest river in china and asia s greatest water towers threatened by climate change it has great environmental and ecological value mainly with a subtropical monsoon climate it has frequent cloud coverage therefore it is a typical area for high quality cloud free mosaicking a land use application is described shown in fig 2 is the main interface of our software there are three panels the first one is for setting global parameters including the spatial and temporal extent sensors and compositing paradigm the second one is a panel for setting pixel or scene based parameters the third one is a visualization panel for results 3 1 visualization of starting time and scene counts fig 3 shows the landsat 5 image information of main parts of china with heatmaps fig 3 a is the first observation year note that the northeast and east coast of china has the images back to 1984 while the earliest images for the northwest are from 1990 similar maps could be drawn in other parts of the world remote sensing images are an important data source for research and for many applications such as water resource management this function allows researchers to efficiently estimate what time could remote sensing images be traced back to for monitoring water dynamics in their study areas fig 3 b shows the available image counts of landsat 5 images in each pixel only with spatial parameter as input it can be seen that the west of china has a relatively small number of observations since its first observations start rather late note that although the first observations start early the central south still has smaller observations this mainly dues to the data filtering strategy of usgs that the images with too much cloud would be eliminated the central south of china is the most cloudy part of the whole country thus in the following part the yangtze river basin with the central south part included would be specifically chosen for producing yearly high quality composited mosaics and a land use land cover lulc change application 3 2 high quality mosaic and a lulc application the compositing framework is tested in the yangtze river basin shown in fig 4 which covers about 1 8 mkm2 in china fig 4 a is the true color composited mosaic in the year 2013 from the mosaic image retrieved by our software there are no obvious mosaic edges and almost no hue difference in the whole image fig 4 b is a simple application analysis to the composite image with manually selected training samples for each land cover types water urban area vegetation and bareland a supervised classification method proposed by mcdonald et al 2009 is performed on the image this function can be used for long term large scale image acquisition and can benefit studies on landsat image analyses and applications for land use classification lobell et al 2015 midekisa et al 2017 xiong et al 2017a ying et al 2017 it can be used for retrieving base maps of training and testing data for hydrology various water extraction indexes like modified normalized difference water index can be computed with bands of landsat mosaic images then the spatial and multi year temporal distribution of water body could be visualized and extracted under 30 m resolution similar studies with gee can be seen from previous studies feyisa et al 2014 fisher et al 2016 pekel et al 2016 3 3 comparison with previous work table 3 shows the comparison with current datasets i e web enabled landsat data weld and gee landsat composite dataset comparing with those two datasets the software provided by this study supports high quality large scale applications with user defined parameters including spatial coverage temporal extent pixel filtering and compositing strategy therefore it could benefit whoever want to use high quality cloud free landsat images by providing a simple framework and open codes with a flexible user interface 4 conclusions since landsat constitutes the longest record of global scale medium spatial resolution earth observation data it is an ideal data source for long term global scale studies we presented here a large scale experiment ready landsat mosaic image acquisition framework which requires only several user defined parameters such as spatio temporal extent cloud coverage threshold and also provides functions to examine the available image count and starting year of accessibility based on these functions the software has a lot of applications first it is suitable for annual scale large regions or global high quality mosaic image preparation for landsat second it can be used as high quality data sources for subsequent research such as feature extraction image classification and model training and validation whenever a cloud free experiment ready landsat image is needed similarly it can also be used as a preparation for training sample extraction for machine learning methods such as convolution neural network moreover for studies using long term landsat data without downloading any dataset and inspecting them on local computers researchers could inspect landsat data provided by gee and select a study area with enough high quality series images to meet their needs the software is built upon gee therefore its realization depends on the gee platform the composite image quality also greatly relies on the quality bands of landsat images since the revisiting time of landsat is about 16 days our future work would integrate more optical sensor data provided by gee e g sentinel 2 and modis to improve the temporal resolution of optical composite images code availability javascript code can be found and downloaded from 1 image count heatmap https gist github com scarlettlee 1f4864084adfa62dddb33ba346334378 2 first image of year https gist github com scarlettlee 3bf08afba1317f89f89c0668a169247f 3 high quality image production with a user interface https gist github com scarlettlee eda18ca801213429636e7a0764b5b59f python code can be found and downloaded from 1 landsat and output https gist github com scarlettlee 7beb591ba7d61bee06a5af9dd3e86cfb 2 batch clip an image with a feature collection and export to google drive https gist github com scarlettlee d979b30941fe0a27c085d20b50b3430e conflicts of interest the authors declare no conflict of interest acknowledgements this work is jointly supported by the nsfc cgiar project china s water and food security under climate extreme impact risk assessment and resilience grant no 7141101024 and the national natural science foundation of china grant no 91437214 the authors would like to thank mr noel gorelick from google for providing excellent suggestions and the authors would also like to thank developers in gee community for sharing very useful sample codes 
26276,preparing experiment ready images is a labor intensive but necessary process for almost all remote sensing related studies this short communication introduces a software to efficiently acquire large scale high quality and cloud free landsat images to support subsequent analysis and applications the software fully leverages google earth engine s processing of huge amount of remote sensing images and the visualization on the availability and accessibility of landsat images provided by heatmaps at both regional and global scales the cloud percentage property and quality bands of landsat surface reflectance products are used for masking clouds cloud shadows snow ice and low quality pixels in this study the software can be applied to effectively choose study areas and prepare data in different fields such as hydrology ecology and human activities this functioning open source software can be accessible via github webpages in both javascript and python formats keywords google earth engine landsat image accessibility cloud free experiment ready images 1 introduction optical remote sensing satellites landsat with about 8 million images have been continuously providing earth observation data for 40 years which constitute the longest uninterrupted records of global scale medium spatial resolution earth observation data wulder et al 2016 it has a great potential of landsat data to provide a consistent long term large area data record roy et al 2010 high quality user ready composited mosaics of landsat can support decadal assessments of environmental and land cover change production of reflectance based biophysical products and applications that merge reflectance data from multiple sensors masek et al 2006 applications include modelling deforestation for multiple years mas et al 2004 water quality estimation montzka et al 2008 mouri et al 2012 land surface phenology variability broich et al 2015 etc usually at national continental even global scales typically greater than 1mkm2 in area hansen and loveland 2012 however it is usually labor intensive to acquire long term experiment ready and high quality dataset to support subsequent analysis and applications due to atmospheric impact and sensor systematic error remote sensing images usually require radiometric and geometric corrections hansen and loveland 2012 to eliminate noises such as cloud cover cloud shadows and snow pixels zhu et al 2015 or to correct geometric geolocation errors preparing long term and large scale remote sensing images of high quality can be inefficient especially for individuals since it includes inspecting and downloading and filtering of big data from the united states geological survey s usgs landsat data archive woodcock et al 2008 the computing intensity work of which thus requires high capacity and computing power users only with fast internet speeds and high performance computers could have more efficient data preprocessing the largest file size achievable is also limited by the amount of addressable memory on a user s personal computer usually conservatively considered to be a 32 bit computer with a maximum file size of 2 gb there are a few image composites algorithms that were proposed for landsat images zhu 2017 since roy et al 2010 studied image compositing criteria for landsat images with etm in 2010 some regional roy et al 2010 potapov et al 2011 white et al 2014 potapov et al 2015 and global tucker et al 2004 roy et al 2010 high quality landsat composite images have been available subsequently global high quality 30 m landsat mosaics are usually in decadal geocover tucker et al 2004 annual or monthly roy et al 2010 for the benchmark work a leading project for high quality user ready landsat image mosaics is web enabled landsat data weld roy et al 2010 a landsat composited mosaics project of usgs providing 2009 2010 and 2011 global monthly and annual images and weekly monthly seasonal and annual 30 m continental u s conus and alaska composites generated from landsat 7 etm data are available for 10 years 2003 2012 https landsat usgs gov web enabled landsat data weld projects weld is based on landsat ecosystem disturbance adaptive processing system ledaps schmidt et al 2013 however these data are limited to specific time periods or areas several famous commercial remote sensing image processing software like envi and erdas provides a series of image mosaicking functions however they basically need the users to download images first and process them locally therefore it would be rather helpful to leverage the power of abundant supercomputers high performance computing systems and large scale cloud computing capacity of modern computer techniques ferdman et al 2012 to get user defined spatial and temporal high quality composited images the google earth engine gee gorelick et al 2017 is a dominant cloud based geospatial processing platform coupled to a growing archive of imagery gathered from nasa s earth observing satellites landsat modis etc and esa s sentinel satellites among many other sources which could facilitate remote sensing data processing and information extraction at large scales without heavy data downloading and data intensive processing with the strong hardware and software platform gee can support efficient data processing and preparation and has been used for many global scale studies such as monitoring global forest changes hansen et al 2013 estimating crop yield lobell et al 2015 producing high spatio temporal resolution ndvi products robinson et al 2017 monitoring land cover changes johansen et al 2015 kline 2017 and mapping long term global high resolution surface water pekel et al 2016 bare ground ying et al 2017 and continental crop xiong et al 2017b gee api has also been used in many third party applications e g flood mapping using python coltin et al 2016 however little literature has shown the process to acquire user defined high quality images for scoring landsat pixels by their relative cloudiness gee itself provides a rudimentary cloud scoring algorithm for landsat top of atmosphere toa images this is not a robust cloud detector and is intended mainly to compare multiple looks at the same point for relative cloud likelihood it also does not consider the cloud shadows or snow cover although many gee generated datasets have been shared no paper has systematically introduced the generating process of high quality landsat images so far the new paradigm would be pixel based rather than conventional scene based image compositing white et al 2014 the target of this short communication is to provide a framework and corresponding software for user defined spatio temporal high quality landsat composite mosaics based on gee cloud platform local regional and global wall to wall land surface high quality user ready mosaics can then be easily produced for experiment ready analysis the sample codes in javascript and python for high quality landsat image production are shared and can be further improved by the gee user community on github 2 design and tool capabilities 2 1 software environment both javascript and python versions of the code are available for users which are consistent with the languages used by gee the javascript code can be executed on the gee webpage platform https code earthengine google com while the python environment can be set up after the installment of gee python api https developers google com earth engine python install manual codes under both environments can be modified with a custom time period and boundary parameters among some others like cloud coverage compared to python javascript version provides more comprehensive tutorials with more active community currently it is an instant visualizing client side and can be shared with just a webpage link however python would be a better choice for personal or third party applications involving self defined functions since javascript does not well support the output of large scale and a huge amount of images 2 2 framework for user defined high quality mosaic images this study proposes a framework for multi year high quality landsat mosaic image production under gee cloud environment fig 1 the general steps are described as follows step 1 inspect the available image count and start time of those images with heatmaps in the user defined study area step 2 define a time period for mosaicking with user input start date and end date step 3 select a compositing paradigm which could be either scene based or pixel based step 4 mosaicking images in the study area step 5 produced high quality composited mosaic images can be exported spatial boundary and time period start date to end date should be specified for further inspection table 1 shows the available time period of landsat surface reflectance data for each sensor the spatial boundary could be defined as points lines or polygons which can be a kml file uploaded to google drive as google fusion tables gft vector boundaries in esri shapefile format can be easily converted to kml format in commercial e g arcgis or open source e g qgis gis software the spatial coverage can also be defined by geometric drawing using a mouse on the javascript visualization client side webpage about the mosaicking process if the scene based paradigm is chosen in the last step a1 the image collection will be filtered with the cloud cover threshold defined by users the cloud cover property of each image is computed by the automatic cloud cover assessment acca algorithm during the landsat archive process irish et al 2006 if the pixel based paradigm is selected a2 the quality bands would be used for filtering it includes selecting the non filled pixels non saturated pixels cloud cloud shadow snow and clip the edge with a user defined width the quality bands of surface reflectance images include pixel qa which is computed by cfmask algorithm zhu et al 2015 and radsat qa as a per pixel saturation mask details about quality bands can be seen from https landsat usgs gov landsat surface reflectance quality assessment for landsat surface reflectance data after the quasi high quality images produced with a1 or a2 one of the pixel wise mosaic strategies can be selected to make the composited high quality images median mosaic is used to select a median value of all qualified images processed by previous steps in order to ensure no obvious spectral difference between images of multiple years similarly the mean mosaic is to compute the mean value of all bands for each pixel max ndvi is to select the greenest pixels while greatest brightness temperature strategy ensures the low temperature clouds are not included the user defined quality settings are realized in two ways user input and selection the first type is user input params including the spatial boundary of the study area time period cloud cover threshold and clipping length for image edges the other type is user selection params when spatial and temporal extent defined the compositing paradigm should be chosen if pixel based paradigm selected quality bands should be chosen to filter qualified pixels with user defined conditions after quasi high quality images or pixels produced a mosaic strategy could be defined the outputs are the mosaic images produced with user defined parameters for javascript a mosaic image can be visualized on the webpage and exported to google drive without too much effort in the task sub window for python hundreds of small area images can be automatically exported in a batch mode without clicking run in the task window one by one 2 3 software functions table 2 shows the summary of functions and the related descriptions codes for high quality image production are provided in both javascript and python with the name highquality it mainly realizes the preparation of experiment ready mosaic images without too much effort inspecting tools include imagecount and firstyearobs imagecount is used to inspect the number of available images that can be easily visualized for study time extents and areas of interest in order to efficiently support the selection of landsat images for case studies for example researchers can input the latitude and longitude extent in their study area and visualize the availability map to get a first knowledge of the accessibility of good quality landsat data available in different months similarly firstyearobs is used to inspect to which year the available remote sensing images can be traced back the computing power of gee makes it convenient to directly process image data on the platform without downloading data however there are still some user specific applications that need to be processed locally the images need to be downloaded for further processing batchdownload written in python can efficiently realize this the function clips images with vector boundaries i e features and saves those clipped small images to google drive which can then be downloaded in bulk 3 results and discussions this section introduces our software in details which include showing heatmaps for starting year information image count presenting the experiment ready landsat mosaic images and land use classification from the composited images the yangtze river basin with a total drainage area about 1 8 mkm2 is chosen as the case study area to show the results as the longest river in china and asia s greatest water towers threatened by climate change it has great environmental and ecological value mainly with a subtropical monsoon climate it has frequent cloud coverage therefore it is a typical area for high quality cloud free mosaicking a land use application is described shown in fig 2 is the main interface of our software there are three panels the first one is for setting global parameters including the spatial and temporal extent sensors and compositing paradigm the second one is a panel for setting pixel or scene based parameters the third one is a visualization panel for results 3 1 visualization of starting time and scene counts fig 3 shows the landsat 5 image information of main parts of china with heatmaps fig 3 a is the first observation year note that the northeast and east coast of china has the images back to 1984 while the earliest images for the northwest are from 1990 similar maps could be drawn in other parts of the world remote sensing images are an important data source for research and for many applications such as water resource management this function allows researchers to efficiently estimate what time could remote sensing images be traced back to for monitoring water dynamics in their study areas fig 3 b shows the available image counts of landsat 5 images in each pixel only with spatial parameter as input it can be seen that the west of china has a relatively small number of observations since its first observations start rather late note that although the first observations start early the central south still has smaller observations this mainly dues to the data filtering strategy of usgs that the images with too much cloud would be eliminated the central south of china is the most cloudy part of the whole country thus in the following part the yangtze river basin with the central south part included would be specifically chosen for producing yearly high quality composited mosaics and a land use land cover lulc change application 3 2 high quality mosaic and a lulc application the compositing framework is tested in the yangtze river basin shown in fig 4 which covers about 1 8 mkm2 in china fig 4 a is the true color composited mosaic in the year 2013 from the mosaic image retrieved by our software there are no obvious mosaic edges and almost no hue difference in the whole image fig 4 b is a simple application analysis to the composite image with manually selected training samples for each land cover types water urban area vegetation and bareland a supervised classification method proposed by mcdonald et al 2009 is performed on the image this function can be used for long term large scale image acquisition and can benefit studies on landsat image analyses and applications for land use classification lobell et al 2015 midekisa et al 2017 xiong et al 2017a ying et al 2017 it can be used for retrieving base maps of training and testing data for hydrology various water extraction indexes like modified normalized difference water index can be computed with bands of landsat mosaic images then the spatial and multi year temporal distribution of water body could be visualized and extracted under 30 m resolution similar studies with gee can be seen from previous studies feyisa et al 2014 fisher et al 2016 pekel et al 2016 3 3 comparison with previous work table 3 shows the comparison with current datasets i e web enabled landsat data weld and gee landsat composite dataset comparing with those two datasets the software provided by this study supports high quality large scale applications with user defined parameters including spatial coverage temporal extent pixel filtering and compositing strategy therefore it could benefit whoever want to use high quality cloud free landsat images by providing a simple framework and open codes with a flexible user interface 4 conclusions since landsat constitutes the longest record of global scale medium spatial resolution earth observation data it is an ideal data source for long term global scale studies we presented here a large scale experiment ready landsat mosaic image acquisition framework which requires only several user defined parameters such as spatio temporal extent cloud coverage threshold and also provides functions to examine the available image count and starting year of accessibility based on these functions the software has a lot of applications first it is suitable for annual scale large regions or global high quality mosaic image preparation for landsat second it can be used as high quality data sources for subsequent research such as feature extraction image classification and model training and validation whenever a cloud free experiment ready landsat image is needed similarly it can also be used as a preparation for training sample extraction for machine learning methods such as convolution neural network moreover for studies using long term landsat data without downloading any dataset and inspecting them on local computers researchers could inspect landsat data provided by gee and select a study area with enough high quality series images to meet their needs the software is built upon gee therefore its realization depends on the gee platform the composite image quality also greatly relies on the quality bands of landsat images since the revisiting time of landsat is about 16 days our future work would integrate more optical sensor data provided by gee e g sentinel 2 and modis to improve the temporal resolution of optical composite images code availability javascript code can be found and downloaded from 1 image count heatmap https gist github com scarlettlee 1f4864084adfa62dddb33ba346334378 2 first image of year https gist github com scarlettlee 3bf08afba1317f89f89c0668a169247f 3 high quality image production with a user interface https gist github com scarlettlee eda18ca801213429636e7a0764b5b59f python code can be found and downloaded from 1 landsat and output https gist github com scarlettlee 7beb591ba7d61bee06a5af9dd3e86cfb 2 batch clip an image with a feature collection and export to google drive https gist github com scarlettlee d979b30941fe0a27c085d20b50b3430e conflicts of interest the authors declare no conflict of interest acknowledgements this work is jointly supported by the nsfc cgiar project china s water and food security under climate extreme impact risk assessment and resilience grant no 7141101024 and the national natural science foundation of china grant no 91437214 the authors would like to thank mr noel gorelick from google for providing excellent suggestions and the authors would also like to thank developers in gee community for sharing very useful sample codes 
26277,the ability to effectively manage water resources to meet present and future human and environmental needs is essential such an ability necessitates a comprehensive understanding of hydrologic processes that affect streamflow at a watershed scale in the united states water resources management at scales ranging from local to national can benefit from a nationally consistent process based watershed modeling capability to provide the requisite understanding the national hydrologic model nhm infrastructure which was developed by the u s geological survey to support coordinated comprehensive and consistent hydrologic modeling at multiple scales for the conterminous united states provides this essential capability nhm based applications provide information to enable more effective water resources planning and management fill knowledge gaps in ungaged areas and support basic scientific inquiry in the future as process algorithms and data sets improve the nhm infrastructure will continue to evolve to better support the nation s water resources research and management needs keywords national hydrologic model infrastructure precipitation runoff modeling system distributed process based model hydrologic response unit watershed conterminous united states model modeling infrastructure national hydrologic model nhm infrastructure https www youtube com watch v zwoukexj0j4 nhm prms conus application nhm prms for the conus uncalibrated version https doi org 10 5066 p9ushpmj hydrologic simulation code precipitation runoff modeling system prms markstrom et al 2015 regan and lafontaine 2017 https wwwbrr cr usgs gov projects sw mows prms html related software tools geospatial fabric for national hydrologic modeling viger 2014 viger and bock 2014 https wwwbrr cr usgs gov projects sw mows geospatialfabric html usgs geodata portal blodgett et al 2011 https cida usgs gov gdp related data sets prms parameters for nhm prms driscoll et al 2017 https doi org 10 5066 f7ns0scw daily surface weather and climatological summaries daymet thornton et al 2014 https daymet ornl gov contact developer steve regan rsregan usgs gov https wwwbrr cr usgs gov projects sw mows affiliation u s geological surveywater mission areaintegrated modeling and prediction divisionearth systems modeling branch section 3 provides a description of the software and data sources see regan et al 2018 for a detailed description of an application of the prms using the nhm infrastructure that includes descriptions of parameter estimation methods and underlying data sources the prms parameter values and related software have been approved for release by the u s geological survey usgs although the software has been subjected to rigorous review the usgs reserves the right to update the software as needed pursuant to further analysis and review no warranty expressed or implied is made by the usgs or the u s government as to the functionality of the software and related material nor shall the fact of release constitute any such warranty furthermore the software is released on condition that neither the usgs nor the u s government shall be held liable for any damages resulting from its authorized or unauthorized use 1 introduction an understanding of water availability is fundamental for the effective management of water resources to meet present and future human and environmental needs comprehensive water resources management requires an understanding of hydrologic processes that affect streamflow at a watershed scale praskievicz and chang 2009 jha 2011 jarsjo et al 2012 hydrologic models can help provide this requisite process understanding freeze and harlan 1969 shen and phanikumar 2010 paniconi and putti 2015 in addition to their application for estimating the availability timing and contributing sources of streamflow hydrologic models can be used to study spatiotemporal patterns and formulate and test hypotheses that otherwise might be difficult or impossible to investigate fatichi et al 2016 variability of hydrologic processes across spatial and temporal scales can be orders of magnitude and individually processes can be represented at characteristic scales skien et al 2003 hydrologic models require that assumptions be made about space time scales to use the selected simulation algorithms climate forcing information and parameterization methods to aggregate the variability across a model domain and time steps clark et al 2015 samaniego et al 2010 mizukami et al 2017 the aggregation effects across space affect the timing of simulated hydrologic responses differently depending on watershed size for example streamflow tends to increase and decrease in response to precipitation and snowmelt events faster in a small watershed compared to a large watershed having similar physical characteristics for a large watershed hydrologic responses are averaged over a large heterogeneous area consequently a large watershed simulation may provide only limited information about small watershed response even if the latter is a subwatershed within the former kiang et al 2013 effective simulation of hydrologic response for progressively smaller watersheds is only possible to the extent that the resolution i e level of detail of the available data is sufficient management decisions for water resources can be supported by an understanding of hydrologic processes that can be provided by a nationally consistent hydrologic modeling capability in the u s such a capability is essential to provide the understanding and predictive ability necessary to support various national priority objectives including national water assessments water allocation water resource evaluation flood forecasting and adaptation to climate change archfield et al 2015 historically the spatial and temporal scales that could be modeled were predominantly limited by computing resources and data availability today computer resources are less of a limiting factor and consistently derived data sets are available at local regional national and global geographic extents at a variety of temporal frequencies the national hydrologic model nhm infrastructure has been developed and continues to be enhanced by the u s geological survey usgs to support coordinated comprehensive and consistent hydrologic modeling at the watershed scale for the conterminous united states conus the nhm itself is not a model but provides a modeling infrastructure upon which models can be built for scales ranging from the entirety of the conus to single watersheds more specifically the infrastructure is a set of input data content from which applications of a set of configurable hydrologic simulation codes can be built the input data content includes a consistent geospatial discretization of the conus into hydrologic response units hrus and stream segments fig 1 defined by the geospatial fabric gf viger and bock 2014 with associated physical and topographic parameter values viger 2014 hrus are defined as the contributing area to a stream segment that can be divided as left and right bank portions applications of the nhm infrastructure are intended to fill the gap between the detailed local models used in engineering hydrology and global land surface models the discretization provided by hrus in the nhm allows for adequate monitoring measurements such as data from streamgages to calibrate and verify simulations of individual watersheds while also providing meaningful information for applications such as watershed management plans for the public and stakeholders these applications can be used to assess and compare local to national spatial and temporal scale effects of past present and future climate as well as land use and water use on water availability with measures of uncertainty modeling applications based on the nhm infrastructure provide a starting point that can be augmented with local knowledge to improve parameterization and climate information while the nhm infrastructure could be adapted for use with other watershed scale hydrologic simulation codes it is currently configured for use with the daily precipitation runoff modeling system prms markstrom et al 2015 regan and lafontaine 2017 and the monthly water balance model mwbm mccabe and markstrom 2007 the prms as implemented in the nhm is referred to as the nhm prms in this paper a parameter database and model extraction software have been developed to facilitate management of parameter values climate forcing data and model applications for the nhm infrastructure regan et al 2018 see bock and others 2016a 2016b and bock 2017 for descriptions of the mwbm as implemented in the nhm infrastructure the objectives of this paper are to 1 explain the rationale for the nhm infrastructure 2 provide an overview description of the nhm prms including modeling options data sources and parameterization and 3 demonstrate the need for the nhm prms as evidenced by actual and potential applications at local to national scales 2 rationale for the usgs national hydrologic model infrastructure a nationally consistent process based watershed modeling infrastructure is needed to improve understanding of hydrologic processes hay et al 2011 archfield et al 2015 continental extent data sets and consistent parameterization and simulation methods enable truly comparable watershed studies throughout the conus comparison of simulation results for various hydrologic processes and streamflow across the conus regions and watersheds with some degree of confidence is requisite for water resources planning and management decisions for example the ability to generate short and long term predictions of future water availability e g streamflow and storage including variability in response to climate change projections could be used to prioritize watersheds as to the need for water conservation and the development of new sources to help meet various needs the nhm provides a modeling infrastructure for nationally consistent and stakeholder relevant simulations of watershed scale hydrologic processes applications of the nhm prms can provide information for water resources planning and management to help fill knowledge gaps in ungaged areas and to support basic scientific inquiry the nhm infrastructure also allows for locally informed improvements of parameter values spatial features and climate forcings which can then be incorporated back into the nhm prms parameter database any data sets relevant to a locale could be used to derive input parameter values based on local understanding and requirements specific to a given modeling study further there is nothing within the nhm infrastructure that prevents the modification of the hrus or stream segment delineations subdivision is the most common method for revising the spatial discretization to better reflect local conditions because it is easier to leverage default information provided by the gf and the related parameter values a knowledge gap exists concerning streamflow conditions in ungaged areas kiang et al 2013 presented a national streamflow network gap analysis for the usgs streamgage network that study showed that streamflow from about 72 percent of the land area of the conus is gaged when considered as watersheds less than or equal to 20 000 mi2 about 52 000 km2 however 80 percent of the conus land area is ungaged when considered as watersheds less than or equal to 500 mi2 about 1300 km2 in size a higher percentage of land area is gaged in the eastern u s as compared to the central and western u s the gaged land area that is minimally affected by human alterations e g upstream land cover change flow regulation and withdrawals is only 8 6 of the conus fig 2 illustrates gaged land area for four watershed size classes in the conus equipped with a nationally consistent modeling capability the gaps can be filled and more comprehensive and therefore effective management of water resources including hazards will be possible the availability of an accessible nationally consistent hydrologic model will enable investigation of a wide variety of scientific questions in addition such a capability will make feasible conus wide scientific investigations that otherwise might be exceedingly difficult or impossible to pursue a key component of nhm prms applications will be scenario analyses to explore and compare the effects of natural and anthropogenic forcings on hydrologic conditions in different environmental settings there is a fundamental scientific need for hydrologic process investigation for the conus study of individual hydrologic processes and development of techniques to effectively simulate these processes and their interactions with other processes across spatiotemporal scales improves fundamental understanding of hydrology while all models are wrong but some are useful box and draper 1987 understanding where a model is most wrong and useful can allow adaptation of a model to produce a variant that is best for a particular location and set of input data and parameters because the nhm prms includes algorithms for simulating all components of the hydrologic cycle necessary to close the water balance users are afforded the opportunity to isolate and evaluate results for individual processes of interest the nhm prms allows users to select alternate simulation algorithms for some processes such as the computation of potential evapotranspiration pet which has six options plus the option to directly specify pet markstrom et al 2015 thus users can isolate and evaluate the effect of different computational methods on the simulation of different processes and the overall hydrologic response this ability supports the goal for the nhm infrastructure of enabling users to select the best models as represented by best fit to each of the process algorithms for specific locations to get the best possible answers moreover it will be possible to investigate how the relative importance of specific processes and associated parameters varies across the conus for example an nhm prms application was used to investigate the variance of dominant hydrologic processes across the conus based on a sensitivity analysis of model input parameters markstrom et al 2016 the study demonstrated that model complexity can be reduced by focusing parameterization efforts based on the dominant hydrologic processes in an area of interest through knowledge of which parameters are most sensitive for each process the study will contribute to an improved understanding of hydrologic processes across the conus and enable enhancements to the nhm prms reduction of model complexity can be beneficial given that as noted by clark et al 2017 complex models actually may achieve less physical realism in sum the nhm infrastructure supports basic and applied research 3 model description the spatial physiographic and climatological heterogeneity that controls hydrologic response locally and across a model domain needs to be accounted for in watershed modeling flugel 1995 kumar et al 2013 fatichi et al 2016 in addition to the potential for improved simulation of streamflow and other hydrologic fluxes the output from distributed models may be used as input to i e loosely coupled with other environmental models such as water quality and sediment transport moreda et al 2006 the following sections provide an overview of the nhm prms and specific modeling options data sources and model parameterization 3 1 overview the nhm prms includes 1 process representation using the prms hydrologic simulation code 2 a consistent geospatial structure for modeling 3 a database of estimated parameter values 4 climate input variables and 5 model extraction software regan et al 2018 the prms hydrologic simulation code is a modular deterministic physical process watershed model that runs on a daily time step and is used to simulate and assess the effects of various combinations of climate and land use on watershed response markstrom et al 2015 it was developed for water resources research management and planning purposes fig 3 illustrates the nhm infrastructure as applied to the prms the prms was originally released by the usgs in 1983 leavesley et al 1983 and has been revised and enhanced numerous times in response to user requests advances in process understanding and new data sources culminating most recently with the release of version 4 prms iv in 2015 markstrom et al 2015 the main objectives have remained consistent across versions 1 simulation of hydrologic processes including evaporation transpiration runoff infiltration interflow and groundwater flow as determined by the energy and water budgets of the plant canopy snowpack and soil zone on the basis of distributed climate information temperature precipitation and solar radiation 2 simulation of hydrologic water budgets at the watershed scale for temporal scales ranging from days to centuries 3 integration of the prms with other models used for natural resource management or with models from other scientific disciplines and 4 provision of a modular design that allows for selection of alternative hydrologic process algorithms from the standard prms module library markstrom et al 2015 distributed parameter capabilities in the prms are provided by subdividing a watershed into hrus that are based on various physical characteristics of the model domain e g elevation slope vegetation type and soil type for each hru it is assumed that the physical characteristics and hydrologic response are homogeneous daily water and energy balances are computed for each hru several methods for routing are available to simulate daily runoff for the watershed markstrom et al 2015 3 2 data sources the spatial discretization of hrus and stream segments for the nhm infrastructure is provided by the geospatial fabric for national hydrologic modeling gf viger and bock 2014 available online at https wwwbrr cr usgs gov projects sw mows geospatialfabric html additionally a national database for hydrologic modeling applications that consists of gis features and tables of physical and topographic attributes about those features was developed viger 2014 for the conus the gf consists of 109 951 hrus linked to a stream network of 56 459 segments the hrus range in size from 90 m2 to 2000 km2 with a mean of approximately 75 km2 landscape attribute data associated with the hrus includes information on hydrography geology soils land cover and topography climate data for the nhm prms includes precipitation and minimum and maximum temperature these data are currently derived from the daymet gridded data set version 2 thornton et al 2014 with values distributed to the hrus using the usgs geo data portal https cida usgs gov gdp blodgett et al 2011 users can substitute alternative climate data and distribution methods using any of the available prms climate input options markstrom et al 2015 3 3 parameterization initial spatially distributed parameter values were computed using national data sets and consistent methodologies markstrom et al 2015 regan et al 2018 the complete set of these initial parameter values for the nhm prms is maintained in the national hydrologic model parameter database nhmparamdb driscoll et al 2017 for detailed information on initial nhm prms parameter derivations and a complete description of all parameters see regan et al 2018 figs 4 and 5 provide animations of simulation results from the initial parameterization of the nhm prms for daily streamflow and lateral flow to the stream network respectively based on a 26 year simulation for the time period 1990 10 01 2016 12 31 the tabular output for the variables used to produce figs 4 and 5 can be accessed at driscoll et al 2018 a 10 year animation water years 2007 2016 of simulated streamflow is shown in fig 4 all values are classified by percentile of the simulated time series of streamflow segment by segment this animation reveals spatial temporal patterns in the simulation of streamflow with general persistence of flow conditions based on region and season and with individual events causing flashiness that diverges from those longer term conditions a 10 year animation water years 2007 2016 of simulated daily mean lateral flow from each hru to the stream network is shown in fig 5 lateral flow is the sum of surface runoff interflow and groundwater flow generated on a daily basis from each hru the values are normalized by dividing by the maximum simulated lateral flow value for each respective hru over the 10 years this animation reveals several spatiotemporal patterns in the simulation of lateral inflow the more mountainous regions exhibit a more lingering response because of slow periods of snowmelt in the spring and summer much quicker response is seen in non mountainous regions in response to more rain dominated weather events in the pacific northwest simulated rain on snow events are common an automated capability is available for extracting subsets from the nhm prms subsets include all of the input files required to run a model for any area of interest in the conus regan et al 2018 the extracted parameters should be evaluated in the context of local knowledge which may dictate the need for parameter refinement for example hay et al 2018 used a model extracted from the nhm infrastructure for the upper pipestem creek basin in the prairie pothole region of north dakota the model was used to improve understanding of surface water depression storage dynamics within the basin by using local knowledge to optimize selected default prms parameters extracted from the nhm infrastructure model subsets of the nhm prms can be requested via the help link at https wwwbrr cr usgs gov projects sw mows see regan et al 2018 for further details on requesting nhm prms model subsets 4 evolution of the prms through applications in this section the objectives are twofold first to provide a representative sample of completed prms applications second to discuss potential future applications for which the nhm prms is well suited historically the prms has been used throughout the conus for applications ranging in size from less than 100 km2 to tens of thousands of km2 and larger it has been used to investigate various topics for water resources planning and management including but not limited to water availability water quality snowpack groundwater recharge aquatic habitat and urbanization see table 1 in markstrom et al 2015 for a list of selected applications of the prms an understanding of streamflow characteristics e g magnitude timing and contributing sources is fundamental for water resources management and can be accomplished through modeling to support water resources planning management and allocation jeton 2000 used the prms to simulate flow in the truckee river basin california and nevada streamflow in the feather river basin california was effectively simulated by koczot et al 2005 to support inflow predictions for and management of lake oroville in a comparison of subbasin streamflow in the upper and lower colorado river basin fassnacht 2006 determined that an uncalibrated prms model approximated the magnitude and timing of peak flows however the receding limb and base flow components of the hydrograph typically were not well simulated walker et al 2011 examined the sensitivity and variability of freshwater resources in the conus by projecting climate change effects on the 1 5 year flood for basins located in 14 different hydroclimatic regions the potential effects of climate change and land cover change on hydrologic conditions is an active area of research in which prms modeling has been widely applied in a study of potential effects of climate and land use changes on monthly streamflow of the trent river basin north carolina qi et al 2009 determined that streamflow was more sensitive to changes in precipitation than temperature and that projected land use changes may increase water yield from the basin for the willamette river basin oregon climate change was projected to result in an increased seasonal variability in runoff with increases in fall and winter and decreases in spring and summer chang and jung 2010 jung and chang 2011 in a study of the flint river basin georgia viger et al 2011 demonstrated that potential future decreases in streamflow caused by climate change may be offset by increases associated with urbanization hay et al 2011 in a study of 14 selected basins in the conus found that the hydrologic response to climate change may vary considerably between regions depending on the dominant physical processes to provide a decision support tool for management of the great lakes christiansen et al 2014 simulated hydrologic conditions in the lake michigan basin to help answer questions about restoration alternatives in relation to possible future climate change lafontaine et al 2015 in a simulation to assess the effects of climate and land cover change on the hydrology of the apalachicola chattahoochee flint river basin georgia alabama florida demonstrated the potential of stormwater management practices to mitigate the effects of land cover change on streamflow as part of the lafontaine et al 2015 study the functionality of the prms was enhanced to accommodate the input of dynamic parameters to enable the incorporation of time series of historical current and projected climate and landscape characteristics to evaluate trends and impacts regan and lafontaine 2017 van beusekom et al 2014 used dynamically changing land cover to quantify effects on streamflow in puerto rico as compared to static parameterization of a prms model the study demonstrated that model streamflow uncertainty through time can be reduced when dynamic land cover is incorporated in simulations of highly altered watersheds where spring snowmelt accounts for much of the annual water supply as is the case in the western conus the potential effect of climate change on snowpack is a major concern for water resource planners day 2009 future changes in snowpack were projected for eight basins seven in the western u s one in maine by mastin et al 2011 for all eight basins the models indicated a future of declining spring snowpack with a seasonal redistribution of streamflow increased winter flows decreased summer flows likewise in a study of the east and yampa river basins colorado battaglin et al 2011 projected declining snowpacks which may adversely impact ski areas in addition to water supplies in their study the ability to use the prms to simulate and compare watersheds was demonstrated knowledge of recharge rates is fundamental for groundwater resources management cherkauer 2004 demonstrated the use of the prms to produce accurate recharge estimates for seven watersheds in wisconsin in a study including the connecticut thames and housatonic river basins new england simulations by bjerklie et al 2011 indicated a possible future increase in recharge throughout much of the modeled region fulton et al 2015 used the prms coupled with a groundwater flow model gsflow markstrom et al 2008 to simulate a water budget and identify areas of greater recharge for the spring creek basin pennsylvania the prms has been applied in various water quality studies as part of a study to examine land use effects on water quality nitrate total suspended solids in the cosumnes river basin california ahearn et al 2005 estimated daily flows and constituent loads for 28 subbasins climate change effects on phosphorus loading to lake michigan were simulated by robertson et al 2016 using a watershed modeling approach that linked results from the prms with the sparrow transport and fate model robertson and saad 2011 the effects of land use and climate change on stream temperature were simulated for the upper tar river basin north carolina by daraio and bales 2014 in their study in which the prms was coupled to the stream temperature network model sntemp theurer et al 1984 markstrom 2012 to forecast daily mean stream temperature it was indicated that the combined effects of land use and climate change were not additive and therefore should be simulated simultaneously implications of using statistically downscaled general circulation model gcm climate simulations of historical conditions to drive the prms and sntemp were assessed for the apalachicola chattahoochee flint river basin by hay et al 2014 the study concluded that the prms and sntemp outputs based on downscaled data may not be reliable for studies requiring analyses on daily or even weekly time scales buccola et al 2016 used the prms with the ce qual w2 hydrodynamic and water quality model cole and wells 2011 to investigate climate change effects on water temperature in the north santiam river oregon habitat assessment is another topic for which the prms has been applied gibson et al 2005 examined potential climate change effects on ecologically relevant aspects of the flow regime for the cle elum river washington findings of ecological concern included a substantial shift in the timing of peak flows and reduced low flows of longer duration to help support protection efforts for atlantic salmon dudley 2008 characterized flow quantity timing and variability in the dennys river basin maine chase et al 2016 simulated flow in seven montana watersheds to assess potential effects of climate change on prairie streams and provide information relevant for the future management of such habitats simulation results were used to assess fish population dynamics in the potato creek basin georgia freeman et al 2013 several potential water resources management and research applications would be enabled by or benefit from the modeling framework provided by the nhm prms examples include projections of future hydrologic conditions hydrologic classification of basins and hypothesis formulation and testing water resources planning and management for an uncertain future will require projections of hydrologic conditions for different scenarios e g climate change at multiple scales and for monitored and unmonitored areas using the consistent methodology afforded by the nhm prms projections of hydrologic conditions are achievable for basins at scales ranging from local to national for example in the western conus projections of the availability and timing of snowmelt runoff are needed to provide important information for various aspects of management including decisions pertaining to water use water allocation among competing interests and interbasin transfers a classification system for basins is needed to advance hydrologic science wagener et al 2007 sivakumar et al 2013 benefits of such a classification would include provision of an organizing principle a common language to improve communication guidance for modeling and monitoring and a foundation for hypothesis testing prediction and decision making mcdonnell and woods 2004 wagener et al 2007 sivakumar et al 2013 for the conus the nhm prms offers the potential to make important contributions toward basin classification using hydrologic response as the primary criterion such a classification would enable several research questions to be addressed including 1 how does hydrologic response vary with physical environment 2 how will basins in different hydrologic response classes respond to climate change 3 how does hydrologic process dominance vary between basins of different response classes and with scale for basins of a given response class answers to these questions will improve our understanding of hydrologic processes and have important implications for water resources planning and management process based models such as the prms can effectively serve as virtual laboratories to formulate and test hypotheses that would otherwise be difficult or impossible to investigate fatichi et al 2016 various research topics with management implications would benefit from such virtual experiments including studies designed to investigate sensitivity of basins to climate and land use change potential runoff contributing areas as affected by climate and land use change variability of hydrologic process dominance as affected by basin size location and climate and land use change and effects of anthropogenic forcings on aquatic habitat availability and quality as quantified by ecologically relevant flow metrics 5 discussion as noted by archfield et al 2015 there is convergence in the hydrologic modeling communities catchment global water security and land surface modeling toward development and application of conus domain hydrologic models while these communities display a large diversity in their modeling approaches this should prove to advance rather than hinder the improvement of process based hydrologic models clark et al 2017 the nhm prms is an example of conus extent model development within the catchment modeling community with national e g usgs water availability and use science program wausp https water usgs gov wausp regional e g southeastern u s lafontaine et al 2015 and local applications e g usgs water science centers wscs national applications are able to explore the parameter space addressing continental domain parameter estimation and water budget characterization for a wide range of climate and landscape settings what is learned from these studies not only enhances the nhm infrastructure but can be applied to other conus domain applications regional applications provide proof of concept test beds for local and national scale developments local applications within the usgs wscs can improve upon parameter estimation and other model inputs with local knowledge and provide insight into process representation resulting in algorithm improvements this combination of national to local scale research with the nhm infrastructure will further identify areas where alternative hydrologic process algorithms additional data collection or finer resolution spatial units or temporal time steps may be required to meet operational goals a schematic of this iterative process to support continuous improvement of nhm infrastructure capabilities and data is illustrated in fig 6 for the research to operations concept shown in fig 6 to effectively lead to better representations of the hydrologic cycle interactions between data collectors data synthesizers and data users must occur with usgs already having a strong network for the first two groups usgs field technicians and scientists are located in offices across all 50 states and u s territories collecting and interpreting data using standard methods and training to provide consistent and unbiased data and results cooperation with personnel across the scientific community can further enhance these data collection and evaluation efforts data users e g resource managers and stakeholders provide valuable input as to what information is most important for their particular applications feedback describing what is needed to further the development of the nhm infrastructure should occur amongst the three groups to provide the best possible national and local scale hydrologic information the usgs reputation of consistently collecting high quality unbiased data at the national scale needs to apply to modeling applications as well with a consistently designed simulation infrastructure such as the nhm combined with the wealth of hydrologic information collected by usgs over the past 100 years the agency is uniquely poised to be a leader in integrated modeling to better capture the dynamics of the interaction between the hydrologic cycle and its influences hydrologic simulations must move beyond static and natural characterizations of the landscape representation of dynamically changing land cover water use and other anthropogenic influences e g on and off stream impoundments population growth or migration and their effect on hydrologic response can be important for management and planning of water resources at local to national scales luo et al 2012 describe the effects of conditional parameterization on improving streamflow prediction with a hydrologic model the nhm prms has functionality to simulate the effects of these types of forcings on the hydrologic cycle but data to sufficiently parameterize the models are often unavailable the usgs wausp has invested in efforts to populate databases with site specific water use estimates that allow for improved incorporation of these types of data into hydrologic applications sohl et al 2014 2016 have developed land cover change products that provide annual time step realizations of historical and potential future land cover distributions the challenge then is how we incorporate these types of data into the nhm infrastructure in a robust and consistent way that is compatible with pre existing parameterizations based on more widely accepted products such as the national land cover database homer et al 2007 by adapting the nhm infrastructure to accommodate these additional types of information more data sets of various types and characteristics can then be incorporated to potentially improve hydrologic simulations and reduce model uncertainty to continue improvement of the nhm prms consistent and comprehensive evaluation metrics must be applied for the conus extent historically various combinations of statistical measures e g nash sutcliffe efficiency index percent bias standard error and normalized root mean square error have been used to evaluate the goodness of fit of streamflow estimates from hydrologic models however only evaluating streamflow and with only one or a few of these metrics may not fully reflect the robustness of a hydrologic model in addition target calibration data sets may have biases and uncertainties that confound the representation of the components e g precipitation snowmelt evapotranspiration or streamflow and closure of the water budget robust evaluation and synthesis of diverse hydrologic data sets and simulations should provide results that are generalizable and transferrable across model infrastructures and spatial and temporal scales several conus scale hydrologic modeling applications including the national weather service s national water model nwm have been developed at various spatial and temporal scales and process representation choices wood et al 1997 arnold et al 1999 maurer et al 2002 yu et al 2006 maxwell et al 2015 bock et al 2016a maidment 2017 the impetus for development of these models varies as does their applicability to address an issue many factors come into play when deciding whether to select an existing model or go in a new direction kauffeldt et al 2016 describe eight aspects for the implementation of a hydrological model on a conus scale that are quite useful guidelines for selecting a modeling direction their list is 1 availability of the code as open source or through agreements to allow for code adaptation 2 existing user community to ensure support and development activities 3 input data requirements can be met from existing databases 4 flexibility to use gridded input data 5 possibility of calibration with suitable tools wood et al 1998 6 flexibility in resolution for straightforward down up scaling 7 facility for data assimilation to update the model and 8 existing large domain model set up the nhm infrastructure provides these aspects note though the prms has input options to update some model states and fluxes the nhm infrastructure is designed to maintain the water balance in process representations and is not recommended for simulation needs that require data assimilation the use of data assimilation is a method used in short term forecasting applications such as the nwm maidment 2017 http water noaa gov about nwm and other hydrologic modeling to address bias due to uncertainty in model structure parameter estimation forcing data and boundary conditions moradkhani et al 2005 rasmussen et al 2016 6 summary and conclusions effective water resources management to meet present and future human and environmental needs in a changing world necessarily requires a comprehensive understanding of hydrologic processes that affect streamflow at a watershed scale because water resources management in the u s is an important issue at all scales local to national a nationally consistent process based watershed modeling capability is a logical choice to provide the requisite understanding of hydrologic processes such a capability the nhm prms has been developed and continually enhanced by the usgs to support coordinated comprehensive and consistent hydrologic modeling at multiple scales for the conus nhm based applications provide information to enable more effective water resources planning and management to help fill knowledge gaps in ungaged areas to support basic scientific inquiry and to facilitate the progression from research to operations in the future as process algorithms and data sets improve so too will the nhm infrastructure continue to evolve to better support the nation s water resources research and management needs acknowledgments the authors acknowledge the collaboration of many colleagues across the united states over many years from u s geological survey water science centers other federal agencies and academia the authors also acknowledge that this science product is the result of long term research and development and that the privilege to pursue it was afforded by the u s geological survey national research program finally we thank our anonymous reviewers and our colleague reviewers at the u s geological survey any use of trade product or firm names is for descriptive purposes only and does not imply endorsement by the u s government appendix a supplementary data the following are the supplementary data to this article nhm seg outflow nhm seg outflow nhru lateral flow 2006 s nhru lateral flow 2006 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 09 023 
26277,the ability to effectively manage water resources to meet present and future human and environmental needs is essential such an ability necessitates a comprehensive understanding of hydrologic processes that affect streamflow at a watershed scale in the united states water resources management at scales ranging from local to national can benefit from a nationally consistent process based watershed modeling capability to provide the requisite understanding the national hydrologic model nhm infrastructure which was developed by the u s geological survey to support coordinated comprehensive and consistent hydrologic modeling at multiple scales for the conterminous united states provides this essential capability nhm based applications provide information to enable more effective water resources planning and management fill knowledge gaps in ungaged areas and support basic scientific inquiry in the future as process algorithms and data sets improve the nhm infrastructure will continue to evolve to better support the nation s water resources research and management needs keywords national hydrologic model infrastructure precipitation runoff modeling system distributed process based model hydrologic response unit watershed conterminous united states model modeling infrastructure national hydrologic model nhm infrastructure https www youtube com watch v zwoukexj0j4 nhm prms conus application nhm prms for the conus uncalibrated version https doi org 10 5066 p9ushpmj hydrologic simulation code precipitation runoff modeling system prms markstrom et al 2015 regan and lafontaine 2017 https wwwbrr cr usgs gov projects sw mows prms html related software tools geospatial fabric for national hydrologic modeling viger 2014 viger and bock 2014 https wwwbrr cr usgs gov projects sw mows geospatialfabric html usgs geodata portal blodgett et al 2011 https cida usgs gov gdp related data sets prms parameters for nhm prms driscoll et al 2017 https doi org 10 5066 f7ns0scw daily surface weather and climatological summaries daymet thornton et al 2014 https daymet ornl gov contact developer steve regan rsregan usgs gov https wwwbrr cr usgs gov projects sw mows affiliation u s geological surveywater mission areaintegrated modeling and prediction divisionearth systems modeling branch section 3 provides a description of the software and data sources see regan et al 2018 for a detailed description of an application of the prms using the nhm infrastructure that includes descriptions of parameter estimation methods and underlying data sources the prms parameter values and related software have been approved for release by the u s geological survey usgs although the software has been subjected to rigorous review the usgs reserves the right to update the software as needed pursuant to further analysis and review no warranty expressed or implied is made by the usgs or the u s government as to the functionality of the software and related material nor shall the fact of release constitute any such warranty furthermore the software is released on condition that neither the usgs nor the u s government shall be held liable for any damages resulting from its authorized or unauthorized use 1 introduction an understanding of water availability is fundamental for the effective management of water resources to meet present and future human and environmental needs comprehensive water resources management requires an understanding of hydrologic processes that affect streamflow at a watershed scale praskievicz and chang 2009 jha 2011 jarsjo et al 2012 hydrologic models can help provide this requisite process understanding freeze and harlan 1969 shen and phanikumar 2010 paniconi and putti 2015 in addition to their application for estimating the availability timing and contributing sources of streamflow hydrologic models can be used to study spatiotemporal patterns and formulate and test hypotheses that otherwise might be difficult or impossible to investigate fatichi et al 2016 variability of hydrologic processes across spatial and temporal scales can be orders of magnitude and individually processes can be represented at characteristic scales skien et al 2003 hydrologic models require that assumptions be made about space time scales to use the selected simulation algorithms climate forcing information and parameterization methods to aggregate the variability across a model domain and time steps clark et al 2015 samaniego et al 2010 mizukami et al 2017 the aggregation effects across space affect the timing of simulated hydrologic responses differently depending on watershed size for example streamflow tends to increase and decrease in response to precipitation and snowmelt events faster in a small watershed compared to a large watershed having similar physical characteristics for a large watershed hydrologic responses are averaged over a large heterogeneous area consequently a large watershed simulation may provide only limited information about small watershed response even if the latter is a subwatershed within the former kiang et al 2013 effective simulation of hydrologic response for progressively smaller watersheds is only possible to the extent that the resolution i e level of detail of the available data is sufficient management decisions for water resources can be supported by an understanding of hydrologic processes that can be provided by a nationally consistent hydrologic modeling capability in the u s such a capability is essential to provide the understanding and predictive ability necessary to support various national priority objectives including national water assessments water allocation water resource evaluation flood forecasting and adaptation to climate change archfield et al 2015 historically the spatial and temporal scales that could be modeled were predominantly limited by computing resources and data availability today computer resources are less of a limiting factor and consistently derived data sets are available at local regional national and global geographic extents at a variety of temporal frequencies the national hydrologic model nhm infrastructure has been developed and continues to be enhanced by the u s geological survey usgs to support coordinated comprehensive and consistent hydrologic modeling at the watershed scale for the conterminous united states conus the nhm itself is not a model but provides a modeling infrastructure upon which models can be built for scales ranging from the entirety of the conus to single watersheds more specifically the infrastructure is a set of input data content from which applications of a set of configurable hydrologic simulation codes can be built the input data content includes a consistent geospatial discretization of the conus into hydrologic response units hrus and stream segments fig 1 defined by the geospatial fabric gf viger and bock 2014 with associated physical and topographic parameter values viger 2014 hrus are defined as the contributing area to a stream segment that can be divided as left and right bank portions applications of the nhm infrastructure are intended to fill the gap between the detailed local models used in engineering hydrology and global land surface models the discretization provided by hrus in the nhm allows for adequate monitoring measurements such as data from streamgages to calibrate and verify simulations of individual watersheds while also providing meaningful information for applications such as watershed management plans for the public and stakeholders these applications can be used to assess and compare local to national spatial and temporal scale effects of past present and future climate as well as land use and water use on water availability with measures of uncertainty modeling applications based on the nhm infrastructure provide a starting point that can be augmented with local knowledge to improve parameterization and climate information while the nhm infrastructure could be adapted for use with other watershed scale hydrologic simulation codes it is currently configured for use with the daily precipitation runoff modeling system prms markstrom et al 2015 regan and lafontaine 2017 and the monthly water balance model mwbm mccabe and markstrom 2007 the prms as implemented in the nhm is referred to as the nhm prms in this paper a parameter database and model extraction software have been developed to facilitate management of parameter values climate forcing data and model applications for the nhm infrastructure regan et al 2018 see bock and others 2016a 2016b and bock 2017 for descriptions of the mwbm as implemented in the nhm infrastructure the objectives of this paper are to 1 explain the rationale for the nhm infrastructure 2 provide an overview description of the nhm prms including modeling options data sources and parameterization and 3 demonstrate the need for the nhm prms as evidenced by actual and potential applications at local to national scales 2 rationale for the usgs national hydrologic model infrastructure a nationally consistent process based watershed modeling infrastructure is needed to improve understanding of hydrologic processes hay et al 2011 archfield et al 2015 continental extent data sets and consistent parameterization and simulation methods enable truly comparable watershed studies throughout the conus comparison of simulation results for various hydrologic processes and streamflow across the conus regions and watersheds with some degree of confidence is requisite for water resources planning and management decisions for example the ability to generate short and long term predictions of future water availability e g streamflow and storage including variability in response to climate change projections could be used to prioritize watersheds as to the need for water conservation and the development of new sources to help meet various needs the nhm provides a modeling infrastructure for nationally consistent and stakeholder relevant simulations of watershed scale hydrologic processes applications of the nhm prms can provide information for water resources planning and management to help fill knowledge gaps in ungaged areas and to support basic scientific inquiry the nhm infrastructure also allows for locally informed improvements of parameter values spatial features and climate forcings which can then be incorporated back into the nhm prms parameter database any data sets relevant to a locale could be used to derive input parameter values based on local understanding and requirements specific to a given modeling study further there is nothing within the nhm infrastructure that prevents the modification of the hrus or stream segment delineations subdivision is the most common method for revising the spatial discretization to better reflect local conditions because it is easier to leverage default information provided by the gf and the related parameter values a knowledge gap exists concerning streamflow conditions in ungaged areas kiang et al 2013 presented a national streamflow network gap analysis for the usgs streamgage network that study showed that streamflow from about 72 percent of the land area of the conus is gaged when considered as watersheds less than or equal to 20 000 mi2 about 52 000 km2 however 80 percent of the conus land area is ungaged when considered as watersheds less than or equal to 500 mi2 about 1300 km2 in size a higher percentage of land area is gaged in the eastern u s as compared to the central and western u s the gaged land area that is minimally affected by human alterations e g upstream land cover change flow regulation and withdrawals is only 8 6 of the conus fig 2 illustrates gaged land area for four watershed size classes in the conus equipped with a nationally consistent modeling capability the gaps can be filled and more comprehensive and therefore effective management of water resources including hazards will be possible the availability of an accessible nationally consistent hydrologic model will enable investigation of a wide variety of scientific questions in addition such a capability will make feasible conus wide scientific investigations that otherwise might be exceedingly difficult or impossible to pursue a key component of nhm prms applications will be scenario analyses to explore and compare the effects of natural and anthropogenic forcings on hydrologic conditions in different environmental settings there is a fundamental scientific need for hydrologic process investigation for the conus study of individual hydrologic processes and development of techniques to effectively simulate these processes and their interactions with other processes across spatiotemporal scales improves fundamental understanding of hydrology while all models are wrong but some are useful box and draper 1987 understanding where a model is most wrong and useful can allow adaptation of a model to produce a variant that is best for a particular location and set of input data and parameters because the nhm prms includes algorithms for simulating all components of the hydrologic cycle necessary to close the water balance users are afforded the opportunity to isolate and evaluate results for individual processes of interest the nhm prms allows users to select alternate simulation algorithms for some processes such as the computation of potential evapotranspiration pet which has six options plus the option to directly specify pet markstrom et al 2015 thus users can isolate and evaluate the effect of different computational methods on the simulation of different processes and the overall hydrologic response this ability supports the goal for the nhm infrastructure of enabling users to select the best models as represented by best fit to each of the process algorithms for specific locations to get the best possible answers moreover it will be possible to investigate how the relative importance of specific processes and associated parameters varies across the conus for example an nhm prms application was used to investigate the variance of dominant hydrologic processes across the conus based on a sensitivity analysis of model input parameters markstrom et al 2016 the study demonstrated that model complexity can be reduced by focusing parameterization efforts based on the dominant hydrologic processes in an area of interest through knowledge of which parameters are most sensitive for each process the study will contribute to an improved understanding of hydrologic processes across the conus and enable enhancements to the nhm prms reduction of model complexity can be beneficial given that as noted by clark et al 2017 complex models actually may achieve less physical realism in sum the nhm infrastructure supports basic and applied research 3 model description the spatial physiographic and climatological heterogeneity that controls hydrologic response locally and across a model domain needs to be accounted for in watershed modeling flugel 1995 kumar et al 2013 fatichi et al 2016 in addition to the potential for improved simulation of streamflow and other hydrologic fluxes the output from distributed models may be used as input to i e loosely coupled with other environmental models such as water quality and sediment transport moreda et al 2006 the following sections provide an overview of the nhm prms and specific modeling options data sources and model parameterization 3 1 overview the nhm prms includes 1 process representation using the prms hydrologic simulation code 2 a consistent geospatial structure for modeling 3 a database of estimated parameter values 4 climate input variables and 5 model extraction software regan et al 2018 the prms hydrologic simulation code is a modular deterministic physical process watershed model that runs on a daily time step and is used to simulate and assess the effects of various combinations of climate and land use on watershed response markstrom et al 2015 it was developed for water resources research management and planning purposes fig 3 illustrates the nhm infrastructure as applied to the prms the prms was originally released by the usgs in 1983 leavesley et al 1983 and has been revised and enhanced numerous times in response to user requests advances in process understanding and new data sources culminating most recently with the release of version 4 prms iv in 2015 markstrom et al 2015 the main objectives have remained consistent across versions 1 simulation of hydrologic processes including evaporation transpiration runoff infiltration interflow and groundwater flow as determined by the energy and water budgets of the plant canopy snowpack and soil zone on the basis of distributed climate information temperature precipitation and solar radiation 2 simulation of hydrologic water budgets at the watershed scale for temporal scales ranging from days to centuries 3 integration of the prms with other models used for natural resource management or with models from other scientific disciplines and 4 provision of a modular design that allows for selection of alternative hydrologic process algorithms from the standard prms module library markstrom et al 2015 distributed parameter capabilities in the prms are provided by subdividing a watershed into hrus that are based on various physical characteristics of the model domain e g elevation slope vegetation type and soil type for each hru it is assumed that the physical characteristics and hydrologic response are homogeneous daily water and energy balances are computed for each hru several methods for routing are available to simulate daily runoff for the watershed markstrom et al 2015 3 2 data sources the spatial discretization of hrus and stream segments for the nhm infrastructure is provided by the geospatial fabric for national hydrologic modeling gf viger and bock 2014 available online at https wwwbrr cr usgs gov projects sw mows geospatialfabric html additionally a national database for hydrologic modeling applications that consists of gis features and tables of physical and topographic attributes about those features was developed viger 2014 for the conus the gf consists of 109 951 hrus linked to a stream network of 56 459 segments the hrus range in size from 90 m2 to 2000 km2 with a mean of approximately 75 km2 landscape attribute data associated with the hrus includes information on hydrography geology soils land cover and topography climate data for the nhm prms includes precipitation and minimum and maximum temperature these data are currently derived from the daymet gridded data set version 2 thornton et al 2014 with values distributed to the hrus using the usgs geo data portal https cida usgs gov gdp blodgett et al 2011 users can substitute alternative climate data and distribution methods using any of the available prms climate input options markstrom et al 2015 3 3 parameterization initial spatially distributed parameter values were computed using national data sets and consistent methodologies markstrom et al 2015 regan et al 2018 the complete set of these initial parameter values for the nhm prms is maintained in the national hydrologic model parameter database nhmparamdb driscoll et al 2017 for detailed information on initial nhm prms parameter derivations and a complete description of all parameters see regan et al 2018 figs 4 and 5 provide animations of simulation results from the initial parameterization of the nhm prms for daily streamflow and lateral flow to the stream network respectively based on a 26 year simulation for the time period 1990 10 01 2016 12 31 the tabular output for the variables used to produce figs 4 and 5 can be accessed at driscoll et al 2018 a 10 year animation water years 2007 2016 of simulated streamflow is shown in fig 4 all values are classified by percentile of the simulated time series of streamflow segment by segment this animation reveals spatial temporal patterns in the simulation of streamflow with general persistence of flow conditions based on region and season and with individual events causing flashiness that diverges from those longer term conditions a 10 year animation water years 2007 2016 of simulated daily mean lateral flow from each hru to the stream network is shown in fig 5 lateral flow is the sum of surface runoff interflow and groundwater flow generated on a daily basis from each hru the values are normalized by dividing by the maximum simulated lateral flow value for each respective hru over the 10 years this animation reveals several spatiotemporal patterns in the simulation of lateral inflow the more mountainous regions exhibit a more lingering response because of slow periods of snowmelt in the spring and summer much quicker response is seen in non mountainous regions in response to more rain dominated weather events in the pacific northwest simulated rain on snow events are common an automated capability is available for extracting subsets from the nhm prms subsets include all of the input files required to run a model for any area of interest in the conus regan et al 2018 the extracted parameters should be evaluated in the context of local knowledge which may dictate the need for parameter refinement for example hay et al 2018 used a model extracted from the nhm infrastructure for the upper pipestem creek basin in the prairie pothole region of north dakota the model was used to improve understanding of surface water depression storage dynamics within the basin by using local knowledge to optimize selected default prms parameters extracted from the nhm infrastructure model subsets of the nhm prms can be requested via the help link at https wwwbrr cr usgs gov projects sw mows see regan et al 2018 for further details on requesting nhm prms model subsets 4 evolution of the prms through applications in this section the objectives are twofold first to provide a representative sample of completed prms applications second to discuss potential future applications for which the nhm prms is well suited historically the prms has been used throughout the conus for applications ranging in size from less than 100 km2 to tens of thousands of km2 and larger it has been used to investigate various topics for water resources planning and management including but not limited to water availability water quality snowpack groundwater recharge aquatic habitat and urbanization see table 1 in markstrom et al 2015 for a list of selected applications of the prms an understanding of streamflow characteristics e g magnitude timing and contributing sources is fundamental for water resources management and can be accomplished through modeling to support water resources planning management and allocation jeton 2000 used the prms to simulate flow in the truckee river basin california and nevada streamflow in the feather river basin california was effectively simulated by koczot et al 2005 to support inflow predictions for and management of lake oroville in a comparison of subbasin streamflow in the upper and lower colorado river basin fassnacht 2006 determined that an uncalibrated prms model approximated the magnitude and timing of peak flows however the receding limb and base flow components of the hydrograph typically were not well simulated walker et al 2011 examined the sensitivity and variability of freshwater resources in the conus by projecting climate change effects on the 1 5 year flood for basins located in 14 different hydroclimatic regions the potential effects of climate change and land cover change on hydrologic conditions is an active area of research in which prms modeling has been widely applied in a study of potential effects of climate and land use changes on monthly streamflow of the trent river basin north carolina qi et al 2009 determined that streamflow was more sensitive to changes in precipitation than temperature and that projected land use changes may increase water yield from the basin for the willamette river basin oregon climate change was projected to result in an increased seasonal variability in runoff with increases in fall and winter and decreases in spring and summer chang and jung 2010 jung and chang 2011 in a study of the flint river basin georgia viger et al 2011 demonstrated that potential future decreases in streamflow caused by climate change may be offset by increases associated with urbanization hay et al 2011 in a study of 14 selected basins in the conus found that the hydrologic response to climate change may vary considerably between regions depending on the dominant physical processes to provide a decision support tool for management of the great lakes christiansen et al 2014 simulated hydrologic conditions in the lake michigan basin to help answer questions about restoration alternatives in relation to possible future climate change lafontaine et al 2015 in a simulation to assess the effects of climate and land cover change on the hydrology of the apalachicola chattahoochee flint river basin georgia alabama florida demonstrated the potential of stormwater management practices to mitigate the effects of land cover change on streamflow as part of the lafontaine et al 2015 study the functionality of the prms was enhanced to accommodate the input of dynamic parameters to enable the incorporation of time series of historical current and projected climate and landscape characteristics to evaluate trends and impacts regan and lafontaine 2017 van beusekom et al 2014 used dynamically changing land cover to quantify effects on streamflow in puerto rico as compared to static parameterization of a prms model the study demonstrated that model streamflow uncertainty through time can be reduced when dynamic land cover is incorporated in simulations of highly altered watersheds where spring snowmelt accounts for much of the annual water supply as is the case in the western conus the potential effect of climate change on snowpack is a major concern for water resource planners day 2009 future changes in snowpack were projected for eight basins seven in the western u s one in maine by mastin et al 2011 for all eight basins the models indicated a future of declining spring snowpack with a seasonal redistribution of streamflow increased winter flows decreased summer flows likewise in a study of the east and yampa river basins colorado battaglin et al 2011 projected declining snowpacks which may adversely impact ski areas in addition to water supplies in their study the ability to use the prms to simulate and compare watersheds was demonstrated knowledge of recharge rates is fundamental for groundwater resources management cherkauer 2004 demonstrated the use of the prms to produce accurate recharge estimates for seven watersheds in wisconsin in a study including the connecticut thames and housatonic river basins new england simulations by bjerklie et al 2011 indicated a possible future increase in recharge throughout much of the modeled region fulton et al 2015 used the prms coupled with a groundwater flow model gsflow markstrom et al 2008 to simulate a water budget and identify areas of greater recharge for the spring creek basin pennsylvania the prms has been applied in various water quality studies as part of a study to examine land use effects on water quality nitrate total suspended solids in the cosumnes river basin california ahearn et al 2005 estimated daily flows and constituent loads for 28 subbasins climate change effects on phosphorus loading to lake michigan were simulated by robertson et al 2016 using a watershed modeling approach that linked results from the prms with the sparrow transport and fate model robertson and saad 2011 the effects of land use and climate change on stream temperature were simulated for the upper tar river basin north carolina by daraio and bales 2014 in their study in which the prms was coupled to the stream temperature network model sntemp theurer et al 1984 markstrom 2012 to forecast daily mean stream temperature it was indicated that the combined effects of land use and climate change were not additive and therefore should be simulated simultaneously implications of using statistically downscaled general circulation model gcm climate simulations of historical conditions to drive the prms and sntemp were assessed for the apalachicola chattahoochee flint river basin by hay et al 2014 the study concluded that the prms and sntemp outputs based on downscaled data may not be reliable for studies requiring analyses on daily or even weekly time scales buccola et al 2016 used the prms with the ce qual w2 hydrodynamic and water quality model cole and wells 2011 to investigate climate change effects on water temperature in the north santiam river oregon habitat assessment is another topic for which the prms has been applied gibson et al 2005 examined potential climate change effects on ecologically relevant aspects of the flow regime for the cle elum river washington findings of ecological concern included a substantial shift in the timing of peak flows and reduced low flows of longer duration to help support protection efforts for atlantic salmon dudley 2008 characterized flow quantity timing and variability in the dennys river basin maine chase et al 2016 simulated flow in seven montana watersheds to assess potential effects of climate change on prairie streams and provide information relevant for the future management of such habitats simulation results were used to assess fish population dynamics in the potato creek basin georgia freeman et al 2013 several potential water resources management and research applications would be enabled by or benefit from the modeling framework provided by the nhm prms examples include projections of future hydrologic conditions hydrologic classification of basins and hypothesis formulation and testing water resources planning and management for an uncertain future will require projections of hydrologic conditions for different scenarios e g climate change at multiple scales and for monitored and unmonitored areas using the consistent methodology afforded by the nhm prms projections of hydrologic conditions are achievable for basins at scales ranging from local to national for example in the western conus projections of the availability and timing of snowmelt runoff are needed to provide important information for various aspects of management including decisions pertaining to water use water allocation among competing interests and interbasin transfers a classification system for basins is needed to advance hydrologic science wagener et al 2007 sivakumar et al 2013 benefits of such a classification would include provision of an organizing principle a common language to improve communication guidance for modeling and monitoring and a foundation for hypothesis testing prediction and decision making mcdonnell and woods 2004 wagener et al 2007 sivakumar et al 2013 for the conus the nhm prms offers the potential to make important contributions toward basin classification using hydrologic response as the primary criterion such a classification would enable several research questions to be addressed including 1 how does hydrologic response vary with physical environment 2 how will basins in different hydrologic response classes respond to climate change 3 how does hydrologic process dominance vary between basins of different response classes and with scale for basins of a given response class answers to these questions will improve our understanding of hydrologic processes and have important implications for water resources planning and management process based models such as the prms can effectively serve as virtual laboratories to formulate and test hypotheses that would otherwise be difficult or impossible to investigate fatichi et al 2016 various research topics with management implications would benefit from such virtual experiments including studies designed to investigate sensitivity of basins to climate and land use change potential runoff contributing areas as affected by climate and land use change variability of hydrologic process dominance as affected by basin size location and climate and land use change and effects of anthropogenic forcings on aquatic habitat availability and quality as quantified by ecologically relevant flow metrics 5 discussion as noted by archfield et al 2015 there is convergence in the hydrologic modeling communities catchment global water security and land surface modeling toward development and application of conus domain hydrologic models while these communities display a large diversity in their modeling approaches this should prove to advance rather than hinder the improvement of process based hydrologic models clark et al 2017 the nhm prms is an example of conus extent model development within the catchment modeling community with national e g usgs water availability and use science program wausp https water usgs gov wausp regional e g southeastern u s lafontaine et al 2015 and local applications e g usgs water science centers wscs national applications are able to explore the parameter space addressing continental domain parameter estimation and water budget characterization for a wide range of climate and landscape settings what is learned from these studies not only enhances the nhm infrastructure but can be applied to other conus domain applications regional applications provide proof of concept test beds for local and national scale developments local applications within the usgs wscs can improve upon parameter estimation and other model inputs with local knowledge and provide insight into process representation resulting in algorithm improvements this combination of national to local scale research with the nhm infrastructure will further identify areas where alternative hydrologic process algorithms additional data collection or finer resolution spatial units or temporal time steps may be required to meet operational goals a schematic of this iterative process to support continuous improvement of nhm infrastructure capabilities and data is illustrated in fig 6 for the research to operations concept shown in fig 6 to effectively lead to better representations of the hydrologic cycle interactions between data collectors data synthesizers and data users must occur with usgs already having a strong network for the first two groups usgs field technicians and scientists are located in offices across all 50 states and u s territories collecting and interpreting data using standard methods and training to provide consistent and unbiased data and results cooperation with personnel across the scientific community can further enhance these data collection and evaluation efforts data users e g resource managers and stakeholders provide valuable input as to what information is most important for their particular applications feedback describing what is needed to further the development of the nhm infrastructure should occur amongst the three groups to provide the best possible national and local scale hydrologic information the usgs reputation of consistently collecting high quality unbiased data at the national scale needs to apply to modeling applications as well with a consistently designed simulation infrastructure such as the nhm combined with the wealth of hydrologic information collected by usgs over the past 100 years the agency is uniquely poised to be a leader in integrated modeling to better capture the dynamics of the interaction between the hydrologic cycle and its influences hydrologic simulations must move beyond static and natural characterizations of the landscape representation of dynamically changing land cover water use and other anthropogenic influences e g on and off stream impoundments population growth or migration and their effect on hydrologic response can be important for management and planning of water resources at local to national scales luo et al 2012 describe the effects of conditional parameterization on improving streamflow prediction with a hydrologic model the nhm prms has functionality to simulate the effects of these types of forcings on the hydrologic cycle but data to sufficiently parameterize the models are often unavailable the usgs wausp has invested in efforts to populate databases with site specific water use estimates that allow for improved incorporation of these types of data into hydrologic applications sohl et al 2014 2016 have developed land cover change products that provide annual time step realizations of historical and potential future land cover distributions the challenge then is how we incorporate these types of data into the nhm infrastructure in a robust and consistent way that is compatible with pre existing parameterizations based on more widely accepted products such as the national land cover database homer et al 2007 by adapting the nhm infrastructure to accommodate these additional types of information more data sets of various types and characteristics can then be incorporated to potentially improve hydrologic simulations and reduce model uncertainty to continue improvement of the nhm prms consistent and comprehensive evaluation metrics must be applied for the conus extent historically various combinations of statistical measures e g nash sutcliffe efficiency index percent bias standard error and normalized root mean square error have been used to evaluate the goodness of fit of streamflow estimates from hydrologic models however only evaluating streamflow and with only one or a few of these metrics may not fully reflect the robustness of a hydrologic model in addition target calibration data sets may have biases and uncertainties that confound the representation of the components e g precipitation snowmelt evapotranspiration or streamflow and closure of the water budget robust evaluation and synthesis of diverse hydrologic data sets and simulations should provide results that are generalizable and transferrable across model infrastructures and spatial and temporal scales several conus scale hydrologic modeling applications including the national weather service s national water model nwm have been developed at various spatial and temporal scales and process representation choices wood et al 1997 arnold et al 1999 maurer et al 2002 yu et al 2006 maxwell et al 2015 bock et al 2016a maidment 2017 the impetus for development of these models varies as does their applicability to address an issue many factors come into play when deciding whether to select an existing model or go in a new direction kauffeldt et al 2016 describe eight aspects for the implementation of a hydrological model on a conus scale that are quite useful guidelines for selecting a modeling direction their list is 1 availability of the code as open source or through agreements to allow for code adaptation 2 existing user community to ensure support and development activities 3 input data requirements can be met from existing databases 4 flexibility to use gridded input data 5 possibility of calibration with suitable tools wood et al 1998 6 flexibility in resolution for straightforward down up scaling 7 facility for data assimilation to update the model and 8 existing large domain model set up the nhm infrastructure provides these aspects note though the prms has input options to update some model states and fluxes the nhm infrastructure is designed to maintain the water balance in process representations and is not recommended for simulation needs that require data assimilation the use of data assimilation is a method used in short term forecasting applications such as the nwm maidment 2017 http water noaa gov about nwm and other hydrologic modeling to address bias due to uncertainty in model structure parameter estimation forcing data and boundary conditions moradkhani et al 2005 rasmussen et al 2016 6 summary and conclusions effective water resources management to meet present and future human and environmental needs in a changing world necessarily requires a comprehensive understanding of hydrologic processes that affect streamflow at a watershed scale because water resources management in the u s is an important issue at all scales local to national a nationally consistent process based watershed modeling capability is a logical choice to provide the requisite understanding of hydrologic processes such a capability the nhm prms has been developed and continually enhanced by the usgs to support coordinated comprehensive and consistent hydrologic modeling at multiple scales for the conus nhm based applications provide information to enable more effective water resources planning and management to help fill knowledge gaps in ungaged areas to support basic scientific inquiry and to facilitate the progression from research to operations in the future as process algorithms and data sets improve so too will the nhm infrastructure continue to evolve to better support the nation s water resources research and management needs acknowledgments the authors acknowledge the collaboration of many colleagues across the united states over many years from u s geological survey water science centers other federal agencies and academia the authors also acknowledge that this science product is the result of long term research and development and that the privilege to pursue it was afforded by the u s geological survey national research program finally we thank our anonymous reviewers and our colleague reviewers at the u s geological survey any use of trade product or firm names is for descriptive purposes only and does not imply endorsement by the u s government appendix a supplementary data the following are the supplementary data to this article nhm seg outflow nhm seg outflow nhru lateral flow 2006 s nhru lateral flow 2006 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 09 023 
26278,bayesian network bn modeling objectives and associated categories of recent advances in bn model applications and integration see table 1 for abbreviations fig 1 fig 2 example of a dynamic bayesian network of a dampened oscillating predator prey system of canadian lynx lynx canadensis and snowshoe hare lepus americanus demonstrating a the collapsed model with feedback loops and b the model expanded to 4 time intervals showing population interactions based on o donoghue et al 1997 fig 2 table 1 recent advances in integration of bayesian network bn modeling with other modeling constructs acronyms listed here are as used in the literature see text table 1 recent integration advances purpose geographic information systems bayesian networks bn gis map geographically referenced posterior probabilities generated from the bn dynamic bayesian networks dbns replicate a bn structure over simulated time period to incorporate time dynamic feedback loops and lag effects bayesian decision networks bdns determine potential effects and expected values of alternative management decisions in a probabilistic framework dynamic decision networks ddns evaluate effects and expected values of a sequence of management decisions structural equation modeling sem bayesian networks use sem to determine appropriate causal network structures for a bn bayesian neural networks use bns to determine neural network node weights continuous variable bayesian networks avoid simplification of ratio scale data into discretized range states hybrid bayesian networks hbns bns containing both discrete and continuous non discretized variables include non parametric bns npbns object oriented bayesian networks oobns treat bn variables as objects that can combine methods and data structures dynamic object oriented bayesian networks doobns conduct oobns in a dynamic simulation where object parameters can vary over simulated time agent based bayesian networks treat bn variables as agents or individual entities with dynamic interactions with their environment state and transition bayesian networks stm bns project changes in amounts and dispersions of conditions under probability distributions quantum bayesian networks qbns model non classical probability outcomes using quantum probability amplitude functions power pc theory in causal bns model causal probability structures with observed and unobserved influences partitioning out independent causes as additive effects advances in bayesian network modelling integration of modelling technologies bruce g marcot a trent d penman b a usda forest service portland or usa usda forest service portland or usa b school of ecosystem and forest sciences university of melbourne victoria australia school of ecosystem and forest sciences university of melbourne victoria australia corresponding author bayesian network bn modeling is a rapidly advancing field here we explore new methods by which bn model development and application are being joined with other tools and model frameworks advances include improving areas of bayesian classifiers and machine learning algorithms for model structuring and parameterization and development of time dynamic models increasingly bn models are being integrated with management decision networks structural equation modeling of causal networks bayesian neural networks combined discrete and continuous variables object oriented and agent based models state and transition models geographic information systems quantum probability and other fields integrated bns ibns are becoming useful tools in risk analysis risk management and decision science for resource planning and environmental management in the near future ibns may become self structuring self learning systems fed by real time monitoring data such advances may make model validation difficult and may question model credibility particularly if based on uncertain sources of knowledge systems and big data keywords bayesian networks decision models model integration machine learning model validation 1 introduction bayesian networks bns are directed acyclic graphs that link variables by conditional probabilities where model outputs are probabilities calculated using bayes theorem fenton and neil 2012 koski and noble 2011 bn modeling is useful for data mining determining and explicitly displaying the relationship among variables representing expert knowledge and combining expert knowledge and empirical data and identifying key uncertainties cheon et al 2009 hanea et al 2010 landuyt et al 2013 outputs are typically expressed as probabilities of various states which lends well to decision science approaches to risk analysis and risk management aalders 2008 farmani et al 2012 general network structure of bn models is highly flexible leading many researchers to find new areas of application as examples bns have become popular in environmental management for projecting potential impacts of proposed projects krger and lakes 2015 forecasting impacts of environmental disturbances such as fire dlamini 2010 and climate change sperotto et al 2017 and providing a basis for making environmental management decisions barton et al 2012 many examples are available of the use of bns in a wide variety of other environmental and resource management contexts such as management of groundwater giordano et al 2013 recreation impacts fortin et al 2016 and green energy production carta et al 2011 if the bn contains no random variable then the outcome generated is fixed i e deterministic for a given set of priors else the outcome is stochastic bns can be made stochastic by introducing random deviates as part of formulae within nodes variables also can be described with formulae combining values of parent nodes such as used by steventon et al 2006 in assessing viability risk of a rare seabird further variables can be denoted with continuous ranges rather than discrete state conditions such as used by hradsky et al 2017 to determine impacts of fire and other stressors on the distribution of terrestrial wildlife bns are a highly useful tool for depicting and modeling current knowledge such as with initial representation of a system or problem to gain a better understanding and perspective on uncertainties and complexities so as to help advise managers and decision makers bns provide a robust statistical framework when little data are available an example is in the context of environmental modeling with time critical situations with scant available data such as active monitoring of key energy infrastructures guerriero et al 2016 and surveillance of endangered species koen et al 2017 increasingly bns are being integrated with other modeling constructs and tools such as geographic information systems gis and remote sensing databases in this paper we explore these new avenues of how bn model development and application are being joined with many other tools and model frameworks for a variety of environmental assessment and management objectives we briefly review the current state and recent advances of bn modeling and then provide examples of an emerging new era of integrating bn models with other frameworks and tools lastly we present a vision of next advances to come concluding with a perspective on ensuring scientific and decision making credibility with cautions on accelerated model advancements 2 the cutting edge of bn modeling the field of bn modeling is advancing swiftly with the number of journal articles using bns continuing to rise including a recent era of exponential growth marcot 2017 recent developments in bn modeling are reviewed in the following sections 2 1 recent advances in bn model structure and applications a number of recent advances in bn model structure and application have followed a diverse track of topics generally related to identifying and exploring system dynamics and aiding decision management fig 1 classification one area of resurgence is in new approaches to the classification problem viz bayesian classifiers and machine learning bn classifiers include a wide variety of algorithms starting with naive bayes and variants thereof bielza and larraaga 2014 related to this are algorithms for bayesian learning of probability structures from empirical data e g tsamardinos et al 2006 do and batzoglou 2008 latent variables a common problem in ecological or environmental modeling is the influence of latent variables which are effects inferred from the relation among observed variables but which are not directly observed marcot 2017 machine learning algorithms used in parameterizing the probability values in bn models such as the expectation maximization algorithm do and batzoglou 2008 can to some degree account for the influence of latent variables and missing data lauritzen 1995 a related problem is how to validate bn models developed entirely from expert elicitation with no case file data by which to structure or parameterize the model such models portray logical or causal relations among variables as inferred by expert knowledge but these relations often are influenced by unspecified latent variables de waal et al 2016 pitchforth and mengersen 2013 proposed methods for evaluating confidence in the validity of such models even in the dearth of empirical data and presence of latent variables thus providing a validation framework for expert elicited bns de waal et al 2016 suggested several approaches to handling latent variables in bns including explaining uncertainties associated with latent variables parameterizing the probability values of bns so as to directly address the roles of latent variables and addressing uncertainty in model validation depicting model confidence although bn models explicitly incorporate uncertainty there is no common method for depicting and quantifying the degree of confidence in the underlying probability values of the model and in the resulting posterior probability calculations that is uncertainty measures can be inferred from the probability distributions of states calculated in the model but these are not necessarily the same as the degree of confidence lever of certainty of that probability distribution pitchforth and mengersen 2013 characterized confidence in bn model behavior as consisting of three components of structure confidence discretization confidence and parameterization confidence for use in bn model validation they further adduced 7 common dimensions of validity as used in psychometry nomological face content concurrent predictive convergent and discriminant validity which collectively pertain to the degree of concordance within accepted norms and credibility within a particular discipline a more quantitative method of depicting bn model confidence as developed by van allen et al 2008 entails estimating error bars around posterior probability calculations from bns which then depict the degree of uncertainty or confidence in model outcomes error bars for bns are termed credible intervals which provide the range of model outcomes within a specified probability level marcot 2012 but which are not often reported in bn modeling projects such error bars should not be confused with frequentist confidence intervals hamilton et al 2015 used credible intervals to measure the strength of the relationship between suitability of habitat of a crayfish and environmental predictor variables links to gis probability outcomes from bn models for evaluating local conditions have been used as input to gis systems to create maps depicting habitat quality of wildlife species raphael et al 2001 havron et al 2017 kininmonth et al 2014 presented a model which combined spatial datasets spatial models and expert opinion in an integrated bn gis structure for evaluating boating damage to the great barrier reef of eastern australia dlamini 2010 developed a bn gis model that uses geographically referenced remote sensing modis data to analyze wildfire in swaziland bn programs that integrate or intersect with gis include geonetica norsys inc hugin hugin expert a s and ecosystem management decision support emds reynolds et al 2014 that integrates the genie bn modeling platform with two gis components of arcmap esri and open source qgis gonzalez redin et al 2016 bns linked to gis to map trade offs of ecosystem services in the french alps to inform planning decisions several projects have explicitly integrated gis and bn modeling frameworks such as the qgis plug in for bns developed by landuyt et al 2015 and the integration of the genie bn modeling framework into the arcgis based ecosystem management decision support system emds mountain viewgroup com dynamic bayesian networks other recent variations on the traditional bn modeling theme include dynamic bayesian networks dbns that model a time series of conditions and contingencies such as with oscillating predator prey dynamics fig 2 dbns typically contain feedback loops which are not allowed in the directed acyclic graph structure of bns but can be modeled when bns are time expanded so that the entire bn structure is replicated for different time periods so that the links become acyclic in some cases dbns have been made spatially explicit by integrating with geographic information systems gis e g chee et al 2016 a variant of dbns are those operating in real time in response to discrete or continuous inputs such as for predicting highway crashes hossain and muromachi 2012 and in analyzing gene networks kim et al 2003 new approaches to structuring dbns combine methods from static and dynamic networks vlasselaer et al 2016 uusitalo et al 2018 used dbns with hidden variables to model major structural changes of a baltic sea food web and orphanou et al 2014 used temporal bayesian networks tbns a synonym for dbn to evaluate temporal relationships in clinical data for medical diagnosis and prognosis their hidden variables represented unobserved processes contributing to the changes and resulted in dbn models that reflected known dynamics of the food web system bayesian decision networks bayesian decision networks bdns extend bn models by explicitly including decision and utility nodes e g barton et al 2008 decision nodes are deterministic nodes that depict unique management decisions and utility nodes are continuous nodes that estimate a cost or benefit of a given outcome resulting from a decision bdns use utility nodes to calculate overall expected values of all costs or benefits of alternative management decisions given the probability structure of the model and can be highly useful in risk analysis and risk management arenas for example loyd and devore 2010 developed a bdn to advise on alternatives for management of feral cats in the united states catenacci and guipponi 2013 used a bdn as a basis for adaptation planning to sea level rise a further variation of bdns is with dynamic decision networks ddns that essentially merge decision networks with time expanded dynamic networks ddns were developed by murray et al 2004 to guide selection of teaching tutorials and by penman et al 2015a to advise on reducing risk of loss of homes to wildfire bns have also been used to assess value of information to optimize resource use decisions such as the fisheries industry kuikka et al 1999 depicting causality in structural equation models bn modeling has been compared to structural equation modeling sem in that both can be used to depict causal networks and influences and can analyze degree of causality pearl 1998 2000 the two approaches differ in that sem is a general suite of statistical tools usually using frequentist multivariate approaches although some sem approaches also support bayesian estimation whereas bns use conditional probabilities and bayes theorem a main difference is that sems are purely statistical tools developed for example to test hypotheses or to test whether an assumed causal relation in the graph is significant whereas bns are probabilistic models trainable by data mainly for investigating the consequences of conditions or events on outcomes or deducing causal conditions resulting in an outcome more recently li et al 2018 compared and combined bn and sem modeling to evaluate the interactive influence of land use and climate change on stream macroinvertebrates they first used sem to develop a conceptual influence diagram of causal effects that is the network structure of variables and their linkages and then they built prediction and diagnostic bns from the same conceptual model using sem helped identify and justify the links used in the bn models their results suggested that modeling all causal factors together in the sem conceptual model and in the subsequent bn model provided a more robust understanding of how positive effects from climate change could mollify negative influences from land use neural networks a somewhat different variant of bn modeling appears in bayesian neural nets that is using bayesian learning to determine neural network node weights neural networks are typically trained using a variety of approaches including variants of gradient descent and least squares methods to minimize loss functions of variables singly or in conjugation using bns can bring greater efficiency in adjusting node weight parameters based on prior knowledge bayesian neural networks have been used to forecast energy load requirements lauret et al 2008 solar irradiation yacef et al 2012 stock market performance ticknor 2013 and internet traffic loads auld et al 2007 continuous bayesian networks another area of recent interest and progress is in developing continuous bn models where quantitative variables are not discretized into exclusive state ranges but instead are represented by continuous values such as equations or statistical distributions continuous bns can be constructed using programming tools such as uninet cooke et al 2007 delgado hernndez et al 2012 winbugs kery 2010 agenarisk neil et al 2007 hugin madsen et al 2003 and genie druzdzel 1999 other bn modeling and graphical modeling software also can deal with continuous nodes 1 1 https www cs ubc ca murphyk software bnsoft html if the multivariate normal is assumed a hanea pers comm hybrid bayesian networks some researchers have developed bn models with both discrete and continuous variables where the latter are not discretized aguilera et al 2010 castillo et al 1998 driver and morrell 1995 these types of models are referred to as hybrid bns hbns hanea et al 2006 a special case of hbns called non parametric bns npbns was reviewed by hanea et al 2015 npbns were initially devised for continuous only bns but are used in situations of hbns as well hanea et al 2010 developed a npbn methodology with data mining to develop prediction models hradsky et al 2017 used npbns to model a presence absence continuous response of wildlife to fire age classes and terrestrial vegetation classes depicted with discrete variables object oriented bayesian networks a further area of recent exploration is with object oriented bayesian networks oobns and dynamic object oriented bayesian networks doobns bangs et al 2004 benjamin fink and reilly 2017 for example oobns and doobns have been used to model health impacts of cyanobacteria blooms johnson et al 2010 viability of populations of cheetahs acinonyx jubatus in namibia johnson et al 2013 and issues of water resource management phan et al 2016 the tools hugin and agenarisk provide for true oobn modeling and genie also can be used as such although it is not a true oobn framework agent based modeling related to oobns is the merging of agent based models and bns agent based models an 2012 are simulations of the dynamics such as movement patterns of individual objects nielsen and parsons 2007 developed a model of consensus building where individual agents were represented by bns that expressed a range of possible agreements sun and mller 2013 presented a bn agent model to explore the economics of ecosystem services and land use decision making state and transition modeling another area of integration is with state and transition models stms that are used to project future proportions or amounts of conditions such as landscape vegetation conditions and species responses under known or hypothesized rates of change mason et al 2017 stms project future conditions such as area covered in vegetation type categories by multiplying a matrix of current area in each category by a matrix of probabilities depicting transitions to the same or other categories e g jorgenson et al 2015 in a hybrid stm bn model transition probabilities are estimated from calculations in the bn network that account for environmental influences on each vegetation type category bashari et al 2009 developed an integrated stm bn model as expanded upon by nicholson and flores 2011 to inform management decisions in rangelands of queensland australia chee et al 2016 integrated stms and bns in a geographic information system gis with object oriented concepts to model spatial and temporal changes in an australian woodland and a wetland in florida quantum bayesian networks bns are being increasingly used in the area of quantum information theory as quantum bayesian networks qbns tucci 1995 qbns are constructed to represent outcomes that deviate from and are paradoxical to classical probability calculations examples include when outcomes are dependent on the sequence of inputs priors and parent nodes in a bn when human decision making deviates from dominant probability outcomes in a bdn when a system can result in 1 dominant probability outcome state and other situations such outcomes could be modeled in traditional bns by including latent variables but such models quickly become overly complex and serve only to describe specific conditions and outcomes not to serve as predictive and explanatory models generally in qbns classical conditional probability tables using bayes calculus are replaced by quantum probability amplitudes a complex number function that describes the behavior of a system moreira and wichert 2018 developed a decision based qbn with quantum probability amplitudes to demonstrate how prediction of some aspects of human decision making is more efficient than with a traditional bn with latent variables similarly trueblood et al 2016 used a qbn approach to model how human judgment can deviate from classical probability in the face of high uncertainty and imperfect information about causality of a system busemeyer and trueblood 2009 explored the use of qbns in quantum theory to model how different sequences of measurements can affect the probabilities of system outcomes leifer and spekkens 2013 developed a qbn framework for depicting how quantum conditional states can result from the influence of two systems at one time or from one system at two times other formulations and applications of qbns are found in the literature although at present there does not seem to be any generally available software by which qbns can be constructed power pc theory and causal bns power pc probabilistic contrast theory states that a system outcome is the sum effect of the relative power of observed and unobserved causal relations which can be depicted and partitioned mathematically cheng 1997 norick and cheng 2004 in applying power pc theory lu et al 2008 demonstrated how the relative influence of different independent causes can be determined empirically and can be represented in bayesian causal networks here we have covered a range of significant recent advances in application of bns still other variations and new approaches to bn modeling continue to appear in the literature 2 2 beyond the network a new era of integration bn models of various forms have been increasingly used in a variety of applications they are also being specifically integrated with other modeling constructs which we refer to here as integrated bayesian networks ibns johnson et al 2010 ibns can be defined as bn structures that are explicitly embedded within the framework of other modeling constructs instead of just being applied to some area of inquiry as reviewed in the previous section examples of ibns include assimilating bns in structured decision making frameworks agent based models and state and transition models table 1 ibns are crafted generally to apply the probabilistic basis of bns to new areas of application and research such as dynamic and stochastic simulation modeling ibns are also becoming useful tools in risk analysis risk management and decision science such as in environmental resource planning johnson and mengersen 2012 fraser et al 2017 and for depicting how deep uncertainty affects policy decisions aven 2013 cox 2012 janssens et al 2006 developed an ibn that combined bns and decision trees for developing decision rules in transportation management qgenie modeler bayesfusion llc has a graphical user interface that provides for rapid prototyping of decision models in a bn environment some bn modeling platforms such as netica norsys inc and hugin provide application program interfaces apis to facilitate integration links to other programs such as geographic information systems which can facilitate their use in integrated risk analysis and structured decision making under uncertainty e g barton et al 2008 an emerging area is the development of ibns operating from real time monitoring data for example maglogiannis et al 2006 have proposed a patient health risk analysis system using ibns operating from vital sign monitoring data their vision is to produce a real time system for homecare telemedicine penman et al 2015b developed a fire danger rating system that updated daily with meteorology forecasts and as new fires appeared in the landscape the model automatically updated the risk projections vagnoli et al 2017 proposed a real time ibn based system to monitor the structural integrity of railway bridges in europe koen et al 2017 developed an ibn model to monitor the poaching illegal hunting of rhinoceroses in kruger national park south africa which is being implemented as a real time management tool 3 things to come the field will continue to advance more rapidly into uncharted territory as ibns become more sophisticated future ibn research foci will include substantial cutting edge advancements in the many areas of integration reviewed above currently ibns are essentially static however we expect that in the near future ibns will emerge that are self updating and self improving and that will learn from real time continuous input of environmental monitoring data being able to dynamically update bn conditional probability values with new data e g using an expectation maximization algorithm and to continually recalculate posterior probabilities are quite feasible with existing software and hardware some current ibns do automatic recalculation of posterior probability outcomes with new input data feeding the models but here we are referring also to the basic structure and underlying conditional probability tables themselves being created anew and updated with machine learning tools the purpose of this would be to continually refine the context accuracy and robustness of the models which is a fundamental precept and advantage of bayesian statistical approaches that improve model predictions from prior data some aspects of this have appeared in the application of bns in areas of finance giudici and spelta 2016 garvey et al 2015 and medical diagnosis and decision support constantinuo et al 2016 in the future we envision self creation of ibns based on emergent information from crowdsourced data park and budescu 2015 also to come will be the further integration of bns with expert system knowledge bases particularly using fuzzy logic or neural net forms of knowledge representation and machine learning algorithms in the past control rule based expert systems used confidence factors or scoring rules zohar and rosenschein 2008 e g indices as provided by the expert and scaled 1 10 to rank the likelihood or the credibility of a particular inference or outcome today the bn modeling shell bayesialab bayesia s a s conrady and jouffe 2015 averages conditional probability values from multiple experts and weights the values by the product of confidence in and credibility of the values as scored by each expert 4 conclusions and perspectives what are some of the main cautions and caveats in this new era of ibns here we address quandaries of ensuring validity and credibility of ibns that are becoming increasingly complex in construction and interpretation particularly as they are induced from automated and big data sources 4 1 ensuring a future of validity and credibility it will be important to ensure the validity and credibility of increasingly complex expert based ibn systems kleemann et al 2017 particularly as they interact with human social systems as they guide resource management decisions and as they operate with greater autonomy as ibns become more complex it will become more problematic to create and test simple intuitive and understandable influence diagrams and mind maps that chart their structures logic and operation a way around this could entail decomposing such complex systems into simpler component submodels and testing and updating each submodel bns are generally constructed as markov processes so that they can be dissected and reassembled without loss of information particularly using cutpoints in the network graph nodes in the bn whose removal would separate the graph direct and indirect causes in process models will be increasingly difficult to clearly identify particularly with models having multiple interaction terms feedback loops latent variables and synergistic functions among covariates and response variables this also means increasing difficulty in parsing out sensitivity and influence effects to specific drivers that is clearly identifying key factors that most influence outcomes and for which management might have greatest uncertainty and greatest or least control the result may limit their acceptance and adoption in management situations if users do not understand and trust the models and if the models have limited face validity this can be a major issue with decision networks and use of models in real time and with models intended to guide planning and management of resources with high opportunity costs support for the model as with advances in validation methods and frameworks as discussed above is needed some work advancing methods of sensitivity analysis of bn models may show promise for further development and application on increasingly complex model structures e g global sensitivity analysis methods of li and mahadevan 2017 increasing complexity of ibns will carry increased difficulty in model calibration testing validation and updating eventually necessitating new heuristic approaches and algorithms that can wade the swamp of big data spiegelhalter 2014 ladeau et al 2017 lv et al 2014 lewis et al 2018 further warned that developing wildlife biology models even with high quality big data sets raises concerns for how opaque modeling algorithms can lead to complex problems of data management exploratory data analysis data sharing and reproducibility paradoxically big data sets with many variables often are sparse in terms of replicated conditions and variable combinations hastie et al 2015 although bn models induced from big data may fit well that is with high calibration accuracy they may lack robustness and perform poorly because of this sparsity that is the models may have low independent validation accuracy and be overfit also bns built on big data collected through citizen science initiatives which are becoming increasingly popular in many topic areas should be carefully vetted for the accuracy of those data kosmala et al 2016 or else the models and their interpretations may be incorrect biased and misleading particularly with self generating and self updated systems ibns will be no more trustworthy than the data on which they are based and on the opacity of the algorithms used to structure their networks and parameterize their probabilities in the end the validity and credibility of ibns are inextricably linked to those of the data and the methods on which they are based and created at stake is clear demonstration of expert knowledge empirical data and model simulations and ultimately of model validity operational robustness and the credibility of the modeling science itself despite potential worries over the testing and validity of big data sourced ibns and the quickly evolving environments in which ibns are constructed their strengths will remain in what if scenario exploration and in being able to combine soft and hard evidence that is expert knowledge and empirical data even with an exponential increase in publicly available information combining expert knowledge with data is best conducted so as to detect and counteract any spurious correlations that may be indicated by machine learning algorithms models that are structured and parameterized solely by use of blind automatic methods such as unguided machine learning algorithms without human oversight lose the key advantage of the bayesian approach to updating human knowledge and insight 4 2 cautions and caveats in the new era of integration we have tried here to chart some routes that development of ibn systems may be taking and some cautions indicated in this fast evolving era salmond et al 2017 soon to come will be dynamic and self modifying and even self creating ibns using big data garnered from automated monitoring e g environmental and earth science remote sensing data li et al 2016 citizen science initiatives and even crowdsourced information sources we ask to what degree should we trust such models for in the end the knowledge source and expertise that may serve to generate and fuel ibns may become a fully realized global emergent artificial intelligence if so how shall the models veracity be determined when their source becomes non human bns developed from empirical data often can be tested using existing cross validation and jackknifing algorithms however the validity of bns developed from expert knowledge is more difficult to determine if independent data sets by which to test the models are unavailable or if specific combinations of variables in big data sources are sparse in such cases peer review of the model at various stages of development is essential to establish credibility and conformity with accepted precepts but determining the external validity of self organizing and self updating bns that are developed with deep learning algorithms in real time from big data sources including citizen science and crowd sourced data especially the multitude of internet blog sources and news posts without independent testing or review may be most problematic and will require new approaches to scientifically evaluating their veracity this will become one of the greater challenges in the fast evolving new era of bn model integration acknowledgments inspiration for this paper comes from a keynote address given by the senior author in 2017 at the joint conference of the australasian bayesian network modelling society and the society for risk analysis australia and new zealand at the university of melbourne australia we thank tom bruce anca hanea sandra johnson and two anonymous reviewers for their helpful reviews of the manuscript appendix a supplementary data the following is the supplementary data to this article data profile data profile appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 09 016 
26278,bayesian network bn modeling objectives and associated categories of recent advances in bn model applications and integration see table 1 for abbreviations fig 1 fig 2 example of a dynamic bayesian network of a dampened oscillating predator prey system of canadian lynx lynx canadensis and snowshoe hare lepus americanus demonstrating a the collapsed model with feedback loops and b the model expanded to 4 time intervals showing population interactions based on o donoghue et al 1997 fig 2 table 1 recent advances in integration of bayesian network bn modeling with other modeling constructs acronyms listed here are as used in the literature see text table 1 recent integration advances purpose geographic information systems bayesian networks bn gis map geographically referenced posterior probabilities generated from the bn dynamic bayesian networks dbns replicate a bn structure over simulated time period to incorporate time dynamic feedback loops and lag effects bayesian decision networks bdns determine potential effects and expected values of alternative management decisions in a probabilistic framework dynamic decision networks ddns evaluate effects and expected values of a sequence of management decisions structural equation modeling sem bayesian networks use sem to determine appropriate causal network structures for a bn bayesian neural networks use bns to determine neural network node weights continuous variable bayesian networks avoid simplification of ratio scale data into discretized range states hybrid bayesian networks hbns bns containing both discrete and continuous non discretized variables include non parametric bns npbns object oriented bayesian networks oobns treat bn variables as objects that can combine methods and data structures dynamic object oriented bayesian networks doobns conduct oobns in a dynamic simulation where object parameters can vary over simulated time agent based bayesian networks treat bn variables as agents or individual entities with dynamic interactions with their environment state and transition bayesian networks stm bns project changes in amounts and dispersions of conditions under probability distributions quantum bayesian networks qbns model non classical probability outcomes using quantum probability amplitude functions power pc theory in causal bns model causal probability structures with observed and unobserved influences partitioning out independent causes as additive effects advances in bayesian network modelling integration of modelling technologies bruce g marcot a trent d penman b a usda forest service portland or usa usda forest service portland or usa b school of ecosystem and forest sciences university of melbourne victoria australia school of ecosystem and forest sciences university of melbourne victoria australia corresponding author bayesian network bn modeling is a rapidly advancing field here we explore new methods by which bn model development and application are being joined with other tools and model frameworks advances include improving areas of bayesian classifiers and machine learning algorithms for model structuring and parameterization and development of time dynamic models increasingly bn models are being integrated with management decision networks structural equation modeling of causal networks bayesian neural networks combined discrete and continuous variables object oriented and agent based models state and transition models geographic information systems quantum probability and other fields integrated bns ibns are becoming useful tools in risk analysis risk management and decision science for resource planning and environmental management in the near future ibns may become self structuring self learning systems fed by real time monitoring data such advances may make model validation difficult and may question model credibility particularly if based on uncertain sources of knowledge systems and big data keywords bayesian networks decision models model integration machine learning model validation 1 introduction bayesian networks bns are directed acyclic graphs that link variables by conditional probabilities where model outputs are probabilities calculated using bayes theorem fenton and neil 2012 koski and noble 2011 bn modeling is useful for data mining determining and explicitly displaying the relationship among variables representing expert knowledge and combining expert knowledge and empirical data and identifying key uncertainties cheon et al 2009 hanea et al 2010 landuyt et al 2013 outputs are typically expressed as probabilities of various states which lends well to decision science approaches to risk analysis and risk management aalders 2008 farmani et al 2012 general network structure of bn models is highly flexible leading many researchers to find new areas of application as examples bns have become popular in environmental management for projecting potential impacts of proposed projects krger and lakes 2015 forecasting impacts of environmental disturbances such as fire dlamini 2010 and climate change sperotto et al 2017 and providing a basis for making environmental management decisions barton et al 2012 many examples are available of the use of bns in a wide variety of other environmental and resource management contexts such as management of groundwater giordano et al 2013 recreation impacts fortin et al 2016 and green energy production carta et al 2011 if the bn contains no random variable then the outcome generated is fixed i e deterministic for a given set of priors else the outcome is stochastic bns can be made stochastic by introducing random deviates as part of formulae within nodes variables also can be described with formulae combining values of parent nodes such as used by steventon et al 2006 in assessing viability risk of a rare seabird further variables can be denoted with continuous ranges rather than discrete state conditions such as used by hradsky et al 2017 to determine impacts of fire and other stressors on the distribution of terrestrial wildlife bns are a highly useful tool for depicting and modeling current knowledge such as with initial representation of a system or problem to gain a better understanding and perspective on uncertainties and complexities so as to help advise managers and decision makers bns provide a robust statistical framework when little data are available an example is in the context of environmental modeling with time critical situations with scant available data such as active monitoring of key energy infrastructures guerriero et al 2016 and surveillance of endangered species koen et al 2017 increasingly bns are being integrated with other modeling constructs and tools such as geographic information systems gis and remote sensing databases in this paper we explore these new avenues of how bn model development and application are being joined with many other tools and model frameworks for a variety of environmental assessment and management objectives we briefly review the current state and recent advances of bn modeling and then provide examples of an emerging new era of integrating bn models with other frameworks and tools lastly we present a vision of next advances to come concluding with a perspective on ensuring scientific and decision making credibility with cautions on accelerated model advancements 2 the cutting edge of bn modeling the field of bn modeling is advancing swiftly with the number of journal articles using bns continuing to rise including a recent era of exponential growth marcot 2017 recent developments in bn modeling are reviewed in the following sections 2 1 recent advances in bn model structure and applications a number of recent advances in bn model structure and application have followed a diverse track of topics generally related to identifying and exploring system dynamics and aiding decision management fig 1 classification one area of resurgence is in new approaches to the classification problem viz bayesian classifiers and machine learning bn classifiers include a wide variety of algorithms starting with naive bayes and variants thereof bielza and larraaga 2014 related to this are algorithms for bayesian learning of probability structures from empirical data e g tsamardinos et al 2006 do and batzoglou 2008 latent variables a common problem in ecological or environmental modeling is the influence of latent variables which are effects inferred from the relation among observed variables but which are not directly observed marcot 2017 machine learning algorithms used in parameterizing the probability values in bn models such as the expectation maximization algorithm do and batzoglou 2008 can to some degree account for the influence of latent variables and missing data lauritzen 1995 a related problem is how to validate bn models developed entirely from expert elicitation with no case file data by which to structure or parameterize the model such models portray logical or causal relations among variables as inferred by expert knowledge but these relations often are influenced by unspecified latent variables de waal et al 2016 pitchforth and mengersen 2013 proposed methods for evaluating confidence in the validity of such models even in the dearth of empirical data and presence of latent variables thus providing a validation framework for expert elicited bns de waal et al 2016 suggested several approaches to handling latent variables in bns including explaining uncertainties associated with latent variables parameterizing the probability values of bns so as to directly address the roles of latent variables and addressing uncertainty in model validation depicting model confidence although bn models explicitly incorporate uncertainty there is no common method for depicting and quantifying the degree of confidence in the underlying probability values of the model and in the resulting posterior probability calculations that is uncertainty measures can be inferred from the probability distributions of states calculated in the model but these are not necessarily the same as the degree of confidence lever of certainty of that probability distribution pitchforth and mengersen 2013 characterized confidence in bn model behavior as consisting of three components of structure confidence discretization confidence and parameterization confidence for use in bn model validation they further adduced 7 common dimensions of validity as used in psychometry nomological face content concurrent predictive convergent and discriminant validity which collectively pertain to the degree of concordance within accepted norms and credibility within a particular discipline a more quantitative method of depicting bn model confidence as developed by van allen et al 2008 entails estimating error bars around posterior probability calculations from bns which then depict the degree of uncertainty or confidence in model outcomes error bars for bns are termed credible intervals which provide the range of model outcomes within a specified probability level marcot 2012 but which are not often reported in bn modeling projects such error bars should not be confused with frequentist confidence intervals hamilton et al 2015 used credible intervals to measure the strength of the relationship between suitability of habitat of a crayfish and environmental predictor variables links to gis probability outcomes from bn models for evaluating local conditions have been used as input to gis systems to create maps depicting habitat quality of wildlife species raphael et al 2001 havron et al 2017 kininmonth et al 2014 presented a model which combined spatial datasets spatial models and expert opinion in an integrated bn gis structure for evaluating boating damage to the great barrier reef of eastern australia dlamini 2010 developed a bn gis model that uses geographically referenced remote sensing modis data to analyze wildfire in swaziland bn programs that integrate or intersect with gis include geonetica norsys inc hugin hugin expert a s and ecosystem management decision support emds reynolds et al 2014 that integrates the genie bn modeling platform with two gis components of arcmap esri and open source qgis gonzalez redin et al 2016 bns linked to gis to map trade offs of ecosystem services in the french alps to inform planning decisions several projects have explicitly integrated gis and bn modeling frameworks such as the qgis plug in for bns developed by landuyt et al 2015 and the integration of the genie bn modeling framework into the arcgis based ecosystem management decision support system emds mountain viewgroup com dynamic bayesian networks other recent variations on the traditional bn modeling theme include dynamic bayesian networks dbns that model a time series of conditions and contingencies such as with oscillating predator prey dynamics fig 2 dbns typically contain feedback loops which are not allowed in the directed acyclic graph structure of bns but can be modeled when bns are time expanded so that the entire bn structure is replicated for different time periods so that the links become acyclic in some cases dbns have been made spatially explicit by integrating with geographic information systems gis e g chee et al 2016 a variant of dbns are those operating in real time in response to discrete or continuous inputs such as for predicting highway crashes hossain and muromachi 2012 and in analyzing gene networks kim et al 2003 new approaches to structuring dbns combine methods from static and dynamic networks vlasselaer et al 2016 uusitalo et al 2018 used dbns with hidden variables to model major structural changes of a baltic sea food web and orphanou et al 2014 used temporal bayesian networks tbns a synonym for dbn to evaluate temporal relationships in clinical data for medical diagnosis and prognosis their hidden variables represented unobserved processes contributing to the changes and resulted in dbn models that reflected known dynamics of the food web system bayesian decision networks bayesian decision networks bdns extend bn models by explicitly including decision and utility nodes e g barton et al 2008 decision nodes are deterministic nodes that depict unique management decisions and utility nodes are continuous nodes that estimate a cost or benefit of a given outcome resulting from a decision bdns use utility nodes to calculate overall expected values of all costs or benefits of alternative management decisions given the probability structure of the model and can be highly useful in risk analysis and risk management arenas for example loyd and devore 2010 developed a bdn to advise on alternatives for management of feral cats in the united states catenacci and guipponi 2013 used a bdn as a basis for adaptation planning to sea level rise a further variation of bdns is with dynamic decision networks ddns that essentially merge decision networks with time expanded dynamic networks ddns were developed by murray et al 2004 to guide selection of teaching tutorials and by penman et al 2015a to advise on reducing risk of loss of homes to wildfire bns have also been used to assess value of information to optimize resource use decisions such as the fisheries industry kuikka et al 1999 depicting causality in structural equation models bn modeling has been compared to structural equation modeling sem in that both can be used to depict causal networks and influences and can analyze degree of causality pearl 1998 2000 the two approaches differ in that sem is a general suite of statistical tools usually using frequentist multivariate approaches although some sem approaches also support bayesian estimation whereas bns use conditional probabilities and bayes theorem a main difference is that sems are purely statistical tools developed for example to test hypotheses or to test whether an assumed causal relation in the graph is significant whereas bns are probabilistic models trainable by data mainly for investigating the consequences of conditions or events on outcomes or deducing causal conditions resulting in an outcome more recently li et al 2018 compared and combined bn and sem modeling to evaluate the interactive influence of land use and climate change on stream macroinvertebrates they first used sem to develop a conceptual influence diagram of causal effects that is the network structure of variables and their linkages and then they built prediction and diagnostic bns from the same conceptual model using sem helped identify and justify the links used in the bn models their results suggested that modeling all causal factors together in the sem conceptual model and in the subsequent bn model provided a more robust understanding of how positive effects from climate change could mollify negative influences from land use neural networks a somewhat different variant of bn modeling appears in bayesian neural nets that is using bayesian learning to determine neural network node weights neural networks are typically trained using a variety of approaches including variants of gradient descent and least squares methods to minimize loss functions of variables singly or in conjugation using bns can bring greater efficiency in adjusting node weight parameters based on prior knowledge bayesian neural networks have been used to forecast energy load requirements lauret et al 2008 solar irradiation yacef et al 2012 stock market performance ticknor 2013 and internet traffic loads auld et al 2007 continuous bayesian networks another area of recent interest and progress is in developing continuous bn models where quantitative variables are not discretized into exclusive state ranges but instead are represented by continuous values such as equations or statistical distributions continuous bns can be constructed using programming tools such as uninet cooke et al 2007 delgado hernndez et al 2012 winbugs kery 2010 agenarisk neil et al 2007 hugin madsen et al 2003 and genie druzdzel 1999 other bn modeling and graphical modeling software also can deal with continuous nodes 1 1 https www cs ubc ca murphyk software bnsoft html if the multivariate normal is assumed a hanea pers comm hybrid bayesian networks some researchers have developed bn models with both discrete and continuous variables where the latter are not discretized aguilera et al 2010 castillo et al 1998 driver and morrell 1995 these types of models are referred to as hybrid bns hbns hanea et al 2006 a special case of hbns called non parametric bns npbns was reviewed by hanea et al 2015 npbns were initially devised for continuous only bns but are used in situations of hbns as well hanea et al 2010 developed a npbn methodology with data mining to develop prediction models hradsky et al 2017 used npbns to model a presence absence continuous response of wildlife to fire age classes and terrestrial vegetation classes depicted with discrete variables object oriented bayesian networks a further area of recent exploration is with object oriented bayesian networks oobns and dynamic object oriented bayesian networks doobns bangs et al 2004 benjamin fink and reilly 2017 for example oobns and doobns have been used to model health impacts of cyanobacteria blooms johnson et al 2010 viability of populations of cheetahs acinonyx jubatus in namibia johnson et al 2013 and issues of water resource management phan et al 2016 the tools hugin and agenarisk provide for true oobn modeling and genie also can be used as such although it is not a true oobn framework agent based modeling related to oobns is the merging of agent based models and bns agent based models an 2012 are simulations of the dynamics such as movement patterns of individual objects nielsen and parsons 2007 developed a model of consensus building where individual agents were represented by bns that expressed a range of possible agreements sun and mller 2013 presented a bn agent model to explore the economics of ecosystem services and land use decision making state and transition modeling another area of integration is with state and transition models stms that are used to project future proportions or amounts of conditions such as landscape vegetation conditions and species responses under known or hypothesized rates of change mason et al 2017 stms project future conditions such as area covered in vegetation type categories by multiplying a matrix of current area in each category by a matrix of probabilities depicting transitions to the same or other categories e g jorgenson et al 2015 in a hybrid stm bn model transition probabilities are estimated from calculations in the bn network that account for environmental influences on each vegetation type category bashari et al 2009 developed an integrated stm bn model as expanded upon by nicholson and flores 2011 to inform management decisions in rangelands of queensland australia chee et al 2016 integrated stms and bns in a geographic information system gis with object oriented concepts to model spatial and temporal changes in an australian woodland and a wetland in florida quantum bayesian networks bns are being increasingly used in the area of quantum information theory as quantum bayesian networks qbns tucci 1995 qbns are constructed to represent outcomes that deviate from and are paradoxical to classical probability calculations examples include when outcomes are dependent on the sequence of inputs priors and parent nodes in a bn when human decision making deviates from dominant probability outcomes in a bdn when a system can result in 1 dominant probability outcome state and other situations such outcomes could be modeled in traditional bns by including latent variables but such models quickly become overly complex and serve only to describe specific conditions and outcomes not to serve as predictive and explanatory models generally in qbns classical conditional probability tables using bayes calculus are replaced by quantum probability amplitudes a complex number function that describes the behavior of a system moreira and wichert 2018 developed a decision based qbn with quantum probability amplitudes to demonstrate how prediction of some aspects of human decision making is more efficient than with a traditional bn with latent variables similarly trueblood et al 2016 used a qbn approach to model how human judgment can deviate from classical probability in the face of high uncertainty and imperfect information about causality of a system busemeyer and trueblood 2009 explored the use of qbns in quantum theory to model how different sequences of measurements can affect the probabilities of system outcomes leifer and spekkens 2013 developed a qbn framework for depicting how quantum conditional states can result from the influence of two systems at one time or from one system at two times other formulations and applications of qbns are found in the literature although at present there does not seem to be any generally available software by which qbns can be constructed power pc theory and causal bns power pc probabilistic contrast theory states that a system outcome is the sum effect of the relative power of observed and unobserved causal relations which can be depicted and partitioned mathematically cheng 1997 norick and cheng 2004 in applying power pc theory lu et al 2008 demonstrated how the relative influence of different independent causes can be determined empirically and can be represented in bayesian causal networks here we have covered a range of significant recent advances in application of bns still other variations and new approaches to bn modeling continue to appear in the literature 2 2 beyond the network a new era of integration bn models of various forms have been increasingly used in a variety of applications they are also being specifically integrated with other modeling constructs which we refer to here as integrated bayesian networks ibns johnson et al 2010 ibns can be defined as bn structures that are explicitly embedded within the framework of other modeling constructs instead of just being applied to some area of inquiry as reviewed in the previous section examples of ibns include assimilating bns in structured decision making frameworks agent based models and state and transition models table 1 ibns are crafted generally to apply the probabilistic basis of bns to new areas of application and research such as dynamic and stochastic simulation modeling ibns are also becoming useful tools in risk analysis risk management and decision science such as in environmental resource planning johnson and mengersen 2012 fraser et al 2017 and for depicting how deep uncertainty affects policy decisions aven 2013 cox 2012 janssens et al 2006 developed an ibn that combined bns and decision trees for developing decision rules in transportation management qgenie modeler bayesfusion llc has a graphical user interface that provides for rapid prototyping of decision models in a bn environment some bn modeling platforms such as netica norsys inc and hugin provide application program interfaces apis to facilitate integration links to other programs such as geographic information systems which can facilitate their use in integrated risk analysis and structured decision making under uncertainty e g barton et al 2008 an emerging area is the development of ibns operating from real time monitoring data for example maglogiannis et al 2006 have proposed a patient health risk analysis system using ibns operating from vital sign monitoring data their vision is to produce a real time system for homecare telemedicine penman et al 2015b developed a fire danger rating system that updated daily with meteorology forecasts and as new fires appeared in the landscape the model automatically updated the risk projections vagnoli et al 2017 proposed a real time ibn based system to monitor the structural integrity of railway bridges in europe koen et al 2017 developed an ibn model to monitor the poaching illegal hunting of rhinoceroses in kruger national park south africa which is being implemented as a real time management tool 3 things to come the field will continue to advance more rapidly into uncharted territory as ibns become more sophisticated future ibn research foci will include substantial cutting edge advancements in the many areas of integration reviewed above currently ibns are essentially static however we expect that in the near future ibns will emerge that are self updating and self improving and that will learn from real time continuous input of environmental monitoring data being able to dynamically update bn conditional probability values with new data e g using an expectation maximization algorithm and to continually recalculate posterior probabilities are quite feasible with existing software and hardware some current ibns do automatic recalculation of posterior probability outcomes with new input data feeding the models but here we are referring also to the basic structure and underlying conditional probability tables themselves being created anew and updated with machine learning tools the purpose of this would be to continually refine the context accuracy and robustness of the models which is a fundamental precept and advantage of bayesian statistical approaches that improve model predictions from prior data some aspects of this have appeared in the application of bns in areas of finance giudici and spelta 2016 garvey et al 2015 and medical diagnosis and decision support constantinuo et al 2016 in the future we envision self creation of ibns based on emergent information from crowdsourced data park and budescu 2015 also to come will be the further integration of bns with expert system knowledge bases particularly using fuzzy logic or neural net forms of knowledge representation and machine learning algorithms in the past control rule based expert systems used confidence factors or scoring rules zohar and rosenschein 2008 e g indices as provided by the expert and scaled 1 10 to rank the likelihood or the credibility of a particular inference or outcome today the bn modeling shell bayesialab bayesia s a s conrady and jouffe 2015 averages conditional probability values from multiple experts and weights the values by the product of confidence in and credibility of the values as scored by each expert 4 conclusions and perspectives what are some of the main cautions and caveats in this new era of ibns here we address quandaries of ensuring validity and credibility of ibns that are becoming increasingly complex in construction and interpretation particularly as they are induced from automated and big data sources 4 1 ensuring a future of validity and credibility it will be important to ensure the validity and credibility of increasingly complex expert based ibn systems kleemann et al 2017 particularly as they interact with human social systems as they guide resource management decisions and as they operate with greater autonomy as ibns become more complex it will become more problematic to create and test simple intuitive and understandable influence diagrams and mind maps that chart their structures logic and operation a way around this could entail decomposing such complex systems into simpler component submodels and testing and updating each submodel bns are generally constructed as markov processes so that they can be dissected and reassembled without loss of information particularly using cutpoints in the network graph nodes in the bn whose removal would separate the graph direct and indirect causes in process models will be increasingly difficult to clearly identify particularly with models having multiple interaction terms feedback loops latent variables and synergistic functions among covariates and response variables this also means increasing difficulty in parsing out sensitivity and influence effects to specific drivers that is clearly identifying key factors that most influence outcomes and for which management might have greatest uncertainty and greatest or least control the result may limit their acceptance and adoption in management situations if users do not understand and trust the models and if the models have limited face validity this can be a major issue with decision networks and use of models in real time and with models intended to guide planning and management of resources with high opportunity costs support for the model as with advances in validation methods and frameworks as discussed above is needed some work advancing methods of sensitivity analysis of bn models may show promise for further development and application on increasingly complex model structures e g global sensitivity analysis methods of li and mahadevan 2017 increasing complexity of ibns will carry increased difficulty in model calibration testing validation and updating eventually necessitating new heuristic approaches and algorithms that can wade the swamp of big data spiegelhalter 2014 ladeau et al 2017 lv et al 2014 lewis et al 2018 further warned that developing wildlife biology models even with high quality big data sets raises concerns for how opaque modeling algorithms can lead to complex problems of data management exploratory data analysis data sharing and reproducibility paradoxically big data sets with many variables often are sparse in terms of replicated conditions and variable combinations hastie et al 2015 although bn models induced from big data may fit well that is with high calibration accuracy they may lack robustness and perform poorly because of this sparsity that is the models may have low independent validation accuracy and be overfit also bns built on big data collected through citizen science initiatives which are becoming increasingly popular in many topic areas should be carefully vetted for the accuracy of those data kosmala et al 2016 or else the models and their interpretations may be incorrect biased and misleading particularly with self generating and self updated systems ibns will be no more trustworthy than the data on which they are based and on the opacity of the algorithms used to structure their networks and parameterize their probabilities in the end the validity and credibility of ibns are inextricably linked to those of the data and the methods on which they are based and created at stake is clear demonstration of expert knowledge empirical data and model simulations and ultimately of model validity operational robustness and the credibility of the modeling science itself despite potential worries over the testing and validity of big data sourced ibns and the quickly evolving environments in which ibns are constructed their strengths will remain in what if scenario exploration and in being able to combine soft and hard evidence that is expert knowledge and empirical data even with an exponential increase in publicly available information combining expert knowledge with data is best conducted so as to detect and counteract any spurious correlations that may be indicated by machine learning algorithms models that are structured and parameterized solely by use of blind automatic methods such as unguided machine learning algorithms without human oversight lose the key advantage of the bayesian approach to updating human knowledge and insight 4 2 cautions and caveats in the new era of integration we have tried here to chart some routes that development of ibn systems may be taking and some cautions indicated in this fast evolving era salmond et al 2017 soon to come will be dynamic and self modifying and even self creating ibns using big data garnered from automated monitoring e g environmental and earth science remote sensing data li et al 2016 citizen science initiatives and even crowdsourced information sources we ask to what degree should we trust such models for in the end the knowledge source and expertise that may serve to generate and fuel ibns may become a fully realized global emergent artificial intelligence if so how shall the models veracity be determined when their source becomes non human bns developed from empirical data often can be tested using existing cross validation and jackknifing algorithms however the validity of bns developed from expert knowledge is more difficult to determine if independent data sets by which to test the models are unavailable or if specific combinations of variables in big data sources are sparse in such cases peer review of the model at various stages of development is essential to establish credibility and conformity with accepted precepts but determining the external validity of self organizing and self updating bns that are developed with deep learning algorithms in real time from big data sources including citizen science and crowd sourced data especially the multitude of internet blog sources and news posts without independent testing or review may be most problematic and will require new approaches to scientifically evaluating their veracity this will become one of the greater challenges in the fast evolving new era of bn model integration acknowledgments inspiration for this paper comes from a keynote address given by the senior author in 2017 at the joint conference of the australasian bayesian network modelling society and the society for risk analysis australia and new zealand at the university of melbourne australia we thank tom bruce anca hanea sandra johnson and two anonymous reviewers for their helpful reviews of the manuscript appendix a supplementary data the following is the supplementary data to this article data profile data profile appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 09 016 
26279,three dimensional dynamic visualization of the long range gas diffusion contributes to disaster forecasting and assessment existing three dimensional visualization methods which calculate the gas concentration without considering the terrain and display the gas concentration in the local range by isosurfaces cannot meet the demand for intensive calculation and dynamic visualization of gas diffusion over a wide range this study proposes a virtual globe based three dimensional dynamic visualization method of the long range gas diffusion consisting of a voxel based multi scale data model a terrain dependent multi scale concentration field calculation method using graphics processing units gpu and a gpu based multi scale spherical ray casting algorithm the method is then applied to the long range natural gas diffusion and is evaluated according to the efficiency of the concentration calculation and dynamic visualization the results show that the proposed method can achieve dynamic visualization of the long range gas diffusion above 30 frames per second keywords virtual globes gas diffusion dynamic volume rendering gpu multi scales software availability name of software virtual globe based gas diffusion visualization contact address state key laboratory of information engineering in surveying mapping and remote sensing wuhan university wuhan 430079 china year first required 2018 hardware required tested on dell pc software required ms windows tested on windows 10 programming language ms visual studio 2010 c glsl version110 program size 97mb compressed availability and cost https github com chengruozhen gasdiffusionvisualization git maintenance contact via e mail 1 introduction leakage of gas transmission pipelines covering large areas and long distances can easily cause the long range gas diffusion this is affected by meteorological conditions gas properties and complex terrain hu et al 2015 visualization of dynamic processes of long range gas diffusions can intuitively demonstrate changes in concentration distribution during gas diffusions and aid in the assessment and prediction of environmental impacts cheng et al 2009 koussoulakou 1994 therefore it is necessary to research the dynamic visualization for long range gas diffusions which can support the auxiliary analysis of the environmental impact caused by gas diffusions the accuracy of the concentration distribution when dynamically visualizing gas diffusion depends on the accuracy of the gas concentration calculation contemporary soft computing technology solves uncertain problems in different fields through neural network chen and chau 2016 gholami et al 2015 taormina et al 2015 fuzzy logic sefeedpari et al 2016 intelligent computing fotovatikhah et al 2018 and other computing models zhang and chau 2009 and does not insist on high accuracy in order to exhibit a more accurate gas concentration distribution during dynamic visualization it is necessary to establish a mathematical model of the gas diffusion process to calculate the exact gas concentration the existing three dimensional visualization of gas diffusion models the gas diffusion process through the gaussian diffusion model and displays the gas concentration in the form of three dimensional isosurfaces jian and fan 2014 wang et al 2013 zahran et al 2013 the gaussian diffusion model is one of the most important methods that are widely used to estimate pollution levels this model has been applied extensively in the study of emissions from large industrial operations as well as a variety of other applications chen and qi 2010 ebrahimi and jahangirian 2013 liu et al 2012 and is suitable for concentration estimation under flat terrain conditions chang and weng 2013 the limitations of the existing three dimensional visualization of gas diffusion include three aspects first it is an idealized simulation within a local range the effect of terrain on the gas concentration is not considered when calculating the gas concentration second the concentration calculations within the local range are based on central processing units cpu which lead to low computational efficiency in addition they can only achieve static visualization because the extraction efficiency of the three dimensional isosurface is affected by the amount of data hou and chen 2016 newman and yi 2006 therefore the existing three dimensional visualization method for gas diffusion cannot meet the demand for intensive calculation and dynamic visualization of gas diffusion over a wide range three dimensional dynamic visualization of the long range gas diffusion must consider 1 a large range of terrain and 2 the high calculation and dynamic visualization requirements of a massive concentration dataset for the first requirement virtual globe software supports the access and visualization of a large range of three dimensional terrain goodchild 2008 providing efficient methods for presenting the long range geographical processes on the earth s surface gong et al 2010 yu and gong 2012 for the second requirement the programmable characteristics of graphics processing units gpu allow its application to general computing owens et al 2007 its simulation speed and performance is substantially higher than that of cpu based computing jr molnar et al 2010 yang and wu 2010 further gpu parallel computing power supports intensive calculations of gas concentration owens et al 2008 gpu based volume rendering techniques based on a virtual globe have been widely used in the visualization of three dimensional meteorological fields li et al 2011 liang et al 2014 liu et al 2015a 2015b and can effectively represent the internal structure of three dimensional or four dimensional phenomena li et al 2013 furthermore the parallelism of volume rendering techniques can realize the migration to gpu and support three dimensional or four dimensional visualizations of real time interactions du et al 2015 su et al 2016 these characteristics enable the three dimensional dynamic visualization of massive gas concentration datasets however the visualization of the above mentioned three dimensional meteorological field is based on a multi scale data model within the meteorological field which makes it difficult to match with the global terrain on the virtual globe in addition in the volume rendering of a three dimensional meteorological field a large number of sampling points outside the field are introduced and frequent coordinate transformation calculations are executed thus this study integrates the gas diffusion model and the data organization characteristics of the virtual globe and proposes a virtual globe based three dimensional dynamic visualization method for gas diffusion this method is designed to meet the calculation demands of massive concentration datasets consider the effect of terrain and perform dynamic visualization of the long range gas diffusion in the method presented in this study 1 a voxel based multi scale data model under a global framework is proposed to organize gas concentration fields this overcomes the fact that the multi scale data model in a certain range in previous studies is difficult to match with the global terrain on the virtual globe and is difficult to express the time characteristics of the three dimensional field 2 a gpu based calculation method for multi scale gas concentration fields which considers terrain is proposed for the rapid calculation of gas concentration this solves the problem of low efficiency and lack of consideration of terrain when calculating the gas concentration field based on the cpu and the calculation results can be directly applied to the subsequent visualization 3 a gpu based multi scale spherical ray casting algorithm is proposed for spherical visualization of the concentration field this algorithm avoids the introduction of sampling points outside the field and improves the ray casting process and 4 the temporal properties of voxels are updated in the concentration field and the concentration calculation and visualization of the concentration field are repeated thus achieving dynamic visualization of the long range gas diffusion the structure of this paper is as follows section 2 describes the virtual globe based three dimensional dynamic visualization method of gas diffusion section 3 uses an example of the long range natural gas diffusion to conduct experiments and then evaluate and analyze the proposed method section 4 presents the conclusions and proposes further research directions 2 methodology our virtual globe based three dimensional dynamic visualization method of gas diffusion includes three parts fig 1 1 to organize the gas concentration field the three dimensional global space is divided into multi scale blocks based on these the voxel based multi scale data model is proposed then multi scale voxels of the concentration field are constructed based on the data model of the concentration field 2 to calculate the gas concentration field which is based on the multi scale voxels of the concentration field gas concentration calculation is implemented through texture mapping according to the gpu based calculation method of the multi scale concentration field considering the terrain and the concentration field is finally stored as a three dimensional texture 3 to render the gas concentration field which is based on the three dimensional texture of concentration the gpu based multi scale spherical ray casting algorithm is used for spherical visualization of the concentration field the time controller updates the temporal properties of voxels in the concentration field at a certain time interval and repeats the concentration calculation and three dimensional visualization of the concentration field to achieve three dimensional dynamic visualization of the gas diffusion 2 1 voxel based data organization spatially continuous concentration fields have multi dimensional spatio temporal features however due to the limitations of texture space memory space and computational performance the concentration fields need to be divided into multi scale data units to visualize the different scales of the concentration field as the viewpoint changes thereby improving the dynamic visualization efficiency therefore we divide the three dimensional global space into multi scale blocks and establish the global framework based on an octree fig 2 a the voxel based multi scale data model fig 2 b is proposed under a global framework and the spatio temporal features of the concentration field are described using voxels a voxel consists of three parts spatial extent sampling points and time the spatial extent describes the spatial scale of the concentration field including the corresponding block and spherical grid the sampling points store the geographical coordinates and the corresponding terrain the time describes the temporal characteristics of the concentration field based on the data model of the concentration field sampling points in the block are used to construct multi scale voxels fig 2 c by dividing the concentration field into multi scale voxels the multi scale concentration field is constructed fig 2 d the blocks consisting of red green and yellow lines in fig 2 d represent voxels of adjacent scales 2 1 1 octree based global framework the octree based global framework can cover the global spatial extent and achieve multi scale expression and fast retrieval of any spatial range the spatial extent under the global framework consists of the entire spherical space and an altitude from 0 to 100 km starting from the point at a longitude of 180 west a latitude of 90 south and an altitude of 0 m the three dimensional global space is divided into two subspaces according to longitude and latitude spans of 90 the two subspaces form level 0 of the octree structure the interior of each subspace is evenly divided in latitude longitude and altitude directions and the segments are encoded from west to east from south to north and from bottom to top thus the three dimensional global space is divided into octree based blocks k layer column row is the identifier of the block which then serves as the identifier of the voxel where k is the subdivision level and layer column row is the coding of altitude longitude and latitude respectively k column row is the identifier of the spherical grid corresponding to the block blocks can be indexed by the identifier table 1 where floor indicates rounding down 2 1 2 multi scale concentration field construction assuming that the concentration field ranges from lon1 lat1 alt1 to lon2 lat2 alt2 where lon1 lat1 alt1 and lon2 lat2 alt2 are the geographical coordinates of the points respectively that is the spatial extent of the concentration field is determined by the longitude ranging from lon1 to lon2 the latitude ranging from lat1 to lat2 and the altitude ranging from alt1 to alt2 the terrain in the concentration field is represented by terrain tiles terrain tiles are organized in a global spherical quadtree structure when constructing a global spherical quadtree starting from the point at a longitude of 180 west and a latitude of 90 south the two dimensional spherical space is divided into two subspaces according to longitude and latitude spans of 90 the two subspaces form level 0 of the quadtree structure the interior of each subspace is evenly divided in latitude and longitude directions and the segments are encoded from west to east and from south to north thus the two dimensional spherical space is divided into quadtree based tiles k column row is the identifier of the terrain tile where k is the subdivision level and column row is the coding of longitude and latitude respectively the size of all terrain tiles is the same and the spatial resolution of the terrain tile at level k is 4 times the spatial resolution of the terrain tile at level k 1 according to the voxel based multi scale data model under the global framework the algorithm for constructing the multi scale concentration field is as follows 1 c o l u m n i f l o o r 2 k  180 180 r o w i f l o o r 2 k 1  90 90 l a y e r i f l o o r 2 k h 100000 2 w e s t c o l u m n a 180 2 k 180 e a s t c o l u m n b 180 2 k 180 180 2 k s o u t h r o w a 90 2 k 1 90 n o r t h r o w b 90 2 k 1 90 90 2 k 1 a l t i t u d e 1 l a y e r a 100000 2 k a l t i t u d e 2 l a y e r b 100000 2 k 100000 2 k input point a lon1 lat1 alt1 and point b lon2 lat2 alt2 multi scale terrain data step 1 for each subdivision level k perform step 2 to step 6 step 2 calculate the identifier of voxels where point a and point b are located using eq 1 and mark them as k layer a column a row a and k layer b column b row b respectively voxels of the concentration field are marked with k layer i column i row i and the corresponding spherical grids are marked with k column i row i thus the concentration field formed by voxels ranges from west south altitude1 to east north altitude2 using eq 2 where l a y e r a l a y e r i l a y e r b c o l u m n a c o l u m n i c o l u m n b r o w a r o w i r o w b   h in eq 1 is the geographical coordinate step 3 for each voxel k layer i column i row i perform step 4 to step 6 step 4 sample in the direction of longitude latitude and altitude in accordance with the number of 2 m 2 n and 2 l to access the geographical coordinates where m 0 n 0 l 0 step 5 match multi scale terrain data by k column i row i and obtain the terrain of the sampling points in step 4 step 6 the initial time of the gas diffusion is stored as the temporal attribute of the voxel output multi scale voxels of the concentration field point a1 west south altitude1 and point b1 east north altitude2 2 1 3 dynamic update of concentration field in order to achieve dynamic visualization of gas diffusion the time controller needs to update the temporal properties of the voxels in the concentration field at certain time intervals and then calculates and visualizes the concentration field at the next diffusion moment since calculating and visualizing the gas concentration field do not involve all voxels directly updating the time attributes of all voxels will lead to unnecessary efficiency costs when updating the time attributes of voxels the update algorithm of voxels is used to reduce the number of updated voxels and to improve the dynamic visualization efficiency of gas diffusion the update algorithm of voxels is as follows input voxels at time t1 step 1 determine if the spatial scale of the concentration field changes at time t2 if not perform step 2 otherwise perform step 3 step 2 update the time attribute of current voxels to time t2 step 3 if the upper level voxels are scheduled the time attributes of the parents of current voxels are updated as t2 if the next level voxels are scheduled the time attributes of the eight children of current voxels are respectively updated as t2 output voxels at time t2 2 2 terrain dependent concentration calculation using gpu the calculation of the long range concentration fields is intensive and independent and both terrain and the accumulation of concentration over time must be considered in the concentration calculation however the existing calculation of the concentration field is implemented on cpu chen and qi 2010 jian and fan 2014 liu et al 2012 the floating point operation of a large number of sampling points imposes a great burden on cpu fan et al 2009 corrected gas concentrations on the ground alone without considering the influence of terrain on gas concentration in the three dimensional space and the accumulation of gas concentrations over time zhang et al 2010 considered the accumulation of concentrations over time but did not consider the effect of terrain therefore we propose the gpu based method for calculating the multi scale concentration field whilst considering the terrain when the concentration calculation is performed on the gpu the gaussian diffusion model is used as the basic model and corrections are taken for considering the effects of time and terrain see eq 3 the terrain factor t see eq 4 is introduced during the accumulation of concentration over time and the influence of the terrain factor is extended to the entire three dimensional space x y z in eq 3 represent the position of the sampling point relative to the leakage source in the cartesian coordinate system while the position of the voxel of the concentration field on the virtual globe uses the geographic coordinates so the relative positions of sampling points in the geographic coordinate system need to be transformed to the cartesian coordinate system using eq 5 the parameters in eqs 3 5 are shown in table 2 3 c x y z t 0 t q d t 2  3 2  x  y  z e x p x u t r t 2 2  x 2 e x p y 2 2  y 2 e x p z t h 2 2  z 2 e x p z t h 2 2  z 2 4 t 0 h h 1 h h h h 5 x l o n g i t u d e l o n g i t u d e s o u r c e  r a d i u s 180 y l a t t i t u d e l a t i t u d e s o u r c e  r a d i u s 180 z a l t i t u d e a l t i t u d e s o u r c e when calculating the multi scale concentration field voxels at subdivision level k and gaussian diffusion model parameters are input to gpu and the concentration calculation is implemented in the shader through texture mapping the result of the concentration calculation is stored as the three dimensional texture where each texel corresponds to a single sampling point in the concentration field and the color of the texel represents the gas concentration at that sampling point because gpu is designed to render into the two dimensional frame buffer the concentration calculations in the shader need to be performed separately for each slice of the three dimensional texture according to section 2 1 2 the algorithm for calculating the concentration field at subdivision level k is as follows 6 l 2 l 1 l a y e r b l a y e r a 1 1 m 2 m 1 c o l u m n b c o l u m n a 1 1 n 2 n 1 r o w b r o w a 1 1 7 l a y e r f l o o r e 2 l 1 l a y e r a c o l u m n f l o o r i 2 m 1 c o l u m n a r o w f l o o r j 2 n 1 r o w a 8 c o l u m n v i 2 m 1 f l o o r i 2 m 1 r o w v j 2 n 1 f l o o r j 2 n 1 l a y e r v e 2 l 1 f l o o r e 2 l 1 9 l o n g i t u d e w e s t i m 1 e a s t w e s t l a t i t u d e s o u t h j n 1 n o r t h s o u t h a l t i t u d e a l t i t u d e 1 e l 1 a l t i t u d e 2 a l t i t u d e 1 10 r 1 c c high c c h i g h c l o w c c h i g h 0 c c l o w g 0 c c h i g h o r c c l o w 1 c c h i g h c l o w c c h i g h b 0 c c l o w c c c l o w a 0 input voxels at subdivision level k gaussian diffusion model parameters point a1 west south altitude1 and point b1 east north altitude2 step 1 input point a1 point b1 time properties of voxels and gaussian diffusion model parameters into gpu step 2 for the eth slice of the three dimensional texture perform step 3 to step 8 respectively where 0 e l 1 step 3 draw m n rectangles of size 1 1 to form a rectangle of size m n the vertex coordinate is i j where 0 i m 1 0 j n 1 and l m n are calculated using eq 6 step 4 determine the voxel k layer column row where the vertex i j is located and the corresponding sampling point column v row v layer v using eqs 7 and 8 thereby acquiring terrain and then input it into gpu as a vertex attribute where column v row v and layer v are the column row and layer respectively within the voxel where the sampling point is located step 5 through the texture mapping of a rectangle of size m n perform step 6 to step 8 in the shader step 6 map the vertex coordinate i j to the geographical coordinate longitude latitude altitude using eq 9 step 7 calculate the gas concentration at longitude latitude altitude using eqs 3 5 step 8 map the concentration to rgba space assuming that the visualized concentration ranges from c low to c high the range of concentration affects the mapping of concentrations to colors the concentration is mapped to color using eq 10 output the three dimensional texture of the concentration field of size m n l during visualization the concentration field of the corresponding spatial scale is calculated according to the current subdivision level and the three dimensional texture of the concentration field is obtained which is used directly in the subsequent rendering process 2 3 spherical volume rendering using gpu in existing methods for visualizing the concentration field using three dimensional isosurfaces the amount of data directly affects the efficiency of isosurface extraction hou and chen 2016 and thus cannot meet the requirements of dynamic visualization the ray casting algorithm in volume rendering assumes that rays shot from the viewpoint pass through the volume bounding box and samples on the ray within the bounding box to accumulate the color of the final screen pixel because the computation of each ray emitted by the viewpoint is relatively independent the algorithm can be easily migrated to gpu to support the dynamic visualization of real time interaction the bounding box of the gas concentration field on the virtual globe is a spherical bounding box therefore the ray casting algorithm on the virtual globe must consider the adaptation to the spherical bounding box in order to not introduce sampling points beyond the spherical bounding box liang et al 2014 directly sampled the ray within the spherical bounding box however each sampling point was transformed from the cartesian coordinate system to the geographic coordinate system and then to the texture coordinate system resulting in the frequent calculation of coordinate transformation during ray casting moreover the algorithm used a single scale volume bounding box and does not consider multi scale visualization of volume data as a result we propose a gpu based multi scale spherical ray casting algorithm the algorithm constructs the multi scale spherical bounding box based on voxels which consists of two spherical shells and four vertical walls fig 3 a through the texture mapping of the spherical bounding box the spherical volume rendering is implemented based on the three dimensional texture in section 2 2 during spherical volume rendering the spherical bounding box is shrunk and the ray casting process is improved to enhance visualization efficiency the spherical bounding box is reduced by ignoring the space below the terrain to reduce the number of sampling points on the ray in the improved ray casting process geographical coordinates of the entry point and exit point on the ray are calculated fig 3 b and transformed to the texture coordinate system and the other sampling points are obtained directly in the texture coordinate space fig 3 c thus reducing the frequent coordinate transformation calculations during ray casting according to sections 2 1 2 and 2 2 the spherical ray casting algorithm at subdivision level k is as follows 11 x r a d i u s a l t i t u d e cos l a t i t u d e cos l o n g i t u d e y r a d i u s a l t i t u d e cos l a t i t u d e sin l o n g i t u d e z r a d i u s a l t i t u d e sin l a t i t u d e 12 l o n g i t u d e a r c t a n y x l a t i t u d e a r c s i n z x 2 y 2 z 2 a l t i t u d e x 2 y 2 z 2 r a d i u s 13 e n t r y e y e e x i t e y e a 1 e y e e x i t e y e e y e a 1 e y e e x i t e y e b 1 e y e e x i t e y e e y e b 1 e y e a 1 e y e b 14 t e x c o o r d e n t r y e n t r y t e x g e n t e x c o o r d e x i t e x i t t e x g e n t e x g e n 1 e a s t w e s t 0 0 w e s t e a s t w e s t 0 1 n o r t h s o u t h 0 s o u t h n o r t h s o u t h 0 0 1 a l t i t u d e 2 a l t i t u d e 1 a l t u t d e 1 a l t i t u d e 2 a l t i t u d e 1 0 0 0 1 15 d e l t a c o o r d t e x c o o r d e n t r y t e x c o o r d e x i t i n t e r v a l l e n g t h 16 f r a g c o l o r r g b f r a g c o l o r r g b c r g b c a 1 c a f r a g c o l o r a f r a g c o l o r a c a input voxels at subdivision level k point a1 west south altitude1 and point b1 east north altitude2 three dimensional texture of the concentration field step 1 construct the spherical bounding box both spherical shells at altitude1 and altitude2 are formed of m 1 n 1 rectangles the two vertical walls in the west and east directions are formed by n 1 rectangles and the two vertical walls in the south and north directions are formed by m 1 rectangles the vertices in the vertical walls are the vertices of the edge of the spherical shells step 2 determine the voxel k layer column row where the vertex is located and the corresponding sampling point column v row v layer v using eqs 7 and 8 thereby acquiring the geographical coordinate and terrain where e is l and 1 at altitude1 and altitude2 in eqs 7 and 8 respectively step 3 compare the terrain of all vertices of the spherical shell at altitude1 to obtain the minimum terrain min thus the reduced spherical bounding box ranges from point a1 west south terrain min to point b1 east north altitude2 step 4 transform the geographical coordinates longitude latitude altitude of the vertices of the smaller spherical bounding box to cartesian coordinates x y z using eq 11 and draw the box where radius is the earth s radius using texture mapping of the reduced spherical bounding box perform step 5 to step 7 in the shader step 5 construct the ray from the viewpoint to the vertex transform the vertex and viewpoint from the cartesian coordinate system to the geographic coordinate system using eq 12 the vertex serves as the exit point of the ray exit and the entry point of the ray entry is calculated using eq 13 where eye is the geographic coordinates of the viewpoint step 6 calculate the texture coordinates of the entry point texcoord entry and the exit point texcoord exit using eq 14 and obtain the length of the ray length in texture coordinate space along the direction from the exit point to the entry point sampling on the ray according to the spacing interval the texture coordinate increment of the adjacent sampling points deltacoord is calculated using eq 15 step 7 sample the three dimensional texture of the concentration field according to the texture coordinates of sampling points and accumulate the color of the screen pixel using eq 16 where fragcolor is the color of the screen pixel and c is the color of the three dimensional texture output the concentration field at subdivision level k during visualization the spherical bounding box of the corresponding spatial scale is constructed according to the current subdivision level based on the three dimensional texture of the concentration field in section 2 2 spherical volume rendering of the concentration field is achieved 3 implementation and analysis here we validate the proposed method by simulating an example of the long range natural gas diffusion the simulated data is shown in table 3 the simulated data includes the spatial extent of the natural gas concentration field and the parameters of the leakage source the spatial extent of the natural gas concentration field is determined by the longitude ranging from 105 5 e to 105 9 e the latitude ranging from 31 0 n to 31 4 n and the altitude ranging from 0 m to 2000 m that is the natural gas concentration field ranges from point a 105 5 e 31 0 n 0 to point b 105 9 e 31 4 n 2000 the parameters of the leakage source include the location the leakage rate the average wind speed and the total leakage time according to section 2 1 2 the multi scale natural gas concentration field is constructed the number of sampling points in the longitude latitude and altitude direction of a single voxel in the concentration field is 2 m 2 n and 2 l respectively in order to ensure a uniform distribution of sampling points the ratio of the number of sampling points in each direction is 2 m 2 l 2 n 2 l 105 9 105 5  r a d i u s 180 2000 22 24 taking into account the sampling density of the concentration field and constraints of texture space the dimension of the sampling points within the voxel is 64 64 2 using subdivision level 8 to level 11 under the global framework as an example to show the dynamic visualization of natural gas diffusion the dimension of sampling points in the concentration field at corresponding subdivision levels are 64 64 7 127 127 12 190 190 22 and 316 379 42 respectively the multi scale terrain data comes from terrain tiles at level 8 to level 11 in google earth and the terrain tiles at level 11 have a spatial resolution of 152 87 m given the fact that c performs more efficiently than other programming languages and glsl is a shader language based on c c and opengl that enables the gpu s programmable features the experiments were implemented on osgearth using c and glsl and run on a windows desktop pc with intel r xeon r e3 1240 v6 3 70 ghz cpus nvdia quadro k620 graphics with 16 gb of ram and 2 gb of video memory the dynamic process of natural gas diffusion on the virtual globe is shown for a time step of 5 min fig 4 fig 4 a to l are snapshots at diffusion times of 5 min 10 min 15 min 20 min 25 min 30 min 35 min 40 min 45 min 50 min 55 min and 60 min respectively at subdivision level 11 the color changing from blue to red indicates the change in concentration from 10mg m3 to 50 mg m3 the dynamic visualization of the natural gas diffusion shows that the concentration distribution presents a ring like structure and expands outwards over time the diffusion areas of natural gas in the concentration range of 10 mg m3 to 50 mg m3 in fig 4 a to l are recorded in table 4 which indicates that the diffusion range increases with the diffusion time with the multi scale strategy different scales of the concentration field are rendered as the viewpoint changes fig 5 a d are snapshots of the concentration field at subdivision levels 8 to 11 for a diffusion time of 30 min it shows that when the viewpoint is closer to the concentration field a finer scale concentration field is rendered which is reflected in the clearer edges of areas in different color when the viewpoint is far from the concentration field the coarser scale concentration field is rendered which is reflected in the gradual blurring of the edges of areas in different color fig 5 d f show the concentration field at subdivision level 11 for a diffusion time of 30 min from different viewpoints the diffusion areas of natural gas in the concentration range of 10 mg m3 to 50 mg m3 in fig 5 are all 278 42 km2 a video component is provided which demonstrates an example of the long range natural gas diffusion simulated in table 3 to access this video component simply click on the image below online version only supplementary video related to this article can be found at https doi org 10 1016 j envsoft 2018 09 019 the following is are the supplementary data related to this article video 1 dynamic visualization of the long range natural gas diffusion video 1 when the gas concentration field is dynamically visualized by the proposed method the calculation and the dynamic visualization of the gas concentration field are successively performed therefore the evaluation of the proposed method includes two aspects efficiency of the concentration field calculation and efficiency of the dynamic visualization the calculation of the concentration field includes the calculation of the concentration of all sampling points in the concentration field so the calculation time of the concentration field is the most direct metric for evaluating the calculation efficiency of the concentration field by comparing the calculation time of the cpu based method and the gpu based method table 5 the efficiency of the proposed method can be qualitatively and quantitatively described when using gpu the rendering environment needs to be initialized thus when the number of sampling points is small such as in subdivision level 8 cpu based computation is more efficient than gpu as the number of sampling points increases the parallelism of gpu based computing increases computational efficiency at subdivision level 11 gpu based computational efficiency is approximately 15 times that of cpu for visualization systems the evaluation of rendering efficiency is necessary the frames per second fps is a direct reflection of the rendering efficiency of the visualization framework so we use the fps which is common in the visualization region to quantitatively evaluate the dynamic visualization efficiency of the proposed method since the proposed method uses multiple strategies to optimize the dynamic visualization efficiency it is necessary to evaluate the impact of various optimization strategies on the efficiency of dynamic visualization evaluations includes a comparison of fps with and without multi scale strategy a comparison of fps before and after shrinking the spherical bounding box and a comparison of fps between liang s method 2014 and the proposed method in order to ensure the reliability of the evaluation the fps of the dynamic visualization is obtained based on the same roaming path and the roaming duration is 10 s fig 6 compares the fps with and without the multi scale strategy in dynamic visualization when roaming the scene as viewpoint changes an increase or decrease in the rendered area of the screen causes the fps to decrease or increase with the multi scale strategy a coarser scale concentration field is rendered when the viewpoint is far from the concentration field meaning that the number of sampling points in the concentration field decreases by 23 times leading to a sharp increase in fps the average fps with and without the multi scale strategy in dynamic visualization are 41 45 and 18 01 respectively the multi scale strategy increases the average fps of dynamic visualization by approximately 2 3 times which proves its effectiveness table 6 shows the reduced proportions of the spherical bounding box at different subdivision levels the average shrinking rate of the spherical bounding box in dynamic visualization is 15 36 the average fps before and after shrinking the spherical bounding box is 20 36 and 41 45 respectively fig 7 by shrinking the spherical bounding box the number of sampling points on the ray within the spherical bounding box is reduced resulting in approximately double the average fps both the proposed method and liang s method 2014 perform volume rendering on the virtual globe based on spherical bounding boxes and do not bring a large number of sampling points outside the field therefore by comparing the visualization efficiency of the proposed method and liang s method 2014 it can directly reflect the advantages of the proposed method in visualization efficiency when performing volume rendering based on spherical bounding boxes fig 8 compares the fps between the proposed method and liang s method in dynamic visualization because the proposed method is based on the multi scale spherical bounding box and an improved ray casting process the fps in dynamic visualization is maintained above 30 however liang s method is based on a single scale spherical bounding box and the point by point calculation of coordinate transformation during ray casting reduces the computational efficiency so the fps in dynamic visualization is always under 15 the average fps values of the proposed method and liang s method are 41 45 and 10 17 respectively thus the proposed method increases the average fps in dynamic visualization by approximately four times the proposed method and liang s method 2014 are also compared in terms of visualization quality applicability and cases in terms of visualization quality since both the proposed method and liang s method 2014 achieve the rendering of a three dimensional field by sampling on the ray within the spherical bounding box and then accumulating the colors of the screen pixels in the case of the same number of sampling points in the concentration field the visualization quality of the two methods is the same in addition the proposed method considers the visualization of multi scale volume data while the liang method 2014 only considers the visualization of single scale volume data therefore when visualizing the multi scale concentration field the proposed method sacrifices the visualization quality of the coarser scale concentration field in order to optimize the dynamic visualization efficiency in terms of applicability liang s method 2014 only considers single scale volume data so it can be applied to the visualization of three dimensional fields on a single scale the proposed method considers multi scale volume data and is suitable for the dynamic visualization of multi scale three dimensional fields in terms of cases liang s method 2014 can be applied to the visualization of datasets of three dimensional fields the proposed method includes the calculation and the dynamic visualization of the gas concentration field so it can be applied to the rapid estimation and dynamic visualization of the long range gas diffusion the voxel based multi scale data model and the gpu based multi scale spherical ray casting algorithm in this study can also be applied to multi scale dynamic visualization of various three dimensional fields 4 conclusions three dimensional dynamic visualization over the long range gas diffusion involves the calculation and dynamic visualization of massive gas concentration datasets while taking into account the effects of a large range of terrain with its massive data organization and multi scale display capabilities the virtual globe is regarded as an ideal platform for the three dimensional visualization of the long range gas diffusion however existing three dimensional visualization of gas diffusion involves local scale static and does not consider the influence of terrain on gas concentration moreover gas concentration calculations using cpu and the extraction of three dimensional isosurfaces make it difficult to achieve dynamic visualization of gas diffusion therefore we proposed a virtual globe based three dimensional dynamic visualization method for gas diffusion the effectiveness of the proposed method was verified using an example of the long range natural gas diffusion and the following conclusions were drawn 1 the voxel based multi scale data model under a global framework enables multi scale data organization of the concentration field which then serves as the data source of the concentration calculation and visualization 2 the gpu based calculation method of a multi scale concentration field considering terrain enables fast calculation of the concentration field when the dimension of the sampling points is 316 379 42 the efficiency of this method is approximately 15 times that of cpu by adapting to the limitation of the texture space the higher the subdivision level the higher the computational efficiency of the gpu based method 3 the gpu based multi scale spherical ray casting algorithm achieves spherical volume rendering of the gas concentration field this method enhances the visualization efficiency through a multi scale strategy shrinking the spherical bounding box and improving the ray casting process the average fps increased by 2 3 times using the multi scale strategy by 2 times through shrinking the spherical bounding boxes and by 4 times due to improving the ray casting process 4 updating the temporal properties of voxels and repeating the calculation and visualization of the gas concentration field resulted in the dynamic visualization of an example of the long range gas diffusion above 30 fps the proposed method is suitable for the dynamic visualization of gas concentration fields on the premise that the number of sampling points adapts to the limitation of the texture space the subsequent improvement to the proposed method is to investigate how to achieve a dynamic visualization of the gas concentration field when the number of sampling points exceeds the limitation of the texture space further research is required to introduce the spatio temporal analysis algorithm based on multi scale organization of the gas concentration field under a global framework this will enable spatio temporal retrieval analysis and spatio temporal statistical analysis of the long range gas concentration fields under a global framework thereby enhancing decision support for disaster assessment and prevention acknowledgments this work was supported by the national key r d program of china grant number 2018yfb0505302 the nature science foundation innovation group project of hubei province china grant number 2016cfa003 comments from the anonymous reviewers and editor are appreciated appendix a supplementary data the following are the supplementary data to this article figs1 figs1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 09 019 
26279,three dimensional dynamic visualization of the long range gas diffusion contributes to disaster forecasting and assessment existing three dimensional visualization methods which calculate the gas concentration without considering the terrain and display the gas concentration in the local range by isosurfaces cannot meet the demand for intensive calculation and dynamic visualization of gas diffusion over a wide range this study proposes a virtual globe based three dimensional dynamic visualization method of the long range gas diffusion consisting of a voxel based multi scale data model a terrain dependent multi scale concentration field calculation method using graphics processing units gpu and a gpu based multi scale spherical ray casting algorithm the method is then applied to the long range natural gas diffusion and is evaluated according to the efficiency of the concentration calculation and dynamic visualization the results show that the proposed method can achieve dynamic visualization of the long range gas diffusion above 30 frames per second keywords virtual globes gas diffusion dynamic volume rendering gpu multi scales software availability name of software virtual globe based gas diffusion visualization contact address state key laboratory of information engineering in surveying mapping and remote sensing wuhan university wuhan 430079 china year first required 2018 hardware required tested on dell pc software required ms windows tested on windows 10 programming language ms visual studio 2010 c glsl version110 program size 97mb compressed availability and cost https github com chengruozhen gasdiffusionvisualization git maintenance contact via e mail 1 introduction leakage of gas transmission pipelines covering large areas and long distances can easily cause the long range gas diffusion this is affected by meteorological conditions gas properties and complex terrain hu et al 2015 visualization of dynamic processes of long range gas diffusions can intuitively demonstrate changes in concentration distribution during gas diffusions and aid in the assessment and prediction of environmental impacts cheng et al 2009 koussoulakou 1994 therefore it is necessary to research the dynamic visualization for long range gas diffusions which can support the auxiliary analysis of the environmental impact caused by gas diffusions the accuracy of the concentration distribution when dynamically visualizing gas diffusion depends on the accuracy of the gas concentration calculation contemporary soft computing technology solves uncertain problems in different fields through neural network chen and chau 2016 gholami et al 2015 taormina et al 2015 fuzzy logic sefeedpari et al 2016 intelligent computing fotovatikhah et al 2018 and other computing models zhang and chau 2009 and does not insist on high accuracy in order to exhibit a more accurate gas concentration distribution during dynamic visualization it is necessary to establish a mathematical model of the gas diffusion process to calculate the exact gas concentration the existing three dimensional visualization of gas diffusion models the gas diffusion process through the gaussian diffusion model and displays the gas concentration in the form of three dimensional isosurfaces jian and fan 2014 wang et al 2013 zahran et al 2013 the gaussian diffusion model is one of the most important methods that are widely used to estimate pollution levels this model has been applied extensively in the study of emissions from large industrial operations as well as a variety of other applications chen and qi 2010 ebrahimi and jahangirian 2013 liu et al 2012 and is suitable for concentration estimation under flat terrain conditions chang and weng 2013 the limitations of the existing three dimensional visualization of gas diffusion include three aspects first it is an idealized simulation within a local range the effect of terrain on the gas concentration is not considered when calculating the gas concentration second the concentration calculations within the local range are based on central processing units cpu which lead to low computational efficiency in addition they can only achieve static visualization because the extraction efficiency of the three dimensional isosurface is affected by the amount of data hou and chen 2016 newman and yi 2006 therefore the existing three dimensional visualization method for gas diffusion cannot meet the demand for intensive calculation and dynamic visualization of gas diffusion over a wide range three dimensional dynamic visualization of the long range gas diffusion must consider 1 a large range of terrain and 2 the high calculation and dynamic visualization requirements of a massive concentration dataset for the first requirement virtual globe software supports the access and visualization of a large range of three dimensional terrain goodchild 2008 providing efficient methods for presenting the long range geographical processes on the earth s surface gong et al 2010 yu and gong 2012 for the second requirement the programmable characteristics of graphics processing units gpu allow its application to general computing owens et al 2007 its simulation speed and performance is substantially higher than that of cpu based computing jr molnar et al 2010 yang and wu 2010 further gpu parallel computing power supports intensive calculations of gas concentration owens et al 2008 gpu based volume rendering techniques based on a virtual globe have been widely used in the visualization of three dimensional meteorological fields li et al 2011 liang et al 2014 liu et al 2015a 2015b and can effectively represent the internal structure of three dimensional or four dimensional phenomena li et al 2013 furthermore the parallelism of volume rendering techniques can realize the migration to gpu and support three dimensional or four dimensional visualizations of real time interactions du et al 2015 su et al 2016 these characteristics enable the three dimensional dynamic visualization of massive gas concentration datasets however the visualization of the above mentioned three dimensional meteorological field is based on a multi scale data model within the meteorological field which makes it difficult to match with the global terrain on the virtual globe in addition in the volume rendering of a three dimensional meteorological field a large number of sampling points outside the field are introduced and frequent coordinate transformation calculations are executed thus this study integrates the gas diffusion model and the data organization characteristics of the virtual globe and proposes a virtual globe based three dimensional dynamic visualization method for gas diffusion this method is designed to meet the calculation demands of massive concentration datasets consider the effect of terrain and perform dynamic visualization of the long range gas diffusion in the method presented in this study 1 a voxel based multi scale data model under a global framework is proposed to organize gas concentration fields this overcomes the fact that the multi scale data model in a certain range in previous studies is difficult to match with the global terrain on the virtual globe and is difficult to express the time characteristics of the three dimensional field 2 a gpu based calculation method for multi scale gas concentration fields which considers terrain is proposed for the rapid calculation of gas concentration this solves the problem of low efficiency and lack of consideration of terrain when calculating the gas concentration field based on the cpu and the calculation results can be directly applied to the subsequent visualization 3 a gpu based multi scale spherical ray casting algorithm is proposed for spherical visualization of the concentration field this algorithm avoids the introduction of sampling points outside the field and improves the ray casting process and 4 the temporal properties of voxels are updated in the concentration field and the concentration calculation and visualization of the concentration field are repeated thus achieving dynamic visualization of the long range gas diffusion the structure of this paper is as follows section 2 describes the virtual globe based three dimensional dynamic visualization method of gas diffusion section 3 uses an example of the long range natural gas diffusion to conduct experiments and then evaluate and analyze the proposed method section 4 presents the conclusions and proposes further research directions 2 methodology our virtual globe based three dimensional dynamic visualization method of gas diffusion includes three parts fig 1 1 to organize the gas concentration field the three dimensional global space is divided into multi scale blocks based on these the voxel based multi scale data model is proposed then multi scale voxels of the concentration field are constructed based on the data model of the concentration field 2 to calculate the gas concentration field which is based on the multi scale voxels of the concentration field gas concentration calculation is implemented through texture mapping according to the gpu based calculation method of the multi scale concentration field considering the terrain and the concentration field is finally stored as a three dimensional texture 3 to render the gas concentration field which is based on the three dimensional texture of concentration the gpu based multi scale spherical ray casting algorithm is used for spherical visualization of the concentration field the time controller updates the temporal properties of voxels in the concentration field at a certain time interval and repeats the concentration calculation and three dimensional visualization of the concentration field to achieve three dimensional dynamic visualization of the gas diffusion 2 1 voxel based data organization spatially continuous concentration fields have multi dimensional spatio temporal features however due to the limitations of texture space memory space and computational performance the concentration fields need to be divided into multi scale data units to visualize the different scales of the concentration field as the viewpoint changes thereby improving the dynamic visualization efficiency therefore we divide the three dimensional global space into multi scale blocks and establish the global framework based on an octree fig 2 a the voxel based multi scale data model fig 2 b is proposed under a global framework and the spatio temporal features of the concentration field are described using voxels a voxel consists of three parts spatial extent sampling points and time the spatial extent describes the spatial scale of the concentration field including the corresponding block and spherical grid the sampling points store the geographical coordinates and the corresponding terrain the time describes the temporal characteristics of the concentration field based on the data model of the concentration field sampling points in the block are used to construct multi scale voxels fig 2 c by dividing the concentration field into multi scale voxels the multi scale concentration field is constructed fig 2 d the blocks consisting of red green and yellow lines in fig 2 d represent voxels of adjacent scales 2 1 1 octree based global framework the octree based global framework can cover the global spatial extent and achieve multi scale expression and fast retrieval of any spatial range the spatial extent under the global framework consists of the entire spherical space and an altitude from 0 to 100 km starting from the point at a longitude of 180 west a latitude of 90 south and an altitude of 0 m the three dimensional global space is divided into two subspaces according to longitude and latitude spans of 90 the two subspaces form level 0 of the octree structure the interior of each subspace is evenly divided in latitude longitude and altitude directions and the segments are encoded from west to east from south to north and from bottom to top thus the three dimensional global space is divided into octree based blocks k layer column row is the identifier of the block which then serves as the identifier of the voxel where k is the subdivision level and layer column row is the coding of altitude longitude and latitude respectively k column row is the identifier of the spherical grid corresponding to the block blocks can be indexed by the identifier table 1 where floor indicates rounding down 2 1 2 multi scale concentration field construction assuming that the concentration field ranges from lon1 lat1 alt1 to lon2 lat2 alt2 where lon1 lat1 alt1 and lon2 lat2 alt2 are the geographical coordinates of the points respectively that is the spatial extent of the concentration field is determined by the longitude ranging from lon1 to lon2 the latitude ranging from lat1 to lat2 and the altitude ranging from alt1 to alt2 the terrain in the concentration field is represented by terrain tiles terrain tiles are organized in a global spherical quadtree structure when constructing a global spherical quadtree starting from the point at a longitude of 180 west and a latitude of 90 south the two dimensional spherical space is divided into two subspaces according to longitude and latitude spans of 90 the two subspaces form level 0 of the quadtree structure the interior of each subspace is evenly divided in latitude and longitude directions and the segments are encoded from west to east and from south to north thus the two dimensional spherical space is divided into quadtree based tiles k column row is the identifier of the terrain tile where k is the subdivision level and column row is the coding of longitude and latitude respectively the size of all terrain tiles is the same and the spatial resolution of the terrain tile at level k is 4 times the spatial resolution of the terrain tile at level k 1 according to the voxel based multi scale data model under the global framework the algorithm for constructing the multi scale concentration field is as follows 1 c o l u m n i f l o o r 2 k  180 180 r o w i f l o o r 2 k 1  90 90 l a y e r i f l o o r 2 k h 100000 2 w e s t c o l u m n a 180 2 k 180 e a s t c o l u m n b 180 2 k 180 180 2 k s o u t h r o w a 90 2 k 1 90 n o r t h r o w b 90 2 k 1 90 90 2 k 1 a l t i t u d e 1 l a y e r a 100000 2 k a l t i t u d e 2 l a y e r b 100000 2 k 100000 2 k input point a lon1 lat1 alt1 and point b lon2 lat2 alt2 multi scale terrain data step 1 for each subdivision level k perform step 2 to step 6 step 2 calculate the identifier of voxels where point a and point b are located using eq 1 and mark them as k layer a column a row a and k layer b column b row b respectively voxels of the concentration field are marked with k layer i column i row i and the corresponding spherical grids are marked with k column i row i thus the concentration field formed by voxels ranges from west south altitude1 to east north altitude2 using eq 2 where l a y e r a l a y e r i l a y e r b c o l u m n a c o l u m n i c o l u m n b r o w a r o w i r o w b   h in eq 1 is the geographical coordinate step 3 for each voxel k layer i column i row i perform step 4 to step 6 step 4 sample in the direction of longitude latitude and altitude in accordance with the number of 2 m 2 n and 2 l to access the geographical coordinates where m 0 n 0 l 0 step 5 match multi scale terrain data by k column i row i and obtain the terrain of the sampling points in step 4 step 6 the initial time of the gas diffusion is stored as the temporal attribute of the voxel output multi scale voxels of the concentration field point a1 west south altitude1 and point b1 east north altitude2 2 1 3 dynamic update of concentration field in order to achieve dynamic visualization of gas diffusion the time controller needs to update the temporal properties of the voxels in the concentration field at certain time intervals and then calculates and visualizes the concentration field at the next diffusion moment since calculating and visualizing the gas concentration field do not involve all voxels directly updating the time attributes of all voxels will lead to unnecessary efficiency costs when updating the time attributes of voxels the update algorithm of voxels is used to reduce the number of updated voxels and to improve the dynamic visualization efficiency of gas diffusion the update algorithm of voxels is as follows input voxels at time t1 step 1 determine if the spatial scale of the concentration field changes at time t2 if not perform step 2 otherwise perform step 3 step 2 update the time attribute of current voxels to time t2 step 3 if the upper level voxels are scheduled the time attributes of the parents of current voxels are updated as t2 if the next level voxels are scheduled the time attributes of the eight children of current voxels are respectively updated as t2 output voxels at time t2 2 2 terrain dependent concentration calculation using gpu the calculation of the long range concentration fields is intensive and independent and both terrain and the accumulation of concentration over time must be considered in the concentration calculation however the existing calculation of the concentration field is implemented on cpu chen and qi 2010 jian and fan 2014 liu et al 2012 the floating point operation of a large number of sampling points imposes a great burden on cpu fan et al 2009 corrected gas concentrations on the ground alone without considering the influence of terrain on gas concentration in the three dimensional space and the accumulation of gas concentrations over time zhang et al 2010 considered the accumulation of concentrations over time but did not consider the effect of terrain therefore we propose the gpu based method for calculating the multi scale concentration field whilst considering the terrain when the concentration calculation is performed on the gpu the gaussian diffusion model is used as the basic model and corrections are taken for considering the effects of time and terrain see eq 3 the terrain factor t see eq 4 is introduced during the accumulation of concentration over time and the influence of the terrain factor is extended to the entire three dimensional space x y z in eq 3 represent the position of the sampling point relative to the leakage source in the cartesian coordinate system while the position of the voxel of the concentration field on the virtual globe uses the geographic coordinates so the relative positions of sampling points in the geographic coordinate system need to be transformed to the cartesian coordinate system using eq 5 the parameters in eqs 3 5 are shown in table 2 3 c x y z t 0 t q d t 2  3 2  x  y  z e x p x u t r t 2 2  x 2 e x p y 2 2  y 2 e x p z t h 2 2  z 2 e x p z t h 2 2  z 2 4 t 0 h h 1 h h h h 5 x l o n g i t u d e l o n g i t u d e s o u r c e  r a d i u s 180 y l a t t i t u d e l a t i t u d e s o u r c e  r a d i u s 180 z a l t i t u d e a l t i t u d e s o u r c e when calculating the multi scale concentration field voxels at subdivision level k and gaussian diffusion model parameters are input to gpu and the concentration calculation is implemented in the shader through texture mapping the result of the concentration calculation is stored as the three dimensional texture where each texel corresponds to a single sampling point in the concentration field and the color of the texel represents the gas concentration at that sampling point because gpu is designed to render into the two dimensional frame buffer the concentration calculations in the shader need to be performed separately for each slice of the three dimensional texture according to section 2 1 2 the algorithm for calculating the concentration field at subdivision level k is as follows 6 l 2 l 1 l a y e r b l a y e r a 1 1 m 2 m 1 c o l u m n b c o l u m n a 1 1 n 2 n 1 r o w b r o w a 1 1 7 l a y e r f l o o r e 2 l 1 l a y e r a c o l u m n f l o o r i 2 m 1 c o l u m n a r o w f l o o r j 2 n 1 r o w a 8 c o l u m n v i 2 m 1 f l o o r i 2 m 1 r o w v j 2 n 1 f l o o r j 2 n 1 l a y e r v e 2 l 1 f l o o r e 2 l 1 9 l o n g i t u d e w e s t i m 1 e a s t w e s t l a t i t u d e s o u t h j n 1 n o r t h s o u t h a l t i t u d e a l t i t u d e 1 e l 1 a l t i t u d e 2 a l t i t u d e 1 10 r 1 c c high c c h i g h c l o w c c h i g h 0 c c l o w g 0 c c h i g h o r c c l o w 1 c c h i g h c l o w c c h i g h b 0 c c l o w c c c l o w a 0 input voxels at subdivision level k gaussian diffusion model parameters point a1 west south altitude1 and point b1 east north altitude2 step 1 input point a1 point b1 time properties of voxels and gaussian diffusion model parameters into gpu step 2 for the eth slice of the three dimensional texture perform step 3 to step 8 respectively where 0 e l 1 step 3 draw m n rectangles of size 1 1 to form a rectangle of size m n the vertex coordinate is i j where 0 i m 1 0 j n 1 and l m n are calculated using eq 6 step 4 determine the voxel k layer column row where the vertex i j is located and the corresponding sampling point column v row v layer v using eqs 7 and 8 thereby acquiring terrain and then input it into gpu as a vertex attribute where column v row v and layer v are the column row and layer respectively within the voxel where the sampling point is located step 5 through the texture mapping of a rectangle of size m n perform step 6 to step 8 in the shader step 6 map the vertex coordinate i j to the geographical coordinate longitude latitude altitude using eq 9 step 7 calculate the gas concentration at longitude latitude altitude using eqs 3 5 step 8 map the concentration to rgba space assuming that the visualized concentration ranges from c low to c high the range of concentration affects the mapping of concentrations to colors the concentration is mapped to color using eq 10 output the three dimensional texture of the concentration field of size m n l during visualization the concentration field of the corresponding spatial scale is calculated according to the current subdivision level and the three dimensional texture of the concentration field is obtained which is used directly in the subsequent rendering process 2 3 spherical volume rendering using gpu in existing methods for visualizing the concentration field using three dimensional isosurfaces the amount of data directly affects the efficiency of isosurface extraction hou and chen 2016 and thus cannot meet the requirements of dynamic visualization the ray casting algorithm in volume rendering assumes that rays shot from the viewpoint pass through the volume bounding box and samples on the ray within the bounding box to accumulate the color of the final screen pixel because the computation of each ray emitted by the viewpoint is relatively independent the algorithm can be easily migrated to gpu to support the dynamic visualization of real time interaction the bounding box of the gas concentration field on the virtual globe is a spherical bounding box therefore the ray casting algorithm on the virtual globe must consider the adaptation to the spherical bounding box in order to not introduce sampling points beyond the spherical bounding box liang et al 2014 directly sampled the ray within the spherical bounding box however each sampling point was transformed from the cartesian coordinate system to the geographic coordinate system and then to the texture coordinate system resulting in the frequent calculation of coordinate transformation during ray casting moreover the algorithm used a single scale volume bounding box and does not consider multi scale visualization of volume data as a result we propose a gpu based multi scale spherical ray casting algorithm the algorithm constructs the multi scale spherical bounding box based on voxels which consists of two spherical shells and four vertical walls fig 3 a through the texture mapping of the spherical bounding box the spherical volume rendering is implemented based on the three dimensional texture in section 2 2 during spherical volume rendering the spherical bounding box is shrunk and the ray casting process is improved to enhance visualization efficiency the spherical bounding box is reduced by ignoring the space below the terrain to reduce the number of sampling points on the ray in the improved ray casting process geographical coordinates of the entry point and exit point on the ray are calculated fig 3 b and transformed to the texture coordinate system and the other sampling points are obtained directly in the texture coordinate space fig 3 c thus reducing the frequent coordinate transformation calculations during ray casting according to sections 2 1 2 and 2 2 the spherical ray casting algorithm at subdivision level k is as follows 11 x r a d i u s a l t i t u d e cos l a t i t u d e cos l o n g i t u d e y r a d i u s a l t i t u d e cos l a t i t u d e sin l o n g i t u d e z r a d i u s a l t i t u d e sin l a t i t u d e 12 l o n g i t u d e a r c t a n y x l a t i t u d e a r c s i n z x 2 y 2 z 2 a l t i t u d e x 2 y 2 z 2 r a d i u s 13 e n t r y e y e e x i t e y e a 1 e y e e x i t e y e e y e a 1 e y e e x i t e y e b 1 e y e e x i t e y e e y e b 1 e y e a 1 e y e b 14 t e x c o o r d e n t r y e n t r y t e x g e n t e x c o o r d e x i t e x i t t e x g e n t e x g e n 1 e a s t w e s t 0 0 w e s t e a s t w e s t 0 1 n o r t h s o u t h 0 s o u t h n o r t h s o u t h 0 0 1 a l t i t u d e 2 a l t i t u d e 1 a l t u t d e 1 a l t i t u d e 2 a l t i t u d e 1 0 0 0 1 15 d e l t a c o o r d t e x c o o r d e n t r y t e x c o o r d e x i t i n t e r v a l l e n g t h 16 f r a g c o l o r r g b f r a g c o l o r r g b c r g b c a 1 c a f r a g c o l o r a f r a g c o l o r a c a input voxels at subdivision level k point a1 west south altitude1 and point b1 east north altitude2 three dimensional texture of the concentration field step 1 construct the spherical bounding box both spherical shells at altitude1 and altitude2 are formed of m 1 n 1 rectangles the two vertical walls in the west and east directions are formed by n 1 rectangles and the two vertical walls in the south and north directions are formed by m 1 rectangles the vertices in the vertical walls are the vertices of the edge of the spherical shells step 2 determine the voxel k layer column row where the vertex is located and the corresponding sampling point column v row v layer v using eqs 7 and 8 thereby acquiring the geographical coordinate and terrain where e is l and 1 at altitude1 and altitude2 in eqs 7 and 8 respectively step 3 compare the terrain of all vertices of the spherical shell at altitude1 to obtain the minimum terrain min thus the reduced spherical bounding box ranges from point a1 west south terrain min to point b1 east north altitude2 step 4 transform the geographical coordinates longitude latitude altitude of the vertices of the smaller spherical bounding box to cartesian coordinates x y z using eq 11 and draw the box where radius is the earth s radius using texture mapping of the reduced spherical bounding box perform step 5 to step 7 in the shader step 5 construct the ray from the viewpoint to the vertex transform the vertex and viewpoint from the cartesian coordinate system to the geographic coordinate system using eq 12 the vertex serves as the exit point of the ray exit and the entry point of the ray entry is calculated using eq 13 where eye is the geographic coordinates of the viewpoint step 6 calculate the texture coordinates of the entry point texcoord entry and the exit point texcoord exit using eq 14 and obtain the length of the ray length in texture coordinate space along the direction from the exit point to the entry point sampling on the ray according to the spacing interval the texture coordinate increment of the adjacent sampling points deltacoord is calculated using eq 15 step 7 sample the three dimensional texture of the concentration field according to the texture coordinates of sampling points and accumulate the color of the screen pixel using eq 16 where fragcolor is the color of the screen pixel and c is the color of the three dimensional texture output the concentration field at subdivision level k during visualization the spherical bounding box of the corresponding spatial scale is constructed according to the current subdivision level based on the three dimensional texture of the concentration field in section 2 2 spherical volume rendering of the concentration field is achieved 3 implementation and analysis here we validate the proposed method by simulating an example of the long range natural gas diffusion the simulated data is shown in table 3 the simulated data includes the spatial extent of the natural gas concentration field and the parameters of the leakage source the spatial extent of the natural gas concentration field is determined by the longitude ranging from 105 5 e to 105 9 e the latitude ranging from 31 0 n to 31 4 n and the altitude ranging from 0 m to 2000 m that is the natural gas concentration field ranges from point a 105 5 e 31 0 n 0 to point b 105 9 e 31 4 n 2000 the parameters of the leakage source include the location the leakage rate the average wind speed and the total leakage time according to section 2 1 2 the multi scale natural gas concentration field is constructed the number of sampling points in the longitude latitude and altitude direction of a single voxel in the concentration field is 2 m 2 n and 2 l respectively in order to ensure a uniform distribution of sampling points the ratio of the number of sampling points in each direction is 2 m 2 l 2 n 2 l 105 9 105 5  r a d i u s 180 2000 22 24 taking into account the sampling density of the concentration field and constraints of texture space the dimension of the sampling points within the voxel is 64 64 2 using subdivision level 8 to level 11 under the global framework as an example to show the dynamic visualization of natural gas diffusion the dimension of sampling points in the concentration field at corresponding subdivision levels are 64 64 7 127 127 12 190 190 22 and 316 379 42 respectively the multi scale terrain data comes from terrain tiles at level 8 to level 11 in google earth and the terrain tiles at level 11 have a spatial resolution of 152 87 m given the fact that c performs more efficiently than other programming languages and glsl is a shader language based on c c and opengl that enables the gpu s programmable features the experiments were implemented on osgearth using c and glsl and run on a windows desktop pc with intel r xeon r e3 1240 v6 3 70 ghz cpus nvdia quadro k620 graphics with 16 gb of ram and 2 gb of video memory the dynamic process of natural gas diffusion on the virtual globe is shown for a time step of 5 min fig 4 fig 4 a to l are snapshots at diffusion times of 5 min 10 min 15 min 20 min 25 min 30 min 35 min 40 min 45 min 50 min 55 min and 60 min respectively at subdivision level 11 the color changing from blue to red indicates the change in concentration from 10mg m3 to 50 mg m3 the dynamic visualization of the natural gas diffusion shows that the concentration distribution presents a ring like structure and expands outwards over time the diffusion areas of natural gas in the concentration range of 10 mg m3 to 50 mg m3 in fig 4 a to l are recorded in table 4 which indicates that the diffusion range increases with the diffusion time with the multi scale strategy different scales of the concentration field are rendered as the viewpoint changes fig 5 a d are snapshots of the concentration field at subdivision levels 8 to 11 for a diffusion time of 30 min it shows that when the viewpoint is closer to the concentration field a finer scale concentration field is rendered which is reflected in the clearer edges of areas in different color when the viewpoint is far from the concentration field the coarser scale concentration field is rendered which is reflected in the gradual blurring of the edges of areas in different color fig 5 d f show the concentration field at subdivision level 11 for a diffusion time of 30 min from different viewpoints the diffusion areas of natural gas in the concentration range of 10 mg m3 to 50 mg m3 in fig 5 are all 278 42 km2 a video component is provided which demonstrates an example of the long range natural gas diffusion simulated in table 3 to access this video component simply click on the image below online version only supplementary video related to this article can be found at https doi org 10 1016 j envsoft 2018 09 019 the following is are the supplementary data related to this article video 1 dynamic visualization of the long range natural gas diffusion video 1 when the gas concentration field is dynamically visualized by the proposed method the calculation and the dynamic visualization of the gas concentration field are successively performed therefore the evaluation of the proposed method includes two aspects efficiency of the concentration field calculation and efficiency of the dynamic visualization the calculation of the concentration field includes the calculation of the concentration of all sampling points in the concentration field so the calculation time of the concentration field is the most direct metric for evaluating the calculation efficiency of the concentration field by comparing the calculation time of the cpu based method and the gpu based method table 5 the efficiency of the proposed method can be qualitatively and quantitatively described when using gpu the rendering environment needs to be initialized thus when the number of sampling points is small such as in subdivision level 8 cpu based computation is more efficient than gpu as the number of sampling points increases the parallelism of gpu based computing increases computational efficiency at subdivision level 11 gpu based computational efficiency is approximately 15 times that of cpu for visualization systems the evaluation of rendering efficiency is necessary the frames per second fps is a direct reflection of the rendering efficiency of the visualization framework so we use the fps which is common in the visualization region to quantitatively evaluate the dynamic visualization efficiency of the proposed method since the proposed method uses multiple strategies to optimize the dynamic visualization efficiency it is necessary to evaluate the impact of various optimization strategies on the efficiency of dynamic visualization evaluations includes a comparison of fps with and without multi scale strategy a comparison of fps before and after shrinking the spherical bounding box and a comparison of fps between liang s method 2014 and the proposed method in order to ensure the reliability of the evaluation the fps of the dynamic visualization is obtained based on the same roaming path and the roaming duration is 10 s fig 6 compares the fps with and without the multi scale strategy in dynamic visualization when roaming the scene as viewpoint changes an increase or decrease in the rendered area of the screen causes the fps to decrease or increase with the multi scale strategy a coarser scale concentration field is rendered when the viewpoint is far from the concentration field meaning that the number of sampling points in the concentration field decreases by 23 times leading to a sharp increase in fps the average fps with and without the multi scale strategy in dynamic visualization are 41 45 and 18 01 respectively the multi scale strategy increases the average fps of dynamic visualization by approximately 2 3 times which proves its effectiveness table 6 shows the reduced proportions of the spherical bounding box at different subdivision levels the average shrinking rate of the spherical bounding box in dynamic visualization is 15 36 the average fps before and after shrinking the spherical bounding box is 20 36 and 41 45 respectively fig 7 by shrinking the spherical bounding box the number of sampling points on the ray within the spherical bounding box is reduced resulting in approximately double the average fps both the proposed method and liang s method 2014 perform volume rendering on the virtual globe based on spherical bounding boxes and do not bring a large number of sampling points outside the field therefore by comparing the visualization efficiency of the proposed method and liang s method 2014 it can directly reflect the advantages of the proposed method in visualization efficiency when performing volume rendering based on spherical bounding boxes fig 8 compares the fps between the proposed method and liang s method in dynamic visualization because the proposed method is based on the multi scale spherical bounding box and an improved ray casting process the fps in dynamic visualization is maintained above 30 however liang s method is based on a single scale spherical bounding box and the point by point calculation of coordinate transformation during ray casting reduces the computational efficiency so the fps in dynamic visualization is always under 15 the average fps values of the proposed method and liang s method are 41 45 and 10 17 respectively thus the proposed method increases the average fps in dynamic visualization by approximately four times the proposed method and liang s method 2014 are also compared in terms of visualization quality applicability and cases in terms of visualization quality since both the proposed method and liang s method 2014 achieve the rendering of a three dimensional field by sampling on the ray within the spherical bounding box and then accumulating the colors of the screen pixels in the case of the same number of sampling points in the concentration field the visualization quality of the two methods is the same in addition the proposed method considers the visualization of multi scale volume data while the liang method 2014 only considers the visualization of single scale volume data therefore when visualizing the multi scale concentration field the proposed method sacrifices the visualization quality of the coarser scale concentration field in order to optimize the dynamic visualization efficiency in terms of applicability liang s method 2014 only considers single scale volume data so it can be applied to the visualization of three dimensional fields on a single scale the proposed method considers multi scale volume data and is suitable for the dynamic visualization of multi scale three dimensional fields in terms of cases liang s method 2014 can be applied to the visualization of datasets of three dimensional fields the proposed method includes the calculation and the dynamic visualization of the gas concentration field so it can be applied to the rapid estimation and dynamic visualization of the long range gas diffusion the voxel based multi scale data model and the gpu based multi scale spherical ray casting algorithm in this study can also be applied to multi scale dynamic visualization of various three dimensional fields 4 conclusions three dimensional dynamic visualization over the long range gas diffusion involves the calculation and dynamic visualization of massive gas concentration datasets while taking into account the effects of a large range of terrain with its massive data organization and multi scale display capabilities the virtual globe is regarded as an ideal platform for the three dimensional visualization of the long range gas diffusion however existing three dimensional visualization of gas diffusion involves local scale static and does not consider the influence of terrain on gas concentration moreover gas concentration calculations using cpu and the extraction of three dimensional isosurfaces make it difficult to achieve dynamic visualization of gas diffusion therefore we proposed a virtual globe based three dimensional dynamic visualization method for gas diffusion the effectiveness of the proposed method was verified using an example of the long range natural gas diffusion and the following conclusions were drawn 1 the voxel based multi scale data model under a global framework enables multi scale data organization of the concentration field which then serves as the data source of the concentration calculation and visualization 2 the gpu based calculation method of a multi scale concentration field considering terrain enables fast calculation of the concentration field when the dimension of the sampling points is 316 379 42 the efficiency of this method is approximately 15 times that of cpu by adapting to the limitation of the texture space the higher the subdivision level the higher the computational efficiency of the gpu based method 3 the gpu based multi scale spherical ray casting algorithm achieves spherical volume rendering of the gas concentration field this method enhances the visualization efficiency through a multi scale strategy shrinking the spherical bounding box and improving the ray casting process the average fps increased by 2 3 times using the multi scale strategy by 2 times through shrinking the spherical bounding boxes and by 4 times due to improving the ray casting process 4 updating the temporal properties of voxels and repeating the calculation and visualization of the gas concentration field resulted in the dynamic visualization of an example of the long range gas diffusion above 30 fps the proposed method is suitable for the dynamic visualization of gas concentration fields on the premise that the number of sampling points adapts to the limitation of the texture space the subsequent improvement to the proposed method is to investigate how to achieve a dynamic visualization of the gas concentration field when the number of sampling points exceeds the limitation of the texture space further research is required to introduce the spatio temporal analysis algorithm based on multi scale organization of the gas concentration field under a global framework this will enable spatio temporal retrieval analysis and spatio temporal statistical analysis of the long range gas concentration fields under a global framework thereby enhancing decision support for disaster assessment and prevention acknowledgments this work was supported by the national key r d program of china grant number 2018yfb0505302 the nature science foundation innovation group project of hubei province china grant number 2016cfa003 comments from the anonymous reviewers and editor are appreciated appendix a supplementary data the following are the supplementary data to this article figs1 figs1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 09 019 
