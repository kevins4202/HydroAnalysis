index,text
23790,we present a deep learning method to downscale low resolution geophysical fields by merging them with high resolution data the downscaling was performed using an ensemble of convolutional neural networks cnns whose prediction values are the average values of the outputs of 20 cnns academic experiments were conducted on simulated ocean data in the gulf stream region given by the outputs of the natl60 model the cnns forced with low resolution 120 120 km sea surface high ssh data and mesoscale resolution 12 12 km sea surface temperature sst data allowed us to obtain mesoscale resolution sea surface currents with good accuracy sensitivity experiments have shown that taking sst into account significantly increases the accuracy of the high resolution velocity retrieval even when noise is added to the ssh data the velocity information embedded in the transport equation modeling the sst advection is taken into account by the cnn which greatly increases the resolution of ocean currents provided by ssh in the present work we only consider spatial downscaling by assuming that ssh and sst are daily observations the method we developed is generic and can be used to improve the resolution of a wide variety of large scale fields by merging them with high resolution fields keywords downscaling machine learning altimeter sst ocean currents data and code availability the datasets we used natl 12 and natl 08 which were extracted from the natl60 model 630 experiments output over the gulf stream area in the north atlantic ocean are already available in the ipsl thredds catalog at the address https doi org 10 14768 c3c33afe 2a37 42e1 bb29 21428387f658 the python codes of the resac models are hosted at gitlab https gitlab in2p3 fr carlos mejia resacpubli ocemod releases v1 0r 1 introduction for several decades a wide variety of satellite sensors has allowed us to considerably improve our knowledge of the state of the planet earth and its potential evolution these sensors observe a multitude of geophysical parameters with various sampling strategies both in space and in time the objective of this study is to show how the combination of several parameters can reinforce each other to obtain geophysical fields observed with better resolution and accuracy this problem is encountered in many aspects of geophysical sciences such as atmospheric sciences oceanography hydrology fluid mechanics geology astrophysics we have therefore decided to conduct an academic study that should benefit a wide range of observations of geophysical parameters in this context we have chosen to focus our efforts on the algorithmic aspect of downscaling these observations by combining them together in the present paper we focused our research on the observation of ocean currents by satellite the ocean has been observed for several decades by numerous satellites equipped with various sensors that have provided global coverage of dynamic parameters such as ocean circulation with sea surface topography hereafter ssh from the topex poseidon and then jason altimeters sea surface temperature hereafter sst with the high resolution avhrr radiometers launched aboard meteorological satellites ocean biological productivity with multispectral ocean color sensors seawifs meris modis viirs and ocean salinity smos and aquarius these satellite sensors have contributed to a better understanding of the ocean and the detection of its changes in response to global warming cazenave et al 2018 ocean currents play an important role in the latitudinal transport of heat and in advecting and structuring ocean properties such as salinity nutrients phytoplankton across a wide range of scales from basin to sub mesoscale sasaki et al 2014 mcwilliams 2016 levy et al 2012 the estimation of ocean currents at high resolution from satellite observations a few kilometers has been addressed by a large number of scientific studies that have combined ssh and sst observations rio et al 2016 rio and santoleri 2018 nardelli et al 2022 or that processed sst image sequences isern fontanet et al 2006 bowen et al 2002 vigan et al 2000 kelly et al 1999 emery et al 1986 other studies have used intrinsic physical properties of ocean surface layers such as conservation of potential vorticity ubelmann et al 2015 recently george et al 2021 and manucharyan et al 2021 have shown that ssh interpolation can be improved by theoretical considerations based on qg turbulence and have given leads based on rational physical arguments to improve the reconstruction of mesoscale ssh from sst data we propose here a deep learning method dl to obtain high resolution ocean currents by combining low resolution ssh and high resolution sst the problem is very complicated as ssh are measured along satellite tracks which are separated of about one hundred kilometers with a repeat sampling period of about 10 days ballarotta et al 2019 while sst are measured as daily two dimension images at a resolution of 4 km the association of these two observations of different resolution implies the solving of a difficult inverse problem with time and spatial constraints as accounted in 4d var data assimilation in the present paper we oversimplified the observing system by focusing our study on spatial downscaling alone assuming that ssh and sst measurements are gridded daily observations of different resolution due to the lack of high resolution ocean parameter observations and to simplify the data handling at various resolutions we used simulated data fields ssh and sst are the outputs of a very high resolution ocean numerical model ssh and sst are therefore linked to each other by well known physical laws that are taken into account by the ocean model and that will be learned by our method we chose to conduct our study in the gulf stream region 35 n 55 w where ocean currents and associated mesoscale dynamical features are important we used the outputs given by the high resolution ocean model natl60 with a horizontal resolution of 1 60 in latitude these simulated observations provide ssh sst and currents at very high resolution down to 2 km which is much better than operational products derived from observations such as those provided by aviso https www aviso altimetry fr en data html due to the high resolution data fields and the large area under study we decided to use deep learning algorithms which are methods suitable for solving problems involving very large datasets recent developments in artificial intelligence combined with the availability of large datasets such as those provided by high resolution climate simulations and increased computational power have led to the emergence of information processing capabilities to learn the hidden structures of geophysical phenomena over the past decade statistical and machine learning methods have greatly facilitated the study of these multidimensional datasets as evidenced by the growing number of publications since 2018 convolutional neural networks cnns which are extensions of multilayer perceptrons le cun et al 1990 2015 simard et al 2003 goodfellow et al 2016 have proved their ability to exploit spatial and temporal data structures find patterns and efficiently fuse heterogeneous information sources reichstein et al 2019 rolnick et al 2019 in addition image analysis of complex geophysical fields using neural networks has been a very productive line of research over the past decade and has led to relevant environmental applications egmont petersen et al 2002 he et al 2016 wang et al 2019 martinez et al 2020 specific neural architectures have recently been developed which allow us to obtain numerical schemes that accurately solve partial differential equations ruthotto and haber 2019 rousseau et al 2019 these neural architectures can be used effectively to represent dynamic phenomena that are usually represented by numerical models the challenge is therefore to introduce prior knowledge provided by numerical and or statistical analyses into these architectures to improve their performance when solving specific environmental problems ducournau and fablet 2018 fablet et al 2018 de bézenac et al 2019 new tools are made available to the scientific community to help them implement adequate architectures to carry out their research as an example recent papers propose automatic differentiation tools embedded in deep learning frameworks to introduce end to end neural network architectures for data assimilation pannekoucke and fablet 2020 in addition several studies using machine learning ml hereafter methods have been devoted to modeling ocean surface variables with success showing the potential power of these methods examples include sea surface temperature reconstruction from satellite observations using a convolutional neural network barth et al 2020 interpolation of sea level anomalies using data driven methods lguensat et al 2019 and cnn manucharyan et al 2021 george et al 2021 ocean ml simulations for climate modeling sonnewald and lguensat 2021 this paper aims at showing the ability of ml to downscale ssh and associated currents by studying an academic case analysis of the geophysical properties of the problem ml learning and estimation of the physical coherence of the reconstructed fields are performed using a high resolution numerical model output the paper is organized as follows in section 1 we describe the problem in section 2 we present the data the ml algorithm is described in section 3 in section 4 we present the performance of our method and analyze the results in section 5 we discuss the results and conclude in section 6 2 data the objective of the present study is to develop a generic machine learning prototype to perform geophysical field downscaling to demonstrate the feasibility of such a study we used academic datasets with physical characteristics similar to those provided by ocean satellite sensors the simulated ssh and sst data used to construct the deep learning algorithms allow us to obtain well documented ocean datasets with no missing data and control for the different resolutions we wish to explore we used the sst ssh and horizontal velocity field u v provided by the natl60 model which is a very high resolution model of the north atlantic ocean extending from 26 n to 66 n fig 1 the natl60 ocean model is based on the nemo 3 6 code in the version we used the tide was not included the horizontal grid is extracted from the global tri polar grid of the orca series madec and imbard 1996 with a resolution of 1 60 at the equator the horizontal grid spacing ranges from 1 6 km at 26 n to 0 9 km at 65 n the grid has been designed so that the model explicitly simulates the scales of motions that will be observed by the swot altimetric mission in practice the model s effective resolution is about 10 15 km in wavelength the resolution increases towards the poles and is about 1 1 km at the latitude of the gulf stream the vertical grid uses 300 levels with a thickness of 1 m near the surface up to 50 m in the deepest part to well reproduce the vertical velocities which condition the vertical exchanges and the interactions with the bottom topography we used the atmospheric forcing dfs5 2 drakkar forcing set 5 2 in dussin et al 2016 initial conditions were taken from the 1 12 mercator ocean reanalysis the output of natl60 was analyzed in ajayi et al 2020 in the present research we studied the gulf stream region 26 n 45 n 40 w 65 w fig 1 which is the most energetic region of the north atlantic ocean this frontier simulation required 16mcpuh on 14 000 cores and was performed during the genci 2016 grand challenge because we focus our study on surface currents recovered from ssh we bypassed the ekman surface drift by considering the horizontal current at a depth of 15 m we processed 366 daily images of 4 variables ssh sst u v covering the period from october 1 2012 to october 1 2013 for training dataset natl 12 below and four months with atmospheric forcings from the year 2008 one for validation december natl 08v below and three for testing march june september designated natl 08t below every image is composed of 1296 1377 pixels for each variable a pixel being a square box of approximately 1 5 1 5 km we denote this dataset r01 in fig 2 we show the mean local kinetic energy and the mean local enstrophy see appendix a for their definition associated with every pixel over the 366 images of the period studied we notice that the area can be split into two areas the one north of 35 n which includes the gulf stream is very energetic 86 of the total energy the second one south of 35 n is less energetic 15 of the total kinetic energy and corresponds to a recirculation area populated with small eddies characteristic statistical values of the two areas are shown in table 1 we note that the average current in the northern region has a fairly large eastern component 3 8 cms 1 corresponding to the gulf stream while the other current components are far less important enstrophy also is much more intense in the north area than in the south one south of 35 this has incited us to estimate the performance of the method in the two areas separately operational satellite altimeters such as topex poseidon and jason measure ssh along tracks whose spacing at the equator is of the order of 315 km ballarotta et al 2019 dufau et al 2016 on a ten day period ocean currents are then retrieved by geostrophy from the ssh gradients as most altimeter observations are a combination of measurements done by different altimeters and as the studied area is situated at mid latitude 35 n the altimeter data are available at a resolution of the order of one hundred kilometers on a ten day period ballarotta et al 2019 figs 2 and 7 besides sea surface temperature can be obtained at 1 1 km and 4 4 km resolution and at a daily frequency from avhrr instruments aboard noaa polar orbiting satellites the combination of ssh and sst involves consideration of two constraints a spatial and a temporal constraint both of which present intrinsic modeling challenges in this paper we focus our study on spatial downscaling only assuming that ssh and sst measurements are made at a daily frequency therefore we simulated satellite observations at different spatial resolutions from the natl60 model outputs we defined five distinct resolution levels for the four variables in multiples of 3x3 and formed 5 datasets r01 r03 r09 r27 and r81 of 366 daily images each each level was calculated from the previous resolution starting with the highest resolution r01 and ending with the lowest r81 at each step the images were degraded by averaging the pixels in a 3 3 window table 2 shows the number of pixels for the different resolutions of each image and the corresponding size of a pixel these considerations have incited us to combine simulated gridded ssh observations at a resolution 120 122 km r81 hereinafter with sst measurements at 13 14 km resolution r09 hereinafter and to estimate the current at the r09 resolution for which the geostrophic approximation is still valid in the present work we have undertaken to statistically learn the existing relationship between ocean currents and ssts with dedicated machine learning algorithms in order to improve the ocean current retrieved from ssh fig 3 presents for october 1 2012 the sst and ssh simulated by the natl60 model at the 5 different resolutions this figure clearly shows that the degradation of the geophysical information is more and more visible as the resolution decreases ssh can be considered as a stream function for the ocean currents with a good approximation for large scale motions i e the ocean currents flow following the isolines of the stream function we also note a similarity between ssh and sst patterns implying that the sst field contains information on ocean currents in the following we label every variable by its symbol ssh sst u v followed by its resolution r81 r27 r09 r03 r01 the two symbols being separated by an underscore as an example ssh r81 stands for the ssh image at the r81 resolution fig 4 presents the distribution of the four variables in the 3 datasets the distribution of every variable in each data is similar except for the sst in the validation set middle row this figure allows us to estimate the range of variability of these variables which is an important information to estimate the performances of our dl downscaling algorithm let us now introduce the neural methodology resac resolution by stages of altimetry and currents we have developed to obtain high resolution ssh and current u v from low resolution ssh and high resolution sst fields we trained resac with the daily images in order to have a large variety of oceanic situations and to test how they are estimated 3 the resac method 3 1 the principle of the method from a physical point of view the resac method follows the approach of rio et al 2016 who showed the feasibility of extracting current information from sst images by inverting the heat conservation equation for retrieving the velocity field from simulated data this method was applied with success on real satellite observations by rio and santoleri 2018 for improving altimeter geostrophic velocities in the present research we aim at improving the resolution of the ocean current field given by low resolution satellite altimeter observations ssh using high resolution satellite sst without applying the geostrophic approximation indeed the nemo natl60 equations model the ocean surface parameters such as sst and ssh with a very good approximation taking into account the well defined relationship existing between them the ocean is described by the primitive equations which are an approximation of the navier stokes equations the surface layers are modeled by equations close to the shallow water equation which is of the form for momentum 1 d d t u f u g s s h d u f u along with the sst advection equation 2 t s s t s s t u d s s t f s s t where u is the velocity vector f represents the coriolis parameter vector directed upward d u and d s s t the parameterization of small scale physics for momentum and temperature equations respectively and f u and f s s t the surface and deep ocean forcing terms of the ocean surface layer the interactions between the two variables ssh and sst are facilitated when they have the same resolution but when they have different resolutions low ssh high sst resolution as in actual satellite observations the relationship linking these two variables is much more complicated and can be biased due to the fact that in the fourier space interactions preferentially occur between close fourier components merilees and warn 1975 maltrud and vallis 1993 the central idea is the locality of the interactions as proposed by kolmogorov in describing the turbulence of incompressible fluids only vortices of the same order of magnitude strongly interact with each other zakharov et al 2012 besides at low resolution eq 1 is approximated by geostrophy terms 2 and 3 while at high resolution term 1 which has a non time dependent component must be added see appendix d for an explanation these considerations have led us to train a cnn architecture consisting of several connected modules such that every module downscales the ssh field to a resolution close to that of the sst field we have finally obtained a ssh field and associated velocities at the high sst field resolution this modular cnn constitutes the framework of the resac model built to retrieve high resolution ocean currents from low resolution ssh and high resolution sst this procedure aims at learning the underlying physical laws embedded in the observations here the natl60 outputs this study can be linked to the physics informed field although our approach has many differences we seek an accurate approximation of the various differential equations used to determine the components u and v of the current u eqs 1 and 2 the ml algorithm models the combination of equations embedded in the natl60 images of ssh sst u and v these images take into account various phenomena such as momentum and sst advection interaction with underlying layers see appendix d whose relationships are guided by the introduction of the existing coherence between the different resolutions as shown in fig 5 in recent physics informed studies dedicated numerical schemes of simulations are approximated using a neural approach huang et al 2021 sonnewald and lguensat 2021 3 2 the resac architecture since the natl60 ocean model represents the links between ssh and sst at every resolution with a good accuracy we trained the resac cnn on the natl 12 dataset in the training phase the resac strategy was to conduct the downscaling gradually stage by stage to avoid spurious interactions between the fourier components of the ssh and sst fields that are far from each other as mentioned above resac therefore consists of 3 stages each being composed of a specific cnn providing intermediate or final estimates of ssh u and v the flow chart of the resac training is presented in fig 5 the progression through the different stages requires estimating the intermediate resolutions until the final downscaling outputs are obtained this is done by using two intermediate cost functions j1 and j2 in fig 5 that control the intermediate ssh resolutions ssh r27 and ssh r09 the final cost functions j3u and j3v in fig 5 control the u and v final outputs u r09 and v r09 fig 5 the minimization considers the sum j of the four intermediate cost functions j1 j2 j3u j3v each one minimizes the sum of the mean square error mse between the value estimated by the cnn y i e s t and the natl60 y i n a t l value the sum being computed over all the available pixels i of all images we denote cnn r81tor27 resp cnn r27tor09 the architecture that allows us to model the downscale procedure from ssh r81 to ssh r27 using sst r027 resp ssh r27 to ssh r09 using sst r09 and we denote cnn uv r09 the cnn architecture that models ssh r09 and sst r09 to u r09 and v r09 fig 5 the first two stages model the flow of information provided by the prognostic variables the third stage models the physics that links ssh and sst with velocities in the natl60 ocean model each cnn takes into account the existing correlations between a given resolution of ssh and the next ssh sst one stage 1 and stage 2 and between the ssh and the velocities at the same resolution the learning process is achieved in the following manner once the architecture of the cnns has been figured out the resac learning phase proceeds from scratch by randomly initializing the weights of the cnns the weights of these cnns are estimated at once by simultaneously minimizing j which controls the passage from one stage to the following one the exact specifications of the cnns such as the number of convolutional layers and feature maps or the size of the filters at each stage are given in appendix b they were heuristically optimized at each stage the cnn specifications being separately determined using their dedicated inputs outputs cost functions and learning set in fig 5 up3x3 denotes the up sampling procedure which increases the size of the image by a factor of 3 3 in order to get the higher resolution corresponding to the next stage in resac we performed a bilinear up sampling method with a 3 3 up sampling ratio we note that the cnn r81tor27 architecture first stage in fig 5 is constituted of a larger number of weights than the other two respectively the cnn r27tor09 associated with the second stage and the cnn uv r09 associated with the third stage see fig 5 this can be explained as follows the ssh and sst natl60 fields are weakly correlated this correlation increases at intermediate resolution when these two fields have interacted through the cnn r81tor27 fig 5 which therefore must perform a more difficult task than the other two cnns and consequently requires a larger number of weights the full learning process learning validation cost functions learning steps hyper parameter assessments is detailed in appendix b training validation and test sets are completely decorrelated see section 2 the number of convolution layers the size and number of filters the activation and loss functions were chosen based on previous experiments the learning rate schedulers and the number of epochs were empirically tuned on the validation set see appendix b at the end of the training when all the weights of resac have been settled resac can be used to retrieve the high resolution ocean currents in the retrieval phase only the variables present in the green frames in fig 5 ssh r81 sst r27 sst r09 are inputted in resac so as to estimate the current u r09 and v r09 resac allows us to downscale ssh r81 to ssh r09 u r09 and v r09 gradually the process being controlled by intermediate and high resolution sst data sst r27 and sst r09 fig 6 shows the inputs of the resac model and its final outputs in the operational phase in section 4 we studied different resac models with different inputs and architectures for each resac model we trained 20 networks with different weights for initialization using an adapted learning rate planner see appendix b the final prediction values are the average values of the predictions of the 20 networks this procedure allowed us to increase the performance of the final cnn we also tried to train a simpler architecture linking the inputs ssh r81 sst r27 sst r09 to ssh r09 directly but we were not able to perform the training phase correctly the minimization was still stuck in a flat local minimum this proves the need to process the downscaling gradually by introducing the ssh sst relationships at different resolutions 4 resac performances 4 1 qualitative comparison of the different models to show the effectiveness of the resac method and the contribution of sst we trained two separate resac models called resac ssh sst and resac ssh both of which provide downscaled ocean parameters the inputs to resac ssh sst are ssh at resolution r81 and sst observations at resolutions r27 and r09 see fig 5 while the inputs to resac ssh are ssh r81 data only appendix b fig b 2 the architectures of these two resac models are very similar but resac ssh is somewhat different due to the removal of the sst since the sst inputs green in fig 5 were not used resac ssh has a smaller number of weights 400 301 than resac ssh sst 402 797 weights in addition we compared our results to those obtained by a simpler approach a well known unsupervised image processing algorithm https opg optica org oe fulltext cfm uri oe 19 27 26161 id 225713 this algorithm hereafter bi cubic geostro consists of performing ssh interpolations with two dimensional bicubic functions we performed the interpolation of ssh r81 data at r09 resolution with the bicubic algorithm and then estimated the r09 current components u v using the geostrophic approximation the sst is not used the training and testing of both resac models were performed in the same manner using the same data and conditions as presented in section 2 training was performed on the 366 days of the natl 12 dataset validation on the natl 08v dataset december 2008 and testing on the natl 08t dataset march june september 2008 all the statistics presented below have been computed on natl 08t first we show the downscaling of ssh and velocity values given by resac ssh sst resac ssh and bi cubic geostro fig 7 second row for march 15 2008 we also compare these sshs with the ssh values from natl 08t by taking their difference third row the fourth row shows the energy obtained from the three models at first glance resac ssh sst reproduces the natl 08t values better than the other two models we note that the area situated north of 35 n which is much more energetic than the south one as it is populated with a large number of energetic mesoscale eddies is better reproduced than the south one clearly resac ssh sst outperforms resac ssh and the worst results are provided by bi cubic geostro proving the interest of using physical information such as the sst during the downscaling procedure we note that the geographic positions of the major oceanic structures are correctly retrieved by resac ssh sst as well as the velocity amplitudes and directions 4 2 quantitative comparison of the different models we compared the outputs of resac ssh sst resac ssh and bi cubic geostro to the natl 08t dataset using several statistical estimators first we estimated the consistency of the variable reconstruction by computing a daily rmse for each variable and for each test image according to eq 3 3 r m s e t e s t 1 n i 1 n y i n a t l y i e s t 2 where n is the number of pixels of a r09 image n 144 153 y i e s t is the value estimated by resac at pixel i y i n a t l is the natl60 value at pixel i we averaged the daily rmse of the test images we therefore obtained a mean rmse denoted mrmse in the following table 3 shows the mrmse computed between the natl 08t variables and the resac ssh sst first row in table 3 resac ssh second row and bi cubic geostro third row outputs for ssh u and v at the r09 resolution it clearly appears that resac ssh sst performs better than resac ssh the mrmse between the natl 08t and the resac ssh sst ssh is much lower than the natl 08t ssh distribution whose most frequent values are of the order of 0 50 m fig 4 which suggests that resac ssh sst is efficient in downscaling the ssh and the associated ocean currents the use of stage 3 in the resac training implies knowledge of the high resolution ocean current field which can only be given by a numerical model we also calculated ocean currents by applying geostrophy to ssh r09 given by the two resac models these outputs are labeled resac ssh sst geostro and resac ssh geostro respectively in fig 8 we present the time series of daily rmses given by resac ssh sst resac ssh and bi cubic geostro for ssh top panel and the daily rmses given by these statistical models for u v middle and bottom panels the best results for u v are given by resac ssh sst followed by resac ssh sst geostro resac ssh resac ssh geostro and the worst by bi cubic geostro the information embedded in the sst advection equation eq 2 improves the r09 ocean currents given by ssh alone analysis of these rmses allows us to see the impact of the sst on the retrieval of ocean currents the fact that the ocean current rmses of resac ssh and resac ssh geostro are nearly similar implies that the cnn uv r09 fig b 2 included in resac ssh is a function close to the geostrophy that would be difficult to improve without the use of additional parameters the cnn uv r09 fig 5 included in resac ssh sst improves the retrieval of ocean currents over geostrophy and thus mitigates the deficiencies of the latter by introducing sst as an additional variable let us now try to understand how sst acts in resac ssh sst to obtain the best ssh and ocean currents as shown in fig 8 schematically the performance improvement has two main sources first sst helps to retrieve a better ssh which leads to a better geostrophic approximation as we can see by comparing the ocean current rmses of resac ssh geostro and resac ssh sst geostro 0 02 ms 1 improvement on the mrmse table 3 second sst helps the cnn uv r09 of resac ssh sst fig 5 to retrieve currents with additional performance with respect to geostrophy in the following we focus our analysis on the performances of the two dl models namely resac ssh sst and resac ssh compared to those of bi cubic geostro we calculated the complex correlation see appendix c for a definition between the velocities given by resac and by bi cubic geostro and with those of natl 08t table 4 in the north and south zones we note that the correlations given by resac ssh sst are higher than those given by resac ssh and bi cubic geostro the difference are of 0 11 and 0 21 respectively values which are highly significant given the high degree of freedom of these correlations 144 153 22 032 which corresponds to the number of pixels by image the angle between the two vectors natl 08t and algorithm estimated has no systematic bias the mean angle between the two vector fields being close to zero appendix c once again resac ssh sst performs better in retrieving the velocities than resac ssh and bi cubic we note that the standard deviation std of these correlations are small which shows the good functioning of the retrieval algorithms 4 3 case of noisy ssh data ssh measurements made by altimeters are often characterized by a high noise signal ratio due to instrumental noise and interpolation of altimeter observations this interpolation introduces noise since the altimeter observations are made under the satellite track and gridded by combining several tracks over a fairly long time interval depending on the satellite repeat period 10 days for topex poseidon ballarotta et al 2019 dufau et al 2016 in the following we simulated those noises by adding a gaussian noise to the ssh r81 of natl 08t for each pixel of each test day we added a gaussian random noise n 0 σ 2 to the ssh r81 images the ssh signal is now sshn r81 ssh r81 n 0 σ 2 we tested two different noises the standard deviation of the first one is σ 1 0 05 m that of the second is σ 2 0 10 m these two values are quite important with respect to the natl12 ssh distribution whose most frequent values are of the order of 0 50 m fig 4 first row fig 9 shows the ssh at the r81 resolution without noise and for the two noise values on 09 01 2013 we note the strong alteration of the ssh r81 patterns for the highest noise level the mean daily rmse mrmse of the two resac models and bi cubic geostro ssh and u v with respect to natl08t values are presented in table 5 for the full area the mrmses increase with the noise level and are smaller when sst is considered showing the important role of the sst in velocity retrieval as expected the higher the noise the larger the mrmse the complex correlation cc of the velocities of both resac and bi cubic geostro models with natl08t velocities is presented in table 6 for the whole area and the north and south zones resac ssh and bi cubic geostro show much poorer performance than resac ssh sst the contribution of the sst is essential as can be seen on the velocity correlations which are always better for resac ssh sst with a difference of 0 15 to 0 20 compared to the other models we note that the downscaling procedure keeps the velocity directions with good accuracy we again find that the performances are better in the northern region than in the southern region this is due to the fact that in the northern region ocean currents are stronger and associated with large stable ocean structures corresponding to the gulf stream while the southern region is more turbulent with smaller currents associated with small ocean structures 5 discussion the performance of resac ssh sst is very satisfactory and superior to that of resac ssh and bicubic geostro as shown by several statistics presented in section 4 tables 3 and 4 the daily rmses displayed in fig 8 allow us to analyze the performances of the different models the good score of resac ssh sst compared to resac ssh and bi cubic geostro is due to sst acting in two different ways on the structure of the algorithm first the sst input on cnn r81 to r27 and cnn r27 to r09 fig 5 steps 1 2 of the flow diagram improves ssh retrieval at the r09 resolution as can be seen by comparing the daily ssh rmses given by the three downscaling models top panel of fig 8 second the sst input on cnn uv r09 fig 5 step 3 provides advection information which improves the r09 ocean current retrieval as can be seen from the daily rmses of the u and v current components fig 8 middle and bottom panels and table 4 moreover resac gives satisfactory performance when ssh is noisy to this end we conducted two experiments with two different noise levels on the ssh one with a standard deviation std of 5 cm the second with a std of 10 cm these values are high compared to the dominant value of the ssh probability density function pdf which is of 50 cm according to fig 4 in all cases without and with noise the complex correlations between resac current vectors and natl 08t current vectors reported in tables 5 and 6 show that resac ssh sst is the best estimator of ocean currents at r09 resolution all these analyses show that sst plays a critical role in the accuracy of ocean current retrieval especially when ssh measurements are noisy the sst which is not considered in resac ssh and bi cubic geostro adds important information on ocean currents in resac ssh sst allowing to improve the resolution of the retrieved ssh field as well as that of the associated velocities the velocity information contained in sst is linked to the structure of the sst advection equation used in the natl60 simulation eq 2 which associates sst with velocity the rationale is that sst which is advected by ocean currents is a stream function for slowly varying ocean motions period greater than a few days which therefore implies that the fluid velocity is almost perpendicular to the sst gradient see appendix d in this study eq 2 was implicitly modeled by resac so that terms 2 3 4 were statistically accounted for from a theoretical point of view we can state that resac ssh sst has approximated the physical laws included in the data fields represented by the pdes constituting the natl60 model see appendix d the performance of resac ssh sst is satisfactory in the northern area but deteriorates in the southern area due to the low signal to noise ratio in the latter region tables 3 and 4 for the natl 08t test set tables 5 and 6 for the noisy observations furthermore comparison between the retrieval of u v by resac ssh sst and resac ssh sst geostro fig 8 shows that geostrophy computed from ssh only is a quite weak estimator of ocean currents that can be improved with the help of additional information sst in the present case and not by algorithmic improvements as shown by the similar u and v rmses provided by resac ssh and bicubic geostro fig 8 second and third rows a major point of resac is to decompose the training phase into three stages as outlined in fig 5 to facilitate the exchanges of physical properties of the fluid associated with ssh and sst which preferentially occur in the spectral domain at wave numbers which are close owing to the locality of the interactions in turbulence merilees and warn 1975 maltrud and vallis 1993 zakharov et al 2012 the training phase uses an intermediate resolution of the sst observations sst r27 which in the present case is simply obtained by filtering sst r09 and two intermediate resolutions of the ssh observations ssh r27 and ssh r09 allowing to control the intermediate estimations of resac ssh r27 and ssh r09 by using specific cost functions see fig 5 for the flow diagram in the operational phase the inputs of resac ssh sst are the ssh observations ssh r81 at low resolution and the sst observations at intermediate sst r27 and mesoscale sst r09 resolutions the outputs are the ocean currents at mesoscale resolution u r09 v r09 together with the ssh at the same resolution ssh r09 this decomposition in stages is an essential ingredient of the resac method the performance of the algorithms without the stage decomposition was found to be much lower not shown resac training was conducted on very high resolution ocean model output which allows for high resolution velocities that are not observable by in situ measurements in a future study we propose to train a resac sat dedicated to processing satellite observations on ssh altimeter tracks and sst fields simulated from an operational ocean model such as mercator models https www mercator ocean eu en as they assimilate several different observed variables these models are close to the real ocean it is therefore expected that the observation operator according to data assimilation jargon permitting to associate satellite observations with ocean model data will be close to identity as found in similar studies such as the scatterometer wind retrieval richaume et al 2000 these authors calibrated a ers1 wind retrieval neural network with era interim wind fields in the construction of resac sat it would be necessary to take into account the complex spatio temporal sampling along the ssh observation tracks associated with the satellite repeat period recent studies have proposed new efficient procedures to improve the consideration of ssh sampling they are based on 4d var febvre et al 2022 or on cnn nardelli et al 2022 the architecture of resac will be adapted according to the sampling characteristics spatial and temporal of the satellite ssh sensors and the chosen procedure a complementary way to improve the efficiency of such an operational algorithm would be to consider the physical properties of the links between ssh and sst we could add new inputs to resac such as air sea heat exchange term 4 in eq 2 which improves the impact of sst on ssh as has been shown rio and santoleri 2018 nardelli et al 2022 for large scale ocean motions of the order of atmospheric structures several hundred kilometers and should therefore improve ssh at r81 resolution small scale 10 kilometers ocean motions are more difficult to consider because they are mainly associated with heat exchanges within the deep ocean which are related to mixed layer dynamics and horizontal eddy fluxes investigating the impact of these processes on the resolution of the ssh and associated ocean currents would deserve a specific study besides ssh resolution in both time and space will be significantly improved by the launch of the surface water and ocean topography swot mission which is a joint nasa cnes csa asc uk space agency program which will facilitate the inclusion of ssh data 6 conclusion we have built an efficient machine learning algorithm the cnn based resac method to increase the resolution of ocean currents provided by low resolution ssh 120 122 km ssh r81 up to mesoscale resolution 13 14 km ssh r09 by combining them with high resolution sst we consider an oversimplified case for which both ssh and sst measurements are gridded daily observations the essence of resac is to combine coarse ssh observations with high resolution sst by introducing intermediate sst resolutions in the training and retrieval phases to increase the accuracy of resac we have constructed a cnn ensemble whose prediction values are the average values of the outputs of 20 cnns with the same architecture and trained on the same datasets but with different initial values more generally resac can used to merge and downscale all data fields that are observed simultaneously the resac architecture has shown the interest of decomposing the learning phase into several stages to facilitate the fusion of these data resac accuracy could be improved by using a larger dataset several years instead of one for training moreover several additional studies could be undertaken such as training two different resac models one over the northern area and the other over the southern area with the expectation that resac s performance will improve especially in the southern area where ocean currents are weak and mostly turbulent furthermore as designed resac should be able to operate at least in theory in equatorial regions where geostrophy is not applicable the present work can be considered as a feasibility study leading to the design of a future operational dl method dedicated to the downscaling of ssh fields in the real world the problem is much more complicated it is necessary to take into account the effect of ssh sampling along satellite tracks and successive interpolations of altimeter data during the repeat period errors in the final satellite products are not due to simple smoothing but to the complex spatiotemporal interpolation ballarotta et al 2019 this challenging problem implies further developments to obtain an efficient operational dl algorithm which is beyond the scope of this paper the present study acts as a proof of concept regarding the merging of datasets of different resolutions to downscale low resolution data fields up to high resolution ones it explicitly relies on the ability of cnns to summarize the geophysical knowledge linking sst ssh and velocity fields u v at every pixel resolution from 120 km2 to 13 km2 moreover a challenging study would be to estimate ocean currents at the r01 resolution 1 5 1 5 km this scale corresponds to ocean filaments for which the geostrophic approximation is no longer valid which makes the velocity downscaling at high resolution more complex this would imply adding a new stage in the resac algorithm owing to the large amplitude of the downscaling range and consequently increasing the size of the cnns and the computational cost of the learning phase this would require access to very powerful gpus these results would have been improved if a longer time series for the training had been at our disposal resac must be learned on high resolution ocean modes outputs since there is no high resolution ocean observations the resac outputs could be somewhat different from actual ocean variables but not too much because ocean models are now very efficient as they assimilate a large number of observations this constraint is inherent to any high resolution dl downscaling operator of ocean variables as mentioned in the introduction deep learning dl is a very productive research area today its use for solving complex problems in the environmental sciences leading to scientific discoveries is rapidly increasing geophysical sciences and climate studies must deal with massive amounts of multidimensional data dl methodologies are well suited to solve the formidable scientific challenge of multi satellite observations and climate model outputs high performance computing hpc is increasingly being used to meet the exponentially growing demand for dl data processing multiple gpus and high speed interconnect networks are available to support dl on hpc systems han et al 2019 all of this research done in the field of artificial intelligence is now available to the environmental research community and should enable rapid development of geophysical knowledge this development of dl should give us confidence in the applications of resac indeed resac learned on simulated data contains a summary of the physics that relates ssh and sst to ocean currents given its generic nature we expect the resac methodology to be applicable to a wide variety of downscaling problems credit authorship contribution statement sylvie thiria designed the research wrote the manuscript charles sorror developed the resac models performed the simulations and validated the results theo archambault developed the resac models performed the simulations and validated the results anastase charantonis participated in the cnns development manuscript writing dominique bereziat participated in the cnns development manuscript writing carlos mejia data management and to the document preparation jean marc molines provided the natl60 outputs ssh sst and ocean velocities michel crépon discussed the results wrote the manuscript and edited it declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments natl60 simulations were performed on cines occigen supercomputer in the frame of the 2014 2015 great challenge organized by genci we also thank dr manucharyan and the anonymous reviewers who contributed to improve our document financial support the present research was supported by sorbonne university ipsl sama and scai appendix a let us denote e i the kinetic energy at a pixel i a 1 e i 1 2 u i 2 v i 2 the global kinetic energy of an image for a day d is given by e d i n e i let us denote s i the local enstrophy at a pixel i a 2 s i 1 2 x v i y u i 2 the global enstrophy of an image for a day d is given by s d i n s i energy characterizes the intensity of the velocity field whilst enstrophy its shear intensity global energy and enstrophy are conservative quantities in the absence of forcing and dissipation for two dimensional flows maltrud and vallis 1993 merilees and warn 1975 appendix b resacnet architecture we used the python 3 9 keras 2 4 0 software to implement the resac model resac was trained on an nvidia gpu with cuda 11 0 under a linux system the resac architecture consists of 3 cnns fig 5 cnn r81tor27 has 7 convolutional layers each layer consisting of 36 feature maps with 6 6 filters using the relu activation function each layer is followed by a batch normalization layer the last layer connects the 36 characteristic maps to a 1 1 mask to fit the output image using a sigmoid activation function cnn r27tor09 has 3 layers consisting of 24 characteristic maps with 5 5 masks using the relu activation function each layer is followed by a batch normalization layer the last layer connects the 24 characteristic maps to a 1 1 mask to fit the output image using a sigmoid activation function cnn uv r09 has the same structure as cnn r27tor09 but repeated 3 times the first one takes the sst r09 and the output of cnn r27tor09 to make one output uv r09 the second and third one get that output and are dedicated to finding respectively the u and v components of the current resac ssh has 400 301 weights and resac ssh sst 402 797 weights from a layer to the next one the size of the input is preserved using a same padding parameter the resac architecture weights are initialized by the the normal method keras documentation the cost function j is the mse of the error for all the cost functions b 1 j 1 n i 1 n y i n a t l y i e s t 2 the learning validation and test sets are completely decorrelated see section 2 the number of convolution layers the size and number of filters the activation and loss functions were chosen based on previous experiments where the learning rate schedulers and the number of epochs were empirically tuned on the validation set for each resac model we trained 20 networks with different weights for the initialization using an adapted learning rate scheduler for resac ssh sst the learning rate starts at 2e 3 and stays constant through 20 epochs is then multiplied by exp 0 02 at each epoch between epoch 20 and 60 and by exp 0 05 between epoch 60 and 100 for resac ssh the learning rate starts at 5e 3 and stays constant through 20 epochs is then multiplied by exp 0 02 at each epoch between epoch 20 and 40 and by exp 0 05 between epoch 40 and 100 the final prediction is the average of the predictions of the 20 networks fig b 1 presents the evolutions of the cost functions during the training phase for the learning and validation datasets the x axis stands for time evolution y axis represents the value of the cost function the weights are those determined by cross validation fig b 2 presents the flow diagram for the training of resac ssh appendix c complex correlation comparing two vector fields is a quite complicated task we can compare the vector components with classical statistical tools std correlation but their interpretation in the physical space is not easy one of the difficulties is to take relationships existing between the vector components into account in the present paper we propose a method leading to a straightforward geometrical interpretation for two dimensional vector fields the trick is to use the isomorphism existing between two dimensional vectors and complex numbers let us consider two two dimensional vector fields u v and u v that we aim at comparing the associated complex numbers u and u are written as u u iv ρ e i α and u u iv ρ e i α β where i is the imaginary symbol ρ and ρ are positive numbers which represent the modules of the vectors α and α β define the angles of the vectors with the real axis β representing the angle between the two vectors let us define the complex correlation cc between u and u c 1 c c 1 n 1 n u j u j 1 n 1 n u j u j 1 n 1 n u j u j where u u i v and u u i v are the complex conjugates associated with u and u respectively the sums are taken over the n vectors constituting the fields cc can also be written as c 2 c c 1 n 1 n ρ j ρ j e i β j 1 n 1 n ρ j ρ j 1 n 1 n ρ j ρ j c e i θ this polar representation allows us geometrical interpretations of cc as shown in the following first let us assume that there is a systematic rotation β between the two vector fields cc writes c 3 c e i β 1 n ρ j ρ j 1 n ρ j ρ j 1 n ρ j ρ j c e i β where c is the correlation between the vector modules and β the systematic angle between the two vector fields this case is often encountered in oceanography constant angle between the wind and the current in the ekman drift rotation of the current with depth in the ekman spiral second let us interpret the angle of the complex correlation when the two fields are quasi co linear which is the case when we compared modeled and observed currents eq c 2 can be written as c 4 c c 1 n ρ j ρ j c o s β j i s i n β j 1 n ρ j ρ j 1 n ρ j ρ j eq c 4 is somewhat difficult to interpret let us consider the following simple case we now assume that each vector field has a constant modulus ρ and ρ are constant eq c 4 writes c 5 c c ρ ρ 1 n c o s β j i s i n β j 1 n ρ j ρ j 1 n ρ j ρ j 1 n 1 n c o s β j i s i n β j if the angles β j are small β 10 and their mean close to zero which is a straightforward hypothesis eq c 5 writes c 6 c c 1 n 1 n c o s β j i s i n β j 1 1 n 1 n i s i n β j 1 i ɛ where ɛ is a small number due to the fact that the mean of β j is close to zero cc can be written as c c e i θ where θ is a small angle this simplified example can explain the fact that the angles θ of the complex correlations we have computed in the present study are always small the cc angle gives an estimate of the mean angle between the two vector fields which is near zero if the angle distribution is gaussian complex correlation has been used in several studies implying two dimensional vector analysis in meteorology and in oceanography wang and moers 1977 hardy and walton 1978 legler 1983 horel 1984 eq c 6 shows that cc gives an estimate of the mean of the angles β j between the two vector fields which is close to zero and does not provide useful information on the structure of the two fields a more informative parameter is the standard deviation of that angle which could be obtained by doing the following operation u j u j ρ j ρ j e i β j which provides the β j and allows the computation of the standard deviation of the β j in fact the correlation between two dimension vector fields is a matrix cm of the form c m c u u c u v c v u c v v where c u v is the correlation between the component u of vector u and v of vector u and so on the matrix formulation of the correlation existing between two vector fields contains more information than cc but cc is easier to interpret in the physical space cc is mainly dedicated to compare vector fields which are related by a scaling and a systematic rotation in the present study we chose the simplicity of the interpretation and dealt with complex correlation appendix d the resac model does not learn the time evolution of eq 1 but it better approximates eq 1 than geostrophy balance between term 2 and 3 by taking into account a statistical modeling of term 4 and of the spatial contribution of term 1 which is of the form u u resac therefore can be considered as a statistical model of the non time dependent pde u u f u g ssh d u coupled with the slowly varying temperature equation eq 2 which is of the form sst u d s s t the operators d u and d s s t model turbulence 
23790,we present a deep learning method to downscale low resolution geophysical fields by merging them with high resolution data the downscaling was performed using an ensemble of convolutional neural networks cnns whose prediction values are the average values of the outputs of 20 cnns academic experiments were conducted on simulated ocean data in the gulf stream region given by the outputs of the natl60 model the cnns forced with low resolution 120 120 km sea surface high ssh data and mesoscale resolution 12 12 km sea surface temperature sst data allowed us to obtain mesoscale resolution sea surface currents with good accuracy sensitivity experiments have shown that taking sst into account significantly increases the accuracy of the high resolution velocity retrieval even when noise is added to the ssh data the velocity information embedded in the transport equation modeling the sst advection is taken into account by the cnn which greatly increases the resolution of ocean currents provided by ssh in the present work we only consider spatial downscaling by assuming that ssh and sst are daily observations the method we developed is generic and can be used to improve the resolution of a wide variety of large scale fields by merging them with high resolution fields keywords downscaling machine learning altimeter sst ocean currents data and code availability the datasets we used natl 12 and natl 08 which were extracted from the natl60 model 630 experiments output over the gulf stream area in the north atlantic ocean are already available in the ipsl thredds catalog at the address https doi org 10 14768 c3c33afe 2a37 42e1 bb29 21428387f658 the python codes of the resac models are hosted at gitlab https gitlab in2p3 fr carlos mejia resacpubli ocemod releases v1 0r 1 introduction for several decades a wide variety of satellite sensors has allowed us to considerably improve our knowledge of the state of the planet earth and its potential evolution these sensors observe a multitude of geophysical parameters with various sampling strategies both in space and in time the objective of this study is to show how the combination of several parameters can reinforce each other to obtain geophysical fields observed with better resolution and accuracy this problem is encountered in many aspects of geophysical sciences such as atmospheric sciences oceanography hydrology fluid mechanics geology astrophysics we have therefore decided to conduct an academic study that should benefit a wide range of observations of geophysical parameters in this context we have chosen to focus our efforts on the algorithmic aspect of downscaling these observations by combining them together in the present paper we focused our research on the observation of ocean currents by satellite the ocean has been observed for several decades by numerous satellites equipped with various sensors that have provided global coverage of dynamic parameters such as ocean circulation with sea surface topography hereafter ssh from the topex poseidon and then jason altimeters sea surface temperature hereafter sst with the high resolution avhrr radiometers launched aboard meteorological satellites ocean biological productivity with multispectral ocean color sensors seawifs meris modis viirs and ocean salinity smos and aquarius these satellite sensors have contributed to a better understanding of the ocean and the detection of its changes in response to global warming cazenave et al 2018 ocean currents play an important role in the latitudinal transport of heat and in advecting and structuring ocean properties such as salinity nutrients phytoplankton across a wide range of scales from basin to sub mesoscale sasaki et al 2014 mcwilliams 2016 levy et al 2012 the estimation of ocean currents at high resolution from satellite observations a few kilometers has been addressed by a large number of scientific studies that have combined ssh and sst observations rio et al 2016 rio and santoleri 2018 nardelli et al 2022 or that processed sst image sequences isern fontanet et al 2006 bowen et al 2002 vigan et al 2000 kelly et al 1999 emery et al 1986 other studies have used intrinsic physical properties of ocean surface layers such as conservation of potential vorticity ubelmann et al 2015 recently george et al 2021 and manucharyan et al 2021 have shown that ssh interpolation can be improved by theoretical considerations based on qg turbulence and have given leads based on rational physical arguments to improve the reconstruction of mesoscale ssh from sst data we propose here a deep learning method dl to obtain high resolution ocean currents by combining low resolution ssh and high resolution sst the problem is very complicated as ssh are measured along satellite tracks which are separated of about one hundred kilometers with a repeat sampling period of about 10 days ballarotta et al 2019 while sst are measured as daily two dimension images at a resolution of 4 km the association of these two observations of different resolution implies the solving of a difficult inverse problem with time and spatial constraints as accounted in 4d var data assimilation in the present paper we oversimplified the observing system by focusing our study on spatial downscaling alone assuming that ssh and sst measurements are gridded daily observations of different resolution due to the lack of high resolution ocean parameter observations and to simplify the data handling at various resolutions we used simulated data fields ssh and sst are the outputs of a very high resolution ocean numerical model ssh and sst are therefore linked to each other by well known physical laws that are taken into account by the ocean model and that will be learned by our method we chose to conduct our study in the gulf stream region 35 n 55 w where ocean currents and associated mesoscale dynamical features are important we used the outputs given by the high resolution ocean model natl60 with a horizontal resolution of 1 60 in latitude these simulated observations provide ssh sst and currents at very high resolution down to 2 km which is much better than operational products derived from observations such as those provided by aviso https www aviso altimetry fr en data html due to the high resolution data fields and the large area under study we decided to use deep learning algorithms which are methods suitable for solving problems involving very large datasets recent developments in artificial intelligence combined with the availability of large datasets such as those provided by high resolution climate simulations and increased computational power have led to the emergence of information processing capabilities to learn the hidden structures of geophysical phenomena over the past decade statistical and machine learning methods have greatly facilitated the study of these multidimensional datasets as evidenced by the growing number of publications since 2018 convolutional neural networks cnns which are extensions of multilayer perceptrons le cun et al 1990 2015 simard et al 2003 goodfellow et al 2016 have proved their ability to exploit spatial and temporal data structures find patterns and efficiently fuse heterogeneous information sources reichstein et al 2019 rolnick et al 2019 in addition image analysis of complex geophysical fields using neural networks has been a very productive line of research over the past decade and has led to relevant environmental applications egmont petersen et al 2002 he et al 2016 wang et al 2019 martinez et al 2020 specific neural architectures have recently been developed which allow us to obtain numerical schemes that accurately solve partial differential equations ruthotto and haber 2019 rousseau et al 2019 these neural architectures can be used effectively to represent dynamic phenomena that are usually represented by numerical models the challenge is therefore to introduce prior knowledge provided by numerical and or statistical analyses into these architectures to improve their performance when solving specific environmental problems ducournau and fablet 2018 fablet et al 2018 de bézenac et al 2019 new tools are made available to the scientific community to help them implement adequate architectures to carry out their research as an example recent papers propose automatic differentiation tools embedded in deep learning frameworks to introduce end to end neural network architectures for data assimilation pannekoucke and fablet 2020 in addition several studies using machine learning ml hereafter methods have been devoted to modeling ocean surface variables with success showing the potential power of these methods examples include sea surface temperature reconstruction from satellite observations using a convolutional neural network barth et al 2020 interpolation of sea level anomalies using data driven methods lguensat et al 2019 and cnn manucharyan et al 2021 george et al 2021 ocean ml simulations for climate modeling sonnewald and lguensat 2021 this paper aims at showing the ability of ml to downscale ssh and associated currents by studying an academic case analysis of the geophysical properties of the problem ml learning and estimation of the physical coherence of the reconstructed fields are performed using a high resolution numerical model output the paper is organized as follows in section 1 we describe the problem in section 2 we present the data the ml algorithm is described in section 3 in section 4 we present the performance of our method and analyze the results in section 5 we discuss the results and conclude in section 6 2 data the objective of the present study is to develop a generic machine learning prototype to perform geophysical field downscaling to demonstrate the feasibility of such a study we used academic datasets with physical characteristics similar to those provided by ocean satellite sensors the simulated ssh and sst data used to construct the deep learning algorithms allow us to obtain well documented ocean datasets with no missing data and control for the different resolutions we wish to explore we used the sst ssh and horizontal velocity field u v provided by the natl60 model which is a very high resolution model of the north atlantic ocean extending from 26 n to 66 n fig 1 the natl60 ocean model is based on the nemo 3 6 code in the version we used the tide was not included the horizontal grid is extracted from the global tri polar grid of the orca series madec and imbard 1996 with a resolution of 1 60 at the equator the horizontal grid spacing ranges from 1 6 km at 26 n to 0 9 km at 65 n the grid has been designed so that the model explicitly simulates the scales of motions that will be observed by the swot altimetric mission in practice the model s effective resolution is about 10 15 km in wavelength the resolution increases towards the poles and is about 1 1 km at the latitude of the gulf stream the vertical grid uses 300 levels with a thickness of 1 m near the surface up to 50 m in the deepest part to well reproduce the vertical velocities which condition the vertical exchanges and the interactions with the bottom topography we used the atmospheric forcing dfs5 2 drakkar forcing set 5 2 in dussin et al 2016 initial conditions were taken from the 1 12 mercator ocean reanalysis the output of natl60 was analyzed in ajayi et al 2020 in the present research we studied the gulf stream region 26 n 45 n 40 w 65 w fig 1 which is the most energetic region of the north atlantic ocean this frontier simulation required 16mcpuh on 14 000 cores and was performed during the genci 2016 grand challenge because we focus our study on surface currents recovered from ssh we bypassed the ekman surface drift by considering the horizontal current at a depth of 15 m we processed 366 daily images of 4 variables ssh sst u v covering the period from october 1 2012 to october 1 2013 for training dataset natl 12 below and four months with atmospheric forcings from the year 2008 one for validation december natl 08v below and three for testing march june september designated natl 08t below every image is composed of 1296 1377 pixels for each variable a pixel being a square box of approximately 1 5 1 5 km we denote this dataset r01 in fig 2 we show the mean local kinetic energy and the mean local enstrophy see appendix a for their definition associated with every pixel over the 366 images of the period studied we notice that the area can be split into two areas the one north of 35 n which includes the gulf stream is very energetic 86 of the total energy the second one south of 35 n is less energetic 15 of the total kinetic energy and corresponds to a recirculation area populated with small eddies characteristic statistical values of the two areas are shown in table 1 we note that the average current in the northern region has a fairly large eastern component 3 8 cms 1 corresponding to the gulf stream while the other current components are far less important enstrophy also is much more intense in the north area than in the south one south of 35 this has incited us to estimate the performance of the method in the two areas separately operational satellite altimeters such as topex poseidon and jason measure ssh along tracks whose spacing at the equator is of the order of 315 km ballarotta et al 2019 dufau et al 2016 on a ten day period ocean currents are then retrieved by geostrophy from the ssh gradients as most altimeter observations are a combination of measurements done by different altimeters and as the studied area is situated at mid latitude 35 n the altimeter data are available at a resolution of the order of one hundred kilometers on a ten day period ballarotta et al 2019 figs 2 and 7 besides sea surface temperature can be obtained at 1 1 km and 4 4 km resolution and at a daily frequency from avhrr instruments aboard noaa polar orbiting satellites the combination of ssh and sst involves consideration of two constraints a spatial and a temporal constraint both of which present intrinsic modeling challenges in this paper we focus our study on spatial downscaling only assuming that ssh and sst measurements are made at a daily frequency therefore we simulated satellite observations at different spatial resolutions from the natl60 model outputs we defined five distinct resolution levels for the four variables in multiples of 3x3 and formed 5 datasets r01 r03 r09 r27 and r81 of 366 daily images each each level was calculated from the previous resolution starting with the highest resolution r01 and ending with the lowest r81 at each step the images were degraded by averaging the pixels in a 3 3 window table 2 shows the number of pixels for the different resolutions of each image and the corresponding size of a pixel these considerations have incited us to combine simulated gridded ssh observations at a resolution 120 122 km r81 hereinafter with sst measurements at 13 14 km resolution r09 hereinafter and to estimate the current at the r09 resolution for which the geostrophic approximation is still valid in the present work we have undertaken to statistically learn the existing relationship between ocean currents and ssts with dedicated machine learning algorithms in order to improve the ocean current retrieved from ssh fig 3 presents for october 1 2012 the sst and ssh simulated by the natl60 model at the 5 different resolutions this figure clearly shows that the degradation of the geophysical information is more and more visible as the resolution decreases ssh can be considered as a stream function for the ocean currents with a good approximation for large scale motions i e the ocean currents flow following the isolines of the stream function we also note a similarity between ssh and sst patterns implying that the sst field contains information on ocean currents in the following we label every variable by its symbol ssh sst u v followed by its resolution r81 r27 r09 r03 r01 the two symbols being separated by an underscore as an example ssh r81 stands for the ssh image at the r81 resolution fig 4 presents the distribution of the four variables in the 3 datasets the distribution of every variable in each data is similar except for the sst in the validation set middle row this figure allows us to estimate the range of variability of these variables which is an important information to estimate the performances of our dl downscaling algorithm let us now introduce the neural methodology resac resolution by stages of altimetry and currents we have developed to obtain high resolution ssh and current u v from low resolution ssh and high resolution sst fields we trained resac with the daily images in order to have a large variety of oceanic situations and to test how they are estimated 3 the resac method 3 1 the principle of the method from a physical point of view the resac method follows the approach of rio et al 2016 who showed the feasibility of extracting current information from sst images by inverting the heat conservation equation for retrieving the velocity field from simulated data this method was applied with success on real satellite observations by rio and santoleri 2018 for improving altimeter geostrophic velocities in the present research we aim at improving the resolution of the ocean current field given by low resolution satellite altimeter observations ssh using high resolution satellite sst without applying the geostrophic approximation indeed the nemo natl60 equations model the ocean surface parameters such as sst and ssh with a very good approximation taking into account the well defined relationship existing between them the ocean is described by the primitive equations which are an approximation of the navier stokes equations the surface layers are modeled by equations close to the shallow water equation which is of the form for momentum 1 d d t u f u g s s h d u f u along with the sst advection equation 2 t s s t s s t u d s s t f s s t where u is the velocity vector f represents the coriolis parameter vector directed upward d u and d s s t the parameterization of small scale physics for momentum and temperature equations respectively and f u and f s s t the surface and deep ocean forcing terms of the ocean surface layer the interactions between the two variables ssh and sst are facilitated when they have the same resolution but when they have different resolutions low ssh high sst resolution as in actual satellite observations the relationship linking these two variables is much more complicated and can be biased due to the fact that in the fourier space interactions preferentially occur between close fourier components merilees and warn 1975 maltrud and vallis 1993 the central idea is the locality of the interactions as proposed by kolmogorov in describing the turbulence of incompressible fluids only vortices of the same order of magnitude strongly interact with each other zakharov et al 2012 besides at low resolution eq 1 is approximated by geostrophy terms 2 and 3 while at high resolution term 1 which has a non time dependent component must be added see appendix d for an explanation these considerations have led us to train a cnn architecture consisting of several connected modules such that every module downscales the ssh field to a resolution close to that of the sst field we have finally obtained a ssh field and associated velocities at the high sst field resolution this modular cnn constitutes the framework of the resac model built to retrieve high resolution ocean currents from low resolution ssh and high resolution sst this procedure aims at learning the underlying physical laws embedded in the observations here the natl60 outputs this study can be linked to the physics informed field although our approach has many differences we seek an accurate approximation of the various differential equations used to determine the components u and v of the current u eqs 1 and 2 the ml algorithm models the combination of equations embedded in the natl60 images of ssh sst u and v these images take into account various phenomena such as momentum and sst advection interaction with underlying layers see appendix d whose relationships are guided by the introduction of the existing coherence between the different resolutions as shown in fig 5 in recent physics informed studies dedicated numerical schemes of simulations are approximated using a neural approach huang et al 2021 sonnewald and lguensat 2021 3 2 the resac architecture since the natl60 ocean model represents the links between ssh and sst at every resolution with a good accuracy we trained the resac cnn on the natl 12 dataset in the training phase the resac strategy was to conduct the downscaling gradually stage by stage to avoid spurious interactions between the fourier components of the ssh and sst fields that are far from each other as mentioned above resac therefore consists of 3 stages each being composed of a specific cnn providing intermediate or final estimates of ssh u and v the flow chart of the resac training is presented in fig 5 the progression through the different stages requires estimating the intermediate resolutions until the final downscaling outputs are obtained this is done by using two intermediate cost functions j1 and j2 in fig 5 that control the intermediate ssh resolutions ssh r27 and ssh r09 the final cost functions j3u and j3v in fig 5 control the u and v final outputs u r09 and v r09 fig 5 the minimization considers the sum j of the four intermediate cost functions j1 j2 j3u j3v each one minimizes the sum of the mean square error mse between the value estimated by the cnn y i e s t and the natl60 y i n a t l value the sum being computed over all the available pixels i of all images we denote cnn r81tor27 resp cnn r27tor09 the architecture that allows us to model the downscale procedure from ssh r81 to ssh r27 using sst r027 resp ssh r27 to ssh r09 using sst r09 and we denote cnn uv r09 the cnn architecture that models ssh r09 and sst r09 to u r09 and v r09 fig 5 the first two stages model the flow of information provided by the prognostic variables the third stage models the physics that links ssh and sst with velocities in the natl60 ocean model each cnn takes into account the existing correlations between a given resolution of ssh and the next ssh sst one stage 1 and stage 2 and between the ssh and the velocities at the same resolution the learning process is achieved in the following manner once the architecture of the cnns has been figured out the resac learning phase proceeds from scratch by randomly initializing the weights of the cnns the weights of these cnns are estimated at once by simultaneously minimizing j which controls the passage from one stage to the following one the exact specifications of the cnns such as the number of convolutional layers and feature maps or the size of the filters at each stage are given in appendix b they were heuristically optimized at each stage the cnn specifications being separately determined using their dedicated inputs outputs cost functions and learning set in fig 5 up3x3 denotes the up sampling procedure which increases the size of the image by a factor of 3 3 in order to get the higher resolution corresponding to the next stage in resac we performed a bilinear up sampling method with a 3 3 up sampling ratio we note that the cnn r81tor27 architecture first stage in fig 5 is constituted of a larger number of weights than the other two respectively the cnn r27tor09 associated with the second stage and the cnn uv r09 associated with the third stage see fig 5 this can be explained as follows the ssh and sst natl60 fields are weakly correlated this correlation increases at intermediate resolution when these two fields have interacted through the cnn r81tor27 fig 5 which therefore must perform a more difficult task than the other two cnns and consequently requires a larger number of weights the full learning process learning validation cost functions learning steps hyper parameter assessments is detailed in appendix b training validation and test sets are completely decorrelated see section 2 the number of convolution layers the size and number of filters the activation and loss functions were chosen based on previous experiments the learning rate schedulers and the number of epochs were empirically tuned on the validation set see appendix b at the end of the training when all the weights of resac have been settled resac can be used to retrieve the high resolution ocean currents in the retrieval phase only the variables present in the green frames in fig 5 ssh r81 sst r27 sst r09 are inputted in resac so as to estimate the current u r09 and v r09 resac allows us to downscale ssh r81 to ssh r09 u r09 and v r09 gradually the process being controlled by intermediate and high resolution sst data sst r27 and sst r09 fig 6 shows the inputs of the resac model and its final outputs in the operational phase in section 4 we studied different resac models with different inputs and architectures for each resac model we trained 20 networks with different weights for initialization using an adapted learning rate planner see appendix b the final prediction values are the average values of the predictions of the 20 networks this procedure allowed us to increase the performance of the final cnn we also tried to train a simpler architecture linking the inputs ssh r81 sst r27 sst r09 to ssh r09 directly but we were not able to perform the training phase correctly the minimization was still stuck in a flat local minimum this proves the need to process the downscaling gradually by introducing the ssh sst relationships at different resolutions 4 resac performances 4 1 qualitative comparison of the different models to show the effectiveness of the resac method and the contribution of sst we trained two separate resac models called resac ssh sst and resac ssh both of which provide downscaled ocean parameters the inputs to resac ssh sst are ssh at resolution r81 and sst observations at resolutions r27 and r09 see fig 5 while the inputs to resac ssh are ssh r81 data only appendix b fig b 2 the architectures of these two resac models are very similar but resac ssh is somewhat different due to the removal of the sst since the sst inputs green in fig 5 were not used resac ssh has a smaller number of weights 400 301 than resac ssh sst 402 797 weights in addition we compared our results to those obtained by a simpler approach a well known unsupervised image processing algorithm https opg optica org oe fulltext cfm uri oe 19 27 26161 id 225713 this algorithm hereafter bi cubic geostro consists of performing ssh interpolations with two dimensional bicubic functions we performed the interpolation of ssh r81 data at r09 resolution with the bicubic algorithm and then estimated the r09 current components u v using the geostrophic approximation the sst is not used the training and testing of both resac models were performed in the same manner using the same data and conditions as presented in section 2 training was performed on the 366 days of the natl 12 dataset validation on the natl 08v dataset december 2008 and testing on the natl 08t dataset march june september 2008 all the statistics presented below have been computed on natl 08t first we show the downscaling of ssh and velocity values given by resac ssh sst resac ssh and bi cubic geostro fig 7 second row for march 15 2008 we also compare these sshs with the ssh values from natl 08t by taking their difference third row the fourth row shows the energy obtained from the three models at first glance resac ssh sst reproduces the natl 08t values better than the other two models we note that the area situated north of 35 n which is much more energetic than the south one as it is populated with a large number of energetic mesoscale eddies is better reproduced than the south one clearly resac ssh sst outperforms resac ssh and the worst results are provided by bi cubic geostro proving the interest of using physical information such as the sst during the downscaling procedure we note that the geographic positions of the major oceanic structures are correctly retrieved by resac ssh sst as well as the velocity amplitudes and directions 4 2 quantitative comparison of the different models we compared the outputs of resac ssh sst resac ssh and bi cubic geostro to the natl 08t dataset using several statistical estimators first we estimated the consistency of the variable reconstruction by computing a daily rmse for each variable and for each test image according to eq 3 3 r m s e t e s t 1 n i 1 n y i n a t l y i e s t 2 where n is the number of pixels of a r09 image n 144 153 y i e s t is the value estimated by resac at pixel i y i n a t l is the natl60 value at pixel i we averaged the daily rmse of the test images we therefore obtained a mean rmse denoted mrmse in the following table 3 shows the mrmse computed between the natl 08t variables and the resac ssh sst first row in table 3 resac ssh second row and bi cubic geostro third row outputs for ssh u and v at the r09 resolution it clearly appears that resac ssh sst performs better than resac ssh the mrmse between the natl 08t and the resac ssh sst ssh is much lower than the natl 08t ssh distribution whose most frequent values are of the order of 0 50 m fig 4 which suggests that resac ssh sst is efficient in downscaling the ssh and the associated ocean currents the use of stage 3 in the resac training implies knowledge of the high resolution ocean current field which can only be given by a numerical model we also calculated ocean currents by applying geostrophy to ssh r09 given by the two resac models these outputs are labeled resac ssh sst geostro and resac ssh geostro respectively in fig 8 we present the time series of daily rmses given by resac ssh sst resac ssh and bi cubic geostro for ssh top panel and the daily rmses given by these statistical models for u v middle and bottom panels the best results for u v are given by resac ssh sst followed by resac ssh sst geostro resac ssh resac ssh geostro and the worst by bi cubic geostro the information embedded in the sst advection equation eq 2 improves the r09 ocean currents given by ssh alone analysis of these rmses allows us to see the impact of the sst on the retrieval of ocean currents the fact that the ocean current rmses of resac ssh and resac ssh geostro are nearly similar implies that the cnn uv r09 fig b 2 included in resac ssh is a function close to the geostrophy that would be difficult to improve without the use of additional parameters the cnn uv r09 fig 5 included in resac ssh sst improves the retrieval of ocean currents over geostrophy and thus mitigates the deficiencies of the latter by introducing sst as an additional variable let us now try to understand how sst acts in resac ssh sst to obtain the best ssh and ocean currents as shown in fig 8 schematically the performance improvement has two main sources first sst helps to retrieve a better ssh which leads to a better geostrophic approximation as we can see by comparing the ocean current rmses of resac ssh geostro and resac ssh sst geostro 0 02 ms 1 improvement on the mrmse table 3 second sst helps the cnn uv r09 of resac ssh sst fig 5 to retrieve currents with additional performance with respect to geostrophy in the following we focus our analysis on the performances of the two dl models namely resac ssh sst and resac ssh compared to those of bi cubic geostro we calculated the complex correlation see appendix c for a definition between the velocities given by resac and by bi cubic geostro and with those of natl 08t table 4 in the north and south zones we note that the correlations given by resac ssh sst are higher than those given by resac ssh and bi cubic geostro the difference are of 0 11 and 0 21 respectively values which are highly significant given the high degree of freedom of these correlations 144 153 22 032 which corresponds to the number of pixels by image the angle between the two vectors natl 08t and algorithm estimated has no systematic bias the mean angle between the two vector fields being close to zero appendix c once again resac ssh sst performs better in retrieving the velocities than resac ssh and bi cubic we note that the standard deviation std of these correlations are small which shows the good functioning of the retrieval algorithms 4 3 case of noisy ssh data ssh measurements made by altimeters are often characterized by a high noise signal ratio due to instrumental noise and interpolation of altimeter observations this interpolation introduces noise since the altimeter observations are made under the satellite track and gridded by combining several tracks over a fairly long time interval depending on the satellite repeat period 10 days for topex poseidon ballarotta et al 2019 dufau et al 2016 in the following we simulated those noises by adding a gaussian noise to the ssh r81 of natl 08t for each pixel of each test day we added a gaussian random noise n 0 σ 2 to the ssh r81 images the ssh signal is now sshn r81 ssh r81 n 0 σ 2 we tested two different noises the standard deviation of the first one is σ 1 0 05 m that of the second is σ 2 0 10 m these two values are quite important with respect to the natl12 ssh distribution whose most frequent values are of the order of 0 50 m fig 4 first row fig 9 shows the ssh at the r81 resolution without noise and for the two noise values on 09 01 2013 we note the strong alteration of the ssh r81 patterns for the highest noise level the mean daily rmse mrmse of the two resac models and bi cubic geostro ssh and u v with respect to natl08t values are presented in table 5 for the full area the mrmses increase with the noise level and are smaller when sst is considered showing the important role of the sst in velocity retrieval as expected the higher the noise the larger the mrmse the complex correlation cc of the velocities of both resac and bi cubic geostro models with natl08t velocities is presented in table 6 for the whole area and the north and south zones resac ssh and bi cubic geostro show much poorer performance than resac ssh sst the contribution of the sst is essential as can be seen on the velocity correlations which are always better for resac ssh sst with a difference of 0 15 to 0 20 compared to the other models we note that the downscaling procedure keeps the velocity directions with good accuracy we again find that the performances are better in the northern region than in the southern region this is due to the fact that in the northern region ocean currents are stronger and associated with large stable ocean structures corresponding to the gulf stream while the southern region is more turbulent with smaller currents associated with small ocean structures 5 discussion the performance of resac ssh sst is very satisfactory and superior to that of resac ssh and bicubic geostro as shown by several statistics presented in section 4 tables 3 and 4 the daily rmses displayed in fig 8 allow us to analyze the performances of the different models the good score of resac ssh sst compared to resac ssh and bi cubic geostro is due to sst acting in two different ways on the structure of the algorithm first the sst input on cnn r81 to r27 and cnn r27 to r09 fig 5 steps 1 2 of the flow diagram improves ssh retrieval at the r09 resolution as can be seen by comparing the daily ssh rmses given by the three downscaling models top panel of fig 8 second the sst input on cnn uv r09 fig 5 step 3 provides advection information which improves the r09 ocean current retrieval as can be seen from the daily rmses of the u and v current components fig 8 middle and bottom panels and table 4 moreover resac gives satisfactory performance when ssh is noisy to this end we conducted two experiments with two different noise levels on the ssh one with a standard deviation std of 5 cm the second with a std of 10 cm these values are high compared to the dominant value of the ssh probability density function pdf which is of 50 cm according to fig 4 in all cases without and with noise the complex correlations between resac current vectors and natl 08t current vectors reported in tables 5 and 6 show that resac ssh sst is the best estimator of ocean currents at r09 resolution all these analyses show that sst plays a critical role in the accuracy of ocean current retrieval especially when ssh measurements are noisy the sst which is not considered in resac ssh and bi cubic geostro adds important information on ocean currents in resac ssh sst allowing to improve the resolution of the retrieved ssh field as well as that of the associated velocities the velocity information contained in sst is linked to the structure of the sst advection equation used in the natl60 simulation eq 2 which associates sst with velocity the rationale is that sst which is advected by ocean currents is a stream function for slowly varying ocean motions period greater than a few days which therefore implies that the fluid velocity is almost perpendicular to the sst gradient see appendix d in this study eq 2 was implicitly modeled by resac so that terms 2 3 4 were statistically accounted for from a theoretical point of view we can state that resac ssh sst has approximated the physical laws included in the data fields represented by the pdes constituting the natl60 model see appendix d the performance of resac ssh sst is satisfactory in the northern area but deteriorates in the southern area due to the low signal to noise ratio in the latter region tables 3 and 4 for the natl 08t test set tables 5 and 6 for the noisy observations furthermore comparison between the retrieval of u v by resac ssh sst and resac ssh sst geostro fig 8 shows that geostrophy computed from ssh only is a quite weak estimator of ocean currents that can be improved with the help of additional information sst in the present case and not by algorithmic improvements as shown by the similar u and v rmses provided by resac ssh and bicubic geostro fig 8 second and third rows a major point of resac is to decompose the training phase into three stages as outlined in fig 5 to facilitate the exchanges of physical properties of the fluid associated with ssh and sst which preferentially occur in the spectral domain at wave numbers which are close owing to the locality of the interactions in turbulence merilees and warn 1975 maltrud and vallis 1993 zakharov et al 2012 the training phase uses an intermediate resolution of the sst observations sst r27 which in the present case is simply obtained by filtering sst r09 and two intermediate resolutions of the ssh observations ssh r27 and ssh r09 allowing to control the intermediate estimations of resac ssh r27 and ssh r09 by using specific cost functions see fig 5 for the flow diagram in the operational phase the inputs of resac ssh sst are the ssh observations ssh r81 at low resolution and the sst observations at intermediate sst r27 and mesoscale sst r09 resolutions the outputs are the ocean currents at mesoscale resolution u r09 v r09 together with the ssh at the same resolution ssh r09 this decomposition in stages is an essential ingredient of the resac method the performance of the algorithms without the stage decomposition was found to be much lower not shown resac training was conducted on very high resolution ocean model output which allows for high resolution velocities that are not observable by in situ measurements in a future study we propose to train a resac sat dedicated to processing satellite observations on ssh altimeter tracks and sst fields simulated from an operational ocean model such as mercator models https www mercator ocean eu en as they assimilate several different observed variables these models are close to the real ocean it is therefore expected that the observation operator according to data assimilation jargon permitting to associate satellite observations with ocean model data will be close to identity as found in similar studies such as the scatterometer wind retrieval richaume et al 2000 these authors calibrated a ers1 wind retrieval neural network with era interim wind fields in the construction of resac sat it would be necessary to take into account the complex spatio temporal sampling along the ssh observation tracks associated with the satellite repeat period recent studies have proposed new efficient procedures to improve the consideration of ssh sampling they are based on 4d var febvre et al 2022 or on cnn nardelli et al 2022 the architecture of resac will be adapted according to the sampling characteristics spatial and temporal of the satellite ssh sensors and the chosen procedure a complementary way to improve the efficiency of such an operational algorithm would be to consider the physical properties of the links between ssh and sst we could add new inputs to resac such as air sea heat exchange term 4 in eq 2 which improves the impact of sst on ssh as has been shown rio and santoleri 2018 nardelli et al 2022 for large scale ocean motions of the order of atmospheric structures several hundred kilometers and should therefore improve ssh at r81 resolution small scale 10 kilometers ocean motions are more difficult to consider because they are mainly associated with heat exchanges within the deep ocean which are related to mixed layer dynamics and horizontal eddy fluxes investigating the impact of these processes on the resolution of the ssh and associated ocean currents would deserve a specific study besides ssh resolution in both time and space will be significantly improved by the launch of the surface water and ocean topography swot mission which is a joint nasa cnes csa asc uk space agency program which will facilitate the inclusion of ssh data 6 conclusion we have built an efficient machine learning algorithm the cnn based resac method to increase the resolution of ocean currents provided by low resolution ssh 120 122 km ssh r81 up to mesoscale resolution 13 14 km ssh r09 by combining them with high resolution sst we consider an oversimplified case for which both ssh and sst measurements are gridded daily observations the essence of resac is to combine coarse ssh observations with high resolution sst by introducing intermediate sst resolutions in the training and retrieval phases to increase the accuracy of resac we have constructed a cnn ensemble whose prediction values are the average values of the outputs of 20 cnns with the same architecture and trained on the same datasets but with different initial values more generally resac can used to merge and downscale all data fields that are observed simultaneously the resac architecture has shown the interest of decomposing the learning phase into several stages to facilitate the fusion of these data resac accuracy could be improved by using a larger dataset several years instead of one for training moreover several additional studies could be undertaken such as training two different resac models one over the northern area and the other over the southern area with the expectation that resac s performance will improve especially in the southern area where ocean currents are weak and mostly turbulent furthermore as designed resac should be able to operate at least in theory in equatorial regions where geostrophy is not applicable the present work can be considered as a feasibility study leading to the design of a future operational dl method dedicated to the downscaling of ssh fields in the real world the problem is much more complicated it is necessary to take into account the effect of ssh sampling along satellite tracks and successive interpolations of altimeter data during the repeat period errors in the final satellite products are not due to simple smoothing but to the complex spatiotemporal interpolation ballarotta et al 2019 this challenging problem implies further developments to obtain an efficient operational dl algorithm which is beyond the scope of this paper the present study acts as a proof of concept regarding the merging of datasets of different resolutions to downscale low resolution data fields up to high resolution ones it explicitly relies on the ability of cnns to summarize the geophysical knowledge linking sst ssh and velocity fields u v at every pixel resolution from 120 km2 to 13 km2 moreover a challenging study would be to estimate ocean currents at the r01 resolution 1 5 1 5 km this scale corresponds to ocean filaments for which the geostrophic approximation is no longer valid which makes the velocity downscaling at high resolution more complex this would imply adding a new stage in the resac algorithm owing to the large amplitude of the downscaling range and consequently increasing the size of the cnns and the computational cost of the learning phase this would require access to very powerful gpus these results would have been improved if a longer time series for the training had been at our disposal resac must be learned on high resolution ocean modes outputs since there is no high resolution ocean observations the resac outputs could be somewhat different from actual ocean variables but not too much because ocean models are now very efficient as they assimilate a large number of observations this constraint is inherent to any high resolution dl downscaling operator of ocean variables as mentioned in the introduction deep learning dl is a very productive research area today its use for solving complex problems in the environmental sciences leading to scientific discoveries is rapidly increasing geophysical sciences and climate studies must deal with massive amounts of multidimensional data dl methodologies are well suited to solve the formidable scientific challenge of multi satellite observations and climate model outputs high performance computing hpc is increasingly being used to meet the exponentially growing demand for dl data processing multiple gpus and high speed interconnect networks are available to support dl on hpc systems han et al 2019 all of this research done in the field of artificial intelligence is now available to the environmental research community and should enable rapid development of geophysical knowledge this development of dl should give us confidence in the applications of resac indeed resac learned on simulated data contains a summary of the physics that relates ssh and sst to ocean currents given its generic nature we expect the resac methodology to be applicable to a wide variety of downscaling problems credit authorship contribution statement sylvie thiria designed the research wrote the manuscript charles sorror developed the resac models performed the simulations and validated the results theo archambault developed the resac models performed the simulations and validated the results anastase charantonis participated in the cnns development manuscript writing dominique bereziat participated in the cnns development manuscript writing carlos mejia data management and to the document preparation jean marc molines provided the natl60 outputs ssh sst and ocean velocities michel crépon discussed the results wrote the manuscript and edited it declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments natl60 simulations were performed on cines occigen supercomputer in the frame of the 2014 2015 great challenge organized by genci we also thank dr manucharyan and the anonymous reviewers who contributed to improve our document financial support the present research was supported by sorbonne university ipsl sama and scai appendix a let us denote e i the kinetic energy at a pixel i a 1 e i 1 2 u i 2 v i 2 the global kinetic energy of an image for a day d is given by e d i n e i let us denote s i the local enstrophy at a pixel i a 2 s i 1 2 x v i y u i 2 the global enstrophy of an image for a day d is given by s d i n s i energy characterizes the intensity of the velocity field whilst enstrophy its shear intensity global energy and enstrophy are conservative quantities in the absence of forcing and dissipation for two dimensional flows maltrud and vallis 1993 merilees and warn 1975 appendix b resacnet architecture we used the python 3 9 keras 2 4 0 software to implement the resac model resac was trained on an nvidia gpu with cuda 11 0 under a linux system the resac architecture consists of 3 cnns fig 5 cnn r81tor27 has 7 convolutional layers each layer consisting of 36 feature maps with 6 6 filters using the relu activation function each layer is followed by a batch normalization layer the last layer connects the 36 characteristic maps to a 1 1 mask to fit the output image using a sigmoid activation function cnn r27tor09 has 3 layers consisting of 24 characteristic maps with 5 5 masks using the relu activation function each layer is followed by a batch normalization layer the last layer connects the 24 characteristic maps to a 1 1 mask to fit the output image using a sigmoid activation function cnn uv r09 has the same structure as cnn r27tor09 but repeated 3 times the first one takes the sst r09 and the output of cnn r27tor09 to make one output uv r09 the second and third one get that output and are dedicated to finding respectively the u and v components of the current resac ssh has 400 301 weights and resac ssh sst 402 797 weights from a layer to the next one the size of the input is preserved using a same padding parameter the resac architecture weights are initialized by the the normal method keras documentation the cost function j is the mse of the error for all the cost functions b 1 j 1 n i 1 n y i n a t l y i e s t 2 the learning validation and test sets are completely decorrelated see section 2 the number of convolution layers the size and number of filters the activation and loss functions were chosen based on previous experiments where the learning rate schedulers and the number of epochs were empirically tuned on the validation set for each resac model we trained 20 networks with different weights for the initialization using an adapted learning rate scheduler for resac ssh sst the learning rate starts at 2e 3 and stays constant through 20 epochs is then multiplied by exp 0 02 at each epoch between epoch 20 and 60 and by exp 0 05 between epoch 60 and 100 for resac ssh the learning rate starts at 5e 3 and stays constant through 20 epochs is then multiplied by exp 0 02 at each epoch between epoch 20 and 40 and by exp 0 05 between epoch 40 and 100 the final prediction is the average of the predictions of the 20 networks fig b 1 presents the evolutions of the cost functions during the training phase for the learning and validation datasets the x axis stands for time evolution y axis represents the value of the cost function the weights are those determined by cross validation fig b 2 presents the flow diagram for the training of resac ssh appendix c complex correlation comparing two vector fields is a quite complicated task we can compare the vector components with classical statistical tools std correlation but their interpretation in the physical space is not easy one of the difficulties is to take relationships existing between the vector components into account in the present paper we propose a method leading to a straightforward geometrical interpretation for two dimensional vector fields the trick is to use the isomorphism existing between two dimensional vectors and complex numbers let us consider two two dimensional vector fields u v and u v that we aim at comparing the associated complex numbers u and u are written as u u iv ρ e i α and u u iv ρ e i α β where i is the imaginary symbol ρ and ρ are positive numbers which represent the modules of the vectors α and α β define the angles of the vectors with the real axis β representing the angle between the two vectors let us define the complex correlation cc between u and u c 1 c c 1 n 1 n u j u j 1 n 1 n u j u j 1 n 1 n u j u j where u u i v and u u i v are the complex conjugates associated with u and u respectively the sums are taken over the n vectors constituting the fields cc can also be written as c 2 c c 1 n 1 n ρ j ρ j e i β j 1 n 1 n ρ j ρ j 1 n 1 n ρ j ρ j c e i θ this polar representation allows us geometrical interpretations of cc as shown in the following first let us assume that there is a systematic rotation β between the two vector fields cc writes c 3 c e i β 1 n ρ j ρ j 1 n ρ j ρ j 1 n ρ j ρ j c e i β where c is the correlation between the vector modules and β the systematic angle between the two vector fields this case is often encountered in oceanography constant angle between the wind and the current in the ekman drift rotation of the current with depth in the ekman spiral second let us interpret the angle of the complex correlation when the two fields are quasi co linear which is the case when we compared modeled and observed currents eq c 2 can be written as c 4 c c 1 n ρ j ρ j c o s β j i s i n β j 1 n ρ j ρ j 1 n ρ j ρ j eq c 4 is somewhat difficult to interpret let us consider the following simple case we now assume that each vector field has a constant modulus ρ and ρ are constant eq c 4 writes c 5 c c ρ ρ 1 n c o s β j i s i n β j 1 n ρ j ρ j 1 n ρ j ρ j 1 n 1 n c o s β j i s i n β j if the angles β j are small β 10 and their mean close to zero which is a straightforward hypothesis eq c 5 writes c 6 c c 1 n 1 n c o s β j i s i n β j 1 1 n 1 n i s i n β j 1 i ɛ where ɛ is a small number due to the fact that the mean of β j is close to zero cc can be written as c c e i θ where θ is a small angle this simplified example can explain the fact that the angles θ of the complex correlations we have computed in the present study are always small the cc angle gives an estimate of the mean angle between the two vector fields which is near zero if the angle distribution is gaussian complex correlation has been used in several studies implying two dimensional vector analysis in meteorology and in oceanography wang and moers 1977 hardy and walton 1978 legler 1983 horel 1984 eq c 6 shows that cc gives an estimate of the mean of the angles β j between the two vector fields which is close to zero and does not provide useful information on the structure of the two fields a more informative parameter is the standard deviation of that angle which could be obtained by doing the following operation u j u j ρ j ρ j e i β j which provides the β j and allows the computation of the standard deviation of the β j in fact the correlation between two dimension vector fields is a matrix cm of the form c m c u u c u v c v u c v v where c u v is the correlation between the component u of vector u and v of vector u and so on the matrix formulation of the correlation existing between two vector fields contains more information than cc but cc is easier to interpret in the physical space cc is mainly dedicated to compare vector fields which are related by a scaling and a systematic rotation in the present study we chose the simplicity of the interpretation and dealt with complex correlation appendix d the resac model does not learn the time evolution of eq 1 but it better approximates eq 1 than geostrophy balance between term 2 and 3 by taking into account a statistical modeling of term 4 and of the spatial contribution of term 1 which is of the form u u resac therefore can be considered as a statistical model of the non time dependent pde u u f u g ssh d u coupled with the slowly varying temperature equation eq 2 which is of the form sst u d s s t the operators d u and d s s t model turbulence 
23791,numerical studies of ice flow have consistently identified the grounding zone of outlet glaciers and ice streams the region where ice starts to float as crucial for predicting the rate of grounded ice loss to the ocean owing to the extreme environments and difficulty of access to ocean cavities beneath ice shelves field observations are rare estimates of melt rates derived from satellites are also difficult to make near grounding zones with confidence therefore numerical ocean models are important tools to investigate these critical and remote regions the relative inflexibility of structured grid models means however that they can struggle to resolve these processes in irregular cavity geometries near grounding zones to help solve this issue we present a new nonhydrostatic unstructured mesh model for flow under ice shelves built using the firedrake finite element framework we demonstrate our ability to simulate full ice shelf cavity domains using the community standard isomip ocean0 test case and compare our results against those obtained with the popular mitgcm model good agreement is found between the two models despite their use of different discretisation schemes and the sensitivity of the melt rate parameterisation to grid resolution verification tests based on the method of manufactured solutions mms show that the new model discretisation is sound and second order accurate a main driver behind using firedrake is the availability of an automatically generated adjoint model our first adjoint calculations of sensitivities of melt rate with respect to different inputs in an idealised grounding zone domain are promising and point to the ability to address a number of important questions on ocean influence on ice shelf vulnerability in the future keywords fem ice shelf cavities firedrake adjoint verification mms data availability no data was used for the research described in the article 1 introduction recent observations have shown that outlet glaciers and ice streams in west antarctica are retreating at an alarming rate mouginot et al 2014 scambos et al 2017 in total there is enough ice in west antarctica to raise sea level by approximately 3 5 m fretwell et al 2013 evidently this has serious implications for communities that live in low lying coastal regions there are still major uncertainties however in predictions of how much ice could be lost and importantly on what time scales this may occur increased basal melting beneath ice shelves due to increased ocean heat is thought to be the main cause for ice loss in west antarctica rignot et al 2013 wherever ice shelves are topographically confined they provide buttressing forces to the grounded glaciers shepherd et al 2018 when ice shelves thin a decrease in buttressing can lead to an increase in ice flow speeds and hence the rate at which grounded ice is lost increases predicting sea level rise is a pressing issue but there are also questions about how changes in melt water flux to the ocean can affect global ocean dynamics dinniman et al 2016 improving understanding of ocean processes beneath ice shelves and their impact on ice shelf vulnerability is therefore an important problem in glaciology and climate science numerical models of ocean flow beneath ice shelves play a key role in our understanding of ocean cavities since direct observations are so limited the model needs to resolve the interaction between salinity temperature and the flow along with the thermodynamics at the complex sloping ice ocean interface that characterise ice shelf cavity environments melting ice injects buoyant fresh water into the domain which in turn drives overturning ocean circulation within the cavity the freezing point of water decreases with depth so in some cases ice that melts at depth in the cavity can refreeze closer to the ocean surface thus transporting ice up the underside of the ice shelf this is known as the ice pump mechanism lewis and perkin 1986 jenkins and bombosch 1995 in coastal regions where changes in coastlines and bathymetry can have important effects on dynamics horizontally unstructured grid models have proven to be extremely useful tools kärnä et al 2018 in an ice shelf cavity setting this is doubly important since the base of the ice shelf can vary as much as the seafloor with the formation of basal crevasses and channels that evolve with the ocean and indeed ice flow the lack of flexibility in traditional structured grid ocean models in resolving these features means that there is still large uncertainty in flow dynamics beneath ice shelves dinniman et al 2016 this is compounded by the fact that results using the commonly employed three equation melt parameterisation depend significantly on vertical resolution and implicitly on the model s choice of vertical discretisation gwyther et al 2020 an issue that will be explored further in section 3 2 satellite measurements indicate that spatial melt rate patterns near grounding zones the region separating the grounded ice sheet from the floating ice shelf are highly variable milillo et al 2019 this implies that there must be a complex combination of processes in the grounding zone that need to be resolved to accurately model melt rates due to the limitations on grid resolution imposed by structured grids used by traditional ocean models it has not been possible to investigate in detail melting at grounding zones ensemble simulations from glacial flow models have shown that the largest uncertainties in projected ice loss come from estimating melt rates at the grounding zone arthern and williams 2017 similarly goldberg et al 2019 used an ice flow model with an associated adjoint model to investigate how sensitive the volume of ice loss was to spatially varying melt for dotson and crosson ice shelves again they found that ice loss was most sensitive to melting at the grounding zone an extension of this study to the wider amundsen sea embayment confirmed the high sensitivity of ice loss to melting at the grounding zone morlighem et al 2021 therefore accurately simulating ocean dynamics near the grounding zone seems to be one of the most important tasks required to help reduce uncertainty in ice loss estimates and consequently reduce uncertainty in sea level projections the finite element method is particularly well suited to solving problems in complicated domains elman et al 2014 the power of the finite element method comes from being able to define the solution using piecewise functions which can be defined on completely arbitrary meshes this is ideal for modelling flow in complicated domains such as ice shelf cavities a fully unstructured finite element ocean model fluidity piggott et al 2008 was previously adapted to enable simulation of ice shelf cavities kimura et al 2013 due to its ability to run on fully unstructured grids fluidity was able to resolve all the key features of an ice shelf cavity pinching grounding lines sloping ice base and the steep vertical ice front kimura et al 2013 as well as investigate flow within basal crevasses jordan et al 2014 the model presented here is similarly motivated by the applicability of finite elements on unstructured meshes and the need to retain a full representation of physics valid at order one aspect ratios for buoyancy driven flows in domains with complicated geometries such as ice shelf cavities a significant departure from previous work is use here of the firedrake toolkit to implement the underlying discretisation of the finite element method rathgeber et al 2016 the philosophy behind firedrake has several advantages by separating the underlying software implementation from the physics of the system it enables a firedrake user to quickly develop and test new discretisation methods on the problem in question the syntax of the unified form language ufl firedrake employs is designed to closely mimic the mathematical description of the weak form prescribing the finite element discretisation alnæs et al 2014 this combined with the readability of the python programming language helps to improve the longevity of the code by reducing the learning curve require for new users before they can start using and implementing features in the model icepack a glacier flow modelling package built on top of firedrake is a good example of this shapero et al 2021 the firedrake framework automatically translates this high level mathematical description of the numerics into highly optimised low level c code which leads to an efficient implementation of the finite element method in addition firedrake uses the portable extensible toolkit for scientific computation petsc a state of the art library to efficiently solve the systems that arise from the finite element discretisation balay et al 1997 there are already ocean models that use horizontally unstructured grids to investigate ice shelf cavities such as fesom timmermann et al 2012 fvcom zhou and hattermann 2020 and mpas o ringler et al 2013 gwyther et al 2020 these models typically retain a distinct difference in how they deal with horizontal and vertical motion in the model in part this means making the hydrostatic approximation provided the horizontal length scales are much larger than the vertical the vertical acceleration terms can be neglected which reduces the computational cost this approximation is evidently true for the open ocean when typically the meshes used for simulations have horizontally stretched anisotropic grids to investigate interesting basal features under ice shelves though such as crevasses jordan et al 2014 or in the proximity of topographically complex grounding zone regions one may want to have more flexibility by using meshes where the aspect ratio of the grid becomes close to order one i e the horizontal grid sizes are comparable to the vertical grid cell sizes in this case the hydrostatic approximation may no longer be valid therefore we include the nonhydrostatic vertical acceleration terms as part of the solve moreover by treating the horizontal and vertical equations on the same footing our model has been designed so that it can easily incorporate isotropic turbulence closure schemes from the cfd literature similar to work carried out by yeager 2018 whilst at the same time able to transition to more traditional gfd turbulence closures in high aspect ratio domains where vertical and horizontal mixing is parameterised separately probably the main advantage of using a framework such as firedrake or similar projects such as fenics alnæs et al 2015 is the fact that it readily facilitates the use of an automatically generated adjoint model thanks to the work of the dolfin adjoint project mitusch et al 2019 adjoint modelling represents an efficient means to calculate gradients of model output functionals with respect to model inputs errico 1997 by solving the adjoint system the gradient of an output functional can be calculated with respect to any number of input parameters independent of the number of these input parameters adjoint modelling is often the only tractable method for calculating gradient information for numerical models that rely on grid based discretisations like ocean models because the number of input parameters scales with the grid resolution errico 1997 early implementations of adjoint models were developed by hand separately from forward models kalnay 2003 as the complexity of the forward code develops this approach becomes time consuming and error prone a more sophisticated method uses what is sometimes referred to as automatic differentiation ad of the forward code by using the chain rule on each line of forward code repeatedly to generate an adjoint model errico 1997 usually careful intervention by the user is required during development and maintenance especially with regards to ensuring the computational efficiency of the adjoint model hence it is common to refer to this process as algorithmic not automatic differentiation the approach implemented by the dolfin adjoint project makes use of the high level symbolic representation of the discrete mathematical problem written in ufl mitusch et al 2019 since the mathematical problem is separate from its software implementation provided the mathematical form of the equations is differentiable then the process is automatic the adjoint model is consistent with the forward discretisation and automatically inherits the solver strategies and the parallelisability of the forward model mitusch et al 2019 sensitivities of a functional with respect to different inputs are very useful in their own right errico 1997 in a time dependent forward model the equivalent adjoint model propagates sensitivities backwards through time from the final time to the initial time this complimentary information can be very useful when trying to understand modelled processes that may not be obvious from considering the forward model alone gradient information is also very useful for optimisation problems in particular parameter estimation and data assimilation in many ocean modelling studies there are many parameters that are unknown but can be constrained by observations gradient information can be used to update these parameters in an efficient manner by minimising a functional that measures the error between observed and modelled values there have been a limited number of studies of an ocean adjoint in an ice shelf cavity context the first used the mitgcm ocean model to simulate the pine island ice shelf cavity heimbach and losch 2012 goldberg et al 2020 extended mitgcm s adjoint model such that it was able to calculate the sensitivity of melt rate to bathymetry notably this was using the open source ad tool openad utke et al 2008 most recently the ecco2 assimilation framework which also relies on mitgcm s adjoint capability was applied to the amundsen and bellingshausen seas and ice shelf cavities nakayama et al 2021b since all the previous adjoint modelling work for ice shelf ocean cavities have used the mitgcm framework they inherit the relative inflexibility of mitgcm s structured grid although mitgcm s resolution can vary in space d x can vary horizontally d z is limited to varying with depth this has limited practical application for ice shelf cavities since the ocean cavity thickness varies significantly from the grounding zone to the open ocean we see our model which can combine flexible unstructured meshes with the adjoint capability as a tool with significant potential for investigating ocean conditions in the complex domains that are ice shelf cavities the outline for the rest of this paper is as follows section 2 covers the model discretisation choices in section 3 1 the accuracy of the second order discretisation is verified using a method of manufactured solution based test which includes melting we demonstrate the resulting model s capability to run simulations for the 3d isomip test case in section 3 2 and compare results with the mitgcm ocean model this includes an investigation into the sensitivity of the melt parameterisation to vertical mixing and resolution choice finally we present preliminary adjoint sensitivity results in an idealised domain in section 3 3 these steps are necessary to prove the capabilities and accuracy of the model so that future work can tackle the challenging problem of simulating ocean dynamics and melt rates near grounding zones 2 model description 2 1 model equations as emphasised in section 1 one of the key motivations is to develop a model with the ability to treat vertical dynamics in the same way as horizontal dynamics when the mesh cells become close to isotropic on fully unstructured meshes this means we solve the full incompressible navier stokes equations for velocity and pressure and we do not make the hydrostatic approximation conservation of momentum in strong form for a rotating fluid under the boussinesq approximation can be written as 1 u t u u 2 ω u 1 ρ 0 p τ f where 2 τ ν u u t the rotation vector of the earth is given by ω and in the applications presented in this work coriolis is implemented using an f plane assumption with f 1 409 1 0 4 s 1 corresponding to a latitude of 75 s vallis 2017 ρ 0 is the reference density of sea water taken as 1027 51 kg m 3 following asay davis et al 2016 note table 1 summarises values for general model parameters kinematic eddy viscosity ν is a rank two tensor and can be spatially variant f is a generic vector body force the velocity u u v w t and pressure p can be solved for using the continuity equation as a constraint to enforce the incompressibility condition 3 u 0 in this work we have not modified the continuity equation to allow for a free surface implementation the top surface of the model is treated as a rigid lid flows with more complicated tidal forcing may require a free surface to accurately capture the dynamics though and we anticipate implementing this feature in future work to complete the description of the problem boundary conditions need to be imposed which are given as follows 4 u u d u n n u t t on γ d 5 u n u n τ n t f tang on γ d n 6 ρ 0 τ n n p p ext τ n t 0 on γ p ext unit normal and tangential vectors are given by n and t respectively on γ d dirichlet boundary conditions for both the normal and tangential components of velocity u n and u t are specified this type of boundary can be used for rigid walls by setting u n u t 0 for no normal no slip conditions although these boundaries are not used in this work they are included for completeness to ensure the correctness of the weak forms in section 2 2 in this work all walls are represented using γ d n boundaries on these boundaries only the normal component of velocity is specified and is combined with a condition for the shear stress in the tangential direction 7 τ n t n τ t in this work all side walls are free slip boundaries these are imposed by setting f tang and u n to zero on the top and bottom boundaries namely the floating ice shelf and the seabed we apply quadratic wall drag 8 f tang c d u u t where c d is a drag coefficient for boundaries γ p ext only an external pressure is specified in 6 with τ n n the normal component of the stress vector on the boundary given by 9 τ n n n τ n although we do not consider these types of boundaries in this paper we include their formulation for completeness to ensure that the surface integrals in the weak forms of section 2 2 are consistent these types of boundaries are necessary in open domains where an external hydrostatic pressure field is applied to balance the stress on either side of the open boundary thus preventing fluid from falling out of the domain to incorporate buoyancy effects we make the boussinesq approximation where perturbations in density are neglected in all terms except the buoyancy term which is implemented as a source f where 10 f ρ ρ 0 g k where g 9 81 m s 2 is the gravitational acceleration and k is the unit vector pointing in the vertical direction ρ is a density perturbation related to temperature t and salinity s by a linear equation of state given by 11 ρ ρ 0 α t t t 0 β s s s 0 where α t 3 733 1 0 5 c 1 and β s 7 843 1 0 4 are respectively the expansion and contraction coefficients for temperature and salinity t 0 1 c and s 0 34 2 are reference temperature and salinity values used in the linear equation of state again with values from asay davis et al 2016 note that a constant hydrostatic pressure term has been subtracted from the momentum equations and incorporated into the definition of pressure so that p is a perturbation pressure 12 p p p hyd where p is the full pressure and p hyd is given by 13 p hyd ρ 0 g z and z is the vertical coordinate temperature and salinity are governed by scalar advection diffusion equations which can be written in strong form as 14 t t u t κ t t r t t res t and 15 s t u s κ s s r s s res s where κ t and κ s are the eddy diffusivity of temperature and salinity respectively these are spatially variant rank two tensors similar to viscosity r t and r s represent restoring frequencies when relaxing the solution in a sponge region to specified values t res and s res boundary conditions for temperature and salinity are given as 16 t t d on γ d t 17 κ t t n φ t on γ φ t 18 s s d on γ d s 19 κ s s n φ s on γ φ s where t d and s d are the dirichlet boundary values for temperature and salinity respectively on walls γ d t and γ d s given in 16 and 18 neumann boundary fluxes for temperature 17 and salinity 19 are given by φ t and φ s respectively note the definition of φ includes diffusivity so if the tracer is temperature the units of φ t would be m k s the next section gives the weak forms of 1 3 14 and 15 necessary for the finite element discretisation 2 2 weak form of model equations 2 2 1 tracers choosing a suitable scalar function space q for a generic tracer q and a test function ϕ the weak form of the advection diffusion equation 14 and 15 can be written as 20 m q a q k f q s ϕ q where 21 m q ω q t ϕ d x 22 a q ω q ϕ u d x γ ϕ q u n d s 23 k ω ϕ κ q d x γ d q ϕ n κ q d s γ φ q ϕ φ q d s 24 f q ω f ϕ d x 25 s ω α res q ϕ d x where the individual terms are denoted m q time derivative a q advection k eddy diffusivity f q source and s sink dirichlet boundary values for the tracer are denoted q d and neumann boundary conditions as φ q note q is the advected scalar value defined in section 2 4 in the context of surface integrals and discontinuous galerkin dg finite elements 2 2 2 velocity pressure to derive the weak forms of 1 and 3 we choose separate suitable function spaces v and w for velocity u u v w t and pressure p respectively multiplying 1 by a vector test function ϕ ϕ u ϕ v ϕ w t v and integrating over the domain ω gives 26 m a c p v f ϕ v where 27 m ω u t ϕ d x 28 a ω u u ϕ d x γ u ϕ u n d s 29 c ω f v ϕ u f u ϕ v d x 30 p ω 1 ρ 0 ϕ p d x γ p ext 1 ρ 0 p p ext ϕ n d s 31 v ω ϕ τ d x γ d γ d n ϕ n τ d s γ d n n τ t ϕ t d s γ d n f tang ϕ t d s 32 f ω f ϕ d x the individual terms being m time derivative a advection c rotation p pressure gradient v eddy viscosity and f source note u similar to q is an advected velocity also defined in section 2 4 the symbol represents an outer product between two vector quantities multiplying 3 by a test function ψ w and integrating by parts gives the weak form of the continuity equation 33 ω ψ u d x γ p ext ψ u n d s γ d γ d n ψ u n d s 0 ψ w defining ψ in the pressure space ensures that the discretised divergence and gradient operators are the negative transpose of each other and it means there are the same number of discrete continuity equations as pressure degrees of freedom this is natural since the pressure can be thought of as a lagrange multiplier that enforces the incompressibility constraint strang 2007 2 3 finite element discretisation fully unstructured and extruded meshes 2 3 1 tracers on fully unstructured meshes 2d triangle elements 3d tetrahedral elements we use linear discontinuous basis functions p1dg for the tracer test and trial space q this is because p1dg elements are well suited to advection dominated problems kärnä et al 2018 for high aspect ratio domains where the horizontal extent of the domain is much greater than the depth for instance when simulating an entire ice shelf cavity an alternative option to using a fully unstructured mesh is to make use of firedrake s in built extruded mesh feature extruded meshes offer increased performance and robustness when elements are highly anisotropic yet retain the flexibility of a horizontally unstructured mesh bercea et al 2016 mcrae et al 2016 although we do not carry out 2d vertical slice modelling on extruded meshes in this paper we describe the 2d case to help build intuition for the 3d case which is used in section 3 2 an extruded 2d vertical slice mesh is made up of quadrilateral elements which can be thought of as the tensor product of an element interval in the horizontal and an element interval in the vertical for 3d domains the elements are triangular prisms which are formed by the tensor product of a triangular element in the horizontal with an interval element in the vertical this means the horizontal part of the mesh can be fully unstructured in the vertical direction the mesh is structured it is possible to choose between a fixed number of columns in the vertical or a fixed cell height as well as shaving cells in the top and bottom columns to accurately fit the domain geometry finite elements on extruded meshes are defined in terms of a tensor product of a finite element on the horizontal mesh and finite element on the vertical mesh this approach controls how the element behaves in the horizontal direction and how the element behaves in the vertical this is particularly useful for more exotic element combinations that have different continuity requirements in the horizontal and vertical directions on extruded meshes we choose the scalar finite element which mimics the p1dg element this is formed by taking the tensor product of p1dg on the horizontal element with p1dg on the vertical interval 2 3 2 velocity pressure the choice of a suitable finite element velocity pressure element pair is crucial to ensuring an accurate and stable discretisation of the continuous equations elman et al 2014 on fully unstructured meshes we use vector p1dg basis functions for the velocity space v and quadratic continuous basis functions p2 for the pressure space w the resulting p1dg p2 velocity pressure element pair has been used with the finite element ocean model fluidity kramer et al 2010 kimura et al 2013 jordan et al 2014 and more recently implemented within the thetis coastally focused ocean model developed using firedrake kärnä et al 2018 wallwork et al 2020 the element pair has been shown to maintain geostrophic balance because the skew gradient of the pressure maps to the velocity space cotter et al 2009b a cotter and ham 2011 on extruded meshes we have chosen an element pair that has similar properties to p1dg p2 so that as the horizontal mesh is refined the discretisation is able to deal with order one aspect ratios defining the equivalent of p2 on extruded meshes is relatively straightforward it is the tensor product of a p2 horizontal element with the equivalent p2 element on the vertical interval as shown on the left side of fig 1 forming the required velocity element on an extruded mesh is more complicated though our solver strategy outlined in section 2 5 relies on the gradient of functions in the pressure function space mapping to functions in the velocity trial space given our discretisation choice for pressure this places a constraint on the continuity of the velocity element along the element boundary the gradient of the pressure is continuous across cells in the tangential direction and varies quadratically in the normal direction the pressure gradient is discontinuous between cells and varies linearly for a 2d vertical slice the horizontal component of this velocity element u is formed by taking the tensor product of p1dg on a horizontal interval and p2 on a vertical interval this ensures that the horizontal component of velocity is discontinuous in the horizontal direction but continuous between elements on vertical facets the vertical component of the velocity w is defined the other way round as a tensor product of p2 in the horizontal and p1dg in the vertical again this ensures that the vertical velocity is continuous in the horizontal direction but discontinuous in the vertical direction the 3d equivalent of the 2d velocity element is built up as follows the horizontal velocity components u and v are formed by taking the tensor product of a nedelec element of the second kind n2 defined on a triangular element with a p2 element in the vertical n2 elements are inherently 2d vector elements that are tangentially continuous between cells and discontinuous in the normal direction kirby et al 2012 the middle panel of fig 1 shows this element discretising the horizontal velocity components the vertical component of velocity is defined in the same way as on the 2d vertical slice mesh with a tensor product of p2 in the horizontal and p1dg in the vertical and is shown on the right hand side of fig 1 similarly fig 2 from section 2 6 detailing the melt parameterisation shows a 2d vertical schematic representation of the p1dg discretisation on the fully unstructured mesh versus the tensor product discretisation on the vertically structured extruded mesh 2 4 discontinuous galerkin surface integrals various terms in the weak form have been integrated by parts resulting in additional surface integrals which can be exploited to enforce boundary conditions weakly with continuous finite element basis functions contributions from either side of the same internal boundary between elements cancel this means only surface integrals defined on exterior boundaries need to be considered in the discontinuous galerkin dg method however the surface contributions from either side of the internal boundaries do not cancel because of the jump in the solution value across the boundary for the advection term in 28 we choose the advected velocity u to be the upwind value on an exterior outflow boundary this simply means u is the solution inside the domain and on inflow boundaries u is set using the dirichlet values at the boundary in the interior the solution fields on either side of a facet between two adjacent elements are arbitrarily labelled with and averages across interfaces are denoted by while denotes a jump across an interface with unit normal n integrals over interior facets are given by γ int d s using this notation the additional term for dg advection is 34 γ int u ϕ u n d s where the upwind velocity u is given by 35 u u if u n 0 36 u u if u n 0 the viscosity term is discretised using the symmetric interior penalty galerkin sipg method epshteyn and rivière 2007 after integrating by parts the discontinuous velocity field leads to an interior facet term 37 γ int ϕ n ν u u t d s adding only this term to 31 would not give a stable scheme instead two additional terms are included which ensure that the form of the diffusivity term is symmetric positive definite given by 38 γ int u n u n t ν ϕ d s γ int σ ϕ n ν u n u n t d s the first term in 38 makes the scheme symmetric the second term is a penalty term penalising jumps in the solution to ensure that the operator remains coercive which is required for stability eqs 3 20 and 3 23 from hillewaert 2013 give a minimal value for the penalty parameter σ as 39 σ α c p n e a f v e where n e is the number of facets for the element type α is a constant the theoretical minimum is 1 for stability but after experimentation we found using a value of 2 improves robustness a f is the area of the facet v e is the volume of the element eq 3 7 from hillewaert 2013 defines c as a constant that depends on polynomial degree p and element type such that 40 f u 2 d s c a f v e e u 2 d x is true for all polynomials u of degree p values for c are given in table 3 1 from hillewaert 2013 the additional sipg terms are also added on exterior surfaces 41 γ d u u d n u u d n t ν ϕ d s γ d 2 σ ϕ n ν u u d n u u d n t d s scalar advection is treated similarly to momentum with the additional interior facet term due to the discontinuous finite element space given by 42 γ int ϕ u n q d s where the upwind value of the tracer q is defined as 43 q q if u n 0 44 q q if u n 0 on dirichlet boundaries q is again only replaced if the flow is into the domain scalar diffusion in 23 is also implemented using the sipg formulation where the additional terms are given by 45 γ int ϕ n κ q d s γ int q n κ ϕ d s γ int σ ϕ n κ q n d s γ d q q q d n κ ϕ d s γ d q 2 σ ϕ n κ q q d n d s and similar interpretations of each term can be made as for momentum viscosity in 31 2 5 timestepping and solver strategy as outlined in the previous section 26 20 and 33 are the conservation of momentum the incompressibility constraint and advection diffusion equations and together they form a coupled system that needs to be solved for velocity pressure temperature and salinity as stated before we are solving for the full nonhydrostatic dynamics and we do not carry out any mode splitting to separate the barotropic and baroclinic components of the flow first we solve for the velocity and pressure using 26 and 33 by the established pressure projection approach chorin 1967 kramer et al 2010 which splits the solve into two parts there is an initial prediction step to find an intermediate velocity where the momentum equation is solved using an implicit backward euler timestep for all the terms in the momentum equation except the pressure p n which uses the value from the previous timestep the nonlinear advection and drag terms are linearised by setting the advecting velocity to the velocity value from the previous time step this intermediate velocity u int is not divergence free however so a correction step enforces the incompressibility constraint this finds the updated pressure p n 1 needed to obtain a divergence free velocity field u n 1 this correction step takes the form of a time splitting step 46 u n 1 u int δ t p n 1 p n which is discretised in the same way as the m and p terms in 26 coupled with an incompressibility constraint eq 33 for u n 1 firedrake provides an interface to choose solver options to pass to petsc the package that implements the solver routines balay et al 1997 typically we choose the gmres krylov subspace method to solve the momentum equation during the prediction step because the matrix is not symmetric it is preconditioned with an algebraic multigrid preconditioner such as petsc s default geometric algebraic multigrid gamg or boomeramg from the hypre suite henson and yang 2002 the block structure of the coupled correction solve is simpler than the full navier stokes system 26 and 33 because the matrix that arises in the top left hand block is only a mass matrix resulting from the time derivative term in 27 where in the full system it would contain contributions from all the other terms in the momentum equation because of the careful choice of velocity pressure finite element function spaces see section 2 3 2 where the gradient of pressure maps pointwise exactly into the velocity function space it can be shown that the schur complement of this system is exactly equivalent to a standard continuous galerkin discretisation of the pressure poisson equation cotter and ham 2011 this poisson equation is solved using the conjugate gradient cg algorithm preconditioned with an algebraic multigrid the efficiency of this solution strategy is highly dependent on the condition number for an anisotropic domain this is proportional to the square of the aspect ratio of the grid cells kramer et al 2010 thus as the aspect ratio goes to infinity the condition number can become unbounded with negative implications for the convergence of the solve however by preconditioning the cg algorithm with a preconditioner that approximates the solution to the depth averaged equations convergence can be made independent of the aspect ratio this so called vertical lumping approach kramer et al 2010 is similar to the idea first used in mitgcm and ensures that when the dynamics are close to a hydrostatic state the solve is as fast as when only solving the hydrostatic equations marshall et al 1997 for parallel efficiency the method does require cells to be aligned in columns in the vertical direction this can be achieved using firedrake s in built extruded mesh feature in combination with the tensor product discretisation discussed in section 2 3 1 we have found that when running 3d simulations on structured tetrahedral meshes on aspect ratios larger than 1 0 2 e g dx dy 2 km dz 20 m using the generic algebraic multigrid preconditioner gamg the wall clock time needed to solve one timestep of the model was dominated by the pressure correction solve however on extruded meshes with the vertical lumping preconditioner the pressure correction solve is faster than the momentum solve for the intermediate velocity after solving for velocity and pressure we solve for temperature and salinity individually we use a 3 stage diagonally implicit runge kutta dirk time stepping method ascher et al 1995 for the temperature and salinity advection diffusion equations 20 the method is third order accurate it is also an l stable scheme so it is suitable for stiff problems we apply a vertex based slope limiter after each time step to prevent the solution from becoming unbounded kuzmin 2010 we have found that using a slope limiter is only necessary in the region of the ice front transition which tends to have the fastest flow as the buoyant melt water plume accelerates up the ice front without the limiter a spurious freezing signal can occur confined to the final grid cell before the ice front transition with the limiter however freezing does not take place although computational performance and parallel scalability are not directly considered in this paper we expect our p1dg finite element based method with implicit time stepping to be about an order of magnitude slower than a conventional low order possibly explicit finite volume model for the same model domain and grid resolution the ability to take longer timesteps can offset this more importantly by using a fully flexible meshing strategy we are able to represent small scale features in a large domain that would be impossible to represent with structured grids typical of other ocean models we have run firedrake simulations of ocean circulation beneath ice shelves in parallel with up to 128 cores the reader is referred to the main firedrake paper for a more in depth study of performance rathgeber et al 2016 future work will investigate the performance of this ocean model when it is applied to more demanding geometrically complex 3d domains that require a fully flexible unstructured mesh 2 6 melt parameterisation melt rates are calculated based on conservation of heat and salt at the ice ocean boundary and under the assumption that the boundary is at the freezing point commonly referred to as the three equation melt parameterisation hellmer and olbers 1989 holland and jenkins 1999 dinniman et al 2016 conservation of heat is expressed as 47 q i t q w t q latent t where the latent heat term q latent t is given by 48 q latent t ρ 0 w b l f with w b the unknown melt rate melting w b 0 freezing w b 0 and l f 3 34 1 0 5 j kg 1 is the latent heat of fusion the conductive heat flux into the ice q i t is 49 q i t ρ i c p i w b t i t b where ρ i 920 kg m 3 is the density of ice c p i 2000 j kg 1 k 1 is the specific heat capacity of ice t i is the far field ice temperature taken to be 20 c and t b is the unknown temperature at the ice ocean boundary the turbulent heat flux from the ocean to the ice q w t is given by 50 q w t ρ 0 c p γ t t b t w where c p 3974 j kg 1 k 1 is the specific heat capacity of water and γ t the turbulent thermal exchange velocity commonly γ t is expressed as γ t u where γ t is a dimensionless constant and u c d u is the friction velocity asay davis et al 2016 we use the velocity at the top surface of the domain to calculate u in effect making the assumption that our computational boundary is in the surface layer where rotation effects are unimportant which may not be valid in the presence of strong stratification mcphee 2008 t w is defined by holland and jenkins 1999 as the temperature in the mixed layer at the edge of the turbulent ice ocean boundary layer in most ocean models this is either taken as the value of the temperature in the top model cell adjacent to the boundary or temperature value representative of some distance from the ice ocean interface gwyther et al 2020 we take t w as the model temperature value at the computational boundary this is perhaps an uncommon choice compared with other ocean models dinniman et al 2016 unless doing direct numerical simulation dns which is prohibitively expensive for ice ocean simulations in more than metre scale domains couston et al 2021 the scalar molecular and turbulence boundary layer will not be modelled explicitly it must be accounted for with a parameterisation since we are unable to model these effects explicitly we choose to imagine that the actual ice ocean interface is offset slightly beyond the computational domain which is similar to the approach taken for the parameterisation of wall regions in reynolds averaged navier stokes rans turbulence models yeager 2018 fig 2 shows a schematic of the melt parameterisation while the finite element solution is valid at any point within each element and this means it is possible to offset where the tracer is sampled towards the interior of the domain as shown by kimura et al 2013 this could cause performance issues when running the model in parallel because the sampled tracer value may not be stored on the same processor the conservation of salt is similar to heat and is given by 51 q i s q w s q brine s where the fresh water melt flux q brine s is given by 52 q b r i n e s ρ 0 w b s i s b and where s i is the salinity of the ice which is taken to be zero and s b is the unknown salinity at the ice base the turbulent salt flux across the ocean boundary layer q w s can be written as 53 q w s ρ 0 γ s s b s w where γ s is the turbulent salinity exchange velocity analogous to γ t and as before we take s w as the value of salinity at the edge of the computational domain offset from the ice ocean boundary the diffusive flux of salt into the ice q i s is assumed to be zero the ice ocean boundary temperature t b is assumed to be at the freezing point 54 t b a s b b c p b where a 5 73 1 0 2 c b 8 32 1 0 2 c and c 7 53 1 0 8 c pa 1 are the coefficients obtained from linearisation of the nonlinear freezing point equation millero 1978 using the values from asay davis et al 2016 and p b is the pressure at the boundary this means there are now three equations 47 51 and 54 to solve for the three unknowns w b s b and t b the melt rate and the salinity and temperature values at the boundary respectively rearranging the equations gives a quadratic equation for s b and the positive value is taken as the solution by using the form of the heat and salt flux from 50 and 53 robin boundary conditions for temperature and salinity can be imposed as shown in fig 2 according to 55 φ t κ n t γ t w b t b t w and 56 φ s κ n s γ s w b s b s w where φ t and φ s are substituted into the third term of 23 since the ice ocean boundary is not a material interface we include the melt water correction term accounting for the advection of melt water with water mass properties corresponding to values at the ice ocean interface into the ocean domain as part of the temperature and salt flux boundary conditions following jenkins et al 2001 2 7 model implementation we have built a python module on top of firedrake to facilitate simulations of ice shelf cavities the core part of our python package is made up of 11 modules we make use of python classes to build up the complexity of the code in blocks each equation inherits useful attributes from a baseequation i e a method for summing the residuals from each term and each equation is made up of terms effectively each term in 20 and 26 themselves based on a baseterm which contains the required features of a term i e a method to evaluate the residual form for the term this approach leads to a compact codebase aids with debugging as individual pieces of the code can be isolated and tested but retains the flexible nature of firedrake which is one of the main reasons for using the software the code is available at https github com thwaitesproject thwaites git it has also been stored in this zenodo repository https doi org 10 5281 zenodo 7584700 along with instructions for running the examples from section 3 the next section presents a simplified verification test applicable to ice ocean models which will hopefully prove of value for other models to ensure that the code is solving the model equations correctly and consistently 3 results 3 1 method of manufactured solution mms tests the method of manufactured solutions mms is a rigorous method for verifying that the numerical scheme as implemented is solving the model equations accurately and consistently farrell et al 2011 roache 1998 this is a separate problem to that of validating whether the model including the choice of underlying equations yields a good approximation to the real system such a validation requires comparison with observations which is challenging for simulations of ice shelf cavity circulation the most reliable way to test whether a numerical approximation is consistent is to ensure that the solution error relative to a known solution decreases at the correct order as the grid is refined salari and knupp 2000 however finding analytical solutions to systems of coupled partial differential equation pde that arise in fluid dynamics is notoriously difficult and only possible for simple geometrical configurations and often simplified boundary conditions bcs mms removes much of this difficulty instead of solving the pde system to find the solution the analytical form of the solution is chosen to begin with solution fields are constructed from smoothly varying differentiable functions often trigonometric and polynomial functions these functions are substituted into the strong form of the equations and the resulting residuals which are expected to be non zero as these functions could not be expected to represent an exact solution provide additional source terms that can be used to force a version of the simulation for which the chosen functions are exact solutions the chosen functions can be manufactured to meet specific constraints for example incompressibility of the flow field or specific bcs alternatively consistent bcs can be found directly from the chosen solutions simply by evaluating the solution at the boundary for the type of bc required for testing following the notation from farrell et al 2011 the error on a given mesh is 57 e h 1 c h 1 c p where c is a constant h 1 is the characteristic mesh size and c p is the order of accuracy of the numerical approximation the error on a finer mesh is 58 e h 2 c h 1 r c p where the new characteristic mesh size h 2 h 1 r by taking the ratio of the two errors the convergence rate can be found using 59 c p log r e h 1 e h 2 the mms solutions should be constructed to be as general as possible to ensure good coverage of the terms in the equations salari and knupp 2000 although this does not require the chosen solution fields to be realistic the solution we have chosen for this mms test is an overturning flow that is a crude representation of an ice pump mechanism for simplicity we have limited ourselves here to flow within a 2d vertical slice the domain is a square box with length l 100 m height h 100 m and with the bottom boundary at a depth d 1000 m the chosen solution for the horizontal velocity is 60 u u 0 x l cos π z d h h where u 0 is a characteristic velocity of 1 m s x and z are the horizontal and vertical coordinates as an example of the mms procedure fig 3 shows the finite element solution fields for the horizontal velocity component on the two coarsest grids alongside the corresponding error relative to the chosen solution the error on the finer mesh is noticeably reduced compared to the error on the coarser mesh despite the two solution fields being qualitatively similar the chosen solution for vertical velocity is 61 w u 0 h π l sin π z d h h this has been chosen to ensure that the flow is incompressible so that the continuity equation u 0 is satisfied for this simple flow configuration the vertical velocity does not depend on x the right hand side is specified as an open boundary and all other boundaries employ free slip conditions with no normal flow imposed the pressure field has been chosen to have homogeneous neumann boundaries p n 0 62 p p 0 cos π x l cos π z d h h p 0 is a constant set to 1 pa to ensure the dimensions remain consistent and the reference density ρ 0 is set to 1 kg m 3 for this test the temperature field is 63 t t c sin 4 π x l a t z 2 b t z c t where t c is 0 1 c a t is 3 89 1 0 4 c m 2 b t is 7 54 1 0 1 c m 1 and c t is 3 64 1 0 2 c the salinity field is 64 s s c cos 4 π x l a s z 2 b s z c s where s c is 0 345 a s is 1 44 1 0 4 m 2 b s is 2 81 1 0 1 m 1 and c s is 1 03 1 0 2 the viscosity and diffusivity are 1 m 2 s as already described the source terms are derived by substituting the solutions into the strong form of the equations the source term for velocity is thus given by 65 s u u t u u 1 ρ 0 p τ ρ ρ 0 g k note the coriolis term is not included because the domain is a 2d vertical slice the implementation of rotation has been verified against analytical solutions of ekman spirals not shown the source term for temperature is given by 66 s t t t u t κ t t the source term for salinity is given by 67 s s s t u s κ s s use of symbolic computation tools such as sympy meurer et al 2017 make this process relatively straightforward to implement for example the source term for the horizontal velocity component in this case is given by 68 s u p 0 π sin π x l cos π h d z h l ρ 0 u 0 2 x sin π h d z h 2 l 2 u 0 2 x cos π h d z h 2 l 2 π 2 ν u 0 x cos π h d z h h 2 l note we chose this term for its compactness because it does not include buoyancy terms dirichlet bcs for temperature and salinity are applied on the left and right boundaries with neumann conditions on the bottom boundary values for these boundary conditions are found in a similar manner to defining source terms the solution itself is substituted to give the exact form of the generally inhomogeneous boundary conditions salari and knupp 2000 particular attention is given to the boundary conditions at the ice ocean interface situated at the top of the domain as verifying the accuracy of the model implementation at this location is a key reason for carrying out the mms test the boundary conditions at the ice ocean interface are represented by robin boundary conditions which define the gradient of the temperature and salinity fields at the boundary as a function of the temperature and salinity at the edge of the computational domain see section 2 6 for more information the default boundary condition for the melt boundary is given by 55 and 56 which are imposed weakly in the third term of 23 for an mms test the temperature boundary condition applied at the top is given by 69 φ t κ n t a φ t h φ t a the first term in 69 represents the temperature gradient of the analytical temperature field t a found from 63 at the boundary with the eddy diffusivity as a multiplication factor as discussed previously this is the only term needed to implement neumann boundary conditions for conventional mms tests the second term φ t h is the flux boundary condition calculated by the melt parameterisation using the modelled temperature and salinity values equivalent to 55 and 56 the final term φ t a is the flux calculated by the melt parameterisation using the analytical temperature field provided the temperature salinity and velocity through u fields converge at the correct order then the melt rate error calculated using the exact temperature salinity and velocity fields should also converge at the correct order because the contribution from φ t h and φ t a cancel out this is a spatial convergence test so the solution fields have been chosen to be stationary farrell et al 2011 the simulation is spun up from rest backward euler is used for timestepping the momentum equations and the scalar equations the timestep is initially set to 4 m u 0 n x chosen for robustness in the initial spin up where n x is the number of cells in the x direction the timestep is increased to l u 0 n x after 100 time steps for convenience table 2 summarises parameters and constants used for the mms convergence test velocity and tracer fields are discretised with p1dg elements and so second order convergence is expected which means that halving the grid size should reduce the error by a factor of four pressure is discretised with p2 elements so third order might be expected however coupling errors associated with the velocity mean that in practice the convergence is about second order cotter and ham 2011 fig 4 plots the errors for each solution field with logarithmic axes velocity temperature and melt rate are all very close to the expected second order convergence or higher salinity and pressure are slightly lower though never below 1 8 and the convergence rate is increasing towards second order as the grid is refined the integrated melt is also initially above second order but drops to 1 82 convergence overall these convergence results give confidence in the accuracy and consistency of the numerical discretisation and hopefully will prove to be a useful tool as the model continues to be developed in the following section we demonstrate our ability to simulate more complex ice shelf cavity geometries in 3d using the isomip ocean0 test case 3 2 3d simulations of an idealised ice shelf cavity isomip ocean0 the aim of this section is to demonstrate our ability to simulate ocean flow in a fully three dimensional ice shelf cavity the ice shelf ocean model intercomparison project isomip consists of a set of idealised ice shelf cavity geometries and parameters for a set of common experiments asay davis et al 2016 here we use the ocean0 experiment which has been used for a number of studies to introduce and compare different ocean models for ice shelf cavity applications zhou and hattermann 2020 gwyther et al 2020 favier et al 2019 this experiment reaches a quasi steady state in the cavity within a few months of simulation time so it is a convenient test case for carrying out model comparisons and testing model parameter choices asay davis et al 2016 there is not an exact solution that all models are aiming to replicate for this reason we have found it helpful to run simulations of mitgcm alongside our model mitgcm is a finite volume z layer fixed vertical resolution ocean model widely used in a number of applications including investigations of ice shelf ocean cavities losch 2008 dansereau et al 2014 seroussi et al 2017 nakayama et al 2017 kimura et al 2017 nakayama et al 2021a naughten et al 2021 it also has a nonhydrostatic option selected for this comparison so that both models solve the same underlying set of equations albeit with different discretisations the ocean0 domain is approximately 400 km long 80 km wide and has a maximum depth of 720 m the bathymetry is specified by an analytical profile representing an idealised fjord and the ice draft is from an ice flow simulation from the corresponding mismip experiment asay davis et al 2016 the target isomip grid resolution is specified as 2 km in the horizontal and 36 vertical layers with the caveat that different vertical meshing strategies will impose constraints on the resolution asay davis et al 2016 the isomip protocol does not specify exactly how the cavity thickness should be defined as the real cavity pinches to zero thickness at the grounding line for mitgcm simulations we have followed the suggested 40 m minimum thickness for z layer models so that there are always at least two cells with the specified 20 m vertical resolution to create the 3d firedrake mesh we used the inbuilt extruded mesh capability of firedrake first we used qgis qgis development team 2022 to generate the horizontal extent of the mesh we chose the grounding line by tracing the 10 m ocean thickness contour ice draft bathymetry we used the qmesh package avdis et al 2018 to generate the horizontal surface mesh that forms the base of the columns in the full 3d extruded mesh qmesh is effectively a wrapper for gmsh geuzaine and remacle 2009 to allow meshing of domains generated from gis data the surface mesh is then extended in the vertical direction to create columns of extruded triangular prism elements bercea et al 2016 mcrae et al 2016 we use 30 layers under the ice with a transition to 36 layers in the open ocean region with this transition occurring over a single horizontal grid cell squashing the domain under the ice gives a terrain following quasi sigma style mesh within the cavity with higher vertical resolution towards the grounding line we use the tensor product elements described in section 2 3 2 for the discretisation of the velocity pressure finite element pair and for tracers the tensor product element described in 2 3 1 the temperature and salinity profiles for initialisation and restoring on the northern boundary are the warm profiles given in table 6 from asay davis et al 2016 we use a timestep of 900 s with backward euler time stepping for the momentum equations and a diagonally implicit runge kutta method dirk33 for the tracers horizontal and vertical kinematic viscosity are specified at 6 m 2 s and 1 1 0 3 m 2 s respectively and horizontal and vertical diffusivity are specified at 1 m 2 s and 5 1 0 5 m 2 s as per the isomip protocol asay davis et al 2016 we include the switch to higher vertical mixing values when the stratification is unstable however the scheme has negligible effect on calculated melt rates due to the build up of stable stratification at the ice ocean boundary during melting the melt parameterisation specified by isomip does not include heat flux into the ice and uses constant turbulent exchange coefficients based on jenkins et al 2010 for this comparison both models use the same turbulent exchange coefficients in the melt parameterisation γ t 0 011 and γ s γ t 35 the vertical averaging scheme for melting is turned off in mitgcm losch 2008 drag boundary conditions are applied on the top and bottom interfaces with a drag coefficient c d 2 5 1 0 3 model parameters are summarised in table 3 fig 5 shows melt rates for the first 100 days of simulation integrated over the region where the ice draft is deeper than 300 m following asay davis et al 2016 we have found that melt rates are very sensitive to the grid resolution mitgcm melt rates are plotted in blue with vertical resolution given by the symbol triangles 20 m target isomip resolution pluses 10 m crosses 5 m circles 2 5 m clearly integrated melt rates calculated by mitgcm are highly dependent on vertical resolution as refining the mesh three times leads to nearly a factor of two difference and there is no sign of convergence our model also shows vertical dependence of melt rate as evidenced by the red lines in fig 5 dashed 18 levels continuous 36 levels dotted 72 levels however the spread is lower than with the z layer model these results are consistent with a recent study by gwyther et al 2020 using a range of models applied to the same isomip ocean0 experiment and we will refer to their work later in the discussion fig 6 shows plan view snapshots of melt rate at 50 days for mitgcm at different vertical resolutions compared with firedrake at the target resolution specified by isomip for layered models 36 layers outside the cavity 30 layers inside the cavity as the vertical resolution of mitgcm increases the spatial melt rate pattern can be seen to approach that from firedrake the streaky time evolving artefacts present in mitgcm at coarser vertical resolutions circled in fig 6 disappear as the grid is refined note that while time mean fields of melt rate act to smooth out the streaks there are still noisy grid scale features after time averaging not shown reducing the vertical diffusivity and viscosity linearly at the same rate as the grid refinement does not bring the streaks back not shown this suggests it is not a numerical instability of the type dependent on grid reynolds number the reynolds number evaluated at the grid scale r e δ x u δ x ν where u is a characteristic velocity δ x is the grid resolution and ν is the eddy viscosity or grid péclet number similarly p e δ x u δ x κ where κ is the eddy diffusivity the streaks may therefore be a grid discretisation error caused by the step representation of the boundary that is inherent to z layer models even with a partial cell representation losch 2008 both models show highest melting at the grounding line which is consistent with the depth dependent melting point warm initial and restoring conditions at the bottom of the water column and steeper basal slopes close to the grounding line that enhance buoyancy driven flow this melt pattern is also consistent with other published isomip ocean0 results asay davis et al 2016 zhou and hattermann 2020 gwyther et al 2020 within the cavity both models show a clearly defined western defined as positive y boundary current due to the effects of rotation this can be seen in fig 7 which is a cross section of the cavity at x 480 km close to the grounding line there is strong northward flow along the western boundary together with colder and fresher conditions which results from melt water accumulating and preferentially flowing through this part of the domain the middle panel of fig 7 shows mitgcm at the higher vertical resolution refining mitgcm leads to better qualitative agreement with firedrake the return boundary flow and the stratification at the top of the domain is qualitatively much better resolved by mitgcm at the higher resolution particularly in the eastern half of the domain at this location the cavity thickness is close to 100 m so with 30 vertical layers firedrake s vertical grid resolution is similar to mitgcm s vertical grid resolution of 2 5 m so it is not unexpected that the higher resolution mitgcm simulation yields results much closer to those from firedrake in reality melting leads to the injection of cold and importantly buoyant fresh water at the top of the domain if the ice shelf base has a sufficient slope the buoyant melt water drives a shear flow and thus enhances mixing however melting can also lead to a negative feedback mechanism since melt water forms a stratified layer especially for low basal slope angles that effectively shuts down vertical mixing mcphee 2008 vreugdenhil and taylor 2019 this leads to reduced melt rates because the ice base is insulated from the warmer saltier water below it is the competition between the buoyancy driven shear and build up of stratification as well as external sources such as tidal currents that controls mixing modelling vertical mixing accurately is therefore of fundamental importance to represent flow in ice shelf cavities and to calculate the associated melt rate the total amount of mixing simulated by a numerical ocean model is a combination of resolved turbulence mixing explicitly specified mixing that aims to account for the unresolved turbulent mixing here we use a simple eddy diffusivity closure with constant diffusivity and a grid size dependent implicit contribution often referred to as numerical mixing this spurious numerical mixing is often dictated by the details of the discretisation of the advection and diffusion operators griffies et al 2000 refining the grid resolution decreases the amount of spurious numerical mixing added by the model in the context of ice shelf cavity modelling reduced mixing can lead to increased stratification and in turn lower melt rates if the basal slopes are not too steep terrain following models with typically higher resolution at the grounding zone resulting from the squashed vertical grids will have lower numerical mixing and thus resolve more of the stratification effects than a z layer model run at typical coarse vertical resolutions as seen in fig 7 this means terrain following models in typical configurations tend to calculate lower melt rates z layer models like mitgcm can produce lower melt rates at higher resolution as shown in fig 5 the specified vertical mixing has to be higher than the spurious mixing value for grid convergence of the melt rate otherwise as the grid is refined the total amount of mixing generated by the model decreases leading to a reduction in melt rates the dependence of melt rate on vertical resolution has been investigated previously in a separate study also using the ocean0 experiment gwyther et al 2020 gwyther et al 2020 used three ocean models with different vertical coordinate systems sigma roms z coco hybrid mpas o and showed that the two models with constant 20 m vertical resolution at the boundary specified by isomip coco and mpas o systematically calculate higher melt rates than the sigma coordinate based model which is consistent with the results presented here gwyther et al 2020 following the isomip protocol framed the problem as a matter of tuning the turbulent exchange coefficients from the melt parameterisation to obtain the same melt rates between models the isomip protocol suggests a target melt rate averaging 30 m a in the grounding zone region where the ice draft is below 300 m depth gwyther et al 2020 showed that the sigma model roms was incapable of reaching this target melt rate purely by adjusting the exchange coefficients here we take a different approach and frame the problem in terms of grid convergence with the aim of achieving a melt rate that is unaffected by grid resolution and keeps the exchange coefficients fixed given the current implementation of the melt parameterisation and turbulence closure constant eddy diffusivity and viscosity this means we need the temperature salinity and velocity fields to remain constant at the boundary as the grid is refined to achieve a fixed melt rate gwyther et al 2020 show that sigma coordinate based models can produce higher melt rates with elevated vertical diffusivity at the boundary fig 8 shows integrated melt rates for firedrake run at different vertical resolutions and vertical diffusivities the lines in red are the same as from fig 5 which is the integrated melt rate at the target isomip vertical diffusivity of 5 1 0 5 m 2 s with this diffusivity the melt rate still depends on grid resolution as discussed the blue and orange lines in fig 8 show the melt rate with diffusivities of 5 1 0 6 m 2 s and 5 1 0 7 m 2 s respectively despite the order of magnitude change there is no significant difference in melt rate this suggests that numerical mixing dominates when the specified mixing is specified at these low levels as a result the melt rate would be expected to be highly dependent on grid resolution and we expect refining the vertical resolution further would reduce melt rates in contrast by increasing the vertical diffusivity by ten times the amount of the isomip specification to 5 1 0 4 m 2 s shown by the black lines in 8 the integrated melt rate calculated with 72 layers is very close to the melt rate calculated with 36 layers the specified number of layers for the isomip profile since the melt rate appears to be converging towards a consistent value as the grid is refined the specified diffusivity must be dominating over the spurious resolution dependent numerical mixing this comparison suggests that a rough estimate for the numerical mixing for a firedrake simulation with the isomip target of 36 layers is likely between 5 1 0 6 m 2 s and 5 1 0 5 m 2 s since the upper bound has the same value as specified in the isomip protocol this serves to highlight the importance of running ice ocean simulations at multiple resolutions to determine how sensitive the melt rate is to resolution for a particular model and setup configuration the implicit assumption made here is that the diffusivity and viscosity do not vary close to the boundary this is not necessarily physically justifiable as monin obukhov theory implies that with strong melting and insufficient shear turbulence to break down the stratification the mixing rate and hence the values of diffusivity and viscosity will be much lower leading to overestimated melt rates vreugdenhil and taylor 2019 mcphee 2008 we have assumed fixed diffusivity and viscosity primarily for reasons of simplicity it is the isomip specification and with a new model we wanted to have confidence that we could understand the general behaviour before implementing more complicated turbulence closure schemes even if the model is responsible for explicitly calculating diffusivity and viscosity the pertinent length scales close to the boundary are very small sub metre scale mcphee 2008 so it may be impractical to account for these effects in large scale ocean simulations potentially the melt parameterisation could be modified to incorporate the enhanced stratification effects that arise during melting similar to the transition of scalar properties across the viscous boundary later which is accounted for holland and jenkins 1999 but this leaves the question of where to sample t w and s w open unless the sampling distance is chosen for physical reasons it seems like it will inherently depend on the grid resolution it is not obvious how applicable an ice shelf ocean cavity simulation will be for different forcing scenarios or geometrical configurations if the exchange coefficients have been tuned specifically to compensate biases arising from a somewhat arbitrary choice of grid resolution overall our integrated melt rate of 7 6 m a for the target isomip resolution of 2 km horizontally and 36 vertical layers orange line fig 5 is consistent with and within the spread of other published results when using the same turbulent exchange coefficients in the melt parameterisation γ t 0 011 and γ s γ t 35 published results for terrain following models with 36 layers include fvcom 10 m a figure 9 from zhou and hattermann 2020 and roms 4 m a figure 4 from gwyther et al 2020 similarly examples of z level models at high vertical resolution dz 2 m include coco 5 m a and mpas o 9 m a figure 5 from gwyther et al 2020 as well as pop2x 7 m a figure 9 from asay davis et al 2016 also note the mitgcm integrated melt found here for this comparison is consistent with coco and mpas o which both gave around 15 m a with 20 m vertical resolution figure 5 from gwyther et al 2020 we also investigated melt rate sensitivity to horizontal resolution within the cavity we ran firedrake at two coarser horizontal resolutions additionally shown in fig 5 purple 8 km red 4 km to compare with the target isomip resolution orange 2 km although the spatial patterns of melt rate are still broadly equivalent with highest melting at the grounding line seen in all simulations not shown the integrated melt rates have not converged to a consistent result to investigate the potential of a horizontally unstructured mesh to focus resolution we ran a preliminary simulation with the target isomip 2 km resolution only along the western boundary and the grounding zone region relaxing to 8 km outside the cavity these areas were chosen to best capture the observed spatial melt pattern with highest melting at the grounding line as well as the fast flowing western boundary current present due to coriolis the integrated melt rate for this non uniform mesh is shown by the black line in fig 5 a positive result is that the melt rate predicted with the non uniform mesh is closer to the value from the uniform 2 km mesh than the 8 km and indeed the 4 km uniform mesh however considering the degrees of freedom dofs associated with each mesh this may not be surprising the number of combined velocity pressure temperature and salinity dofs is 964 059 for the 8 km mesh 3 448 681 for the 4 km mesh and 13 693 896 for the 2 km mesh the non uniform mesh has 5 011 517 dofs which is actually 1 45 times more dofs than the 4 km mesh in more complicated cavity geometries the ability to vary the mesh resolution flexibly especially in the presence of large channels and crevasses may prove crucial zhou and hattermann 2020 as emphasised in section 1 one of the main motivations for using the firedrake framework to simulate ice ocean interactions is the availability of an automatically generated adjoint model in the final section we show our first steps at using this capability to investigate sensitivities in an idealised ice shelf cavity 3 3 preliminary adjoint sensitivity calculations in an ice shelf cavity we present preliminary results of sensitivity information obtained with the adjoint to our model a simplified domain has been chosen to make interpretation of the adjoint sensitivity patterns easier the domain is a vertical 2d slice within 10 km of an idealised grounding zone domain with a 2 m wall at the grounding line and a 100 m wall at the open ocean the domain has a uniform seabed depth of 600 m the temperature and salinity are initialised with constant temperature and salinity of 1 c and 34 4 respectively the temperature and salinity fields are relaxed to these values in a sponge region which linearly ramps up over the final four grid cells to the right hand side of the domain with a restoring period of one day at the boundary the grid is made up of triangles arranged in columns similar to a z layer discretisation for an example the reader is referred to kimura et al 2013 the grid resolution is 500 m in the horizontal and 2 m in the vertical horizontal viscosity is 0 25 m 2 s and vertical viscosity is 1 1 0 3 m 2 s matching the grid aspect ratio diffusivity for both tracers is set equal to viscosity we use a p1dg p2 discretisation for velocity and pressure and p1dg is used for the tracers the timestep is 300 s table 4 summarises parameters and constants used for the adjoint sensitivity experiment the forward simulation is run for 50 days to reach a spun up state fig 9 shows the spun up state at 50 days the flow is characteristic of the ice pump mechanism with a clockwise overturning flow concentrated near the ice ocean boundary driven by buoyant melt water at the grounding line the water column is well mixed in the vertical though the temperature field becomes progressively more stratified away from the grounding line this type of flow is similar to that described in holland 2008 where background tidal mixing dominates at the grounding zone if the overturning circulation weakens in this instance it is likely caused by the coarse grid resolution and constant eddy diffusivity values because at the 2 m high grounding zone wall on the left hand side of the domain there is only one cell to resolve the overturning circulation it is likely that refining the grid resolution effectively reducing implicit spurious mixing as described in section 3 2 or using a turbulence model would change the rate of mixing in this region and the water column would become more vertically stratified the model is run for a further 20 days with the adjoint model an objective functional is chosen to be the total basal melt beneath the ice shelf at the final time step although using the average basal melt beneath the ice shelf may have been a more conventional quantity this can be obtained by dividing the results by the length of the ice shelf we calculate sensitivities of this functional to perturbations of the spatially varying temperature and salinity fields at each time level as well as to spatially varying perturbations of the viscosity and diffusivity fields which are kept fixed in time these fields are known as controls errico 1997 the accuracy of the adjoint sensitivity can be verified by a taylor remainder convergence test farrell et al 2013 if the gradient is correct then with j m as the functional m as the control and δ m as a small perturbation in an arbitrary direction j m δ m j m j δ m should converge to zero with second order accuracy as the magnitude of the perturbation is reduced i e halving the size of the perturbation should cause the result to decrease by a factor of four we have carried out such taylor remainder convergence tests for each control field with the melt functional evaluated after ten timesteps 3000 s from the 50 day checkpoint the errors do indeed display convergence at second order and verify that the gradient information is being calculated accurately by the adjoint model fig 10 shows adjoint sensitivity of the final total melt flux note defined here as a volume flux not a mass flux to temperature and salinity at three times in the simulation the adjoint model is effectively a transpose of a linearised version of the full non linear forward model this linear forward model is referred to as the tangent linear model tlm and it relates perturbations of the control fields to changes in the objective function the adjoint model reverses the flow of information because it is the transpose of the tlm instead calculating the sensitivity of the objective function to the controls for time dependent models the adjoint sensitivities thus flow backwards in time from the time when the objective function is evaluated towards the earlier times provided that the controls had an impact on the objective function errico 1997 the top panel of fig 10 shows six hours before the final time where the sensitivity fields are concentrated at the ice ocean boundary where the objective function is evaluated since we have evaluated the objective function at the final time this is equivalent to adding an initial condition to the adjoint model as such even though the forward model is steady the adjoint sensitivities evolve backwards with time in the opposite sense to the forward model at two days before the final time middle panel the sensitivity patterns are concentrated closer to the grounding line and at the bottom of the cavity finally five days before the final time bottom panel most of the sensitivity of the objective function to the temperature and salinity fields has been lost the counterclockwise motion of the sensitivity fields is effectively the ice pump mechanism but viewed in reverse the time for the sensitivity fields to be lost is consistent with the forward velocity flow speeds are on the order of 0 05 m s and an advection path length of 20 km twice the domain length gives a circulation time of 4 6 days an interesting feature of fig 10 is that the signs of adjoint temperature and salinity are opposite in the same region a small positive perturbation to temperature is required to increase the final total melt flux whereas an increase in salinity would decrease the final total melt flux initially this seems counter intuitive since increasing salinity decreases the freezing point which should lead to more melting and thus have the same effect as increasing temperature this dependency has been found before in an adjoint simulation of pine island glacier ice shelf cavity heimbach and losch 2012 heimbach and losch 2012 suggest that buoyancy effects may account for the sensitivity difference since increasing temperature and decreasing salinity both increase buoyancy this in turn should strengthen the overturning ice pump and lead to higher melt rates scaling the sensitivity fields by the expansion coefficients in the linear equation of state α t 2 1 0 4 c 1 β s 7 1 0 4 the two sensitivity fields are almost identical in sign and magnitude strongly suggesting that buoyancy is controlling the sensitivity pattern even though this is a highly idealised experiment the adjoint model shows potential to investigate the importance of subglacial hydrology and the formation of buoyant plumes at the grounding line and hence the overall effect on melting as a complimentary tool to the forward model fig 11 shows patterns of adjoint sensitivity for vertical viscosity temperature diffusivity and salinity diffusivity sensitivity with respect to the vertical components were an order of magnitude stronger than the horizontal components which is not surprising considering how sensitive the melt rate and hence the flow is to vertical stratification as explored in section 3 2 sensitivity for all three fields is concentrated towards the grounding line and the ice ocean interface this is intuitive because the overturning flow brings water through the grounding zone region results from yeager 2018 using a k ϵ reynolds averaged navier stokes rans model to investigate ocean conditions near grounding lines suggest that changes in diffusivity of several orders of magnitude may occur within 1 km of the grounding line even though these large changes in diffusivity would not linearly relate to changes in melt rate the fact that total melt rates are sensitive to grounding line eddy diffusivity values which probably vary significantly highlights the importance of modelling these values accurately 4 conclusion and future work we have presented a new model for ocean flow in complex ice shelf cavity environments by carrying out an mms test we confirmed that the numerical discretisation of velocity pressure and tracer fields and consequently the melt rate are second order accurate as expected from our model discretisation choice we believe that it is important to carry out rigorous code verification checks for which the mms technique is ideally suited because comparing solely with other models may mask model errors this is especially true considering the strong dependence of the melt parameterisation on grid resolution as seen in the simulations of the isomip test case the model is capable of running robustly in 3d domains of a full ice shelf cavity demonstrated by simulations of the isomip test case our simulation results compare favourably when run alongside mitgcm and the melt rates are consistent with the spread found in the literature asay davis et al 2016 zhou and hattermann 2020 gwyther et al 2020 we find melt rates are highly sensitive to grid resolution as has been observed before ultimately the balance between specified mixing and spurious numerical mixing dictates how sensitive the melt rate is to changes in the grid resolution and discretisation choice our study emphasises the importance of running ice ocean cavity simulations at multiple resolutions to gauge this sensitivity as it will always be problem specific as discussed in section 3 2 given the assumption of fixed diffusivity and viscosity close to the boundary finding a melt rate that is insensitive to grid resolution using the existing melt rate parameterisation may not be directly applicable to real ice shelf cavity environments a detailed investigation into the melt parameterisation is beyond the scope of this paper and developing a new more robust melt parameterisation for large scale applications is not our immediate aim studies of the ice ocean boundary layer are underway that use a hierarchy of models from direct numerical simulations middleton et al 2021 couston et al 2021 to large eddy simulations vreugdenhil and taylor 2019 begeman et al 2022 vreugdenhil et al 2022 as well as theoretical models jenkins 2016 2021 it is likely that these combined approaches will be needed to develop a parameterisation that is consistent with the physics explicitly resolved by the numerical model and the model s own mixing schemes and is applicable to the unique environment of the ice shelf ocean boundary layer where the effects of stratification and basal slope are important potentially a model like the one implemented here using firedrake with flexible grids in the horizontal and vertical will be a useful tool to bridge the gap between high resolution models of boundary flow and the necessarily coarser regional and global scale ocean models ultimately model validation will remain a challenge until more direct observations of sub ice shelf ocean conditions are made in tandem with measurements of melt rate adjoint models are powerful tools to assess model uncertainties and incorporate observations into numerical ocean models errico 1997 heimbach and losch 2012 goldberg et al 2020 nakayama et al 2021b we showed preliminary adjoint sensitivity results for an idealised grounding zone domain which clearly showed a reverse ice pump mechanism the link between buoyancy and melt rate also suggests the importance of subglacial outflow into the cavity the sensitivity of melt rate to vertical mixing especially at the grounding zone was also identified by the adjoint model this again suggests the importance of accurately modelling or accounting for mixing within ice shelf cavities we plan to implement additional turbulence closure schemes such as rans and les to investigate these effects with the caveat that the turbulence closure problem is inherently uncertain and thus challenging to model in future work we aim to apply the model to more complicated geometries in particular ocean flow in realistic grounding zone regions and the impact that basal crevasses have on the ocean flow we anticipate that the use of fully unstructured meshes will be necessary to resolve these features accurately we also intend to extend our adjoint capability to solve optimisation problems to help constrain unknown parameters such as turbulent mixing coefficients under the ice shelf based on observations credit authorship contribution statement william i scott conceptualization methodology software validation writing original draft visualization stephan c kramer conceptualization methodology software writing review editing supervision paul r holland conceptualization writing review editing supervision keith w nicholls conceptualization writing review editing supervision martin j siegert conceptualization writing review editing supervision matthew d piggott conceptualization writing review editing supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like to thank benjamin yeager for useful discussions and initial help setting up mitgcm simulations were carried out using the high performance cluster of the research computing service at imperial college london we would like to thank xylar asay davies and one other anonymous reviewer for their constructive comments that helped to improve this paper this work was supported by the natural environment research council ne s007415 1 and ne g018391 1 this work is also from the melt project a component of the international thwaites glacier collaboration itgc support from national science foundation nsf grant nsf plr 1739003 and natural environment research council nerc grant ne s006656 1 and ne s006427 1 logistics provided by nsf u s antarctic program and nerc british antarctic survey itgc contribution no itgc 097 this work was also supported by the engineering and physical sciences research council ep r029423 1 code availability the code is available at https github com thwaitesproject thwaites git it has also been stored in this zenodo repository https doi org 10 5281 zenodo 7584700 along with instructions for running the examples from section 3 
23791,numerical studies of ice flow have consistently identified the grounding zone of outlet glaciers and ice streams the region where ice starts to float as crucial for predicting the rate of grounded ice loss to the ocean owing to the extreme environments and difficulty of access to ocean cavities beneath ice shelves field observations are rare estimates of melt rates derived from satellites are also difficult to make near grounding zones with confidence therefore numerical ocean models are important tools to investigate these critical and remote regions the relative inflexibility of structured grid models means however that they can struggle to resolve these processes in irregular cavity geometries near grounding zones to help solve this issue we present a new nonhydrostatic unstructured mesh model for flow under ice shelves built using the firedrake finite element framework we demonstrate our ability to simulate full ice shelf cavity domains using the community standard isomip ocean0 test case and compare our results against those obtained with the popular mitgcm model good agreement is found between the two models despite their use of different discretisation schemes and the sensitivity of the melt rate parameterisation to grid resolution verification tests based on the method of manufactured solutions mms show that the new model discretisation is sound and second order accurate a main driver behind using firedrake is the availability of an automatically generated adjoint model our first adjoint calculations of sensitivities of melt rate with respect to different inputs in an idealised grounding zone domain are promising and point to the ability to address a number of important questions on ocean influence on ice shelf vulnerability in the future keywords fem ice shelf cavities firedrake adjoint verification mms data availability no data was used for the research described in the article 1 introduction recent observations have shown that outlet glaciers and ice streams in west antarctica are retreating at an alarming rate mouginot et al 2014 scambos et al 2017 in total there is enough ice in west antarctica to raise sea level by approximately 3 5 m fretwell et al 2013 evidently this has serious implications for communities that live in low lying coastal regions there are still major uncertainties however in predictions of how much ice could be lost and importantly on what time scales this may occur increased basal melting beneath ice shelves due to increased ocean heat is thought to be the main cause for ice loss in west antarctica rignot et al 2013 wherever ice shelves are topographically confined they provide buttressing forces to the grounded glaciers shepherd et al 2018 when ice shelves thin a decrease in buttressing can lead to an increase in ice flow speeds and hence the rate at which grounded ice is lost increases predicting sea level rise is a pressing issue but there are also questions about how changes in melt water flux to the ocean can affect global ocean dynamics dinniman et al 2016 improving understanding of ocean processes beneath ice shelves and their impact on ice shelf vulnerability is therefore an important problem in glaciology and climate science numerical models of ocean flow beneath ice shelves play a key role in our understanding of ocean cavities since direct observations are so limited the model needs to resolve the interaction between salinity temperature and the flow along with the thermodynamics at the complex sloping ice ocean interface that characterise ice shelf cavity environments melting ice injects buoyant fresh water into the domain which in turn drives overturning ocean circulation within the cavity the freezing point of water decreases with depth so in some cases ice that melts at depth in the cavity can refreeze closer to the ocean surface thus transporting ice up the underside of the ice shelf this is known as the ice pump mechanism lewis and perkin 1986 jenkins and bombosch 1995 in coastal regions where changes in coastlines and bathymetry can have important effects on dynamics horizontally unstructured grid models have proven to be extremely useful tools kärnä et al 2018 in an ice shelf cavity setting this is doubly important since the base of the ice shelf can vary as much as the seafloor with the formation of basal crevasses and channels that evolve with the ocean and indeed ice flow the lack of flexibility in traditional structured grid ocean models in resolving these features means that there is still large uncertainty in flow dynamics beneath ice shelves dinniman et al 2016 this is compounded by the fact that results using the commonly employed three equation melt parameterisation depend significantly on vertical resolution and implicitly on the model s choice of vertical discretisation gwyther et al 2020 an issue that will be explored further in section 3 2 satellite measurements indicate that spatial melt rate patterns near grounding zones the region separating the grounded ice sheet from the floating ice shelf are highly variable milillo et al 2019 this implies that there must be a complex combination of processes in the grounding zone that need to be resolved to accurately model melt rates due to the limitations on grid resolution imposed by structured grids used by traditional ocean models it has not been possible to investigate in detail melting at grounding zones ensemble simulations from glacial flow models have shown that the largest uncertainties in projected ice loss come from estimating melt rates at the grounding zone arthern and williams 2017 similarly goldberg et al 2019 used an ice flow model with an associated adjoint model to investigate how sensitive the volume of ice loss was to spatially varying melt for dotson and crosson ice shelves again they found that ice loss was most sensitive to melting at the grounding zone an extension of this study to the wider amundsen sea embayment confirmed the high sensitivity of ice loss to melting at the grounding zone morlighem et al 2021 therefore accurately simulating ocean dynamics near the grounding zone seems to be one of the most important tasks required to help reduce uncertainty in ice loss estimates and consequently reduce uncertainty in sea level projections the finite element method is particularly well suited to solving problems in complicated domains elman et al 2014 the power of the finite element method comes from being able to define the solution using piecewise functions which can be defined on completely arbitrary meshes this is ideal for modelling flow in complicated domains such as ice shelf cavities a fully unstructured finite element ocean model fluidity piggott et al 2008 was previously adapted to enable simulation of ice shelf cavities kimura et al 2013 due to its ability to run on fully unstructured grids fluidity was able to resolve all the key features of an ice shelf cavity pinching grounding lines sloping ice base and the steep vertical ice front kimura et al 2013 as well as investigate flow within basal crevasses jordan et al 2014 the model presented here is similarly motivated by the applicability of finite elements on unstructured meshes and the need to retain a full representation of physics valid at order one aspect ratios for buoyancy driven flows in domains with complicated geometries such as ice shelf cavities a significant departure from previous work is use here of the firedrake toolkit to implement the underlying discretisation of the finite element method rathgeber et al 2016 the philosophy behind firedrake has several advantages by separating the underlying software implementation from the physics of the system it enables a firedrake user to quickly develop and test new discretisation methods on the problem in question the syntax of the unified form language ufl firedrake employs is designed to closely mimic the mathematical description of the weak form prescribing the finite element discretisation alnæs et al 2014 this combined with the readability of the python programming language helps to improve the longevity of the code by reducing the learning curve require for new users before they can start using and implementing features in the model icepack a glacier flow modelling package built on top of firedrake is a good example of this shapero et al 2021 the firedrake framework automatically translates this high level mathematical description of the numerics into highly optimised low level c code which leads to an efficient implementation of the finite element method in addition firedrake uses the portable extensible toolkit for scientific computation petsc a state of the art library to efficiently solve the systems that arise from the finite element discretisation balay et al 1997 there are already ocean models that use horizontally unstructured grids to investigate ice shelf cavities such as fesom timmermann et al 2012 fvcom zhou and hattermann 2020 and mpas o ringler et al 2013 gwyther et al 2020 these models typically retain a distinct difference in how they deal with horizontal and vertical motion in the model in part this means making the hydrostatic approximation provided the horizontal length scales are much larger than the vertical the vertical acceleration terms can be neglected which reduces the computational cost this approximation is evidently true for the open ocean when typically the meshes used for simulations have horizontally stretched anisotropic grids to investigate interesting basal features under ice shelves though such as crevasses jordan et al 2014 or in the proximity of topographically complex grounding zone regions one may want to have more flexibility by using meshes where the aspect ratio of the grid becomes close to order one i e the horizontal grid sizes are comparable to the vertical grid cell sizes in this case the hydrostatic approximation may no longer be valid therefore we include the nonhydrostatic vertical acceleration terms as part of the solve moreover by treating the horizontal and vertical equations on the same footing our model has been designed so that it can easily incorporate isotropic turbulence closure schemes from the cfd literature similar to work carried out by yeager 2018 whilst at the same time able to transition to more traditional gfd turbulence closures in high aspect ratio domains where vertical and horizontal mixing is parameterised separately probably the main advantage of using a framework such as firedrake or similar projects such as fenics alnæs et al 2015 is the fact that it readily facilitates the use of an automatically generated adjoint model thanks to the work of the dolfin adjoint project mitusch et al 2019 adjoint modelling represents an efficient means to calculate gradients of model output functionals with respect to model inputs errico 1997 by solving the adjoint system the gradient of an output functional can be calculated with respect to any number of input parameters independent of the number of these input parameters adjoint modelling is often the only tractable method for calculating gradient information for numerical models that rely on grid based discretisations like ocean models because the number of input parameters scales with the grid resolution errico 1997 early implementations of adjoint models were developed by hand separately from forward models kalnay 2003 as the complexity of the forward code develops this approach becomes time consuming and error prone a more sophisticated method uses what is sometimes referred to as automatic differentiation ad of the forward code by using the chain rule on each line of forward code repeatedly to generate an adjoint model errico 1997 usually careful intervention by the user is required during development and maintenance especially with regards to ensuring the computational efficiency of the adjoint model hence it is common to refer to this process as algorithmic not automatic differentiation the approach implemented by the dolfin adjoint project makes use of the high level symbolic representation of the discrete mathematical problem written in ufl mitusch et al 2019 since the mathematical problem is separate from its software implementation provided the mathematical form of the equations is differentiable then the process is automatic the adjoint model is consistent with the forward discretisation and automatically inherits the solver strategies and the parallelisability of the forward model mitusch et al 2019 sensitivities of a functional with respect to different inputs are very useful in their own right errico 1997 in a time dependent forward model the equivalent adjoint model propagates sensitivities backwards through time from the final time to the initial time this complimentary information can be very useful when trying to understand modelled processes that may not be obvious from considering the forward model alone gradient information is also very useful for optimisation problems in particular parameter estimation and data assimilation in many ocean modelling studies there are many parameters that are unknown but can be constrained by observations gradient information can be used to update these parameters in an efficient manner by minimising a functional that measures the error between observed and modelled values there have been a limited number of studies of an ocean adjoint in an ice shelf cavity context the first used the mitgcm ocean model to simulate the pine island ice shelf cavity heimbach and losch 2012 goldberg et al 2020 extended mitgcm s adjoint model such that it was able to calculate the sensitivity of melt rate to bathymetry notably this was using the open source ad tool openad utke et al 2008 most recently the ecco2 assimilation framework which also relies on mitgcm s adjoint capability was applied to the amundsen and bellingshausen seas and ice shelf cavities nakayama et al 2021b since all the previous adjoint modelling work for ice shelf ocean cavities have used the mitgcm framework they inherit the relative inflexibility of mitgcm s structured grid although mitgcm s resolution can vary in space d x can vary horizontally d z is limited to varying with depth this has limited practical application for ice shelf cavities since the ocean cavity thickness varies significantly from the grounding zone to the open ocean we see our model which can combine flexible unstructured meshes with the adjoint capability as a tool with significant potential for investigating ocean conditions in the complex domains that are ice shelf cavities the outline for the rest of this paper is as follows section 2 covers the model discretisation choices in section 3 1 the accuracy of the second order discretisation is verified using a method of manufactured solution based test which includes melting we demonstrate the resulting model s capability to run simulations for the 3d isomip test case in section 3 2 and compare results with the mitgcm ocean model this includes an investigation into the sensitivity of the melt parameterisation to vertical mixing and resolution choice finally we present preliminary adjoint sensitivity results in an idealised domain in section 3 3 these steps are necessary to prove the capabilities and accuracy of the model so that future work can tackle the challenging problem of simulating ocean dynamics and melt rates near grounding zones 2 model description 2 1 model equations as emphasised in section 1 one of the key motivations is to develop a model with the ability to treat vertical dynamics in the same way as horizontal dynamics when the mesh cells become close to isotropic on fully unstructured meshes this means we solve the full incompressible navier stokes equations for velocity and pressure and we do not make the hydrostatic approximation conservation of momentum in strong form for a rotating fluid under the boussinesq approximation can be written as 1 u t u u 2 ω u 1 ρ 0 p τ f where 2 τ ν u u t the rotation vector of the earth is given by ω and in the applications presented in this work coriolis is implemented using an f plane assumption with f 1 409 1 0 4 s 1 corresponding to a latitude of 75 s vallis 2017 ρ 0 is the reference density of sea water taken as 1027 51 kg m 3 following asay davis et al 2016 note table 1 summarises values for general model parameters kinematic eddy viscosity ν is a rank two tensor and can be spatially variant f is a generic vector body force the velocity u u v w t and pressure p can be solved for using the continuity equation as a constraint to enforce the incompressibility condition 3 u 0 in this work we have not modified the continuity equation to allow for a free surface implementation the top surface of the model is treated as a rigid lid flows with more complicated tidal forcing may require a free surface to accurately capture the dynamics though and we anticipate implementing this feature in future work to complete the description of the problem boundary conditions need to be imposed which are given as follows 4 u u d u n n u t t on γ d 5 u n u n τ n t f tang on γ d n 6 ρ 0 τ n n p p ext τ n t 0 on γ p ext unit normal and tangential vectors are given by n and t respectively on γ d dirichlet boundary conditions for both the normal and tangential components of velocity u n and u t are specified this type of boundary can be used for rigid walls by setting u n u t 0 for no normal no slip conditions although these boundaries are not used in this work they are included for completeness to ensure the correctness of the weak forms in section 2 2 in this work all walls are represented using γ d n boundaries on these boundaries only the normal component of velocity is specified and is combined with a condition for the shear stress in the tangential direction 7 τ n t n τ t in this work all side walls are free slip boundaries these are imposed by setting f tang and u n to zero on the top and bottom boundaries namely the floating ice shelf and the seabed we apply quadratic wall drag 8 f tang c d u u t where c d is a drag coefficient for boundaries γ p ext only an external pressure is specified in 6 with τ n n the normal component of the stress vector on the boundary given by 9 τ n n n τ n although we do not consider these types of boundaries in this paper we include their formulation for completeness to ensure that the surface integrals in the weak forms of section 2 2 are consistent these types of boundaries are necessary in open domains where an external hydrostatic pressure field is applied to balance the stress on either side of the open boundary thus preventing fluid from falling out of the domain to incorporate buoyancy effects we make the boussinesq approximation where perturbations in density are neglected in all terms except the buoyancy term which is implemented as a source f where 10 f ρ ρ 0 g k where g 9 81 m s 2 is the gravitational acceleration and k is the unit vector pointing in the vertical direction ρ is a density perturbation related to temperature t and salinity s by a linear equation of state given by 11 ρ ρ 0 α t t t 0 β s s s 0 where α t 3 733 1 0 5 c 1 and β s 7 843 1 0 4 are respectively the expansion and contraction coefficients for temperature and salinity t 0 1 c and s 0 34 2 are reference temperature and salinity values used in the linear equation of state again with values from asay davis et al 2016 note that a constant hydrostatic pressure term has been subtracted from the momentum equations and incorporated into the definition of pressure so that p is a perturbation pressure 12 p p p hyd where p is the full pressure and p hyd is given by 13 p hyd ρ 0 g z and z is the vertical coordinate temperature and salinity are governed by scalar advection diffusion equations which can be written in strong form as 14 t t u t κ t t r t t res t and 15 s t u s κ s s r s s res s where κ t and κ s are the eddy diffusivity of temperature and salinity respectively these are spatially variant rank two tensors similar to viscosity r t and r s represent restoring frequencies when relaxing the solution in a sponge region to specified values t res and s res boundary conditions for temperature and salinity are given as 16 t t d on γ d t 17 κ t t n φ t on γ φ t 18 s s d on γ d s 19 κ s s n φ s on γ φ s where t d and s d are the dirichlet boundary values for temperature and salinity respectively on walls γ d t and γ d s given in 16 and 18 neumann boundary fluxes for temperature 17 and salinity 19 are given by φ t and φ s respectively note the definition of φ includes diffusivity so if the tracer is temperature the units of φ t would be m k s the next section gives the weak forms of 1 3 14 and 15 necessary for the finite element discretisation 2 2 weak form of model equations 2 2 1 tracers choosing a suitable scalar function space q for a generic tracer q and a test function ϕ the weak form of the advection diffusion equation 14 and 15 can be written as 20 m q a q k f q s ϕ q where 21 m q ω q t ϕ d x 22 a q ω q ϕ u d x γ ϕ q u n d s 23 k ω ϕ κ q d x γ d q ϕ n κ q d s γ φ q ϕ φ q d s 24 f q ω f ϕ d x 25 s ω α res q ϕ d x where the individual terms are denoted m q time derivative a q advection k eddy diffusivity f q source and s sink dirichlet boundary values for the tracer are denoted q d and neumann boundary conditions as φ q note q is the advected scalar value defined in section 2 4 in the context of surface integrals and discontinuous galerkin dg finite elements 2 2 2 velocity pressure to derive the weak forms of 1 and 3 we choose separate suitable function spaces v and w for velocity u u v w t and pressure p respectively multiplying 1 by a vector test function ϕ ϕ u ϕ v ϕ w t v and integrating over the domain ω gives 26 m a c p v f ϕ v where 27 m ω u t ϕ d x 28 a ω u u ϕ d x γ u ϕ u n d s 29 c ω f v ϕ u f u ϕ v d x 30 p ω 1 ρ 0 ϕ p d x γ p ext 1 ρ 0 p p ext ϕ n d s 31 v ω ϕ τ d x γ d γ d n ϕ n τ d s γ d n n τ t ϕ t d s γ d n f tang ϕ t d s 32 f ω f ϕ d x the individual terms being m time derivative a advection c rotation p pressure gradient v eddy viscosity and f source note u similar to q is an advected velocity also defined in section 2 4 the symbol represents an outer product between two vector quantities multiplying 3 by a test function ψ w and integrating by parts gives the weak form of the continuity equation 33 ω ψ u d x γ p ext ψ u n d s γ d γ d n ψ u n d s 0 ψ w defining ψ in the pressure space ensures that the discretised divergence and gradient operators are the negative transpose of each other and it means there are the same number of discrete continuity equations as pressure degrees of freedom this is natural since the pressure can be thought of as a lagrange multiplier that enforces the incompressibility constraint strang 2007 2 3 finite element discretisation fully unstructured and extruded meshes 2 3 1 tracers on fully unstructured meshes 2d triangle elements 3d tetrahedral elements we use linear discontinuous basis functions p1dg for the tracer test and trial space q this is because p1dg elements are well suited to advection dominated problems kärnä et al 2018 for high aspect ratio domains where the horizontal extent of the domain is much greater than the depth for instance when simulating an entire ice shelf cavity an alternative option to using a fully unstructured mesh is to make use of firedrake s in built extruded mesh feature extruded meshes offer increased performance and robustness when elements are highly anisotropic yet retain the flexibility of a horizontally unstructured mesh bercea et al 2016 mcrae et al 2016 although we do not carry out 2d vertical slice modelling on extruded meshes in this paper we describe the 2d case to help build intuition for the 3d case which is used in section 3 2 an extruded 2d vertical slice mesh is made up of quadrilateral elements which can be thought of as the tensor product of an element interval in the horizontal and an element interval in the vertical for 3d domains the elements are triangular prisms which are formed by the tensor product of a triangular element in the horizontal with an interval element in the vertical this means the horizontal part of the mesh can be fully unstructured in the vertical direction the mesh is structured it is possible to choose between a fixed number of columns in the vertical or a fixed cell height as well as shaving cells in the top and bottom columns to accurately fit the domain geometry finite elements on extruded meshes are defined in terms of a tensor product of a finite element on the horizontal mesh and finite element on the vertical mesh this approach controls how the element behaves in the horizontal direction and how the element behaves in the vertical this is particularly useful for more exotic element combinations that have different continuity requirements in the horizontal and vertical directions on extruded meshes we choose the scalar finite element which mimics the p1dg element this is formed by taking the tensor product of p1dg on the horizontal element with p1dg on the vertical interval 2 3 2 velocity pressure the choice of a suitable finite element velocity pressure element pair is crucial to ensuring an accurate and stable discretisation of the continuous equations elman et al 2014 on fully unstructured meshes we use vector p1dg basis functions for the velocity space v and quadratic continuous basis functions p2 for the pressure space w the resulting p1dg p2 velocity pressure element pair has been used with the finite element ocean model fluidity kramer et al 2010 kimura et al 2013 jordan et al 2014 and more recently implemented within the thetis coastally focused ocean model developed using firedrake kärnä et al 2018 wallwork et al 2020 the element pair has been shown to maintain geostrophic balance because the skew gradient of the pressure maps to the velocity space cotter et al 2009b a cotter and ham 2011 on extruded meshes we have chosen an element pair that has similar properties to p1dg p2 so that as the horizontal mesh is refined the discretisation is able to deal with order one aspect ratios defining the equivalent of p2 on extruded meshes is relatively straightforward it is the tensor product of a p2 horizontal element with the equivalent p2 element on the vertical interval as shown on the left side of fig 1 forming the required velocity element on an extruded mesh is more complicated though our solver strategy outlined in section 2 5 relies on the gradient of functions in the pressure function space mapping to functions in the velocity trial space given our discretisation choice for pressure this places a constraint on the continuity of the velocity element along the element boundary the gradient of the pressure is continuous across cells in the tangential direction and varies quadratically in the normal direction the pressure gradient is discontinuous between cells and varies linearly for a 2d vertical slice the horizontal component of this velocity element u is formed by taking the tensor product of p1dg on a horizontal interval and p2 on a vertical interval this ensures that the horizontal component of velocity is discontinuous in the horizontal direction but continuous between elements on vertical facets the vertical component of the velocity w is defined the other way round as a tensor product of p2 in the horizontal and p1dg in the vertical again this ensures that the vertical velocity is continuous in the horizontal direction but discontinuous in the vertical direction the 3d equivalent of the 2d velocity element is built up as follows the horizontal velocity components u and v are formed by taking the tensor product of a nedelec element of the second kind n2 defined on a triangular element with a p2 element in the vertical n2 elements are inherently 2d vector elements that are tangentially continuous between cells and discontinuous in the normal direction kirby et al 2012 the middle panel of fig 1 shows this element discretising the horizontal velocity components the vertical component of velocity is defined in the same way as on the 2d vertical slice mesh with a tensor product of p2 in the horizontal and p1dg in the vertical and is shown on the right hand side of fig 1 similarly fig 2 from section 2 6 detailing the melt parameterisation shows a 2d vertical schematic representation of the p1dg discretisation on the fully unstructured mesh versus the tensor product discretisation on the vertically structured extruded mesh 2 4 discontinuous galerkin surface integrals various terms in the weak form have been integrated by parts resulting in additional surface integrals which can be exploited to enforce boundary conditions weakly with continuous finite element basis functions contributions from either side of the same internal boundary between elements cancel this means only surface integrals defined on exterior boundaries need to be considered in the discontinuous galerkin dg method however the surface contributions from either side of the internal boundaries do not cancel because of the jump in the solution value across the boundary for the advection term in 28 we choose the advected velocity u to be the upwind value on an exterior outflow boundary this simply means u is the solution inside the domain and on inflow boundaries u is set using the dirichlet values at the boundary in the interior the solution fields on either side of a facet between two adjacent elements are arbitrarily labelled with and averages across interfaces are denoted by while denotes a jump across an interface with unit normal n integrals over interior facets are given by γ int d s using this notation the additional term for dg advection is 34 γ int u ϕ u n d s where the upwind velocity u is given by 35 u u if u n 0 36 u u if u n 0 the viscosity term is discretised using the symmetric interior penalty galerkin sipg method epshteyn and rivière 2007 after integrating by parts the discontinuous velocity field leads to an interior facet term 37 γ int ϕ n ν u u t d s adding only this term to 31 would not give a stable scheme instead two additional terms are included which ensure that the form of the diffusivity term is symmetric positive definite given by 38 γ int u n u n t ν ϕ d s γ int σ ϕ n ν u n u n t d s the first term in 38 makes the scheme symmetric the second term is a penalty term penalising jumps in the solution to ensure that the operator remains coercive which is required for stability eqs 3 20 and 3 23 from hillewaert 2013 give a minimal value for the penalty parameter σ as 39 σ α c p n e a f v e where n e is the number of facets for the element type α is a constant the theoretical minimum is 1 for stability but after experimentation we found using a value of 2 improves robustness a f is the area of the facet v e is the volume of the element eq 3 7 from hillewaert 2013 defines c as a constant that depends on polynomial degree p and element type such that 40 f u 2 d s c a f v e e u 2 d x is true for all polynomials u of degree p values for c are given in table 3 1 from hillewaert 2013 the additional sipg terms are also added on exterior surfaces 41 γ d u u d n u u d n t ν ϕ d s γ d 2 σ ϕ n ν u u d n u u d n t d s scalar advection is treated similarly to momentum with the additional interior facet term due to the discontinuous finite element space given by 42 γ int ϕ u n q d s where the upwind value of the tracer q is defined as 43 q q if u n 0 44 q q if u n 0 on dirichlet boundaries q is again only replaced if the flow is into the domain scalar diffusion in 23 is also implemented using the sipg formulation where the additional terms are given by 45 γ int ϕ n κ q d s γ int q n κ ϕ d s γ int σ ϕ n κ q n d s γ d q q q d n κ ϕ d s γ d q 2 σ ϕ n κ q q d n d s and similar interpretations of each term can be made as for momentum viscosity in 31 2 5 timestepping and solver strategy as outlined in the previous section 26 20 and 33 are the conservation of momentum the incompressibility constraint and advection diffusion equations and together they form a coupled system that needs to be solved for velocity pressure temperature and salinity as stated before we are solving for the full nonhydrostatic dynamics and we do not carry out any mode splitting to separate the barotropic and baroclinic components of the flow first we solve for the velocity and pressure using 26 and 33 by the established pressure projection approach chorin 1967 kramer et al 2010 which splits the solve into two parts there is an initial prediction step to find an intermediate velocity where the momentum equation is solved using an implicit backward euler timestep for all the terms in the momentum equation except the pressure p n which uses the value from the previous timestep the nonlinear advection and drag terms are linearised by setting the advecting velocity to the velocity value from the previous time step this intermediate velocity u int is not divergence free however so a correction step enforces the incompressibility constraint this finds the updated pressure p n 1 needed to obtain a divergence free velocity field u n 1 this correction step takes the form of a time splitting step 46 u n 1 u int δ t p n 1 p n which is discretised in the same way as the m and p terms in 26 coupled with an incompressibility constraint eq 33 for u n 1 firedrake provides an interface to choose solver options to pass to petsc the package that implements the solver routines balay et al 1997 typically we choose the gmres krylov subspace method to solve the momentum equation during the prediction step because the matrix is not symmetric it is preconditioned with an algebraic multigrid preconditioner such as petsc s default geometric algebraic multigrid gamg or boomeramg from the hypre suite henson and yang 2002 the block structure of the coupled correction solve is simpler than the full navier stokes system 26 and 33 because the matrix that arises in the top left hand block is only a mass matrix resulting from the time derivative term in 27 where in the full system it would contain contributions from all the other terms in the momentum equation because of the careful choice of velocity pressure finite element function spaces see section 2 3 2 where the gradient of pressure maps pointwise exactly into the velocity function space it can be shown that the schur complement of this system is exactly equivalent to a standard continuous galerkin discretisation of the pressure poisson equation cotter and ham 2011 this poisson equation is solved using the conjugate gradient cg algorithm preconditioned with an algebraic multigrid the efficiency of this solution strategy is highly dependent on the condition number for an anisotropic domain this is proportional to the square of the aspect ratio of the grid cells kramer et al 2010 thus as the aspect ratio goes to infinity the condition number can become unbounded with negative implications for the convergence of the solve however by preconditioning the cg algorithm with a preconditioner that approximates the solution to the depth averaged equations convergence can be made independent of the aspect ratio this so called vertical lumping approach kramer et al 2010 is similar to the idea first used in mitgcm and ensures that when the dynamics are close to a hydrostatic state the solve is as fast as when only solving the hydrostatic equations marshall et al 1997 for parallel efficiency the method does require cells to be aligned in columns in the vertical direction this can be achieved using firedrake s in built extruded mesh feature in combination with the tensor product discretisation discussed in section 2 3 1 we have found that when running 3d simulations on structured tetrahedral meshes on aspect ratios larger than 1 0 2 e g dx dy 2 km dz 20 m using the generic algebraic multigrid preconditioner gamg the wall clock time needed to solve one timestep of the model was dominated by the pressure correction solve however on extruded meshes with the vertical lumping preconditioner the pressure correction solve is faster than the momentum solve for the intermediate velocity after solving for velocity and pressure we solve for temperature and salinity individually we use a 3 stage diagonally implicit runge kutta dirk time stepping method ascher et al 1995 for the temperature and salinity advection diffusion equations 20 the method is third order accurate it is also an l stable scheme so it is suitable for stiff problems we apply a vertex based slope limiter after each time step to prevent the solution from becoming unbounded kuzmin 2010 we have found that using a slope limiter is only necessary in the region of the ice front transition which tends to have the fastest flow as the buoyant melt water plume accelerates up the ice front without the limiter a spurious freezing signal can occur confined to the final grid cell before the ice front transition with the limiter however freezing does not take place although computational performance and parallel scalability are not directly considered in this paper we expect our p1dg finite element based method with implicit time stepping to be about an order of magnitude slower than a conventional low order possibly explicit finite volume model for the same model domain and grid resolution the ability to take longer timesteps can offset this more importantly by using a fully flexible meshing strategy we are able to represent small scale features in a large domain that would be impossible to represent with structured grids typical of other ocean models we have run firedrake simulations of ocean circulation beneath ice shelves in parallel with up to 128 cores the reader is referred to the main firedrake paper for a more in depth study of performance rathgeber et al 2016 future work will investigate the performance of this ocean model when it is applied to more demanding geometrically complex 3d domains that require a fully flexible unstructured mesh 2 6 melt parameterisation melt rates are calculated based on conservation of heat and salt at the ice ocean boundary and under the assumption that the boundary is at the freezing point commonly referred to as the three equation melt parameterisation hellmer and olbers 1989 holland and jenkins 1999 dinniman et al 2016 conservation of heat is expressed as 47 q i t q w t q latent t where the latent heat term q latent t is given by 48 q latent t ρ 0 w b l f with w b the unknown melt rate melting w b 0 freezing w b 0 and l f 3 34 1 0 5 j kg 1 is the latent heat of fusion the conductive heat flux into the ice q i t is 49 q i t ρ i c p i w b t i t b where ρ i 920 kg m 3 is the density of ice c p i 2000 j kg 1 k 1 is the specific heat capacity of ice t i is the far field ice temperature taken to be 20 c and t b is the unknown temperature at the ice ocean boundary the turbulent heat flux from the ocean to the ice q w t is given by 50 q w t ρ 0 c p γ t t b t w where c p 3974 j kg 1 k 1 is the specific heat capacity of water and γ t the turbulent thermal exchange velocity commonly γ t is expressed as γ t u where γ t is a dimensionless constant and u c d u is the friction velocity asay davis et al 2016 we use the velocity at the top surface of the domain to calculate u in effect making the assumption that our computational boundary is in the surface layer where rotation effects are unimportant which may not be valid in the presence of strong stratification mcphee 2008 t w is defined by holland and jenkins 1999 as the temperature in the mixed layer at the edge of the turbulent ice ocean boundary layer in most ocean models this is either taken as the value of the temperature in the top model cell adjacent to the boundary or temperature value representative of some distance from the ice ocean interface gwyther et al 2020 we take t w as the model temperature value at the computational boundary this is perhaps an uncommon choice compared with other ocean models dinniman et al 2016 unless doing direct numerical simulation dns which is prohibitively expensive for ice ocean simulations in more than metre scale domains couston et al 2021 the scalar molecular and turbulence boundary layer will not be modelled explicitly it must be accounted for with a parameterisation since we are unable to model these effects explicitly we choose to imagine that the actual ice ocean interface is offset slightly beyond the computational domain which is similar to the approach taken for the parameterisation of wall regions in reynolds averaged navier stokes rans turbulence models yeager 2018 fig 2 shows a schematic of the melt parameterisation while the finite element solution is valid at any point within each element and this means it is possible to offset where the tracer is sampled towards the interior of the domain as shown by kimura et al 2013 this could cause performance issues when running the model in parallel because the sampled tracer value may not be stored on the same processor the conservation of salt is similar to heat and is given by 51 q i s q w s q brine s where the fresh water melt flux q brine s is given by 52 q b r i n e s ρ 0 w b s i s b and where s i is the salinity of the ice which is taken to be zero and s b is the unknown salinity at the ice base the turbulent salt flux across the ocean boundary layer q w s can be written as 53 q w s ρ 0 γ s s b s w where γ s is the turbulent salinity exchange velocity analogous to γ t and as before we take s w as the value of salinity at the edge of the computational domain offset from the ice ocean boundary the diffusive flux of salt into the ice q i s is assumed to be zero the ice ocean boundary temperature t b is assumed to be at the freezing point 54 t b a s b b c p b where a 5 73 1 0 2 c b 8 32 1 0 2 c and c 7 53 1 0 8 c pa 1 are the coefficients obtained from linearisation of the nonlinear freezing point equation millero 1978 using the values from asay davis et al 2016 and p b is the pressure at the boundary this means there are now three equations 47 51 and 54 to solve for the three unknowns w b s b and t b the melt rate and the salinity and temperature values at the boundary respectively rearranging the equations gives a quadratic equation for s b and the positive value is taken as the solution by using the form of the heat and salt flux from 50 and 53 robin boundary conditions for temperature and salinity can be imposed as shown in fig 2 according to 55 φ t κ n t γ t w b t b t w and 56 φ s κ n s γ s w b s b s w where φ t and φ s are substituted into the third term of 23 since the ice ocean boundary is not a material interface we include the melt water correction term accounting for the advection of melt water with water mass properties corresponding to values at the ice ocean interface into the ocean domain as part of the temperature and salt flux boundary conditions following jenkins et al 2001 2 7 model implementation we have built a python module on top of firedrake to facilitate simulations of ice shelf cavities the core part of our python package is made up of 11 modules we make use of python classes to build up the complexity of the code in blocks each equation inherits useful attributes from a baseequation i e a method for summing the residuals from each term and each equation is made up of terms effectively each term in 20 and 26 themselves based on a baseterm which contains the required features of a term i e a method to evaluate the residual form for the term this approach leads to a compact codebase aids with debugging as individual pieces of the code can be isolated and tested but retains the flexible nature of firedrake which is one of the main reasons for using the software the code is available at https github com thwaitesproject thwaites git it has also been stored in this zenodo repository https doi org 10 5281 zenodo 7584700 along with instructions for running the examples from section 3 the next section presents a simplified verification test applicable to ice ocean models which will hopefully prove of value for other models to ensure that the code is solving the model equations correctly and consistently 3 results 3 1 method of manufactured solution mms tests the method of manufactured solutions mms is a rigorous method for verifying that the numerical scheme as implemented is solving the model equations accurately and consistently farrell et al 2011 roache 1998 this is a separate problem to that of validating whether the model including the choice of underlying equations yields a good approximation to the real system such a validation requires comparison with observations which is challenging for simulations of ice shelf cavity circulation the most reliable way to test whether a numerical approximation is consistent is to ensure that the solution error relative to a known solution decreases at the correct order as the grid is refined salari and knupp 2000 however finding analytical solutions to systems of coupled partial differential equation pde that arise in fluid dynamics is notoriously difficult and only possible for simple geometrical configurations and often simplified boundary conditions bcs mms removes much of this difficulty instead of solving the pde system to find the solution the analytical form of the solution is chosen to begin with solution fields are constructed from smoothly varying differentiable functions often trigonometric and polynomial functions these functions are substituted into the strong form of the equations and the resulting residuals which are expected to be non zero as these functions could not be expected to represent an exact solution provide additional source terms that can be used to force a version of the simulation for which the chosen functions are exact solutions the chosen functions can be manufactured to meet specific constraints for example incompressibility of the flow field or specific bcs alternatively consistent bcs can be found directly from the chosen solutions simply by evaluating the solution at the boundary for the type of bc required for testing following the notation from farrell et al 2011 the error on a given mesh is 57 e h 1 c h 1 c p where c is a constant h 1 is the characteristic mesh size and c p is the order of accuracy of the numerical approximation the error on a finer mesh is 58 e h 2 c h 1 r c p where the new characteristic mesh size h 2 h 1 r by taking the ratio of the two errors the convergence rate can be found using 59 c p log r e h 1 e h 2 the mms solutions should be constructed to be as general as possible to ensure good coverage of the terms in the equations salari and knupp 2000 although this does not require the chosen solution fields to be realistic the solution we have chosen for this mms test is an overturning flow that is a crude representation of an ice pump mechanism for simplicity we have limited ourselves here to flow within a 2d vertical slice the domain is a square box with length l 100 m height h 100 m and with the bottom boundary at a depth d 1000 m the chosen solution for the horizontal velocity is 60 u u 0 x l cos π z d h h where u 0 is a characteristic velocity of 1 m s x and z are the horizontal and vertical coordinates as an example of the mms procedure fig 3 shows the finite element solution fields for the horizontal velocity component on the two coarsest grids alongside the corresponding error relative to the chosen solution the error on the finer mesh is noticeably reduced compared to the error on the coarser mesh despite the two solution fields being qualitatively similar the chosen solution for vertical velocity is 61 w u 0 h π l sin π z d h h this has been chosen to ensure that the flow is incompressible so that the continuity equation u 0 is satisfied for this simple flow configuration the vertical velocity does not depend on x the right hand side is specified as an open boundary and all other boundaries employ free slip conditions with no normal flow imposed the pressure field has been chosen to have homogeneous neumann boundaries p n 0 62 p p 0 cos π x l cos π z d h h p 0 is a constant set to 1 pa to ensure the dimensions remain consistent and the reference density ρ 0 is set to 1 kg m 3 for this test the temperature field is 63 t t c sin 4 π x l a t z 2 b t z c t where t c is 0 1 c a t is 3 89 1 0 4 c m 2 b t is 7 54 1 0 1 c m 1 and c t is 3 64 1 0 2 c the salinity field is 64 s s c cos 4 π x l a s z 2 b s z c s where s c is 0 345 a s is 1 44 1 0 4 m 2 b s is 2 81 1 0 1 m 1 and c s is 1 03 1 0 2 the viscosity and diffusivity are 1 m 2 s as already described the source terms are derived by substituting the solutions into the strong form of the equations the source term for velocity is thus given by 65 s u u t u u 1 ρ 0 p τ ρ ρ 0 g k note the coriolis term is not included because the domain is a 2d vertical slice the implementation of rotation has been verified against analytical solutions of ekman spirals not shown the source term for temperature is given by 66 s t t t u t κ t t the source term for salinity is given by 67 s s s t u s κ s s use of symbolic computation tools such as sympy meurer et al 2017 make this process relatively straightforward to implement for example the source term for the horizontal velocity component in this case is given by 68 s u p 0 π sin π x l cos π h d z h l ρ 0 u 0 2 x sin π h d z h 2 l 2 u 0 2 x cos π h d z h 2 l 2 π 2 ν u 0 x cos π h d z h h 2 l note we chose this term for its compactness because it does not include buoyancy terms dirichlet bcs for temperature and salinity are applied on the left and right boundaries with neumann conditions on the bottom boundary values for these boundary conditions are found in a similar manner to defining source terms the solution itself is substituted to give the exact form of the generally inhomogeneous boundary conditions salari and knupp 2000 particular attention is given to the boundary conditions at the ice ocean interface situated at the top of the domain as verifying the accuracy of the model implementation at this location is a key reason for carrying out the mms test the boundary conditions at the ice ocean interface are represented by robin boundary conditions which define the gradient of the temperature and salinity fields at the boundary as a function of the temperature and salinity at the edge of the computational domain see section 2 6 for more information the default boundary condition for the melt boundary is given by 55 and 56 which are imposed weakly in the third term of 23 for an mms test the temperature boundary condition applied at the top is given by 69 φ t κ n t a φ t h φ t a the first term in 69 represents the temperature gradient of the analytical temperature field t a found from 63 at the boundary with the eddy diffusivity as a multiplication factor as discussed previously this is the only term needed to implement neumann boundary conditions for conventional mms tests the second term φ t h is the flux boundary condition calculated by the melt parameterisation using the modelled temperature and salinity values equivalent to 55 and 56 the final term φ t a is the flux calculated by the melt parameterisation using the analytical temperature field provided the temperature salinity and velocity through u fields converge at the correct order then the melt rate error calculated using the exact temperature salinity and velocity fields should also converge at the correct order because the contribution from φ t h and φ t a cancel out this is a spatial convergence test so the solution fields have been chosen to be stationary farrell et al 2011 the simulation is spun up from rest backward euler is used for timestepping the momentum equations and the scalar equations the timestep is initially set to 4 m u 0 n x chosen for robustness in the initial spin up where n x is the number of cells in the x direction the timestep is increased to l u 0 n x after 100 time steps for convenience table 2 summarises parameters and constants used for the mms convergence test velocity and tracer fields are discretised with p1dg elements and so second order convergence is expected which means that halving the grid size should reduce the error by a factor of four pressure is discretised with p2 elements so third order might be expected however coupling errors associated with the velocity mean that in practice the convergence is about second order cotter and ham 2011 fig 4 plots the errors for each solution field with logarithmic axes velocity temperature and melt rate are all very close to the expected second order convergence or higher salinity and pressure are slightly lower though never below 1 8 and the convergence rate is increasing towards second order as the grid is refined the integrated melt is also initially above second order but drops to 1 82 convergence overall these convergence results give confidence in the accuracy and consistency of the numerical discretisation and hopefully will prove to be a useful tool as the model continues to be developed in the following section we demonstrate our ability to simulate more complex ice shelf cavity geometries in 3d using the isomip ocean0 test case 3 2 3d simulations of an idealised ice shelf cavity isomip ocean0 the aim of this section is to demonstrate our ability to simulate ocean flow in a fully three dimensional ice shelf cavity the ice shelf ocean model intercomparison project isomip consists of a set of idealised ice shelf cavity geometries and parameters for a set of common experiments asay davis et al 2016 here we use the ocean0 experiment which has been used for a number of studies to introduce and compare different ocean models for ice shelf cavity applications zhou and hattermann 2020 gwyther et al 2020 favier et al 2019 this experiment reaches a quasi steady state in the cavity within a few months of simulation time so it is a convenient test case for carrying out model comparisons and testing model parameter choices asay davis et al 2016 there is not an exact solution that all models are aiming to replicate for this reason we have found it helpful to run simulations of mitgcm alongside our model mitgcm is a finite volume z layer fixed vertical resolution ocean model widely used in a number of applications including investigations of ice shelf ocean cavities losch 2008 dansereau et al 2014 seroussi et al 2017 nakayama et al 2017 kimura et al 2017 nakayama et al 2021a naughten et al 2021 it also has a nonhydrostatic option selected for this comparison so that both models solve the same underlying set of equations albeit with different discretisations the ocean0 domain is approximately 400 km long 80 km wide and has a maximum depth of 720 m the bathymetry is specified by an analytical profile representing an idealised fjord and the ice draft is from an ice flow simulation from the corresponding mismip experiment asay davis et al 2016 the target isomip grid resolution is specified as 2 km in the horizontal and 36 vertical layers with the caveat that different vertical meshing strategies will impose constraints on the resolution asay davis et al 2016 the isomip protocol does not specify exactly how the cavity thickness should be defined as the real cavity pinches to zero thickness at the grounding line for mitgcm simulations we have followed the suggested 40 m minimum thickness for z layer models so that there are always at least two cells with the specified 20 m vertical resolution to create the 3d firedrake mesh we used the inbuilt extruded mesh capability of firedrake first we used qgis qgis development team 2022 to generate the horizontal extent of the mesh we chose the grounding line by tracing the 10 m ocean thickness contour ice draft bathymetry we used the qmesh package avdis et al 2018 to generate the horizontal surface mesh that forms the base of the columns in the full 3d extruded mesh qmesh is effectively a wrapper for gmsh geuzaine and remacle 2009 to allow meshing of domains generated from gis data the surface mesh is then extended in the vertical direction to create columns of extruded triangular prism elements bercea et al 2016 mcrae et al 2016 we use 30 layers under the ice with a transition to 36 layers in the open ocean region with this transition occurring over a single horizontal grid cell squashing the domain under the ice gives a terrain following quasi sigma style mesh within the cavity with higher vertical resolution towards the grounding line we use the tensor product elements described in section 2 3 2 for the discretisation of the velocity pressure finite element pair and for tracers the tensor product element described in 2 3 1 the temperature and salinity profiles for initialisation and restoring on the northern boundary are the warm profiles given in table 6 from asay davis et al 2016 we use a timestep of 900 s with backward euler time stepping for the momentum equations and a diagonally implicit runge kutta method dirk33 for the tracers horizontal and vertical kinematic viscosity are specified at 6 m 2 s and 1 1 0 3 m 2 s respectively and horizontal and vertical diffusivity are specified at 1 m 2 s and 5 1 0 5 m 2 s as per the isomip protocol asay davis et al 2016 we include the switch to higher vertical mixing values when the stratification is unstable however the scheme has negligible effect on calculated melt rates due to the build up of stable stratification at the ice ocean boundary during melting the melt parameterisation specified by isomip does not include heat flux into the ice and uses constant turbulent exchange coefficients based on jenkins et al 2010 for this comparison both models use the same turbulent exchange coefficients in the melt parameterisation γ t 0 011 and γ s γ t 35 the vertical averaging scheme for melting is turned off in mitgcm losch 2008 drag boundary conditions are applied on the top and bottom interfaces with a drag coefficient c d 2 5 1 0 3 model parameters are summarised in table 3 fig 5 shows melt rates for the first 100 days of simulation integrated over the region where the ice draft is deeper than 300 m following asay davis et al 2016 we have found that melt rates are very sensitive to the grid resolution mitgcm melt rates are plotted in blue with vertical resolution given by the symbol triangles 20 m target isomip resolution pluses 10 m crosses 5 m circles 2 5 m clearly integrated melt rates calculated by mitgcm are highly dependent on vertical resolution as refining the mesh three times leads to nearly a factor of two difference and there is no sign of convergence our model also shows vertical dependence of melt rate as evidenced by the red lines in fig 5 dashed 18 levels continuous 36 levels dotted 72 levels however the spread is lower than with the z layer model these results are consistent with a recent study by gwyther et al 2020 using a range of models applied to the same isomip ocean0 experiment and we will refer to their work later in the discussion fig 6 shows plan view snapshots of melt rate at 50 days for mitgcm at different vertical resolutions compared with firedrake at the target resolution specified by isomip for layered models 36 layers outside the cavity 30 layers inside the cavity as the vertical resolution of mitgcm increases the spatial melt rate pattern can be seen to approach that from firedrake the streaky time evolving artefacts present in mitgcm at coarser vertical resolutions circled in fig 6 disappear as the grid is refined note that while time mean fields of melt rate act to smooth out the streaks there are still noisy grid scale features after time averaging not shown reducing the vertical diffusivity and viscosity linearly at the same rate as the grid refinement does not bring the streaks back not shown this suggests it is not a numerical instability of the type dependent on grid reynolds number the reynolds number evaluated at the grid scale r e δ x u δ x ν where u is a characteristic velocity δ x is the grid resolution and ν is the eddy viscosity or grid péclet number similarly p e δ x u δ x κ where κ is the eddy diffusivity the streaks may therefore be a grid discretisation error caused by the step representation of the boundary that is inherent to z layer models even with a partial cell representation losch 2008 both models show highest melting at the grounding line which is consistent with the depth dependent melting point warm initial and restoring conditions at the bottom of the water column and steeper basal slopes close to the grounding line that enhance buoyancy driven flow this melt pattern is also consistent with other published isomip ocean0 results asay davis et al 2016 zhou and hattermann 2020 gwyther et al 2020 within the cavity both models show a clearly defined western defined as positive y boundary current due to the effects of rotation this can be seen in fig 7 which is a cross section of the cavity at x 480 km close to the grounding line there is strong northward flow along the western boundary together with colder and fresher conditions which results from melt water accumulating and preferentially flowing through this part of the domain the middle panel of fig 7 shows mitgcm at the higher vertical resolution refining mitgcm leads to better qualitative agreement with firedrake the return boundary flow and the stratification at the top of the domain is qualitatively much better resolved by mitgcm at the higher resolution particularly in the eastern half of the domain at this location the cavity thickness is close to 100 m so with 30 vertical layers firedrake s vertical grid resolution is similar to mitgcm s vertical grid resolution of 2 5 m so it is not unexpected that the higher resolution mitgcm simulation yields results much closer to those from firedrake in reality melting leads to the injection of cold and importantly buoyant fresh water at the top of the domain if the ice shelf base has a sufficient slope the buoyant melt water drives a shear flow and thus enhances mixing however melting can also lead to a negative feedback mechanism since melt water forms a stratified layer especially for low basal slope angles that effectively shuts down vertical mixing mcphee 2008 vreugdenhil and taylor 2019 this leads to reduced melt rates because the ice base is insulated from the warmer saltier water below it is the competition between the buoyancy driven shear and build up of stratification as well as external sources such as tidal currents that controls mixing modelling vertical mixing accurately is therefore of fundamental importance to represent flow in ice shelf cavities and to calculate the associated melt rate the total amount of mixing simulated by a numerical ocean model is a combination of resolved turbulence mixing explicitly specified mixing that aims to account for the unresolved turbulent mixing here we use a simple eddy diffusivity closure with constant diffusivity and a grid size dependent implicit contribution often referred to as numerical mixing this spurious numerical mixing is often dictated by the details of the discretisation of the advection and diffusion operators griffies et al 2000 refining the grid resolution decreases the amount of spurious numerical mixing added by the model in the context of ice shelf cavity modelling reduced mixing can lead to increased stratification and in turn lower melt rates if the basal slopes are not too steep terrain following models with typically higher resolution at the grounding zone resulting from the squashed vertical grids will have lower numerical mixing and thus resolve more of the stratification effects than a z layer model run at typical coarse vertical resolutions as seen in fig 7 this means terrain following models in typical configurations tend to calculate lower melt rates z layer models like mitgcm can produce lower melt rates at higher resolution as shown in fig 5 the specified vertical mixing has to be higher than the spurious mixing value for grid convergence of the melt rate otherwise as the grid is refined the total amount of mixing generated by the model decreases leading to a reduction in melt rates the dependence of melt rate on vertical resolution has been investigated previously in a separate study also using the ocean0 experiment gwyther et al 2020 gwyther et al 2020 used three ocean models with different vertical coordinate systems sigma roms z coco hybrid mpas o and showed that the two models with constant 20 m vertical resolution at the boundary specified by isomip coco and mpas o systematically calculate higher melt rates than the sigma coordinate based model which is consistent with the results presented here gwyther et al 2020 following the isomip protocol framed the problem as a matter of tuning the turbulent exchange coefficients from the melt parameterisation to obtain the same melt rates between models the isomip protocol suggests a target melt rate averaging 30 m a in the grounding zone region where the ice draft is below 300 m depth gwyther et al 2020 showed that the sigma model roms was incapable of reaching this target melt rate purely by adjusting the exchange coefficients here we take a different approach and frame the problem in terms of grid convergence with the aim of achieving a melt rate that is unaffected by grid resolution and keeps the exchange coefficients fixed given the current implementation of the melt parameterisation and turbulence closure constant eddy diffusivity and viscosity this means we need the temperature salinity and velocity fields to remain constant at the boundary as the grid is refined to achieve a fixed melt rate gwyther et al 2020 show that sigma coordinate based models can produce higher melt rates with elevated vertical diffusivity at the boundary fig 8 shows integrated melt rates for firedrake run at different vertical resolutions and vertical diffusivities the lines in red are the same as from fig 5 which is the integrated melt rate at the target isomip vertical diffusivity of 5 1 0 5 m 2 s with this diffusivity the melt rate still depends on grid resolution as discussed the blue and orange lines in fig 8 show the melt rate with diffusivities of 5 1 0 6 m 2 s and 5 1 0 7 m 2 s respectively despite the order of magnitude change there is no significant difference in melt rate this suggests that numerical mixing dominates when the specified mixing is specified at these low levels as a result the melt rate would be expected to be highly dependent on grid resolution and we expect refining the vertical resolution further would reduce melt rates in contrast by increasing the vertical diffusivity by ten times the amount of the isomip specification to 5 1 0 4 m 2 s shown by the black lines in 8 the integrated melt rate calculated with 72 layers is very close to the melt rate calculated with 36 layers the specified number of layers for the isomip profile since the melt rate appears to be converging towards a consistent value as the grid is refined the specified diffusivity must be dominating over the spurious resolution dependent numerical mixing this comparison suggests that a rough estimate for the numerical mixing for a firedrake simulation with the isomip target of 36 layers is likely between 5 1 0 6 m 2 s and 5 1 0 5 m 2 s since the upper bound has the same value as specified in the isomip protocol this serves to highlight the importance of running ice ocean simulations at multiple resolutions to determine how sensitive the melt rate is to resolution for a particular model and setup configuration the implicit assumption made here is that the diffusivity and viscosity do not vary close to the boundary this is not necessarily physically justifiable as monin obukhov theory implies that with strong melting and insufficient shear turbulence to break down the stratification the mixing rate and hence the values of diffusivity and viscosity will be much lower leading to overestimated melt rates vreugdenhil and taylor 2019 mcphee 2008 we have assumed fixed diffusivity and viscosity primarily for reasons of simplicity it is the isomip specification and with a new model we wanted to have confidence that we could understand the general behaviour before implementing more complicated turbulence closure schemes even if the model is responsible for explicitly calculating diffusivity and viscosity the pertinent length scales close to the boundary are very small sub metre scale mcphee 2008 so it may be impractical to account for these effects in large scale ocean simulations potentially the melt parameterisation could be modified to incorporate the enhanced stratification effects that arise during melting similar to the transition of scalar properties across the viscous boundary later which is accounted for holland and jenkins 1999 but this leaves the question of where to sample t w and s w open unless the sampling distance is chosen for physical reasons it seems like it will inherently depend on the grid resolution it is not obvious how applicable an ice shelf ocean cavity simulation will be for different forcing scenarios or geometrical configurations if the exchange coefficients have been tuned specifically to compensate biases arising from a somewhat arbitrary choice of grid resolution overall our integrated melt rate of 7 6 m a for the target isomip resolution of 2 km horizontally and 36 vertical layers orange line fig 5 is consistent with and within the spread of other published results when using the same turbulent exchange coefficients in the melt parameterisation γ t 0 011 and γ s γ t 35 published results for terrain following models with 36 layers include fvcom 10 m a figure 9 from zhou and hattermann 2020 and roms 4 m a figure 4 from gwyther et al 2020 similarly examples of z level models at high vertical resolution dz 2 m include coco 5 m a and mpas o 9 m a figure 5 from gwyther et al 2020 as well as pop2x 7 m a figure 9 from asay davis et al 2016 also note the mitgcm integrated melt found here for this comparison is consistent with coco and mpas o which both gave around 15 m a with 20 m vertical resolution figure 5 from gwyther et al 2020 we also investigated melt rate sensitivity to horizontal resolution within the cavity we ran firedrake at two coarser horizontal resolutions additionally shown in fig 5 purple 8 km red 4 km to compare with the target isomip resolution orange 2 km although the spatial patterns of melt rate are still broadly equivalent with highest melting at the grounding line seen in all simulations not shown the integrated melt rates have not converged to a consistent result to investigate the potential of a horizontally unstructured mesh to focus resolution we ran a preliminary simulation with the target isomip 2 km resolution only along the western boundary and the grounding zone region relaxing to 8 km outside the cavity these areas were chosen to best capture the observed spatial melt pattern with highest melting at the grounding line as well as the fast flowing western boundary current present due to coriolis the integrated melt rate for this non uniform mesh is shown by the black line in fig 5 a positive result is that the melt rate predicted with the non uniform mesh is closer to the value from the uniform 2 km mesh than the 8 km and indeed the 4 km uniform mesh however considering the degrees of freedom dofs associated with each mesh this may not be surprising the number of combined velocity pressure temperature and salinity dofs is 964 059 for the 8 km mesh 3 448 681 for the 4 km mesh and 13 693 896 for the 2 km mesh the non uniform mesh has 5 011 517 dofs which is actually 1 45 times more dofs than the 4 km mesh in more complicated cavity geometries the ability to vary the mesh resolution flexibly especially in the presence of large channels and crevasses may prove crucial zhou and hattermann 2020 as emphasised in section 1 one of the main motivations for using the firedrake framework to simulate ice ocean interactions is the availability of an automatically generated adjoint model in the final section we show our first steps at using this capability to investigate sensitivities in an idealised ice shelf cavity 3 3 preliminary adjoint sensitivity calculations in an ice shelf cavity we present preliminary results of sensitivity information obtained with the adjoint to our model a simplified domain has been chosen to make interpretation of the adjoint sensitivity patterns easier the domain is a vertical 2d slice within 10 km of an idealised grounding zone domain with a 2 m wall at the grounding line and a 100 m wall at the open ocean the domain has a uniform seabed depth of 600 m the temperature and salinity are initialised with constant temperature and salinity of 1 c and 34 4 respectively the temperature and salinity fields are relaxed to these values in a sponge region which linearly ramps up over the final four grid cells to the right hand side of the domain with a restoring period of one day at the boundary the grid is made up of triangles arranged in columns similar to a z layer discretisation for an example the reader is referred to kimura et al 2013 the grid resolution is 500 m in the horizontal and 2 m in the vertical horizontal viscosity is 0 25 m 2 s and vertical viscosity is 1 1 0 3 m 2 s matching the grid aspect ratio diffusivity for both tracers is set equal to viscosity we use a p1dg p2 discretisation for velocity and pressure and p1dg is used for the tracers the timestep is 300 s table 4 summarises parameters and constants used for the adjoint sensitivity experiment the forward simulation is run for 50 days to reach a spun up state fig 9 shows the spun up state at 50 days the flow is characteristic of the ice pump mechanism with a clockwise overturning flow concentrated near the ice ocean boundary driven by buoyant melt water at the grounding line the water column is well mixed in the vertical though the temperature field becomes progressively more stratified away from the grounding line this type of flow is similar to that described in holland 2008 where background tidal mixing dominates at the grounding zone if the overturning circulation weakens in this instance it is likely caused by the coarse grid resolution and constant eddy diffusivity values because at the 2 m high grounding zone wall on the left hand side of the domain there is only one cell to resolve the overturning circulation it is likely that refining the grid resolution effectively reducing implicit spurious mixing as described in section 3 2 or using a turbulence model would change the rate of mixing in this region and the water column would become more vertically stratified the model is run for a further 20 days with the adjoint model an objective functional is chosen to be the total basal melt beneath the ice shelf at the final time step although using the average basal melt beneath the ice shelf may have been a more conventional quantity this can be obtained by dividing the results by the length of the ice shelf we calculate sensitivities of this functional to perturbations of the spatially varying temperature and salinity fields at each time level as well as to spatially varying perturbations of the viscosity and diffusivity fields which are kept fixed in time these fields are known as controls errico 1997 the accuracy of the adjoint sensitivity can be verified by a taylor remainder convergence test farrell et al 2013 if the gradient is correct then with j m as the functional m as the control and δ m as a small perturbation in an arbitrary direction j m δ m j m j δ m should converge to zero with second order accuracy as the magnitude of the perturbation is reduced i e halving the size of the perturbation should cause the result to decrease by a factor of four we have carried out such taylor remainder convergence tests for each control field with the melt functional evaluated after ten timesteps 3000 s from the 50 day checkpoint the errors do indeed display convergence at second order and verify that the gradient information is being calculated accurately by the adjoint model fig 10 shows adjoint sensitivity of the final total melt flux note defined here as a volume flux not a mass flux to temperature and salinity at three times in the simulation the adjoint model is effectively a transpose of a linearised version of the full non linear forward model this linear forward model is referred to as the tangent linear model tlm and it relates perturbations of the control fields to changes in the objective function the adjoint model reverses the flow of information because it is the transpose of the tlm instead calculating the sensitivity of the objective function to the controls for time dependent models the adjoint sensitivities thus flow backwards in time from the time when the objective function is evaluated towards the earlier times provided that the controls had an impact on the objective function errico 1997 the top panel of fig 10 shows six hours before the final time where the sensitivity fields are concentrated at the ice ocean boundary where the objective function is evaluated since we have evaluated the objective function at the final time this is equivalent to adding an initial condition to the adjoint model as such even though the forward model is steady the adjoint sensitivities evolve backwards with time in the opposite sense to the forward model at two days before the final time middle panel the sensitivity patterns are concentrated closer to the grounding line and at the bottom of the cavity finally five days before the final time bottom panel most of the sensitivity of the objective function to the temperature and salinity fields has been lost the counterclockwise motion of the sensitivity fields is effectively the ice pump mechanism but viewed in reverse the time for the sensitivity fields to be lost is consistent with the forward velocity flow speeds are on the order of 0 05 m s and an advection path length of 20 km twice the domain length gives a circulation time of 4 6 days an interesting feature of fig 10 is that the signs of adjoint temperature and salinity are opposite in the same region a small positive perturbation to temperature is required to increase the final total melt flux whereas an increase in salinity would decrease the final total melt flux initially this seems counter intuitive since increasing salinity decreases the freezing point which should lead to more melting and thus have the same effect as increasing temperature this dependency has been found before in an adjoint simulation of pine island glacier ice shelf cavity heimbach and losch 2012 heimbach and losch 2012 suggest that buoyancy effects may account for the sensitivity difference since increasing temperature and decreasing salinity both increase buoyancy this in turn should strengthen the overturning ice pump and lead to higher melt rates scaling the sensitivity fields by the expansion coefficients in the linear equation of state α t 2 1 0 4 c 1 β s 7 1 0 4 the two sensitivity fields are almost identical in sign and magnitude strongly suggesting that buoyancy is controlling the sensitivity pattern even though this is a highly idealised experiment the adjoint model shows potential to investigate the importance of subglacial hydrology and the formation of buoyant plumes at the grounding line and hence the overall effect on melting as a complimentary tool to the forward model fig 11 shows patterns of adjoint sensitivity for vertical viscosity temperature diffusivity and salinity diffusivity sensitivity with respect to the vertical components were an order of magnitude stronger than the horizontal components which is not surprising considering how sensitive the melt rate and hence the flow is to vertical stratification as explored in section 3 2 sensitivity for all three fields is concentrated towards the grounding line and the ice ocean interface this is intuitive because the overturning flow brings water through the grounding zone region results from yeager 2018 using a k ϵ reynolds averaged navier stokes rans model to investigate ocean conditions near grounding lines suggest that changes in diffusivity of several orders of magnitude may occur within 1 km of the grounding line even though these large changes in diffusivity would not linearly relate to changes in melt rate the fact that total melt rates are sensitive to grounding line eddy diffusivity values which probably vary significantly highlights the importance of modelling these values accurately 4 conclusion and future work we have presented a new model for ocean flow in complex ice shelf cavity environments by carrying out an mms test we confirmed that the numerical discretisation of velocity pressure and tracer fields and consequently the melt rate are second order accurate as expected from our model discretisation choice we believe that it is important to carry out rigorous code verification checks for which the mms technique is ideally suited because comparing solely with other models may mask model errors this is especially true considering the strong dependence of the melt parameterisation on grid resolution as seen in the simulations of the isomip test case the model is capable of running robustly in 3d domains of a full ice shelf cavity demonstrated by simulations of the isomip test case our simulation results compare favourably when run alongside mitgcm and the melt rates are consistent with the spread found in the literature asay davis et al 2016 zhou and hattermann 2020 gwyther et al 2020 we find melt rates are highly sensitive to grid resolution as has been observed before ultimately the balance between specified mixing and spurious numerical mixing dictates how sensitive the melt rate is to changes in the grid resolution and discretisation choice our study emphasises the importance of running ice ocean cavity simulations at multiple resolutions to gauge this sensitivity as it will always be problem specific as discussed in section 3 2 given the assumption of fixed diffusivity and viscosity close to the boundary finding a melt rate that is insensitive to grid resolution using the existing melt rate parameterisation may not be directly applicable to real ice shelf cavity environments a detailed investigation into the melt parameterisation is beyond the scope of this paper and developing a new more robust melt parameterisation for large scale applications is not our immediate aim studies of the ice ocean boundary layer are underway that use a hierarchy of models from direct numerical simulations middleton et al 2021 couston et al 2021 to large eddy simulations vreugdenhil and taylor 2019 begeman et al 2022 vreugdenhil et al 2022 as well as theoretical models jenkins 2016 2021 it is likely that these combined approaches will be needed to develop a parameterisation that is consistent with the physics explicitly resolved by the numerical model and the model s own mixing schemes and is applicable to the unique environment of the ice shelf ocean boundary layer where the effects of stratification and basal slope are important potentially a model like the one implemented here using firedrake with flexible grids in the horizontal and vertical will be a useful tool to bridge the gap between high resolution models of boundary flow and the necessarily coarser regional and global scale ocean models ultimately model validation will remain a challenge until more direct observations of sub ice shelf ocean conditions are made in tandem with measurements of melt rate adjoint models are powerful tools to assess model uncertainties and incorporate observations into numerical ocean models errico 1997 heimbach and losch 2012 goldberg et al 2020 nakayama et al 2021b we showed preliminary adjoint sensitivity results for an idealised grounding zone domain which clearly showed a reverse ice pump mechanism the link between buoyancy and melt rate also suggests the importance of subglacial outflow into the cavity the sensitivity of melt rate to vertical mixing especially at the grounding zone was also identified by the adjoint model this again suggests the importance of accurately modelling or accounting for mixing within ice shelf cavities we plan to implement additional turbulence closure schemes such as rans and les to investigate these effects with the caveat that the turbulence closure problem is inherently uncertain and thus challenging to model in future work we aim to apply the model to more complicated geometries in particular ocean flow in realistic grounding zone regions and the impact that basal crevasses have on the ocean flow we anticipate that the use of fully unstructured meshes will be necessary to resolve these features accurately we also intend to extend our adjoint capability to solve optimisation problems to help constrain unknown parameters such as turbulent mixing coefficients under the ice shelf based on observations credit authorship contribution statement william i scott conceptualization methodology software validation writing original draft visualization stephan c kramer conceptualization methodology software writing review editing supervision paul r holland conceptualization writing review editing supervision keith w nicholls conceptualization writing review editing supervision martin j siegert conceptualization writing review editing supervision matthew d piggott conceptualization writing review editing supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like to thank benjamin yeager for useful discussions and initial help setting up mitgcm simulations were carried out using the high performance cluster of the research computing service at imperial college london we would like to thank xylar asay davies and one other anonymous reviewer for their constructive comments that helped to improve this paper this work was supported by the natural environment research council ne s007415 1 and ne g018391 1 this work is also from the melt project a component of the international thwaites glacier collaboration itgc support from national science foundation nsf grant nsf plr 1739003 and natural environment research council nerc grant ne s006656 1 and ne s006427 1 logistics provided by nsf u s antarctic program and nerc british antarctic survey itgc contribution no itgc 097 this work was also supported by the engineering and physical sciences research council ep r029423 1 code availability the code is available at https github com thwaitesproject thwaites git it has also been stored in this zenodo repository https doi org 10 5281 zenodo 7584700 along with instructions for running the examples from section 3 
23792,typhoon fungwong a category 2 but deadly typhoon in the 2008 pacific typhoon season made landfall first on taiwan and then on southeast china during its approach toward taiwan it triggered an extreme sea surface temperature sst drop of 12 5 c which was the strongest sst drop recorded by longdong buoy northeast of taiwan coast from 1998 to 2017 in this study including moored buoy temperature measurements argo float temperature profiles satellite observed ssts and a suite of numerical experiments performed using the regional ocean modeling system were used to unveil the detailed processes of how fungwong triggered such an extreme cooling subsequently the source of the cold waters feeding the extreme cooling possible mechanisms triggering the cooling and consequential effects of cooling on the ambient ocean environment were systematically investigated results show that the extreme cooling was triggered mainly by a process of uplift of subsurface cold water tied to shore ward kuroshio intrusion driven by easterly northeasterly winds and consequential entrainment mixing while coastal upwelling driven by persistent longshore southerly winds plays a minor role nevertheless the southerly winds still help the enhancement of entrainment mixing and thus the sea surface cooling finally modeled float trajectories with temperature tracers identified where the cold water goes and indicate that the temperature drop might extend all the way toward the south end of japan kyushu along the flowing path of kuroshio keywords typhoons kuroshio intrusion sea surface temperature moored buoy numerical modeling float trajectory data availability data will be made available on request 1 introduction sea surface temperature sst variability triggered by tropical cyclone tc passages which might affect regional weather system oceanic environment ecosystems fisheries and typhoon characteristics attracts considerable attentions from the oceanic atmospheric metrological and even climatological communities cione and uhlhorn 2003 lin et al 2003 babin et al 2004 siswanto et al 2007 morimoto et al 2009 zheng et al 2010 2015 kuo et al 2017 mohanty et al 2019 for long warm ocean has been considered an energy source for tc development schade and emanuel 1999 shay et al 2000 wu et al 2007 lin et al 2008 in other words upper ocean temperature variations in response to tc ocean interaction ahead or just behind the passage of the eye center play a key role in tc intensity development schade and emanuel 1999 lee and chen 2014 zheng et al 2015 glenn et al 2016 kuo et al 2018 in addition to possible feedback to tc intensity tcs have also been shown to play a key role affecting the regional ocean environment morimoto et al 2009 showed that typhoon passages not only pushed the kuroshio axis shelf ward but also enhanced the intensity of the kuroshio east of taiwan zheng et al 2014 showed the marked kuroshio modulation in response to the passage of typhoon morakot 2008 in addition typhoon morakot also triggered a cooling response of more than 4 c off the southeast corner of taiwan that was advected along the kuroshio east of taiwan toward northeast of taiwan kuo et al 2017 by contrast on the basis of numerical experiments zheng et al 2017 indicated that the strong local wind off northeast taiwan carried by tc passage might drive the kuroshio shore ward and cause marked cooling over the east china sea ecs continental shelf an air sea coupling model study by kuo et al 2018 focused on the interaction between tc and the kuroshio in the luzon strait they reported an unexpected movement of subsurface frontal structure in the luzon strait which later resulted in a 3 c 4 c sea surface cooling and subsequent reduction of tc intensity this negative feedback resulting from the tc kuroshio interaction to tc intensity contradicts the former concept that warmer kuroshio waters would favor the development of tc passing over it recently doong et al 2019 investigated typhoon induced sst cooling in the coastal region by continuous moored buoy observations they indicated that typhoon fungwong 2008 induced an extreme cooling reaching up to 12 5 c which is the maximum sst drop ever recorded by moored buoy deployed at longdong from 1998 to 2017 on the basis of indirect evidences they concluded that the strong and persistent longshore southerly winds inducing coastal upwelling might be the dominant cause leading to the extreme sst drops surrounding longdong this extreme cooling occurred at the flowing path where kuroshio passes through thus the cold water pulses might get advected all the way down to the kuroshio downstream region and cause threats along its path such as a number of physiological behavioral and fitness related consequences for fish termed as cold shock stress donaldson et al 2008 troy et al 2012 kuo et al 2017 additionally given its extraordinary strength this cooling response to fungwong largely attracts our attention in this study moored buoy measurements of temperature argo float temperature profiles satellite observed sst and a series of numerical experiments using the regional ocean modeling system roms were applied to resolve the detailed progress of how the tc fungwong triggers such an extreme cooling observed temperatures retrieved from three different methodologies were integrated to validate model simulated temperature responses in different regions at different spatial scales simulated floats trajectories with temperature tracers were used to answer the question of where the cold water goes overall the source of the cold waters feeding this extreme sea surface cooling the possible mechanisms triggering it and the consequential effects sourcing from the cooling on the surrounding oceanic environment were systematically investigated 2 data and methods 2 1 satellite sst in this study daily microwave optimally interpolated sst merged from tropical rainfall measuring mission trmm microwave imager tmi the advanced microwave scanning radiometer amsr e and windsat radiometer were used to quantify the offshore sea surface cooling to tc fungwong passage under all weather conditions the spatial resolution of this gridded sst product is 0 25 degree this merged product was processed and distributed by remote sensing systems through http www remss com measurements sea surface temperature oisst description in addition to pure microwave sensor merged product multiscale ultrahigh resolution sst mursst was also collected for cross analysis especially for the coastal regions this product was processed as a global gap free gridded daily 1 km sst by merging infrared data from the advanced very high resolution radiometer avhrr moderate resolution imaging spectroradiometers modis and microwave data from amsr e and amsr 2 as well as in situ sst observations from the noaa iquam project chin et al 2017 mursst data were processed and released by nasa jet propulsion laboratory physical oceanography distributed active archive center po daac through https podaac jpl nasa gov 2 2 in situ temperature sst can be measured either by satellite based remote sensing techniques or in situ measurements from ships floats or moored buoys matthews 2013 doong et al 2019 moored buoys record continuous and long term time series of the sst at certain site here the long term sst measurement record located at longdong coast was provided by a 2 5 m discus shaped moored buoy which was deployed by the coastal ocean monitoring center of national cheng kung university in 1998 doong et al 2019 this buoy is approximately 0 6 km off the longdong coast and is situated in the water at a depth of 23 m the buoy is anchored to the sea bottom the sst was measured by a platinum resistance temperature detector which can cover a range of 10 c 70 c and installed at 0 6 m below the sea surface further description about the moored buoy data can be seen in doong et al 2019 in addition the long term buoy measured ssts covering the period from 1998 to 2017 can be accessed through https doi org 10 1594 pangaea 895002 2 3 ocean model description and experiment design relative to sparse observations provided by either moored buoy temperature meter at certain site or satellite snapshots model simulation usually plays an important role in improving the continuous understanding of certain physical process to understand the detailed progress of upper ocean variability surrounding longdong to the passage of tc fungwong regional ocean circulation was simulated by the roms model here the roms is a relatively new generation ocean circulation model that has been applied to multidisciplinary ocean modeling research it is a free surface primitive equation curvilinear coordinate oceanic model barotropic and baroclinic momentum equations in roms are separately resolved meanwhile a non local k profile planetary boundary layer scheme large et al 1994 was applied to parameterize the subgrid scale mixing processes in the vertical in addition to have a higher spatial resolution for resolving the small scale circulation features surrounding longdong and a larger model domain for resolving the large scale circulation features including the whole progress corresponding to tc fungwong passage from open ocean toward onshore region simultaneously a multilevels nested grid roms was implemented the parent model with a horizontal resolution of approximately 7 km covered the region from 17 to 30 n and from 116 to 133 e fig 1 and the nested model with a horizontal resolution of approximately 2 km covered the region from 21 7 to 26 2 n and from 120 4 to 123 5 e given that the buoy measured temperature sourcing mainly from a large amount of offshore water temperature will be demonstrated in following analyses thus a 2 km spatial resolution was considered good enough to reproduce the processes leading to the extreme cooling the vertical gridding of the parent and nested models consists of 20 s coordinate levels theta s 6 theta b 0 hc 10 that were unevenly distributed for better resolution of the upper ocean relative to that of bottom layer or lower layers song and haidvogel 1994 model bathymetry was created by merging regional bathymetry of 500 m spatial resolution processed and distributed by ocean data bank odb sponsored by the ministry of science and technology taiwan and etopo2 global ocean bottom topography smith and sandwell 1997 additionally the model was driven by momentum forcing from hourly sampled gridded 0 5 degree latitude x 0 625 degree longitude modern era retrospective analysis for research and applications version 2 merra 2 winds https disc gsfc nasa gov which is one of the most up to date wind forcing products the comparison between winds retrieved from in situ measurement and merra 2 from july 24 to 31 can be seen in figure s1 in supplementary online materials som generally merra 2 winds show consistent patterns as the winds measured by the weather buoy fixing to 10 m height using the log law model manwell et al 2009 therefore the wind product was deemed to be reasonable to drive roms by contrast atmospheric fields retrieved also from merra 2 including air temperature relative humidity precipitation rate incoming shortwave radiation and outgoing longwave radiation 0 5 degree latitude x 0 625 degree longitude were integrated for calculating net heat flux across the air sea interface which consequently influenced the sst variability the model initial conditions and lateral boundary conditions were derived from the data assimilated hybrid coordinate ocean model hycom global solutions with 1 12 degree spatial resolution through https www hycom org dataserver gofs 3pt1 reanalysis cummings 2005 for the fungwong study model simulations were executed from july 19 to july 31 august 5 for trajectories analysis 2008 temporal frequency of the model is 5 min and the output frequency is 1 h in addition first three days outputs of individual simulations are excluded in following analyses for avoiding possible insufficient spinning up of smaller scale features in the inner domain detailed description of roms is given by shchepetkin and mcwilliams 2003 2005 related validations of the model skill for storm simulations can be seen in zheng et al 2010 2014 and shen et al 2021 fig 2 shows the systematic comparison of temperature variations retrieved from all available argo floats passing the study area during the fungwong passage and corresponding roms model simulations positions of the floats used for the evaluation can be seen in figure s2 in som 3 extreme cooling response to fungwong 3 1 satellite observations fig 3 a e shows the cooling responses derived from satellite observations of tmi amsr e windsat gridded sst as the typhoon fungwong progresses westward on 26 july it starts affecting the upper ocean temperature and causes a near surface cooling of about 3 c centered at 22 n 129 e afterward the cold water patch grows to stronger cooling response with an sst drop of more than 4 c relative to the state of pre typhoon passage subsequently the main offshore cooling surrounding 20 n 23 n 126 e 129 e triggered by fungwong starts to weaken on 28 july at the same time fungwong leads to a marked cooling on the ecs shelf region off northeast taiwan on 29 july cooling at nearshore and offshore regions strengthens and weakens respectively afterward offshore and nearshore coolings decrease gradually until august 5 figure not shown recovering to the state of pre typhoon passage 3 2 roms simulations fig 3 f j shows the corresponding cooling progress during july 26 30 2008 from model simulations overall simulations reproduce the satellite observed cooling progress reasonably well with respect to the timing and location of the cold water patches despite inconsistent of time averaging for composite sst image and model simulation leading to minor difference between modeling and remote sensing ko et al 2016 however in the nearshore region satellite observed sst shows much weaker cooling relative to simulated sst brewin et al 2017 compared sst derived from avhrr with sst collected by in situ instrument at coastal and offshore regions respectively they indicated that the lower performance of satellite based sst at the coastal region when compared with offshore waters this result could serve as a possible reason for the discrepancy in our comparison between model simulations and satellite observations at the shelf region 3 3 moored buoy temperature measurements fig 4 shows direct measured temperature of approximately 12 5 c cooling at 0 6 m below the sea surface red line by the moored buoy temperature meter deployed at longdong 25 0983 n 121 9219 e black dot in fig 1 together with model simulated sst blue line and satellite observed sst gray dashed line during the passage of fungwong here mursst data were used in the comparison because merged microwave data are absent in the nearshore region model simulations show relatively good skill in resolving the temporal variation of near surface temperature by contrast mursst at nearshore regions largely underestimates and almost fails to capture the cooling feature in response to progression of fungwong the result shows again the disadvantage of satellite observations for monitoring temperature surrounding shelf regions same conclusion can be seen in woo and park 2020 their results emphasized the importance of using real time in situ measurements as much as possible to overcome the increasing sst errors in coastal regions nevertheless for offshore regions satellite observations still provide reliable sst measurements brewin et al 2017 woo and park 2020 which can help validate model simulations at different regions 3 4 detailed progression of strong cooling surrounding longdong fig 5 a e shows the simulated progress of fungwong triggered extreme sea surface cooling surrounding longdong off northeast taiwan the sea surface cooling starts at 06 00 on july 28 with a temperature drop of approximately 6 c relative to surrounding regions fig 5c cooling appears just for a while when wind direction changes from due east to southeast this phenomenon shows a marked discrepancy about the timing of the generation of cooling response relative to the findings by doong et al 2019 which suggested that the presence of strong and persistent southerly winds inducing coastal upwelling is the dominant cause for the extreme cooling surrounding longdong later the cooling grows sharply and extends northeastward with an sst drop of more than 10 c meanwhile an sst drop of over 10 c covers an area of up to 2 500 square kilometers fig 5d examining the state below the sea surface provides additional information about the generation of sea surface cooling fig 6 shows transects of temperature in color shadings currents white arrows denoting vectors of u and w 103 the position of subsurface cold water denoted by 20 c isotherms in bold red contours as well as the position where the kuroshio mainstream kms passes through during the fungwong passage the position of kms was defined by outlines with northward current speeds greater than 70 cm s 1 in black contours contour intervals are 20 cm s 1 bold black contours denote current speed equal to 110 cm s 1 pink plus signs mark core positions of kms along 25 098 n the core position was defined by maximum northward current speed average between 0 50 m along the 25 098 n transect first in reference to the variation of kms the current core of kuroshio moves shoreward from 06 00 on july 27 to 06 00 on july 28 see fig 6a 6c and then gradually retreats toward offshore back to the original position from 18 00 on july 28 to 06 00 on july 29 6d 6e subsequently in reference to the variations of subsurface cold water with the kuroshio intrusion onto the shelf the subsurface cold water uplifts markedly from 18 00 on july 27 to 18 00 on july 28 6b 6d the dynamic linkage between onshore ward kuroshio intrusion and uplift of subsurface cold water can be explained by the geostrophic adjustment of the temperature field reacting to the presence of a strong current see fig 7 and tomczak and godfrey 1994 because isotherm depths on the offshore ward side of the current cannot change thus this process would lead to a steep rise of the thermocline from the ocean toward the coast tomczak and godfrey 1994 moreover the uplifted subsurface cold water feeds in the cooling feature revealed in the sst see fig 5c 5d after 06 00 on july 29 the uplifted cold water falls down gradually back to deeper subsurface see white arrows in fig 6e detailed cooling progress transects during the whole passage of fungwong can be seen in animation a1 in som 4 how fungwong triggers extreme cooling 4 1 three dimensional ocean state and its evolution control experiment in this section to further understand the exact mechanism s therein the variations of three dimensional velocity fields u v w and temperature during the entire typhoon passage at the longdong site centered at 25 08 n 122 e were examined it is noting that the analysis was performed at the position of maximum surface cooling 25 08 n 122 e instead of exactly the position where buoy station deployed because the dynamic analysis must capture the most active process located at corresponding area by contrast the temperature comparison was conducted exactly corresponding to the position of buoy station to provide a validation of the simulated temperature variations since the buoy measured temperature was considered representative of a large amount of offshore water temperature see animation a1 in som this is the main reason why the analysis was performed slightly offshore relative to the temperature comparison corresponding to exactly the location of moored buoy in fig 8 u velocity shows a sudden and sharp shoreward shift from 18 00 on july 27 to 15 00 on july 28 afterward u velocity changes to almost zero a few hours later the kms starts to move offshore with an eastward velocity larger than 1 m s 1 meanwhile with the shoreward shift of u component velocity the v component velocity increases drastically to more than 1 3 m s 1 this result implies that the kms intrudes shoreward toward the longdong in situ measurement site subsequently just behind the westward shift of the kms an extreme and rapid upwelling appeared from 00 00 on july 28 to almost 20 00 on july 28 as shown in the w component velocity field accordingly a strong subsurface cooling originating from the deeper region depth 100 m can be found immediately after the drastic upwelling from 00 00 on july 28 during this time the uplifted subsurface cold water attains a depth of about 25 m and provides a favorable source of cold water for entrainment vertical mixing price 1981 later a strong temperature drop extends all the way to the sea surface meanwhile the drop in temperature reaches up to approximately 15 c relative to the state prior to the fungwong passage this result shows the important role of the uplift of the subsurface cold water in the response of extreme sea surface cooling to the fungwong passage as shown in the previous section nevertheless the exact role of strong southerly winds wind boost during 10 00 12 00 on 28 july on the extreme cooling response to fungwong remains unclear 4 2 experimental group to further differentiate the effects of the shoreward intrusion of the kuroshio and southerly wind on the uplift of subsurface cold water and thus the generation of the extreme cooling here we conduct two additional experiments 1 exp nosw experiment using complete wind forcing but excluding southerly wind over the whole model domain and 2 exp sw experiment using only southerly wind during the fungwong passage the difference between the three model experiments can be seen in table 1 it should be noted that wind fields in those experiments are no longer physical fig 9a shows the results of exp nosw simulation compared with the results of control simulation with complete dynamics fig 8 exp nosw shows a similar pattern of kuroshio intrusion but with slightly weaker strength without the contribution of southerly wind it still leads to a sea surface cooling about 8 c 9 c with similar pattern and timing as the cooling response simulated in the control experiment fig 9b shows the result of exp sw which can be used to elucidate the role of southerly wind on the generation of extreme cooling to fungwong with only the influence of southerly wind and excluding the influence of northeasterly and easterly winds carried by the first half passage of fungwong kuroshio intrusion like signals e g negative u velocity anomaly sharp increase in positive v velocity anomaly are absent furthermore the upwelling and consequential cooling largely decrease in comparison with either exp nosw or control simulation the results of these experiments clearly demonstrate that the southerly wind and consequential upwelling are not the main cause for triggering the extreme cooling to fungwong by contrast the prior stronger upwelling driven by the shoreward intrusion of the kuroshio tied to easterly and northeasterly winds plays a dominant role the comparison of results of exp nosw fig 9a to control experiment shows that the strong southerly wind plays a positive role in further enhancing the sea surface cooling this result could be due to southerly wind enhancing entrainment mixing by injecting more momentum entering the upper ocean and mixing more uplifted cold water 25 m depth entering the mixed layer 4 3 heat budget analysis the relationship between the dominant mechanisms triggering extreme cooling and consequential temperature variations were further examined by heat budget analysis following glenn et al 2016 and kuo et al 2018 the roms conservation of heat equation was used to quantify the relative contributions of the different terms responsible for the simulated distinctive sea surface cooling during the passage of fungwong the general conservation equation for the heat budget in roms is given below 1 t tendency t u t x v t y horizontal advection w t z vertical advection z k t z vertical mixing d t horizontal mixing residual with the following vertical boundary conditions for surface eq 2 and bottom eq 3 respectively 2 k t z surface q n e t ρ 0 c p 3 k t z z h 0 where t is time t is the temperature u v w are the three component of velocity k is the vertical diffusivity coefficient d t is the horizontal mixing term q n e t is the net surface heat flux ρ 0 is density of seawater c p is the specific heat capacity of seawater and h is the depth fig 10 displays evolutions of separate terms in eq 1 from the surface to a depth of 100 m at the model grid closest to the maximum sea surface cooling 25 08 n 122 e it is noted that the net surface heat flux q n e t was included in the simulation in roms through the term of vertical diffusivity mixing following the surface boundary condition of k t z z 0 q n e t ρ 0 c p eq 2 as noted in both hedstrom 2018 and glenn et al 2016 thus q n e t was not shown directly in the heat budget analysis fig 10 more details about the temperature budget in roms can be seen in section of roms heat balance analysis in glenn et al 2016 and section of vertical boundary conditions 3 2 in hedstrom 2018 here residual is related to the slightly different temporal integrating intervals time varying vertical s coordinate used in roms and influence of friction glenn et al 2016 in addition the magnitude of the average residual in our analysis is 5 10 5 c s which is approximately one order smaller than the other terms except horizontal mixing fig 10a shows the temperature rate of change which is the sum of the zonal advection fig 10b meridional advection fig 10c vertical advection fig 10d and vertical mixing terms fig 10e additionally the horizontal mixing term fig 10f is close to zero and does not contribute to the rate of change of temperature as shown in fig 10a a relatively marked decrease in surface mixed layer temperature began at approximately 06 00 on july 28 showing consistent progress as shown in two dimensional sea surface cooling fig 5c the cooling tendency continued to almost 24 h till 06 00 on july 29 when it reached its lowest value however before the cooling tendency reached the sea surface at approximately 00 00 on july 28 another cooling tendency originated from the deeper region 100 m depth and progressed all the way toward the sea surface with time a few hours later this cooling tendency originated from deeper layers combined with the prior cooling tendency of the shallow region 0 25 m depth according to temperature contributions by individual terms fig 10b 10e the influence of vertical advection w t z occurred approximately at 19 00 on july 27 seems to dominate the cooling tendency originating from the deeper region by contrast the gradual uplift of deep cold water tied to vertical advection upwelling shows consistency with the uplift of cold water shown in transect plots fig 6 overall during this period the uplift of cold water from the deeper region provides a favorable cold water source for later entrainment vertical mixing associated with strong wind forcing carried by the fungwong passage in the meantime the variation of temperature in the upper ocean 0 25 m depth was due to the combined action of vertical advection and vertical mixing the vertical mixing contributed to cooling in the mixed layer but warming in the upper thermocline underlying strong wind forcing due to the process of mixed layer deepening price 1981 generally this would result in a cooler and thicker mixed layer however as shown in fig 10e the upper thermocline rises and depresses the thickness of the mixed layer the thermocline rising is attributed to the influence of the strong vertical advection of cold water the continued supply of cold water from the deeper region would largely enhance the sea surface cooling through entrainment mixing in the experiment of exp sw the cooling tendency in the surface layer triggered by entrainment mixing is reduced to 24 relative to control simulation forced using complete wind fields calculated within 0 25 m again this result confirms the key role of the uplift of subsurface cold water driven by the onshore intrusion of the kuroshio on the generation of extreme cooling response to fungwong passage 5 where the extremely cold water advects to track where the surface cold water advects in three dimensions i e the sink of the extreme sea surface cooling water triggered by fungwong roms modeled floats with temperature tracers were released over the region where maximum sea surface cooling occurred surrounding longdong see fig 1c and fig 5 the release period from 06 00 on july 28 to 06 00 on july 29 is decided in accordance with the period of occurrence of maximum cooling refer to fig 4 the initial depths of the released floats are 0 6 m below the sea surface fig 11 shows the trajectories of modeled floats along with their temperature variations in color shadings the individual float trajectories during different stages of release show how the cold water moves in response to the influences of strong wind forcing carried by fungwong and the northward flowing kuroshio current for the initial stage fig 11a showing trajectories of floats released during 06 00 12 00 on july 28 part of the floats drift northward due to the influence of strong southeasterly southerly wind meanwhile cold water trajectories extend northward to about 200 km with temperature gradually recovering to the state of ambient waters subsequently for the stages of floats encountering extreme cold waters fig 11b c the direction of majority of the trajectories shifts gradually from north northeast toward northeast at this time the cold water trajectories with a temperature drop larger than 10 c extend northward and northeastward over 300 km subsequently with the diminishing strength of southerly wind float trajectories bend to northeast eastward toward the path of kms fig 11c d at this moment the kuroshio current gradually becomes the dominant forcing determining the movement of most of the floats trajectories record cold waters extending more than 500 km with a temperature drop above 5 c fig 11d finally floats with a temperature drop of more than 5 c extend all the way eastward northeastward toward the south end of japan kyushu along the kuroshio downstream figure s4 in som all the floats released during the fungwong passage can be separated into two main groups the first group accounting for 58 of the total float trajectories are those floats that move northward onto the ecs shelf that eventually mixes into deeper region refer to the depth variability of all the modeled floats in figure s3 in som with slower movement due to the lack of energetic circulation over the shelf after typhoon passage the other group of floats are those which get on the highway kms and move eastward all the way with a higher velocity of about 1 1 5 m s 1 this groups account for 42 of the total trajectories the second group of floats move eastward for about 600 km on the basis of the model simulation limited to a duration of 7 days to compare the difference of particle trajectories seeded from the extraordinary cooled region ecr and the other regions we conduct another online experiment with floats seeded at 0 5 degree east and 0 5 degree north of the original ecr respectively the results show interesting but unsurprising results that is for floats released east of the ecr trajectories were advected along the kms due to the influence of the kuroshio current by contrast once the trajectories seeded 0 5 degree north of the ecr majority of the trajectories were advected northward toward the ecs continent shelf see figure s5 in som as noted in previous studies the transport of water with anomalous temperature differences is also responsible for unusual weather patterns including surface winds clouds regional atmospheric circulation and rainfall e g barrick et al 1977 reason 2001 beal et al 2011 this finding implies that the far reach effect of the extreme cooling response to tc passages such as fungwong should not be overlooked nevertheless the trajectories analysis was conducted on the basis of the currents provided by roms model simulation characteristics revealed by the trajectories analysis must include errors because no model is perfect 6 summary this study focused on an extreme sst drop of 12 5 c in response to tc fungwong 2008 passage which was one of the strongest cooling responses recorded by historical in situ instruments after a typhoon passage on the basis of limited in situ measurements of temperature satellite observed sst and a suite of numerical experiments performed using roms the detailed progress of how the fungwong triggers such an extreme cooling were reproduced and systematically investigated source of the cold waters feeding the extreme cooling and possible mechanisms triggering the cooling as well as consequential effects of extreme cooling on the surrounding oceanic environment were unveiled the major results are summarized as follows 1 the source of cold water feeding the extreme sea surface cooling in response to fungwong is due mainly to the strong uplift of subsurface cold water near the coastal region tied to an onshore intrusion of the kuroshio current driven by easterly northeasterly winds 2 sensitivity experiments differentiated the effects of shoreward kuroshio intrusion and the later southerly wind on the uplift of subsurface cold water the shoreward kuroshio intrusion plays a relatively dominant role on the generation of extreme cooling 3 southerly wind plays a constructive role in the enhancement of entrainment mixing and thus sea surface cooling 4 heat budget analysis explains the direct relationship between the possible mechanisms and consequential temperature variations 5 overall the extreme sea surface cooling response to fungwong results mainly from a combined effect of prior uplift of subsurface cold water due to wind driven onshore kuroshio intrusion and later entrainment mixing partially enhanced by persistent southerly wind 6 modeled float trajectories with temperature variations outline the affected region due to sharp sea surface cooling to fungwong and indicate that the effect of cooling might extend all the way toward the south end of the japan main island previous studies indicated that the transport of water with anomalous temperature differences is responsible for unusual weather patterns barrick et al 1977 reason 2001 moreover the effects of rapid decreases in water temperature on fish and aquaculture have also been well documented hoag 2003 troy et al 2012 our modeled float trajectories suggest that the corresponding effects resulting from the cold water patches on the ecs continental shelf and far field oceanic environment ecological systems weather patterns and aquaculture may be important the progress of the strong wind driven onshore kuroshio intrusion might lead to nutrient influx upward into the euphotic layer e g tsai et al 2008 which may consequently enter the ecs continental shelf this process deserves the attention of biological ecological and geochemical researchers declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work is supported by national science and technology council of taiwan through grants of 108 2628 m 003 001 my3 and 111 2611 m 003 003 my3 the authors would like to thank the anonymous reviewers and editors for their very constructive comments their valuable comments and suggestions largely improve the presentation of this work thanks to nasa jpl po daac remss gsfc central weather bureau taiwan comc ncku scientific teams for processing and providing the essential data sets as well as hycom and roms teams 
23792,typhoon fungwong a category 2 but deadly typhoon in the 2008 pacific typhoon season made landfall first on taiwan and then on southeast china during its approach toward taiwan it triggered an extreme sea surface temperature sst drop of 12 5 c which was the strongest sst drop recorded by longdong buoy northeast of taiwan coast from 1998 to 2017 in this study including moored buoy temperature measurements argo float temperature profiles satellite observed ssts and a suite of numerical experiments performed using the regional ocean modeling system were used to unveil the detailed processes of how fungwong triggered such an extreme cooling subsequently the source of the cold waters feeding the extreme cooling possible mechanisms triggering the cooling and consequential effects of cooling on the ambient ocean environment were systematically investigated results show that the extreme cooling was triggered mainly by a process of uplift of subsurface cold water tied to shore ward kuroshio intrusion driven by easterly northeasterly winds and consequential entrainment mixing while coastal upwelling driven by persistent longshore southerly winds plays a minor role nevertheless the southerly winds still help the enhancement of entrainment mixing and thus the sea surface cooling finally modeled float trajectories with temperature tracers identified where the cold water goes and indicate that the temperature drop might extend all the way toward the south end of japan kyushu along the flowing path of kuroshio keywords typhoons kuroshio intrusion sea surface temperature moored buoy numerical modeling float trajectory data availability data will be made available on request 1 introduction sea surface temperature sst variability triggered by tropical cyclone tc passages which might affect regional weather system oceanic environment ecosystems fisheries and typhoon characteristics attracts considerable attentions from the oceanic atmospheric metrological and even climatological communities cione and uhlhorn 2003 lin et al 2003 babin et al 2004 siswanto et al 2007 morimoto et al 2009 zheng et al 2010 2015 kuo et al 2017 mohanty et al 2019 for long warm ocean has been considered an energy source for tc development schade and emanuel 1999 shay et al 2000 wu et al 2007 lin et al 2008 in other words upper ocean temperature variations in response to tc ocean interaction ahead or just behind the passage of the eye center play a key role in tc intensity development schade and emanuel 1999 lee and chen 2014 zheng et al 2015 glenn et al 2016 kuo et al 2018 in addition to possible feedback to tc intensity tcs have also been shown to play a key role affecting the regional ocean environment morimoto et al 2009 showed that typhoon passages not only pushed the kuroshio axis shelf ward but also enhanced the intensity of the kuroshio east of taiwan zheng et al 2014 showed the marked kuroshio modulation in response to the passage of typhoon morakot 2008 in addition typhoon morakot also triggered a cooling response of more than 4 c off the southeast corner of taiwan that was advected along the kuroshio east of taiwan toward northeast of taiwan kuo et al 2017 by contrast on the basis of numerical experiments zheng et al 2017 indicated that the strong local wind off northeast taiwan carried by tc passage might drive the kuroshio shore ward and cause marked cooling over the east china sea ecs continental shelf an air sea coupling model study by kuo et al 2018 focused on the interaction between tc and the kuroshio in the luzon strait they reported an unexpected movement of subsurface frontal structure in the luzon strait which later resulted in a 3 c 4 c sea surface cooling and subsequent reduction of tc intensity this negative feedback resulting from the tc kuroshio interaction to tc intensity contradicts the former concept that warmer kuroshio waters would favor the development of tc passing over it recently doong et al 2019 investigated typhoon induced sst cooling in the coastal region by continuous moored buoy observations they indicated that typhoon fungwong 2008 induced an extreme cooling reaching up to 12 5 c which is the maximum sst drop ever recorded by moored buoy deployed at longdong from 1998 to 2017 on the basis of indirect evidences they concluded that the strong and persistent longshore southerly winds inducing coastal upwelling might be the dominant cause leading to the extreme sst drops surrounding longdong this extreme cooling occurred at the flowing path where kuroshio passes through thus the cold water pulses might get advected all the way down to the kuroshio downstream region and cause threats along its path such as a number of physiological behavioral and fitness related consequences for fish termed as cold shock stress donaldson et al 2008 troy et al 2012 kuo et al 2017 additionally given its extraordinary strength this cooling response to fungwong largely attracts our attention in this study moored buoy measurements of temperature argo float temperature profiles satellite observed sst and a series of numerical experiments using the regional ocean modeling system roms were applied to resolve the detailed progress of how the tc fungwong triggers such an extreme cooling observed temperatures retrieved from three different methodologies were integrated to validate model simulated temperature responses in different regions at different spatial scales simulated floats trajectories with temperature tracers were used to answer the question of where the cold water goes overall the source of the cold waters feeding this extreme sea surface cooling the possible mechanisms triggering it and the consequential effects sourcing from the cooling on the surrounding oceanic environment were systematically investigated 2 data and methods 2 1 satellite sst in this study daily microwave optimally interpolated sst merged from tropical rainfall measuring mission trmm microwave imager tmi the advanced microwave scanning radiometer amsr e and windsat radiometer were used to quantify the offshore sea surface cooling to tc fungwong passage under all weather conditions the spatial resolution of this gridded sst product is 0 25 degree this merged product was processed and distributed by remote sensing systems through http www remss com measurements sea surface temperature oisst description in addition to pure microwave sensor merged product multiscale ultrahigh resolution sst mursst was also collected for cross analysis especially for the coastal regions this product was processed as a global gap free gridded daily 1 km sst by merging infrared data from the advanced very high resolution radiometer avhrr moderate resolution imaging spectroradiometers modis and microwave data from amsr e and amsr 2 as well as in situ sst observations from the noaa iquam project chin et al 2017 mursst data were processed and released by nasa jet propulsion laboratory physical oceanography distributed active archive center po daac through https podaac jpl nasa gov 2 2 in situ temperature sst can be measured either by satellite based remote sensing techniques or in situ measurements from ships floats or moored buoys matthews 2013 doong et al 2019 moored buoys record continuous and long term time series of the sst at certain site here the long term sst measurement record located at longdong coast was provided by a 2 5 m discus shaped moored buoy which was deployed by the coastal ocean monitoring center of national cheng kung university in 1998 doong et al 2019 this buoy is approximately 0 6 km off the longdong coast and is situated in the water at a depth of 23 m the buoy is anchored to the sea bottom the sst was measured by a platinum resistance temperature detector which can cover a range of 10 c 70 c and installed at 0 6 m below the sea surface further description about the moored buoy data can be seen in doong et al 2019 in addition the long term buoy measured ssts covering the period from 1998 to 2017 can be accessed through https doi org 10 1594 pangaea 895002 2 3 ocean model description and experiment design relative to sparse observations provided by either moored buoy temperature meter at certain site or satellite snapshots model simulation usually plays an important role in improving the continuous understanding of certain physical process to understand the detailed progress of upper ocean variability surrounding longdong to the passage of tc fungwong regional ocean circulation was simulated by the roms model here the roms is a relatively new generation ocean circulation model that has been applied to multidisciplinary ocean modeling research it is a free surface primitive equation curvilinear coordinate oceanic model barotropic and baroclinic momentum equations in roms are separately resolved meanwhile a non local k profile planetary boundary layer scheme large et al 1994 was applied to parameterize the subgrid scale mixing processes in the vertical in addition to have a higher spatial resolution for resolving the small scale circulation features surrounding longdong and a larger model domain for resolving the large scale circulation features including the whole progress corresponding to tc fungwong passage from open ocean toward onshore region simultaneously a multilevels nested grid roms was implemented the parent model with a horizontal resolution of approximately 7 km covered the region from 17 to 30 n and from 116 to 133 e fig 1 and the nested model with a horizontal resolution of approximately 2 km covered the region from 21 7 to 26 2 n and from 120 4 to 123 5 e given that the buoy measured temperature sourcing mainly from a large amount of offshore water temperature will be demonstrated in following analyses thus a 2 km spatial resolution was considered good enough to reproduce the processes leading to the extreme cooling the vertical gridding of the parent and nested models consists of 20 s coordinate levels theta s 6 theta b 0 hc 10 that were unevenly distributed for better resolution of the upper ocean relative to that of bottom layer or lower layers song and haidvogel 1994 model bathymetry was created by merging regional bathymetry of 500 m spatial resolution processed and distributed by ocean data bank odb sponsored by the ministry of science and technology taiwan and etopo2 global ocean bottom topography smith and sandwell 1997 additionally the model was driven by momentum forcing from hourly sampled gridded 0 5 degree latitude x 0 625 degree longitude modern era retrospective analysis for research and applications version 2 merra 2 winds https disc gsfc nasa gov which is one of the most up to date wind forcing products the comparison between winds retrieved from in situ measurement and merra 2 from july 24 to 31 can be seen in figure s1 in supplementary online materials som generally merra 2 winds show consistent patterns as the winds measured by the weather buoy fixing to 10 m height using the log law model manwell et al 2009 therefore the wind product was deemed to be reasonable to drive roms by contrast atmospheric fields retrieved also from merra 2 including air temperature relative humidity precipitation rate incoming shortwave radiation and outgoing longwave radiation 0 5 degree latitude x 0 625 degree longitude were integrated for calculating net heat flux across the air sea interface which consequently influenced the sst variability the model initial conditions and lateral boundary conditions were derived from the data assimilated hybrid coordinate ocean model hycom global solutions with 1 12 degree spatial resolution through https www hycom org dataserver gofs 3pt1 reanalysis cummings 2005 for the fungwong study model simulations were executed from july 19 to july 31 august 5 for trajectories analysis 2008 temporal frequency of the model is 5 min and the output frequency is 1 h in addition first three days outputs of individual simulations are excluded in following analyses for avoiding possible insufficient spinning up of smaller scale features in the inner domain detailed description of roms is given by shchepetkin and mcwilliams 2003 2005 related validations of the model skill for storm simulations can be seen in zheng et al 2010 2014 and shen et al 2021 fig 2 shows the systematic comparison of temperature variations retrieved from all available argo floats passing the study area during the fungwong passage and corresponding roms model simulations positions of the floats used for the evaluation can be seen in figure s2 in som 3 extreme cooling response to fungwong 3 1 satellite observations fig 3 a e shows the cooling responses derived from satellite observations of tmi amsr e windsat gridded sst as the typhoon fungwong progresses westward on 26 july it starts affecting the upper ocean temperature and causes a near surface cooling of about 3 c centered at 22 n 129 e afterward the cold water patch grows to stronger cooling response with an sst drop of more than 4 c relative to the state of pre typhoon passage subsequently the main offshore cooling surrounding 20 n 23 n 126 e 129 e triggered by fungwong starts to weaken on 28 july at the same time fungwong leads to a marked cooling on the ecs shelf region off northeast taiwan on 29 july cooling at nearshore and offshore regions strengthens and weakens respectively afterward offshore and nearshore coolings decrease gradually until august 5 figure not shown recovering to the state of pre typhoon passage 3 2 roms simulations fig 3 f j shows the corresponding cooling progress during july 26 30 2008 from model simulations overall simulations reproduce the satellite observed cooling progress reasonably well with respect to the timing and location of the cold water patches despite inconsistent of time averaging for composite sst image and model simulation leading to minor difference between modeling and remote sensing ko et al 2016 however in the nearshore region satellite observed sst shows much weaker cooling relative to simulated sst brewin et al 2017 compared sst derived from avhrr with sst collected by in situ instrument at coastal and offshore regions respectively they indicated that the lower performance of satellite based sst at the coastal region when compared with offshore waters this result could serve as a possible reason for the discrepancy in our comparison between model simulations and satellite observations at the shelf region 3 3 moored buoy temperature measurements fig 4 shows direct measured temperature of approximately 12 5 c cooling at 0 6 m below the sea surface red line by the moored buoy temperature meter deployed at longdong 25 0983 n 121 9219 e black dot in fig 1 together with model simulated sst blue line and satellite observed sst gray dashed line during the passage of fungwong here mursst data were used in the comparison because merged microwave data are absent in the nearshore region model simulations show relatively good skill in resolving the temporal variation of near surface temperature by contrast mursst at nearshore regions largely underestimates and almost fails to capture the cooling feature in response to progression of fungwong the result shows again the disadvantage of satellite observations for monitoring temperature surrounding shelf regions same conclusion can be seen in woo and park 2020 their results emphasized the importance of using real time in situ measurements as much as possible to overcome the increasing sst errors in coastal regions nevertheless for offshore regions satellite observations still provide reliable sst measurements brewin et al 2017 woo and park 2020 which can help validate model simulations at different regions 3 4 detailed progression of strong cooling surrounding longdong fig 5 a e shows the simulated progress of fungwong triggered extreme sea surface cooling surrounding longdong off northeast taiwan the sea surface cooling starts at 06 00 on july 28 with a temperature drop of approximately 6 c relative to surrounding regions fig 5c cooling appears just for a while when wind direction changes from due east to southeast this phenomenon shows a marked discrepancy about the timing of the generation of cooling response relative to the findings by doong et al 2019 which suggested that the presence of strong and persistent southerly winds inducing coastal upwelling is the dominant cause for the extreme cooling surrounding longdong later the cooling grows sharply and extends northeastward with an sst drop of more than 10 c meanwhile an sst drop of over 10 c covers an area of up to 2 500 square kilometers fig 5d examining the state below the sea surface provides additional information about the generation of sea surface cooling fig 6 shows transects of temperature in color shadings currents white arrows denoting vectors of u and w 103 the position of subsurface cold water denoted by 20 c isotherms in bold red contours as well as the position where the kuroshio mainstream kms passes through during the fungwong passage the position of kms was defined by outlines with northward current speeds greater than 70 cm s 1 in black contours contour intervals are 20 cm s 1 bold black contours denote current speed equal to 110 cm s 1 pink plus signs mark core positions of kms along 25 098 n the core position was defined by maximum northward current speed average between 0 50 m along the 25 098 n transect first in reference to the variation of kms the current core of kuroshio moves shoreward from 06 00 on july 27 to 06 00 on july 28 see fig 6a 6c and then gradually retreats toward offshore back to the original position from 18 00 on july 28 to 06 00 on july 29 6d 6e subsequently in reference to the variations of subsurface cold water with the kuroshio intrusion onto the shelf the subsurface cold water uplifts markedly from 18 00 on july 27 to 18 00 on july 28 6b 6d the dynamic linkage between onshore ward kuroshio intrusion and uplift of subsurface cold water can be explained by the geostrophic adjustment of the temperature field reacting to the presence of a strong current see fig 7 and tomczak and godfrey 1994 because isotherm depths on the offshore ward side of the current cannot change thus this process would lead to a steep rise of the thermocline from the ocean toward the coast tomczak and godfrey 1994 moreover the uplifted subsurface cold water feeds in the cooling feature revealed in the sst see fig 5c 5d after 06 00 on july 29 the uplifted cold water falls down gradually back to deeper subsurface see white arrows in fig 6e detailed cooling progress transects during the whole passage of fungwong can be seen in animation a1 in som 4 how fungwong triggers extreme cooling 4 1 three dimensional ocean state and its evolution control experiment in this section to further understand the exact mechanism s therein the variations of three dimensional velocity fields u v w and temperature during the entire typhoon passage at the longdong site centered at 25 08 n 122 e were examined it is noting that the analysis was performed at the position of maximum surface cooling 25 08 n 122 e instead of exactly the position where buoy station deployed because the dynamic analysis must capture the most active process located at corresponding area by contrast the temperature comparison was conducted exactly corresponding to the position of buoy station to provide a validation of the simulated temperature variations since the buoy measured temperature was considered representative of a large amount of offshore water temperature see animation a1 in som this is the main reason why the analysis was performed slightly offshore relative to the temperature comparison corresponding to exactly the location of moored buoy in fig 8 u velocity shows a sudden and sharp shoreward shift from 18 00 on july 27 to 15 00 on july 28 afterward u velocity changes to almost zero a few hours later the kms starts to move offshore with an eastward velocity larger than 1 m s 1 meanwhile with the shoreward shift of u component velocity the v component velocity increases drastically to more than 1 3 m s 1 this result implies that the kms intrudes shoreward toward the longdong in situ measurement site subsequently just behind the westward shift of the kms an extreme and rapid upwelling appeared from 00 00 on july 28 to almost 20 00 on july 28 as shown in the w component velocity field accordingly a strong subsurface cooling originating from the deeper region depth 100 m can be found immediately after the drastic upwelling from 00 00 on july 28 during this time the uplifted subsurface cold water attains a depth of about 25 m and provides a favorable source of cold water for entrainment vertical mixing price 1981 later a strong temperature drop extends all the way to the sea surface meanwhile the drop in temperature reaches up to approximately 15 c relative to the state prior to the fungwong passage this result shows the important role of the uplift of the subsurface cold water in the response of extreme sea surface cooling to the fungwong passage as shown in the previous section nevertheless the exact role of strong southerly winds wind boost during 10 00 12 00 on 28 july on the extreme cooling response to fungwong remains unclear 4 2 experimental group to further differentiate the effects of the shoreward intrusion of the kuroshio and southerly wind on the uplift of subsurface cold water and thus the generation of the extreme cooling here we conduct two additional experiments 1 exp nosw experiment using complete wind forcing but excluding southerly wind over the whole model domain and 2 exp sw experiment using only southerly wind during the fungwong passage the difference between the three model experiments can be seen in table 1 it should be noted that wind fields in those experiments are no longer physical fig 9a shows the results of exp nosw simulation compared with the results of control simulation with complete dynamics fig 8 exp nosw shows a similar pattern of kuroshio intrusion but with slightly weaker strength without the contribution of southerly wind it still leads to a sea surface cooling about 8 c 9 c with similar pattern and timing as the cooling response simulated in the control experiment fig 9b shows the result of exp sw which can be used to elucidate the role of southerly wind on the generation of extreme cooling to fungwong with only the influence of southerly wind and excluding the influence of northeasterly and easterly winds carried by the first half passage of fungwong kuroshio intrusion like signals e g negative u velocity anomaly sharp increase in positive v velocity anomaly are absent furthermore the upwelling and consequential cooling largely decrease in comparison with either exp nosw or control simulation the results of these experiments clearly demonstrate that the southerly wind and consequential upwelling are not the main cause for triggering the extreme cooling to fungwong by contrast the prior stronger upwelling driven by the shoreward intrusion of the kuroshio tied to easterly and northeasterly winds plays a dominant role the comparison of results of exp nosw fig 9a to control experiment shows that the strong southerly wind plays a positive role in further enhancing the sea surface cooling this result could be due to southerly wind enhancing entrainment mixing by injecting more momentum entering the upper ocean and mixing more uplifted cold water 25 m depth entering the mixed layer 4 3 heat budget analysis the relationship between the dominant mechanisms triggering extreme cooling and consequential temperature variations were further examined by heat budget analysis following glenn et al 2016 and kuo et al 2018 the roms conservation of heat equation was used to quantify the relative contributions of the different terms responsible for the simulated distinctive sea surface cooling during the passage of fungwong the general conservation equation for the heat budget in roms is given below 1 t tendency t u t x v t y horizontal advection w t z vertical advection z k t z vertical mixing d t horizontal mixing residual with the following vertical boundary conditions for surface eq 2 and bottom eq 3 respectively 2 k t z surface q n e t ρ 0 c p 3 k t z z h 0 where t is time t is the temperature u v w are the three component of velocity k is the vertical diffusivity coefficient d t is the horizontal mixing term q n e t is the net surface heat flux ρ 0 is density of seawater c p is the specific heat capacity of seawater and h is the depth fig 10 displays evolutions of separate terms in eq 1 from the surface to a depth of 100 m at the model grid closest to the maximum sea surface cooling 25 08 n 122 e it is noted that the net surface heat flux q n e t was included in the simulation in roms through the term of vertical diffusivity mixing following the surface boundary condition of k t z z 0 q n e t ρ 0 c p eq 2 as noted in both hedstrom 2018 and glenn et al 2016 thus q n e t was not shown directly in the heat budget analysis fig 10 more details about the temperature budget in roms can be seen in section of roms heat balance analysis in glenn et al 2016 and section of vertical boundary conditions 3 2 in hedstrom 2018 here residual is related to the slightly different temporal integrating intervals time varying vertical s coordinate used in roms and influence of friction glenn et al 2016 in addition the magnitude of the average residual in our analysis is 5 10 5 c s which is approximately one order smaller than the other terms except horizontal mixing fig 10a shows the temperature rate of change which is the sum of the zonal advection fig 10b meridional advection fig 10c vertical advection fig 10d and vertical mixing terms fig 10e additionally the horizontal mixing term fig 10f is close to zero and does not contribute to the rate of change of temperature as shown in fig 10a a relatively marked decrease in surface mixed layer temperature began at approximately 06 00 on july 28 showing consistent progress as shown in two dimensional sea surface cooling fig 5c the cooling tendency continued to almost 24 h till 06 00 on july 29 when it reached its lowest value however before the cooling tendency reached the sea surface at approximately 00 00 on july 28 another cooling tendency originated from the deeper region 100 m depth and progressed all the way toward the sea surface with time a few hours later this cooling tendency originated from deeper layers combined with the prior cooling tendency of the shallow region 0 25 m depth according to temperature contributions by individual terms fig 10b 10e the influence of vertical advection w t z occurred approximately at 19 00 on july 27 seems to dominate the cooling tendency originating from the deeper region by contrast the gradual uplift of deep cold water tied to vertical advection upwelling shows consistency with the uplift of cold water shown in transect plots fig 6 overall during this period the uplift of cold water from the deeper region provides a favorable cold water source for later entrainment vertical mixing associated with strong wind forcing carried by the fungwong passage in the meantime the variation of temperature in the upper ocean 0 25 m depth was due to the combined action of vertical advection and vertical mixing the vertical mixing contributed to cooling in the mixed layer but warming in the upper thermocline underlying strong wind forcing due to the process of mixed layer deepening price 1981 generally this would result in a cooler and thicker mixed layer however as shown in fig 10e the upper thermocline rises and depresses the thickness of the mixed layer the thermocline rising is attributed to the influence of the strong vertical advection of cold water the continued supply of cold water from the deeper region would largely enhance the sea surface cooling through entrainment mixing in the experiment of exp sw the cooling tendency in the surface layer triggered by entrainment mixing is reduced to 24 relative to control simulation forced using complete wind fields calculated within 0 25 m again this result confirms the key role of the uplift of subsurface cold water driven by the onshore intrusion of the kuroshio on the generation of extreme cooling response to fungwong passage 5 where the extremely cold water advects to track where the surface cold water advects in three dimensions i e the sink of the extreme sea surface cooling water triggered by fungwong roms modeled floats with temperature tracers were released over the region where maximum sea surface cooling occurred surrounding longdong see fig 1c and fig 5 the release period from 06 00 on july 28 to 06 00 on july 29 is decided in accordance with the period of occurrence of maximum cooling refer to fig 4 the initial depths of the released floats are 0 6 m below the sea surface fig 11 shows the trajectories of modeled floats along with their temperature variations in color shadings the individual float trajectories during different stages of release show how the cold water moves in response to the influences of strong wind forcing carried by fungwong and the northward flowing kuroshio current for the initial stage fig 11a showing trajectories of floats released during 06 00 12 00 on july 28 part of the floats drift northward due to the influence of strong southeasterly southerly wind meanwhile cold water trajectories extend northward to about 200 km with temperature gradually recovering to the state of ambient waters subsequently for the stages of floats encountering extreme cold waters fig 11b c the direction of majority of the trajectories shifts gradually from north northeast toward northeast at this time the cold water trajectories with a temperature drop larger than 10 c extend northward and northeastward over 300 km subsequently with the diminishing strength of southerly wind float trajectories bend to northeast eastward toward the path of kms fig 11c d at this moment the kuroshio current gradually becomes the dominant forcing determining the movement of most of the floats trajectories record cold waters extending more than 500 km with a temperature drop above 5 c fig 11d finally floats with a temperature drop of more than 5 c extend all the way eastward northeastward toward the south end of japan kyushu along the kuroshio downstream figure s4 in som all the floats released during the fungwong passage can be separated into two main groups the first group accounting for 58 of the total float trajectories are those floats that move northward onto the ecs shelf that eventually mixes into deeper region refer to the depth variability of all the modeled floats in figure s3 in som with slower movement due to the lack of energetic circulation over the shelf after typhoon passage the other group of floats are those which get on the highway kms and move eastward all the way with a higher velocity of about 1 1 5 m s 1 this groups account for 42 of the total trajectories the second group of floats move eastward for about 600 km on the basis of the model simulation limited to a duration of 7 days to compare the difference of particle trajectories seeded from the extraordinary cooled region ecr and the other regions we conduct another online experiment with floats seeded at 0 5 degree east and 0 5 degree north of the original ecr respectively the results show interesting but unsurprising results that is for floats released east of the ecr trajectories were advected along the kms due to the influence of the kuroshio current by contrast once the trajectories seeded 0 5 degree north of the ecr majority of the trajectories were advected northward toward the ecs continent shelf see figure s5 in som as noted in previous studies the transport of water with anomalous temperature differences is also responsible for unusual weather patterns including surface winds clouds regional atmospheric circulation and rainfall e g barrick et al 1977 reason 2001 beal et al 2011 this finding implies that the far reach effect of the extreme cooling response to tc passages such as fungwong should not be overlooked nevertheless the trajectories analysis was conducted on the basis of the currents provided by roms model simulation characteristics revealed by the trajectories analysis must include errors because no model is perfect 6 summary this study focused on an extreme sst drop of 12 5 c in response to tc fungwong 2008 passage which was one of the strongest cooling responses recorded by historical in situ instruments after a typhoon passage on the basis of limited in situ measurements of temperature satellite observed sst and a suite of numerical experiments performed using roms the detailed progress of how the fungwong triggers such an extreme cooling were reproduced and systematically investigated source of the cold waters feeding the extreme cooling and possible mechanisms triggering the cooling as well as consequential effects of extreme cooling on the surrounding oceanic environment were unveiled the major results are summarized as follows 1 the source of cold water feeding the extreme sea surface cooling in response to fungwong is due mainly to the strong uplift of subsurface cold water near the coastal region tied to an onshore intrusion of the kuroshio current driven by easterly northeasterly winds 2 sensitivity experiments differentiated the effects of shoreward kuroshio intrusion and the later southerly wind on the uplift of subsurface cold water the shoreward kuroshio intrusion plays a relatively dominant role on the generation of extreme cooling 3 southerly wind plays a constructive role in the enhancement of entrainment mixing and thus sea surface cooling 4 heat budget analysis explains the direct relationship between the possible mechanisms and consequential temperature variations 5 overall the extreme sea surface cooling response to fungwong results mainly from a combined effect of prior uplift of subsurface cold water due to wind driven onshore kuroshio intrusion and later entrainment mixing partially enhanced by persistent southerly wind 6 modeled float trajectories with temperature variations outline the affected region due to sharp sea surface cooling to fungwong and indicate that the effect of cooling might extend all the way toward the south end of the japan main island previous studies indicated that the transport of water with anomalous temperature differences is responsible for unusual weather patterns barrick et al 1977 reason 2001 moreover the effects of rapid decreases in water temperature on fish and aquaculture have also been well documented hoag 2003 troy et al 2012 our modeled float trajectories suggest that the corresponding effects resulting from the cold water patches on the ecs continental shelf and far field oceanic environment ecological systems weather patterns and aquaculture may be important the progress of the strong wind driven onshore kuroshio intrusion might lead to nutrient influx upward into the euphotic layer e g tsai et al 2008 which may consequently enter the ecs continental shelf this process deserves the attention of biological ecological and geochemical researchers declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work is supported by national science and technology council of taiwan through grants of 108 2628 m 003 001 my3 and 111 2611 m 003 003 my3 the authors would like to thank the anonymous reviewers and editors for their very constructive comments their valuable comments and suggestions largely improve the presentation of this work thanks to nasa jpl po daac remss gsfc central weather bureau taiwan comc ncku scientific teams for processing and providing the essential data sets as well as hycom and roms teams 
23793,a number of models have been developed to simulate hypoxia in the chesapeake bay but these models vary in complexity and in which processes they represent in this study we implement a previously published biogeochemical code bioredoxcnps developed for open ocean waters that includes cryptic microbial sulfur cycling within the chesroms physical model of the chesapeake bay sulfur cycling can increase rates of denitrification and anammox in anoxic waters but the net impacts of such changes on oxygen nitrate and ammonium are not understood we compare the results to a physically identical simulation with an estuarine biogeochemical cycling code previously implemented and calibrated in the bay ecb the ecb code neglects sulfur cycling but includes burial of particulate organic matter pom and cycling of dissolved organic matter dom and uses different values for many parameters governing phytoplankton growth and particle dynamics although the bioredoxcnps model produces a better simulation of oxygen and nitrate at a key test site this turns out not to be due to the inclusion of sulfur cycling instead large differences in modeled oxygen and ammonium are largely due to whether or not the biogeochemical codes include cycling of dom and sedimentary burial of pom changes in light attenuation produce large changes in nitrate changes in parameters used in both biogeochemical codes in particular particle sinking velocities tended to compensate the other differences in model construction the quantitative impacts of these choices for simulating chesapeake bay have not previously been documented in the peer reviewed literature predictions of hydrogen sulfide from our merged model were very sensitive to the choice of parameters and light attenuation this suggests that observations of hydrogen sulfide could help to constrain these processes in future models keywords coupled nitrogen and sulfur cycles biogeochemical parameters model comparison predictions of hydrogen sulfide chesapeake bay data availability data will be made available on request 1 introduction estuaries are key locations where rivers couple terrestrial processes with ocean biology and chemistry these systems have generated research interest due to their abundant biological resources and their crucial role in global carbon and biogeochemical cycles bauer et al 2013 bianchi and bauer 2011 canuel et al 2012 as the largest estuary in north america the chesapeake bay plays a particularly important role in coastal nutrient transformation transport and burial boynton et al 1995 much effort has been made to study these processes which can impact the bay s ecosystem and its economic productivity of all the processes affecting the bay eutrophication has emerged as a principal threat eutrophication arises from an increase in nutrient and dissolved organic matter dom concentrations leading to a greater production of particulate organic matter pom in the water column or on the seabed gray et al 2002 this results in hypoxia defined here as oxygen concentrations less than 62 5 mmol m3 when the oxygen consumed during the degradation of pom and dom exceeds the oxygen supplied from gas exchange mixing and advection hypoxia has been shown to cause mortality events for recent events within the chesapeake bay see luckett 2020 contributing to metazoan population decline and resulting in so called dead zones devoid of fisheries resources including crabs shrimp and fish rabalais et al 2002 renaud 1983 benthic organisms are especially vulnerable to coastal hypoxia anoxia and euxinia when water is both anoxic and sulfidic because they live in and near the sediments where oxygen tends to be depleted relative to the overlying water column seliger et al 1985 vaquer sunyer and duarte 2008 under intense hypoxia as oxygen levels become undetectable nitrate becomes the dominant oxidant for organic matter degradation subsequently when nitrate is depleted sulfate reduction becomes the dominant decomposition pathway and produces hydrogen sulfide h 2 s detection of which is often the key criterion for euxinia in the water column euxinia can reduce biodiversity by harming surviving organisms through lethal and sublethal impacts luther and church 1988 the production of h 2 s also has the potential to change biogeochemical cycling in the chesapeake bay marvin dipasquale and capone 1998 estimated that decomposition of organic matter via sulfate reduction remineralized 18 32 of the primary production at three sites in the bay h 2 s produced by this process can move upwards in the water column and act as a sink for oxygen when it is oxidized further accelerating hypoxia roden and tuttle 1992 however recent work has shown that sulfide can also be oxidized using nitrite and nitrate resulting in a loss of bioavailable nitrogen canfield et al 2010 such losses reduce the potential for hypoxia this process has been referred to as cryptic sulfur cycling as sulfide produced from sulfate can be rapidly recycled and thus may not be detected in the water column on observational time scales arora williams et al 2022 find that organisms which are known to have these capabilities are ubiquitous and relatively abundant within the chesapeake bay however so far there is no observational proof that these organisms are actually engaged in cryptic sulfur cycling moreover rates of cryptic sulfur cycling and what thresholds in oxidants govern the transitions between oxic respiration denitrification and sulfur cycling within the bay are not known earlier chesapeake bay models testa et al 2014 cerco and noel 2017 incorporate biogeochemical cycling bgc codes which have a simplified representation of the impacts of sulfur cycling in which an idealized reductant representing either h 2 s or methane is released from sediments and oxidized in the water column however these models do not directly simulate water column sulfate reduction sulfide oxidation by nitrate or sulfide oxidation by nitrite recently azhar et al 2014 developed a code referred to as bioredoxcnps to specifically study these processes and calibrated it to fit the peru upwelling system where an active cryptic sulfur cycle has been documented canfield et al 2010 this code was later updated to work with roms version 3 6 in hantsoo et al 2018 which is the version we use here bioredoxcnps simulates a number of additional species e g nitrite and simulates rates of sulfate reduction annamox and canonical denitrification which are consistent with the peru upwelling system canfield et al 2010 azhar et al 2014 an obvious question is how well such a model would do if it were simply translated to chesapeake bay since at present there are no constraints on the rates of cryptic sulfur cycling in the bay it is theoretically worthwhile to perform such an exercise and evaluate the skill of the resulting model against a baseline model the ecb code of feng et al 2015 that has been calibrated in the bay if it were the case that we could simply translate the open ocean model to the coastal zone this would be a powerful demonstration that our understanding of sulfur cycling is robust as we describe below however translating the model to the bay turns out not to be as straightforward as we had hoped when the bioredoxcnps code and ecb code are run in a physically identical simulation of the chesapeake bay bioredoxcnps does improve certain aspects oxygen nitrate of the simulation however closer examination showed that other fields nitrite ammonium were degraded this spurred us to try to understand the sources of the differences between the simulations both biogeochemical components build on the fennel et al 2006 bgc code which partitions fixed nitrogen between nitrate and ammonium the ecb code adds to the fennel bgc module by a including dissolved organic nitrogen and carbon b simulating the burial of sinking particles in sediment and c allowing for absorption of photosynthetically active radiation par by chromophoric dissolved organic matter cdom and particles while the bioredoxcnps code has many similarities to the chesroms ecb code it was developed for the open ocean thus it does not include dom organic matter burial or absorption of par by cdom and particles additionally many processes common to the two codes have different parameter settings in order to isolate which processes were most important for contributing to the differences between the two codes we developed a merged version that includes both the sulfur and nitrogen cycling of bioredoxcnps and the burial and dissolved organic matter cycling of chesroms ecb in what follows we will distinguish between codes models and simulations codes have different representations of biogeochemical or physical processes models implement these codes in a particular configuration but may produce simulations with different values of parameters our merged model which we refer to as snp bur dom ches can then be regarded as a baseline for future work studying sulfur cycling even though sulfur cycling does not make a huge difference in our simulations in this paper we focus on understanding the differences in modeled biogeochemistry between simulations run with our merged code and the two published codes this manuscript is structured as follows the codes used in this study the details of how they are implemented into models and the simulations run with them are described in section two we begin our results in section three by looking at how the different changes affect predicted oxygen nitrate and ammonium fields when we change from the ecb to bioredoxcnps step by step while bioredoxcnps model does produce an improvement in the simulation of oxygen in the bay this is not primarily driven by adding sulfur cycling instead we find that the bioredoxcnps simulation s a exclusion of burial and dom cycling b exclusion of absorption by cdom c different values for parameters common to the codes each produced large effects but often in different directions in section four we conduct a sensitivity study to highlight three important factors that affect the model results we also show that the h 2 s distribution is more sensitive to how we construct our model than tracers like oxygen this study moves towards a more complete model for simulating chemical species and highlights key processes and parameters that control biogeochemical cycles in the chesapeake bay as such our results provide guidance for future experimental studies focused on hypoxia anoxia and euxinia 2 model description 2 1 physical model the coupled physical biogeochemical models used in this study were run with version 3 6 revision 898 of the regional ocean modeling system roms roms is a three dimensional time dependent simulation that uses the hydrostatic primitive equations shchepetkin and mcwilliams 2005 physical circulations were set to be identical across the different model runs as there was no feedback between biological and physical circulation while accounting for feedbacks between chlorophyll and shortwave absorption may improve temperature simulations kim et al 2020 ignoring such feedbacks for now allows us to attribute all differences between the models to the direct impacts of biogeochemical processes we use an implementation of the roms code for the chesapeake bay developed by xu et al 2012 and known in the literature as chesroms the chesroms model domain extends from 77 2 w to 75 0 w and from 36 n to 40 n covering the main stem and primary tributaries of the chesapeake bay the model extends seaward to the mid atlantic bight fig 1 to prevent boundary effects from altering tracer fields and mean velocity fields the horizontal grid uses orthogonal curvilinear coordinates with varying resolution but with a nominal resolution of around 1 km in the central bay more details are presented in accompanying methodsx paper jin et al subm 2 2 bgc codes and simulation setups 2 2 1 organizing the differences between the two original codes in this study we started by comparing two published biogeochemical models ecb bioredoxcnps shown in red in fig 2 which have been configured for specific locales and validated against observations both of them are descendants of the code of fennel et al 2006 and have many identical equations but they also differ in many ways we identify three classes of differences broken down in fig 2 parameters common to the two models which are assigned different values pathways only included in ecb burial and dom inclusion of cdom in light attenuation and pathways only included in bioredoxcnps sulfur cycling and phosphorus in order to evaluate the importance of each of these differences we developed a merged version of the two codes which enables us to perform a series of experiments schematically summarized in fig 2 that isolate the impacts of each change below we describe in more detail the three classes of differences between the two initial simulations 2 2 2 parameters common to each simulation the first set of differences are in equations in common to the two models both models have a single class of phytoplankton whose growth is limited by nutrients and whose major loss term is coagulation and sinking the particles produced by coagulation are divided into two classes small and large with the large particles sinking faster these particles then remineralize with the mode of remineralization oxic vs denitrifying to ammonium and subsequent nitrification determined by thresholds however as summarized in table 1 the parameter set governing these processes taken from the da et al 2018 model of the chesapeake has a significant number of differences with the parameter set taken from the hantsoo et al 2018 version of the azhar et al 2014 model of the peru upwelling system we can break the parameter differences down into three groups namely parameters governing photosynthesis those associated with particle dynamics and those governing the transformation of inorganic compounds because a full exploration of all these differences is too much for a single manuscript in this paper we highlight two sets of parameters we found to be important namely sinking velocities and nitrification rates sinking velocities are important because they control where remineralization occurs in water column sinking velocities from da et al 2018 are quite slow as the vertical sinking velocity is 5 m day for large detritus and 0 1 m day for small detritus sinking velocities from hantsoo et al 2018 are much faster with the vertical sinking velocity for large detritus being set at 20 m day and for small detritus at 2 m day although the remineralization rate for detritus in ecb is very low e g 0 03 day 1 at 0 c it is also temperature dependent so that at 25 c it has a value of around 0 1 day 1 giving an e folding depth of 1 m for small detritus and 5 m for large detritus by contrast in bioredoxcnps while the remineralization rate is 0 1 day 1 the e folding depth is 20 m for small detritus and 200 m for large detritus the water column depth in the chesapeake is around 20 m in the middle of the channel while the flat region away from the channel averages about 6 m this means that in ecb small and large detritus produced in the top few meters of the water column will mostly remineralize within the water column while in bioredoxcnps a large fraction of the detritus will reach the bottom and get remineralized buried there nitrification is another important process which can be thought of as having a different parametric representation in the two codes in ecb nitrification occurs in one stage with ammonium being directly transformed to nitrate at a maximum temperature independent rate of 0 05 day 1 this can be thought of as equivalent to allowing ammonium to be transformed to nitrite with a rate of 0 05 day 1 and nitrite subsequently being instantaneously remineralized to nitrate nitrification is limited by oxygen with a half saturation coefficient of 1 mmol m3 in bioredoxcnps there are two stages the first takes nh4 to no2 with a maximum rate of 0 05 day 1 the same as in ecb however the second step transforms no2 to no3 with a maximum rate of 0 005 day 1 both stages are limited by oxygen at 1 mmol m3 in bioredoxcnps because the e folding scale over which biomass decays is so much longer the rates of the production of ammonium are much lower as a result the rate of nitrification has to be slower in order to allow for reasonable levels of dissolved species particularly nitrite 2 2 3 inclusion of burial and dissolved organic matter the second set of differences is that ecb has burial of organic matter and explicitly represents don doc a fraction of phytoplankton and large detritus are partially resuspended as small detritus once they reach the bottom depending on near bottom turbulent velocities some fraction of the remaining benthic flux is buried permanently with the rest being remineralized the burial fraction follows henrichs and reeburgh 1987 where it is a function of the carbon flux to the bottom f b u r min 0 75 0 023 c a r b o n f l u x t o t h e b o t t o m 0 5797 this means that burial is very small when the flux of material is small and increases nonlinearly as the flux to the bottom does note that this can interact with sinking velocity higher sinking velocities mean that a larger fraction of biomass reaches the bottom and a higher fraction of that flux gets buried as we will demonstrate below including burial significantly reduces overall nitrogen concentrations however if all of the organic matter is diverted to particles most of them would be buried in shallow rivers and never make it out of rivers to main stem of the bay don is present at much higher concentration than nitrate and ammonium and is thus an important reservoir of nitrogen in the bay allowing some fraction of the organic matter to be diverted to don as in ecb was found to be necessary to allow hypoxia in the deep bay we therefore examine the inclusion of burial and don together in the model 2 2 4 inclusion of sulfur and phosphorus cycling the third set of differences is that bioredoxcnps includes sulfur and phosphorus cycling and has explicit no2 by including no2 nitrification denitrification is changed because no2 serves as an important pool of nitrogen that is not immediately accessible by phytoplankton in this sense it behaves like don in ecb however unlike don it can be directly lost via denitrification po4 also has the potential to produce nutrient limitation in phytoplankton though it turns out not to be very important in the model 2 2 5 light attenuation bioredoxcnps also retains the fennel et al 2006 scheme for the absorption of photosynthetically active radiation par which includes absorption by chlorophyll the vertical attenuation of light is described by the equation d p a r d z k p a r p a r where k p a r 0 04 0 024 c h l the first term on the right hand side is the absorption due to pure water and the second captures absorption by chlorophyll by contrast in the ecb code there is a formulation that implicitly relates the absorption to refractory doc delivered down rivers k p a r max 1 4 0 063 t s s 0 057 s 0 6 where tss represents total suspended solids this scheme has very low penetration of light high k par when s 0 at the river mouths with increasing light penetration as salinity increases the implicit assumption is that the additional cdom added by rivers is slowly diluted as it moves out to sea in a study to be reported fully in a future manuscript we find that this parameterization can be mimicked by adding cdom dependent absorption to the original fennel et al 2006 scheme as cdom in the bay shows a strong negative correlation with salinity 2 2 6 merged version of the model in order to identify and quantify sources of intermodel differences we developed a merged version of these codes a full schematic of this merged version is shown in fig 3 nitrate phosphate and ammonium come down the rivers light blue lines and can be taken up by phytoplankton via photosynthesis green lines phytoplankton are primarily lost via coagulation into large and small detritus red lines which sink to the bottom a fraction of phytoplankton and large detritus are partially resuspended fluorescent blue lines as small detritus once they reach the bottom there is a small loss to zooplankton gray lines which we do not focus on here detritus is solubilized to dom purple lines both detritus and dom can be remineralized brown lines to phosphate and ammonium this remineralization consumes oxygen but in the absence of oxygen dotted brown lines is associated with heterotrophic denitrification and consumes nitrate and nitrite in the absence of nitrate nitrite and oxygen remineralization proceeds using sulfate and produces hydrogen sulfide dashed brown lines hydrogen sulfide is oxidized back to sulfate orange lines using oxygen solid or nitrate nitrite dotted with the latter process resulting in denitrification ammonium can either be nitrified dark blue lines or consumed with nitrite via anammox dotted magenta lines in the absence of oxygen in sediments organic matter that is not buried is immediately remineralized black lines to phosphate and ammonium this remineralization consumes oxygen but in the absence of oxygen dotted brown lines is associated with heterotrophic denitrification and consumes nitrate and nitrite in the absence of nitrate nitrite and oxygen remineralization proceeds using sulfate and produces hydrogen sulfide dashed brown lines which pathway ends up being used depends on the concentration of oxidants in the bottom most layer 2 3 experimental design this new code allows us to develop an experimental design by combining three codes with two parameter sets for each code giving us a total of six potential simulations in order to highlight the differences between simulations we use a nomenclature that makes it evident what nutrients are cycled whether the model includes burial and dom and which parameter set peru vs chesapeake is used within each simulation the resulting nomenclature shows the increasing complexity and realism in the setup of the simulations in this paper we consider four of the six possible combinations in detail namely n bur dom ches snp peru snp bur dom peru and snp bur dom ches the other two are considered in the accompanying methodsx paper n vs snp contrasts whether the code models only nitrogen as in chesroms ecb or nitrogen sulfur and phosphorus as in bioredoxcnps bur dom indicates that the code includes organic matter burial in sediments and dissolved organic matter as in chesroms ecb finally ches vs peru denotes whether the biogeochemical parameters common to both of the two original codes are taken from da et al 2018 in the chesapeake or azhar et al 2014 in the peru upwelling system as modified by hantsoo et al 2018 for example the chesroms ecb model of da et al 2018 thus is identical to our n bur dom ches simulation while the implementation of bioredoxcnps with the original parameters used in hantsoo et al 2018 corresponds to our snp peru simulation a summary of how the differences between the original codes map across the simulations is shown in the upper half of table 2 as can be seen by examining table 2 and fig 2 comparing snp bur dom ches with n bur dom ches allows us to see differences due to adding sulfur cycling nitrite and changing light attenuation while retaining burial and dom and leaving common parameters unchanged in contrast by comparing snp bur dom peru with snp bur dom ches we can identify differences due to changing parameters while leaving the processes represented in the model unchanged finally by comparing snp peru with snp bur dom peru we can isolate differences due to removing burial and dom we note that this is not the only possible way to transform one model to another our full set of six simulations is presented in the companion methodsx paper jin et al subm where we demonstrate that changes in dissolved quantities are relatively robust to the order in which changes in the model configuration are made 2 4 initial conditions and boundary forcings all simulations were run for the year 2017 riverine inputs for n bur dom ches were taken from the dynamic land ecosystem model as in feng et al 2015 tracers found in common across multiple models iss nh4 no3 and don when included were set to have the same inputs for snp peru snp bur dom peru and snp bur dom ches the riverine input po4 was set to be the riverine input no3 divided by 36 6 a ratio calculated from field data https www chesapeakebay net state pollution the riverine inputs of sdep and ldep were set to the values of sden and lden divided by 16 respectively which is the redfield ratio reflecting observations of particulate nitrogen and phosphorus within the bay semilabile and refractory dop were also set to the corresponding don concentrations divided by 16 when included sulfur was not included in the riverine input in this study consistent with burke et al 2018 who found sulfate concentrations in these waters being low 0 5 mm compared to much higher concentrations in seawater at the seaward boundary we applied a mix of radiative boundary conditions in which tracers like detrital organic matter are allowed to leave the domain but do not return through the boundary and radiation with nudging in which tracers like temperature and salinity entering the domain are set to climatological values our new sulfur variables are set to have zero flux on the seaward boundary which makes little difference on the short time scales for which we run here especially given the low levels of water column sulfur cycling on the shelf we will amend this in future iterations of the code all our sulfur thus starts in form of dissolved sulfate and cycles between this form and h 2 s atmospheric deposition of dissolved inorganic nitrogen din was also included in the models as a source of din to the estuary since it is an important fraction of the total din inputs to the chesapeake bay da et al 2018 initial conditions for the n bur dom ches simulation were taken from a previously run chesroms ecb simulation that was initialized in 1979 and run for 38 years it thus represents a spun up state of the system those initial conditions in common with n bur dom ches were set to be the same in snp peru snp bur dom peru and snp bur dom ches the initial values of po4 sdep ldep and dop were all set to be 16 times smaller than their corresponding nitrogen variables from da et al 2018 all the other initial values of new state variables were set to zero 3 model data comparison 3 1 comparing the base simulations found in the literature n bur dom ches and snp peru 3 1 1 qualitative comparison of annual cycle of oxygen at cb3 3c both n bur dom ches and snp peru produce reasonable simulations of oxygen figs 4a and 4b show the oxygen concentrations in these two simulations with observations overlaid as colored circles mismatches can be seen where the circles are visible against the background of the model n bur dom ches simulates a relatively high oxygen concentration near the surface from january to mid april around 350 mmol o 2 m3 from mid may to late august a large hypoxic zone the so called dead zone shown by magenta shading extends from near the bottom to around 8 m in depth around this time period the oxygen concentration is still high near the surface but decreases rapidly at increasing depths in the water column corresponding to water column stratification and warming in the bay during the summer however during may and october the observations show noticeably lower oxygen concentration near the bottom than the n bur dom ches simulation does the snp peru simulation as shown in fig 4b shows a similar distribution of oxygen although the hypoxic zone lasts longer and is thus more consistent with early onset of hypoxia in 2017 3 1 2 quantitative evaluation of model skill in simulating oxygen compared to observations n bur dom ches fits both very low and very high concentrations of oxygen well but overpredicts intermediate values in the 50 200 mmol m3 range fig 5a snp peru does better in this range a useful way to objectively compare these fields is the coefficient of determination referred to as r2 which can be written as 1 error variance sample variance note that the coefficient of determination can become negative if the error variance exceeds the sample variance in this sense it differs from the r2 produced by a regression model where by definition the error variance is smaller than the sample variance both r2 and r2 are affected by differences in the pattern of spatiotemporal variation between modeled and predicted fields however r2 also incorporates the contribution to error variance from differences in the mean value and from the amplitude of spatiotemporal variation and as such it is a more comprehensive normalized measure of the error with respect to observed oxygen snp peru produces a substantial increase in r2 from 0 72 to 0 85 table 3 even though it underpredicts oxygen near the surface this is because lower observed oxygen concentrations near the bottom are better simulated in snp peru than in n bur dom ches 3 1 3 evaluation of the simulations of nitrate and ammonium simulations of nitrate from n bur dom ches and snp peru at the bay bridge station are shown in figs 4d and 4e in the n bur dom ches simulation the nitrate concentration near the surface is around 40 50 mmol n m3 from january to late may with some occasional drops this is somewhat higher than the observations nitrate then drops quickly beginning in early june the nitrate concentration remains between 0 and 8 mmol n m3 throughout the water column during the summer months until early november the low values are due to primary productivity taking up nitrate in the spring but also to denitrification removing nitrate in the summer months surprisingly the 1 56 gmol uptake of nitrate by new production and 1 81 gmol removal by denitrification are nearly equal in magnitude see table 4 in snp peru the spatiotemporal distribution of nitrate is similar to n bur dom ches from june to november although the maximum nitrate concentration in the spring is lower around 48 mmol n m3 depleted nitrate throughout the water column is also observed in this model in the same time period as in n bur dom ches however from near the bottom to around 11 m in depth nitrate decreases in mid april and remains low until late october comparing with observations shows that snp peru more accurately models low nitrate concentrations between around 10 m in depth and the bottom from mid january to mid april while results from n bur dom ches are higher than the observations a scatter plot of nitrate fig 5b also shows that modeled nitrate in snp peru is closer to the observational data with the linear fit red line lying on top of the black 1 1 line while the linear fit for n bur dom ches is offset above this line the r2 for nitrate is much higher in snp peru 0 46 than in n bur dom ches 0 29 with the negative value indicating that the rms error variance is larger than the observational variance at this site fig 4g and fig 4h compare the simulations of ammonium from n bur dom ches and snp peru in n bur dom ches the ammonium concentration from near the bottom to around 10 m in depth begins to increase from mid april and peaks at 42 mmol n m3 in mid june then from late july it drops gradually and becomes low again in early october given that peak values of ammonium between 2015 and 2019 at this site never exceeded 25 mmol n m3 we conclude that n bur dom ches predicts too much ammonium during the summer in snp peru the ammonium concentration near the bottom increases in mid april and decreases in early september it peaks at a value of 68 mmol n m3 in june the ammonium depleted zone near the surface is similar to n bur dom ches after early september the ammonium concentration throughout the water column is lower than n bur dom ches by contrast in the summer the ammonium concentration in snp peru is about twice that in n bur dom ches a scatter plot of observed vs modeled ammonium fig 5c shows that the modeled results of n bur dom ches are closer to the observational data while snp peru gets worse results when it comes to ammonium the significant overprediction in ammonium means that the r2 for this variable decreases between n bur dom ches 0 32 and snp peru 1 12 though clearly errors are large in both simulations note however that the overprediction in snp peru is greatest deep in the water column there is actually less ammonium above the pycnocline thermocline oxycline during the summertime compare fig 4g and h more blue symbols show up above the pycnocline in fig 4g also fig 4i 3 1 4 annual cycle of differences between the two published models the differences between the two codes show clear annual cycles highlighted in the right hand column of fig 4 for most of the year the oxygen difference between n bur dom ches and snp peru is small in the range of 0 30 mmol o 2 m3 fig 4c from the bottom to around 10 m in depth snp peru shows obviously lower oxygen than n bur dom ches between mid april and mid may near the surface during the same time period oxygen in snp peru is slightly higher than in n bur dom ches during the summer months near the surface snp peru shows a lower oxygen concentration nitrate predicted by snp peru is lower than that predicted by n bur dom ches for the whole year fig 4f specifically from mid april to early june nitrate concentrations in snp peru are much lower than n bur dom ches throughout the water column compared to other times with differences up to 50 mmol n m3 the high nitrate associated with the spring freshet is less persistent in snp peru than in n bur dom ches fig 4i shows the ammonium difference between snp peru and n bur dom ches snp peru simulates more ammonium than n bur dom ches for the most part from january to august from mid april to the end of june and from the near bottom to around 10 m in depth ammonium in snp peru is about 20 30 mmol n m3 higher than n bur dom ches the differences in ammonium have a pattern that is somewhat anticorrelated with the differences in oxygen suggesting a tradeoff between oxygen and ammonium that we will see more clearly in some of our other simulations 3 2 pathways from n bur dom ches to snp peru as shown in fig 2 and table 2 we can break the differences between snp peru and n bur don ches down into three consecutive changes in model configuration the detailed differences resulting from each change are shown in fig 6 to illustrate which aspects of the models contribute to the differences seen in the right hand column of fig 4 when adding sulfur and phosphorus cycling and changing the light attenuation fig 6 left hand column there is a huge decrease in nitrate especially in late april and may r2 for nitrate shows an increase from 0 29 to 0 20 table 3 in comparison changes in oxygen and ammonium are relatively small though oxygen does increase slightly in deep waters in the spring r2 for oxygen decreases from 0 72 to 0 59 while for ammonium increases from 0 32 to 0 03 changing parameters from the ches to peru set fig 6 middle column results in an increase of nitrate from february to may partially canceling the change produced by adding pathways and changing light attenuation r2 for nitrate increases from 0 20 to 0 49 however changing the parameters also increases oxygen during the summertime from near the bottom to 8 m in depth while ammonium drops during the same period r2 for oxygen drops significantly from 0 59 to 0 19 but r2 for ammonium increases from 0 20 to 0 49 removing burial and dom leads to opposite sign changes in oxygen and ammonium fig 6 right hand column in contrast with the middle column oxygen decreases and ammonium increases during the summertime r2 for oxygen significantly increases from 0 19 to 0 85 while ammonium drops to 1 13 however nitrate does not change a lot in this step r2 for nitrate only changes a little from 0 49 to 0 46 comparing fig 6 with fig 4 suggests that changes in oxygen and ammonium between snp peru and n bur dom ches are mainly due to removing burial and dom however changes in these two fields produced by removing burial and dom right hand column fig 6 are significantly counteracted by changing parameters in common to the two models middle column fig 6 for nitrate changes between the two original models result from adding sulfur and phosphorus cycling and changing the light attenuation left hand column fig 6 but also show significant cancellation from changes in the common parameters middle column fig 6 these changes in results call for a detailed examination of the budget of nitrogen table 4 along with the partitioning of nitrogen between different species fig 7 adding the new pathways results in a redistribution of the loss of nitrogen from water column denitrification to burial nitrogen burial between january and july increases from 1 348 gmol in n bur dom ches to 2 773 gmol in snp bur dom ches however sediment denitrification and water column denitrification over the same time period decrease from 0 81 gmol to 0 0079 gmol and from 1 015 gmol to 0 062 gmol respectively when looking at the pie charts for n bur dom ches and snp bur dom ches it is noticeable that there is a buildup of nitrite when compared to observations blue sections so that this drop in denitrification is not because we lack nitrite when changing the parameters in common between the two codes from the ches to the peru parameter set it is shown that using peru parameters substantially increases the nitrogen burial from 2 773 gmol to 4 736 gmol while the total nitrogen drops a lot from 2 86 gmol to 2 33 gmol corresponding to a decrease in mean concentration of 7 5 μ m total amount of sediment denitrification plus water column denitrification also drops from 0 0079 gmol to 0 039 gmol and from 0 062 gmol to 0 055 gmol respectively when removing the burial and dom the nitrogen budget shifts in the opposite direction resulting in a huge amount of denitrification however in order to get that much denitrification in the water column the bioavailable nitrogen and nitrite end up being heavily overestimated as shown by the pie chart fig 7 for snp peru orange and blue sector these results raise a number of questions one is whether the realistic decreases in nitrate in snp bur dom ches are due to the unrealistic buildup of nitrite alternatively could the different light attenuation scheme be responsible can we fix nitrite without making the budgets of other nitrogen species worse another question is which parameters in common to the two models have the biggest impact on the solution in order to examine these questions in more detail we now turn to a set of sensitivity studies 4 results and discussion 4 1 sensitivity studies light attenuation sinking velocities and nitrification rates in order to develop an understanding of which of the many parameters changed between the models had the biggest impact on model performance we performed a number of sensitivity studies based on the baseline model snp bur dom ches here we report on two that we found to have major impacts on some subset of oxygen nitrate and ammonium namely 1 light attenuation and 2 particle sinking velocities we also show that we can eliminate the unrealistic buildup of nitrite found in snp simulations without changing oxygen ammonium and nitrate significantly the three such simulations here involved 1 changing the light attenuation scheme of snp bur dom ches so that cdom absorption was implicitly included as in the ecb code 2 increasing the sinking velocities for large and small detritus in snp bur dom ches to those used in the peru code 3 increasing the nitrification rate to 0 05 day 1 decreasing the half saturation concentration of no2 for denitrification to 1 mmol m3 and removing a 2 mmol m3 threshold for nitrite to participate in denitrification in snp bur dom ches again making the numbers more similar to ecb in fig 8 we examine the changes in oxygen nitrate and ammonium paralleling those in the right hand column of fig 4 and in fig 6 for three sensitivity studies relative to snp bur dom ches in fig 9 we examine the changes in bay wide inventory of nitrogen species changing the light attenuation produces a decrease in nitrate at cb3 3c from february to may similar to the nitrate changes in the left column of fig 6 when changing from n bur dom ches to snp bur dom ches this suggests that the light attenuation scheme accounts for differences in nitrate as using the light attenuation scheme from ecb results in nitrate persisting later in the spring looking at nitrogen budget changes in fig 9 we also see that changing the light attenuation scheme to that in ecb significantly increases bioavailable nitrogen and nitrite in both cases exceeding what is in observations more detailed analysis showed that using the light attenuation scheme from ecb caused productivity to move up in the water column this means that particles take longer to reach the bottom and have more time to remineralize which is then reflected in reduced nitrogen burial see table 5 nitrogen burial with ecb light attenuation decreases to 1 232 gmol compared to 2 773 gmol in snp bur dom ches similarly burial would be expected to increase when increasing sinking velocities from the ches values to peru values as we see in table 5 burial does in fact increase to 4 845 gmol unsurprisingly this parameter change also has big effects on dissolved species as illustrated in the middle column of fig 8 increasing sinking velocities from the ches values to the peru values produces similar changes in oxygen and ammonium to those seen in the middle column of fig 6 which examined the impact of changing all the common parameters listed in table 1 this shows that sinking velocity is a major factor affecting the simulation of oxygen and ammonium however it does not explain the changes in nitrate which means the buildup of nitrate when changing common parameters is due to other parameters it appears that phytoplankton growth rates are higher in the ches parameter set and this may result in more nitrate drawdown in the spring finally we examine whether the changes between snp and ecb could simply be due to whether nitrification is governed by different half saturation concentrations and thresholds for nitrite or too slow a rate in snp in fig 9 we can see there is a decrease in nitrite inventory when higher nitrification rates lower half saturation constant and zero threshold for nitrite are used however in the right hand column from fig 8 we find that changes are very small for oxygen nitrate and ammonium the drawdown of oxygen and buildup of nitrite nitrate and ammonium in the deep bay are mainly set by the organic matter remineralization rate in mol m3 day which in turn is set by productivity divided by loss rates associated with mixing nitrification etc changing the rate constant governing how fast nitrite is converted to n 2 and nitrate with units of day 1 changes the inventory of nitrite but not the rate in mmol m3 day at which it is converted as such changing nitrification rates and half saturation concentrations produce little feedback on productivity this is also because light limitation is much more important than nutrient limitation in our simulations of the bay as indicated by r2 see table 6 adding the implicit optical effects of cdom to snp bur dom ches significantly increases model skill in predicting the hydrographic parameters at cb3 3c increasing sinking velocities to bioredoxcnps values in snp bur dom ches increases r2 for ammonium but decreases r2 for oxygen and nitrate which suggests that if one wants to get a more skillful model sinking velocities from ecb should be chosen making nitrification and denitrification easier increases r2 for oxygen and nitrate but decreases r2 for nh4 by a much larger amount however it helps to decrease the overestimated nitrite 4 2 model predictions of h 2 s our suite of simulations shows wide variation in the predictions of the h 2 s concentration fig 10 illustrates the sensitivity of simulated bottom water h 2 s concentration within snp peru snp bur dom peru snp bur dom ches and three sensitivity study cases the distribution of maximum h 2 s in july is very sensitive to whether organic matter burial and dom are included in the model in snp peru significant levels of h 2 s appear in the upper bay peaking at 120 mmol h 2 s m3 along the main stem in snp bur dom peru the zone of euxinia appears in the same region but it is smaller in extent than snp peru and the peak values are roughly 3 5 mmol h 2 s m3 nearly two orders of magnitude smaller the distribution of maximum h 2 s in july is also sensitive to sinking velocities in snp bur dom ches significant levels of h 2 s appear in the upper bay peaking at 15 mmol h 2 s m3 along the main stem in the case with higher sinking velocities the zone of euxinia moves further north and is smaller in extent than in snp bur dom ches the peak values are roughly 4 6 mmol h 2 s m3 nearly three times smaller adding the light attenuation from ecb helps to lower h 2 s concentration while making nitrification and denitrification easier tends to increase h 2 s concentration these results suggest that h 2 s could be a sensitive diagnostic for improving models of the bay as differences in model formulations that produce changes in simulated oxygen that are 10 of the peak values result in order of magnitude changes in peak h 2 s note differences in the range in fig 10 in addition to improving simulations of the seasonal cycling of nitrogen and ammonium our new snp bur dom model allows for predictions of h 2 s in the main stem of the bay fig 10 roden and tuttle 1992 found concentrations of h 2 s between 6 1 and 27 0 mmol h 2 s m3 at the mouth of the choptank river in oldham et al 2015 the concentration ranges more from 4 28 to 39 7 mmol h 2 s m3 at the bay bridge station even higher values of h 2 s concentration at the bay bridge up to 60 mmol h 2 s m3 were reported in luther and church 1988 though we were unable to find measurements of h 2 s within the bay during 2017 our model suite is able to bracket the historical observations additionally in preheim et al subm we show that h 2 s distribution predicted by the model is correlated to concentration of sulfur cycling genes suggesting future pathways of research 4 3 future work in this paper we focus on 2017 because we have abundant genomic data during this year in a companion paper preheim et al subm we compare rates from the model with gene abundances and find interesting patterns including some involving sulfur genes it will be important to extend these runs to both dry and wet years in the future 2018 is a particularly good candidate there are many improvements that can be brought into future iterations of our model feedbacks between chlorophyll and shortwave absorption ignored in our simulations should be taken into consideration to improve the simulation of temperature following kim et al 2020 our sulfur variables are currently set to have zero flux on the seaward boundary in the future iterations we will let the sulfate flux scale as the salinity flux and impose an open boundary condition for hydrogen sulfide we anticipate that the impact of such changes will be relatively small based on a few test simulations another concern is the stoichiometric ratio given that oxygen is the field most of interest to bay water quality managers we believe that we will need to pursue alternative hypotheses to get a simulation that produces comparable improvements in nitrogen species while not compromising the simulation of oxygen the fundamental tradeoff between oxygen and nitrogen accuracy seen across these simulations suggests that there are also issues with the relationship between them represented by the redfield ratio in particular the stoichiometric ratios used in both of the original codes o n of 138 16 are lower than those used in many modern models lenton and watson 2000 emerson and hedges 1988 2008 with too little oxygen consumed per unit nitrogen added preliminary work suggests that changing the stoichiometry of remineralization as well as making the changes we discussed above would generate a simulation which predicts hypoxic volume with comparable skill as n bur dom ches while giving a better prediction for oxygen nitrate and ammonium we report more fully on this work in the accompanying methodsx paper the fact that the nitrogen budgets show very different balances between water column processes and burial highlights the important role of the sediments which are treated in a highly simplified manner in this model including an explicit sediment term may be important for simulating the rise of phosphate late in the season sediment processes that we are interested in expanding also include the action of cable bacteria which are capable of harvesting electrons from free sulfide in deeper sediment malkin and meysman 2015 and deposition of organic sulfur in sediments jiang et al 2021 there are also potential improvements that could be made to water column processes one process not included in our version of the model is nitrogen fixation recent work by preheim et al subm shows that nitrogen fixation genes are found at higher levels in high phosphate deep waters in the late summer suggesting an important role for heterotrophic bacteria and a greater role for phosphate in the ecosystem than previously realized additionally analysis of the genes shows denitrification genes at relatively high levels in oxic waters in the spring potentially associated with particle microenvironments finally we have by no means exhausted the range of sensitivity studies that can be performed with the models in particular the temperature dependence of the remineralization differs between the n bur dom chesroms ecb and the snp bioredoxcnps models with remineralization rates generally being higher in the former in the absence of burial if we decrease the remineralization rates we will increase the pom partially compensating the decreased remineralization rate however decreasing the remineralization rates does allow more of the pom to get transported from the head of the bay to the deep channel and consume more oxygen there in the presence of burial it gets trickier to understand the impact of remineralization rates because if we decrease the rates more particulate organic matter reaches the sediment as this means more organic matter is buried we do not increase the organic matter as much because more nutrient is buried and the vertical distribution of nutrients is then different while changing sinking velocities also changes burial and the vertical distribution of nutrients we have found the resulting changes to nutrient budgets more straightforward to understand one challenge to investigating the impact of these processes is that they affect small detritus large detritus and semilabile don differently and only total particulate and dissolved organic nitrogen are currently measured in the bay 5 conclusion to date most models of the chesapeake bay have focused on heterotrophic denitrification as the major loss term for fixed nitrogen while the release of sulfide from sediments has previously been proposed to play an important role in biogeochemical cycling within the chesapeake bay roden and tuttle 1992 testa et al 2014 cerco and noel 2017 it has been mostly thought of as a sink for oxygen however in recent years it has become clear that other processes including anammox and cryptic sulfur cycling can be significant drivers of fixed nitrogen loss in anoxic waters canfield et al 2010 in order to model these additional processes in the bay a biogeochemical model for the peru upwelling system that included both anammox and sulfide oxidation with denitrification azhar et al 2014 was implemented in the bay using the original set of parameters calibrated for the open ocean snp peru while the snp peru model apparently resulted in an improved simulation for oxygen and nitrate it did not necessarily do so for the right reasons its improvement in modeled oxygen and nitrate concentrations came at the cost of overpredicting the concentration of ammonium we found the differences in oxygen concentrations were not driven by the inclusion of new sulfur cycling terms but rather by the neglect of burial and dissolved organic matter cycling omitting organic matter burial and dom cycling also resulted in increasing the error in ammonium concentrations by allowing ammonium to accumulate in the water column while differences in nitrate were found to be associated with differences in the equations sulfur cycling anammox light attenuation we found that light attenuation played an important role in explaining these differences rather than the inclusion of the cryptic sulfur cycle differences in parameters common to the two codes peru vs ches tended to compensate the differences due to adding burial and dom or light attenuation so that using the parameters calibrated for the chesapeake in the model developed for the open ocean actually made the solution worse this highlights the extent to which model parameters in chesapeake bay models are best depends critically on which processes are included within the model through a series of sensitivity studies sinking velocities were identified as playing a particularly important role amongst all parameters differing between the original simulations decreasing half saturation concentrations of no2 removing a threshold for nitrite and or increasing the nitrification rate for no2 helped to reduce nitrite without changing oxygen nitrate and ammonium too much our results also suggest a number of targets for future experimental work in the bay better characterization of particle sinking the thresholds for nitrification and the distribution of hydrogen sulfide are obvious ones that emerge from our results however we would suggest it is also important to make more measurements of rates in particular denitrification sulfate reduction and sulfide oxidation across a range of oxygen concentrations this would help us to determine whether thresholds for microbial processes like sulfate reduction are too low as previous work arora williams 2020 arora williams et al 2022 preheim et al 2022 shows that genes associated with sulfur cycling may not be limited to the lowest oxygen levels credit authorship contribution statement rui jin implemented and performed the snp model simulations performed the analysis and led the writing of the manuscript marie aude pradal set up and performed the original chesroms ecb simulations helped with the implementation of the new code and with experimental design and edited the manuscript kalev hantsoo provided the version of the snp code we used here and edited the manuscript anand gnanadesikan helped with analysis of the models experimental design and helped write and edit the manuscript pierre st laurent provided the original chesroms ecb code and model forcing helped to validate the code at johns hopkins and edited the manuscript christian j bjerrum original developer of the bioredoxcnps code identified key differences that guided the experimental design helped to edit the manuscript declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments cjb supported by the danish national research foundation denmark dnrf 53 toward development of bioredoxcnps and villum foundation denmark grant16518 toward kh visiting at ucph 
23793,a number of models have been developed to simulate hypoxia in the chesapeake bay but these models vary in complexity and in which processes they represent in this study we implement a previously published biogeochemical code bioredoxcnps developed for open ocean waters that includes cryptic microbial sulfur cycling within the chesroms physical model of the chesapeake bay sulfur cycling can increase rates of denitrification and anammox in anoxic waters but the net impacts of such changes on oxygen nitrate and ammonium are not understood we compare the results to a physically identical simulation with an estuarine biogeochemical cycling code previously implemented and calibrated in the bay ecb the ecb code neglects sulfur cycling but includes burial of particulate organic matter pom and cycling of dissolved organic matter dom and uses different values for many parameters governing phytoplankton growth and particle dynamics although the bioredoxcnps model produces a better simulation of oxygen and nitrate at a key test site this turns out not to be due to the inclusion of sulfur cycling instead large differences in modeled oxygen and ammonium are largely due to whether or not the biogeochemical codes include cycling of dom and sedimentary burial of pom changes in light attenuation produce large changes in nitrate changes in parameters used in both biogeochemical codes in particular particle sinking velocities tended to compensate the other differences in model construction the quantitative impacts of these choices for simulating chesapeake bay have not previously been documented in the peer reviewed literature predictions of hydrogen sulfide from our merged model were very sensitive to the choice of parameters and light attenuation this suggests that observations of hydrogen sulfide could help to constrain these processes in future models keywords coupled nitrogen and sulfur cycles biogeochemical parameters model comparison predictions of hydrogen sulfide chesapeake bay data availability data will be made available on request 1 introduction estuaries are key locations where rivers couple terrestrial processes with ocean biology and chemistry these systems have generated research interest due to their abundant biological resources and their crucial role in global carbon and biogeochemical cycles bauer et al 2013 bianchi and bauer 2011 canuel et al 2012 as the largest estuary in north america the chesapeake bay plays a particularly important role in coastal nutrient transformation transport and burial boynton et al 1995 much effort has been made to study these processes which can impact the bay s ecosystem and its economic productivity of all the processes affecting the bay eutrophication has emerged as a principal threat eutrophication arises from an increase in nutrient and dissolved organic matter dom concentrations leading to a greater production of particulate organic matter pom in the water column or on the seabed gray et al 2002 this results in hypoxia defined here as oxygen concentrations less than 62 5 mmol m3 when the oxygen consumed during the degradation of pom and dom exceeds the oxygen supplied from gas exchange mixing and advection hypoxia has been shown to cause mortality events for recent events within the chesapeake bay see luckett 2020 contributing to metazoan population decline and resulting in so called dead zones devoid of fisheries resources including crabs shrimp and fish rabalais et al 2002 renaud 1983 benthic organisms are especially vulnerable to coastal hypoxia anoxia and euxinia when water is both anoxic and sulfidic because they live in and near the sediments where oxygen tends to be depleted relative to the overlying water column seliger et al 1985 vaquer sunyer and duarte 2008 under intense hypoxia as oxygen levels become undetectable nitrate becomes the dominant oxidant for organic matter degradation subsequently when nitrate is depleted sulfate reduction becomes the dominant decomposition pathway and produces hydrogen sulfide h 2 s detection of which is often the key criterion for euxinia in the water column euxinia can reduce biodiversity by harming surviving organisms through lethal and sublethal impacts luther and church 1988 the production of h 2 s also has the potential to change biogeochemical cycling in the chesapeake bay marvin dipasquale and capone 1998 estimated that decomposition of organic matter via sulfate reduction remineralized 18 32 of the primary production at three sites in the bay h 2 s produced by this process can move upwards in the water column and act as a sink for oxygen when it is oxidized further accelerating hypoxia roden and tuttle 1992 however recent work has shown that sulfide can also be oxidized using nitrite and nitrate resulting in a loss of bioavailable nitrogen canfield et al 2010 such losses reduce the potential for hypoxia this process has been referred to as cryptic sulfur cycling as sulfide produced from sulfate can be rapidly recycled and thus may not be detected in the water column on observational time scales arora williams et al 2022 find that organisms which are known to have these capabilities are ubiquitous and relatively abundant within the chesapeake bay however so far there is no observational proof that these organisms are actually engaged in cryptic sulfur cycling moreover rates of cryptic sulfur cycling and what thresholds in oxidants govern the transitions between oxic respiration denitrification and sulfur cycling within the bay are not known earlier chesapeake bay models testa et al 2014 cerco and noel 2017 incorporate biogeochemical cycling bgc codes which have a simplified representation of the impacts of sulfur cycling in which an idealized reductant representing either h 2 s or methane is released from sediments and oxidized in the water column however these models do not directly simulate water column sulfate reduction sulfide oxidation by nitrate or sulfide oxidation by nitrite recently azhar et al 2014 developed a code referred to as bioredoxcnps to specifically study these processes and calibrated it to fit the peru upwelling system where an active cryptic sulfur cycle has been documented canfield et al 2010 this code was later updated to work with roms version 3 6 in hantsoo et al 2018 which is the version we use here bioredoxcnps simulates a number of additional species e g nitrite and simulates rates of sulfate reduction annamox and canonical denitrification which are consistent with the peru upwelling system canfield et al 2010 azhar et al 2014 an obvious question is how well such a model would do if it were simply translated to chesapeake bay since at present there are no constraints on the rates of cryptic sulfur cycling in the bay it is theoretically worthwhile to perform such an exercise and evaluate the skill of the resulting model against a baseline model the ecb code of feng et al 2015 that has been calibrated in the bay if it were the case that we could simply translate the open ocean model to the coastal zone this would be a powerful demonstration that our understanding of sulfur cycling is robust as we describe below however translating the model to the bay turns out not to be as straightforward as we had hoped when the bioredoxcnps code and ecb code are run in a physically identical simulation of the chesapeake bay bioredoxcnps does improve certain aspects oxygen nitrate of the simulation however closer examination showed that other fields nitrite ammonium were degraded this spurred us to try to understand the sources of the differences between the simulations both biogeochemical components build on the fennel et al 2006 bgc code which partitions fixed nitrogen between nitrate and ammonium the ecb code adds to the fennel bgc module by a including dissolved organic nitrogen and carbon b simulating the burial of sinking particles in sediment and c allowing for absorption of photosynthetically active radiation par by chromophoric dissolved organic matter cdom and particles while the bioredoxcnps code has many similarities to the chesroms ecb code it was developed for the open ocean thus it does not include dom organic matter burial or absorption of par by cdom and particles additionally many processes common to the two codes have different parameter settings in order to isolate which processes were most important for contributing to the differences between the two codes we developed a merged version that includes both the sulfur and nitrogen cycling of bioredoxcnps and the burial and dissolved organic matter cycling of chesroms ecb in what follows we will distinguish between codes models and simulations codes have different representations of biogeochemical or physical processes models implement these codes in a particular configuration but may produce simulations with different values of parameters our merged model which we refer to as snp bur dom ches can then be regarded as a baseline for future work studying sulfur cycling even though sulfur cycling does not make a huge difference in our simulations in this paper we focus on understanding the differences in modeled biogeochemistry between simulations run with our merged code and the two published codes this manuscript is structured as follows the codes used in this study the details of how they are implemented into models and the simulations run with them are described in section two we begin our results in section three by looking at how the different changes affect predicted oxygen nitrate and ammonium fields when we change from the ecb to bioredoxcnps step by step while bioredoxcnps model does produce an improvement in the simulation of oxygen in the bay this is not primarily driven by adding sulfur cycling instead we find that the bioredoxcnps simulation s a exclusion of burial and dom cycling b exclusion of absorption by cdom c different values for parameters common to the codes each produced large effects but often in different directions in section four we conduct a sensitivity study to highlight three important factors that affect the model results we also show that the h 2 s distribution is more sensitive to how we construct our model than tracers like oxygen this study moves towards a more complete model for simulating chemical species and highlights key processes and parameters that control biogeochemical cycles in the chesapeake bay as such our results provide guidance for future experimental studies focused on hypoxia anoxia and euxinia 2 model description 2 1 physical model the coupled physical biogeochemical models used in this study were run with version 3 6 revision 898 of the regional ocean modeling system roms roms is a three dimensional time dependent simulation that uses the hydrostatic primitive equations shchepetkin and mcwilliams 2005 physical circulations were set to be identical across the different model runs as there was no feedback between biological and physical circulation while accounting for feedbacks between chlorophyll and shortwave absorption may improve temperature simulations kim et al 2020 ignoring such feedbacks for now allows us to attribute all differences between the models to the direct impacts of biogeochemical processes we use an implementation of the roms code for the chesapeake bay developed by xu et al 2012 and known in the literature as chesroms the chesroms model domain extends from 77 2 w to 75 0 w and from 36 n to 40 n covering the main stem and primary tributaries of the chesapeake bay the model extends seaward to the mid atlantic bight fig 1 to prevent boundary effects from altering tracer fields and mean velocity fields the horizontal grid uses orthogonal curvilinear coordinates with varying resolution but with a nominal resolution of around 1 km in the central bay more details are presented in accompanying methodsx paper jin et al subm 2 2 bgc codes and simulation setups 2 2 1 organizing the differences between the two original codes in this study we started by comparing two published biogeochemical models ecb bioredoxcnps shown in red in fig 2 which have been configured for specific locales and validated against observations both of them are descendants of the code of fennel et al 2006 and have many identical equations but they also differ in many ways we identify three classes of differences broken down in fig 2 parameters common to the two models which are assigned different values pathways only included in ecb burial and dom inclusion of cdom in light attenuation and pathways only included in bioredoxcnps sulfur cycling and phosphorus in order to evaluate the importance of each of these differences we developed a merged version of the two codes which enables us to perform a series of experiments schematically summarized in fig 2 that isolate the impacts of each change below we describe in more detail the three classes of differences between the two initial simulations 2 2 2 parameters common to each simulation the first set of differences are in equations in common to the two models both models have a single class of phytoplankton whose growth is limited by nutrients and whose major loss term is coagulation and sinking the particles produced by coagulation are divided into two classes small and large with the large particles sinking faster these particles then remineralize with the mode of remineralization oxic vs denitrifying to ammonium and subsequent nitrification determined by thresholds however as summarized in table 1 the parameter set governing these processes taken from the da et al 2018 model of the chesapeake has a significant number of differences with the parameter set taken from the hantsoo et al 2018 version of the azhar et al 2014 model of the peru upwelling system we can break the parameter differences down into three groups namely parameters governing photosynthesis those associated with particle dynamics and those governing the transformation of inorganic compounds because a full exploration of all these differences is too much for a single manuscript in this paper we highlight two sets of parameters we found to be important namely sinking velocities and nitrification rates sinking velocities are important because they control where remineralization occurs in water column sinking velocities from da et al 2018 are quite slow as the vertical sinking velocity is 5 m day for large detritus and 0 1 m day for small detritus sinking velocities from hantsoo et al 2018 are much faster with the vertical sinking velocity for large detritus being set at 20 m day and for small detritus at 2 m day although the remineralization rate for detritus in ecb is very low e g 0 03 day 1 at 0 c it is also temperature dependent so that at 25 c it has a value of around 0 1 day 1 giving an e folding depth of 1 m for small detritus and 5 m for large detritus by contrast in bioredoxcnps while the remineralization rate is 0 1 day 1 the e folding depth is 20 m for small detritus and 200 m for large detritus the water column depth in the chesapeake is around 20 m in the middle of the channel while the flat region away from the channel averages about 6 m this means that in ecb small and large detritus produced in the top few meters of the water column will mostly remineralize within the water column while in bioredoxcnps a large fraction of the detritus will reach the bottom and get remineralized buried there nitrification is another important process which can be thought of as having a different parametric representation in the two codes in ecb nitrification occurs in one stage with ammonium being directly transformed to nitrate at a maximum temperature independent rate of 0 05 day 1 this can be thought of as equivalent to allowing ammonium to be transformed to nitrite with a rate of 0 05 day 1 and nitrite subsequently being instantaneously remineralized to nitrate nitrification is limited by oxygen with a half saturation coefficient of 1 mmol m3 in bioredoxcnps there are two stages the first takes nh4 to no2 with a maximum rate of 0 05 day 1 the same as in ecb however the second step transforms no2 to no3 with a maximum rate of 0 005 day 1 both stages are limited by oxygen at 1 mmol m3 in bioredoxcnps because the e folding scale over which biomass decays is so much longer the rates of the production of ammonium are much lower as a result the rate of nitrification has to be slower in order to allow for reasonable levels of dissolved species particularly nitrite 2 2 3 inclusion of burial and dissolved organic matter the second set of differences is that ecb has burial of organic matter and explicitly represents don doc a fraction of phytoplankton and large detritus are partially resuspended as small detritus once they reach the bottom depending on near bottom turbulent velocities some fraction of the remaining benthic flux is buried permanently with the rest being remineralized the burial fraction follows henrichs and reeburgh 1987 where it is a function of the carbon flux to the bottom f b u r min 0 75 0 023 c a r b o n f l u x t o t h e b o t t o m 0 5797 this means that burial is very small when the flux of material is small and increases nonlinearly as the flux to the bottom does note that this can interact with sinking velocity higher sinking velocities mean that a larger fraction of biomass reaches the bottom and a higher fraction of that flux gets buried as we will demonstrate below including burial significantly reduces overall nitrogen concentrations however if all of the organic matter is diverted to particles most of them would be buried in shallow rivers and never make it out of rivers to main stem of the bay don is present at much higher concentration than nitrate and ammonium and is thus an important reservoir of nitrogen in the bay allowing some fraction of the organic matter to be diverted to don as in ecb was found to be necessary to allow hypoxia in the deep bay we therefore examine the inclusion of burial and don together in the model 2 2 4 inclusion of sulfur and phosphorus cycling the third set of differences is that bioredoxcnps includes sulfur and phosphorus cycling and has explicit no2 by including no2 nitrification denitrification is changed because no2 serves as an important pool of nitrogen that is not immediately accessible by phytoplankton in this sense it behaves like don in ecb however unlike don it can be directly lost via denitrification po4 also has the potential to produce nutrient limitation in phytoplankton though it turns out not to be very important in the model 2 2 5 light attenuation bioredoxcnps also retains the fennel et al 2006 scheme for the absorption of photosynthetically active radiation par which includes absorption by chlorophyll the vertical attenuation of light is described by the equation d p a r d z k p a r p a r where k p a r 0 04 0 024 c h l the first term on the right hand side is the absorption due to pure water and the second captures absorption by chlorophyll by contrast in the ecb code there is a formulation that implicitly relates the absorption to refractory doc delivered down rivers k p a r max 1 4 0 063 t s s 0 057 s 0 6 where tss represents total suspended solids this scheme has very low penetration of light high k par when s 0 at the river mouths with increasing light penetration as salinity increases the implicit assumption is that the additional cdom added by rivers is slowly diluted as it moves out to sea in a study to be reported fully in a future manuscript we find that this parameterization can be mimicked by adding cdom dependent absorption to the original fennel et al 2006 scheme as cdom in the bay shows a strong negative correlation with salinity 2 2 6 merged version of the model in order to identify and quantify sources of intermodel differences we developed a merged version of these codes a full schematic of this merged version is shown in fig 3 nitrate phosphate and ammonium come down the rivers light blue lines and can be taken up by phytoplankton via photosynthesis green lines phytoplankton are primarily lost via coagulation into large and small detritus red lines which sink to the bottom a fraction of phytoplankton and large detritus are partially resuspended fluorescent blue lines as small detritus once they reach the bottom there is a small loss to zooplankton gray lines which we do not focus on here detritus is solubilized to dom purple lines both detritus and dom can be remineralized brown lines to phosphate and ammonium this remineralization consumes oxygen but in the absence of oxygen dotted brown lines is associated with heterotrophic denitrification and consumes nitrate and nitrite in the absence of nitrate nitrite and oxygen remineralization proceeds using sulfate and produces hydrogen sulfide dashed brown lines hydrogen sulfide is oxidized back to sulfate orange lines using oxygen solid or nitrate nitrite dotted with the latter process resulting in denitrification ammonium can either be nitrified dark blue lines or consumed with nitrite via anammox dotted magenta lines in the absence of oxygen in sediments organic matter that is not buried is immediately remineralized black lines to phosphate and ammonium this remineralization consumes oxygen but in the absence of oxygen dotted brown lines is associated with heterotrophic denitrification and consumes nitrate and nitrite in the absence of nitrate nitrite and oxygen remineralization proceeds using sulfate and produces hydrogen sulfide dashed brown lines which pathway ends up being used depends on the concentration of oxidants in the bottom most layer 2 3 experimental design this new code allows us to develop an experimental design by combining three codes with two parameter sets for each code giving us a total of six potential simulations in order to highlight the differences between simulations we use a nomenclature that makes it evident what nutrients are cycled whether the model includes burial and dom and which parameter set peru vs chesapeake is used within each simulation the resulting nomenclature shows the increasing complexity and realism in the setup of the simulations in this paper we consider four of the six possible combinations in detail namely n bur dom ches snp peru snp bur dom peru and snp bur dom ches the other two are considered in the accompanying methodsx paper n vs snp contrasts whether the code models only nitrogen as in chesroms ecb or nitrogen sulfur and phosphorus as in bioredoxcnps bur dom indicates that the code includes organic matter burial in sediments and dissolved organic matter as in chesroms ecb finally ches vs peru denotes whether the biogeochemical parameters common to both of the two original codes are taken from da et al 2018 in the chesapeake or azhar et al 2014 in the peru upwelling system as modified by hantsoo et al 2018 for example the chesroms ecb model of da et al 2018 thus is identical to our n bur dom ches simulation while the implementation of bioredoxcnps with the original parameters used in hantsoo et al 2018 corresponds to our snp peru simulation a summary of how the differences between the original codes map across the simulations is shown in the upper half of table 2 as can be seen by examining table 2 and fig 2 comparing snp bur dom ches with n bur dom ches allows us to see differences due to adding sulfur cycling nitrite and changing light attenuation while retaining burial and dom and leaving common parameters unchanged in contrast by comparing snp bur dom peru with snp bur dom ches we can identify differences due to changing parameters while leaving the processes represented in the model unchanged finally by comparing snp peru with snp bur dom peru we can isolate differences due to removing burial and dom we note that this is not the only possible way to transform one model to another our full set of six simulations is presented in the companion methodsx paper jin et al subm where we demonstrate that changes in dissolved quantities are relatively robust to the order in which changes in the model configuration are made 2 4 initial conditions and boundary forcings all simulations were run for the year 2017 riverine inputs for n bur dom ches were taken from the dynamic land ecosystem model as in feng et al 2015 tracers found in common across multiple models iss nh4 no3 and don when included were set to have the same inputs for snp peru snp bur dom peru and snp bur dom ches the riverine input po4 was set to be the riverine input no3 divided by 36 6 a ratio calculated from field data https www chesapeakebay net state pollution the riverine inputs of sdep and ldep were set to the values of sden and lden divided by 16 respectively which is the redfield ratio reflecting observations of particulate nitrogen and phosphorus within the bay semilabile and refractory dop were also set to the corresponding don concentrations divided by 16 when included sulfur was not included in the riverine input in this study consistent with burke et al 2018 who found sulfate concentrations in these waters being low 0 5 mm compared to much higher concentrations in seawater at the seaward boundary we applied a mix of radiative boundary conditions in which tracers like detrital organic matter are allowed to leave the domain but do not return through the boundary and radiation with nudging in which tracers like temperature and salinity entering the domain are set to climatological values our new sulfur variables are set to have zero flux on the seaward boundary which makes little difference on the short time scales for which we run here especially given the low levels of water column sulfur cycling on the shelf we will amend this in future iterations of the code all our sulfur thus starts in form of dissolved sulfate and cycles between this form and h 2 s atmospheric deposition of dissolved inorganic nitrogen din was also included in the models as a source of din to the estuary since it is an important fraction of the total din inputs to the chesapeake bay da et al 2018 initial conditions for the n bur dom ches simulation were taken from a previously run chesroms ecb simulation that was initialized in 1979 and run for 38 years it thus represents a spun up state of the system those initial conditions in common with n bur dom ches were set to be the same in snp peru snp bur dom peru and snp bur dom ches the initial values of po4 sdep ldep and dop were all set to be 16 times smaller than their corresponding nitrogen variables from da et al 2018 all the other initial values of new state variables were set to zero 3 model data comparison 3 1 comparing the base simulations found in the literature n bur dom ches and snp peru 3 1 1 qualitative comparison of annual cycle of oxygen at cb3 3c both n bur dom ches and snp peru produce reasonable simulations of oxygen figs 4a and 4b show the oxygen concentrations in these two simulations with observations overlaid as colored circles mismatches can be seen where the circles are visible against the background of the model n bur dom ches simulates a relatively high oxygen concentration near the surface from january to mid april around 350 mmol o 2 m3 from mid may to late august a large hypoxic zone the so called dead zone shown by magenta shading extends from near the bottom to around 8 m in depth around this time period the oxygen concentration is still high near the surface but decreases rapidly at increasing depths in the water column corresponding to water column stratification and warming in the bay during the summer however during may and october the observations show noticeably lower oxygen concentration near the bottom than the n bur dom ches simulation does the snp peru simulation as shown in fig 4b shows a similar distribution of oxygen although the hypoxic zone lasts longer and is thus more consistent with early onset of hypoxia in 2017 3 1 2 quantitative evaluation of model skill in simulating oxygen compared to observations n bur dom ches fits both very low and very high concentrations of oxygen well but overpredicts intermediate values in the 50 200 mmol m3 range fig 5a snp peru does better in this range a useful way to objectively compare these fields is the coefficient of determination referred to as r2 which can be written as 1 error variance sample variance note that the coefficient of determination can become negative if the error variance exceeds the sample variance in this sense it differs from the r2 produced by a regression model where by definition the error variance is smaller than the sample variance both r2 and r2 are affected by differences in the pattern of spatiotemporal variation between modeled and predicted fields however r2 also incorporates the contribution to error variance from differences in the mean value and from the amplitude of spatiotemporal variation and as such it is a more comprehensive normalized measure of the error with respect to observed oxygen snp peru produces a substantial increase in r2 from 0 72 to 0 85 table 3 even though it underpredicts oxygen near the surface this is because lower observed oxygen concentrations near the bottom are better simulated in snp peru than in n bur dom ches 3 1 3 evaluation of the simulations of nitrate and ammonium simulations of nitrate from n bur dom ches and snp peru at the bay bridge station are shown in figs 4d and 4e in the n bur dom ches simulation the nitrate concentration near the surface is around 40 50 mmol n m3 from january to late may with some occasional drops this is somewhat higher than the observations nitrate then drops quickly beginning in early june the nitrate concentration remains between 0 and 8 mmol n m3 throughout the water column during the summer months until early november the low values are due to primary productivity taking up nitrate in the spring but also to denitrification removing nitrate in the summer months surprisingly the 1 56 gmol uptake of nitrate by new production and 1 81 gmol removal by denitrification are nearly equal in magnitude see table 4 in snp peru the spatiotemporal distribution of nitrate is similar to n bur dom ches from june to november although the maximum nitrate concentration in the spring is lower around 48 mmol n m3 depleted nitrate throughout the water column is also observed in this model in the same time period as in n bur dom ches however from near the bottom to around 11 m in depth nitrate decreases in mid april and remains low until late october comparing with observations shows that snp peru more accurately models low nitrate concentrations between around 10 m in depth and the bottom from mid january to mid april while results from n bur dom ches are higher than the observations a scatter plot of nitrate fig 5b also shows that modeled nitrate in snp peru is closer to the observational data with the linear fit red line lying on top of the black 1 1 line while the linear fit for n bur dom ches is offset above this line the r2 for nitrate is much higher in snp peru 0 46 than in n bur dom ches 0 29 with the negative value indicating that the rms error variance is larger than the observational variance at this site fig 4g and fig 4h compare the simulations of ammonium from n bur dom ches and snp peru in n bur dom ches the ammonium concentration from near the bottom to around 10 m in depth begins to increase from mid april and peaks at 42 mmol n m3 in mid june then from late july it drops gradually and becomes low again in early october given that peak values of ammonium between 2015 and 2019 at this site never exceeded 25 mmol n m3 we conclude that n bur dom ches predicts too much ammonium during the summer in snp peru the ammonium concentration near the bottom increases in mid april and decreases in early september it peaks at a value of 68 mmol n m3 in june the ammonium depleted zone near the surface is similar to n bur dom ches after early september the ammonium concentration throughout the water column is lower than n bur dom ches by contrast in the summer the ammonium concentration in snp peru is about twice that in n bur dom ches a scatter plot of observed vs modeled ammonium fig 5c shows that the modeled results of n bur dom ches are closer to the observational data while snp peru gets worse results when it comes to ammonium the significant overprediction in ammonium means that the r2 for this variable decreases between n bur dom ches 0 32 and snp peru 1 12 though clearly errors are large in both simulations note however that the overprediction in snp peru is greatest deep in the water column there is actually less ammonium above the pycnocline thermocline oxycline during the summertime compare fig 4g and h more blue symbols show up above the pycnocline in fig 4g also fig 4i 3 1 4 annual cycle of differences between the two published models the differences between the two codes show clear annual cycles highlighted in the right hand column of fig 4 for most of the year the oxygen difference between n bur dom ches and snp peru is small in the range of 0 30 mmol o 2 m3 fig 4c from the bottom to around 10 m in depth snp peru shows obviously lower oxygen than n bur dom ches between mid april and mid may near the surface during the same time period oxygen in snp peru is slightly higher than in n bur dom ches during the summer months near the surface snp peru shows a lower oxygen concentration nitrate predicted by snp peru is lower than that predicted by n bur dom ches for the whole year fig 4f specifically from mid april to early june nitrate concentrations in snp peru are much lower than n bur dom ches throughout the water column compared to other times with differences up to 50 mmol n m3 the high nitrate associated with the spring freshet is less persistent in snp peru than in n bur dom ches fig 4i shows the ammonium difference between snp peru and n bur dom ches snp peru simulates more ammonium than n bur dom ches for the most part from january to august from mid april to the end of june and from the near bottom to around 10 m in depth ammonium in snp peru is about 20 30 mmol n m3 higher than n bur dom ches the differences in ammonium have a pattern that is somewhat anticorrelated with the differences in oxygen suggesting a tradeoff between oxygen and ammonium that we will see more clearly in some of our other simulations 3 2 pathways from n bur dom ches to snp peru as shown in fig 2 and table 2 we can break the differences between snp peru and n bur don ches down into three consecutive changes in model configuration the detailed differences resulting from each change are shown in fig 6 to illustrate which aspects of the models contribute to the differences seen in the right hand column of fig 4 when adding sulfur and phosphorus cycling and changing the light attenuation fig 6 left hand column there is a huge decrease in nitrate especially in late april and may r2 for nitrate shows an increase from 0 29 to 0 20 table 3 in comparison changes in oxygen and ammonium are relatively small though oxygen does increase slightly in deep waters in the spring r2 for oxygen decreases from 0 72 to 0 59 while for ammonium increases from 0 32 to 0 03 changing parameters from the ches to peru set fig 6 middle column results in an increase of nitrate from february to may partially canceling the change produced by adding pathways and changing light attenuation r2 for nitrate increases from 0 20 to 0 49 however changing the parameters also increases oxygen during the summertime from near the bottom to 8 m in depth while ammonium drops during the same period r2 for oxygen drops significantly from 0 59 to 0 19 but r2 for ammonium increases from 0 20 to 0 49 removing burial and dom leads to opposite sign changes in oxygen and ammonium fig 6 right hand column in contrast with the middle column oxygen decreases and ammonium increases during the summertime r2 for oxygen significantly increases from 0 19 to 0 85 while ammonium drops to 1 13 however nitrate does not change a lot in this step r2 for nitrate only changes a little from 0 49 to 0 46 comparing fig 6 with fig 4 suggests that changes in oxygen and ammonium between snp peru and n bur dom ches are mainly due to removing burial and dom however changes in these two fields produced by removing burial and dom right hand column fig 6 are significantly counteracted by changing parameters in common to the two models middle column fig 6 for nitrate changes between the two original models result from adding sulfur and phosphorus cycling and changing the light attenuation left hand column fig 6 but also show significant cancellation from changes in the common parameters middle column fig 6 these changes in results call for a detailed examination of the budget of nitrogen table 4 along with the partitioning of nitrogen between different species fig 7 adding the new pathways results in a redistribution of the loss of nitrogen from water column denitrification to burial nitrogen burial between january and july increases from 1 348 gmol in n bur dom ches to 2 773 gmol in snp bur dom ches however sediment denitrification and water column denitrification over the same time period decrease from 0 81 gmol to 0 0079 gmol and from 1 015 gmol to 0 062 gmol respectively when looking at the pie charts for n bur dom ches and snp bur dom ches it is noticeable that there is a buildup of nitrite when compared to observations blue sections so that this drop in denitrification is not because we lack nitrite when changing the parameters in common between the two codes from the ches to the peru parameter set it is shown that using peru parameters substantially increases the nitrogen burial from 2 773 gmol to 4 736 gmol while the total nitrogen drops a lot from 2 86 gmol to 2 33 gmol corresponding to a decrease in mean concentration of 7 5 μ m total amount of sediment denitrification plus water column denitrification also drops from 0 0079 gmol to 0 039 gmol and from 0 062 gmol to 0 055 gmol respectively when removing the burial and dom the nitrogen budget shifts in the opposite direction resulting in a huge amount of denitrification however in order to get that much denitrification in the water column the bioavailable nitrogen and nitrite end up being heavily overestimated as shown by the pie chart fig 7 for snp peru orange and blue sector these results raise a number of questions one is whether the realistic decreases in nitrate in snp bur dom ches are due to the unrealistic buildup of nitrite alternatively could the different light attenuation scheme be responsible can we fix nitrite without making the budgets of other nitrogen species worse another question is which parameters in common to the two models have the biggest impact on the solution in order to examine these questions in more detail we now turn to a set of sensitivity studies 4 results and discussion 4 1 sensitivity studies light attenuation sinking velocities and nitrification rates in order to develop an understanding of which of the many parameters changed between the models had the biggest impact on model performance we performed a number of sensitivity studies based on the baseline model snp bur dom ches here we report on two that we found to have major impacts on some subset of oxygen nitrate and ammonium namely 1 light attenuation and 2 particle sinking velocities we also show that we can eliminate the unrealistic buildup of nitrite found in snp simulations without changing oxygen ammonium and nitrate significantly the three such simulations here involved 1 changing the light attenuation scheme of snp bur dom ches so that cdom absorption was implicitly included as in the ecb code 2 increasing the sinking velocities for large and small detritus in snp bur dom ches to those used in the peru code 3 increasing the nitrification rate to 0 05 day 1 decreasing the half saturation concentration of no2 for denitrification to 1 mmol m3 and removing a 2 mmol m3 threshold for nitrite to participate in denitrification in snp bur dom ches again making the numbers more similar to ecb in fig 8 we examine the changes in oxygen nitrate and ammonium paralleling those in the right hand column of fig 4 and in fig 6 for three sensitivity studies relative to snp bur dom ches in fig 9 we examine the changes in bay wide inventory of nitrogen species changing the light attenuation produces a decrease in nitrate at cb3 3c from february to may similar to the nitrate changes in the left column of fig 6 when changing from n bur dom ches to snp bur dom ches this suggests that the light attenuation scheme accounts for differences in nitrate as using the light attenuation scheme from ecb results in nitrate persisting later in the spring looking at nitrogen budget changes in fig 9 we also see that changing the light attenuation scheme to that in ecb significantly increases bioavailable nitrogen and nitrite in both cases exceeding what is in observations more detailed analysis showed that using the light attenuation scheme from ecb caused productivity to move up in the water column this means that particles take longer to reach the bottom and have more time to remineralize which is then reflected in reduced nitrogen burial see table 5 nitrogen burial with ecb light attenuation decreases to 1 232 gmol compared to 2 773 gmol in snp bur dom ches similarly burial would be expected to increase when increasing sinking velocities from the ches values to peru values as we see in table 5 burial does in fact increase to 4 845 gmol unsurprisingly this parameter change also has big effects on dissolved species as illustrated in the middle column of fig 8 increasing sinking velocities from the ches values to the peru values produces similar changes in oxygen and ammonium to those seen in the middle column of fig 6 which examined the impact of changing all the common parameters listed in table 1 this shows that sinking velocity is a major factor affecting the simulation of oxygen and ammonium however it does not explain the changes in nitrate which means the buildup of nitrate when changing common parameters is due to other parameters it appears that phytoplankton growth rates are higher in the ches parameter set and this may result in more nitrate drawdown in the spring finally we examine whether the changes between snp and ecb could simply be due to whether nitrification is governed by different half saturation concentrations and thresholds for nitrite or too slow a rate in snp in fig 9 we can see there is a decrease in nitrite inventory when higher nitrification rates lower half saturation constant and zero threshold for nitrite are used however in the right hand column from fig 8 we find that changes are very small for oxygen nitrate and ammonium the drawdown of oxygen and buildup of nitrite nitrate and ammonium in the deep bay are mainly set by the organic matter remineralization rate in mol m3 day which in turn is set by productivity divided by loss rates associated with mixing nitrification etc changing the rate constant governing how fast nitrite is converted to n 2 and nitrate with units of day 1 changes the inventory of nitrite but not the rate in mmol m3 day at which it is converted as such changing nitrification rates and half saturation concentrations produce little feedback on productivity this is also because light limitation is much more important than nutrient limitation in our simulations of the bay as indicated by r2 see table 6 adding the implicit optical effects of cdom to snp bur dom ches significantly increases model skill in predicting the hydrographic parameters at cb3 3c increasing sinking velocities to bioredoxcnps values in snp bur dom ches increases r2 for ammonium but decreases r2 for oxygen and nitrate which suggests that if one wants to get a more skillful model sinking velocities from ecb should be chosen making nitrification and denitrification easier increases r2 for oxygen and nitrate but decreases r2 for nh4 by a much larger amount however it helps to decrease the overestimated nitrite 4 2 model predictions of h 2 s our suite of simulations shows wide variation in the predictions of the h 2 s concentration fig 10 illustrates the sensitivity of simulated bottom water h 2 s concentration within snp peru snp bur dom peru snp bur dom ches and three sensitivity study cases the distribution of maximum h 2 s in july is very sensitive to whether organic matter burial and dom are included in the model in snp peru significant levels of h 2 s appear in the upper bay peaking at 120 mmol h 2 s m3 along the main stem in snp bur dom peru the zone of euxinia appears in the same region but it is smaller in extent than snp peru and the peak values are roughly 3 5 mmol h 2 s m3 nearly two orders of magnitude smaller the distribution of maximum h 2 s in july is also sensitive to sinking velocities in snp bur dom ches significant levels of h 2 s appear in the upper bay peaking at 15 mmol h 2 s m3 along the main stem in the case with higher sinking velocities the zone of euxinia moves further north and is smaller in extent than in snp bur dom ches the peak values are roughly 4 6 mmol h 2 s m3 nearly three times smaller adding the light attenuation from ecb helps to lower h 2 s concentration while making nitrification and denitrification easier tends to increase h 2 s concentration these results suggest that h 2 s could be a sensitive diagnostic for improving models of the bay as differences in model formulations that produce changes in simulated oxygen that are 10 of the peak values result in order of magnitude changes in peak h 2 s note differences in the range in fig 10 in addition to improving simulations of the seasonal cycling of nitrogen and ammonium our new snp bur dom model allows for predictions of h 2 s in the main stem of the bay fig 10 roden and tuttle 1992 found concentrations of h 2 s between 6 1 and 27 0 mmol h 2 s m3 at the mouth of the choptank river in oldham et al 2015 the concentration ranges more from 4 28 to 39 7 mmol h 2 s m3 at the bay bridge station even higher values of h 2 s concentration at the bay bridge up to 60 mmol h 2 s m3 were reported in luther and church 1988 though we were unable to find measurements of h 2 s within the bay during 2017 our model suite is able to bracket the historical observations additionally in preheim et al subm we show that h 2 s distribution predicted by the model is correlated to concentration of sulfur cycling genes suggesting future pathways of research 4 3 future work in this paper we focus on 2017 because we have abundant genomic data during this year in a companion paper preheim et al subm we compare rates from the model with gene abundances and find interesting patterns including some involving sulfur genes it will be important to extend these runs to both dry and wet years in the future 2018 is a particularly good candidate there are many improvements that can be brought into future iterations of our model feedbacks between chlorophyll and shortwave absorption ignored in our simulations should be taken into consideration to improve the simulation of temperature following kim et al 2020 our sulfur variables are currently set to have zero flux on the seaward boundary in the future iterations we will let the sulfate flux scale as the salinity flux and impose an open boundary condition for hydrogen sulfide we anticipate that the impact of such changes will be relatively small based on a few test simulations another concern is the stoichiometric ratio given that oxygen is the field most of interest to bay water quality managers we believe that we will need to pursue alternative hypotheses to get a simulation that produces comparable improvements in nitrogen species while not compromising the simulation of oxygen the fundamental tradeoff between oxygen and nitrogen accuracy seen across these simulations suggests that there are also issues with the relationship between them represented by the redfield ratio in particular the stoichiometric ratios used in both of the original codes o n of 138 16 are lower than those used in many modern models lenton and watson 2000 emerson and hedges 1988 2008 with too little oxygen consumed per unit nitrogen added preliminary work suggests that changing the stoichiometry of remineralization as well as making the changes we discussed above would generate a simulation which predicts hypoxic volume with comparable skill as n bur dom ches while giving a better prediction for oxygen nitrate and ammonium we report more fully on this work in the accompanying methodsx paper the fact that the nitrogen budgets show very different balances between water column processes and burial highlights the important role of the sediments which are treated in a highly simplified manner in this model including an explicit sediment term may be important for simulating the rise of phosphate late in the season sediment processes that we are interested in expanding also include the action of cable bacteria which are capable of harvesting electrons from free sulfide in deeper sediment malkin and meysman 2015 and deposition of organic sulfur in sediments jiang et al 2021 there are also potential improvements that could be made to water column processes one process not included in our version of the model is nitrogen fixation recent work by preheim et al subm shows that nitrogen fixation genes are found at higher levels in high phosphate deep waters in the late summer suggesting an important role for heterotrophic bacteria and a greater role for phosphate in the ecosystem than previously realized additionally analysis of the genes shows denitrification genes at relatively high levels in oxic waters in the spring potentially associated with particle microenvironments finally we have by no means exhausted the range of sensitivity studies that can be performed with the models in particular the temperature dependence of the remineralization differs between the n bur dom chesroms ecb and the snp bioredoxcnps models with remineralization rates generally being higher in the former in the absence of burial if we decrease the remineralization rates we will increase the pom partially compensating the decreased remineralization rate however decreasing the remineralization rates does allow more of the pom to get transported from the head of the bay to the deep channel and consume more oxygen there in the presence of burial it gets trickier to understand the impact of remineralization rates because if we decrease the rates more particulate organic matter reaches the sediment as this means more organic matter is buried we do not increase the organic matter as much because more nutrient is buried and the vertical distribution of nutrients is then different while changing sinking velocities also changes burial and the vertical distribution of nutrients we have found the resulting changes to nutrient budgets more straightforward to understand one challenge to investigating the impact of these processes is that they affect small detritus large detritus and semilabile don differently and only total particulate and dissolved organic nitrogen are currently measured in the bay 5 conclusion to date most models of the chesapeake bay have focused on heterotrophic denitrification as the major loss term for fixed nitrogen while the release of sulfide from sediments has previously been proposed to play an important role in biogeochemical cycling within the chesapeake bay roden and tuttle 1992 testa et al 2014 cerco and noel 2017 it has been mostly thought of as a sink for oxygen however in recent years it has become clear that other processes including anammox and cryptic sulfur cycling can be significant drivers of fixed nitrogen loss in anoxic waters canfield et al 2010 in order to model these additional processes in the bay a biogeochemical model for the peru upwelling system that included both anammox and sulfide oxidation with denitrification azhar et al 2014 was implemented in the bay using the original set of parameters calibrated for the open ocean snp peru while the snp peru model apparently resulted in an improved simulation for oxygen and nitrate it did not necessarily do so for the right reasons its improvement in modeled oxygen and nitrate concentrations came at the cost of overpredicting the concentration of ammonium we found the differences in oxygen concentrations were not driven by the inclusion of new sulfur cycling terms but rather by the neglect of burial and dissolved organic matter cycling omitting organic matter burial and dom cycling also resulted in increasing the error in ammonium concentrations by allowing ammonium to accumulate in the water column while differences in nitrate were found to be associated with differences in the equations sulfur cycling anammox light attenuation we found that light attenuation played an important role in explaining these differences rather than the inclusion of the cryptic sulfur cycle differences in parameters common to the two codes peru vs ches tended to compensate the differences due to adding burial and dom or light attenuation so that using the parameters calibrated for the chesapeake in the model developed for the open ocean actually made the solution worse this highlights the extent to which model parameters in chesapeake bay models are best depends critically on which processes are included within the model through a series of sensitivity studies sinking velocities were identified as playing a particularly important role amongst all parameters differing between the original simulations decreasing half saturation concentrations of no2 removing a threshold for nitrite and or increasing the nitrification rate for no2 helped to reduce nitrite without changing oxygen nitrate and ammonium too much our results also suggest a number of targets for future experimental work in the bay better characterization of particle sinking the thresholds for nitrification and the distribution of hydrogen sulfide are obvious ones that emerge from our results however we would suggest it is also important to make more measurements of rates in particular denitrification sulfate reduction and sulfide oxidation across a range of oxygen concentrations this would help us to determine whether thresholds for microbial processes like sulfate reduction are too low as previous work arora williams 2020 arora williams et al 2022 preheim et al 2022 shows that genes associated with sulfur cycling may not be limited to the lowest oxygen levels credit authorship contribution statement rui jin implemented and performed the snp model simulations performed the analysis and led the writing of the manuscript marie aude pradal set up and performed the original chesroms ecb simulations helped with the implementation of the new code and with experimental design and edited the manuscript kalev hantsoo provided the version of the snp code we used here and edited the manuscript anand gnanadesikan helped with analysis of the models experimental design and helped write and edit the manuscript pierre st laurent provided the original chesroms ecb code and model forcing helped to validate the code at johns hopkins and edited the manuscript christian j bjerrum original developer of the bioredoxcnps code identified key differences that guided the experimental design helped to edit the manuscript declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments cjb supported by the danish national research foundation denmark dnrf 53 toward development of bioredoxcnps and villum foundation denmark grant16518 toward kh visiting at ucph 
23794,this study applies artificial neural network based methodology to predict the breaker height h b 1120 data samples from 53 experiments covering a wide range of seabed slopes are used to examine 36 existing models and artificial neural network ann models the results indicate that the developed models are more reliable than existing models not only in overall cases but also in any slope range horizontal gentle intermediate and steep slope especially for ann model 1 root mean square relative error reduces 3 2 5 9 in comparison with the three best existing formulas in general additionally there is more than 90 of calculated data that differ from measured data by less than 20 as using ann model 1 this result is better than all existing formulas in case of classification according to surf similarity wave ann model 1 is also better at estimating breaking wave height for both spilling and plunging types the sensitivity analyses for seabed slope water depth and deep water wavelength with various models are also carried out the effect of slope breaker depth deep water wavelength and deep water wave height on h b is quite similar to existing models based on the outstanding results ann model 1 is acceptable and highly recommended to apply in estimation of breaker height keywords breaking wave height iribarren number artificial neural network spilling type breaking waves plunging type breaking waves data availability data will be made available on request 1 introduction breaking wave one of the most complicated phenomena in coastal engineering has been investigated for a century although it is still a far distance to have an explicit understanding of all aspects of breaking wave i e breaking wave height breaking wave velocity run up etc many authors have put their efforts into contributing to the accumulated knowledge of breaking wave gao et al 2021 2019 especially breaking wave height since breaking wave height plays an important role in wave transformation and coastal infrastructure design many researchers have paid attention to developing and modifying breaker height formula by using wave theory or empirical methods mccowan 1894 published the initial research on breaker index of solitary wave ratio of breaker height to breaking water depth and reported that this index is constant and equal to 0 78 later on several authors have discovered that other breaker indices h b l o h b l b and h b h o can be derived from wave parameters e g breaking water depth h b wavelength at deep water l o breaking wavelength l b wave height at deep water h o deep water wave steepness h o l o the ratio of breaking water depth to breaking wavelength h b l b and seabed slope m miche 1944 took the initiative in expressing the ratio of breaker height to breaking wavelength h b l b by a function of two parameters including breaking water depth and breaking wavelength according to munk 1949 a power function of deep water wave steepness acceptably represents the term h b h o thereafter goda 1974 found that the breaker index of h b l o can be predicted following exponential functions of beach slope breaking water depth and deep water wavelength this formula has been popularly applied to routine design works by coastal engineering in japan because of its reputation in application several researchers have calibrated or modified this model for better prediction rattanapitikon et al 2015 rattanapitikon and shibayama 2000 tomasicchio et al 2020 in general existing breaking wave height models were developed by combining governing wave propagation physics and empirical fitting to experimental data robertson et al 2015b these models have specific errors for prediction of breaker height due to limitations in technique as well as the number of data samples therefore an alternative method should be applied to propose breaker height formula and the number of data should be enlarged for model development recently machine learning is an effective method for model establishment it becomes more and more popular in engineering especially in coastal and ocean engineering because the understanding of relationships between input and output parameters is not requisite this method has been applied in wave field for predicting wave parameters deo et al 2001 applied a 3 layered feed forward neural network to predict significant wave heights and average wave periods kazeminezhad et al 2005 developed two models to estimate wave characteristics including wave height and wave period by adaptive network based fis method elbisy 2015 applied support vector machine svm backpropagation neural network and cascade correlation neural network method for prediction of wave parameters i e wave height direction and period several studies reported that ann method exhibits good performance at wave characteristic estimation deo and jagdale 2003 robertson et al 2015a b for instance deo and sridhar naidu 1999 evaluated the workability of two methods consisting of ann and auto regressive in predicting wave height the authors revealed that ann model provides higher covariation of measured and predicted wave height malekmohamadi et al 2011 predicted wave height by using svm ann bayesian network and adaptive neuro fuzzy inference system and compared the predicting efficiency among these methods the study indicated that the model developed from ann shows the best performance in estimation yun et al 2022 developed an ann model for regular breaking wave index based on experimental data and concluded developed model better than existing breaking wave index models in prediction although some studies already applied artificial neural network to predict breaking wave height the number of data used in their studies seems to be limited such as only 630 data points used by yun et al 2022 because of the great performance of ann approach from earlier researches and the limitation of data number this study will apply artificial neural network based approach to propose breaking wave height models with the hope that ann models are able to show better performance compared to existing breaker height models in various slope ranges and breaker types in order to become universal models ann models have to display outstanding performance in a wide range of wave conditions therefore a large amount of data with various wave conditions as well as seabed slopes are indeed necessary to propose ann models in summary this research aims to verify the performance of existing breaker height formulas with a huge amount of collected data and then develop new models by using artificial neural network for predicting breaking wave height with a wide range of wave conditions a total of 1120 experimental data from 53 sources are collected for development and verification processes the most reliable existing formulas are selected and then compared with developed ann models according to root mean square relative error rmse mean square error mse mean absolute error mae as well as error indices p10 and p20 in this research the performance of existing and developed models is examined following not only seabed slopes i e horizontal gentle intermediate and steep slopes but also surf similarity wave spilling and plunging types the comparison between developed and existing models through sensitivity analysis is also investigated 2 existing breaking wave height formulas until now a lot of breaker height models have been proposed based on empirical or semi empirical approaches due to complicated phenomenon of breaking wave liu et al 2011 classified existing models based on breaker indices into four types including mccowan h b h b miche h b l b munk h b h o and goda type h b l o in general these breaker indices are usually expressed as linear trigonometric or exponential functions of wave and seabed parameters which consist of beach slope m wave period t breaking water depth h b breaking wavelength l b deep water wave height h o and deep water wavelength l o in the present work 36 existing breaking wave formulas are taken into account for checking the performance of estimation as shown in table 1 3 data collection and existing formulas examination many experimental data are indeed necessary to propose artificial neuron network models for estimating breaking wave height at different beach slope conditions the authors therefore make an extensive effort to collect as much data as possible and finally 1120 data samples from 53 sources are collected for developing ann models table 2 shows 53 small and large scale laboratory experiments which were performed under various beach conditions plane stepped barred and movable beach covering a wide range of beach slope from 0 to 0 41 deep water wave steepness is lying between 0 001 and 0 127 in the present work the authors apply linear wave theory to determine breaking wavelength for all cases and deep water wavelength for some cases missing data in order to determine the most reliable formulas among 36 existing models the accuracy of existing models is evaluated according to root mean square relative errors as eq 1 table 3 shows rmse of existing formulas according to different slope conditions rattanapitikon and shibayama 2000 including horizontal slope m 0 gentle slope 0 m 0 07 intermediate slope 0 07 m 0 1 and steep slope m 0 1 as can be obviously seen in this table four different models give the best prediction in four ranges of slope kg72b is the most reliable model for computing breaking wave height with the lowest rmse of 16 3 for horizontal slope in case of slope ranging between 0 and 0 07 sw80a is the best with rmse of 10 5 but this formula is not applicable for computing breaker height when m 0 lk89b is the most advantageous formula in 0 07 m 0 1 rmse 13 meanwhile rs00a gives the best prediction of breaker height when m 0 1 and the root mean square relative error of this model for m 0 1 is only 8 7 in addition this study also examines the advantageous existing formulas according to surf similarity parameter which is used to classify breaker types i e spilling plunging surging and collapsing the authors determine surf similarity parameter or iribarren number ξ o based on battjes s method as eq 2 the prediction capacity of all existing models in spilling and plunging types is also displayed in table 3 and the results indicate that lk89b is the most reliable formula for both spilling and plunging breaker types when considering overall performance the four best existing models are lk89b gl92 rs00a and ps01 with respect to rmse of 10 5 13 1 11 6 and 13 2 although these models present the lowest rmse in general they are not the best ones in each specific range of slope for this reason the best reliable models in each slope range breaker type and all cases are used for further quantification to assure the fairest comparison between existing and developed models as a result six existing models including kg72b sw80a lk89b gl92 rs00a and ps01 are picked up and continuously quantified based on the capacity prediction e standard deviation ν and error indices p10 and p20 the error indices p10 and p20 are found by the percentage of data that relative error is less than 10 and 20 respectively the e and v values are calculated corresponding to eqs 3 and 4 1 r m s e 100 i 1 n h b c i h b m i 2 i 1 n h b m i 2 2 ξ o m h o l o 3 e 100 1 n 1 n h b c i h b m i h b c i h b m i 2 4 ν 100 1 n 1 n h b c i h b m i h b c i h b m i 2 2 where i is the case number i th h b c i is the computed breaker height at case number i th h b m i is the computed breaker height at case number i th rmse is the root mean square relative error e is the prediction capacity which is overestimation when e 0 and vice versa ν is standard deviation m is the seabed slope h o is the deep water wave height l o is the deep water wavelength and ξ o is the iribarren number when ξ o 0 5 the breaking type is called as spilling for cases of ξ o 0 5 and ξ o 3 3 the breaking wave is classified as plunging for ξ o 3 3 the breaking wave is surging or collapsing the overall performance of selected formulas is shown in table 4 all six selected existing formulas are underestimated since their e values are negative except for kg72b 3 the standard deviation of these formulas is pretty parallel about 14 except for sw80a and gl92 with standard deviations of 16 2 and 17 6 respectively besides fig 1 illustrates the representative relationship between measured breaker index h b m h b and calculated breaker index h b c h b it is clear to observe that the number of data points outside p20 region for gl92 is much more than for other formulas gl92 also shows the worst error indices p10 49 2 and p20 74 2 while the other formulas have the same level of error indices 50 higher for p10 and 80 higher for p20 except for sw80a p20 76 5 as a result it can be easily concluded that gl92 and sw80a are less reliable compared to the others in other words kg72b lk89b rs00a and ps01 are the top four advantageous existing models for predicting breaking wave height in all cases in case of classification according to breaker types including spilling and plunging in table 4 the performances of six selected formulas in spilling and plunging waves are quite similar to general case except for kg72b kg72b is one of the top four models that show good results at error indices in general case however its result on p10 42 1 in plunging type is the worst therefore kg72b is neglected from the top four existing models in breaker height prediction from the results in spilling plunging and general cases top three existing models are lk89b rs00a and ps01 these three formulas have the lowest rmse as well as the same level of standard deviation p10 and p20 all these formulas are underestimating in both spilling and plunging types except for rs00a in plunging type when making a comparison between spilling and plunging types it is easy to see that the three most reliable formulas give better prediction in spilling type because of better evaluation indices rmse ν p10 and p20 these results could be visually observed from fig 2 which presents the performances of six selected existing formulas in spilling and plunging types 4 development of artificial neural network based models as mentioned above five wave parameters including beach slope wave period breaker depth deep water wave height and deep water wavelength are usually included in existing formulas because of this these five parameters are also considered as possible input parameters in artificial neural network models which are used to estimate breaking wave height h b output parameter the value ranges of both input and output parameters are shown in table 5 three groups of data including training group 60 of all data calibration group 20 of all data and verification group 20 of all data are randomly selected and are used to propose and verify ann models the purpose of training data group is to train ann models calibration group is used for stopping the learning process while developed ann models are examined by using a verification group all data are normalized in the range from 1 to 1 as eq 5 in this research multi layer perceptron mlp a type of feed forward architecture with full interconnection among neurons is used in developing ann models the simplest form of mlp is considered and this form consists of three layers which are input hidden and output layers in input layer there are five neurons with respect to five possible effective parameters in regard to hidden layer the number of interconnected neurons is selected based on the trial and error method to simplify the selection of neuron number in this layer eq 6 is used to limit the number of neurons following the suggestion of caudill 1987 and therefore the neuron number in hidden layer should be less than 11 neurons for output layer there is one neuron corresponding to breaking wave height the transfer functions for hidden and output layers in ann development are sigmoid and linear as eqs 7 and 8 respectively duong and tran 2022 fig 3 shows the geometry networks following to six and ten neurons in hidden layer for developing ann models note that the number of neurons in hidden layer is carefully selected to see the difference in the prediction performance of ann models with different neuron numbers matlab software r2021 is used for ann model development the mathematical models are shown as eq 9 respected to normalized breaker height in order to determine h b values the de normalization process is necessary 5 x n 2 x x min x max x min 1 6 n max h i d d e n 2 t 1 7 tansig x 2 1 e 2 x 1 8 linear x x 9 h b n b k j 1 5 or 10 w j k f s i g b j i 1 5 w i j i i n where x n is the value of normalized parameter x max is the maximum value of parameter x min is the minimum value of parameter x is the value of parameter n max hidden is the upper limited number of neurons in hidden layer t is the number of input parameters h b n are the normalized output variables i i n is the i th normalized input parameter b j and b k are the j th and k th biases with respect to hidden layer and output layer w i j and w j k are the connection weights corresponding to input hidden layer connection between i th neuron and j th neuron and hidden output layer connection between j th neuron and k th neuron f s i g is the sigmoid transfer function connection weights and biases are determined by using the backpropagation training algorithm according to levenberg marquardt optimization 5 results and discussion after training procedure the values of connected weights and biases for both two ann models are obtained as shown in table 6 from these values ann models six and ten neurons in hidden layer are able to be established for estimating breaking wave height the relationship between computed h b and measured h b is displayed with different data groups in figs 4 and 5 it is clear to observe in these two plots that both ann models are good at prediction with high correlation coefficients r the degree of reliability of ann models based on root mean square relative error rmse mean square error mse and mean absolute error mae is compared to the best existing formulas following training calibration and verification groups shown in tables 7 and 8 among the two developed models ann model 2 ten neurons in hidden layer gives better prediction in general because of a larger correlation coefficient 0 9964 compared to 0 9961 and higher concentration of data on identity line however the ann model 2 seems to be too complicated with 10 neurons and overfitting because the mse for training group 13 is much smaller than mse for verification group 19 1 meanwhile the ann model 1 six neurons in hidden layer is not overfitting like ann model 2 because of the small difference in mse of training and verification groups 14 3 for training and 17 2 for verification in consequence ann model 1 is recommended to be used in breaker height prediction among two developed ann models in comparison between existing models and ann model 1 table 7 the results indicate that ann model 1 shows better rmse mse and mae in every group of data for instance the application of ann model 1 in breaker height prediction can reduce rmse from 4 to 6 8 for training group 1 3 3 4 for calibration group and 2 2 5 5 for verification group in comparison with lk89b rs00a and ps01 additionally as can be seen from fig 6 the relationship between measured and computed breaker index of ann model 1 is better than that of the three most reliable existing models the percentage of data within p10 and p20 regions when using ann model 1 is 63 8 and 91 table 4 respectively in addition ann model 1 is compared to the existing formulas which give the best prediction in each range of slope as table 3 for horizontal slope m 0 ann model 1 gives better prediction than the most existing valid formula kg72b and rmse of ann model 1 is lower than that of kg72b almost 2 2 with 0 m 0 07 ann model 1 has higher accuracy as compared to sw80a in estimation of breaking wave height rmse of 7 4 compared to rmse of 10 5 for slope from 0 07 to 0 1 ann model 1 can predict breaker height with rmse reduction of 4 8 as compared to lk89b in case of m 0 1 rmse of ann model 1 is only 6 5 compared to 8 7 of rs00a as a result ann model 1 shows great performance in computing breaking wave height for any beach slope range with the rmse reduction from 2 2 to 4 8 in case of classification following breaker types comprising spilling and plunging the performance of ann model 1 is still greater than the existing formulas all evaluation indices are significantly better when using ann model 1 as shown in table 4 to sum up according to root mean square relative error standard deviation and error indices it is concluded that artificial neural network based models are more reliable than existing formulas in general spilling and plunging cases in these developed models ann model 1 is suggested to be used in predicting breaking wave height because it has exceptional results in root mean square relative error 7 3 as well as error index p20 91 when compared to all formulas 6 sensitivity analysis the effect of parameters including seabed slope breaker depth deep water wavelength and deep water wave height on breaking wave height is investigated in this part the comparison between developed models and top reliable existing formulas on this aspect is also conducted the sensitivity analysis for parameters of m and l o is carried out by changing the value of examined parameter while other parameters are assumed as average values of data except for the parameter of t the value of t is calculated following the equation of l o gt 2 2 π as can be seen in fig 7 the existing formulas can be classified into 2 groups the first group consisting of lk89b and gl92 is independent of m horizontal line the second group includes rs00a and ps01 with breaker height affected by seabed slope variation ann model 1 has a tendency quite similar to the second group and can be described by a polynomial function in the case of effectiveness of l o as shown as fig 8 all considered formulas have the same tendency apart from ps01 breaking wave height increases with the increase of deep water wavelength fig 9 illustrates the effect of water depth on breaking wave height with various models to draw this figure all parameters are assumed as mentioned above except for deep water wave height h o h o is equal to 0 68 times h b and the value of 0 68 is determined from the ratio of average h o to average h b in general the developed models are pretty similar to existing models and breaking wave height increases with the increase in water depth the effect of h o on h b is evaluated in the same way as parameter of h b the difference is only assuming that h o varies from 0 to 200 cm and the values of h b are calculated following h o h b 0 68 corresponding to each value of h o the effect of h o on h b is identical to that of h b on h b this section therefore does not show the investigated results for h o from the results of sensitivity analysis for m l o h b and h o it is clear to conclude that the effect of considered parameters on breaking wave height is quite similar to existing formulas 7 conclusion the present work develops two models six and ten neurons in hidden layer based on artificial neural network for predicting breaking wave height a great amount of data points with 1120 cases from 53 laboratory experiments are made use of developing as well as checking the reliability of developed and existing models these new two models are compared to 36 existing breaker height formulas according to the results of rmse and error indices p10 and p20 developed ann formulas give better performance in predicting breaking wave height compared to existing formulas in general ann model 1 is the most reliable among the rest of considered formulas for p20 criterion ann model 1 gives the highest p20 of 91 it is higher than the three most reliable existing formulas from 3 9 to 9 4 in case of evaluating model s performance according to root mean square relative error rmse when using ann model 1 is only 7 3 for all slope conditions this is lower by about 3 2 in comparison with the best existing formula lk89b ann model 1 is also the best formula for estimation at any beach slope conditions m 0 0 m 0 07 0 07 m 0 1 and m 0 1 and similar results can be obtained as classified according to iribaren number compared to the four best existing formulas in each slope range the reduction of rmse is from 2 2 to 4 8 when using ann model 1 in conclusion the developed ann model 1 displays outstanding performances in prediction of breaking wave height and artificial neural based approach is strongly recommended to be applied in coastal research with the positive result of using artificial neural network models in predicting breaking wave height it is feasible to apply other models developed from long short term memory neural networks random forest regression support vector machine and recurrent neural networks to estimate breaking wave height this would be discussed in future studies 8 limitations of study the developed ann models are encouraged to apply in the wave and seabed conditions following table 5 the application of ann models outside those ranges of slope and wave conditions may result in undesired consequences although the developed models in this study can be applied in a wide range of seabed and wave conditions future studies are necessarily conducted to extend the scope of applying breaker height models in practice declaration of competing interest the authors in the present work confirm that this article is original and not under consideration for publishing in any other journal the manuscript had been approved by other authors there are no ethical issues or conflict of interest in this article acknowledgment we acknowledge the support of time and facilities from ho chi minh city university of technology hcmut vnu hcm for this study 
23794,this study applies artificial neural network based methodology to predict the breaker height h b 1120 data samples from 53 experiments covering a wide range of seabed slopes are used to examine 36 existing models and artificial neural network ann models the results indicate that the developed models are more reliable than existing models not only in overall cases but also in any slope range horizontal gentle intermediate and steep slope especially for ann model 1 root mean square relative error reduces 3 2 5 9 in comparison with the three best existing formulas in general additionally there is more than 90 of calculated data that differ from measured data by less than 20 as using ann model 1 this result is better than all existing formulas in case of classification according to surf similarity wave ann model 1 is also better at estimating breaking wave height for both spilling and plunging types the sensitivity analyses for seabed slope water depth and deep water wavelength with various models are also carried out the effect of slope breaker depth deep water wavelength and deep water wave height on h b is quite similar to existing models based on the outstanding results ann model 1 is acceptable and highly recommended to apply in estimation of breaker height keywords breaking wave height iribarren number artificial neural network spilling type breaking waves plunging type breaking waves data availability data will be made available on request 1 introduction breaking wave one of the most complicated phenomena in coastal engineering has been investigated for a century although it is still a far distance to have an explicit understanding of all aspects of breaking wave i e breaking wave height breaking wave velocity run up etc many authors have put their efforts into contributing to the accumulated knowledge of breaking wave gao et al 2021 2019 especially breaking wave height since breaking wave height plays an important role in wave transformation and coastal infrastructure design many researchers have paid attention to developing and modifying breaker height formula by using wave theory or empirical methods mccowan 1894 published the initial research on breaker index of solitary wave ratio of breaker height to breaking water depth and reported that this index is constant and equal to 0 78 later on several authors have discovered that other breaker indices h b l o h b l b and h b h o can be derived from wave parameters e g breaking water depth h b wavelength at deep water l o breaking wavelength l b wave height at deep water h o deep water wave steepness h o l o the ratio of breaking water depth to breaking wavelength h b l b and seabed slope m miche 1944 took the initiative in expressing the ratio of breaker height to breaking wavelength h b l b by a function of two parameters including breaking water depth and breaking wavelength according to munk 1949 a power function of deep water wave steepness acceptably represents the term h b h o thereafter goda 1974 found that the breaker index of h b l o can be predicted following exponential functions of beach slope breaking water depth and deep water wavelength this formula has been popularly applied to routine design works by coastal engineering in japan because of its reputation in application several researchers have calibrated or modified this model for better prediction rattanapitikon et al 2015 rattanapitikon and shibayama 2000 tomasicchio et al 2020 in general existing breaking wave height models were developed by combining governing wave propagation physics and empirical fitting to experimental data robertson et al 2015b these models have specific errors for prediction of breaker height due to limitations in technique as well as the number of data samples therefore an alternative method should be applied to propose breaker height formula and the number of data should be enlarged for model development recently machine learning is an effective method for model establishment it becomes more and more popular in engineering especially in coastal and ocean engineering because the understanding of relationships between input and output parameters is not requisite this method has been applied in wave field for predicting wave parameters deo et al 2001 applied a 3 layered feed forward neural network to predict significant wave heights and average wave periods kazeminezhad et al 2005 developed two models to estimate wave characteristics including wave height and wave period by adaptive network based fis method elbisy 2015 applied support vector machine svm backpropagation neural network and cascade correlation neural network method for prediction of wave parameters i e wave height direction and period several studies reported that ann method exhibits good performance at wave characteristic estimation deo and jagdale 2003 robertson et al 2015a b for instance deo and sridhar naidu 1999 evaluated the workability of two methods consisting of ann and auto regressive in predicting wave height the authors revealed that ann model provides higher covariation of measured and predicted wave height malekmohamadi et al 2011 predicted wave height by using svm ann bayesian network and adaptive neuro fuzzy inference system and compared the predicting efficiency among these methods the study indicated that the model developed from ann shows the best performance in estimation yun et al 2022 developed an ann model for regular breaking wave index based on experimental data and concluded developed model better than existing breaking wave index models in prediction although some studies already applied artificial neural network to predict breaking wave height the number of data used in their studies seems to be limited such as only 630 data points used by yun et al 2022 because of the great performance of ann approach from earlier researches and the limitation of data number this study will apply artificial neural network based approach to propose breaking wave height models with the hope that ann models are able to show better performance compared to existing breaker height models in various slope ranges and breaker types in order to become universal models ann models have to display outstanding performance in a wide range of wave conditions therefore a large amount of data with various wave conditions as well as seabed slopes are indeed necessary to propose ann models in summary this research aims to verify the performance of existing breaker height formulas with a huge amount of collected data and then develop new models by using artificial neural network for predicting breaking wave height with a wide range of wave conditions a total of 1120 experimental data from 53 sources are collected for development and verification processes the most reliable existing formulas are selected and then compared with developed ann models according to root mean square relative error rmse mean square error mse mean absolute error mae as well as error indices p10 and p20 in this research the performance of existing and developed models is examined following not only seabed slopes i e horizontal gentle intermediate and steep slopes but also surf similarity wave spilling and plunging types the comparison between developed and existing models through sensitivity analysis is also investigated 2 existing breaking wave height formulas until now a lot of breaker height models have been proposed based on empirical or semi empirical approaches due to complicated phenomenon of breaking wave liu et al 2011 classified existing models based on breaker indices into four types including mccowan h b h b miche h b l b munk h b h o and goda type h b l o in general these breaker indices are usually expressed as linear trigonometric or exponential functions of wave and seabed parameters which consist of beach slope m wave period t breaking water depth h b breaking wavelength l b deep water wave height h o and deep water wavelength l o in the present work 36 existing breaking wave formulas are taken into account for checking the performance of estimation as shown in table 1 3 data collection and existing formulas examination many experimental data are indeed necessary to propose artificial neuron network models for estimating breaking wave height at different beach slope conditions the authors therefore make an extensive effort to collect as much data as possible and finally 1120 data samples from 53 sources are collected for developing ann models table 2 shows 53 small and large scale laboratory experiments which were performed under various beach conditions plane stepped barred and movable beach covering a wide range of beach slope from 0 to 0 41 deep water wave steepness is lying between 0 001 and 0 127 in the present work the authors apply linear wave theory to determine breaking wavelength for all cases and deep water wavelength for some cases missing data in order to determine the most reliable formulas among 36 existing models the accuracy of existing models is evaluated according to root mean square relative errors as eq 1 table 3 shows rmse of existing formulas according to different slope conditions rattanapitikon and shibayama 2000 including horizontal slope m 0 gentle slope 0 m 0 07 intermediate slope 0 07 m 0 1 and steep slope m 0 1 as can be obviously seen in this table four different models give the best prediction in four ranges of slope kg72b is the most reliable model for computing breaking wave height with the lowest rmse of 16 3 for horizontal slope in case of slope ranging between 0 and 0 07 sw80a is the best with rmse of 10 5 but this formula is not applicable for computing breaker height when m 0 lk89b is the most advantageous formula in 0 07 m 0 1 rmse 13 meanwhile rs00a gives the best prediction of breaker height when m 0 1 and the root mean square relative error of this model for m 0 1 is only 8 7 in addition this study also examines the advantageous existing formulas according to surf similarity parameter which is used to classify breaker types i e spilling plunging surging and collapsing the authors determine surf similarity parameter or iribarren number ξ o based on battjes s method as eq 2 the prediction capacity of all existing models in spilling and plunging types is also displayed in table 3 and the results indicate that lk89b is the most reliable formula for both spilling and plunging breaker types when considering overall performance the four best existing models are lk89b gl92 rs00a and ps01 with respect to rmse of 10 5 13 1 11 6 and 13 2 although these models present the lowest rmse in general they are not the best ones in each specific range of slope for this reason the best reliable models in each slope range breaker type and all cases are used for further quantification to assure the fairest comparison between existing and developed models as a result six existing models including kg72b sw80a lk89b gl92 rs00a and ps01 are picked up and continuously quantified based on the capacity prediction e standard deviation ν and error indices p10 and p20 the error indices p10 and p20 are found by the percentage of data that relative error is less than 10 and 20 respectively the e and v values are calculated corresponding to eqs 3 and 4 1 r m s e 100 i 1 n h b c i h b m i 2 i 1 n h b m i 2 2 ξ o m h o l o 3 e 100 1 n 1 n h b c i h b m i h b c i h b m i 2 4 ν 100 1 n 1 n h b c i h b m i h b c i h b m i 2 2 where i is the case number i th h b c i is the computed breaker height at case number i th h b m i is the computed breaker height at case number i th rmse is the root mean square relative error e is the prediction capacity which is overestimation when e 0 and vice versa ν is standard deviation m is the seabed slope h o is the deep water wave height l o is the deep water wavelength and ξ o is the iribarren number when ξ o 0 5 the breaking type is called as spilling for cases of ξ o 0 5 and ξ o 3 3 the breaking wave is classified as plunging for ξ o 3 3 the breaking wave is surging or collapsing the overall performance of selected formulas is shown in table 4 all six selected existing formulas are underestimated since their e values are negative except for kg72b 3 the standard deviation of these formulas is pretty parallel about 14 except for sw80a and gl92 with standard deviations of 16 2 and 17 6 respectively besides fig 1 illustrates the representative relationship between measured breaker index h b m h b and calculated breaker index h b c h b it is clear to observe that the number of data points outside p20 region for gl92 is much more than for other formulas gl92 also shows the worst error indices p10 49 2 and p20 74 2 while the other formulas have the same level of error indices 50 higher for p10 and 80 higher for p20 except for sw80a p20 76 5 as a result it can be easily concluded that gl92 and sw80a are less reliable compared to the others in other words kg72b lk89b rs00a and ps01 are the top four advantageous existing models for predicting breaking wave height in all cases in case of classification according to breaker types including spilling and plunging in table 4 the performances of six selected formulas in spilling and plunging waves are quite similar to general case except for kg72b kg72b is one of the top four models that show good results at error indices in general case however its result on p10 42 1 in plunging type is the worst therefore kg72b is neglected from the top four existing models in breaker height prediction from the results in spilling plunging and general cases top three existing models are lk89b rs00a and ps01 these three formulas have the lowest rmse as well as the same level of standard deviation p10 and p20 all these formulas are underestimating in both spilling and plunging types except for rs00a in plunging type when making a comparison between spilling and plunging types it is easy to see that the three most reliable formulas give better prediction in spilling type because of better evaluation indices rmse ν p10 and p20 these results could be visually observed from fig 2 which presents the performances of six selected existing formulas in spilling and plunging types 4 development of artificial neural network based models as mentioned above five wave parameters including beach slope wave period breaker depth deep water wave height and deep water wavelength are usually included in existing formulas because of this these five parameters are also considered as possible input parameters in artificial neural network models which are used to estimate breaking wave height h b output parameter the value ranges of both input and output parameters are shown in table 5 three groups of data including training group 60 of all data calibration group 20 of all data and verification group 20 of all data are randomly selected and are used to propose and verify ann models the purpose of training data group is to train ann models calibration group is used for stopping the learning process while developed ann models are examined by using a verification group all data are normalized in the range from 1 to 1 as eq 5 in this research multi layer perceptron mlp a type of feed forward architecture with full interconnection among neurons is used in developing ann models the simplest form of mlp is considered and this form consists of three layers which are input hidden and output layers in input layer there are five neurons with respect to five possible effective parameters in regard to hidden layer the number of interconnected neurons is selected based on the trial and error method to simplify the selection of neuron number in this layer eq 6 is used to limit the number of neurons following the suggestion of caudill 1987 and therefore the neuron number in hidden layer should be less than 11 neurons for output layer there is one neuron corresponding to breaking wave height the transfer functions for hidden and output layers in ann development are sigmoid and linear as eqs 7 and 8 respectively duong and tran 2022 fig 3 shows the geometry networks following to six and ten neurons in hidden layer for developing ann models note that the number of neurons in hidden layer is carefully selected to see the difference in the prediction performance of ann models with different neuron numbers matlab software r2021 is used for ann model development the mathematical models are shown as eq 9 respected to normalized breaker height in order to determine h b values the de normalization process is necessary 5 x n 2 x x min x max x min 1 6 n max h i d d e n 2 t 1 7 tansig x 2 1 e 2 x 1 8 linear x x 9 h b n b k j 1 5 or 10 w j k f s i g b j i 1 5 w i j i i n where x n is the value of normalized parameter x max is the maximum value of parameter x min is the minimum value of parameter x is the value of parameter n max hidden is the upper limited number of neurons in hidden layer t is the number of input parameters h b n are the normalized output variables i i n is the i th normalized input parameter b j and b k are the j th and k th biases with respect to hidden layer and output layer w i j and w j k are the connection weights corresponding to input hidden layer connection between i th neuron and j th neuron and hidden output layer connection between j th neuron and k th neuron f s i g is the sigmoid transfer function connection weights and biases are determined by using the backpropagation training algorithm according to levenberg marquardt optimization 5 results and discussion after training procedure the values of connected weights and biases for both two ann models are obtained as shown in table 6 from these values ann models six and ten neurons in hidden layer are able to be established for estimating breaking wave height the relationship between computed h b and measured h b is displayed with different data groups in figs 4 and 5 it is clear to observe in these two plots that both ann models are good at prediction with high correlation coefficients r the degree of reliability of ann models based on root mean square relative error rmse mean square error mse and mean absolute error mae is compared to the best existing formulas following training calibration and verification groups shown in tables 7 and 8 among the two developed models ann model 2 ten neurons in hidden layer gives better prediction in general because of a larger correlation coefficient 0 9964 compared to 0 9961 and higher concentration of data on identity line however the ann model 2 seems to be too complicated with 10 neurons and overfitting because the mse for training group 13 is much smaller than mse for verification group 19 1 meanwhile the ann model 1 six neurons in hidden layer is not overfitting like ann model 2 because of the small difference in mse of training and verification groups 14 3 for training and 17 2 for verification in consequence ann model 1 is recommended to be used in breaker height prediction among two developed ann models in comparison between existing models and ann model 1 table 7 the results indicate that ann model 1 shows better rmse mse and mae in every group of data for instance the application of ann model 1 in breaker height prediction can reduce rmse from 4 to 6 8 for training group 1 3 3 4 for calibration group and 2 2 5 5 for verification group in comparison with lk89b rs00a and ps01 additionally as can be seen from fig 6 the relationship between measured and computed breaker index of ann model 1 is better than that of the three most reliable existing models the percentage of data within p10 and p20 regions when using ann model 1 is 63 8 and 91 table 4 respectively in addition ann model 1 is compared to the existing formulas which give the best prediction in each range of slope as table 3 for horizontal slope m 0 ann model 1 gives better prediction than the most existing valid formula kg72b and rmse of ann model 1 is lower than that of kg72b almost 2 2 with 0 m 0 07 ann model 1 has higher accuracy as compared to sw80a in estimation of breaking wave height rmse of 7 4 compared to rmse of 10 5 for slope from 0 07 to 0 1 ann model 1 can predict breaker height with rmse reduction of 4 8 as compared to lk89b in case of m 0 1 rmse of ann model 1 is only 6 5 compared to 8 7 of rs00a as a result ann model 1 shows great performance in computing breaking wave height for any beach slope range with the rmse reduction from 2 2 to 4 8 in case of classification following breaker types comprising spilling and plunging the performance of ann model 1 is still greater than the existing formulas all evaluation indices are significantly better when using ann model 1 as shown in table 4 to sum up according to root mean square relative error standard deviation and error indices it is concluded that artificial neural network based models are more reliable than existing formulas in general spilling and plunging cases in these developed models ann model 1 is suggested to be used in predicting breaking wave height because it has exceptional results in root mean square relative error 7 3 as well as error index p20 91 when compared to all formulas 6 sensitivity analysis the effect of parameters including seabed slope breaker depth deep water wavelength and deep water wave height on breaking wave height is investigated in this part the comparison between developed models and top reliable existing formulas on this aspect is also conducted the sensitivity analysis for parameters of m and l o is carried out by changing the value of examined parameter while other parameters are assumed as average values of data except for the parameter of t the value of t is calculated following the equation of l o gt 2 2 π as can be seen in fig 7 the existing formulas can be classified into 2 groups the first group consisting of lk89b and gl92 is independent of m horizontal line the second group includes rs00a and ps01 with breaker height affected by seabed slope variation ann model 1 has a tendency quite similar to the second group and can be described by a polynomial function in the case of effectiveness of l o as shown as fig 8 all considered formulas have the same tendency apart from ps01 breaking wave height increases with the increase of deep water wavelength fig 9 illustrates the effect of water depth on breaking wave height with various models to draw this figure all parameters are assumed as mentioned above except for deep water wave height h o h o is equal to 0 68 times h b and the value of 0 68 is determined from the ratio of average h o to average h b in general the developed models are pretty similar to existing models and breaking wave height increases with the increase in water depth the effect of h o on h b is evaluated in the same way as parameter of h b the difference is only assuming that h o varies from 0 to 200 cm and the values of h b are calculated following h o h b 0 68 corresponding to each value of h o the effect of h o on h b is identical to that of h b on h b this section therefore does not show the investigated results for h o from the results of sensitivity analysis for m l o h b and h o it is clear to conclude that the effect of considered parameters on breaking wave height is quite similar to existing formulas 7 conclusion the present work develops two models six and ten neurons in hidden layer based on artificial neural network for predicting breaking wave height a great amount of data points with 1120 cases from 53 laboratory experiments are made use of developing as well as checking the reliability of developed and existing models these new two models are compared to 36 existing breaker height formulas according to the results of rmse and error indices p10 and p20 developed ann formulas give better performance in predicting breaking wave height compared to existing formulas in general ann model 1 is the most reliable among the rest of considered formulas for p20 criterion ann model 1 gives the highest p20 of 91 it is higher than the three most reliable existing formulas from 3 9 to 9 4 in case of evaluating model s performance according to root mean square relative error rmse when using ann model 1 is only 7 3 for all slope conditions this is lower by about 3 2 in comparison with the best existing formula lk89b ann model 1 is also the best formula for estimation at any beach slope conditions m 0 0 m 0 07 0 07 m 0 1 and m 0 1 and similar results can be obtained as classified according to iribaren number compared to the four best existing formulas in each slope range the reduction of rmse is from 2 2 to 4 8 when using ann model 1 in conclusion the developed ann model 1 displays outstanding performances in prediction of breaking wave height and artificial neural based approach is strongly recommended to be applied in coastal research with the positive result of using artificial neural network models in predicting breaking wave height it is feasible to apply other models developed from long short term memory neural networks random forest regression support vector machine and recurrent neural networks to estimate breaking wave height this would be discussed in future studies 8 limitations of study the developed ann models are encouraged to apply in the wave and seabed conditions following table 5 the application of ann models outside those ranges of slope and wave conditions may result in undesired consequences although the developed models in this study can be applied in a wide range of seabed and wave conditions future studies are necessarily conducted to extend the scope of applying breaker height models in practice declaration of competing interest the authors in the present work confirm that this article is original and not under consideration for publishing in any other journal the manuscript had been approved by other authors there are no ethical issues or conflict of interest in this article acknowledgment we acknowledge the support of time and facilities from ho chi minh city university of technology hcmut vnu hcm for this study 
