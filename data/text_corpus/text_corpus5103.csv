index,text
25515,recently the conceptual data driven approach cdda was proposed to correct residuals of ensemble hydrological models hms using data driven models ddms followed by the stochastic cdda scdda that used hm simulations as input to ddms within a stochastic framework both approaches improved ensemble hms simulations here a new scdda is introduced where cdda uncertainty is estimated instead of ddm uncertainty in the original scdda using nine hm ddm combinations for daily streamflow simulation in three swiss catchments the new scdda improved cdda s mean continuous ranked probability score up to 15 and performed similarly without a snow routine in a snowy catchment suggesting that scdda may account for missing processes in hms the stochastic framework can convert unreliable ensemble models into more reliable stochastic models at the cost of simulation sharpness the coverage probability plot is proposed as a diagnostic tool predicting scdda s out of sample reliability using validation set data cdda simulations and observations keywords ensemble stochastic streamflow simulation data driven model hydrological model uncertainty 1 introduction hydrologists and water resources practitioners are commonly tasked with estimating the uncertainty of hydrological variables for various applications such as flood forecasting and water management benefitting from process based physical or conceptual hydrological models hms anand et al 2018 jain et al 2018 although statistical methods have been used in the hydrology domain for decades singh 2018 the accelerated advancements in technology and computer science especially data driven models ddms has given a new perspective to the hydrological modelling paradigm by producing accurate models that have similar or better predictive capability than hms beven 2020 nearing et al 2021 to contrast the two modelling approaches hms attempt to simplify and mimic natural processes using mathematical expressions relevant to the hydrological process under study i e explicit descriptions of the perceived catchment response to rainfall runoff using boundary conditions initial conditions mass balance energy balance etc beven 2012 while ddms estimate statistical relationships between explanatory input e g rainfall and response target e g streamflow variables using historical measurements enabling simulations of the target variable although research on these two approaches has primarily advanced independently of one another in the hydrological sciences there is an increasing interest in combining process based hms and ddms to increase our understanding of natural systems and improve predictive performance beyond what is achievable by the individual models adombi et al 2021 karpatne et al 2017 in the hydrological sciences research on combining theoretical knowledge adombi et al 2021 with ddms has progressed using various approaches and many of these approaches do not follow a common naming convention which although outside the scope of this study addressing this issue represents a valuable topic of future research for example coupling hms and ddms have been referred to as physics informed ddms liang et al 2019 hybrid models kurian et al 2020 or simply post processing frame et al 2021 as another example the term physics informed neural networks has been used to represent an approach where partial differential equations are incorporated within artificial neural networks raissi et al 2019 shen and lawson 2021 furthermore several recent studies have used hm outputs as input to ddms demonstrating that such an approach can be used to improve predictive performance compared to the standalone hm frame et al 2021 ghaith et al 2019 konapala et al 2020a kumanlioglu and fistikoglu 2019 lu et al 2021 quilty et al 2022 another combined hm ddm approach gaining popularity is correcting hm simulations by using ddms to simulate the hm residuals cho and kim 2022 li et al 2021a 2021b papacharalampous et al 2020a 2020b sharma et al 2021 shen et al 2022 sikorska senoner and quilty 2021 incorporating model errors in hydrological simulations has traditionally been used for uncertainty estimation for example stochastic resampling from the hm error distribution for assessing predictive uncertainty in streamflow simulations montanari and koutsoyiannis 2012 sikorska et al 2014 as such in a companion study sikorska senoner and quilty 2021 hereinafter referred to as ssq21 developed the conceptual data driven approach cdda to include an ensemble of hms each paired with a ddm that simulates the hm residuals the cdda only characterizes uncertainty in the hm parameters while the ddms simulate the expectation of the residuals associated with each hm parameter set a key outcome of ssq21 showed that all of the nonlinear ddms considered in the study improved the streamflow simulations generated by the lumped conceptual hm although the cdda adopted conceptual hms any deterministic e g process based hm can be used within the cdda as uncertainty in streamflow simulations plays a significant role in water management decisions the companion paper by quilty et al 2022 hereinafter referred to as q22 modified the cdda by including other sources of uncertainty neglected in the cdda input data input variable selection ivs ddm parameters and model output referred to as the stochastic conceptual data driven approach scdda this framework improved streamflow simulations in all study catchments when compared to the cdda however unlike the cdda which uses ddms to directly simulate the residuals of the ensemble hms the ddms within the scdda directly simulate streamflow using the ensemble hm simulations as ddm inputs in other words the scdda proposed in q22 did not follow the same approach for using ddms as in the cdda proposed in ssq21 in contrast to q22 this study develops a new scdda an analogue of the cdda proposed in ssq21 where ddms are used to simulate hm residuals in a stochastic framework that similar to q22 can account for uncertainty in input data ivs parameters and model output the new scdda takes advantage of the multiple parameter sets generated by the routine used to optimize the ddms hyperparameters in the cdda in the companion studies ssq21 and q22 only one hm structure based on the hydrologiska byråns vattenbalansavdelning hbv model specifically hbv light seibert and vis 2012 was utilized making it challenging to identify whether the performance of the cdda and scdda was specific to the hm s structure therefore along with hbv light two additional hms the technische universität wien model tuwmodel parajka et al 2007 sleziak et al 2020 and modèle du génie rural à 4 paramètres journalier perrin et al 2003 are used in this study to explore the performance of the cdda and scdda with respect to different hm structures based on a recommendation from ssq21 which found that extreme gradient boosting xgb chen and guestrin 2016 and random forests rf breiman 2001 were the most suitable ddms to simulate hm residuals out of the eight ddms that were considered this study considers xgb and rf within the cdda and scdda along with these two ddms the long short term memory network lstm hochreiter and schmidhuber 1997 was also included due to its prominent role in hydrological applications involving deep learning shen 2018 lstm is an extension of recurrent neural networks rnn with the capacity to learn time dependencies over long timescales which is important when simulating streamflow as persistence is commonly manifested in hydrological processes hurst 1951 iliopoulou et al 2018 koutsoyiannis 2021 pagano and garen 2005 the competitive predictive performance of lstm has been well recognized in hydrology and water resources and is becoming the model of choice for deep learning applications in these domains see for example feng et al 2021 2020 kratzert et al 2019 liu et al 2022 shen 2018 thus the research objectives of this study are to 1 apply the cdda and the new scdda for all nine combinations of the hms hbv light tuwmodel gr4j and ddms xgb rf lstm for daily streamflow simulation in three swiss catchments 2 compare the performance of the cdda and scdda against one another as well as the ensemble and stochastic hms considering the different combinations of hms and ddms 3 propose and explore the use of a diagnostic tool to predict if the scdda can improve upon the reliability obtained by the cdda the third objective is not only useful for measuring improvements by converting the cdda to its stochastic counterpart but it also serves a practical purpose suppose the diagnostic tool predicts that the scdda will have lower performance than the cdda in this case the scdda may be neglected saving implementation time and computational costs similar to ssq21 and q22 the same three swiss catchments were used to develop the different models and address the research objectives the main novelty of this research lies in the use of the multiple parameter sets explored by the optimization routine in the cdda the cdda requires a computationally demanding search for optimal ddm hyper parameters settings external to the model that affect the calibration of parameters model structure etc each hm in the ensemble is associated with a single set of ddm hyper parameters and a single set of ddm hyper parameters is used to estimate ddm parameters which are responsible for mapping the ddm inputs to the hm residuals however by retaining all parameter sets explored during optimization the uncertainty of the cdda hm and ddm parameters can be utilized within the stochastic framework leading to the new scdda in addition adopting nine different hm ddm within the cdda and scdda for simulating streamflow in multiple catchments allows for the exploration of a diagnostic tool the coverage probability plot cpp to determine if the reliability of the cdda can be improved upon using the scdda the cpp also referred to as the predictive probability probability plot has been proven to be a valuable tool when converting deterministic simulations to stochastic ones koutsoyiannis and montanari 2022 finally the new scdda can also be viewed as the second stage in a post processing framework where after the cdda is used to correct the hm outputs first stage the stochastic framework is used to refine the cdda and assess its uncertainty through stochastic resampling transforming the cdda into the new scdda the rest of the paper is structured as follows section 2 outlines the background and methods related to the cdda and new and original scdda as well as the adopted hms and ddms section 3 describes the experimental setup associated with the hms ddms and stochastic simulation as well as the performance assessment of the different models section 4 presents the results and includes a discussion on the salient findings and section 5 concludes this study 2 background and methods 2 1 ensemble based conceptual data driven approach cdda the cdda utilizes an ensemble of hms using a single model structure and multiple parameter sets to generate streamflow simulations where a ddm is used to correct the residuals of each hm the cdda adopted in ssq21 and q22 can be written as quilty et al 2022 1 y i p t t d t t t d q t 1 t d θ c d d a i y i p t t t θ h m i r i p t t d t t t d q t 1 t d θ d d m i where y i y i and r i are the cdda hm streamflow and ddm hm residuals simulations for ensemble member i at time t here t is removed to simplify the notation respectively p t t d t t t d and q t 1 t d are observed precipitation and air temperature at time lags t t d and streamflow at time lags t 1 t d where d is the maximum time lag respectively θ c d d a i θ h m i θ d d m i are the parameter sets associated with the i th ensemble member of the cdda hm and ddm respectively with θ c d d a i θ h m i θ d d m i while for a given hm e g hbv light gr4j the input variables remain fixed the ddm can accept various input variables not limited to those in equation 1 which may be useful for improving the cdda simulations the cdda adopted in the study is of the general form 2 y i x h m x d d m θ c d d a i y i x h m θ h m i r i x d d m θ d d m i where x h m and x d d m represent the hm and ddm inputs respectively since this study considers hms with different input variable requirements x h m includes p t as well as t mean t and or p e t t depending on the hm see section 2 4 where t mean is the mean air temperature and p e t is the potential evapotranspiration lindström and bergström 1992 however for all hm ddm combinations x d d m includes p t t d t min t t d t max t t d t mean t t d p e t t t d and q t 1 t d where t min and t max are the minimum and maximum air temperatures respectively the most significant limitation of the cdda is that it only explicitly accounts for the hm parameter uncertainty uncertainty associated with the true value of the estimated parameter s thus the cdda can only estimate confidence intervals rather than prediction intervals when quantifying uncertainty in its simulations since prediction intervals account for the uncertainty associated with the prediction of the true value of a given hydrological variable montanari and koutsoyiannis 2012 in other words as a single ddm is used to correct the residuals of each hm ensemble member one ddm for hm the ddms output within the cdda is related to the expected value of the hm residuals not their distribution 2 2 stochastic conceptual data driven approach scdda the scdda was introduced in q22 to account for additional sources of uncertainty i e input data ivs model output not considered in the cdda the scdda is an extension of the earlier frameworks proposed by montanari and koutsoyiannis 2012 which focused on hms and quilty et al 2019 which focused on ddms where an hm is coupled with a ddm in a stochastic framework the scdda makes use of the original equation proposed in montanari and koutsoyiannis 2012 for converting a deterministic hm into a stochastic one 3 f q q θ x f e q s θ x θ x f θ θ f x x d θ d x where f q represents the probability density function pdf of the target variable e g streamflow to be simulated or forecasted and f x f θ and f e represent the pdf of the input data the pdf of the parameter s and the pdf of the model error conditioned on the parameter s and input data respectively with q x and θ as the target variable i e streamflow input data and parameter s respectively and s as the deterministic model for streamflow in q22 the scdda is obtained by modifying equation 3 to include ensemble hm simulations within x along with other hydro meteorological variables and using a ddm for s rather than an hm along with the original assumptions about the stochastic framework in montanari and koutsoyiannis 2012 it was further assumed in q22 that the ddm parameters could be estimated independently of the hm parameters since the ensemble hm simulations were used as input to the ddms and represented input data uncertainty the interested reader is referred to quilty et al 2022 for additional details on the original formulation of the scdda the difference between the original scdda used in q22 and the one proposed here is discussed in the next sub section 2 3 comparing the new and original scddas the scdda proposed in this study uses the ensemble hm simulations differently than in q22 notably the scdda proposed in this study sums the hm simulations and the ddm outputs used to correct the hm residuals while in q22 the hm simulations were used as input to the ddms fig 1 shows the difference between the original scdda presented in q22 a and the new scdda proposed in this study b in fig 1 a p and t are inputs to the hms with parameter vector θ h m producing streamflow simulation y the ddm uses the ensemble mean of the hm simulations p and t and their time lagged versions up to time t d where d is the maximum time lag along with ddm parameters θ d d m to produce the deterministic streamflow simulation optionally the ddms can also use time lagged versions t 1 t d of observed streamflow q as additional inputs for the stochastic framework given a new set of inputs the distribution of the ddm model inputs f x where the uncertainty is due solely to the ensemble hm simulations as the other inputs remain fixed at their observed values model parameters f θ d d m and errors f e θ d d m x are stochastically sampled to estimate f q q in fig 1 b the hm residuals q y are used as the target variable for the ddm and its associated simulation r is summed with the hm simulation to produce a simulation from the cdda for a given ensemble member y for the new scdda proposed here the ensemble of hms and ddms are simulated via stochastic resampling using the distribution of model parameters f θ h m f θ d d m θ h m and errors f e θ h m θ d d m to estimate the streamflow distribution f q q in this setup the ddm parameters are conditioned by the hm parameters while the error distribution f e θ h m θ d d m is related to the cdda simulation y it is also possible to formulate the scdda such that the hm and ddm parameters are jointly estimated see section 5 by contrasting panels a and b in fig 1 it can be seen that the new scdda directly incorporates the cdda b while the scdda from q22 a only adopts the ensemble hm simulations as input data and does not generate simulations by summing the outputs of the hms and ddms an important benefit of having the cdda built in to the scdda is that users can preserve both the hms and ddms generated by the cdda and use the stochastic framework to estimate the uncertainty of the cdda as the new scdda uses stochastic resampling as the second stage in the post processing framework see section 1 it is possible to revert to the cdda if the scdda as determined by the cpp is not expected to improve model performance a significant difference between the scdda in q22 and the new version proposed here is how hm parameter uncertainty is accounted for in the stochastic framework the scdda in q22 accounts for hm parameter uncertainty by using the ensemble hm simulations to represent input data uncertainty in the ddm specifically for a new set of model inputs a single parameter vector θ h m from the ensemble of hm parameter vectors θ h m 1 θ h m m is randomly sampled the hm associated with this parameter vector is used to generate a simulation and concatenated to the other hydro meteorological variables and used as input to the ddm in the new scdda proposed here ddm parameter uncertainty is conditioned on the hm parameters in detail for a randomly selected hm parameter vector a ddm parameter vector conditioned on the randomly selected hm parameter vector is chosen at random from θ ddm 1 θ ddm n where n is the total number of ddm parameter sets for each hm ensemble member since multiple ddms are trained to simulate the residuals associated with each hm parameter vector see section 3 4 thus the model s in equation 3 is a ddm in the original scdda in q22 and the cdda in the new scdda proposed here in the case study explored herein the input data uncertainty is not considered in the scdda since information on the input variable uncertainty was not available ivs uncertainty was considered in two ddms rf and xgb since these methods inherently perform ivs when model parameters are calibrated thus the model parameter uncertainty also includes ivs uncertainty in rf and xgb however ddms that do not inherently perform ivs as part of the parameter calibration stage require ivs uncertainty to be estimated through explicit methods e g via the bootstrap if this source of uncertainty is to be considered see for example quilty and adamowski 2020 however the lstm implemented in this study did not account for ivs uncertainty it was assumed that ivs would not significantly impact the model performance since the lstm model structure inherently accounts for the relationship between the target variable and previous time lags of the explanatory variables which was previously shown to be important to consider when simulating streamflow in the study catchments sikorska senoner and quilty 2021 2 4 hydrological models previous work on the cdda and scdda focused on a single hm structure hbv light seibert and vis 2012 to better assess the impact of the model structure on the performance of the cdda and scdda three different hm structures were considered in this study the conceptual tuwmodel parajka et al 2007 viglione and parajka 2020 was adopted as the model is formulated based on the hbv model structure similarities and differences between the tuwmodel and hbv light are discussed in section 2 4 2 furthermore to assess the performance of cdda and scdda when using a model with lower structural complexity and to benchmark the performance of such a model against higher complexity hms hbv light and tuwmodel the modèle du génie rural à 4 paramètres journalier gr4j perrin et al 2003 was also adopted both tuwmodel and gr4j were calibrated using bayesian optimization with gaussian processes bo snoek et al 2012 a popular algorithm for tuning ddm hyper parameters shahriari et al 2016 snoek et al 2012 that was recently used for calibrating process based hms ma et al 2021a as the bo algorithm is known for finding suitable parameter sets at low computational costs it is expected that the tuwmodel will not provide the same level of performance as achieved by hbv light in ssq21 which used a higher number of calibration iterations however by utilizing bo to find suitable but not necessarily optimal model parameters bo may provide the opportunity for the cdda and scdda to correct under calibrated hms leading to reliable simulations the following three sub sections briefly summarize the hms used in this study 2 4 1 hbv light an hbv variant hbv light seibert and vis 2012 was adopted as a conceptual lumped catchment rainfall runoff model that simulates the catchment response to hydro meteorological input data through four routines precipitation and snowmelt soil moisture groundwater and routing with p t mean and long term averaged p e t as inputs to hbv light the model consists of 15 tunable parameters the hbv light simulations used in this study are from ssq21 where the model parameters were calibrated using the genetic algorithm and powell method gap seibert 2000 and the kling gupta efficiency kge gupta 2009 as the objective function for more information on hbv light and the calibration procedure see sikorska senoner et al 2020 2 4 2 tuwmodel tuwmodel another hbv based model is a lumped catchment rainfall runoff model that consists of three major routines precipitation and snowmelt soil moisture and routing and 15 tunable parameters while most of the processes are similar the main differences between tuwmodel and hbv light are summarized below 1 for the snow routine tuwmodel uses a threshold temperature interval to distinguish rain snow or a mixture of both while hbv light uses a single temperature threshold where meltwater and rainfall are contained in the snow until it exceeds a certain threshold with a refreezing component 2 p e t is required as a user defined input to tuwmodel and is calculated by hbv light 3 the triangular transfer function used for routing includes an additional free scaling parameter along with p e t p and t mean are also used as inputs to tuwmodel the parameters in tuwmodel were calibrated using bo with additional details described in section 3 2 the mathematical background on tuwmodel can be found in parajka et al 2007 2 4 3 gr4j gr4j is a lumped rainfall runoff model with four tunable parameters where p and p e t are used as input to simulate streamflow perrin et al 2003 due to the model s parsimony robustness computation speed and simplicity gr4j has become popular in the hydrology domain and has been shown to provide competitive performance when benchmarked against other hms darbandsari and coulibaly 2020 gaborit et al 2017 kunnath poovakka and eldho 2019 oudin et al 2008 perrin et al 2003 wijayarathne and coulibaly 2020 the four parameters in gr4j that require calibration include the maximum capacity of the production store mm the catchment water exchange coefficient mm d the maximum capacity of the routing store mm and the time base of the unit hydrograph d gr4j is also coupled with the cema neige snow routine valery 2010 in section 4 5 resulting in gr4jcn to explore the impact of the snow routine on model performance in this case gr4jcn has two additional parameters ponderation coefficient dimensionless and degree day factor mm c d that require calibration as gr4jcn is only adopted in a single experiment and follows the same model development procedure as gr4j gr4j is primarily referred to throughout the text the gr4j parameters were calibrated using bo according to the details provided in section 3 2 and the p e t estimates from hbv light were used as used as model input the mathematical formulation of gr4j can be found in perrin et al 2003 2 5 data driven models based on the recommendations in ssq21 xgb and rf were considered in the cdda and scdda furthermore due to its increasing popularity in the hydrological modelling literature lstm was also adopted within the cdda and scdda brief descriptions of the three ddms are outlined in this section 2 5 1 extreme gradient boosting xgb a recent tree based ensemble learning method extreme gradient boosting chen and guestrin 2016 was shown to accurately simulate hm residuals related to streamflow in ssq21 in contrast to rf where predictions are made by bagging an ensemble of trees the trees in xgb are combined sequentially by scaling each tree according to a learning rate also known as boosting similar to the gradient boosting method hastie et al 2009 in short xgb is an efficient ensemble learning method that results in a parsimonious structure through regularization and inherently measures input variable importance while providing competitive performance compared to existing tree based methods chen and guestrin 2016 recent applications of xgb in the hydrology and water resources domains include flash flood risk assessment ma et al 2021b prediction of dew point temperature dong et al 2022 detecting leakage in urban water distribution networks wu et al 2021 water quality prediction wang et al 2022 and modelling lake bathymetry liu and song 2022 2 5 2 random forests rf based on the family of decision tree models rf was first introduced by breiman 2001 and has gained widespread popularity due to its high performance flexibility amenability to perform quantile regression and ability to measure each input variables importance among other useful qualities rf generates a bootstrapped dataset by randomly selecting samples from the training data with replacement builds decision trees using a random subset of input variables for each root and node of the tree repeats these steps and generates a final set of predictions by taking the mean of the outputs from all decision trees by using the bootstrapped dataset and aggregating predictions across multiple trees also known as bagging breiman 1996 the diversity of decision trees created by rf assists in the bias variance trade off von luxburg and schölkopf 2011 in addition to streamflow simulation and forecasting papacharalampous and tyralis 2018 schoppa et al 2020 rf has been used for numerous hydro meteorological applications including the classification of severity of mid winter ice breakups de coste et al 2022 estimating regional groundwater fluoride concentrations rosecrans et al 2022 downscaling spatial resolution of soil moisture satellite products triantakonstantis et al 2022 prediction of the seasonal freeze thaw cycle zhong et al 2022 spatial interpolation of climate surfaces precipitation and air temperature tan et al 2021 regional flood frequency analysis desai and ouarda 2021 for a detailed exploration of rf within water resources see tyralis et al 2019b as noted above rf intrinsically measures input variable importance this useful feature of rf was formulated into a new ivs method guided regularized random forests by deng and runger 2013 and was shown to select a lower number of input variables that provide similar or better performance than the original input variable set when used in rf in q22 guided regularized random forests were used for ivs in the rf based scdda the same is done for both cdda and scdda in this study for the input variable importance score the residual sum of squares is calculated for each split to measure the total decrease in node impurities from splitting on the input variable which is then averaged across all trees the input variable importance score is then normalized afterwards an importance weight and a regularization coefficient are used to calculate a penalty weight vector for all input variables the penalty weight vector is used to guide the ivs procedure within rf for more information on guided regularized random forests see deng and runger 2013 and quilty et al 2022 2 5 3 long short term memory network lstm along with various deep learning neural networks applied to model time series the recurrent neural network rnn has been used for embedding sequential memory time based correlation in the network architecture the main limitation of rnn is that it is incapable of learning long term dependencies due to vanishing or exploding gradients when training models with backpropagation hochreiter and schmidhuber 1997 which is undesirable for simulating streamflow that exhibits long term dependence hence a modified configuration of the rnn the lstm overcomes this weakness of the rnn with the capability of storing long term information through cell states hochreiter and schmidhuber 1997 compared to the rnn the lstm includes a cell state that stores long term information and multiple gates i e the forget gate input gate output gate controlling the flow of information within the network the forget gate controls the flow of information from the cell state to the forget gate the input gate controls what new information can be updated in the cell state and the output gate controls the information that passes from the cell state to the next hidden state the output of the lstm is connected through a single neuron dense layer that simulates the target variable for a detailed description of the lstm in the context of a large scale rainfall runoff modelling case study see kratzert et al 2018 the lstm has become increasing popular relative to other ddms in hydrology in the last three years due to its ability to process large datasets inherently capture long term dependencies and accurately predict hydrological variables chen et al 2020 fan et al 2020 gauch et al 2021 kratzert et al 2018 rahimzad et al 2021 in particular several recent studies have focused on combining hms with lstms for example frame et al 2021 konapala et al 2020b and lu et al 2021 used hms as inputs to the lstm where the lstm was able to improve the simulation quality of the standalone hms furthermore similar to the approach adopted here lstms have also been used to simulate the residuals of the hm outputs for instance cho and kim 2022 han 2021 and sharma et al 2021 used lstm to correct the hm outputs improving upon the streamflow simulations of the standalone hm however to date no studies have used lstm within a stochastic framework to correct the outputs of multiple hms 3 experiment setup 3 1 study area this study uses the same three swiss mountainous catchments as presented in ssq21 and also adopted in q22 all catchments have an insignificant contribution from glaciers and human impacts during the time period used in this case study calendar year of 1981 2014 the dünnern catchment 234 km2 is driven by rainfall while both kleine emme 478 km2 and muota 317 km2 are driven by a mixture of rainfall and snowmelt as dominant hydrological processes as the goal of this study is to simulate the streamflow at the outlet of the catchment all models were developed using catchment averaged variables with the thiessens polygon method including precipitation mm d minimum maximum and mean air temperature c and potential evapotranspiration mm d at daily timesteps measurements of all variables including the streamflow at the catchment outlet mm d were available during 1981 2014 and sourced from the swiss federal office for the environment dataset partitioning followed the same periods used in q22 where 1981 1984 was used as warm up for the hms 1985 2004 as calibration 2005 2009 as validation and 2010 2014 for testing apart from the warm up period both hms and ddms followed the same dataset partitioning i e the ddms did not require a warm up period 3 2 hm setup of the three hms adopted in this study hbv light used the same model setup as presented in ssq21 where the gap method was used to calibrate the model with kge as the objective function the parameter ranges considered during the calibration of the individual hbv light models are listed in sikorska senoner et al 2020 for gr4j and tuwmodel bo is used to calibrate the model parameters in detail using bo each ensemble member was optimized using 100 iterations including 15 initial evaluations with kge as the objective function although the cdda in ssq21 used 1000 ensemble members their analysis showed that for the studied catchments approximately 100 members led to performance that did not significantly differ from the 1000 member ensembles as measured by the mean continuous ranked probability score crps since the number of ensemble members significantly impacts the computation time 200 members were generated for all three hms and catchments to ensure a stable crps was achieved for each hm across all catchments for hbv light the first 200 members of the original 1000 member ensemble were selected from ssq21 the data were partitioned according to section 3 1 both tuwmodel and gr4j used the p e t estimates from hbv light see section 2 4 1 2 4 2 and 2 4 3 for the input requirements for hbv light tuwmodel and gr4j respectively tuwmodel and gr4j were implemented using tuwmodel viglione and parajka 2020 and airgr coron et al 2017 2022 r packages respectively the parameter ranges considered by bo for tuwmodel and gr4j models are listed in appendix a 3 3 ddm setup 3 3 1 xgb and rf setup since ddms do not consider explicit state variables as in hms the warm up period 1981 1984 was not used as part of the ddm training instead the same calibration 1985 2004 and validation periods 2005 2009 adopted for the hm were used to train and validate the ddms the ddms used the residual of the hm simulations as the target variable and considered time lagged versions of p t min t max t mean p e t and q as input variables with the maximum time lag d for each input variable determined by the conditional mutual information brown et al 2012 to minimize the number of input variables while ensuring a sufficient number of previous time lags the ddms considered the current and previous nine time lagged versions of p t min t max t mean and p e t as well as the previous nine time lagged versions of q using the same maximum time lag d 9 as in ssq21 although the maximum time lag can be considered as a hyper parameter to be optimized by bo considering a single maximum time lag significantly reduces the computation time and is followed in this study the same xgb and rf hyper parameters and their ranges as described in ssq21 and q22 respectively are also used here thus for brevity the bo setup for xgb and rf is not discussed in detail due to the memory size requirements and computational cost for each hm ensemble member the bo configuration for rf and xgb considered 16 iterations in addition to four initial evaluations and 25 iterations in addition to 5 initial evaluations respectively the number of iterations and initial evaluations differed for rf and xgb as they had different memory size requirements thus rf and xgb had 20 and 30 potentially different parameter sets for each of the 200 hm parameter set s respectively the xgb and rf models were implemented using the xgboost chen et al 2021 and rrf deng 2013 r packages respectively 3 3 2 lstm setup in addition to the maximum time lag considered for each input variable p t min t max t mean and p e t the lstm requires a time window sequence length hyper parameter preliminary experimentation showed that a 90 day sequence length provided suitable performance while keeping the computation time reasonable the remaining hyper parameters that required careful tuning were optimized using bo and are listed below along with the ranges considered during optimization which are values similar to those adopted in alizadeh et al 2021 where integers are denoted by l learning rate 1e 4 1e 1 number of hidden units 16l 128l dropout rate 0 2 0 5 the lstm used the adam optimizer kingma and ba 2015 for training via backpropagation kge as the loss function and the same number of iterations 25 and initial evaluations five for bo as xgb however unlike xgb and rf the target and explanatory input variables were normalized using each variable s mean and standard deviation from the training set to enable faster convergence during model training the cdda requires training multiple ddms for each hm ensemble member thus to keep model training time reasonable while enabling high quality simulations the batch size and number of training epochs were set to 256 and 5 respectively the batch size was set according to the upper limit used in alizadeh et al 2021 while the number of training epochs was determined based on trial and error it is important to note that the goal of optimizing the individual lstm models within the cdda and scdda was not to achieve the best possible performance for every single model but to generate a set of models that provided high performance with diverse simulations thus the performance of individual lstm models may be further improved using other settings lstm was implemented using tensorflow abadi et al 2015 and keras chollet 2015 with scikit optimize for bo head et al 2021 3 4 stochastic simulation similar to q22 the stochastic simulation framework used in the new scdda is formulated using two modes an offline mode where pdfs e g parameter model error are estimated and an online mode where the distribution of streamflow is estimated using a new set of inputs and the previously estimated pdfs as mentioned in section 2 3 input data uncertainty was not considered in the scdda presented here since uncertainty in the input variables was not available thus all input variables remain fixed at their observed values during stochastic simulation the parameter uncertainty is represented by the hm parameter sets obtained by the gap hbv light or bo tuwmodel or gr4j methods and the ddm parameter sets obtained by bo that are attached to each hm parameter set during the online mode parameter uncertainty is estimated by first sampling uniformly at random from the hm parameter sets and then by sampling uniformly at random from the ddm parameter set associated with the earlier sampled hm parameter set i e each of the 200 hm parameter sets is considered at each step of the stochastic simulation and determine condition the ddm parameter sets that are sampled from thereafter see section 3 3 1 and 3 3 2 for the number of parameter sets associated with the ddms using the randomly sampled hm and ddm parameters the streamflow is simulated on the validation set by combining hm and ddm i e the cdda outputs the cdda simulation for the validation set is compared against the observed streamflow also for the validation set to generate a set of residuals which are used to estimate the conditional pdf of the cdda model error the cdda represented by the randomly sampled hm and ddm parameter sets is simulated for the new set of inputs and the simulation is used to conditionally sample a model error from the conditional pdf of the cdda model error since the optimization of the cdda considers multiple ddms for each hm the new scdda exploits the different ddm parameter sets associated with each hm parameter set to give a more realistic assessment of simulation uncertainty i e the probability distribution of the true value of the variable to be simulated opposite of the cdda that considers only a single ddm parameter set to implement the new scdda in an operational setting the online mode of the stochastic simulation is carried out using the following steps 1 for new input data at time t p t t mean t and if applicable p e t t an hm parameter vector θ h m i is sampled uniformly at random from θ h m 1 θ h m m and used alongside the new input data to generate an hm simulation 2 a ddm parameter vector conditioned on the randomly selected hm parameter vector from step 1 θ d d m j θ h m i is sampled uniformly at random from the parameter set θ d d m 1 θ d d m n θ h m i and is used alongside p t t mean t and if applicable p e t t previously observed flow q t 1 and their time lagged values e g p t 1 p t d t min t 1 t min t d t max t 1 t max t d t mean t 1 t mean t d q t 2 q t d to generate a ddm simulation i e a simulation of the hm residual from step 1 3 hm and ddm simulations are summed together to retrieve a streamflow simulation from the cdda for the new inputs 4 an error is sampled from the conditional pdf of the cdda model error f e θ d d m j θ h m i and added to the cdda simulation for the new inputs 5 steps 1 4 are repeated multiple times here 200 times to generate an estimate of f q similar to q22 the conditional pdf of the cdda model error is estimated using the k nearest neighbours knn resampling approach sikorska et al 2014 where the cdda simulation for new input data i e from the test set is used to conditionally sample model errors from the same cdda s validation set the same method can retrieve the stochastic hm by disregarding the ddm component and resampling the model error from the hm as described in the original blueprint paper montanari and koutsoyiannis 2012 while the knn approach was adopted here other approaches could be used to estimate the conditional pdf of the model error such as those described in papacharalampous et al 2020a 2020b or tyralis et al 2019a finally if the modeller prefers the ddms can use other available explanatory variables and or disregard using previously observed streamflow time lags as input 3 5 performance assessment deterministic and probabilistic metrics are used to assess the performance of the different approaches hm cdda and scdda on the test set to measure out of sample performance the deterministic metrics are calculated using the mean of the ensemble or stochastic simulations while the probabilistic metrics consider all ensemble members or stochastic simulations the performance of the ensemble and stochastic hms hbv light tuwmodel and gr4j serve as a benchmark for the cdda and scdda respectively the scdda is also compared against the cdda to better understand the added value of adopting the stochastic framework an intercomparison of hms is considered where hbv light is used as the benchmark as it was used in ssq21 and q22 tuwmodel is compared against hbv light to explore the potential of bo to find suitable model parameters given that both hms have the same number of parameters and a similar model structure gr4j is compared against hbv light and tuwmodel to explore whether similar performance can be achieved with a simplified model structure the intent is similar in section 4 5 where gr4j is compared against gr4jcn although the purpose of the comparison is to decipher whether cdda and scdda can inherently account for snow processes absent from gr4j the cdda and scdda variants based on the different ddms xgb rf and lstm are compared against one another and the ensemble and stochastic hms to ascertain whether there are any hm ddm combinations that consistently perform better than the others the performance assessment uses several deterministic metrics including the mean absolute error mae root mean squared error rmse nash sutcliffe efficiency nse kge and percent bias pbias althoff and rodrigues 2021 in addition the following probabilistic metrics are considered average width aw papacharalampous et al 2020a crps gneiting and raftery 2007 and the alpha index α r renard et al 2010 since these performance metrics are commonly used in the hydrology and water resources literature the above mentioned sources should be referred to for further details the α r a measure of an ensemble or probabilistic simulation s reliability is estimated from the cpp see montanari and koutsoyiannis 2012 by measuring the area between the cpp and the bisector the cpp has been given different names in the literature for example the predictive quantile quantile plot eslamian 2014 and the predictive probability probability plot koutsoyiannis and montanari 2022 the cpp provides a visual assessment of the ensemble or probabilistic simulations characterizing the simulations profile e g narrow sharp simulation large over dispersed simulation over prediction under prediction and helping diagnose issues with the simulations spread and bias hence simulations generated by the ensemble hms stochastic hms the cdda and the scdda are evaluated via the cpp to visually assess the simulations performance complementing the other deterministic and probabilistic metrics given that cpps can characterize the profile of ensemble and probabilistic simulations the cpps for the three different hms and the nine different cdda and scdda variants are analyzed for the three study catchments see section 4 3 the validation set cpps for the ensemble models hm and cdda were compared against the test set cpps of their stochastic counterparts stochastic hm or scdda to determine whether the cpps could be used as a diagnostic tool to predict whether the stochastic framework can be used improve upon the reliability of the ensemble models one of the main limitations of the scdda is the computational time needed for training the ddms and running the online mode of the stochastic simulation computation time can be reduced by selecting models with lower complexity and or selecting fewer ensemble members although this may reduce model performance thus the probabilistic metrics are computed for various ensemble sizes to depict the trade off between model performance and ensemble size in addition to the probabilistic metrics mentioned above the decomposed crps hersbach 2000 is also considered to further assess the impact of ensemble size on reliability and sharpness as the crps jointly considers reliability and sharpness in a single metric the deterministic metrics were computed using the hydrogof r package zambrano bigiarini 2020 while the crps was estimated using the verification r package ncar research applications laboratory 2015 the remaining metrics were calculated using custom r scripts 4 results and discussion this section investigates the new scdda comparing it against the ensemble and stochastic hms and the cdda unless otherwise stated the various metrics and plots in the section are for the test set it should be noted that all metrics are reported to two decimal places except pbias which is rounded to the nearest decimal place as returned by the hydrogof package zambrano bigiarini 2020 the mae rmse aw and crps have units of mm d while nse kge and α r are unitless the pbias is reported as a percentage 4 1 assessment of ensemble hms and cdda variants table 1 summarizes the deterministic performance of the ensemble hms using the mean of the ensemble streamflow simulations for dünnern kleine emme and muota catchments in table 1 hbv light shows superior deterministic performance over gr4j and tuwmodel with lower mae and rmse and higher nse and kge scores although gr4j and tuwmodel have similar nse as hbv light in dünnern and kleine emme catchments they show unsatisfactory performance in the muota catchment with nse values below 0 5 moriasi et al 2007 as well as mae and rmse that are 86 135 higher the discrepancy in the deterministic model performance between the two hbv variants is likely caused by the different calibration procedures since the models have a similar model structure regarding pbias tuwmodel significantly underperforms compared to the other models while gr4j and hbv light show similar performance despite the poor model performance in the muota catchment in dünnern and kleine emme catchments gr4j calibrated using bo shows deterministic performance that is competitive with hbv light for most metrics the probabilistic metrics of the three ensemble hms are given in table 2 considering the sharpness of the three hms aw in table 2 it is evident that hbv light consistently results in the lowest aw for each catchment despite sharp simulations being desirable they may not be useful if deemed unreliable since the goal is to have sharp and reliable simulations sharper simulations may only be justified if they maintain an acceptable level of reliability here the α r for hbv light indicates lower reliability than gr4j for all catchments however hbv light should not be disregarded due to its lower reliability since the crps which simultaneously accounts for sharpness and reliability shows that hbv light outperforms tuwmodel and gr4j across all catchments comparing the two ensemble hms calibrated using bo gr4j provides sharper simulations lower aw while having similar or higher reliability than tuwmodel of note all three hms did not overfit the training data and in some cases had a better out of sample performance training set results are not shown for brevity the ensemble hm results from tables 1 and 2 make it possible to identify some potential strengths and weaknesses of using bo to calibrate hms it is important to recognize that the goal of bo is to find reasonable parameters with few model evaluations calibration runs since they are designed for computationally expensive problems for example deep learning models shahriari et al 2016 indeed with only a small fraction of model evaluations compared to the hbv light calibration procedure bo identified parameters that lead to high and moderate performance for gr4j and tuwmodel respectively for dünnern and kleine emme basins however bo did not lead to a satisfactory deterministic or probabilistic performance in the muota basin next the deterministic performance of the cdda variants is summarized in table 3 from table 3 the results show that the three cdda variants xgb rf and lstm provide similar deterministic performance for each hm except for pbias the lstm cdda variant results in the best pbias in all cases with the most substantial difference shown in the muota basin comparing the cdda performance across the hms all variants provide strong deterministic performance in each basin for example the cdda variants achieve kge above 0 8 for the dünnern 0 81 0 86 kleine emme 0 86 0 89 and muota 0 82 0 91 basins it was anticipated that given the superior deterministic performance of hbv light as shown in table 1 the corresponding cdda variants would also provide the highest performance however the results demonstrate that poorly performing hms can be substantially improved through the cdda leading to competitive models furthermore in several cases the tuwmodel cdda variants result in much better pbias scores than that of the hbv light cdda variants despite the tuwmodel having the highest pbias amongst the hms table 1 to assess the generalization performance of the ddms used within the cdda the deterministic metrics for the training validation and test sets were evaluated this analysis showed that xgb and rf overfit the training data with training set kge of 0 99 being common yet both xgb and rf provided test set kge of 0 85 however lstm exhibited the most stable performance with a kge of 0 85 across the three sets training validation and test the probabilistic performance of the cdda variants is summarized in table 4 considering the sharpness of the simulations aw in table 4 the rf cdda and lstm cdda produce similar aw scores while xgb cdda provides simulations with a higher spread notably the aw of xgb cdda is nearly double that of rf cdda and lstm cdda considering gr4j in the muota catchment in terms of reliability the xgb cdda and lstm cdda result in the highest α r across the three catchments while rf cdda frequently results in lower reliability than the former approaches despite the similar levels of sharpness for rf cdda and lstm cdda the higher reliability of lstm cdda indicates that lstm should be preferred instead of rf when simulating streamflow residuals from the hms in the study catchments when considering the crps the best performing cdda variant for each hm provides similar crps across each basin thus it can be seen that rf cdda frequently underperforms compared to its xgb and lstm counterparts xgb cdda tends to have higher reliability than the rf and lstm cdda variants but this comes at the cost of a higher aw summarizing results from tables 1 4 the cdda improves the ensemble hm performance for example for the three catchments the cdda improves the rmse by 7 20 20 57 and 23 55 for hbv light tuwmodel and gr4j respectively however it should be noted that there are several instances where the cdda variants have higher pbias than the ensemble hms the exception being the tuwmodel and most lstm variants for instance the best performing hbv light cdda for the dünnern basin increase pbias from 1 4 to 4 for the probabilistic performance the xgb cdda maintains or increases the ensemble hms aw while the rf and lstm cdda tend to decrease the aw thus increasing the sharpness in most cases the reliability of the hms improves through the cdda however a decrease in reliability is found when using rf cdda with gr4j and hbv light for the kleine emme basin finally the cdda variants improve the ensemble hms crps by 18 45 40 70 and 34 69 for hbv light tuwmodel and gr4j respectively 4 2 assessment of the stochastic hms and scdda variants the deterministic and probabilistic performance of the stochastic approaches stochastic hm and scdda are presented in table 5 and table 6 respectively the results from table 5 show that the scdda variants provide superior deterministic performance for most metrics compared to the stochastic hms analyzing each scdda for a given hm reveals that the performance of the mean of the stochastic simulations has low variance across the models similar to the cdda variants the lstm scdda has lower pbias compared to the other variants the probabilistic scores table 6 reveal that the stochastic hms have higher aw than the scdda variants but similar reliability interestingly there seems to be no discernible pattern as to which ddm leads to the highest reliability and lowest crps amongst scdda variants however in all cases the scdda variants result in superior crps compared to the stochastic hms thus given that the scdda variants have similar reliability but sharper simulations and lower crps than their stochastic hm counterparts the scdda variants should be preferred to the stochastic hm for the study catchments comparing the stochastic methods with the ensemble hm from tables 1 and 2 the scdda variants substantially improve most performance metrics for example for the three catchments the scdda variants improve the rmse by 5 20 22 53 and 21 54 considering hbv light tuwmodel and gr4j respectively furthermore the scdda improves the crps by 27 49 41 66 and 39 68 considering the same hms it is important to note that unlike the cdda the lstm scdda improves the pbias of the ensemble hms for all cases thus it can be said that lstm scdda dominated the ensemble hm across all performance metrics comparing the cdda variants tables 3 and 4 with the corresponding scdda variants tables 5 and 6 interesting outcomes are found first not all scdda variants significantly improve the mae rmse nse or kge of the corresponding cdda although most scdda variants tend to have similar deterministic scores as their cdda counterparts some scdda variants have poorer performance for example considering tuwmodel for the muota catchment the kge of xgb cdda and xgb scdda was 0 91 and 0 78 respectively representing a relative difference of 17 however this could be due to deficiencies with the tuwmodel rather than the stochastic framework as there was a higher relative difference 28 in kge for the ensemble hm 0 51 see table 1 versus the stochastic hm 0 40 see table 5 than the cdda versus scdda for the same hm and catchment the aw of the scdda is higher compared to the cdda suggesting that the scdda is more conservative than the cdda in terms of reliability the scdda has higher lower reliability than its cdda counterparts that have α r 0 85 α r 0 85 for most cases this interesting finding explored further in section 4 3 suggests that there may be a level of reliability beyond which cdda cannot be further improved by the stochastic framework considering both sharpness and reliability the scdda variants that improved the reliability of their cdda counterparts tend to have similar or improved crps to visualize the streamflow simulations generated by the cdda and scdda as well as the ensemble and stochastic hms figs 2 4 include time series plots of the different models simulations the mean simulation is included for each model along with the 95 confidence ensemble hm and cdda or prediction stochastic hm and scdda intervals the difference between confidence and prediction intervals as adopted in this case study is discussed in section 2 6 of ssq21 the time series plots in figs 2 4 illustrate several items worth mentioning the aw scores reported above indicate that the cdda variants result in sharp simulations which is confirmed in figs 2 4 however some of the cdda variants have extremely sharp simulations for example the 95 confidence intervals of the hbv light rf and lstm cdda variants are concentrated at the mean of the simulated streamflow for all catchments thus the 95 confidence interval is unable to capture high streamflow events however the cdda variants with sharp simulations appear to improve upon the low and mid flow simulations of the corresponding ensemble hm considering the stochastic approaches the scdda seems to compensate for the cdda and provide conservative simulations by increasing their spread for instance many high flow events missed by the ensemble hm and cdda are captured by the scdda though not all observations are covered within the 95 prediction interval as is expected the scdda is also able to noticeably reduce the bias of the ensemble hm in particular the scdda seems to improve the bias of the ensemble hm simulations for gr4j and tuwmodel in the muota catchment fig 4 finally figs 2 4 reveal that in general the scdda variants have sharp simulations for low flows and wide simulations for high flows this result is likely due to the knn algorithm used for estimating the conditional pdf of the model error which inherently accounts for the heteroscedasticity of the model error sikorska et al 2014 analyzing the distribution of the streamflow simulations for specific events such as floods may extract additional characteristics such as modality to better understand the ensemble and probabilistic simulations here raincloud plots allen et al 2019 were used to enhance the visualization of the simulations distribution which combines a boxplot a jittered scatter plot and a probability density plot the boxplot provides the summary statistics e g the median the jittered scatter plot shows the raw data which can be used to identify outliers and the density plot shows the distribution to check the spread and the modes of the simulated streamflow in figs 5 7 an example high flow event was extracted from the test set to visualize the distribution of the streamflow simulations generated by the different models figs 5 7 show that hbv light provides sharp simulations where no ensemble members capture the high streamflow event furthermore hbv light and its corresponding cdda generally result in symmetric distributions however the modality of the streamflow simulations distribution can change substantially by incorporating multiple uncertainty sources e g model output via the stochastic framework for example the ensemble simulations generated by hbv light in kleine emme and tuwmodel in muota are approximately symmetric about their median while their scdda counterparts produce simulations with multiple modes that often contain the observed value in general the scdda tends to capture the high streamflow events more so than its ensemble hm and cdda counterparts although there are many cases where the scdda captures the observed streamflow and in many cases has a simulation distribution that shifts upwards surrounding higher flows the distribution widens assigning non negligible probability to lower streamflow ranges for example the lstm scdda using tuwmodel in the kleine emme catchment has a wide simulation distribution which better captures the observed value although the mode of its distribution is much lower compared to its ensemble hm counterpart in the following sub section the cpp is used to assess the reliability of the ensemble and stochastic models where it is shown how the cpp can be used as a diagnostic tool to predict whether the stochastic framework i e the scdda can further improve the cdda 4 3 utilizing cpps as a diagnostic tool the validation and test set cpps for the ensemble and stochastic hms as well as the cdda and scdda variants are given in figs 8 10 for the three catchments utilizing the shape of the cpps it is possible to classify the streamflow simulation profile first when analyzing the ensemble hm results for all basins gr4j and hbv light provide sharp simulations which confirms earlier results see section 4 1 and 4 2 for tuwmodel the cpps show that the ensemble hm has a large bias for example the dünnern and kleine emme basins for tuwmodel distinctly result in over prediction furthermore the validation set cpps of the ensemble hms and cdda variants are very similar to the test set cpps thus the validation set cpps can be used to predict the reliability of the test for the study catchments and the hms and hm ddm combinations explored herein when analyzing the cpps a pattern can be seen with respect to the ensemble hms and the cdda variants an important observation is that all hbv light and most gr4j simulations for the ensemble hm have sharp simulations and their cdda counterparts result in the same profile suggesting that hms with sharp simulations tend to result in cdda counterparts with sharp simulations however it is difficult to find a common pattern with over and under predictions notably tuwmodel has cpps that show over prediction however the corresponding cdda variants produce cpps with different shapes next looking at the validation set cpps for the cdda tuwmodel in fig 8 gr4j in fig 9 and tuwmodel in fig 10 the cdda variants that lie close to the bisector have corresponding scdda variants that are less reliable this result is a visual representation of the previous section where the cdda variants with α r 0 85 had scdda counterparts that were less reliable one explanation is that for a highly reliable cdda the error approaches white noise is purely random thus the knn based stochastic resampling of model errors adds noise to the streamflow simulation therefore for a cdda that already has high reliability the stochastic resampling scheme results in scdda variants with lower reliability however the more important discovery is that the cdda s cpps from the validation set can be used to predict the reliability of the scdda on the test set in general the cdda and scdda have similar or higher reliability than their ensemble or stochastic hm counterparts another important finding is that the majority of the scdda variants produce cpps that are either close to the bisector line or indicate large simulations since reliable and conservative large simulations are of high importance in water resources applications e g flood forecasting the scdda can be a useful framework for hydrologists as well as water scientists and practitioners further considering the practicality of the above mentioned results suppose computational resources are limited then using the cpps as a diagnostic tool modellers and or users can decide to implement the stochastic resampling scheme depending on the cdda s validation set cpp thus in a practical setting if the cpp shows that the cdda is sufficiently reliable e g α r 0 85 the user may benefit from lower computation time when generating simulations and or forecasts in real time since the stochastic resampling scheme may be abandoned 4 4 effect of ensemble size on model performance since the computational demand of ddms and or stochastic resampling may deter some users from using the cdda and or scdda it is critical to assess the possibility of reducing the computational requirements while still providing sufficient performance one way to achieve this is to evaluate the effect of ensemble size the number of ensemble members on model performance to find a lower number of ensemble members that characterize the simulation uncertainty to a similar degree as ensembles with more members the probabilistic performance metrics from tables 2 4 and 6 were chosen to evaluate the effect of ensemble size on model performance although the aw and α r can indicate changes in the sharpness and reliability respectively of an ensemble or probabilistic simulation for increasing or decreasing ensemble size it is much more challenging to understand the overall improvement in model performance using these metrics compared to a single metric that simultaneously considers sharpness and reliability such as the crps of practical value to the following analysis it is possible to decompose the crps into reliability and sharpness components hersbach 2000 known as the reliability crps and potential crps where the latter is defined as the crps that would be achieved if the simulation was perfectly reliable and sensitive to the average spread of the ensemble simulation for more information on crps decomposition see hersbach 2000 in what follows the aw α r crps and the reliability and potential components of the crps are evaluated for various ensemble member sizes in fig 11 the aw is plotted as a function of ensemble size for the ensemble and stochastic hm cdda and scdda from fig 11 it appears that most ensemble hms and cdda variants have similar aw after 100 ensemble members one noticeable difference between the hms is that models calibrated by bo seem to be less stable with fluctuating aw for increasing ensemble member sizes this outcome is likely caused by bo searching diverse parameter spaces resulting in highly variable parameter sets whereas the gap method due to the high number of calibration runs seems to converge to similar parameter sets with low variance similar levels of instability in the aw are also found for the cdda variants adopting hms calibrated by bo regardless of the adopted ddm for the stochastic hm and scdda all models appear to have similar curves as the ensemble size increases notably beyond 25 ensemble members the aw seems to have stabilized however an inflection point occurs around 40 ensemble members causing a substantial change in aw that eventually stabilizes although the cause of this inflection point is unknown the change in aw from 100 to 200 members is minor next the effect of ensemble size on the α r is shown in fig 12 regarding the reliability of the ensemble hm and cdda fig 12 it appears that the α r for most models is relatively stable beyond 50 members however for the stochastic models all models seem to rapidly increase in reliability from one to approximately 15 members then slowly stabilize around 100 members for some models e g tuwmodel xgb scdda it appears that the reliability of the model asymptotically decreases from 15 to 100 ensemble members the instability of the α r for a low number of ensemble members may be due to a poor estimation of the simulation uncertainty which could be explained by the stabilization of the α r around 100 ensemble members finally the crps and its decomposed metrics potential crps and reliability crps for varying ensemble sizes are shown in figs 13 15 for the three catchments it appears that most models stabilize around 100 ensemble members according to the potential crps of the ensemble hm cdda and the stochastic models in all catchments however the potential crps of some models may continue to improve beyond 200 members for example the stochastic hm using gr4j in dünnern and kleine emme catchments although the improvements may not be significant comparing the results of the potential crps with the aw from fig 11 it appears that the increase in the aw with an increasing number of ensemble members does not meaningfully impact the potential crps beyond 100 members next analyzing the reliability crps shows that for most models it stabilizes around 100 ensemble members similar to the α r furthermore the shape of the α r curves in fig 12 closely match the reliability crps curves for the stochastic hm and scdda although the potential crps and reliability crps of some models in figs 13 15 do not appear to stabilize by 200 members the overall crps suggests that most models provide a stable estimation of simulation uncertainty by 100 members showing little improvement in performance beyond this point therefore the analysis of ensemble size versus model performance indicates that approximately 100 members are required for a stable estimation of simulation uncertainty in the study catchments it is important to recognize that the crps decomposition also estimates how much the sharpness and reliability of the simulations contribute to the overall crps in detail for most ensemble models ensemble hm and cdda the reliability crps contributes the most to the overall crps in contrast the potential crps contributes the most to the overall crps for the stochastic models stochastic hm and scdda with a minimal contribution coming from the reliability crps comparing the overall crps of cdda and scdda variants it was previously determined that the scdda could improve upon or maintain the crps of its unreliable cdda counterpart thus it appears that the stochastic framework tends to convert the ensemble models ensemble hm and cdda into more reliable models at the cost of simulation sharpness this result enhances the previous conclusion that the scdda tends to generate simulations that are more conservative while improving the reliability of its less reliable cdda counterpart 4 5 effect on model performance using a snow module in gr4j among the three hms gr4j does not incorporate a snow routine although snow processes play a significant role in the hydrology of two study catchments kleine emme and muota not including snow processes in an hm can be viewed as an error of the perceptual model i e the perceptions of how the catchment responds to meteorological forcings where deciding on the important hydrological processes is the first step of modelling and iteratively refined until the model is deemed to be satisfactory beven 2012 however since the cdda and scdda are used to correct the residuals of the ensemble hm they may also implicitly account for processes not included in the hm e g overcoming the need to couple a snow module with gr4j in what follows gr4j is used to test whether similar performance could be achieved by the cdda and scdda when an important hydrological process is and is not explicitly accounted for in the model therefore the cemaneige snow module valery 2010 is coupled with gr4j gr4jcn and compared against gr4j without the snow module using the cdda and scdda variants with the same calibration procedure as gr4j see section 2 4 3 and 3 2 the crps for gr4jcn in dünnern kleine emme and muota catchments is estimated as 0 47 0 68 and 2 22 representing approximately a 4 12 and 6 improvement over gr4j respectively on the test set since kleine emme showed the most substantial improvement in performance when adopting cemaneige in gr4j this catchment was chosen to evaluate any differences in performance kge α r and crps between the gr4j and gr4jcn cdda and scdda variants in table 7 the test set performance for the ensemble and stochastic hms using gr4jcn as well as gr4jcn cdda and scdda variants is summarized for the kleine emme catchment from table 7 it is possible to identify that the gr4j and gr4jcn cdda and scdda variants provide similar results considering all three ddms the difference in crps for gr4j and gr4jcn cdda and scdda variants is 0 02 or less similarly the α r differs by 0 03 or less while the deterministic kge is the same except for the stochastic hm therefore it appears that the cdda and scdda have the potential to correct for hydrological processes absent from the hm this result is aligned with lees et al 2022 where it was shown that the lstm is able to reproduce hydrological processes from data historical streamflow meteorological variables and catchment attributes thus it appears that xgb and rf also have this ability however to generalize this finding a much larger experiment with additional catchments hm parameter optimization methods and more flexible hms where model structure complexity can be closely controlled e g raven craig et al 2020 should be considered 5 conclusion and future work combining process based hydrological models hms with data driven models ddms has led to substantial improvements in the simulation of hydrological variables e g streamflow various approaches for combining hms and ddms have been introduced in the literature including the ensemble based conceptual data driven approach cdda which showed that several nonlinear ddms could be used to correct the residuals of an ensemble of hms and improve upon the ensemble hm streamflow simulations sikorska senoner and quilty 2021 due to the importance of uncertainty estimation in real world decision making its companion paper quilty et al 2022 developed the stochastic cdda scdda to account for various sources of uncertainty e g parameter model output using the ensemble hm simulations as input to ddms this follow up study proposes and explores a new scdda that estimates cdda uncertainty rather than ddm uncertainty in the original scdda the new scdda is tested using nine hm ddm combinations three hms and three ddms and compared against its cdda counterparts as well as ensemble and stochastic hms for daily streamflow simulation in three swiss catchments the different models are evaluated using several deterministic and probabilistic metrics as well as graphical aids time series plots raincloud plots etc the coverage probability plot cpp is proposed as a diagnostic tool for predicting when the out of sample reliability of the ensemble models ensemble hms and cdda can be improved by the stochastic framework using validation set data ensemble simulations and observations experiments showed that the scdda could significantly improve the ensemble hm simulations across most performance metrics with improvements in the mean continuous ranked probability score crps of 27 68 while the scdda improved upon the crps of the cdda by up to 15 it did not always outperform the cdda across all metrics the cpps showed that ensemble hms with sharp simulations tended to result in cdda variants with sharp simulations and unreliable ensemble models were improved using the stochastic framework meanwhile cdda variants with high reliability alpha index above 0 85 had scdda counterparts with lower reliability regardless all scdda variants had reliable or conservative simulations highlighting its potential as a valuable tool for decision making studying probabilistic performance as a function of ensemble size number of ensemble members revealed that an ensemble size of 100 members led to stable performance in one of the snow dominated catchments kleine emme not including a snow module in a simple hm led to lower performance than when it was included however neglecting the snow module had no discernible impact on the deterministic performance and negligible impact on the probabilistic performance of the cdda and scdda indicating that both approaches have the potential to account for missing processes in hms given the flexibility of the scdda it is possible to identify several potential improvements for the framework as the ddms are not restricted to the input variables used in this study performance may be improved using other variables generated by the hm such as actual evapotranspiration furthermore other hm structures and optimization algorithms may enhance the scdda simulations for example future work may involve using a blended model structure mai and craig 2020 with simultaneous calibration of structure and parameters chlumsky et al 2021 within scdda and testing its efficacy in diverse catchments newman et al 2014 in future studies bo could also be used alongside other optimization algorithms for jointly estimating hm and ddm parameters by optimizing hm parameters and ddm hyper parameters simultaneously if successful this approach may significantly reduce the computation time required for the proposed scdda data availability the observed streamflow and meteorological data can be ordered from the swiss federal office for the environment foen https www bafu admin ch last access december 11 2020 and meteoswiss http www meteoswiss ch last access december 11 2020 respectively the latest version of hbv light can be downloaded at https www geo uzh ch en units h2k services hbv model html funding the research was financially supported by natural sciences and engineering research council discovery grant launch supplement jq university of waterloo department of civil and environmental engineering graduate student support allowance jq queen elizabeth ii graduate scholarship in science and technology dh and university of waterloo president s graduate scholarship dh this research and decision to publish its findings was not influenced by the institutions that provided the above mentioned financial support declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors sincerely thank meteoswiss for providing the meteorological data and the foen for the streamflow data from project no 15 0054 pj o503 1381 the hbv light model parameters were calibrated using sciencecloud provided by s3it at the university of zurich appendix a hm parameter ranges table a1 parameter ranges for tuwmodel used in bo retrieved from the tuwmodel r package viglione and parajka 2020 table a1 parameter explanation minimum maximum units scf snow correction factor 0 9 1 5 ddf degree day factor 0 5 mm c d tr threshold temperature for rain above 1 3 c ts threshold temperature for snow below 3 1 c tm threshold temperature for melt above 2 2 c lprat parameter for potential evaporation 0 1 fc field capacity 0 600 mm beta parameter for runoff production 0 20 k0 storage coefficient for very fast response 0 2 d k1 storage coefficient for fast response 2 30 d k2 storage coefficient for slow response 30 250 d lsuz threshold storage state 1 100 mm cperc constant percolation rate 0 8 mm d bmax maximum base at low flows 0 30 d croute free scaling parameter 0 50 d2 mm table a2 parameter ranges for gr4jcn used in bo retrieved from the airgr r package coron et al 2022 note gr4j used the same parameter ranges as gr4jcn but did not consider x5 and x6 table a2 parameter explanation minimum maximum units x1 production store maximal capacity 0 9 1 5 mm c d x2 catchment water exchange coefficient 0 5 mm d x3 one day maximal capacity of routing reservoir 1 3 mm x4 unit hydrograph time base 3 1 d x5 ponderation coefficient 0 1 x6 degree day factor 2 6 mm c d 
25515,recently the conceptual data driven approach cdda was proposed to correct residuals of ensemble hydrological models hms using data driven models ddms followed by the stochastic cdda scdda that used hm simulations as input to ddms within a stochastic framework both approaches improved ensemble hms simulations here a new scdda is introduced where cdda uncertainty is estimated instead of ddm uncertainty in the original scdda using nine hm ddm combinations for daily streamflow simulation in three swiss catchments the new scdda improved cdda s mean continuous ranked probability score up to 15 and performed similarly without a snow routine in a snowy catchment suggesting that scdda may account for missing processes in hms the stochastic framework can convert unreliable ensemble models into more reliable stochastic models at the cost of simulation sharpness the coverage probability plot is proposed as a diagnostic tool predicting scdda s out of sample reliability using validation set data cdda simulations and observations keywords ensemble stochastic streamflow simulation data driven model hydrological model uncertainty 1 introduction hydrologists and water resources practitioners are commonly tasked with estimating the uncertainty of hydrological variables for various applications such as flood forecasting and water management benefitting from process based physical or conceptual hydrological models hms anand et al 2018 jain et al 2018 although statistical methods have been used in the hydrology domain for decades singh 2018 the accelerated advancements in technology and computer science especially data driven models ddms has given a new perspective to the hydrological modelling paradigm by producing accurate models that have similar or better predictive capability than hms beven 2020 nearing et al 2021 to contrast the two modelling approaches hms attempt to simplify and mimic natural processes using mathematical expressions relevant to the hydrological process under study i e explicit descriptions of the perceived catchment response to rainfall runoff using boundary conditions initial conditions mass balance energy balance etc beven 2012 while ddms estimate statistical relationships between explanatory input e g rainfall and response target e g streamflow variables using historical measurements enabling simulations of the target variable although research on these two approaches has primarily advanced independently of one another in the hydrological sciences there is an increasing interest in combining process based hms and ddms to increase our understanding of natural systems and improve predictive performance beyond what is achievable by the individual models adombi et al 2021 karpatne et al 2017 in the hydrological sciences research on combining theoretical knowledge adombi et al 2021 with ddms has progressed using various approaches and many of these approaches do not follow a common naming convention which although outside the scope of this study addressing this issue represents a valuable topic of future research for example coupling hms and ddms have been referred to as physics informed ddms liang et al 2019 hybrid models kurian et al 2020 or simply post processing frame et al 2021 as another example the term physics informed neural networks has been used to represent an approach where partial differential equations are incorporated within artificial neural networks raissi et al 2019 shen and lawson 2021 furthermore several recent studies have used hm outputs as input to ddms demonstrating that such an approach can be used to improve predictive performance compared to the standalone hm frame et al 2021 ghaith et al 2019 konapala et al 2020a kumanlioglu and fistikoglu 2019 lu et al 2021 quilty et al 2022 another combined hm ddm approach gaining popularity is correcting hm simulations by using ddms to simulate the hm residuals cho and kim 2022 li et al 2021a 2021b papacharalampous et al 2020a 2020b sharma et al 2021 shen et al 2022 sikorska senoner and quilty 2021 incorporating model errors in hydrological simulations has traditionally been used for uncertainty estimation for example stochastic resampling from the hm error distribution for assessing predictive uncertainty in streamflow simulations montanari and koutsoyiannis 2012 sikorska et al 2014 as such in a companion study sikorska senoner and quilty 2021 hereinafter referred to as ssq21 developed the conceptual data driven approach cdda to include an ensemble of hms each paired with a ddm that simulates the hm residuals the cdda only characterizes uncertainty in the hm parameters while the ddms simulate the expectation of the residuals associated with each hm parameter set a key outcome of ssq21 showed that all of the nonlinear ddms considered in the study improved the streamflow simulations generated by the lumped conceptual hm although the cdda adopted conceptual hms any deterministic e g process based hm can be used within the cdda as uncertainty in streamflow simulations plays a significant role in water management decisions the companion paper by quilty et al 2022 hereinafter referred to as q22 modified the cdda by including other sources of uncertainty neglected in the cdda input data input variable selection ivs ddm parameters and model output referred to as the stochastic conceptual data driven approach scdda this framework improved streamflow simulations in all study catchments when compared to the cdda however unlike the cdda which uses ddms to directly simulate the residuals of the ensemble hms the ddms within the scdda directly simulate streamflow using the ensemble hm simulations as ddm inputs in other words the scdda proposed in q22 did not follow the same approach for using ddms as in the cdda proposed in ssq21 in contrast to q22 this study develops a new scdda an analogue of the cdda proposed in ssq21 where ddms are used to simulate hm residuals in a stochastic framework that similar to q22 can account for uncertainty in input data ivs parameters and model output the new scdda takes advantage of the multiple parameter sets generated by the routine used to optimize the ddms hyperparameters in the cdda in the companion studies ssq21 and q22 only one hm structure based on the hydrologiska byråns vattenbalansavdelning hbv model specifically hbv light seibert and vis 2012 was utilized making it challenging to identify whether the performance of the cdda and scdda was specific to the hm s structure therefore along with hbv light two additional hms the technische universität wien model tuwmodel parajka et al 2007 sleziak et al 2020 and modèle du génie rural à 4 paramètres journalier perrin et al 2003 are used in this study to explore the performance of the cdda and scdda with respect to different hm structures based on a recommendation from ssq21 which found that extreme gradient boosting xgb chen and guestrin 2016 and random forests rf breiman 2001 were the most suitable ddms to simulate hm residuals out of the eight ddms that were considered this study considers xgb and rf within the cdda and scdda along with these two ddms the long short term memory network lstm hochreiter and schmidhuber 1997 was also included due to its prominent role in hydrological applications involving deep learning shen 2018 lstm is an extension of recurrent neural networks rnn with the capacity to learn time dependencies over long timescales which is important when simulating streamflow as persistence is commonly manifested in hydrological processes hurst 1951 iliopoulou et al 2018 koutsoyiannis 2021 pagano and garen 2005 the competitive predictive performance of lstm has been well recognized in hydrology and water resources and is becoming the model of choice for deep learning applications in these domains see for example feng et al 2021 2020 kratzert et al 2019 liu et al 2022 shen 2018 thus the research objectives of this study are to 1 apply the cdda and the new scdda for all nine combinations of the hms hbv light tuwmodel gr4j and ddms xgb rf lstm for daily streamflow simulation in three swiss catchments 2 compare the performance of the cdda and scdda against one another as well as the ensemble and stochastic hms considering the different combinations of hms and ddms 3 propose and explore the use of a diagnostic tool to predict if the scdda can improve upon the reliability obtained by the cdda the third objective is not only useful for measuring improvements by converting the cdda to its stochastic counterpart but it also serves a practical purpose suppose the diagnostic tool predicts that the scdda will have lower performance than the cdda in this case the scdda may be neglected saving implementation time and computational costs similar to ssq21 and q22 the same three swiss catchments were used to develop the different models and address the research objectives the main novelty of this research lies in the use of the multiple parameter sets explored by the optimization routine in the cdda the cdda requires a computationally demanding search for optimal ddm hyper parameters settings external to the model that affect the calibration of parameters model structure etc each hm in the ensemble is associated with a single set of ddm hyper parameters and a single set of ddm hyper parameters is used to estimate ddm parameters which are responsible for mapping the ddm inputs to the hm residuals however by retaining all parameter sets explored during optimization the uncertainty of the cdda hm and ddm parameters can be utilized within the stochastic framework leading to the new scdda in addition adopting nine different hm ddm within the cdda and scdda for simulating streamflow in multiple catchments allows for the exploration of a diagnostic tool the coverage probability plot cpp to determine if the reliability of the cdda can be improved upon using the scdda the cpp also referred to as the predictive probability probability plot has been proven to be a valuable tool when converting deterministic simulations to stochastic ones koutsoyiannis and montanari 2022 finally the new scdda can also be viewed as the second stage in a post processing framework where after the cdda is used to correct the hm outputs first stage the stochastic framework is used to refine the cdda and assess its uncertainty through stochastic resampling transforming the cdda into the new scdda the rest of the paper is structured as follows section 2 outlines the background and methods related to the cdda and new and original scdda as well as the adopted hms and ddms section 3 describes the experimental setup associated with the hms ddms and stochastic simulation as well as the performance assessment of the different models section 4 presents the results and includes a discussion on the salient findings and section 5 concludes this study 2 background and methods 2 1 ensemble based conceptual data driven approach cdda the cdda utilizes an ensemble of hms using a single model structure and multiple parameter sets to generate streamflow simulations where a ddm is used to correct the residuals of each hm the cdda adopted in ssq21 and q22 can be written as quilty et al 2022 1 y i p t t d t t t d q t 1 t d θ c d d a i y i p t t t θ h m i r i p t t d t t t d q t 1 t d θ d d m i where y i y i and r i are the cdda hm streamflow and ddm hm residuals simulations for ensemble member i at time t here t is removed to simplify the notation respectively p t t d t t t d and q t 1 t d are observed precipitation and air temperature at time lags t t d and streamflow at time lags t 1 t d where d is the maximum time lag respectively θ c d d a i θ h m i θ d d m i are the parameter sets associated with the i th ensemble member of the cdda hm and ddm respectively with θ c d d a i θ h m i θ d d m i while for a given hm e g hbv light gr4j the input variables remain fixed the ddm can accept various input variables not limited to those in equation 1 which may be useful for improving the cdda simulations the cdda adopted in the study is of the general form 2 y i x h m x d d m θ c d d a i y i x h m θ h m i r i x d d m θ d d m i where x h m and x d d m represent the hm and ddm inputs respectively since this study considers hms with different input variable requirements x h m includes p t as well as t mean t and or p e t t depending on the hm see section 2 4 where t mean is the mean air temperature and p e t is the potential evapotranspiration lindström and bergström 1992 however for all hm ddm combinations x d d m includes p t t d t min t t d t max t t d t mean t t d p e t t t d and q t 1 t d where t min and t max are the minimum and maximum air temperatures respectively the most significant limitation of the cdda is that it only explicitly accounts for the hm parameter uncertainty uncertainty associated with the true value of the estimated parameter s thus the cdda can only estimate confidence intervals rather than prediction intervals when quantifying uncertainty in its simulations since prediction intervals account for the uncertainty associated with the prediction of the true value of a given hydrological variable montanari and koutsoyiannis 2012 in other words as a single ddm is used to correct the residuals of each hm ensemble member one ddm for hm the ddms output within the cdda is related to the expected value of the hm residuals not their distribution 2 2 stochastic conceptual data driven approach scdda the scdda was introduced in q22 to account for additional sources of uncertainty i e input data ivs model output not considered in the cdda the scdda is an extension of the earlier frameworks proposed by montanari and koutsoyiannis 2012 which focused on hms and quilty et al 2019 which focused on ddms where an hm is coupled with a ddm in a stochastic framework the scdda makes use of the original equation proposed in montanari and koutsoyiannis 2012 for converting a deterministic hm into a stochastic one 3 f q q θ x f e q s θ x θ x f θ θ f x x d θ d x where f q represents the probability density function pdf of the target variable e g streamflow to be simulated or forecasted and f x f θ and f e represent the pdf of the input data the pdf of the parameter s and the pdf of the model error conditioned on the parameter s and input data respectively with q x and θ as the target variable i e streamflow input data and parameter s respectively and s as the deterministic model for streamflow in q22 the scdda is obtained by modifying equation 3 to include ensemble hm simulations within x along with other hydro meteorological variables and using a ddm for s rather than an hm along with the original assumptions about the stochastic framework in montanari and koutsoyiannis 2012 it was further assumed in q22 that the ddm parameters could be estimated independently of the hm parameters since the ensemble hm simulations were used as input to the ddms and represented input data uncertainty the interested reader is referred to quilty et al 2022 for additional details on the original formulation of the scdda the difference between the original scdda used in q22 and the one proposed here is discussed in the next sub section 2 3 comparing the new and original scddas the scdda proposed in this study uses the ensemble hm simulations differently than in q22 notably the scdda proposed in this study sums the hm simulations and the ddm outputs used to correct the hm residuals while in q22 the hm simulations were used as input to the ddms fig 1 shows the difference between the original scdda presented in q22 a and the new scdda proposed in this study b in fig 1 a p and t are inputs to the hms with parameter vector θ h m producing streamflow simulation y the ddm uses the ensemble mean of the hm simulations p and t and their time lagged versions up to time t d where d is the maximum time lag along with ddm parameters θ d d m to produce the deterministic streamflow simulation optionally the ddms can also use time lagged versions t 1 t d of observed streamflow q as additional inputs for the stochastic framework given a new set of inputs the distribution of the ddm model inputs f x where the uncertainty is due solely to the ensemble hm simulations as the other inputs remain fixed at their observed values model parameters f θ d d m and errors f e θ d d m x are stochastically sampled to estimate f q q in fig 1 b the hm residuals q y are used as the target variable for the ddm and its associated simulation r is summed with the hm simulation to produce a simulation from the cdda for a given ensemble member y for the new scdda proposed here the ensemble of hms and ddms are simulated via stochastic resampling using the distribution of model parameters f θ h m f θ d d m θ h m and errors f e θ h m θ d d m to estimate the streamflow distribution f q q in this setup the ddm parameters are conditioned by the hm parameters while the error distribution f e θ h m θ d d m is related to the cdda simulation y it is also possible to formulate the scdda such that the hm and ddm parameters are jointly estimated see section 5 by contrasting panels a and b in fig 1 it can be seen that the new scdda directly incorporates the cdda b while the scdda from q22 a only adopts the ensemble hm simulations as input data and does not generate simulations by summing the outputs of the hms and ddms an important benefit of having the cdda built in to the scdda is that users can preserve both the hms and ddms generated by the cdda and use the stochastic framework to estimate the uncertainty of the cdda as the new scdda uses stochastic resampling as the second stage in the post processing framework see section 1 it is possible to revert to the cdda if the scdda as determined by the cpp is not expected to improve model performance a significant difference between the scdda in q22 and the new version proposed here is how hm parameter uncertainty is accounted for in the stochastic framework the scdda in q22 accounts for hm parameter uncertainty by using the ensemble hm simulations to represent input data uncertainty in the ddm specifically for a new set of model inputs a single parameter vector θ h m from the ensemble of hm parameter vectors θ h m 1 θ h m m is randomly sampled the hm associated with this parameter vector is used to generate a simulation and concatenated to the other hydro meteorological variables and used as input to the ddm in the new scdda proposed here ddm parameter uncertainty is conditioned on the hm parameters in detail for a randomly selected hm parameter vector a ddm parameter vector conditioned on the randomly selected hm parameter vector is chosen at random from θ ddm 1 θ ddm n where n is the total number of ddm parameter sets for each hm ensemble member since multiple ddms are trained to simulate the residuals associated with each hm parameter vector see section 3 4 thus the model s in equation 3 is a ddm in the original scdda in q22 and the cdda in the new scdda proposed here in the case study explored herein the input data uncertainty is not considered in the scdda since information on the input variable uncertainty was not available ivs uncertainty was considered in two ddms rf and xgb since these methods inherently perform ivs when model parameters are calibrated thus the model parameter uncertainty also includes ivs uncertainty in rf and xgb however ddms that do not inherently perform ivs as part of the parameter calibration stage require ivs uncertainty to be estimated through explicit methods e g via the bootstrap if this source of uncertainty is to be considered see for example quilty and adamowski 2020 however the lstm implemented in this study did not account for ivs uncertainty it was assumed that ivs would not significantly impact the model performance since the lstm model structure inherently accounts for the relationship between the target variable and previous time lags of the explanatory variables which was previously shown to be important to consider when simulating streamflow in the study catchments sikorska senoner and quilty 2021 2 4 hydrological models previous work on the cdda and scdda focused on a single hm structure hbv light seibert and vis 2012 to better assess the impact of the model structure on the performance of the cdda and scdda three different hm structures were considered in this study the conceptual tuwmodel parajka et al 2007 viglione and parajka 2020 was adopted as the model is formulated based on the hbv model structure similarities and differences between the tuwmodel and hbv light are discussed in section 2 4 2 furthermore to assess the performance of cdda and scdda when using a model with lower structural complexity and to benchmark the performance of such a model against higher complexity hms hbv light and tuwmodel the modèle du génie rural à 4 paramètres journalier gr4j perrin et al 2003 was also adopted both tuwmodel and gr4j were calibrated using bayesian optimization with gaussian processes bo snoek et al 2012 a popular algorithm for tuning ddm hyper parameters shahriari et al 2016 snoek et al 2012 that was recently used for calibrating process based hms ma et al 2021a as the bo algorithm is known for finding suitable parameter sets at low computational costs it is expected that the tuwmodel will not provide the same level of performance as achieved by hbv light in ssq21 which used a higher number of calibration iterations however by utilizing bo to find suitable but not necessarily optimal model parameters bo may provide the opportunity for the cdda and scdda to correct under calibrated hms leading to reliable simulations the following three sub sections briefly summarize the hms used in this study 2 4 1 hbv light an hbv variant hbv light seibert and vis 2012 was adopted as a conceptual lumped catchment rainfall runoff model that simulates the catchment response to hydro meteorological input data through four routines precipitation and snowmelt soil moisture groundwater and routing with p t mean and long term averaged p e t as inputs to hbv light the model consists of 15 tunable parameters the hbv light simulations used in this study are from ssq21 where the model parameters were calibrated using the genetic algorithm and powell method gap seibert 2000 and the kling gupta efficiency kge gupta 2009 as the objective function for more information on hbv light and the calibration procedure see sikorska senoner et al 2020 2 4 2 tuwmodel tuwmodel another hbv based model is a lumped catchment rainfall runoff model that consists of three major routines precipitation and snowmelt soil moisture and routing and 15 tunable parameters while most of the processes are similar the main differences between tuwmodel and hbv light are summarized below 1 for the snow routine tuwmodel uses a threshold temperature interval to distinguish rain snow or a mixture of both while hbv light uses a single temperature threshold where meltwater and rainfall are contained in the snow until it exceeds a certain threshold with a refreezing component 2 p e t is required as a user defined input to tuwmodel and is calculated by hbv light 3 the triangular transfer function used for routing includes an additional free scaling parameter along with p e t p and t mean are also used as inputs to tuwmodel the parameters in tuwmodel were calibrated using bo with additional details described in section 3 2 the mathematical background on tuwmodel can be found in parajka et al 2007 2 4 3 gr4j gr4j is a lumped rainfall runoff model with four tunable parameters where p and p e t are used as input to simulate streamflow perrin et al 2003 due to the model s parsimony robustness computation speed and simplicity gr4j has become popular in the hydrology domain and has been shown to provide competitive performance when benchmarked against other hms darbandsari and coulibaly 2020 gaborit et al 2017 kunnath poovakka and eldho 2019 oudin et al 2008 perrin et al 2003 wijayarathne and coulibaly 2020 the four parameters in gr4j that require calibration include the maximum capacity of the production store mm the catchment water exchange coefficient mm d the maximum capacity of the routing store mm and the time base of the unit hydrograph d gr4j is also coupled with the cema neige snow routine valery 2010 in section 4 5 resulting in gr4jcn to explore the impact of the snow routine on model performance in this case gr4jcn has two additional parameters ponderation coefficient dimensionless and degree day factor mm c d that require calibration as gr4jcn is only adopted in a single experiment and follows the same model development procedure as gr4j gr4j is primarily referred to throughout the text the gr4j parameters were calibrated using bo according to the details provided in section 3 2 and the p e t estimates from hbv light were used as used as model input the mathematical formulation of gr4j can be found in perrin et al 2003 2 5 data driven models based on the recommendations in ssq21 xgb and rf were considered in the cdda and scdda furthermore due to its increasing popularity in the hydrological modelling literature lstm was also adopted within the cdda and scdda brief descriptions of the three ddms are outlined in this section 2 5 1 extreme gradient boosting xgb a recent tree based ensemble learning method extreme gradient boosting chen and guestrin 2016 was shown to accurately simulate hm residuals related to streamflow in ssq21 in contrast to rf where predictions are made by bagging an ensemble of trees the trees in xgb are combined sequentially by scaling each tree according to a learning rate also known as boosting similar to the gradient boosting method hastie et al 2009 in short xgb is an efficient ensemble learning method that results in a parsimonious structure through regularization and inherently measures input variable importance while providing competitive performance compared to existing tree based methods chen and guestrin 2016 recent applications of xgb in the hydrology and water resources domains include flash flood risk assessment ma et al 2021b prediction of dew point temperature dong et al 2022 detecting leakage in urban water distribution networks wu et al 2021 water quality prediction wang et al 2022 and modelling lake bathymetry liu and song 2022 2 5 2 random forests rf based on the family of decision tree models rf was first introduced by breiman 2001 and has gained widespread popularity due to its high performance flexibility amenability to perform quantile regression and ability to measure each input variables importance among other useful qualities rf generates a bootstrapped dataset by randomly selecting samples from the training data with replacement builds decision trees using a random subset of input variables for each root and node of the tree repeats these steps and generates a final set of predictions by taking the mean of the outputs from all decision trees by using the bootstrapped dataset and aggregating predictions across multiple trees also known as bagging breiman 1996 the diversity of decision trees created by rf assists in the bias variance trade off von luxburg and schölkopf 2011 in addition to streamflow simulation and forecasting papacharalampous and tyralis 2018 schoppa et al 2020 rf has been used for numerous hydro meteorological applications including the classification of severity of mid winter ice breakups de coste et al 2022 estimating regional groundwater fluoride concentrations rosecrans et al 2022 downscaling spatial resolution of soil moisture satellite products triantakonstantis et al 2022 prediction of the seasonal freeze thaw cycle zhong et al 2022 spatial interpolation of climate surfaces precipitation and air temperature tan et al 2021 regional flood frequency analysis desai and ouarda 2021 for a detailed exploration of rf within water resources see tyralis et al 2019b as noted above rf intrinsically measures input variable importance this useful feature of rf was formulated into a new ivs method guided regularized random forests by deng and runger 2013 and was shown to select a lower number of input variables that provide similar or better performance than the original input variable set when used in rf in q22 guided regularized random forests were used for ivs in the rf based scdda the same is done for both cdda and scdda in this study for the input variable importance score the residual sum of squares is calculated for each split to measure the total decrease in node impurities from splitting on the input variable which is then averaged across all trees the input variable importance score is then normalized afterwards an importance weight and a regularization coefficient are used to calculate a penalty weight vector for all input variables the penalty weight vector is used to guide the ivs procedure within rf for more information on guided regularized random forests see deng and runger 2013 and quilty et al 2022 2 5 3 long short term memory network lstm along with various deep learning neural networks applied to model time series the recurrent neural network rnn has been used for embedding sequential memory time based correlation in the network architecture the main limitation of rnn is that it is incapable of learning long term dependencies due to vanishing or exploding gradients when training models with backpropagation hochreiter and schmidhuber 1997 which is undesirable for simulating streamflow that exhibits long term dependence hence a modified configuration of the rnn the lstm overcomes this weakness of the rnn with the capability of storing long term information through cell states hochreiter and schmidhuber 1997 compared to the rnn the lstm includes a cell state that stores long term information and multiple gates i e the forget gate input gate output gate controlling the flow of information within the network the forget gate controls the flow of information from the cell state to the forget gate the input gate controls what new information can be updated in the cell state and the output gate controls the information that passes from the cell state to the next hidden state the output of the lstm is connected through a single neuron dense layer that simulates the target variable for a detailed description of the lstm in the context of a large scale rainfall runoff modelling case study see kratzert et al 2018 the lstm has become increasing popular relative to other ddms in hydrology in the last three years due to its ability to process large datasets inherently capture long term dependencies and accurately predict hydrological variables chen et al 2020 fan et al 2020 gauch et al 2021 kratzert et al 2018 rahimzad et al 2021 in particular several recent studies have focused on combining hms with lstms for example frame et al 2021 konapala et al 2020b and lu et al 2021 used hms as inputs to the lstm where the lstm was able to improve the simulation quality of the standalone hms furthermore similar to the approach adopted here lstms have also been used to simulate the residuals of the hm outputs for instance cho and kim 2022 han 2021 and sharma et al 2021 used lstm to correct the hm outputs improving upon the streamflow simulations of the standalone hm however to date no studies have used lstm within a stochastic framework to correct the outputs of multiple hms 3 experiment setup 3 1 study area this study uses the same three swiss mountainous catchments as presented in ssq21 and also adopted in q22 all catchments have an insignificant contribution from glaciers and human impacts during the time period used in this case study calendar year of 1981 2014 the dünnern catchment 234 km2 is driven by rainfall while both kleine emme 478 km2 and muota 317 km2 are driven by a mixture of rainfall and snowmelt as dominant hydrological processes as the goal of this study is to simulate the streamflow at the outlet of the catchment all models were developed using catchment averaged variables with the thiessens polygon method including precipitation mm d minimum maximum and mean air temperature c and potential evapotranspiration mm d at daily timesteps measurements of all variables including the streamflow at the catchment outlet mm d were available during 1981 2014 and sourced from the swiss federal office for the environment dataset partitioning followed the same periods used in q22 where 1981 1984 was used as warm up for the hms 1985 2004 as calibration 2005 2009 as validation and 2010 2014 for testing apart from the warm up period both hms and ddms followed the same dataset partitioning i e the ddms did not require a warm up period 3 2 hm setup of the three hms adopted in this study hbv light used the same model setup as presented in ssq21 where the gap method was used to calibrate the model with kge as the objective function the parameter ranges considered during the calibration of the individual hbv light models are listed in sikorska senoner et al 2020 for gr4j and tuwmodel bo is used to calibrate the model parameters in detail using bo each ensemble member was optimized using 100 iterations including 15 initial evaluations with kge as the objective function although the cdda in ssq21 used 1000 ensemble members their analysis showed that for the studied catchments approximately 100 members led to performance that did not significantly differ from the 1000 member ensembles as measured by the mean continuous ranked probability score crps since the number of ensemble members significantly impacts the computation time 200 members were generated for all three hms and catchments to ensure a stable crps was achieved for each hm across all catchments for hbv light the first 200 members of the original 1000 member ensemble were selected from ssq21 the data were partitioned according to section 3 1 both tuwmodel and gr4j used the p e t estimates from hbv light see section 2 4 1 2 4 2 and 2 4 3 for the input requirements for hbv light tuwmodel and gr4j respectively tuwmodel and gr4j were implemented using tuwmodel viglione and parajka 2020 and airgr coron et al 2017 2022 r packages respectively the parameter ranges considered by bo for tuwmodel and gr4j models are listed in appendix a 3 3 ddm setup 3 3 1 xgb and rf setup since ddms do not consider explicit state variables as in hms the warm up period 1981 1984 was not used as part of the ddm training instead the same calibration 1985 2004 and validation periods 2005 2009 adopted for the hm were used to train and validate the ddms the ddms used the residual of the hm simulations as the target variable and considered time lagged versions of p t min t max t mean p e t and q as input variables with the maximum time lag d for each input variable determined by the conditional mutual information brown et al 2012 to minimize the number of input variables while ensuring a sufficient number of previous time lags the ddms considered the current and previous nine time lagged versions of p t min t max t mean and p e t as well as the previous nine time lagged versions of q using the same maximum time lag d 9 as in ssq21 although the maximum time lag can be considered as a hyper parameter to be optimized by bo considering a single maximum time lag significantly reduces the computation time and is followed in this study the same xgb and rf hyper parameters and their ranges as described in ssq21 and q22 respectively are also used here thus for brevity the bo setup for xgb and rf is not discussed in detail due to the memory size requirements and computational cost for each hm ensemble member the bo configuration for rf and xgb considered 16 iterations in addition to four initial evaluations and 25 iterations in addition to 5 initial evaluations respectively the number of iterations and initial evaluations differed for rf and xgb as they had different memory size requirements thus rf and xgb had 20 and 30 potentially different parameter sets for each of the 200 hm parameter set s respectively the xgb and rf models were implemented using the xgboost chen et al 2021 and rrf deng 2013 r packages respectively 3 3 2 lstm setup in addition to the maximum time lag considered for each input variable p t min t max t mean and p e t the lstm requires a time window sequence length hyper parameter preliminary experimentation showed that a 90 day sequence length provided suitable performance while keeping the computation time reasonable the remaining hyper parameters that required careful tuning were optimized using bo and are listed below along with the ranges considered during optimization which are values similar to those adopted in alizadeh et al 2021 where integers are denoted by l learning rate 1e 4 1e 1 number of hidden units 16l 128l dropout rate 0 2 0 5 the lstm used the adam optimizer kingma and ba 2015 for training via backpropagation kge as the loss function and the same number of iterations 25 and initial evaluations five for bo as xgb however unlike xgb and rf the target and explanatory input variables were normalized using each variable s mean and standard deviation from the training set to enable faster convergence during model training the cdda requires training multiple ddms for each hm ensemble member thus to keep model training time reasonable while enabling high quality simulations the batch size and number of training epochs were set to 256 and 5 respectively the batch size was set according to the upper limit used in alizadeh et al 2021 while the number of training epochs was determined based on trial and error it is important to note that the goal of optimizing the individual lstm models within the cdda and scdda was not to achieve the best possible performance for every single model but to generate a set of models that provided high performance with diverse simulations thus the performance of individual lstm models may be further improved using other settings lstm was implemented using tensorflow abadi et al 2015 and keras chollet 2015 with scikit optimize for bo head et al 2021 3 4 stochastic simulation similar to q22 the stochastic simulation framework used in the new scdda is formulated using two modes an offline mode where pdfs e g parameter model error are estimated and an online mode where the distribution of streamflow is estimated using a new set of inputs and the previously estimated pdfs as mentioned in section 2 3 input data uncertainty was not considered in the scdda presented here since uncertainty in the input variables was not available thus all input variables remain fixed at their observed values during stochastic simulation the parameter uncertainty is represented by the hm parameter sets obtained by the gap hbv light or bo tuwmodel or gr4j methods and the ddm parameter sets obtained by bo that are attached to each hm parameter set during the online mode parameter uncertainty is estimated by first sampling uniformly at random from the hm parameter sets and then by sampling uniformly at random from the ddm parameter set associated with the earlier sampled hm parameter set i e each of the 200 hm parameter sets is considered at each step of the stochastic simulation and determine condition the ddm parameter sets that are sampled from thereafter see section 3 3 1 and 3 3 2 for the number of parameter sets associated with the ddms using the randomly sampled hm and ddm parameters the streamflow is simulated on the validation set by combining hm and ddm i e the cdda outputs the cdda simulation for the validation set is compared against the observed streamflow also for the validation set to generate a set of residuals which are used to estimate the conditional pdf of the cdda model error the cdda represented by the randomly sampled hm and ddm parameter sets is simulated for the new set of inputs and the simulation is used to conditionally sample a model error from the conditional pdf of the cdda model error since the optimization of the cdda considers multiple ddms for each hm the new scdda exploits the different ddm parameter sets associated with each hm parameter set to give a more realistic assessment of simulation uncertainty i e the probability distribution of the true value of the variable to be simulated opposite of the cdda that considers only a single ddm parameter set to implement the new scdda in an operational setting the online mode of the stochastic simulation is carried out using the following steps 1 for new input data at time t p t t mean t and if applicable p e t t an hm parameter vector θ h m i is sampled uniformly at random from θ h m 1 θ h m m and used alongside the new input data to generate an hm simulation 2 a ddm parameter vector conditioned on the randomly selected hm parameter vector from step 1 θ d d m j θ h m i is sampled uniformly at random from the parameter set θ d d m 1 θ d d m n θ h m i and is used alongside p t t mean t and if applicable p e t t previously observed flow q t 1 and their time lagged values e g p t 1 p t d t min t 1 t min t d t max t 1 t max t d t mean t 1 t mean t d q t 2 q t d to generate a ddm simulation i e a simulation of the hm residual from step 1 3 hm and ddm simulations are summed together to retrieve a streamflow simulation from the cdda for the new inputs 4 an error is sampled from the conditional pdf of the cdda model error f e θ d d m j θ h m i and added to the cdda simulation for the new inputs 5 steps 1 4 are repeated multiple times here 200 times to generate an estimate of f q similar to q22 the conditional pdf of the cdda model error is estimated using the k nearest neighbours knn resampling approach sikorska et al 2014 where the cdda simulation for new input data i e from the test set is used to conditionally sample model errors from the same cdda s validation set the same method can retrieve the stochastic hm by disregarding the ddm component and resampling the model error from the hm as described in the original blueprint paper montanari and koutsoyiannis 2012 while the knn approach was adopted here other approaches could be used to estimate the conditional pdf of the model error such as those described in papacharalampous et al 2020a 2020b or tyralis et al 2019a finally if the modeller prefers the ddms can use other available explanatory variables and or disregard using previously observed streamflow time lags as input 3 5 performance assessment deterministic and probabilistic metrics are used to assess the performance of the different approaches hm cdda and scdda on the test set to measure out of sample performance the deterministic metrics are calculated using the mean of the ensemble or stochastic simulations while the probabilistic metrics consider all ensemble members or stochastic simulations the performance of the ensemble and stochastic hms hbv light tuwmodel and gr4j serve as a benchmark for the cdda and scdda respectively the scdda is also compared against the cdda to better understand the added value of adopting the stochastic framework an intercomparison of hms is considered where hbv light is used as the benchmark as it was used in ssq21 and q22 tuwmodel is compared against hbv light to explore the potential of bo to find suitable model parameters given that both hms have the same number of parameters and a similar model structure gr4j is compared against hbv light and tuwmodel to explore whether similar performance can be achieved with a simplified model structure the intent is similar in section 4 5 where gr4j is compared against gr4jcn although the purpose of the comparison is to decipher whether cdda and scdda can inherently account for snow processes absent from gr4j the cdda and scdda variants based on the different ddms xgb rf and lstm are compared against one another and the ensemble and stochastic hms to ascertain whether there are any hm ddm combinations that consistently perform better than the others the performance assessment uses several deterministic metrics including the mean absolute error mae root mean squared error rmse nash sutcliffe efficiency nse kge and percent bias pbias althoff and rodrigues 2021 in addition the following probabilistic metrics are considered average width aw papacharalampous et al 2020a crps gneiting and raftery 2007 and the alpha index α r renard et al 2010 since these performance metrics are commonly used in the hydrology and water resources literature the above mentioned sources should be referred to for further details the α r a measure of an ensemble or probabilistic simulation s reliability is estimated from the cpp see montanari and koutsoyiannis 2012 by measuring the area between the cpp and the bisector the cpp has been given different names in the literature for example the predictive quantile quantile plot eslamian 2014 and the predictive probability probability plot koutsoyiannis and montanari 2022 the cpp provides a visual assessment of the ensemble or probabilistic simulations characterizing the simulations profile e g narrow sharp simulation large over dispersed simulation over prediction under prediction and helping diagnose issues with the simulations spread and bias hence simulations generated by the ensemble hms stochastic hms the cdda and the scdda are evaluated via the cpp to visually assess the simulations performance complementing the other deterministic and probabilistic metrics given that cpps can characterize the profile of ensemble and probabilistic simulations the cpps for the three different hms and the nine different cdda and scdda variants are analyzed for the three study catchments see section 4 3 the validation set cpps for the ensemble models hm and cdda were compared against the test set cpps of their stochastic counterparts stochastic hm or scdda to determine whether the cpps could be used as a diagnostic tool to predict whether the stochastic framework can be used improve upon the reliability of the ensemble models one of the main limitations of the scdda is the computational time needed for training the ddms and running the online mode of the stochastic simulation computation time can be reduced by selecting models with lower complexity and or selecting fewer ensemble members although this may reduce model performance thus the probabilistic metrics are computed for various ensemble sizes to depict the trade off between model performance and ensemble size in addition to the probabilistic metrics mentioned above the decomposed crps hersbach 2000 is also considered to further assess the impact of ensemble size on reliability and sharpness as the crps jointly considers reliability and sharpness in a single metric the deterministic metrics were computed using the hydrogof r package zambrano bigiarini 2020 while the crps was estimated using the verification r package ncar research applications laboratory 2015 the remaining metrics were calculated using custom r scripts 4 results and discussion this section investigates the new scdda comparing it against the ensemble and stochastic hms and the cdda unless otherwise stated the various metrics and plots in the section are for the test set it should be noted that all metrics are reported to two decimal places except pbias which is rounded to the nearest decimal place as returned by the hydrogof package zambrano bigiarini 2020 the mae rmse aw and crps have units of mm d while nse kge and α r are unitless the pbias is reported as a percentage 4 1 assessment of ensemble hms and cdda variants table 1 summarizes the deterministic performance of the ensemble hms using the mean of the ensemble streamflow simulations for dünnern kleine emme and muota catchments in table 1 hbv light shows superior deterministic performance over gr4j and tuwmodel with lower mae and rmse and higher nse and kge scores although gr4j and tuwmodel have similar nse as hbv light in dünnern and kleine emme catchments they show unsatisfactory performance in the muota catchment with nse values below 0 5 moriasi et al 2007 as well as mae and rmse that are 86 135 higher the discrepancy in the deterministic model performance between the two hbv variants is likely caused by the different calibration procedures since the models have a similar model structure regarding pbias tuwmodel significantly underperforms compared to the other models while gr4j and hbv light show similar performance despite the poor model performance in the muota catchment in dünnern and kleine emme catchments gr4j calibrated using bo shows deterministic performance that is competitive with hbv light for most metrics the probabilistic metrics of the three ensemble hms are given in table 2 considering the sharpness of the three hms aw in table 2 it is evident that hbv light consistently results in the lowest aw for each catchment despite sharp simulations being desirable they may not be useful if deemed unreliable since the goal is to have sharp and reliable simulations sharper simulations may only be justified if they maintain an acceptable level of reliability here the α r for hbv light indicates lower reliability than gr4j for all catchments however hbv light should not be disregarded due to its lower reliability since the crps which simultaneously accounts for sharpness and reliability shows that hbv light outperforms tuwmodel and gr4j across all catchments comparing the two ensemble hms calibrated using bo gr4j provides sharper simulations lower aw while having similar or higher reliability than tuwmodel of note all three hms did not overfit the training data and in some cases had a better out of sample performance training set results are not shown for brevity the ensemble hm results from tables 1 and 2 make it possible to identify some potential strengths and weaknesses of using bo to calibrate hms it is important to recognize that the goal of bo is to find reasonable parameters with few model evaluations calibration runs since they are designed for computationally expensive problems for example deep learning models shahriari et al 2016 indeed with only a small fraction of model evaluations compared to the hbv light calibration procedure bo identified parameters that lead to high and moderate performance for gr4j and tuwmodel respectively for dünnern and kleine emme basins however bo did not lead to a satisfactory deterministic or probabilistic performance in the muota basin next the deterministic performance of the cdda variants is summarized in table 3 from table 3 the results show that the three cdda variants xgb rf and lstm provide similar deterministic performance for each hm except for pbias the lstm cdda variant results in the best pbias in all cases with the most substantial difference shown in the muota basin comparing the cdda performance across the hms all variants provide strong deterministic performance in each basin for example the cdda variants achieve kge above 0 8 for the dünnern 0 81 0 86 kleine emme 0 86 0 89 and muota 0 82 0 91 basins it was anticipated that given the superior deterministic performance of hbv light as shown in table 1 the corresponding cdda variants would also provide the highest performance however the results demonstrate that poorly performing hms can be substantially improved through the cdda leading to competitive models furthermore in several cases the tuwmodel cdda variants result in much better pbias scores than that of the hbv light cdda variants despite the tuwmodel having the highest pbias amongst the hms table 1 to assess the generalization performance of the ddms used within the cdda the deterministic metrics for the training validation and test sets were evaluated this analysis showed that xgb and rf overfit the training data with training set kge of 0 99 being common yet both xgb and rf provided test set kge of 0 85 however lstm exhibited the most stable performance with a kge of 0 85 across the three sets training validation and test the probabilistic performance of the cdda variants is summarized in table 4 considering the sharpness of the simulations aw in table 4 the rf cdda and lstm cdda produce similar aw scores while xgb cdda provides simulations with a higher spread notably the aw of xgb cdda is nearly double that of rf cdda and lstm cdda considering gr4j in the muota catchment in terms of reliability the xgb cdda and lstm cdda result in the highest α r across the three catchments while rf cdda frequently results in lower reliability than the former approaches despite the similar levels of sharpness for rf cdda and lstm cdda the higher reliability of lstm cdda indicates that lstm should be preferred instead of rf when simulating streamflow residuals from the hms in the study catchments when considering the crps the best performing cdda variant for each hm provides similar crps across each basin thus it can be seen that rf cdda frequently underperforms compared to its xgb and lstm counterparts xgb cdda tends to have higher reliability than the rf and lstm cdda variants but this comes at the cost of a higher aw summarizing results from tables 1 4 the cdda improves the ensemble hm performance for example for the three catchments the cdda improves the rmse by 7 20 20 57 and 23 55 for hbv light tuwmodel and gr4j respectively however it should be noted that there are several instances where the cdda variants have higher pbias than the ensemble hms the exception being the tuwmodel and most lstm variants for instance the best performing hbv light cdda for the dünnern basin increase pbias from 1 4 to 4 for the probabilistic performance the xgb cdda maintains or increases the ensemble hms aw while the rf and lstm cdda tend to decrease the aw thus increasing the sharpness in most cases the reliability of the hms improves through the cdda however a decrease in reliability is found when using rf cdda with gr4j and hbv light for the kleine emme basin finally the cdda variants improve the ensemble hms crps by 18 45 40 70 and 34 69 for hbv light tuwmodel and gr4j respectively 4 2 assessment of the stochastic hms and scdda variants the deterministic and probabilistic performance of the stochastic approaches stochastic hm and scdda are presented in table 5 and table 6 respectively the results from table 5 show that the scdda variants provide superior deterministic performance for most metrics compared to the stochastic hms analyzing each scdda for a given hm reveals that the performance of the mean of the stochastic simulations has low variance across the models similar to the cdda variants the lstm scdda has lower pbias compared to the other variants the probabilistic scores table 6 reveal that the stochastic hms have higher aw than the scdda variants but similar reliability interestingly there seems to be no discernible pattern as to which ddm leads to the highest reliability and lowest crps amongst scdda variants however in all cases the scdda variants result in superior crps compared to the stochastic hms thus given that the scdda variants have similar reliability but sharper simulations and lower crps than their stochastic hm counterparts the scdda variants should be preferred to the stochastic hm for the study catchments comparing the stochastic methods with the ensemble hm from tables 1 and 2 the scdda variants substantially improve most performance metrics for example for the three catchments the scdda variants improve the rmse by 5 20 22 53 and 21 54 considering hbv light tuwmodel and gr4j respectively furthermore the scdda improves the crps by 27 49 41 66 and 39 68 considering the same hms it is important to note that unlike the cdda the lstm scdda improves the pbias of the ensemble hms for all cases thus it can be said that lstm scdda dominated the ensemble hm across all performance metrics comparing the cdda variants tables 3 and 4 with the corresponding scdda variants tables 5 and 6 interesting outcomes are found first not all scdda variants significantly improve the mae rmse nse or kge of the corresponding cdda although most scdda variants tend to have similar deterministic scores as their cdda counterparts some scdda variants have poorer performance for example considering tuwmodel for the muota catchment the kge of xgb cdda and xgb scdda was 0 91 and 0 78 respectively representing a relative difference of 17 however this could be due to deficiencies with the tuwmodel rather than the stochastic framework as there was a higher relative difference 28 in kge for the ensemble hm 0 51 see table 1 versus the stochastic hm 0 40 see table 5 than the cdda versus scdda for the same hm and catchment the aw of the scdda is higher compared to the cdda suggesting that the scdda is more conservative than the cdda in terms of reliability the scdda has higher lower reliability than its cdda counterparts that have α r 0 85 α r 0 85 for most cases this interesting finding explored further in section 4 3 suggests that there may be a level of reliability beyond which cdda cannot be further improved by the stochastic framework considering both sharpness and reliability the scdda variants that improved the reliability of their cdda counterparts tend to have similar or improved crps to visualize the streamflow simulations generated by the cdda and scdda as well as the ensemble and stochastic hms figs 2 4 include time series plots of the different models simulations the mean simulation is included for each model along with the 95 confidence ensemble hm and cdda or prediction stochastic hm and scdda intervals the difference between confidence and prediction intervals as adopted in this case study is discussed in section 2 6 of ssq21 the time series plots in figs 2 4 illustrate several items worth mentioning the aw scores reported above indicate that the cdda variants result in sharp simulations which is confirmed in figs 2 4 however some of the cdda variants have extremely sharp simulations for example the 95 confidence intervals of the hbv light rf and lstm cdda variants are concentrated at the mean of the simulated streamflow for all catchments thus the 95 confidence interval is unable to capture high streamflow events however the cdda variants with sharp simulations appear to improve upon the low and mid flow simulations of the corresponding ensemble hm considering the stochastic approaches the scdda seems to compensate for the cdda and provide conservative simulations by increasing their spread for instance many high flow events missed by the ensemble hm and cdda are captured by the scdda though not all observations are covered within the 95 prediction interval as is expected the scdda is also able to noticeably reduce the bias of the ensemble hm in particular the scdda seems to improve the bias of the ensemble hm simulations for gr4j and tuwmodel in the muota catchment fig 4 finally figs 2 4 reveal that in general the scdda variants have sharp simulations for low flows and wide simulations for high flows this result is likely due to the knn algorithm used for estimating the conditional pdf of the model error which inherently accounts for the heteroscedasticity of the model error sikorska et al 2014 analyzing the distribution of the streamflow simulations for specific events such as floods may extract additional characteristics such as modality to better understand the ensemble and probabilistic simulations here raincloud plots allen et al 2019 were used to enhance the visualization of the simulations distribution which combines a boxplot a jittered scatter plot and a probability density plot the boxplot provides the summary statistics e g the median the jittered scatter plot shows the raw data which can be used to identify outliers and the density plot shows the distribution to check the spread and the modes of the simulated streamflow in figs 5 7 an example high flow event was extracted from the test set to visualize the distribution of the streamflow simulations generated by the different models figs 5 7 show that hbv light provides sharp simulations where no ensemble members capture the high streamflow event furthermore hbv light and its corresponding cdda generally result in symmetric distributions however the modality of the streamflow simulations distribution can change substantially by incorporating multiple uncertainty sources e g model output via the stochastic framework for example the ensemble simulations generated by hbv light in kleine emme and tuwmodel in muota are approximately symmetric about their median while their scdda counterparts produce simulations with multiple modes that often contain the observed value in general the scdda tends to capture the high streamflow events more so than its ensemble hm and cdda counterparts although there are many cases where the scdda captures the observed streamflow and in many cases has a simulation distribution that shifts upwards surrounding higher flows the distribution widens assigning non negligible probability to lower streamflow ranges for example the lstm scdda using tuwmodel in the kleine emme catchment has a wide simulation distribution which better captures the observed value although the mode of its distribution is much lower compared to its ensemble hm counterpart in the following sub section the cpp is used to assess the reliability of the ensemble and stochastic models where it is shown how the cpp can be used as a diagnostic tool to predict whether the stochastic framework i e the scdda can further improve the cdda 4 3 utilizing cpps as a diagnostic tool the validation and test set cpps for the ensemble and stochastic hms as well as the cdda and scdda variants are given in figs 8 10 for the three catchments utilizing the shape of the cpps it is possible to classify the streamflow simulation profile first when analyzing the ensemble hm results for all basins gr4j and hbv light provide sharp simulations which confirms earlier results see section 4 1 and 4 2 for tuwmodel the cpps show that the ensemble hm has a large bias for example the dünnern and kleine emme basins for tuwmodel distinctly result in over prediction furthermore the validation set cpps of the ensemble hms and cdda variants are very similar to the test set cpps thus the validation set cpps can be used to predict the reliability of the test for the study catchments and the hms and hm ddm combinations explored herein when analyzing the cpps a pattern can be seen with respect to the ensemble hms and the cdda variants an important observation is that all hbv light and most gr4j simulations for the ensemble hm have sharp simulations and their cdda counterparts result in the same profile suggesting that hms with sharp simulations tend to result in cdda counterparts with sharp simulations however it is difficult to find a common pattern with over and under predictions notably tuwmodel has cpps that show over prediction however the corresponding cdda variants produce cpps with different shapes next looking at the validation set cpps for the cdda tuwmodel in fig 8 gr4j in fig 9 and tuwmodel in fig 10 the cdda variants that lie close to the bisector have corresponding scdda variants that are less reliable this result is a visual representation of the previous section where the cdda variants with α r 0 85 had scdda counterparts that were less reliable one explanation is that for a highly reliable cdda the error approaches white noise is purely random thus the knn based stochastic resampling of model errors adds noise to the streamflow simulation therefore for a cdda that already has high reliability the stochastic resampling scheme results in scdda variants with lower reliability however the more important discovery is that the cdda s cpps from the validation set can be used to predict the reliability of the scdda on the test set in general the cdda and scdda have similar or higher reliability than their ensemble or stochastic hm counterparts another important finding is that the majority of the scdda variants produce cpps that are either close to the bisector line or indicate large simulations since reliable and conservative large simulations are of high importance in water resources applications e g flood forecasting the scdda can be a useful framework for hydrologists as well as water scientists and practitioners further considering the practicality of the above mentioned results suppose computational resources are limited then using the cpps as a diagnostic tool modellers and or users can decide to implement the stochastic resampling scheme depending on the cdda s validation set cpp thus in a practical setting if the cpp shows that the cdda is sufficiently reliable e g α r 0 85 the user may benefit from lower computation time when generating simulations and or forecasts in real time since the stochastic resampling scheme may be abandoned 4 4 effect of ensemble size on model performance since the computational demand of ddms and or stochastic resampling may deter some users from using the cdda and or scdda it is critical to assess the possibility of reducing the computational requirements while still providing sufficient performance one way to achieve this is to evaluate the effect of ensemble size the number of ensemble members on model performance to find a lower number of ensemble members that characterize the simulation uncertainty to a similar degree as ensembles with more members the probabilistic performance metrics from tables 2 4 and 6 were chosen to evaluate the effect of ensemble size on model performance although the aw and α r can indicate changes in the sharpness and reliability respectively of an ensemble or probabilistic simulation for increasing or decreasing ensemble size it is much more challenging to understand the overall improvement in model performance using these metrics compared to a single metric that simultaneously considers sharpness and reliability such as the crps of practical value to the following analysis it is possible to decompose the crps into reliability and sharpness components hersbach 2000 known as the reliability crps and potential crps where the latter is defined as the crps that would be achieved if the simulation was perfectly reliable and sensitive to the average spread of the ensemble simulation for more information on crps decomposition see hersbach 2000 in what follows the aw α r crps and the reliability and potential components of the crps are evaluated for various ensemble member sizes in fig 11 the aw is plotted as a function of ensemble size for the ensemble and stochastic hm cdda and scdda from fig 11 it appears that most ensemble hms and cdda variants have similar aw after 100 ensemble members one noticeable difference between the hms is that models calibrated by bo seem to be less stable with fluctuating aw for increasing ensemble member sizes this outcome is likely caused by bo searching diverse parameter spaces resulting in highly variable parameter sets whereas the gap method due to the high number of calibration runs seems to converge to similar parameter sets with low variance similar levels of instability in the aw are also found for the cdda variants adopting hms calibrated by bo regardless of the adopted ddm for the stochastic hm and scdda all models appear to have similar curves as the ensemble size increases notably beyond 25 ensemble members the aw seems to have stabilized however an inflection point occurs around 40 ensemble members causing a substantial change in aw that eventually stabilizes although the cause of this inflection point is unknown the change in aw from 100 to 200 members is minor next the effect of ensemble size on the α r is shown in fig 12 regarding the reliability of the ensemble hm and cdda fig 12 it appears that the α r for most models is relatively stable beyond 50 members however for the stochastic models all models seem to rapidly increase in reliability from one to approximately 15 members then slowly stabilize around 100 members for some models e g tuwmodel xgb scdda it appears that the reliability of the model asymptotically decreases from 15 to 100 ensemble members the instability of the α r for a low number of ensemble members may be due to a poor estimation of the simulation uncertainty which could be explained by the stabilization of the α r around 100 ensemble members finally the crps and its decomposed metrics potential crps and reliability crps for varying ensemble sizes are shown in figs 13 15 for the three catchments it appears that most models stabilize around 100 ensemble members according to the potential crps of the ensemble hm cdda and the stochastic models in all catchments however the potential crps of some models may continue to improve beyond 200 members for example the stochastic hm using gr4j in dünnern and kleine emme catchments although the improvements may not be significant comparing the results of the potential crps with the aw from fig 11 it appears that the increase in the aw with an increasing number of ensemble members does not meaningfully impact the potential crps beyond 100 members next analyzing the reliability crps shows that for most models it stabilizes around 100 ensemble members similar to the α r furthermore the shape of the α r curves in fig 12 closely match the reliability crps curves for the stochastic hm and scdda although the potential crps and reliability crps of some models in figs 13 15 do not appear to stabilize by 200 members the overall crps suggests that most models provide a stable estimation of simulation uncertainty by 100 members showing little improvement in performance beyond this point therefore the analysis of ensemble size versus model performance indicates that approximately 100 members are required for a stable estimation of simulation uncertainty in the study catchments it is important to recognize that the crps decomposition also estimates how much the sharpness and reliability of the simulations contribute to the overall crps in detail for most ensemble models ensemble hm and cdda the reliability crps contributes the most to the overall crps in contrast the potential crps contributes the most to the overall crps for the stochastic models stochastic hm and scdda with a minimal contribution coming from the reliability crps comparing the overall crps of cdda and scdda variants it was previously determined that the scdda could improve upon or maintain the crps of its unreliable cdda counterpart thus it appears that the stochastic framework tends to convert the ensemble models ensemble hm and cdda into more reliable models at the cost of simulation sharpness this result enhances the previous conclusion that the scdda tends to generate simulations that are more conservative while improving the reliability of its less reliable cdda counterpart 4 5 effect on model performance using a snow module in gr4j among the three hms gr4j does not incorporate a snow routine although snow processes play a significant role in the hydrology of two study catchments kleine emme and muota not including snow processes in an hm can be viewed as an error of the perceptual model i e the perceptions of how the catchment responds to meteorological forcings where deciding on the important hydrological processes is the first step of modelling and iteratively refined until the model is deemed to be satisfactory beven 2012 however since the cdda and scdda are used to correct the residuals of the ensemble hm they may also implicitly account for processes not included in the hm e g overcoming the need to couple a snow module with gr4j in what follows gr4j is used to test whether similar performance could be achieved by the cdda and scdda when an important hydrological process is and is not explicitly accounted for in the model therefore the cemaneige snow module valery 2010 is coupled with gr4j gr4jcn and compared against gr4j without the snow module using the cdda and scdda variants with the same calibration procedure as gr4j see section 2 4 3 and 3 2 the crps for gr4jcn in dünnern kleine emme and muota catchments is estimated as 0 47 0 68 and 2 22 representing approximately a 4 12 and 6 improvement over gr4j respectively on the test set since kleine emme showed the most substantial improvement in performance when adopting cemaneige in gr4j this catchment was chosen to evaluate any differences in performance kge α r and crps between the gr4j and gr4jcn cdda and scdda variants in table 7 the test set performance for the ensemble and stochastic hms using gr4jcn as well as gr4jcn cdda and scdda variants is summarized for the kleine emme catchment from table 7 it is possible to identify that the gr4j and gr4jcn cdda and scdda variants provide similar results considering all three ddms the difference in crps for gr4j and gr4jcn cdda and scdda variants is 0 02 or less similarly the α r differs by 0 03 or less while the deterministic kge is the same except for the stochastic hm therefore it appears that the cdda and scdda have the potential to correct for hydrological processes absent from the hm this result is aligned with lees et al 2022 where it was shown that the lstm is able to reproduce hydrological processes from data historical streamflow meteorological variables and catchment attributes thus it appears that xgb and rf also have this ability however to generalize this finding a much larger experiment with additional catchments hm parameter optimization methods and more flexible hms where model structure complexity can be closely controlled e g raven craig et al 2020 should be considered 5 conclusion and future work combining process based hydrological models hms with data driven models ddms has led to substantial improvements in the simulation of hydrological variables e g streamflow various approaches for combining hms and ddms have been introduced in the literature including the ensemble based conceptual data driven approach cdda which showed that several nonlinear ddms could be used to correct the residuals of an ensemble of hms and improve upon the ensemble hm streamflow simulations sikorska senoner and quilty 2021 due to the importance of uncertainty estimation in real world decision making its companion paper quilty et al 2022 developed the stochastic cdda scdda to account for various sources of uncertainty e g parameter model output using the ensemble hm simulations as input to ddms this follow up study proposes and explores a new scdda that estimates cdda uncertainty rather than ddm uncertainty in the original scdda the new scdda is tested using nine hm ddm combinations three hms and three ddms and compared against its cdda counterparts as well as ensemble and stochastic hms for daily streamflow simulation in three swiss catchments the different models are evaluated using several deterministic and probabilistic metrics as well as graphical aids time series plots raincloud plots etc the coverage probability plot cpp is proposed as a diagnostic tool for predicting when the out of sample reliability of the ensemble models ensemble hms and cdda can be improved by the stochastic framework using validation set data ensemble simulations and observations experiments showed that the scdda could significantly improve the ensemble hm simulations across most performance metrics with improvements in the mean continuous ranked probability score crps of 27 68 while the scdda improved upon the crps of the cdda by up to 15 it did not always outperform the cdda across all metrics the cpps showed that ensemble hms with sharp simulations tended to result in cdda variants with sharp simulations and unreliable ensemble models were improved using the stochastic framework meanwhile cdda variants with high reliability alpha index above 0 85 had scdda counterparts with lower reliability regardless all scdda variants had reliable or conservative simulations highlighting its potential as a valuable tool for decision making studying probabilistic performance as a function of ensemble size number of ensemble members revealed that an ensemble size of 100 members led to stable performance in one of the snow dominated catchments kleine emme not including a snow module in a simple hm led to lower performance than when it was included however neglecting the snow module had no discernible impact on the deterministic performance and negligible impact on the probabilistic performance of the cdda and scdda indicating that both approaches have the potential to account for missing processes in hms given the flexibility of the scdda it is possible to identify several potential improvements for the framework as the ddms are not restricted to the input variables used in this study performance may be improved using other variables generated by the hm such as actual evapotranspiration furthermore other hm structures and optimization algorithms may enhance the scdda simulations for example future work may involve using a blended model structure mai and craig 2020 with simultaneous calibration of structure and parameters chlumsky et al 2021 within scdda and testing its efficacy in diverse catchments newman et al 2014 in future studies bo could also be used alongside other optimization algorithms for jointly estimating hm and ddm parameters by optimizing hm parameters and ddm hyper parameters simultaneously if successful this approach may significantly reduce the computation time required for the proposed scdda data availability the observed streamflow and meteorological data can be ordered from the swiss federal office for the environment foen https www bafu admin ch last access december 11 2020 and meteoswiss http www meteoswiss ch last access december 11 2020 respectively the latest version of hbv light can be downloaded at https www geo uzh ch en units h2k services hbv model html funding the research was financially supported by natural sciences and engineering research council discovery grant launch supplement jq university of waterloo department of civil and environmental engineering graduate student support allowance jq queen elizabeth ii graduate scholarship in science and technology dh and university of waterloo president s graduate scholarship dh this research and decision to publish its findings was not influenced by the institutions that provided the above mentioned financial support declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors sincerely thank meteoswiss for providing the meteorological data and the foen for the streamflow data from project no 15 0054 pj o503 1381 the hbv light model parameters were calibrated using sciencecloud provided by s3it at the university of zurich appendix a hm parameter ranges table a1 parameter ranges for tuwmodel used in bo retrieved from the tuwmodel r package viglione and parajka 2020 table a1 parameter explanation minimum maximum units scf snow correction factor 0 9 1 5 ddf degree day factor 0 5 mm c d tr threshold temperature for rain above 1 3 c ts threshold temperature for snow below 3 1 c tm threshold temperature for melt above 2 2 c lprat parameter for potential evaporation 0 1 fc field capacity 0 600 mm beta parameter for runoff production 0 20 k0 storage coefficient for very fast response 0 2 d k1 storage coefficient for fast response 2 30 d k2 storage coefficient for slow response 30 250 d lsuz threshold storage state 1 100 mm cperc constant percolation rate 0 8 mm d bmax maximum base at low flows 0 30 d croute free scaling parameter 0 50 d2 mm table a2 parameter ranges for gr4jcn used in bo retrieved from the airgr r package coron et al 2022 note gr4j used the same parameter ranges as gr4jcn but did not consider x5 and x6 table a2 parameter explanation minimum maximum units x1 production store maximal capacity 0 9 1 5 mm c d x2 catchment water exchange coefficient 0 5 mm d x3 one day maximal capacity of routing reservoir 1 3 mm x4 unit hydrograph time base 3 1 d x5 ponderation coefficient 0 1 x6 degree day factor 2 6 mm c d 
25516,calibration and validation 2 6 statistical analysis 2 7 model uncertainty estimation 3 results 3 1 sensitivity analysis 3 2 model calibration and validation 3 3 temporal trends of simulated and observed data 3 4 uncertainty analysis 4 discussion 4 1 sensitivity analysis 4 2 calibration and validation 4 3 optimization of parameters during calibration 5 conclusion software and data availability acknowledgment appendix a supplementary data americansocietyofagricultural 2015 1 designconstructionsubsurfacedrainagesystemsagriculturallandsinhumidareas babson 2020 d systemsformonitoringanalyticsforrenewabletransportationfuelsagriculturalresourcesmanagementofficenepapolicycompliance banger 2020 137851 k brighenti 2019 103 113 t brimhall 1987 567 587 g congreves 2016 179 189 k doherty 2015 j doherty 2018 j pestmodelindependentparameterestimationusermanualpartipestsensanglobaloptimisers doherty 2021 j pestroadmapsroadmap10modelcalibration donner 2008 4513 4518 s dutta 2018 54 72 b francis 2006 g wintermanagementmaizepaddocks franssen 2003 281 295 h haan 1998 65 70 c halopka 2018 r he 2018 187 198 w he 2019 1016 w helton 2003 23 69 j hill 1998 m documentationucodeacomputercodeforuniversalinversemodeling ingraham 2019 79 87 p 2012 usersguidefordndcmodel izaurralde 2012 r developmentapplicationepicmodelforcarboncyclegreenhousegasmitigationbiofuelstudiespnnlsa83721 jeong 2010 4505 4527 j jones 1986 c ceresnmaizeasimulationmodelmaizegrowthdevelopment kamkar 2014 b kohne 2006 1 32 j krobel 2011 503 520 r lamers 2007 52 58 m li 1992 9759 9776 c li 2000 4369 4384 c li 2006 116 130 c li 2012 163 200 c li 2014 108 118 h liang 2017 201 210 h liu 2019 x greetanalysisforterrarootssuccessscenarios liu 2022 107325 f ma 2003 39 l malone 2010 1711 1723 r malone 2014 10 22 r mckay 1979 239 245 m mcmillan 2021 e1499 h miskewitz 2007 r soilwaterassessmenttoolswat121 moriasi 2015 1609 1618 d musyoka 2021 f 2021 powerproject 2019 annualmaps necpalova 2015 110 130 m 2021 prismclimatedata parton 1998 35 48 w perin 2020 374 r perlman 2013 183 192 j pribyl 2010 75 83 d qin 2013 26 36 x rafique 2013 r rafique 2014 106 114 r sabo 2019 3104 3124 r sieber 2005 216 235 a smith 2013 139 150 w smith 2019 3 30 w 2019asabeannualinternationalmeetingjuly towardsimprovingdndcmodelforsimulatingsoilhydrologytiledrainage smith 2020 104577 w swangjang 2015 36 40 k swiler 1998 l ausersguidesandiaslatinhypercubesamplingsoftware tabatabaie 2018 428 439 s tang 2007 793 817 y tonitto 2007 51 63 c trybula 2015 1185 1202 e 2015 websoilsurveynaturalresourcesconservationservice wang 2005 1041 1054 x wang 2019 3523 3539 ward 2015 161 222 a environmentalhydrologyissue2015 runoffdrainage williams m xu 2020 105904 j zhang 2015 388 398 y zhang 2021 112640 j zhang 2021 114701 j zhao 2015 q annualmeetinggermansocietyforphotogrammetryremotesensinggeoinformation regionalapplicationsitespecificbiochemicalprocessbasedcropmodeldndcforriceinnechina zhao 2020 104587 z bhattaraix2022x105494 bhattaraix2022x105494xa 2023 08 11t00 00 00 000z http www elsevier com open access userlicense 1 0 https vtw elsevier com content oragreement 10138 chu doa publishacceptedmanuscriptindexable 2023 08 11t00 00 00 000z http creativecommons org licenses by nc nd 4 0 2022 published by elsevier ltd 2022 09 21t04 48 13 380z http vtw elsevier com data voc addontypes 50 7 aggregated refined 0 https doi org 10 15223 policy 017 https doi org 10 15223 policy 037 https doi org 10 15223 policy 012 https doi org 10 15223 policy 029 https doi org 10 15223 policy 004 item s1364 8152 22 00197 9 s1364815222001979 1 s2 0 s1364815222001979 10 1016 j envsoft 2022 105494 271872 2022 10 04t10 15 39 547552z 2022 11 01 2022 11 30 1 s2 0 s1364815222001979 main pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 main application pdf 7f7d418b03dd8e837c0237e447805101 main pdf main pdf pdf true 7976300 main 15 1 s2 0 s1364815222001979 main 1 png https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 preview image png c4d955b583558ef48e98696219d30f4f main 1 png main 1 png png 55855 849 656 image web pdf 1 1 s2 0 s1364815222001979 fx1 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 fx1 downsampled image jpeg 940b65e4b04619eecf982c1e3ddfa4ff fx1 jpg fx1 fx1 jpg jpg 209658 388 624 image downsampled 1 s2 0 s1364815222001979 gr1 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr1 downsampled image jpeg 054ffe602fbc66641772dbf74e05dcfe gr1 jpg gr1 gr1 jpg jpg 181696 605 535 image downsampled 1 s2 0 s1364815222001979 gr4 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr4 downsampled image jpeg 80e1cab0d9edd883d98e7cc3fc561fe3 gr4 jpg gr4 gr4 jpg jpg 168132 548 691 image downsampled 1 s2 0 s1364815222001979 gr5 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr5 downsampled image jpeg 0465c026fa149852f725425dc37cbe45 gr5 jpg gr5 gr5 jpg jpg 153471 347 535 image downsampled 1 s2 0 s1364815222001979 gr2 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr2 downsampled image jpeg 2e1374ab158b7868bb03de56942d5a56 gr2 jpg gr2 gr2 jpg jpg 132266 384 389 image downsampled 1 s2 0 s1364815222001979 gr3 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr3 downsampled image jpeg c2a30c209575df0eee842201c9f00a01 gr3 jpg gr3 gr3 jpg jpg 150216 653 535 image downsampled 1 s2 0 s1364815222001979 gr8 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr8 downsampled image jpeg 73545a452e861b6ddcd1036a719dbac2 gr8 jpg gr8 gr8 jpg jpg 103084 249 535 image downsampled 1 s2 0 s1364815222001979 gr6 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr6 downsampled image jpeg 45e114a254c48c2490bb9d7eaa1ee21d gr6 jpg gr6 gr6 jpg jpg 141154 516 535 image downsampled 1 s2 0 s1364815222001979 gr7 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr7 downsampled image jpeg 0486e6f066d00bd6b46b59cf88f82d5c gr7 jpg gr7 gr7 jpg jpg 101611 235 535 image downsampled 1 s2 0 s1364815222001979 fx1 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 fx1 thumbnail image gif 051e9ad6c253a96f09bbc9f55573622c fx1 sml fx1 fx1 sml sml 86687 136 219 image thumbnail 1 s2 0 s1364815222001979 gr1 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr1 thumbnail image gif 89b47a5485a25c5b953a36f09f011a29 gr1 sml gr1 gr1 sml sml 83071 164 145 image thumbnail 1 s2 0 s1364815222001979 gr4 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr4 thumbnail image gif 24edc2b605c79407d30ed37b64e1f472 gr4 sml gr4 gr4 sml sml 79531 164 207 image thumbnail 1 s2 0 s1364815222001979 gr5 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr5 thumbnail image gif 2b5b6d120ac7e42127ca0fcb9cb4c495 gr5 sml gr5 gr5 sml sml 80963 142 219 image thumbnail 1 s2 0 s1364815222001979 gr2 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr2 thumbnail image gif 18651af9f88c5a200c0c9c8a3f0c2823 gr2 sml gr2 gr2 sml sml 82188 164 166 image thumbnail 1 s2 0 s1364815222001979 gr3 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr3 thumbnail image gif bac3b6511405896072155a264d145d55 gr3 sml gr3 gr3 sml sml 75568 164 134 image thumbnail 1 s2 0 s1364815222001979 gr8 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr8 thumbnail image gif b7a3dd92d6ae57ac8a59b75f8f59eaef gr8 sml gr8 gr8 sml sml 71964 102 219 image thumbnail 1 s2 0 s1364815222001979 gr6 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr6 thumbnail image gif 1fcdc56e4bab30dda2e3da1f00aa9326 gr6 sml gr6 gr6 sml sml 77073 164 170 image thumbnail 1 s2 0 s1364815222001979 gr7 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr7 thumbnail image gif 4f67176a136431bf9495aae2a9497cbc gr7 sml gr7 gr7 sml sml 71632 96 219 image thumbnail 1 s2 0 s1364815222001979 fx1 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 highres image jpeg e39d8b0eb684864d4e6d5b9940f51734 fx1 lrg jpg fx1 fx1 lrg jpg jpg 1124793 1718 2764 image high res 1 s2 0 s1364815222001979 gr1 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 highres image jpeg beef9229cea289f25831db82e142446f gr1 lrg jpg gr1 gr1 lrg jpg jpg 890698 2679 2370 image high res 1 s2 0 s1364815222001979 gr4 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 highres image jpeg 16f57afcedadc9b0008383eef5392657 gr4 lrg jpg gr4 gr4 lrg jpg jpg 840274 2425 3058 image high res 1 s2 0 s1364815222001979 gr5 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 highres image jpeg b1beb63d767567533b1a1c208f150688 gr5 lrg jpg gr5 gr5 lrg jpg jpg 687135 1537 2370 image high res 1 s2 0 s1364815222001979 gr2 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 highres image jpeg 76e0502f775b9db87bbf907b6e64d645 gr2 lrg jpg gr2 gr2 lrg jpg jpg 444142 1700 1722 image high res 1 s2 0 s1364815222001979 gr3 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 highres image jpeg bfcabf4da3cb30b41810e6a92e5b5e52 gr3 lrg jpg gr3 gr3 lrg jpg jpg 544972 2892 2370 image high res 1 s2 0 s1364815222001979 gr8 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 highres image jpeg 3786b9807cb9da9f8b87f12f153bba2a gr8 lrg jpg gr8 gr8 lrg jpg jpg 300133 1102 2371 image high res 1 s2 0 s1364815222001979 gr6 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 highres image jpeg 96dd3a6536ba72072150be2f43e01cc1 gr6 lrg jpg gr6 gr6 lrg jpg jpg 515885 2284 2370 image high res 1 s2 0 s1364815222001979 gr7 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 highres image jpeg ea8079872473cd656ccceef38b42b0de gr7 lrg jpg gr7 gr7 lrg jpg jpg 318142 1043 2370 image high res 1 s2 0 s1364815222001979 mmc1 docx https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 mmc1 main application vnd openxmlformats officedocument wordprocessingml document 3c06b47a60f563b21354d08209c8353c mmc1 docx mmc1 mmc1 docx docx 553130 application 1 s2 0 s1364815222001979 si9 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 image svg xml d55965687a16f7845db0d287848a7f8e si9 svg si9 si9 svg svg 7357 altimg 1 s2 0 s1364815222001979 si7 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 image svg xml 36509ea894c130bf2200e047ac72f7ee si7 svg si7 si7 svg svg 54588 altimg 1 s2 0 s1364815222001979 si2 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 image svg xml b13ab6eb684f169eb173930d95cfaeac si2 svg si2 si2 svg svg 91492 altimg 1 s2 0 s1364815222001979 si6 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 image svg xml e45b4b586a1611816a8764147f0f5ecd si6 svg si6 si6 svg svg 71161 altimg 1 s2 0 s1364815222001979 si3 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 image svg xml f98afecad7eb662e4894701d31c28cd5 si3 svg si3 si3 svg svg 48211 altimg 1 s2 0 s1364815222001979 si4 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 image svg xml 19b29430f1d81a48404b8fbd0c42eaa0 si4 svg si4 si4 svg svg 60804 altimg 1 s2 0 s1364815222001979 si1 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 image svg xml d456c3831e4dcc7f62bae35560888314 si1 svg si1 si1 svg svg 58084 altimg 1 s2 0 s1364815222001979 si5 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 image svg xml 068167a39f15c2e40f757fd446dc0a93 si5 svg si5 si5 svg svg 81357 altimg 1 s2 0 s1364815222001979 si8 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 image svg xml 65c44ff6bdd8b5f4572a942293a08a3b si8 svg si8 si8 svg svg 2363 altimg 1 s2 0 s1364815222001979 si11 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 image svg xml d148c100302fe70399da1ac341e8bc98 si11 svg si11 si11 svg svg 3153 altimg 1 s2 0 s1364815222001979 si10 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 image svg xml ebba7251c80406184c391cbcec36ac5d si10 svg si10 si10 svg svg 9338 altimg 1 s2 0 s1364815222001979 am pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content egi 106qhctm1h7 main application pdf 9d85fb5442e6dcb7394169d7e2bdd06b am pdf am am pdf pdf false 1824938 aam pdf enso 105494 105494 s1364 8152 22 00197 9 10 1016 j envsoft 2022 105494 fig 1 location of the study area within the sub basin of maumee river watershed fig 1 fig 2 flow chart diagram of the algorithmic calibration of dndc using pest fig 2 fig 3 bar plot of dndc relative composite sensitivity to individual observation groups a water leaching b nitrate leaching c crop yield and d the entire composite of all outputs fig 3 fig 4 optimized values of the 14 parameters for each calibration strategy included in the sensitivity analysis in simultaneous calibration it took eleven optimization iterations to optimize swsr units for each parameter s y axis are given in parenthesis after the parameter name and can also be found in table 3 fig 4 fig 5 dndc simulated values simultaneous and sequential and field observations for water leaching top nitrate leaching middle and weather data bottom fig 5 fig 6 cumulative water and nitrate leaching from 2014 to 2020 for observed default and calibrated outputs fig 6 fig 7 dndc simulated values and field observations for cumulative crop biomass yield during calibration and validation period fig 7 fig 8 95 prediction bounds and field observations for monthly water leaching top and nitrate leaching bottom fig 8 table 1 dndc climate and soil input data and their sources table 1 category input data climate daily maximum and minimum temperature precipitation northwest alliance for computational science engineering 2021 radiation humidity and windspeed nasa langley research center larc power nasa langley research center power project 2021 n concentration in rainfall national atmospheric deposition program 2019 soiltopsoil properties soil structure scs musle soil texture bulk density porosity a ph field capacity wilting point hydro conductivity clay fraction soc b field slope usda 2015 curve number cn manning s roughness coefficient miskewitz 2007 tile drainage components tile drain depth drain space american society of agricultural and engineers 2015 a porosity was determined using bulk density and particle density 2 56 g cm 3 brimhall and dietrich 1987 q zhao et al 2015 b soc was calculated using som from web soil survey and assuming a soc to som ratio of 0 58 pribyl 2010 table 2 management and crop rotation details of the edge of field study site table 2 2014 2015 2016 2017 2018 2019 2020 crop management jul 2 aug 1 may 21 jul 4 jun 30 jun 7 may 6 clover p a clover t c corn p wheat h alfalfa h alfalfa t corn p jul 7 sept 4 sept 22 aug 28 aug 5 jun 26 sept 15 wheat h b radish p corn h alfalfa p alfalfa h sorghum p corn h oct 18 aug 30 sept 21 wheat p sorghum h wheat p sept 10 cereal rye p fertilizer and manure management oct 17 may 21 jun 16 mar 29 jul 6 gypsum fert 9337 kg ha jul 5 may 6 n fert n fert 52 kg ha n fert sept 17 n fert 42 kg ha 17 kg ha 78 kg ha manure n 1726 kg ha jun 15 aug 23 n fert 81 kg ha manure n sept 23 313 kg ha manure n 1726 kg ha tillage management aug 2 25 aug 16 chisel chisel oct 17 aug 22 field cultivator disc harrow aug 27 field cultivator a refers to planting b refers to harvest c refers to termination table 3 initial upper lower and optimized values from sequential calibration of dndc adjustable parameters included in the inverse modeling calibration process table 3 parameter description reference units lower bound higher bound default manual optimized cly clay content of topsoil 0 10 cm ssurgo fraction 0 4 0 6 0 49 0 58 0 588 por soil porosity ssurgo fraction 0 482 0 65 0 5 0 602 0 5024 blk bulk density of topsoil field g cm³ 1 1 45 1 28 1 02 1 427 cn soil conservation service scs curve number miskewitz 2007 ratio 64 95 70 77 7 85 6 hdc c n ratio of humads pool swangjang 2015 c n ratio 5 12 8 8 299 8 234 yd c maximum yield corn typical of region kg c ha 3000 6000 4000 3044 3084 6 cngc grain c n corn francis et al 2006 c n ratio 20 60 30 23 05 28 76 tdc thermal degree days to reach crop maturity corn halopka 2018 zero c 2000 3000 2400 2512 2806 cngw grain c n ratio winter wheat kamkar et al 2014 ratio 20 60 50 54 14 60 dd tile drain depth in the 2m profile american society of agricultural and engineers 2015 m 0 5 1 2 0 9 0 684 0 752 ds tile drain spacing american society of agricultural and engineers 2015 m 9 18 13 15 3 9 kdr effective horizontal rate of saturated conductivity to the tiles as a function of veritical ksat smith et al 2020 factor 0 5 2 5 0 7 1 965 0 881 nfac fraction of nitrate that moves with water fluxes within layers user guide dndcv can 2020 fraction 1 3 5 1 6 3 2 913 pref2 fraction of n that does not preferentially leach out of the soil profile through macropore flow user guide dndcv can 2020 fraction 0 85 1 0 95 0 9 0 8538 note the study site has two soil map units table 1 and the values for soil parameters were varied to reflect potential in field variability vol volumetric the optimized values are for the sequential calibration approach table 4 monthly calibration and validation statistics by calibration approaches for all measured output types table 4 development of a calibration approach using dndc and pest for improving estimates of management impacts on water and nutrient dynamics in an agricultural system abha bhattarai a 1 garrett steinbeck a 1 brian b grant b margaret kalcic a c kevin king d ward smith b nuo xu f jia deng e sami khanal a a department of food agricultural and biological engineering the ohio state university columbus oh 43210 usa department of food agricultural and biological engineering the ohio state university columbus oh 43210 usa department of food agricultural and biological engineering the ohio state university columbus oh 43210 usa b ottawa research and development centre aafc 960 carling ave ottawa k1a 0c5 canada ottawa research and development centre aafc 960 carling ave ottawa k1a 0c5 canada ottawa research and development centre aafc 960 carling ave ottawa k1a 0c5 canada c biological systems engineering university of wisconsin madison madison wi 53706 usa biological systems engineering university of wisconsin madison madison wi 53706 usa biological systems engineering university of wisconsin madison madison wi 53706 usa d u s department of agriculture agricultural research service soil drainage research unit columbus oh 43210 usa u s department of agriculture agricultural research service soil drainage research unit columbus oh 43210 usa u s department of agriculture agricultural research service soil drainage research unit columbus oh 43210 usa e earth systems research center institute for the study of earth oceans and space university of new hampshire durham nh 03824 usa earth systems research center institute for the study of earth oceans and space university of new hampshire durham nh 03824 usa earth systems research center institute for the study of earth oceans and space university of new hampshire durham nh 03824 usa f department of computer science and engineering the ohio state university columbus oh 43210 usa department of computer science and engineering the ohio state university columbus oh 43210 usa department of computer science and engineering the ohio state university columbus oh 43210 usa corresponding author 590 woody hayes drive columbus oh 43210 usa 590 woody hayes drive columbus oh 43210 usa 1 these authors contributed equally to this work calibration and validation are standardized practices to establish the credibility of biogeochemical models for understanding agroecosystem nutrient dynamics we evaluated three automatic calibration approaches including simultaneous sequential and separate for the calibration of model parameters of the biogeochemical denitrification decomposition dndc model through inverse modeling using pest open source parameter estimation and uncertainty analysis software while manual calibration by experts performed the best during calibration period followed by simultaneous calibration sequential calibration had the best model performance during the validation period model sensitivity analyses demonstrated water leaching to be sensitive to curve number and drain spacing nitrate leaching to be sensitive to porosity and clay content and corn yield to be sensitive to accumulative temperature and grain c n ratio while some level of expertise is required to inform the automated calibration procedure it represents a more efficient and robust approach toward increasing the performance of biogeochemical models keywords agroecosystem model calibration dndc inverse modeling pest data availability data will be made available on request 1 introduction agriculture in the united states accounts for 75 of the nation s n2o emissions and is also a significant contributor to nutrient loading donner and kucharik 2008 sabo et al 2019 negatively affecting ecosystem services pribyl 2010 reducing greenhouse gas ghg emissions and improving water quality require agricultural producers to adopt new technologies and management practices state and federal funding programs e g nrcs environmental quality incentives program eqip and conservation stewardship program csp liu et al 2019 as well as voluntary market based carbon credit systems e g verified carbon standard promote practices and technology adoption to meet water quality goals and enhance soil carbon sequestration however there is much to be understood about the effects of conservation practices on ecosystem services in varied and complex environments babson 2020 although conventional approaches involving in field measurements of agricultural emissions from various practices and technologies are useful they are often also cost prohibitive and time consuming as a result recommendations for current conservation practices are based on limited field i e low resolution observations with high uncertainty ecosystem models when evaluated with measured data are powerful tools for estimating the impact of management practices on nutrient fluxes drainage and runoff they are cost effective and time efficient and hence can be used as a tool to make decisions about conservation agricultural practices over the last two decades a number of ecosystem models such as daycent parton et al 1998 denitrification decomposition dndc li et al 1992 epic izaurralde et al 2012 and ceres egc jones and kiniry 1986 have been developed to quantify nutrient fluxes such as ghgs nitrate leaching and runoff at the field and regional scales these agroecosystem models are composed of inter dependent process based equations representing fundamental physical chemical and or biological ecosystem processes necpálová et al 2015 thus these models generally utilize a high number of input parameters that require calibration and validation to produce meaningful results the utility of these models depends primarily on how well they have been parameterized and calibrated dndc is a well recognized model that has been used globally to simulate nutrient fluxes and runoff from various ecosystems and management practices banger et al 2020 liu et al 2022 zhang et al 2015 it has been shown to simulate nutrient fluxes such as nitrate leaching relatively well ingraham and salas 2019 zhang et al 2021a zhang et al 2021b however it has primarily been calibrated manually using a trial and error method although useful in defining parameter ranges for calibration to a specific ecosystem the trial and error approach is a laborious process as it needs to be iterated through many times to provide insight into model behavior it can also be subjective and unreliable because the proper calibration of a model is highly dependent upon prior experience and user knowledge and not all users have the same level of insights and experience with the model rafique et al 2013 another drawback to manual calibration is that some degree of uncertainty remains as to whether the optimal set has been achieved as a local optimum calibration is obtained inverse modeling which employs algorithmic search methods guided by statistical measures to objectively adjust parameters and assess the model s fit to measured data is one approach to overcome the limitations of manual calibration inverse modeling accelerates the identification of combined parameter effects on simulated outputs reducing the required time to gain insight into model dynamics although inverse modeling has been commonly utilized for parameter optimization and sensitivity analysis of hydrological and geophysical models franssen et al 2003 köhne et al 2006 it has been applied successfully to only a few biogeochemical models such as daycent rafique et al 2013 2014 rzwqm malone et al 2010 2014 forest dndc lamers et al 2007 and dndc specifically for wheat growth kröbel et al 2011 lamers et al 2007 used an algorithm search method based on the ucode program hill and poeter 1998 originally designed to optimize the groundwater model to automate the calibration of forest dndc a specific adaption of the main dndc to forest ecosystems li et al 2000 and obtained considerable model simulation improvement of n2o there are several input parameters required by the dndc model that can vary spatially and temporally which if kept constant across soil climatic regions can significantly affect model predictions calibration and validation of the model are thus required prior to its application to provide reasonable estimates of ecosystem services including soil carbon ghg fluxes and crop yield under what if management and climate change scenarios in addition to model calibration and validation uncertainty analysis is critical to offer insights on uncertainties in the model output that arises from uncertainties in the model parameters there are several approaches to model calibration including simultaneous sequential brighenti et al 2019 musyoka et al 2021 and separate the simultaneous calibration approach involves calibrating multiple parameters at once based on ground truth observations for multiple output variables sequential calibration on the other hand involves calibrating multiple parameters for multiple output variables in a stepwise manner in this process the most sensitive parameters for one output variable are calibrated first and the calibrated parameters from that step are used to calibrate for another output variable a study by musyoka et al 2021 found that the sequential approach worked better for streamflow prediction while a study by brighenti et al 2019 found the simultaneous approach to be better for estimating streamflow and suspended sediment both of these studies used the soil water assessment tool swat model meanwhile a separate calibration approach involves calibrating single multiple parameters based on ground truth observations for a single output variable these different calibration methods may lead to varied model performance depending on the model output of interest hence it is important to assess which calibration methods work best for calibrating the model of interest as well as the model output the objectives of this study were to 1 develop a framework that guides automatic calibration of the dndc model using inverse modeling and 2 demonstrate and compare calibration approaches manual separate simultaneous sequential within the framework to simulate water and nitrate leaching and crop yield in an agricultural system this unique effort is the first attempt to calibrate the dndc model for nitrate leaching using inverse modeling and offers an approach for rigorously estimating the uncertainty of model outputs 2 methods 2 1 study site and data the field site 24 7 ha in area is located in the north powell creek which is an huc 12 sub watershed of the maumee river basin fig 1 and is part of the usda agricultural research service edge of field network williams et al 2016 the site has 97 paulding clay pa and 3 roselms silty clay loam rsa during the study period the site was managed in a diverse crop rotation involving corn wheat alfalfa sorghum cereal rye and radish cover crop the field is systematically tile drained with lateral spacing every 17 5 m and since 2013 the tile drainage from the field has been continuously monitored tillage at the site is defined as conventional multiple tillage events in some year while nutrient application occurred annually prior to corn planting nitrogen n in the form of urea ammonium nitrate uan was applied while liquid dairy manure was applied after wheat harvest and prior to cover crop planting table 1 inputs required for the dndc model include i daily precipitation cm maximum and minimum temperature degree celsius radiation mj m2 day humidity windspeed m s ii initial topsoil top 10 cm physical and chemical properties i e soil texture percent clay bulk density g cm³ ph soc kg c kg hydraulic conductivity m hr etc table 1 iii crop parameters including biomass fraction and c n ratios of leaf stem root and grain components water demand g water g dry matter and optimal temperature degree celsius iv management practices table 2 and v tile drainage components i e drain spacing m depth m and tile radius m etc these data were obtained from various sources weather data precipitation and temperature was collected from the nearest meteorological station id usc00336342 from the prism database northwest alliance for computational science engineering 2021 and radiation humidity and windspeed data were available from the nasa power project nasa langley research center larc power nasa langley research center power project 2021 bulk density and ph values were available directly from field soil analysis and other soil related parameters were obtained from the soil survey geographic database ssurgo ground truth data used in the calibration and validation of the dndc model include monthly water tile discharge and total nitrate for the period 2014 2018 for calibration and 2019 2020 for validation and annual crop yield data for the period 2013 2014 for calibration and 2016 2017 for validation please refer to williams et al 2016 for further details regarding how water quality data were collected 2 2 dndc model dndc was originally conceived to simulate n2o emissions li et al 1992 over time it was developed and expanded to simulate the effects of soil c and n water cycling climate and agricultural management li et al 2006 2012 on crop growth ghg emissions soil nutrient loss and soc sequestration on a daily timescale the model is structured as a collection of sub models that generally operate as a two component system he et al 2018 qin et al 2013 the first component consisting of soil climate crop growth and decomposition sub models uses climate soil crop and management inputs to predict soil environmental conditions including soil temperature moisture and nutrient substrates the second component consisting of denitrification nitrification and fermentation sub models then uses these intermediary soil factors to predict ghg emissions derived from the plant soil system institute for the study of earth oceans and space 2012 the canadian version of the dndc model is a development branch first initiated in 2011 congreves et al 2016 kröbel et al 2011 smith et al 2013 to better simulate ghgs soil c n dynamics hydrology and crop growth under temperate climate the water content is estimated by a cascade flow algorithm whereby water content per layer tips to field capacity nitrate moves along with the water flux vertically to the layer below and it is also preferentially leached through macropores out of the soil profile as nitrate flow was simulated without water flux sometimes in earlier versions the preferential flow was correlated with water movement in this version so that the movement of nitrate to the tiles is iterative on an hourly basis the current version of the model incorporates a heterogeneous soil profile extended from 0 5 m to 2 m root penetration and density functions and mechanistic tile drainage smith et al 2020 a more detailed description of the new features the executables and examples can be found at https github com brianbgrant dndcv can in this study we ran the model using a batch simulation mode whereby multiple sets of simulations can be run consecutively to automate the calibration of the dndc model the model was integrated with an inverse model pest discussed below at a source code level the model was run using historical daily weather data from 2008 to 2020 with the recorded management practices and crop rotation of the current field repeated over this period the first 5 years were treated as a spin up period to bring initial soc levels to steady state perlman et al 2013 tabatabaie et al 2018 while the years 2013 2020 which aligned with observations were used for calibration and validation purposes 2 3 pest for inverse modeling dndc parameters were calibrated using measured data through inverse modeling using the open source parameter estimation and uncertainty analysis software pest doherty 2015 pest is a model independent parameter estimation software that uses a sum of weighted square residuals swsr between model based outputs and measured data as an objective function to determine an optimized parameter set pest iteratively and systematically varies model inputs runs the model and evaluates the model fit using an objective function until optimal values have been determined fig 2 upon the start of each iteration the relationship between the model parameters and model outputs is linearized using a taylor series expansion based on the current best parameter set doherty 2015 this allows the matrix computation of all first order partial derivatives of the simulated values corresponding to observations in the calibration dataset which form the sensitivity or jacobian matrix using this sensitivity matrix in the gauss marquardt levenberg gml algorithmic search method parameters that minimize the swsr are obtained for the overall simultaneous calibration of the dndc model a weighted multi component objective function doherty 2015 was used in which each observation type was assigned to a group constituting a portion of the objective function inter group weights were calculated using the inverse of the standard deviation of observations such that each group contributed proportionately to the objective function at the start of the calibration process for the sequential calibration approach the parameters were calibrated using water leaching outputs only at first so only one observation group was contributing to the objective function for the second step where nitrate and crop yield were calibrated together a weighted multi component objective function was used for the separate calibration methods only one output group water leaching nitrate leaching or crop yield was contributing to the overall objective function to ensure numerical stability of the inversion process singular value decomposition svd of the weighted jacobian matrix was truncated on an iteration by iteration basis through regularization truncation was automatically calculated based on a stability criterion the regularization transforms the original model parameters into linear combinations determines the most sensitive parameters and truncates the matrix reducing the number of estimated parameters to maintain numerical stability 2 4 model integration pest is integrated with dndc at a source code level and communicates with dndc fig 2 using three input files template control and instruction the template file has the structure of a dndc input file and is used to create model input files with modified parameter values the instruction file contains a series of pest commands that direct the reading of simulated outputs of interest the control file contains initial values for input parameters observed data with weights gml search algorithm constraints model run commands and parameter estimation termination criteria upon initiation pest reads the control file for initial parameter values observation data and numerical stability and regularization constraints to execute the model the first run uses user specified default parameter values table 3 after the first run is completed pest extracts the simulated output values e g water leaching from the dndc output files calls the dndc model repeatedly adjusting one parameter value at a time and allowing the computation of the sensitivity matrix the updated model parameter values are then used to begin another iteration this process is repeated until the objective function is minimized or until another convergence parameter e g the maximum number of iterations or minimum change in parameter values compared to the previous runs is achieved 2 5 calibration and validation prior to parameter estimation pre calibration composite sensitivities of each parameter with respect to each output type i e water leaching nitrate leaching and crop yield are calculated necpálová et al 2015 rafique et al 2013 composite sensitivities provide a comprehensive representation of a parameter s influence on the entire calibration dataset a total of 34 parameters were first included in the pre calibration sensitivity analysis by pest table 3 table s1 and fig s1 parameter sensitivities were calculated as a jacobian matrix using initial parameter values the jacobian matrix is composed of all partial derivatives of parameters to outputs each row consists of all parameters partial derivatives for the output corresponding to that row parameter sensitivities were calculated by first multiplying the output weighted q jacobian matrix by its transpose resulting in the sum of squares of individual weighted outputs this was then multiplied by the absolute value of the parameter p which in this case is the log base 10 transformed value 1 s i j t q j i i 1 2 l o g p where s i is relative composite sensitivity of parameter i j is the jacobian matrix of parameter sensitivities with respect to model operator matrix relative to output q is the diagonal weight matrix with observation weights as diagonal values and p is the initial parameter value to be able to compare the performance of pest to manual calibration the dndc model was calibrated manually by experts also the co authors of this study before integrating the dndc model with pest this manual approach helped select some parameters sensitive to water leaching nitrate leaching and yield looking at the relative composite sensitivities of the 34 parameters fig s1 a total of 14 parameters were chosen on the basis of relative composite sensitivities 10 of the maximum relative composite sensitivity necpálová et al 2015 rafique et al 2013 and according to expert opinion so that we could see the effect of sensitive parameters only then sensitivity analyses by pest were done for these 14 parameters fig 3 the sensitivity analysis was then used to optimize the parameters during the calibration process table 3 table 3 provides details on the dndc parameters description and references their default values manually calibrated values the upper and lower bound for each parameter during model calibration and the optimized values of the parameters for sequential calibration the optimized values for other modes of calibration are given in table s2 in most instances the difference between upper and lower bounds of a parameter is equal to approximately four to six prior standard deviations of the parameter doherty 2018 or they are sometimes set very low and very high to reduce parameter bound collisions doherty 2021 but these bounds must be managed to ensure they don t fall out of scope of model defined bounds for the parameters hence since the information on prior parameter standard deviations was unavailable the bounds were chosen based on realistic limits for the parameters these were determined from field measurements manual calibration literature review and values used by the dndc model expert opinion was also used for ensuring that the bounds were representative of the site conditions the default values for the parameters were kept between these bounds and were mostly the mid value of the bounds so that the optimized parameters could be found on either the lower or the upper spectrum the model was run with these default values in order to compare it with the performance of the calibrated model soil parameters were included in the sensitivity analysis and calibration due to the measured water and nitrate leaching data being collected at the edge of the field these measurements are impacted by characteristics of the whole field rather than a few points additionally the inclusion of soil parameters in sensitivity analysis and calibration reflects the variation that exists in the soil values reported in surrgo for the area of the field site in this study the model was calibrated using monthly water and nitrate leaching from 2014 to 2018 and annual crop yield data from 2013 to 2014 and validated using 2019 2020 monthly water and nitrate leaching and 2016 2017 yield data the use of yield data from 2013 to 2014 for calibration and 2016 2017 for validation reflects similar crop system rotations i e corn winter and wheat cover during these periods since water and nitrate leaching data were collected at an edge of the study site and the dndc model does not account for the transport of water and nutrient between locations within the field it was assumed that the dndc model would perform relatively well if calibrated with monthly data than when calibrated with daily data three calibration strategies simultaneous sequential and separate were employed to calibrate the dndc model the simultaneous approach used a multi component objective function to calibrate the model for the three dndc outputs i e water leaching nitrate leaching and crop yield for which observed data exists co currently in the sequential approach the model was calibrated for one output type by adjusting the most sensitive parameters corresponding to that output before calibrating for the next output variable the parameters were calibrated using water leaching first followed concurrently by nitrate leaching and yield water movement through the soil leads to the movement of nutrients and nutrients are leached out of the soil along with water hence correctly simulating water dynamics helps to ensure that n dynamics can be modeled well ma et al 2003 xu et al 2020 also calibrated soil water first followed by soil n and then crop growth the nitrate leaching and crop yield were calibrated together as crop yield is dependent on soil n availability through fertilizer inputs and mineralization additionally crop n export is critical in determining how much n is available in the soil for leaching relative sensitivity of 20 for parameter output sensitivities and expert opinion was used for a cutoff in delegating parameters to each output type used in calibration this 20 was arbitrarily chosen some parameters showed 20 relative sensitivity to multiple output groups e g cn pref2 por kdr etc in this case the optimized value of these parameters from water leaching was used as default values for calibration using nitrate leaching and yield lastly the model was calibrated separately for each output variable incorporating the entire parameter set each time the use of multiple output variables in the calibration and validation process presents the modeler with a selection of approaches that vary in distribution and order for which model parameters are adjusted and model fit to measured data is assessed 2 6 statistical analysis model calibration performance was quantified using the sum of weighted square residuals swsr index of agreement d mean bias nash sutcliffe efficiency nse absolute root mean square error rmse percent bias pbias and pearson s correlation coefficient r moriasi et al 2015 2 s w s r i 1 m w e i g h t e d y i y i 2 3 d 1 i 1 m y i y i 2 i 1 m y i y y i y 2 4 n s e 1 i 1 m y i y i 2 i 1 m y i y 2 5 r m s e 1 m i 1 m y i y i 2 6 p b i a s i 1 m y i y i 100 i 1 m y i 7 r i 1 m y i y y i y i 1 m y i y 2 i 1 m y i y 2 where subscript i represents the ith observation m is the total number of observations y is the observation y is the simulated value and y and y are the mean of measured and simulated data respectively swsr is interpreted by evaluating the change between two sets of simulated to measured data residuals with values closer to 0 indicating a smaller deviation of simulated outputs to measured data nse is a measure of the magnitude of residual variance relative to measured data variance and has a range of negative infinity to 1 with values closer to 1 indicating a good match between simulated and observed data and values equal to 0 or negative indicating poor model performance index of agreement d ranges from 0 to 1 with values nearer to 1 indicating greater agreement between simulated and measured data the scale used in smith et al 2019 was adopted with a d 0 9 indicating excellent agreement between simulated and measured values 0 9 d 0 8 indicating good agreement 0 8 d 0 7 indicating fair agreement and d less than 0 7 indicating poor agreement rmse is the standard deviation of residuals and ranges from positive infinity to 0 with values near 0 indicating model residuals concentrated around the measured data pbias is the average tendency of the simulated data to be larger or smaller than the measured data and is expressed as a percentage a value of 0 is optimal with low magnitudes indicating accurate model simulation pearson s correlation coefficient r describes the linear correlation between simulated and measured data with a range of 1 to 1 and a value of 0 indicating no linear relationship and either 1 or 1 indicating a perfect negative or positive linear relationship statistics were calculated on a monthly and weekly timescale for water and nitrate leaching and an annual basis for crop yield 2 7 model uncertainty estimation the impact of parameter uncertainties on model predictions was quantified using monte carlo simulation incorporating latin hypercube sampling lhs helton and davis 2003 mckay et al 1979 monte carlo simulation is a stochastic sampling method that generates many random sets of parameter values contained within user defined parameter bounds and assigns probability distribution functions pdf to each parameter to characterize the distribution of possible values within the parameter bounds triangular pdfs were assumed for all parameters included in the uncertainty analysis haan et al 1998 wang et al 2005 monte carlo analysis as a stand alone method can be computationally intensive thus lhs was incorporated to limit the number of parameters used in uncertainty analyses and reduce the total number of monte carlo simulation runs lhs integrates stratified sampling into random sampling by dividing each parameter s range into n intervals so that each interval has the same probability 1 n of being randomly sampled mckay et al 1979 tang et al 2007 pest2lhs was used to generate the input file for the lhs program in which the user defines conditions for generating a random parameter set including parameter pdfs and maximum value of non uniform distributions and a random number seed swiler and wyss 1998 the parameter sets are then generated by the lhs utility program finally lhs2pest generates separate parameter value files for each parameter set and a windows batch file that runs pest repeatedly utilizing all three main input files described in section 2 3 in this process the dndc model is run once with each set of parameter values and the corresponding model outputs are extracted from the model run files and recorded crop yield was excluded from uncertainty analysis due to limited observation data over the study period a sample size of 200 parameter sets was used for uncertainty analysis based on prior studies sieber and uhlenbrook 2005 that demonstrated similar statistical measurement values between lhs and random sampling for several tens of thousands of model runs when the sample size exceeds ten times the number of varied model parameters in this study 200 is about 14 times the number of varied model parameters i e 14 3 results 3 1 sensitivity analysis water leaching was most sensitive to model parameters that influence water runoff curve number cn crop growth i e maximum biomass of corn yd c thermal degree days tdc and grain c n ratio of corn cngc and tile drainage drain space ds drain depth dd fraction of n not preferentially leached pref2 saturated hydraulic conductivity factor kdr factor that controls nitrate movement with water flux nfac and soil parameters i e clay content cly and porosity por fig 3a nitrate leaching was also sensitive to these same sets of parameters but for nitrate leaching the highest sensitivities 80 relative sensitivities were shown by por nfac and pref2 fig 3b while for water leaching the highest sensitivities were shown by cn yd c ds cly and pref2 with respect to crop yield yd c was considerably more sensitive than all other parameters fig 3c across water and nitrate leaching and crop yield the most sensitive parameters were cn pref2 yd c por and ds fig 3d the other parameters grain c n ratio of wheat cngw and bulk density blk had distinctively low sensitivities compared to these parameters the implemented svd regularization changed the values of 12 of the 14 adjustable parameters supplied in the calibration for the sequential calibration approach while the two other parameters cngw and ds were stuck at the upper and lower bounds respectively parameters with the greatest relative change in value from the default values were cly cn yd c ds and kdr which had relative composite sensitivities 50 there were differences in optimized values between the simultaneous and the other calibration strategies fig 4 which impacted the dndc model performance for estimating water and nutrient leaching and crop yield for instance when comparing the optimized parameter values in sequential calibration to the optimized parameter values in the simultaneous approach there was a high variation 30 60 between the two sets of optimized parameters with high composite sensitivities e g dd ds kdr yd c which led to around 20 30 change in model performance across all output groups 3 2 model calibration and validation for simultaneous calibration dndc was run 364 times over 11 iterations in order to determine the optimal parameter values meanwhile for sequential calibration dndc was run 417 times over 14 iterations in total for the calibration period manual calibration had an improved performance overall for all three output groups of water and nitrate leaching and crop yield compared to other calibration methods table 4 table s4 this was followed by simultaneous calibration where the objective function i e swsr was reduced by 73 overall 48 water leaching 95 nitrate leaching and 61 crop yield both weekly table s3 and monthly statistics table 4 show that edge of field leaching is well represented by the model when evaluating the output groups individually simultaneous calibration had better statistical performance than other methods for water leaching during the calibration period it reduced the rmse by 23 compared to the default and had a positive nse value it was followed closely by separate calibration using just water leaching outputs which also reduced the rmse by 20 and had a positive nse value both of these methods had better statistical performance than the manual calibration which reduced the rmse by 2 compared to the default and had a very low positive nse value all the other calibration methods had slight increases in the rmse value compared to the default and a negative nse value separate calibration using just nitrate leaching outputs led to the poorest statistics for water leaching but it gave the best statistics for nitrate leaching outputs with a 57 reduction in rmse and double the nse compared to the default this was followed closely by the sequential calibration with a 49 reduction in rmse and 2 7 times the nse both methods performed as well as the manual calibration the poorest statistics for nitrate leaching were from the separate calibration using just yield outputs which increased the rmse by 8 compared to the default and reduced the nse by more than half for yield the best statistics were derived from the separate calibration of the parameters using just yield outputs which reduced the rmse by 84 compared to the default and had an nse value close to 1 which signifies a good match between observed and simulated outputs these statistics were better than the manual calibration this was followed by the separate calibration using just nitrate leaching outputs which reduced the rmse by 41 compared to the default and had an nse value of 0 57 compared to the default value of 0 3 nse values close to 1 indicate a good fit between observed and simulated outputs while values equal to 0 or negative means that the model performance is poor after model calibration the simulated results were compared with measured data year 2019 2020 for nitrate and water leaching and 2016 and 2017 for crop yield that were not included in the calibration steps for the validation period sequential calibration had the best model performance overall for all three output groups of water and nitrate leaching and crop yield followed by manual calibration table 4 table s4 looking at the individual output groups the statistics for water leaching were better in the validation period compared to the calibration period all of the calibration methods led to similar water leaching performance during the validation period for nitrate leaching the rmse values were lower during the validation period but the other statistics such as nse d and r were better during the calibration period compared to the validation period among the different calibration methods the sequential calibration method had the best statistics for nitrate leaching closely followed by the separate calibration by nitrate leaching outputs the simultaneous calibration method had a poorer performance compared to other methods during the validation period for nitrate leaching the sequential calibration method performed as well as the manual calibration hence it was able to both calibrate and validate the model well for nitrate leaching all of the methods simulated low nitrate leaching in the validation period which aligns with measurements explaining the low rmse the simultaneous calibration method performed well for yield outputs during the validation period with statistics better than manual calibration the simultaneous calibration method produced better yield statistics during validation compared to calibration sequential calibration also performed well during the calibration period with yield values close to the observed for both corn and winter wheat fig 7 however it did not perform well for corn yield during validation 3 3 temporal trends of simulated and observed data the monthly model performance metrics generally show good agreement with observed data during calibration and also during validation for water and yields nitrate levels were also generally well simulated during validation low rmse however the remaining statistics were poor which can be expected when nitrate levels are very low near the detection limit the observed temporal distribution of both water and nitrate leaching was captured in the calibrated dndc model fig 5 fig s2 fig s3 simulated water leaching fluxes followed rainfall events while nitrate leaching fluxes were dependent mainly on rainfall events and fertilization applications particularly in late spring seasons fig 5 the calibrated models had similar water and nitrate leaching peaks as the manually calibrated model the timing of water leaching fluxes was better captured during the validation period compared to the calibration period where the magnitude of the fluxes was only under predicted by 2 on average across calibration methods during the validation period compared to the 15 over prediction during the calibration period for nitrate leaching all the calibration methods led to an under prediction 19 45 of the peak nitrate leaching fluxes except for separate calibration by nitrate leaching outputs fig s3 which led to an over prediction of just 2 and had the best performance for nitrate leaching outputs there are no observed peak fluxes for nitrate leaching during the validation period which contrasts the multiple peak fluxes that occurred during the calibration period and contributed to the calibrated models having a lower performance metric for the validation period for both water and nitrate leaching manual calibration also had similar over and under predictions respectively if we look at the cumulative water leaching fig 6 there is a good fit between observed and calibrated outputs for both the calibration and validation periods there is a good fit between observed and all calibrated outputs till may 2015 then there is a slight overestimation by all calibration methods till october 2016 followed by a slight underestimation till october 2017 during the validation period there is a slight under estimation by calibrated models the underestimation is higher in simultaneous calibration compared to other calibration methods for cumulative nitrate leaching fig 6 manual calibration has a good fit with the observed data from 2014 to 2015 compared to other calibration methods that are underestimating nitrate leaching during that period from 2016 the default outputs have a very poor fit with the observed data there is a good fit between observed and all calibrated outputs from january 2016 to may 2016 but from may 2016 to may 2017 there is a larger underestimation by all calibration methods after that the simultaneous calibration method has a good fit with the observed data till february 2018 while the other methods are still underestimating nitrate leaching and the simultaneous calibration method overestimates nitrate leaching all the way through february 2018 to the validation period of 2019 and 2020 all the other calibration methods continue underestimating nitrate leaching looking at the different calibration models there was some difference in the timing of both water and nitrate peaks among them for water leaching when analyzing the simultaneous and sequential method the timing of peaks given by the calibration methods was different on five instances january 3 2014 january 7 2014 january 12 2014 january 1 2017 and january 12 2018 for nitrate leaching the timing was similar for all calibration methods except for one peak in separate calibration by yield fig s3 on january 7 2017 that was not present in the other methods separate calibration by yield outputs also had the poorest performance for nitrate leaching outputs for crop yield for both the calibration and validation period the default calibration had a much higher cumulative corn yield than the other calibration methods which have similar cumulative corn yield fig 7 for instance during the calibration period corn was planted on 5 21 2013 and harvested on 9 22 2013 and had the highest cumulative biomass of 27 020 kg ha 20 293 kg ha 19 953 kg ha and 18 493 kg ha for default manual simultaneous and sequential calibration respectively however for winter wheat the sequential calibration method had the highest cumulative biomass during the calibration period and the simultaneous calibration method had the lowest cumulative biomass for the validation period all methods had similar cumulative biomass except for the simultaneous calibration method which had a slightly lower cumulative biomass compared to the others winter wheat was planted on 10 18 2013 and harvested on july 7 2014 and had the highest cumulative biomass of 6890 kg ha 10 730 kg ha 2313 kg ha and 11 973 kg ha for default manual simultaneous and sequential calibration respectively 3 4 uncertainty analysis when the 95 confidence bounds at a monthly timescale were captured for water and nitrate leaching for the period of 2014 2020 most of the observed peak fluxes for water and nitrate leaching were contained within the uncertainty bounds fig 8 further 89 of the total 27 monthly water fluxes higher than 25 mm and 70 of the 10 nitrate leaching fluxes greater than 6 kg n ha were captured within the 95 confidence bounds 4 discussion 4 1 sensitivity analysis the systematic evaluation of the sensitivity analysis provided insights into the mechanistic relationships within a complex biogeochemical model that directly or indirectly affect outputs but are not obvious to model users ranked relative parameter sensitivities provided a comprehensive and objective basis to identify and prioritize the parameters and processes of the model that affect outputs of interest as reported in prior studies parameters such as curve number cn for water leaching musyoka et al 2021 clay content of soil cly zhao et al 2020 soil porosity por for nitrate leaching li et al 2014 corn accumulative temperature tdc and corn grain c n ratio cngc liang et al 2017 trybula et al 2015 for yield table 3 were identified to be sensitive drain space ds is also found to be sensitive to water leaching as the water flow rate from a tile drained agricultural system is inversely proportional to the square of drain space ward et al 2015 as the curve number is used to determine the amount of rainfall that goes to direct runoff the high sensitivity of water leaching to cn is explained crop evapotranspiration and water leaching are the two main pathways that remove water from the simulated soil space since increased plant growth has a proportional demand on soil water extracted for plant transpiration the maximum biomass of corn i e yd c parameter has a high sensitivity to water leaching soil water porosity at field capacity is used to calculate the volume of water held in soil pore space nitrate leaching is sensitive to porosity as it increases the volume of water and nutrients that can be transported through the soil profile li et al 2014 similarly a model factor controlling the fraction of nitrate mobilized in a layer that can move with hourly layer water fluxes nfac was a sensitive nitrate leaching factor model factor pref2 was also sensitive as it controls the amount of nitrate that preferentially leaches through the soil there is sensitivity of nitrate leaching to cn as well because nitrate moves along with the water flux and cn controls the runoff these parameters such as nfac and pref2 impact the distribution and availability of water and nutrients to crop roots for uptake and thus indirectly affect crop growth the maximum biomass of corn yd c was found to directly impact annual crop yield other parameters may also be sensitive to water leaching nitrate leaching and crop yield but the number of parameters was kept low because the number of observations for calibration just ranged from 2 to 122 in a study by perin et al 2020 the performance was better when the number of observations was equal to 6 times the number of adjustable parameters for the simultaneous calibration method the number of observations was roughly 8 times the number of adjustable parameters while for the sequential approach they were roughly 5 times the number of adjustable parameters hence only a couple of sensitive parameters were calibrated 4 2 calibration and validation the use of pest to calibrate the dndc model using the mathematical principles of inverse modeling improved the model s ability to simulate nitrate leaching and crop yield over the default model as well as provided an objective approach for calibration both the calibrated and validated models captured the timing of the water and nitrate leaching peaks relatively well during the calibration period manual calibration led to the best model performance for all output groups overall and the simultaneous calibration method had comparable results to the manual calibration based on model performance metrics such as the d value the simultaneously calibrated model had an excellent match for monthly nitrate leaching and annual crop yield and a good match for water leaching during the calibration period similarly looking at nse the model had good or acceptable performance while simulating monthly water and nitrate leaching and annual crop yield respectively he et al 2019 zhang et al 2021a for water leaching outputs only simultaneous calibration had higher nse values than manual but for nitrate leaching and crop yield manual calibration had higher nse values both were in the acceptable range for nse for all output groups and had excellent matches for nitrate leaching and yield and a good match for water leaching during the calibration period monthly water and nitrate leaching and annual crop yield had r values of 0 7 0 6 and 1 0 respectively which indicate the simultaneously calibrated model matched the pattern of measured data the range of r values reported in tonitto et al 2007 between simulated and observed water and nitrate leaching during calibration 0 55 to 0 8 and 0 54 to 0 76 respectively were found to be comparable to the r values of the current study albeit the ranges were reported at an annual scale which is more easily simulated by a model however during the validation period sequential calibration had the best model performance followed by manual calibration this demonstrates that sequential calibration could achieve similar model performance as manual calibration with the benefit of a lot fewer iterations and reduced expert oversight monthly nse and d values for nitrate leaching 0 83 and 0 96 were comparable to those reported in he et al 2019 where the nse and d values were 0 24 and 0 77 respectively as they were reported on a daily scale statistically the calibrated models struggled to maintain improvement during the validation period for nitrate leaching but maintained improved performance for water leaching and crop yield for water leaching the sequentially calibrated model had nse and d values of 0 8 and 0 95 respectively leading to good and excellent performance this was comparable to smith et al 2020 where the nse and d values for water leaching during the validation period were 0 74 and 0 92 respectively necpálová et al 2015 had also used inverse modeling to calibrate the daycent model using an extensive set of measured data over a three year period and most of the calibrated outputs performed well during the validation period for nitrate leaching the model was calibrated during times of peak fluxes while the observed data during the validation period had low nitrate leaching flows with no peaks the model simulated similar leaching levels with low rmse but other performance statistics were poor which is characteristic of n leaching levels that are near the measurement detection threshold as the data for validation was limited in this study future work can look at a wider range of data for validation with the presence of both peak and non peak flows if we take into account the overall effect on all three output groups the statistics were better during the calibration period compared to the validation period table s4 which was likewise observed by lamers et al 2007 and rafique et al 2013 however both the calibrated and validated models improved the model performance for all three output groups compared to the default model simulation dutta et al 2018 improved the model timing of leaching events during freezing and thawing periods in the dndcvcan model by incorporating and revising the effects of crop residue and snow cover on soil temperature these improvements are evident in this study as the calibrated model closely matched major and minor leaching events for both nitrate and water during spring thawing periods march and freezing december the canadian version of the dndc model used in this study has shown an improved simulation of monthly nitrogen leaching to the tiles smith et al 2020 hence the performance was good for nitrate leaching as the site used in this study is tile drained 4 3 optimization of parameters during calibration a total of 14 soil crop and tile drainage parameters were optimized for the calibration of the dndc model using pest for sequential calibration 12 out of 14 adjustable parameters were optimized while parameters such as corn grain c n ratio cngc and drain space ds reached their bounds similarly for simultaneous calibration 10 out of 14 adjustable parameters were optimized while parameters such as maximum biomass of corn yd c n not preferentially leached pref2 drain depth dd and ds reached their bounds these parameters reached their bounds for other calibration methods as well and had relative composite sensitivities of 30 they were kept at their bounds by pest as pest did not find another optimal set that further lowered the objective function and the algorithmic determined parameter adjustment in earlier iterations may have exceeded the user defined upper or lower parameter value bounds while widening the bounds may have further improved the calibration performance it would have resulted in parameterization outside of justifiable ranges in a study by wangwang et al 2019 it was also found that widened ranges of some hydrological parameters led to values that were not appropriate in the physical world hence widening the bounds was not an acceptable procedure for some model parameters while manual calibration is often complex and benefits from the inherent knowledge of parameter interdependencies automatic calibration is easier to employ as its only guiding principle is on improving a performance measure mcmillan 2021 from our study we saw that automatic calibration led to a similar model performance as manual calibration in terms of model to measured fit and statistics such as nse d and rmse while it significantly cuts down the number of iterations required up to 200 for manual calibration compared to just 11 for automatic calibration by pest care needs to be taken with automatic calibration because it does not consider parameter interdependencies and thus can result in unrealistic parameter optimizations calibrating the model using a manual approach first can help narrow the parameter ranges for automatic calibration and can help identify sensitive parameters while following up with an automatic sequence can find the best model to measured fit within those ranges jeong et al 2010 thus the results from automatic calibration can be more useful when guided by the user s understanding of the agronomic and soil processes and field management practices but it is also a valuable tool for novice model users for whom it may be necessary to use automatic calibration solely 5 conclusion the study integrated pest with dndc to facilitate automatic calibration of the model as well as gain insights into the sensitivity of model parameters on model outputs such as water leaching nitrate leaching and crop yields sensitivity analysis provided a comparison of relative composite sensitivities for individual model outputs and identified the most influential model parameters for accurate simulation of water and nutrient cycling in agricultural soils the manual calibration method led to the best model performance during the calibration period followed by simultaneous calibration that reduced the total sum of weighted square residuals by 73 for the validation period the sequential calibration method had the best model performance followed by manual calibration there was limited data for validation for nitrate leaching for which a mixture of peak and non peak flow data would have been ideal for evaluating model performance the model was able to emulate the low n leaching which occurred during the period but other datasets could be used in the future to further test the calibration techniques another option is to expand the assessments across a wider array of model input parameters which might provide more insight into parameter relationships impacting model performance the application of inverse modeling proved to be a powerful statistics based approach for calibration which allowed for an objective examination of many fundamental processes captured by a highly parameterized model the comparison of calibration strategies highlighted the importance of the type of measurements needed to calibrate the model depending on the targeted model outputs for instance for nitrate leaching the best model performance was obtained by the separate calibration of nitrate leaching hence it may be possible to narrow down the type of calibration method to use depending on the modeling objective automatic calibration offered advantages in terms of time saving while reaching a similar model to measured fit compared to manual calibration however care needs to be taken during automatic calibration to ensure that the parameter values make sense in the real world and should be aided by the understanding of soil and agronomic processes whenever possible software and data availability software name denitrification decompostion dndc developers brian b grant and ward smith contact address brian grant agr gc ca ward smith agr gc ca program language c software name pestdevelopers john doherty contact address https pesthomepage org contact us year first available 2015 program language fortran availability https pesthomepage org software 0 cost free declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment the authors are grateful for the support of this project by funding from the united states department of agriculture nifa award no 2017 67021 26141 and 2019 67019 29310 hatch project nc1195 and the sustainability research seed grant program the authors would also like to acknowledge the help from anna apostel haley stockham elizabeth callow asmita murumkar and grey evenson in the preparation of weather and management practice data for model runs appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105494 
25516,calibration and validation 2 6 statistical analysis 2 7 model uncertainty estimation 3 results 3 1 sensitivity analysis 3 2 model calibration and validation 3 3 temporal trends of simulated and observed data 3 4 uncertainty analysis 4 discussion 4 1 sensitivity analysis 4 2 calibration and validation 4 3 optimization of parameters during calibration 5 conclusion software and data availability acknowledgment appendix a supplementary data americansocietyofagricultural 2015 1 designconstructionsubsurfacedrainagesystemsagriculturallandsinhumidareas babson 2020 d systemsformonitoringanalyticsforrenewabletransportationfuelsagriculturalresourcesmanagementofficenepapolicycompliance banger 2020 137851 k brighenti 2019 103 113 t brimhall 1987 567 587 g congreves 2016 179 189 k doherty 2015 j doherty 2018 j pestmodelindependentparameterestimationusermanualpartipestsensanglobaloptimisers doherty 2021 j pestroadmapsroadmap10modelcalibration donner 2008 4513 4518 s dutta 2018 54 72 b francis 2006 g wintermanagementmaizepaddocks franssen 2003 281 295 h haan 1998 65 70 c halopka 2018 r he 2018 187 198 w he 2019 1016 w helton 2003 23 69 j hill 1998 m documentationucodeacomputercodeforuniversalinversemodeling ingraham 2019 79 87 p 2012 usersguidefordndcmodel izaurralde 2012 r developmentapplicationepicmodelforcarboncyclegreenhousegasmitigationbiofuelstudiespnnlsa83721 jeong 2010 4505 4527 j jones 1986 c ceresnmaizeasimulationmodelmaizegrowthdevelopment kamkar 2014 b kohne 2006 1 32 j krobel 2011 503 520 r lamers 2007 52 58 m li 1992 9759 9776 c li 2000 4369 4384 c li 2006 116 130 c li 2012 163 200 c li 2014 108 118 h liang 2017 201 210 h liu 2019 x greetanalysisforterrarootssuccessscenarios liu 2022 107325 f ma 2003 39 l malone 2010 1711 1723 r malone 2014 10 22 r mckay 1979 239 245 m mcmillan 2021 e1499 h miskewitz 2007 r soilwaterassessmenttoolswat121 moriasi 2015 1609 1618 d musyoka 2021 f 2021 powerproject 2019 annualmaps necpalova 2015 110 130 m 2021 prismclimatedata parton 1998 35 48 w perin 2020 374 r perlman 2013 183 192 j pribyl 2010 75 83 d qin 2013 26 36 x rafique 2013 r rafique 2014 106 114 r sabo 2019 3104 3124 r sieber 2005 216 235 a smith 2013 139 150 w smith 2019 3 30 w 2019asabeannualinternationalmeetingjuly towardsimprovingdndcmodelforsimulatingsoilhydrologytiledrainage smith 2020 104577 w swangjang 2015 36 40 k swiler 1998 l ausersguidesandiaslatinhypercubesamplingsoftware tabatabaie 2018 428 439 s tang 2007 793 817 y tonitto 2007 51 63 c trybula 2015 1185 1202 e 2015 websoilsurveynaturalresourcesconservationservice wang 2005 1041 1054 x wang 2019 3523 3539 ward 2015 161 222 a environmentalhydrologyissue2015 runoffdrainage williams m xu 2020 105904 j zhang 2015 388 398 y zhang 2021 112640 j zhang 2021 114701 j zhao 2015 q annualmeetinggermansocietyforphotogrammetryremotesensinggeoinformation regionalapplicationsitespecificbiochemicalprocessbasedcropmodeldndcforriceinnechina zhao 2020 104587 z bhattaraix2022x105494 bhattaraix2022x105494xa 2023 08 11t00 00 00 000z http www elsevier com open access userlicense 1 0 https vtw elsevier com content oragreement 10138 chu doa publishacceptedmanuscriptindexable 2023 08 11t00 00 00 000z http creativecommons org licenses by nc nd 4 0 2022 published by elsevier ltd 2022 09 21t04 48 13 380z http vtw elsevier com data voc addontypes 50 7 aggregated refined 0 https doi org 10 15223 policy 017 https doi org 10 15223 policy 037 https doi org 10 15223 policy 012 https doi org 10 15223 policy 029 https doi org 10 15223 policy 004 item s1364 8152 22 00197 9 s1364815222001979 1 s2 0 s1364815222001979 10 1016 j envsoft 2022 105494 271872 2022 10 04t10 15 39 547552z 2022 11 01 2022 11 30 1 s2 0 s1364815222001979 main pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 main application pdf 7f7d418b03dd8e837c0237e447805101 main pdf main pdf pdf true 7976300 main 15 1 s2 0 s1364815222001979 main 1 png https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 preview image png c4d955b583558ef48e98696219d30f4f main 1 png main 1 png png 55855 849 656 image web pdf 1 1 s2 0 s1364815222001979 fx1 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 fx1 downsampled image jpeg 940b65e4b04619eecf982c1e3ddfa4ff fx1 jpg fx1 fx1 jpg jpg 209658 388 624 image downsampled 1 s2 0 s1364815222001979 gr1 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr1 downsampled image jpeg 054ffe602fbc66641772dbf74e05dcfe gr1 jpg gr1 gr1 jpg jpg 181696 605 535 image downsampled 1 s2 0 s1364815222001979 gr4 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr4 downsampled image jpeg 80e1cab0d9edd883d98e7cc3fc561fe3 gr4 jpg gr4 gr4 jpg jpg 168132 548 691 image downsampled 1 s2 0 s1364815222001979 gr5 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr5 downsampled image jpeg 0465c026fa149852f725425dc37cbe45 gr5 jpg gr5 gr5 jpg jpg 153471 347 535 image downsampled 1 s2 0 s1364815222001979 gr2 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr2 downsampled image jpeg 2e1374ab158b7868bb03de56942d5a56 gr2 jpg gr2 gr2 jpg jpg 132266 384 389 image downsampled 1 s2 0 s1364815222001979 gr3 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr3 downsampled image jpeg c2a30c209575df0eee842201c9f00a01 gr3 jpg gr3 gr3 jpg jpg 150216 653 535 image downsampled 1 s2 0 s1364815222001979 gr8 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr8 downsampled image jpeg 73545a452e861b6ddcd1036a719dbac2 gr8 jpg gr8 gr8 jpg jpg 103084 249 535 image downsampled 1 s2 0 s1364815222001979 gr6 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr6 downsampled image jpeg 45e114a254c48c2490bb9d7eaa1ee21d gr6 jpg gr6 gr6 jpg jpg 141154 516 535 image downsampled 1 s2 0 s1364815222001979 gr7 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr7 downsampled image jpeg 0486e6f066d00bd6b46b59cf88f82d5c gr7 jpg gr7 gr7 jpg jpg 101611 235 535 image downsampled 1 s2 0 s1364815222001979 fx1 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 fx1 thumbnail image gif 051e9ad6c253a96f09bbc9f55573622c fx1 sml fx1 fx1 sml sml 86687 136 219 image thumbnail 1 s2 0 s1364815222001979 gr1 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr1 thumbnail image gif 89b47a5485a25c5b953a36f09f011a29 gr1 sml gr1 gr1 sml sml 83071 164 145 image thumbnail 1 s2 0 s1364815222001979 gr4 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr4 thumbnail image gif 24edc2b605c79407d30ed37b64e1f472 gr4 sml gr4 gr4 sml sml 79531 164 207 image thumbnail 1 s2 0 s1364815222001979 gr5 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr5 thumbnail image gif 2b5b6d120ac7e42127ca0fcb9cb4c495 gr5 sml gr5 gr5 sml sml 80963 142 219 image thumbnail 1 s2 0 s1364815222001979 gr2 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr2 thumbnail image gif 18651af9f88c5a200c0c9c8a3f0c2823 gr2 sml gr2 gr2 sml sml 82188 164 166 image thumbnail 1 s2 0 s1364815222001979 gr3 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr3 thumbnail image gif bac3b6511405896072155a264d145d55 gr3 sml gr3 gr3 sml sml 75568 164 134 image thumbnail 1 s2 0 s1364815222001979 gr8 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr8 thumbnail image gif b7a3dd92d6ae57ac8a59b75f8f59eaef gr8 sml gr8 gr8 sml sml 71964 102 219 image thumbnail 1 s2 0 s1364815222001979 gr6 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr6 thumbnail image gif 1fcdc56e4bab30dda2e3da1f00aa9326 gr6 sml gr6 gr6 sml sml 77073 164 170 image thumbnail 1 s2 0 s1364815222001979 gr7 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 gr7 thumbnail image gif 4f67176a136431bf9495aae2a9497cbc gr7 sml gr7 gr7 sml sml 71632 96 219 image thumbnail 1 s2 0 s1364815222001979 fx1 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 highres image jpeg e39d8b0eb684864d4e6d5b9940f51734 fx1 lrg jpg fx1 fx1 lrg jpg jpg 1124793 1718 2764 image high res 1 s2 0 s1364815222001979 gr1 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 highres image jpeg beef9229cea289f25831db82e142446f gr1 lrg jpg gr1 gr1 lrg jpg jpg 890698 2679 2370 image high res 1 s2 0 s1364815222001979 gr4 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 highres image jpeg 16f57afcedadc9b0008383eef5392657 gr4 lrg jpg gr4 gr4 lrg jpg jpg 840274 2425 3058 image high res 1 s2 0 s1364815222001979 gr5 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 highres image jpeg b1beb63d767567533b1a1c208f150688 gr5 lrg jpg gr5 gr5 lrg jpg jpg 687135 1537 2370 image high res 1 s2 0 s1364815222001979 gr2 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 highres image jpeg 76e0502f775b9db87bbf907b6e64d645 gr2 lrg jpg gr2 gr2 lrg jpg jpg 444142 1700 1722 image high res 1 s2 0 s1364815222001979 gr3 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 highres image jpeg bfcabf4da3cb30b41810e6a92e5b5e52 gr3 lrg jpg gr3 gr3 lrg jpg jpg 544972 2892 2370 image high res 1 s2 0 s1364815222001979 gr8 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 highres image jpeg 3786b9807cb9da9f8b87f12f153bba2a gr8 lrg jpg gr8 gr8 lrg jpg jpg 300133 1102 2371 image high res 1 s2 0 s1364815222001979 gr6 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 highres image jpeg 96dd3a6536ba72072150be2f43e01cc1 gr6 lrg jpg gr6 gr6 lrg jpg jpg 515885 2284 2370 image high res 1 s2 0 s1364815222001979 gr7 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 highres image jpeg ea8079872473cd656ccceef38b42b0de gr7 lrg jpg gr7 gr7 lrg jpg jpg 318142 1043 2370 image high res 1 s2 0 s1364815222001979 mmc1 docx https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 mmc1 main application vnd openxmlformats officedocument wordprocessingml document 3c06b47a60f563b21354d08209c8353c mmc1 docx mmc1 mmc1 docx docx 553130 application 1 s2 0 s1364815222001979 si9 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 image svg xml d55965687a16f7845db0d287848a7f8e si9 svg si9 si9 svg svg 7357 altimg 1 s2 0 s1364815222001979 si7 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 image svg xml 36509ea894c130bf2200e047ac72f7ee si7 svg si7 si7 svg svg 54588 altimg 1 s2 0 s1364815222001979 si2 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 image svg xml b13ab6eb684f169eb173930d95cfaeac si2 svg si2 si2 svg svg 91492 altimg 1 s2 0 s1364815222001979 si6 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 image svg xml e45b4b586a1611816a8764147f0f5ecd si6 svg si6 si6 svg svg 71161 altimg 1 s2 0 s1364815222001979 si3 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 image svg xml f98afecad7eb662e4894701d31c28cd5 si3 svg si3 si3 svg svg 48211 altimg 1 s2 0 s1364815222001979 si4 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 image svg xml 19b29430f1d81a48404b8fbd0c42eaa0 si4 svg si4 si4 svg svg 60804 altimg 1 s2 0 s1364815222001979 si1 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 image svg xml d456c3831e4dcc7f62bae35560888314 si1 svg si1 si1 svg svg 58084 altimg 1 s2 0 s1364815222001979 si5 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 image svg xml 068167a39f15c2e40f757fd446dc0a93 si5 svg si5 si5 svg svg 81357 altimg 1 s2 0 s1364815222001979 si8 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 image svg xml 65c44ff6bdd8b5f4572a942293a08a3b si8 svg si8 si8 svg svg 2363 altimg 1 s2 0 s1364815222001979 si11 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 image svg xml d148c100302fe70399da1ac341e8bc98 si11 svg si11 si11 svg svg 3153 altimg 1 s2 0 s1364815222001979 si10 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222001979 image svg xml ebba7251c80406184c391cbcec36ac5d si10 svg si10 si10 svg svg 9338 altimg 1 s2 0 s1364815222001979 am pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content egi 106qhctm1h7 main application pdf 9d85fb5442e6dcb7394169d7e2bdd06b am pdf am am pdf pdf false 1824938 aam pdf enso 105494 105494 s1364 8152 22 00197 9 10 1016 j envsoft 2022 105494 fig 1 location of the study area within the sub basin of maumee river watershed fig 1 fig 2 flow chart diagram of the algorithmic calibration of dndc using pest fig 2 fig 3 bar plot of dndc relative composite sensitivity to individual observation groups a water leaching b nitrate leaching c crop yield and d the entire composite of all outputs fig 3 fig 4 optimized values of the 14 parameters for each calibration strategy included in the sensitivity analysis in simultaneous calibration it took eleven optimization iterations to optimize swsr units for each parameter s y axis are given in parenthesis after the parameter name and can also be found in table 3 fig 4 fig 5 dndc simulated values simultaneous and sequential and field observations for water leaching top nitrate leaching middle and weather data bottom fig 5 fig 6 cumulative water and nitrate leaching from 2014 to 2020 for observed default and calibrated outputs fig 6 fig 7 dndc simulated values and field observations for cumulative crop biomass yield during calibration and validation period fig 7 fig 8 95 prediction bounds and field observations for monthly water leaching top and nitrate leaching bottom fig 8 table 1 dndc climate and soil input data and their sources table 1 category input data climate daily maximum and minimum temperature precipitation northwest alliance for computational science engineering 2021 radiation humidity and windspeed nasa langley research center larc power nasa langley research center power project 2021 n concentration in rainfall national atmospheric deposition program 2019 soiltopsoil properties soil structure scs musle soil texture bulk density porosity a ph field capacity wilting point hydro conductivity clay fraction soc b field slope usda 2015 curve number cn manning s roughness coefficient miskewitz 2007 tile drainage components tile drain depth drain space american society of agricultural and engineers 2015 a porosity was determined using bulk density and particle density 2 56 g cm 3 brimhall and dietrich 1987 q zhao et al 2015 b soc was calculated using som from web soil survey and assuming a soc to som ratio of 0 58 pribyl 2010 table 2 management and crop rotation details of the edge of field study site table 2 2014 2015 2016 2017 2018 2019 2020 crop management jul 2 aug 1 may 21 jul 4 jun 30 jun 7 may 6 clover p a clover t c corn p wheat h alfalfa h alfalfa t corn p jul 7 sept 4 sept 22 aug 28 aug 5 jun 26 sept 15 wheat h b radish p corn h alfalfa p alfalfa h sorghum p corn h oct 18 aug 30 sept 21 wheat p sorghum h wheat p sept 10 cereal rye p fertilizer and manure management oct 17 may 21 jun 16 mar 29 jul 6 gypsum fert 9337 kg ha jul 5 may 6 n fert n fert 52 kg ha n fert sept 17 n fert 42 kg ha 17 kg ha 78 kg ha manure n 1726 kg ha jun 15 aug 23 n fert 81 kg ha manure n sept 23 313 kg ha manure n 1726 kg ha tillage management aug 2 25 aug 16 chisel chisel oct 17 aug 22 field cultivator disc harrow aug 27 field cultivator a refers to planting b refers to harvest c refers to termination table 3 initial upper lower and optimized values from sequential calibration of dndc adjustable parameters included in the inverse modeling calibration process table 3 parameter description reference units lower bound higher bound default manual optimized cly clay content of topsoil 0 10 cm ssurgo fraction 0 4 0 6 0 49 0 58 0 588 por soil porosity ssurgo fraction 0 482 0 65 0 5 0 602 0 5024 blk bulk density of topsoil field g cm³ 1 1 45 1 28 1 02 1 427 cn soil conservation service scs curve number miskewitz 2007 ratio 64 95 70 77 7 85 6 hdc c n ratio of humads pool swangjang 2015 c n ratio 5 12 8 8 299 8 234 yd c maximum yield corn typical of region kg c ha 3000 6000 4000 3044 3084 6 cngc grain c n corn francis et al 2006 c n ratio 20 60 30 23 05 28 76 tdc thermal degree days to reach crop maturity corn halopka 2018 zero c 2000 3000 2400 2512 2806 cngw grain c n ratio winter wheat kamkar et al 2014 ratio 20 60 50 54 14 60 dd tile drain depth in the 2m profile american society of agricultural and engineers 2015 m 0 5 1 2 0 9 0 684 0 752 ds tile drain spacing american society of agricultural and engineers 2015 m 9 18 13 15 3 9 kdr effective horizontal rate of saturated conductivity to the tiles as a function of veritical ksat smith et al 2020 factor 0 5 2 5 0 7 1 965 0 881 nfac fraction of nitrate that moves with water fluxes within layers user guide dndcv can 2020 fraction 1 3 5 1 6 3 2 913 pref2 fraction of n that does not preferentially leach out of the soil profile through macropore flow user guide dndcv can 2020 fraction 0 85 1 0 95 0 9 0 8538 note the study site has two soil map units table 1 and the values for soil parameters were varied to reflect potential in field variability vol volumetric the optimized values are for the sequential calibration approach table 4 monthly calibration and validation statistics by calibration approaches for all measured output types table 4 development of a calibration approach using dndc and pest for improving estimates of management impacts on water and nutrient dynamics in an agricultural system abha bhattarai a 1 garrett steinbeck a 1 brian b grant b margaret kalcic a c kevin king d ward smith b nuo xu f jia deng e sami khanal a a department of food agricultural and biological engineering the ohio state university columbus oh 43210 usa department of food agricultural and biological engineering the ohio state university columbus oh 43210 usa department of food agricultural and biological engineering the ohio state university columbus oh 43210 usa b ottawa research and development centre aafc 960 carling ave ottawa k1a 0c5 canada ottawa research and development centre aafc 960 carling ave ottawa k1a 0c5 canada ottawa research and development centre aafc 960 carling ave ottawa k1a 0c5 canada c biological systems engineering university of wisconsin madison madison wi 53706 usa biological systems engineering university of wisconsin madison madison wi 53706 usa biological systems engineering university of wisconsin madison madison wi 53706 usa d u s department of agriculture agricultural research service soil drainage research unit columbus oh 43210 usa u s department of agriculture agricultural research service soil drainage research unit columbus oh 43210 usa u s department of agriculture agricultural research service soil drainage research unit columbus oh 43210 usa e earth systems research center institute for the study of earth oceans and space university of new hampshire durham nh 03824 usa earth systems research center institute for the study of earth oceans and space university of new hampshire durham nh 03824 usa earth systems research center institute for the study of earth oceans and space university of new hampshire durham nh 03824 usa f department of computer science and engineering the ohio state university columbus oh 43210 usa department of computer science and engineering the ohio state university columbus oh 43210 usa department of computer science and engineering the ohio state university columbus oh 43210 usa corresponding author 590 woody hayes drive columbus oh 43210 usa 590 woody hayes drive columbus oh 43210 usa 1 these authors contributed equally to this work calibration and validation are standardized practices to establish the credibility of biogeochemical models for understanding agroecosystem nutrient dynamics we evaluated three automatic calibration approaches including simultaneous sequential and separate for the calibration of model parameters of the biogeochemical denitrification decomposition dndc model through inverse modeling using pest open source parameter estimation and uncertainty analysis software while manual calibration by experts performed the best during calibration period followed by simultaneous calibration sequential calibration had the best model performance during the validation period model sensitivity analyses demonstrated water leaching to be sensitive to curve number and drain spacing nitrate leaching to be sensitive to porosity and clay content and corn yield to be sensitive to accumulative temperature and grain c n ratio while some level of expertise is required to inform the automated calibration procedure it represents a more efficient and robust approach toward increasing the performance of biogeochemical models keywords agroecosystem model calibration dndc inverse modeling pest data availability data will be made available on request 1 introduction agriculture in the united states accounts for 75 of the nation s n2o emissions and is also a significant contributor to nutrient loading donner and kucharik 2008 sabo et al 2019 negatively affecting ecosystem services pribyl 2010 reducing greenhouse gas ghg emissions and improving water quality require agricultural producers to adopt new technologies and management practices state and federal funding programs e g nrcs environmental quality incentives program eqip and conservation stewardship program csp liu et al 2019 as well as voluntary market based carbon credit systems e g verified carbon standard promote practices and technology adoption to meet water quality goals and enhance soil carbon sequestration however there is much to be understood about the effects of conservation practices on ecosystem services in varied and complex environments babson 2020 although conventional approaches involving in field measurements of agricultural emissions from various practices and technologies are useful they are often also cost prohibitive and time consuming as a result recommendations for current conservation practices are based on limited field i e low resolution observations with high uncertainty ecosystem models when evaluated with measured data are powerful tools for estimating the impact of management practices on nutrient fluxes drainage and runoff they are cost effective and time efficient and hence can be used as a tool to make decisions about conservation agricultural practices over the last two decades a number of ecosystem models such as daycent parton et al 1998 denitrification decomposition dndc li et al 1992 epic izaurralde et al 2012 and ceres egc jones and kiniry 1986 have been developed to quantify nutrient fluxes such as ghgs nitrate leaching and runoff at the field and regional scales these agroecosystem models are composed of inter dependent process based equations representing fundamental physical chemical and or biological ecosystem processes necpálová et al 2015 thus these models generally utilize a high number of input parameters that require calibration and validation to produce meaningful results the utility of these models depends primarily on how well they have been parameterized and calibrated dndc is a well recognized model that has been used globally to simulate nutrient fluxes and runoff from various ecosystems and management practices banger et al 2020 liu et al 2022 zhang et al 2015 it has been shown to simulate nutrient fluxes such as nitrate leaching relatively well ingraham and salas 2019 zhang et al 2021a zhang et al 2021b however it has primarily been calibrated manually using a trial and error method although useful in defining parameter ranges for calibration to a specific ecosystem the trial and error approach is a laborious process as it needs to be iterated through many times to provide insight into model behavior it can also be subjective and unreliable because the proper calibration of a model is highly dependent upon prior experience and user knowledge and not all users have the same level of insights and experience with the model rafique et al 2013 another drawback to manual calibration is that some degree of uncertainty remains as to whether the optimal set has been achieved as a local optimum calibration is obtained inverse modeling which employs algorithmic search methods guided by statistical measures to objectively adjust parameters and assess the model s fit to measured data is one approach to overcome the limitations of manual calibration inverse modeling accelerates the identification of combined parameter effects on simulated outputs reducing the required time to gain insight into model dynamics although inverse modeling has been commonly utilized for parameter optimization and sensitivity analysis of hydrological and geophysical models franssen et al 2003 köhne et al 2006 it has been applied successfully to only a few biogeochemical models such as daycent rafique et al 2013 2014 rzwqm malone et al 2010 2014 forest dndc lamers et al 2007 and dndc specifically for wheat growth kröbel et al 2011 lamers et al 2007 used an algorithm search method based on the ucode program hill and poeter 1998 originally designed to optimize the groundwater model to automate the calibration of forest dndc a specific adaption of the main dndc to forest ecosystems li et al 2000 and obtained considerable model simulation improvement of n2o there are several input parameters required by the dndc model that can vary spatially and temporally which if kept constant across soil climatic regions can significantly affect model predictions calibration and validation of the model are thus required prior to its application to provide reasonable estimates of ecosystem services including soil carbon ghg fluxes and crop yield under what if management and climate change scenarios in addition to model calibration and validation uncertainty analysis is critical to offer insights on uncertainties in the model output that arises from uncertainties in the model parameters there are several approaches to model calibration including simultaneous sequential brighenti et al 2019 musyoka et al 2021 and separate the simultaneous calibration approach involves calibrating multiple parameters at once based on ground truth observations for multiple output variables sequential calibration on the other hand involves calibrating multiple parameters for multiple output variables in a stepwise manner in this process the most sensitive parameters for one output variable are calibrated first and the calibrated parameters from that step are used to calibrate for another output variable a study by musyoka et al 2021 found that the sequential approach worked better for streamflow prediction while a study by brighenti et al 2019 found the simultaneous approach to be better for estimating streamflow and suspended sediment both of these studies used the soil water assessment tool swat model meanwhile a separate calibration approach involves calibrating single multiple parameters based on ground truth observations for a single output variable these different calibration methods may lead to varied model performance depending on the model output of interest hence it is important to assess which calibration methods work best for calibrating the model of interest as well as the model output the objectives of this study were to 1 develop a framework that guides automatic calibration of the dndc model using inverse modeling and 2 demonstrate and compare calibration approaches manual separate simultaneous sequential within the framework to simulate water and nitrate leaching and crop yield in an agricultural system this unique effort is the first attempt to calibrate the dndc model for nitrate leaching using inverse modeling and offers an approach for rigorously estimating the uncertainty of model outputs 2 methods 2 1 study site and data the field site 24 7 ha in area is located in the north powell creek which is an huc 12 sub watershed of the maumee river basin fig 1 and is part of the usda agricultural research service edge of field network williams et al 2016 the site has 97 paulding clay pa and 3 roselms silty clay loam rsa during the study period the site was managed in a diverse crop rotation involving corn wheat alfalfa sorghum cereal rye and radish cover crop the field is systematically tile drained with lateral spacing every 17 5 m and since 2013 the tile drainage from the field has been continuously monitored tillage at the site is defined as conventional multiple tillage events in some year while nutrient application occurred annually prior to corn planting nitrogen n in the form of urea ammonium nitrate uan was applied while liquid dairy manure was applied after wheat harvest and prior to cover crop planting table 1 inputs required for the dndc model include i daily precipitation cm maximum and minimum temperature degree celsius radiation mj m2 day humidity windspeed m s ii initial topsoil top 10 cm physical and chemical properties i e soil texture percent clay bulk density g cm³ ph soc kg c kg hydraulic conductivity m hr etc table 1 iii crop parameters including biomass fraction and c n ratios of leaf stem root and grain components water demand g water g dry matter and optimal temperature degree celsius iv management practices table 2 and v tile drainage components i e drain spacing m depth m and tile radius m etc these data were obtained from various sources weather data precipitation and temperature was collected from the nearest meteorological station id usc00336342 from the prism database northwest alliance for computational science engineering 2021 and radiation humidity and windspeed data were available from the nasa power project nasa langley research center larc power nasa langley research center power project 2021 bulk density and ph values were available directly from field soil analysis and other soil related parameters were obtained from the soil survey geographic database ssurgo ground truth data used in the calibration and validation of the dndc model include monthly water tile discharge and total nitrate for the period 2014 2018 for calibration and 2019 2020 for validation and annual crop yield data for the period 2013 2014 for calibration and 2016 2017 for validation please refer to williams et al 2016 for further details regarding how water quality data were collected 2 2 dndc model dndc was originally conceived to simulate n2o emissions li et al 1992 over time it was developed and expanded to simulate the effects of soil c and n water cycling climate and agricultural management li et al 2006 2012 on crop growth ghg emissions soil nutrient loss and soc sequestration on a daily timescale the model is structured as a collection of sub models that generally operate as a two component system he et al 2018 qin et al 2013 the first component consisting of soil climate crop growth and decomposition sub models uses climate soil crop and management inputs to predict soil environmental conditions including soil temperature moisture and nutrient substrates the second component consisting of denitrification nitrification and fermentation sub models then uses these intermediary soil factors to predict ghg emissions derived from the plant soil system institute for the study of earth oceans and space 2012 the canadian version of the dndc model is a development branch first initiated in 2011 congreves et al 2016 kröbel et al 2011 smith et al 2013 to better simulate ghgs soil c n dynamics hydrology and crop growth under temperate climate the water content is estimated by a cascade flow algorithm whereby water content per layer tips to field capacity nitrate moves along with the water flux vertically to the layer below and it is also preferentially leached through macropores out of the soil profile as nitrate flow was simulated without water flux sometimes in earlier versions the preferential flow was correlated with water movement in this version so that the movement of nitrate to the tiles is iterative on an hourly basis the current version of the model incorporates a heterogeneous soil profile extended from 0 5 m to 2 m root penetration and density functions and mechanistic tile drainage smith et al 2020 a more detailed description of the new features the executables and examples can be found at https github com brianbgrant dndcv can in this study we ran the model using a batch simulation mode whereby multiple sets of simulations can be run consecutively to automate the calibration of the dndc model the model was integrated with an inverse model pest discussed below at a source code level the model was run using historical daily weather data from 2008 to 2020 with the recorded management practices and crop rotation of the current field repeated over this period the first 5 years were treated as a spin up period to bring initial soc levels to steady state perlman et al 2013 tabatabaie et al 2018 while the years 2013 2020 which aligned with observations were used for calibration and validation purposes 2 3 pest for inverse modeling dndc parameters were calibrated using measured data through inverse modeling using the open source parameter estimation and uncertainty analysis software pest doherty 2015 pest is a model independent parameter estimation software that uses a sum of weighted square residuals swsr between model based outputs and measured data as an objective function to determine an optimized parameter set pest iteratively and systematically varies model inputs runs the model and evaluates the model fit using an objective function until optimal values have been determined fig 2 upon the start of each iteration the relationship between the model parameters and model outputs is linearized using a taylor series expansion based on the current best parameter set doherty 2015 this allows the matrix computation of all first order partial derivatives of the simulated values corresponding to observations in the calibration dataset which form the sensitivity or jacobian matrix using this sensitivity matrix in the gauss marquardt levenberg gml algorithmic search method parameters that minimize the swsr are obtained for the overall simultaneous calibration of the dndc model a weighted multi component objective function doherty 2015 was used in which each observation type was assigned to a group constituting a portion of the objective function inter group weights were calculated using the inverse of the standard deviation of observations such that each group contributed proportionately to the objective function at the start of the calibration process for the sequential calibration approach the parameters were calibrated using water leaching outputs only at first so only one observation group was contributing to the objective function for the second step where nitrate and crop yield were calibrated together a weighted multi component objective function was used for the separate calibration methods only one output group water leaching nitrate leaching or crop yield was contributing to the overall objective function to ensure numerical stability of the inversion process singular value decomposition svd of the weighted jacobian matrix was truncated on an iteration by iteration basis through regularization truncation was automatically calculated based on a stability criterion the regularization transforms the original model parameters into linear combinations determines the most sensitive parameters and truncates the matrix reducing the number of estimated parameters to maintain numerical stability 2 4 model integration pest is integrated with dndc at a source code level and communicates with dndc fig 2 using three input files template control and instruction the template file has the structure of a dndc input file and is used to create model input files with modified parameter values the instruction file contains a series of pest commands that direct the reading of simulated outputs of interest the control file contains initial values for input parameters observed data with weights gml search algorithm constraints model run commands and parameter estimation termination criteria upon initiation pest reads the control file for initial parameter values observation data and numerical stability and regularization constraints to execute the model the first run uses user specified default parameter values table 3 after the first run is completed pest extracts the simulated output values e g water leaching from the dndc output files calls the dndc model repeatedly adjusting one parameter value at a time and allowing the computation of the sensitivity matrix the updated model parameter values are then used to begin another iteration this process is repeated until the objective function is minimized or until another convergence parameter e g the maximum number of iterations or minimum change in parameter values compared to the previous runs is achieved 2 5 calibration and validation prior to parameter estimation pre calibration composite sensitivities of each parameter with respect to each output type i e water leaching nitrate leaching and crop yield are calculated necpálová et al 2015 rafique et al 2013 composite sensitivities provide a comprehensive representation of a parameter s influence on the entire calibration dataset a total of 34 parameters were first included in the pre calibration sensitivity analysis by pest table 3 table s1 and fig s1 parameter sensitivities were calculated as a jacobian matrix using initial parameter values the jacobian matrix is composed of all partial derivatives of parameters to outputs each row consists of all parameters partial derivatives for the output corresponding to that row parameter sensitivities were calculated by first multiplying the output weighted q jacobian matrix by its transpose resulting in the sum of squares of individual weighted outputs this was then multiplied by the absolute value of the parameter p which in this case is the log base 10 transformed value 1 s i j t q j i i 1 2 l o g p where s i is relative composite sensitivity of parameter i j is the jacobian matrix of parameter sensitivities with respect to model operator matrix relative to output q is the diagonal weight matrix with observation weights as diagonal values and p is the initial parameter value to be able to compare the performance of pest to manual calibration the dndc model was calibrated manually by experts also the co authors of this study before integrating the dndc model with pest this manual approach helped select some parameters sensitive to water leaching nitrate leaching and yield looking at the relative composite sensitivities of the 34 parameters fig s1 a total of 14 parameters were chosen on the basis of relative composite sensitivities 10 of the maximum relative composite sensitivity necpálová et al 2015 rafique et al 2013 and according to expert opinion so that we could see the effect of sensitive parameters only then sensitivity analyses by pest were done for these 14 parameters fig 3 the sensitivity analysis was then used to optimize the parameters during the calibration process table 3 table 3 provides details on the dndc parameters description and references their default values manually calibrated values the upper and lower bound for each parameter during model calibration and the optimized values of the parameters for sequential calibration the optimized values for other modes of calibration are given in table s2 in most instances the difference between upper and lower bounds of a parameter is equal to approximately four to six prior standard deviations of the parameter doherty 2018 or they are sometimes set very low and very high to reduce parameter bound collisions doherty 2021 but these bounds must be managed to ensure they don t fall out of scope of model defined bounds for the parameters hence since the information on prior parameter standard deviations was unavailable the bounds were chosen based on realistic limits for the parameters these were determined from field measurements manual calibration literature review and values used by the dndc model expert opinion was also used for ensuring that the bounds were representative of the site conditions the default values for the parameters were kept between these bounds and were mostly the mid value of the bounds so that the optimized parameters could be found on either the lower or the upper spectrum the model was run with these default values in order to compare it with the performance of the calibrated model soil parameters were included in the sensitivity analysis and calibration due to the measured water and nitrate leaching data being collected at the edge of the field these measurements are impacted by characteristics of the whole field rather than a few points additionally the inclusion of soil parameters in sensitivity analysis and calibration reflects the variation that exists in the soil values reported in surrgo for the area of the field site in this study the model was calibrated using monthly water and nitrate leaching from 2014 to 2018 and annual crop yield data from 2013 to 2014 and validated using 2019 2020 monthly water and nitrate leaching and 2016 2017 yield data the use of yield data from 2013 to 2014 for calibration and 2016 2017 for validation reflects similar crop system rotations i e corn winter and wheat cover during these periods since water and nitrate leaching data were collected at an edge of the study site and the dndc model does not account for the transport of water and nutrient between locations within the field it was assumed that the dndc model would perform relatively well if calibrated with monthly data than when calibrated with daily data three calibration strategies simultaneous sequential and separate were employed to calibrate the dndc model the simultaneous approach used a multi component objective function to calibrate the model for the three dndc outputs i e water leaching nitrate leaching and crop yield for which observed data exists co currently in the sequential approach the model was calibrated for one output type by adjusting the most sensitive parameters corresponding to that output before calibrating for the next output variable the parameters were calibrated using water leaching first followed concurrently by nitrate leaching and yield water movement through the soil leads to the movement of nutrients and nutrients are leached out of the soil along with water hence correctly simulating water dynamics helps to ensure that n dynamics can be modeled well ma et al 2003 xu et al 2020 also calibrated soil water first followed by soil n and then crop growth the nitrate leaching and crop yield were calibrated together as crop yield is dependent on soil n availability through fertilizer inputs and mineralization additionally crop n export is critical in determining how much n is available in the soil for leaching relative sensitivity of 20 for parameter output sensitivities and expert opinion was used for a cutoff in delegating parameters to each output type used in calibration this 20 was arbitrarily chosen some parameters showed 20 relative sensitivity to multiple output groups e g cn pref2 por kdr etc in this case the optimized value of these parameters from water leaching was used as default values for calibration using nitrate leaching and yield lastly the model was calibrated separately for each output variable incorporating the entire parameter set each time the use of multiple output variables in the calibration and validation process presents the modeler with a selection of approaches that vary in distribution and order for which model parameters are adjusted and model fit to measured data is assessed 2 6 statistical analysis model calibration performance was quantified using the sum of weighted square residuals swsr index of agreement d mean bias nash sutcliffe efficiency nse absolute root mean square error rmse percent bias pbias and pearson s correlation coefficient r moriasi et al 2015 2 s w s r i 1 m w e i g h t e d y i y i 2 3 d 1 i 1 m y i y i 2 i 1 m y i y y i y 2 4 n s e 1 i 1 m y i y i 2 i 1 m y i y 2 5 r m s e 1 m i 1 m y i y i 2 6 p b i a s i 1 m y i y i 100 i 1 m y i 7 r i 1 m y i y y i y i 1 m y i y 2 i 1 m y i y 2 where subscript i represents the ith observation m is the total number of observations y is the observation y is the simulated value and y and y are the mean of measured and simulated data respectively swsr is interpreted by evaluating the change between two sets of simulated to measured data residuals with values closer to 0 indicating a smaller deviation of simulated outputs to measured data nse is a measure of the magnitude of residual variance relative to measured data variance and has a range of negative infinity to 1 with values closer to 1 indicating a good match between simulated and observed data and values equal to 0 or negative indicating poor model performance index of agreement d ranges from 0 to 1 with values nearer to 1 indicating greater agreement between simulated and measured data the scale used in smith et al 2019 was adopted with a d 0 9 indicating excellent agreement between simulated and measured values 0 9 d 0 8 indicating good agreement 0 8 d 0 7 indicating fair agreement and d less than 0 7 indicating poor agreement rmse is the standard deviation of residuals and ranges from positive infinity to 0 with values near 0 indicating model residuals concentrated around the measured data pbias is the average tendency of the simulated data to be larger or smaller than the measured data and is expressed as a percentage a value of 0 is optimal with low magnitudes indicating accurate model simulation pearson s correlation coefficient r describes the linear correlation between simulated and measured data with a range of 1 to 1 and a value of 0 indicating no linear relationship and either 1 or 1 indicating a perfect negative or positive linear relationship statistics were calculated on a monthly and weekly timescale for water and nitrate leaching and an annual basis for crop yield 2 7 model uncertainty estimation the impact of parameter uncertainties on model predictions was quantified using monte carlo simulation incorporating latin hypercube sampling lhs helton and davis 2003 mckay et al 1979 monte carlo simulation is a stochastic sampling method that generates many random sets of parameter values contained within user defined parameter bounds and assigns probability distribution functions pdf to each parameter to characterize the distribution of possible values within the parameter bounds triangular pdfs were assumed for all parameters included in the uncertainty analysis haan et al 1998 wang et al 2005 monte carlo analysis as a stand alone method can be computationally intensive thus lhs was incorporated to limit the number of parameters used in uncertainty analyses and reduce the total number of monte carlo simulation runs lhs integrates stratified sampling into random sampling by dividing each parameter s range into n intervals so that each interval has the same probability 1 n of being randomly sampled mckay et al 1979 tang et al 2007 pest2lhs was used to generate the input file for the lhs program in which the user defines conditions for generating a random parameter set including parameter pdfs and maximum value of non uniform distributions and a random number seed swiler and wyss 1998 the parameter sets are then generated by the lhs utility program finally lhs2pest generates separate parameter value files for each parameter set and a windows batch file that runs pest repeatedly utilizing all three main input files described in section 2 3 in this process the dndc model is run once with each set of parameter values and the corresponding model outputs are extracted from the model run files and recorded crop yield was excluded from uncertainty analysis due to limited observation data over the study period a sample size of 200 parameter sets was used for uncertainty analysis based on prior studies sieber and uhlenbrook 2005 that demonstrated similar statistical measurement values between lhs and random sampling for several tens of thousands of model runs when the sample size exceeds ten times the number of varied model parameters in this study 200 is about 14 times the number of varied model parameters i e 14 3 results 3 1 sensitivity analysis water leaching was most sensitive to model parameters that influence water runoff curve number cn crop growth i e maximum biomass of corn yd c thermal degree days tdc and grain c n ratio of corn cngc and tile drainage drain space ds drain depth dd fraction of n not preferentially leached pref2 saturated hydraulic conductivity factor kdr factor that controls nitrate movement with water flux nfac and soil parameters i e clay content cly and porosity por fig 3a nitrate leaching was also sensitive to these same sets of parameters but for nitrate leaching the highest sensitivities 80 relative sensitivities were shown by por nfac and pref2 fig 3b while for water leaching the highest sensitivities were shown by cn yd c ds cly and pref2 with respect to crop yield yd c was considerably more sensitive than all other parameters fig 3c across water and nitrate leaching and crop yield the most sensitive parameters were cn pref2 yd c por and ds fig 3d the other parameters grain c n ratio of wheat cngw and bulk density blk had distinctively low sensitivities compared to these parameters the implemented svd regularization changed the values of 12 of the 14 adjustable parameters supplied in the calibration for the sequential calibration approach while the two other parameters cngw and ds were stuck at the upper and lower bounds respectively parameters with the greatest relative change in value from the default values were cly cn yd c ds and kdr which had relative composite sensitivities 50 there were differences in optimized values between the simultaneous and the other calibration strategies fig 4 which impacted the dndc model performance for estimating water and nutrient leaching and crop yield for instance when comparing the optimized parameter values in sequential calibration to the optimized parameter values in the simultaneous approach there was a high variation 30 60 between the two sets of optimized parameters with high composite sensitivities e g dd ds kdr yd c which led to around 20 30 change in model performance across all output groups 3 2 model calibration and validation for simultaneous calibration dndc was run 364 times over 11 iterations in order to determine the optimal parameter values meanwhile for sequential calibration dndc was run 417 times over 14 iterations in total for the calibration period manual calibration had an improved performance overall for all three output groups of water and nitrate leaching and crop yield compared to other calibration methods table 4 table s4 this was followed by simultaneous calibration where the objective function i e swsr was reduced by 73 overall 48 water leaching 95 nitrate leaching and 61 crop yield both weekly table s3 and monthly statistics table 4 show that edge of field leaching is well represented by the model when evaluating the output groups individually simultaneous calibration had better statistical performance than other methods for water leaching during the calibration period it reduced the rmse by 23 compared to the default and had a positive nse value it was followed closely by separate calibration using just water leaching outputs which also reduced the rmse by 20 and had a positive nse value both of these methods had better statistical performance than the manual calibration which reduced the rmse by 2 compared to the default and had a very low positive nse value all the other calibration methods had slight increases in the rmse value compared to the default and a negative nse value separate calibration using just nitrate leaching outputs led to the poorest statistics for water leaching but it gave the best statistics for nitrate leaching outputs with a 57 reduction in rmse and double the nse compared to the default this was followed closely by the sequential calibration with a 49 reduction in rmse and 2 7 times the nse both methods performed as well as the manual calibration the poorest statistics for nitrate leaching were from the separate calibration using just yield outputs which increased the rmse by 8 compared to the default and reduced the nse by more than half for yield the best statistics were derived from the separate calibration of the parameters using just yield outputs which reduced the rmse by 84 compared to the default and had an nse value close to 1 which signifies a good match between observed and simulated outputs these statistics were better than the manual calibration this was followed by the separate calibration using just nitrate leaching outputs which reduced the rmse by 41 compared to the default and had an nse value of 0 57 compared to the default value of 0 3 nse values close to 1 indicate a good fit between observed and simulated outputs while values equal to 0 or negative means that the model performance is poor after model calibration the simulated results were compared with measured data year 2019 2020 for nitrate and water leaching and 2016 and 2017 for crop yield that were not included in the calibration steps for the validation period sequential calibration had the best model performance overall for all three output groups of water and nitrate leaching and crop yield followed by manual calibration table 4 table s4 looking at the individual output groups the statistics for water leaching were better in the validation period compared to the calibration period all of the calibration methods led to similar water leaching performance during the validation period for nitrate leaching the rmse values were lower during the validation period but the other statistics such as nse d and r were better during the calibration period compared to the validation period among the different calibration methods the sequential calibration method had the best statistics for nitrate leaching closely followed by the separate calibration by nitrate leaching outputs the simultaneous calibration method had a poorer performance compared to other methods during the validation period for nitrate leaching the sequential calibration method performed as well as the manual calibration hence it was able to both calibrate and validate the model well for nitrate leaching all of the methods simulated low nitrate leaching in the validation period which aligns with measurements explaining the low rmse the simultaneous calibration method performed well for yield outputs during the validation period with statistics better than manual calibration the simultaneous calibration method produced better yield statistics during validation compared to calibration sequential calibration also performed well during the calibration period with yield values close to the observed for both corn and winter wheat fig 7 however it did not perform well for corn yield during validation 3 3 temporal trends of simulated and observed data the monthly model performance metrics generally show good agreement with observed data during calibration and also during validation for water and yields nitrate levels were also generally well simulated during validation low rmse however the remaining statistics were poor which can be expected when nitrate levels are very low near the detection limit the observed temporal distribution of both water and nitrate leaching was captured in the calibrated dndc model fig 5 fig s2 fig s3 simulated water leaching fluxes followed rainfall events while nitrate leaching fluxes were dependent mainly on rainfall events and fertilization applications particularly in late spring seasons fig 5 the calibrated models had similar water and nitrate leaching peaks as the manually calibrated model the timing of water leaching fluxes was better captured during the validation period compared to the calibration period where the magnitude of the fluxes was only under predicted by 2 on average across calibration methods during the validation period compared to the 15 over prediction during the calibration period for nitrate leaching all the calibration methods led to an under prediction 19 45 of the peak nitrate leaching fluxes except for separate calibration by nitrate leaching outputs fig s3 which led to an over prediction of just 2 and had the best performance for nitrate leaching outputs there are no observed peak fluxes for nitrate leaching during the validation period which contrasts the multiple peak fluxes that occurred during the calibration period and contributed to the calibrated models having a lower performance metric for the validation period for both water and nitrate leaching manual calibration also had similar over and under predictions respectively if we look at the cumulative water leaching fig 6 there is a good fit between observed and calibrated outputs for both the calibration and validation periods there is a good fit between observed and all calibrated outputs till may 2015 then there is a slight overestimation by all calibration methods till october 2016 followed by a slight underestimation till october 2017 during the validation period there is a slight under estimation by calibrated models the underestimation is higher in simultaneous calibration compared to other calibration methods for cumulative nitrate leaching fig 6 manual calibration has a good fit with the observed data from 2014 to 2015 compared to other calibration methods that are underestimating nitrate leaching during that period from 2016 the default outputs have a very poor fit with the observed data there is a good fit between observed and all calibrated outputs from january 2016 to may 2016 but from may 2016 to may 2017 there is a larger underestimation by all calibration methods after that the simultaneous calibration method has a good fit with the observed data till february 2018 while the other methods are still underestimating nitrate leaching and the simultaneous calibration method overestimates nitrate leaching all the way through february 2018 to the validation period of 2019 and 2020 all the other calibration methods continue underestimating nitrate leaching looking at the different calibration models there was some difference in the timing of both water and nitrate peaks among them for water leaching when analyzing the simultaneous and sequential method the timing of peaks given by the calibration methods was different on five instances january 3 2014 january 7 2014 january 12 2014 january 1 2017 and january 12 2018 for nitrate leaching the timing was similar for all calibration methods except for one peak in separate calibration by yield fig s3 on january 7 2017 that was not present in the other methods separate calibration by yield outputs also had the poorest performance for nitrate leaching outputs for crop yield for both the calibration and validation period the default calibration had a much higher cumulative corn yield than the other calibration methods which have similar cumulative corn yield fig 7 for instance during the calibration period corn was planted on 5 21 2013 and harvested on 9 22 2013 and had the highest cumulative biomass of 27 020 kg ha 20 293 kg ha 19 953 kg ha and 18 493 kg ha for default manual simultaneous and sequential calibration respectively however for winter wheat the sequential calibration method had the highest cumulative biomass during the calibration period and the simultaneous calibration method had the lowest cumulative biomass for the validation period all methods had similar cumulative biomass except for the simultaneous calibration method which had a slightly lower cumulative biomass compared to the others winter wheat was planted on 10 18 2013 and harvested on july 7 2014 and had the highest cumulative biomass of 6890 kg ha 10 730 kg ha 2313 kg ha and 11 973 kg ha for default manual simultaneous and sequential calibration respectively 3 4 uncertainty analysis when the 95 confidence bounds at a monthly timescale were captured for water and nitrate leaching for the period of 2014 2020 most of the observed peak fluxes for water and nitrate leaching were contained within the uncertainty bounds fig 8 further 89 of the total 27 monthly water fluxes higher than 25 mm and 70 of the 10 nitrate leaching fluxes greater than 6 kg n ha were captured within the 95 confidence bounds 4 discussion 4 1 sensitivity analysis the systematic evaluation of the sensitivity analysis provided insights into the mechanistic relationships within a complex biogeochemical model that directly or indirectly affect outputs but are not obvious to model users ranked relative parameter sensitivities provided a comprehensive and objective basis to identify and prioritize the parameters and processes of the model that affect outputs of interest as reported in prior studies parameters such as curve number cn for water leaching musyoka et al 2021 clay content of soil cly zhao et al 2020 soil porosity por for nitrate leaching li et al 2014 corn accumulative temperature tdc and corn grain c n ratio cngc liang et al 2017 trybula et al 2015 for yield table 3 were identified to be sensitive drain space ds is also found to be sensitive to water leaching as the water flow rate from a tile drained agricultural system is inversely proportional to the square of drain space ward et al 2015 as the curve number is used to determine the amount of rainfall that goes to direct runoff the high sensitivity of water leaching to cn is explained crop evapotranspiration and water leaching are the two main pathways that remove water from the simulated soil space since increased plant growth has a proportional demand on soil water extracted for plant transpiration the maximum biomass of corn i e yd c parameter has a high sensitivity to water leaching soil water porosity at field capacity is used to calculate the volume of water held in soil pore space nitrate leaching is sensitive to porosity as it increases the volume of water and nutrients that can be transported through the soil profile li et al 2014 similarly a model factor controlling the fraction of nitrate mobilized in a layer that can move with hourly layer water fluxes nfac was a sensitive nitrate leaching factor model factor pref2 was also sensitive as it controls the amount of nitrate that preferentially leaches through the soil there is sensitivity of nitrate leaching to cn as well because nitrate moves along with the water flux and cn controls the runoff these parameters such as nfac and pref2 impact the distribution and availability of water and nutrients to crop roots for uptake and thus indirectly affect crop growth the maximum biomass of corn yd c was found to directly impact annual crop yield other parameters may also be sensitive to water leaching nitrate leaching and crop yield but the number of parameters was kept low because the number of observations for calibration just ranged from 2 to 122 in a study by perin et al 2020 the performance was better when the number of observations was equal to 6 times the number of adjustable parameters for the simultaneous calibration method the number of observations was roughly 8 times the number of adjustable parameters while for the sequential approach they were roughly 5 times the number of adjustable parameters hence only a couple of sensitive parameters were calibrated 4 2 calibration and validation the use of pest to calibrate the dndc model using the mathematical principles of inverse modeling improved the model s ability to simulate nitrate leaching and crop yield over the default model as well as provided an objective approach for calibration both the calibrated and validated models captured the timing of the water and nitrate leaching peaks relatively well during the calibration period manual calibration led to the best model performance for all output groups overall and the simultaneous calibration method had comparable results to the manual calibration based on model performance metrics such as the d value the simultaneously calibrated model had an excellent match for monthly nitrate leaching and annual crop yield and a good match for water leaching during the calibration period similarly looking at nse the model had good or acceptable performance while simulating monthly water and nitrate leaching and annual crop yield respectively he et al 2019 zhang et al 2021a for water leaching outputs only simultaneous calibration had higher nse values than manual but for nitrate leaching and crop yield manual calibration had higher nse values both were in the acceptable range for nse for all output groups and had excellent matches for nitrate leaching and yield and a good match for water leaching during the calibration period monthly water and nitrate leaching and annual crop yield had r values of 0 7 0 6 and 1 0 respectively which indicate the simultaneously calibrated model matched the pattern of measured data the range of r values reported in tonitto et al 2007 between simulated and observed water and nitrate leaching during calibration 0 55 to 0 8 and 0 54 to 0 76 respectively were found to be comparable to the r values of the current study albeit the ranges were reported at an annual scale which is more easily simulated by a model however during the validation period sequential calibration had the best model performance followed by manual calibration this demonstrates that sequential calibration could achieve similar model performance as manual calibration with the benefit of a lot fewer iterations and reduced expert oversight monthly nse and d values for nitrate leaching 0 83 and 0 96 were comparable to those reported in he et al 2019 where the nse and d values were 0 24 and 0 77 respectively as they were reported on a daily scale statistically the calibrated models struggled to maintain improvement during the validation period for nitrate leaching but maintained improved performance for water leaching and crop yield for water leaching the sequentially calibrated model had nse and d values of 0 8 and 0 95 respectively leading to good and excellent performance this was comparable to smith et al 2020 where the nse and d values for water leaching during the validation period were 0 74 and 0 92 respectively necpálová et al 2015 had also used inverse modeling to calibrate the daycent model using an extensive set of measured data over a three year period and most of the calibrated outputs performed well during the validation period for nitrate leaching the model was calibrated during times of peak fluxes while the observed data during the validation period had low nitrate leaching flows with no peaks the model simulated similar leaching levels with low rmse but other performance statistics were poor which is characteristic of n leaching levels that are near the measurement detection threshold as the data for validation was limited in this study future work can look at a wider range of data for validation with the presence of both peak and non peak flows if we take into account the overall effect on all three output groups the statistics were better during the calibration period compared to the validation period table s4 which was likewise observed by lamers et al 2007 and rafique et al 2013 however both the calibrated and validated models improved the model performance for all three output groups compared to the default model simulation dutta et al 2018 improved the model timing of leaching events during freezing and thawing periods in the dndcvcan model by incorporating and revising the effects of crop residue and snow cover on soil temperature these improvements are evident in this study as the calibrated model closely matched major and minor leaching events for both nitrate and water during spring thawing periods march and freezing december the canadian version of the dndc model used in this study has shown an improved simulation of monthly nitrogen leaching to the tiles smith et al 2020 hence the performance was good for nitrate leaching as the site used in this study is tile drained 4 3 optimization of parameters during calibration a total of 14 soil crop and tile drainage parameters were optimized for the calibration of the dndc model using pest for sequential calibration 12 out of 14 adjustable parameters were optimized while parameters such as corn grain c n ratio cngc and drain space ds reached their bounds similarly for simultaneous calibration 10 out of 14 adjustable parameters were optimized while parameters such as maximum biomass of corn yd c n not preferentially leached pref2 drain depth dd and ds reached their bounds these parameters reached their bounds for other calibration methods as well and had relative composite sensitivities of 30 they were kept at their bounds by pest as pest did not find another optimal set that further lowered the objective function and the algorithmic determined parameter adjustment in earlier iterations may have exceeded the user defined upper or lower parameter value bounds while widening the bounds may have further improved the calibration performance it would have resulted in parameterization outside of justifiable ranges in a study by wangwang et al 2019 it was also found that widened ranges of some hydrological parameters led to values that were not appropriate in the physical world hence widening the bounds was not an acceptable procedure for some model parameters while manual calibration is often complex and benefits from the inherent knowledge of parameter interdependencies automatic calibration is easier to employ as its only guiding principle is on improving a performance measure mcmillan 2021 from our study we saw that automatic calibration led to a similar model performance as manual calibration in terms of model to measured fit and statistics such as nse d and rmse while it significantly cuts down the number of iterations required up to 200 for manual calibration compared to just 11 for automatic calibration by pest care needs to be taken with automatic calibration because it does not consider parameter interdependencies and thus can result in unrealistic parameter optimizations calibrating the model using a manual approach first can help narrow the parameter ranges for automatic calibration and can help identify sensitive parameters while following up with an automatic sequence can find the best model to measured fit within those ranges jeong et al 2010 thus the results from automatic calibration can be more useful when guided by the user s understanding of the agronomic and soil processes and field management practices but it is also a valuable tool for novice model users for whom it may be necessary to use automatic calibration solely 5 conclusion the study integrated pest with dndc to facilitate automatic calibration of the model as well as gain insights into the sensitivity of model parameters on model outputs such as water leaching nitrate leaching and crop yields sensitivity analysis provided a comparison of relative composite sensitivities for individual model outputs and identified the most influential model parameters for accurate simulation of water and nutrient cycling in agricultural soils the manual calibration method led to the best model performance during the calibration period followed by simultaneous calibration that reduced the total sum of weighted square residuals by 73 for the validation period the sequential calibration method had the best model performance followed by manual calibration there was limited data for validation for nitrate leaching for which a mixture of peak and non peak flow data would have been ideal for evaluating model performance the model was able to emulate the low n leaching which occurred during the period but other datasets could be used in the future to further test the calibration techniques another option is to expand the assessments across a wider array of model input parameters which might provide more insight into parameter relationships impacting model performance the application of inverse modeling proved to be a powerful statistics based approach for calibration which allowed for an objective examination of many fundamental processes captured by a highly parameterized model the comparison of calibration strategies highlighted the importance of the type of measurements needed to calibrate the model depending on the targeted model outputs for instance for nitrate leaching the best model performance was obtained by the separate calibration of nitrate leaching hence it may be possible to narrow down the type of calibration method to use depending on the modeling objective automatic calibration offered advantages in terms of time saving while reaching a similar model to measured fit compared to manual calibration however care needs to be taken during automatic calibration to ensure that the parameter values make sense in the real world and should be aided by the understanding of soil and agronomic processes whenever possible software and data availability software name denitrification decompostion dndc developers brian b grant and ward smith contact address brian grant agr gc ca ward smith agr gc ca program language c software name pestdevelopers john doherty contact address https pesthomepage org contact us year first available 2015 program language fortran availability https pesthomepage org software 0 cost free declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment the authors are grateful for the support of this project by funding from the united states department of agriculture nifa award no 2017 67021 26141 and 2019 67019 29310 hatch project nc1195 and the sustainability research seed grant program the authors would also like to acknowledge the help from anna apostel haley stockham elizabeth callow asmita murumkar and grey evenson in the preparation of weather and management practice data for model runs appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105494 
25517,prism a new forest management decision support system was developed for united states national forest planning at the same time existing tools were deprecated prism has a friendly graphical user interface to facilitate model development database management sensitivity analysis and data visualization it integrates sqlite engine for database storage and exploration it supports both classic forest planning model i and ii to schedule management activities and achieve multiple management objectives through the lifecycle of a forest plan multiple models may be created through the prism interface and solved in batch mode for convenient sensitivity analysis various options to visualize model outputs are available through the embedded jfreechart library the open source nature of prism makes it free to use and allows for long term software development and collaboration this paper presents the architecture core functionality and selected applications of prism in the united states national forest planning keywords graphical user interface goal programming optimization forest management open source data availability link to source code is included in the manuscript software availability name of software prism description open source decision support system for forest planning developers dung nguyen eric henderson yu wei david anderson year first available 2016 hardware required 64 bit computer software required java jdk 15 or later versions program language java program size 72 megabytes version 3 2 01 cost and license free under the gnu general public license version 3 availability http bitbucket org prism members prism src master 1 introduction forest planning often includes identifying a long term forest management direction and evaluating effectiveness of forest management practices in achieving the objectives of the management plan historically forest managers have used decision support systems dss to inform the development and implementation of forest management plans in this paper we introduce prism a newly developed dss that has been used to support planning efforts by the united states forest service usfs here we present the architecture core functionality and performance of prism and discuss its utility in real world applications the usfs administers 188 million acres of land organized into 154 national forests hoover 2019 each forest maintains a land management plan i e forest plan that describes how the forest should be managed to achieve recreation livestock grazing timber harvesting watershed protection and wildlife habitat conservation objectives usda forest service 2008 the national forest management act of 1976 16 u s c 1604 requires every national forest to periodically revise its forest plan to address changing management goals forest conditions and public values plan development and revision is often carried out by an interdisciplinary planning team of natural resource experts with input from the public and stakeholders and is a complex process that may take five to seven years to complete haber 2015 a forest plan is comprised of several components including long term aspirational goals called desired conditions shorter term measurable objectives a set of management standards and guidelines to follow and the suitability of lands that defines where certain management activities are permissible 36 cfr 219 7 2 3 moving a forest from its current conditions to the desired conditions is often a complex problem that involves consideration of multiple management goals physical and financial constraints and the interaction of management actions and vegetation conditions through time the best solution to this problem is not usually readily apparent evaluating multiple management strategy options and their associated effects is one way to determine a good management plan united states national forest planning implements the national environmental policy act 42 u s c 4321 et seq where alternative management strategies such as suitability of lands or harvesting levels are analyzed to determine their environmental social and economic effects analysis of land management plan alternatives is often supported by dsss to provide comparative information for decision makers by comparing the beneficial and adverse effects of different alternatives forest managers can evaluate trade offs and make an informed decision about which alternative should be adopted as the forest plan for more than half a century dsss have been used to assist forest managers in making decisions that account for multiple economic ecological social and legal forest planning considerations reynolds et al 2008 segura et al 2014 packalen et al 2013 the development and evolution of dsss are driven by both the increasing demand for solving complex contemporary planning and the availability of advanced computer technologies vacik and lexer 2014 a 2014 synthesis of decision support systems for forest management planning reported 265 prevalent dsss in 26 countries that can address a broad range of forest planning problems borges et al 2014 many of those rely on mathematical formulations that implement quantitative techniques such as decision analysis simulation and optimization power and sharda 2007 nižetić et al 2007 the usfs has a long history with both simulation based and optimization based dsss to support strategic planning borges et al 2014 schuster et al 1993 rauscher 1999 reynolds 2005 in the period from 1979 to 1996 when us forest managers and policy makers emphasized systematic planning and optimal management choices optimization based dsss including forplan johnson and stuart 1987 field 1984 and its successor spectrum greer and meneghin 2000 were widely used the preference of using optimization dsss were driven by the 1982 planning rule 36 cfr 219 that stipulated the use of optimization methods to determine key plan requirements such as sustainable harvest maximum present net value and the suitability of lands beginning in the late 1990s there has been a trend to adopt simulation systems for the us national forest planning typical simulation based dsss for national forest planning in this period were vddt and its current version syncrosim st sim daniel et al 2012 simpplle chew et al 2004 and landis hallett et al 2017 scheller et al 2007 xi et al 2020 optimization models and simulation models have different strengths and associated weaknesses while optimization models are strong at finding good strategies for achieving the desired objective they must be told explicitly the level of disturbance to expect in the future which cannot be exactly known in reality boychuk and martell 1996 simulation models can be used more flexibly to reflect the uncertainty of future disturbances but often must be given the rules for when and how much management to implement gustafson et al 2000 shang et al 2012 simulation models may not achieve the desired objective as efficiently as optimization models forest managers often have to consider these trade offs when deciding which model to use in 2016 the northern region of the usfs and colorado state university began collaborating to develop the optimization model prism as a pilot project to support the custer gallatin national forest plan revision the name prism is an acronym for plan level forest activity scheduling model this tool allows usfs managers to rapidly develop long term forest management strategies and compare the effects of various plan alternatives two main factors contributed to the genesis of the prism project first there was a need to respond to the promulgation of the forest service s 2012 planning rule 36 cfr 219 which emphasizes 1 using the best available scientific information to inform forest planning 2 achieving vegetation desired conditions and ecosystem integrity objectives 3 a financial shift from maximizing present net value to completing projects within budget and 4 completing a planning revision within a shorter length of time usda forest service 2011 nie 2019 second the usfs planning software spectrum was deprecated due to obsolete programming languages and a lack of maintenance capacity the resulting prism dss makes several improvements over its predecessors it incorporates better computer programming technology and contemporary decision science development was done with java and an established long term support plan prism is maintained as an open source project which provides opportunities for long term software maintenance and developer collaboration it incorporates enhanced modelling techniques such as a flexible model i and ii formulation structure prism s design facilitates rapid model development and streamlined debugging the software reduces the complexity of model development by simplifying input requirements the software user interface provides an intuitive platform to include natural disturbances in the model formulation prism allows for virtually limitless attribute tracking that can be used in both the model formulation and back end reporting model runs can be batched to allow for efficient sensitivity analyses and to minimize user set up finally run outputs can be readily analyzed through both predeveloped and customized graphs charts tables or summary queries that can be saved and used in other related model runs the challenges in developing prism were similar to those associated with developing environmental modelling dsss including user engagement application of advanced technologies product adoption and measurement of product success mcintosh et al 2011 therefore we adopted some of the environmental dss development best practices when developing prism mcintosh et al 2006 2011 laniak et al 2013 jakeman et al 2006 pastorella et al 2015 for example to overcome the challenge of engaging end users we participated in several team meetings with the custer gallatin national forest to ensure prism provided the functions required by the forest planning process to ensure technology competitiveness we used a contemporary programming language java to build prism utilized well established methods i e model i and ii in forest management gunn 2007 davis et al 2001 martin et al 2017 mcdill et al 2016 belavenutti et al 2018 martin 2013 to develop the prism optimization model established connections to advanced solvers cplex for better model performance and integrated third party libraries to improve data management sqlite bhosale et al 2015 and data visualization jfreechart jfreechart homepage 2022 we created an easy to use interface for building models and displaying model results at different stages of the planning process this in turn creates opportunities for silviculturists biometricians analysts and planners to participate in model development and validation through the entire planning process finally prism was developed as an open source software package so that it is inexpensive to use update and maintain this paper describes the architecture interface design and key functionality of prism to emphasize the software s utility and ease of use several real world applications are discussed to showcase the software s capabilities and performance while initially developed for national forest planning prism has the potential to support more general forest management and we expect the software will be of interest to forest owners and managers outside of the usfs this software can be freely downloaded utilized and customized with the acceptance of an open source licensing agreement 2 forest planning analysis components and processes developing a prism model requires the user to compile two key datasets including a description of the current state of the forest strata and a menu of options yield tables for managing forest strata through time forest vegetation in each of these strata is grown forward through numerous future management trajectories prescriptions that is compiled in yield tables information about strata and yield tables are stored as tables in a relational database in prism database information is then used to build forest planning models directly through the software interface users must describe any restrictions on management as the model s constraints a measure of solution quality is defined through the model s objective function the goal is then to determine the prescription s to choose for each stratum that stay within the constraints and result in the best objective function value possible the first dataset required by prism is a description of the forest delineated into multiple strata forest areas within each stratum share identical characteristics that represent a unique and meaningful ecological economic and or social class these characteristics also called forest attributes can be static or dynamic depending on whether they would stay as a constant or change across time for example a geographic location on the forest is a static feature that represents a spatial aspect of the forest and does not change through time forest tree size class forest species composition and structural states are all dynamic attributes that are expected to change through time but may be included in the strata definitions that depict the current condition of the forest yield tables are the set of possible vegetation growth trajectories for each stratum resulting from different management strategies i e prescriptions yield tables store dynamic attributes of the strata at each time step or planning period such as timber volume wildlife habitat suitability or species composition some important dynamic attributes including vegetation type size and density are often used together to represent the state of forest strata which in turn can be used to define the vegetation desired conditions each prescription in a yield table is a unique series of forest management activities applied to a stratum through time powell 2013 british columbia and ministry of forests 2000 salwasser management activities may include thinning clearcut planting prescribed burning herbicide and or fertilization etc prescriptions must be designed such that the types and timing of management activities are technically practical and ecologically sound for example a harvest prescription may include a thinning activity to prepare for the final clearcut several decades later prescriptions may also be included to capture the effects of natural disturbances such as wildfire insect outbreaks or wind events although natural disturbances are generally modeled as predetermined proportions of forest impacted rather than management choices a no management prescription option for each stratum is also included to describe the trajectory of vegetation change in the absence of both management and disturbances constraints in a forest planning model typically reflect the limitations on implementing management activities they are often built to comply with the standards guidelines and suitability of lands described in the forest plan usda forest service 2008 for example there may be limits on the amount of prescribed burning that can be implemented in a year or a desirable amount of wildlife habitat to maintain through time another common constraint is the sustainability of timber harvest levels through time called non declining yield under the 2012 rule forests are expected to act within its fiscal capability i e stay within budget therefore the total cost across all management activities in a given time period can be included as a constraint that should not be exceeded the model s objective function is a measure of solution quality and often represents the primary objective of forest management in usfs planning under the older 1982 rule often the objective was the maximum total present net value from all planned forest management activities through a planning horizon under the 2012 rule the objective function value is typically the minimum departure from the vegetation desired conditions in a forest which represents the ecological objectives of the forest plan objective functions can also be used for exploratory information such as to identify the most amount of timber that can be sustainably harvested in the near future once the basic information about a forest is defined managers must then coordinate the management of all forest strata through multiple periods of the planning horizon to achieve the primary objective while meeting all the management constraints this complex problem is why dss models are often needed to inform the selection process 3 prism architecture 3 1 overview prism is an optimization dss built on linear programming lp models lp model formulations have been widely used for the design and analysis of forest management strategies two common lp formulations used in forestry are named model i and model ii gunn 2007 davis et al 2001 martin et al 2017 both formulations are designed to allocate management activities across a forest landscape over multiple periods of a planning horizon a detailed review of both formulations is beyond the scope of this study but can be found in many past publications in forestry literature martin et al 2017 mcdill et al 2016 belavenutti et al 2018 model i and model ii formulations have been applied in forest planning dsss such as forplan spectrum or woodstock www remsoft com prism allows users to implement both model i and model ii formulations descriptions and illustrations of the model i model ii and the prism hybrid model will be described in next sections we developed prism as an open source software and made it accessible under the terms of gnu gpl3 a free copyleft open source software license the gnu gpl3 license allows users to freely download use distribute and modify prism the copyleft clause requires that derivative works from prism i e its modified program and source code must be released to the public under the same gnu gpl3 license the open source nature of prism makes the software inexpensive to use update and maintain the prism software includes three subsystems a database subsystem for managing forest specific data a modeling subsystem for designing forest planning problems as lp models and solving those models and an output subsystem for examining model solutions fig 1 the three subsystems are wrapped into a gui to facilitate interaction between users and the software many open source add ins e g icons tools and libraries are used to build the gui and provide functionality for the subsystems detailed design and functionality of the prism s subsystems are described in the next sections 3 2 the database subsystem the database subsystem facilitates storing manipulating and examining forest planning input data in a relational database databases are displayed hierarchically with a tree structure in the gui of this subsystem fig 2 a a forest database is comprised of two key information strata inventory and yield tables i e prescriptions forest attributes are used to delineate forest strata and project forest condition changes over time for each stratum under the impact of different prescriptions there is no restriction on the number of attributes that can be included in a forest database prism includes an import function that loads information from delimited text files i e csv files and store them as database s tables delimited file specifications are defined in the prism user manual link available in the software availability section a noteworthy data management feature is that users may include multiple sets of strata and yield tables in the database to distinguish different forest plan alternatives sqlite serves as the engine in prism to support many database operations including sql structured query language commands a user friendly feature of prism is it includes prewritten sql commands i e queries designed to manipulate and examine forest data fig 2b so most users do not need to learn sql to do basic data exploration and manipulation for additional flexibility users may write their own sql queries through the database subsystem gui user defined queries may be saved to the prism sql library that will be added to the dropdown query menu fig 2b for future usage 3 3 the modeling subsystem the modeling subsystem reads and validates forest strata and yield tables data from the database subsystem allows users to design forest planning problems and calls an lp solver to determine the optimal management strategy fig 3 a modeling subsystem gui is provided to help set up the following model components planning horizon how many time periods are in the projection and how long is each period goal oriented objective function what is the main objective of the forest and how do you measure success e g movement towards desired conditions model formulation prism automatically decides whether to use a model i a model ii or a combination thereof the decision of which model to use is based on the design of management prescriptions in the input database natural disturbances how many types of natural disturbances will be defined in the model how likely are they to occur and what are their impacts management costs what are the financial costs associated with different management activities user defined constraints what are the management requirements this encompasses a wide range of options including timber production and sustainability targets restrictions on management activities wildlife habitat area requirements budget limitations desired forest species distribution etc solver the lp solver used to identify the optimal management strategy sections from 3 3 1 to 3 3 6 will highlight some of these features in the prism s modeling subsystem more detailed information can be found in the user manual of prism available for download from the website provided in the software availability section 3 3 1 goal oriented objective function in prism management problems are defined using a non preemptive goal programming approach this approach was also used in other software such as a project scale forest planning tool called ned twery et al 2000 and its successor ned 2 twery et al 2005 in this approach forest management goals are optimized simultaneously and weighted according to their importance for example desired future forest species composition can be constructed as a series of goals for each unique vegetation condition each goal specifies a target level as either an absolute value or a numeric range between the minimum and maximum acceptable levels the importance of each goal is defined with a user defined weight which is used to calculate a penalty if the goal is not met when a goal cannot be achieved under achievement or over achievement compared to the target level the weight is multiplied by the deviation from its target level to determine the penalty the objective function of a prism model is to minimize the sum of penalties across all goals prism can also examine single objective problems such as finding maximum timber volume in this case users simply set an arbitrarily large timber volume as the only goal in the formulation with a non zero weight assuming the goal is larger than the volume determined by the solution the solution will represent the maximum possible volume the detailed goal programming formulation implemented in prism is described in the appendix allowing users to easily model unlimited number of goals is a major contribution of prism design that makes it suitable for forest planning with multiple objectives toward desired future conditions 3 3 2 model formulation model i fig 4 a and model ii fig 4b are the two well established lp model formulations used in forest management and planning a major difference between these two models is the consistency through time of forest management strata responding to regeneration events such as timber harvest or lethal disturbance regeneration events result in bare land that has associated management choices such as natural regeneration or planting to a particular tree species a model i formulation allocates the area in each stratum to management prescriptions through the entire model duration or planning horizon in contrast a model ii formulation tracks the area in each stratum assigned to prescriptions that span the length of time between forest regeneration events in contrast to the entire planning horizon each forest stratum used in model i retains a constant area through the planning horizon the area of model ii strata change through time as portions may become bare lands at every period due to harvest or disturbance events bare land areas originated in the same period can be combined to create new strata for regenerating different vegetation species prism uses a hybrid model structure to define management options for forest strata fig 4c this has several advantages 1 users do not have to design all prescriptions to expand through the entire planning horizon typical model i design for example the same model ii harvest prescription may be created and used repeatedly to manage a stratum through any 5 consecutive periods 1 5 2 6 3 7 etc of a planning horizon 2 prism model allows flexible strata management because users can define rules for merging strata that regenerate in the same time period for example users can require that only the mixed conifer and lodgepole pine strata harvested in the same period may be merged they can also choose specific vegetation available from those model ii prescriptions for regeneration such as the merged mixed conifer and lodgepole strata after harvest may be naturally regenerated as mixed conifer or can be planted for a cost to lodgepole 3 model selection requires no user effort because prism automatically implements the appropriate model i or ii or both for each stratum based on its available prescriptions 3 3 3 ability to use forest attributes in the problem formulation prism has a user friendly gui to account for both static and dynamic forest attributes fig 5 in the lp formulation specifically the gui allows users to select forest strata based on static attributes fig 5a specific periods of interest fig 5b and dynamic attributes fig 5c for example the checked boxes in fig 5 would select strata in the montane area with lynx habitat where the initial vegetation is ponderosa with medium and large tree sizes given that in the second period the trees in those strata are at age class 3 and the density is within 300 600 trees per acre the selected strata can then be used to allocate natural disturbances assign management costs and define constraints representing various management restrictions on area or cost or forest attributes e g carbon storage wildlife habitat value associated with those strata across time as described in the next sections of this paper 3 3 4 natural disturbances prism can model both non lethal disturbances nds and lethal disturbances lds nd events are assumed to damage a portion of the trees in a stratum while ld events are lethal to most or all trees in the affected stratum area after an ld event a newly regenerated forest starts a new life cycle non lethal disturbances are modeled at the prescription generation stage with model i prescriptions built to track the effects of predefined sequences of nd events through the entire planning horizon e g mountain pine beetle outbreaks every 30 years prism s gui allows users to assign appropriate prescriptions with nd events to a portion of each stratum in contrast to nds lethal ld disturbances are modeled without using prescriptions ld events are defined directly through the prism gui according to what strata the events occur in occurrence times proportion of each stratum impacted corresponding regenerated vegetation and regeneration trajectories after each ld event i e appropriate model ii prescriptions to manage the new bare land strata after lds prism models the impacts from nds and lds in different ways nds are modeled deterministically as fixed percentages of the total stratum area disturbed while lds are modeled as stochastic percentages the deterministic approach simply uses the user defined average probability in each stratum during each planning period the stochastic approach draws random disturbance levels across the planning horizon based on the user defined probability distributions 3 3 5 management costs prism also has a gui to help users define management activity costs including the per area treatment cost cost associated with a dynamic attribute such as timber volume and regeneration cost that varies by specific vegetation types to plant costs can also be built as fixed values into prescriptions in the forest database however defining costs directly through the prism s gui makes it easier to change and test different assumptions without the need to modify the database for instance two strata managed by the same prescription may be defined in the gui to have different treatment costs if road building is required to access one stratum but not the other 3 3 6 user defined constraints prism is designed to efficiently develop and test constraints in a forest planning model constraints are categorized as non relational and relational each with a separate interface design some relational constraints model the flows of timber cash or other management elements across time and are called flow constraints non relational constraints define direct limitations on area cost or other quantified forest conditions or production levels fig 6 examples include a minimum amount of wildlife habitat area a maximum amount of budget spending or a limit on the timber volume that can be generated from clearcuts etc relational constraints can be used to model other relationship requirements fig 7 for example they can be used to limit timber volume fluctuation between two consecutive time periods control the ratio of even aged to uneven aged forest treatments within the same period or define ending period inventory of standing volume relative to the average standing inventory of the previous several periods the combination of the two constraint categories creates a flexible modeling framework to describe a forest management problem with various management requirements more comprehensive definitions and explanation of these constraints can be found in the prism goal programming formulation section of appendix 3 3 7 solution options prism supports running multiple models in two available batch modes the sequential mode solves models one after another while the parallel mode implements a java concurrency technique multi threads to enable solving many models simultaneously presently the prism interface provides two solver options lpsolve free open source solver or cplex commercial solver requiring a license lpsolve can handle small to medium size lp problems large models such as those with more than one million variables may require the more powerful cplex solver those two options make prism useful for both academic training lower cost and industrial applications 3 4 the output subsystem after an lp model is solved the output subsystem gives users a variety of options to manage and visualize the model solution through both standard and custom output options standard outputs include the following solution metrics such as solution time objective function value number of decision variables and number of constraints information about decision variables in the optimal solution including their values and reduced costs information about constraint levels at the optimal solution including dual and slack values a query framework that includes predefined sql commands to summarize model outputs reports and graphical displays of the model outputs such as selected management activities for each stratum and constraint levels the output subsystem uses an open source chart library jfreechart to visualize the output data by building multiple types of charts e g pie or bar charts in fig 8 the data visualization feature can help users get quick insights from the model results in addition to the standard outputs prism allows for the development and storage of customized sql commands as well as gui enabled query tools to examine the optimal solution without sql outputs from sql queries and the non sql gui tool queries can be exported to external software for supplemental analysis and display options 4 prism application and performance since 2016 prism has been used to support land management planning for several us national forests these include the custer gallatin national forest cgnf in southern montana and western south dakota the helena lewis and clark national forest hlc in north central montana and the nez perce clearwater national forest npc in north central idaho the land management plan for each forest required a detailed analysis of multiple management alternatives to comply with the direction in the 2012 planning rule and the national environmental policy act for each national forest six management alternatives were developed based on the forest s internal assessment and public comments and represent a range of different management strategies and outcomes alternatives may differ by management emphasis appendix table a1 associated with different levels of contribution to social and economic sustainability appendix table a2 differences among alternatives are due to local and national stakeholder group preferences that prioritize certain social and economic benefits over the others for example the cgnf designed alternative d for the greatest contributions to clean water fish and wildlife conservation wildfire and fuel management cultural historic tribal resource protections and scenery while alternative e was designed for the greatest contributions to clean air timber production income and job opportunities alternatives b c and f were aimed at balancing a mix of contributions across all stakeholder groups appendix table a2 prism was used to analyze the long term outcomes and effects of the alternatives for each of these plans each plan alternative is built as a distinct prism model the objective for each model was to schedule vegetation treatments to move the forest toward desired conditions within a 15 decade planning horizon through weighting the benefits and trade offs of each alternative over the life of the forest plan forest managers can make informed decisions on which alternative to implement in the plan comprehensive prism results for each plan are available on the usfs plan revision web pages usda forest service 2020a usda forest service 2021 usda forest service 2019 sections below describe the application and performance of the software 4 1 prism application table 1 summarizes the prism model inputs from the three different national forests for each forest strata were delineated based on various static spatial attributes such as geophysical political and management area designations vegetation attributes were derived from the northern region existing vegetation mapping program usda forest service 2020b prescriptions and associated forest dynamic attributes were derived from projecting forest inventory and analysis data wurtzebach et al 2020 of each forest into the future with the forest vegetation simulator software crookston and dixon 2005 the three forests used a broad range of forest management cost categories including timber sale preparation and administration reforestation timber stand improvement road reconstruction and administration prescribed burning weeds treatment and other surcharges specific cost values varied by management activities static attributes such as physical location and dynamic attributes such as vegetation type and density three types of natural disturbances were modeled on the three forests two disturbances were non lethal mixed severity wildfire and severe insect infestation and the other was lethal wildfire the usfs relied on historical burn severity and insect outbreak data to identify the expected amounts of future natural disturbance events for example the cgnf assumed a 50 year interval for mixed severity wildfire 60 year interval for severe insect outbreaks and between 9000 and 11 500 acres per year of lethal fire disturbance amounts were put into prism as fixed percentages of area that would be disturbed in every decade a wide variety of constraints were built to ensure compliance with laws policy and the management direction of the proposed plan alternatives constraints were classified into several groups including budget constraints budget limits on management activities harvest policy constraints non declining volume and other restrictions on timber harvest dispersion of openings constraints distribution of management activities across strata to prevent habitat fragmentation at any point in time prescribed burning constraints the upper limit of prescribed burning area to meet air quality standards operational capacity and weather limitations wildlife habitat constraints to help ensure sustainable habitat levels and treatment constraints to meet fuel reduction area targets and other management requirements those constraints were designed to meet management requirement targets both under and over achievements of the required management targets are not allowed another group of constraints was built to model the desired area distribution including both the minimum and maximum levels for different forest vegetation types and structure size classes these constraints allow for deviations from the desired vegetation condition levels that were used as the basis for the goal based objective function prism was used to generate the projected acres of management activities and annual timber volume see appendix table a3 for the cgnf reports included directly in each forest s land management plan prism was also used for sensitivity and trade off analyses to evaluate the effect of various plan components expressed as constraints on the forest s ability to meet desired conditions for example by systematically eliminating constraints from the model and measuring the maximum volume achieved in the first decade the cgnf found that the non declining timber volume constraint had the most impact on management results appendix table a4 another analysis was conducted by the hlc where prism was used to quantify trade offs between different budget allocations and identify whether the budget was not a limiting factor 4 2 prism performance to evaluate prism performance models from the three national forests were solved by using ibm s ilog cplex 12 8 on a 64 bit workstation equipped with a dual core 2 5 ghz processor and 32 gb of ram for each national forest six prism models associated with the six plan alternatives developed for that forest were solved prism was able to solve every existing model and reported corresponding results within 30 min the average results are presented for each forest in fig 9 model size and complexity vary for each forest reflected by number of variables and constraints in the model model performance reflected by total run time and memory usage is different depending on model size and complexity models developed for the cgnf hlc and npc respectively have increasing sizes as measured by the number of variables and the number of constraints fig 9a larger models require more computer memory to store and process the model data and solve the associated lp problems fig 9d larger models typically require longer total run times fig 9c here the total run time is the sum of 1 input processing time the time it takes prism to read the user inputs from the gui and formulate the lp problem 2 solution time how long it takes cplex the default solver to solve the lp model and 3 output writing time how long it takes to prepare and import model results to the output subsystem there is a positive relationship between model size fig 9a and the input processing time and the output writing time fig 9c the number of non relational constraints is often much larger than the number of flow constraints for each forest fig 9b both non relational constraints with tighter bounds and flow constraints can make an lp model difficult to solve for example even though the npc models are larger fig 9a they take less time to solve fig 9c likely because they have fewer flow constraints fig 9b sensitivity tests could help define the impacts of different constraints on solution time but this is not our focus in this study 5 discussion and conclusions prism is a new forest planning dss developed through an agreement between the usfs northern region and colorado state university it has been used by several us national forests to support the development of forest plans but its functionality is flexible enough to support planning efforts by other forestland owners and managers as well prism has several strengths that may make it a preferable tool for forest management analysis first it is available through a no cost open source license its database management is supported by the open source sqlite database engine and visualization options are provided by the open source jfreechart library while a powerful commercial solver may incur a licensing fee smaller problems may be solved by the open source solver lpsolve integrated in prism another strength of prism is its architecture to support rapid model development and testing process prism allows users to delineate a forest into many strata incorporate multiple choices for strata management and define various model components natural disturbances management costs management constraints directly through the gui gui features such as the constraints section in particular that allows users to quickly switch constraint types facilitate solution space exploration and pinpointing infeasibilities the ability to run models in batch mode to quickly compare the results is another advantage of prism it saves significant time for analysts to run multiple scenarios and present results to decision makers finally the source code for prism is stored and managed through a git control open source website that is convenient for future software maintenance and expansion natural disturbance and associated uncertainty create challenges for determining an optimal management schedule based solely on using optimization dss to overcome these challenges the three us national forests discussed here integrated prism with other dss software to form an analytical system specifically at the front end the forest vegetation simulator crookston and dixon 2005 was used to determine vegetation growth trajectories for forest strata including the effects of non lethal disturbances this information made the yield tables that served as inputs for prism prism was then used to schedule forest management activities through time under deterministic impacts from both non lethal and lethal disturbances lethal disturbances are defined through prism s gui the management schedule from prism was in turn analyzed with multiple stochastic disturbance scenarios by a state transition model simpplle chew et al 2004 to evaluate the management effectiveness in the context of future uncertainty in such an iterative analytical system parameters of each model can be adjusted based on the outcomes of another to gain a better understanding of the interaction between management and ecological processes chick et al 2019 integrating multiple dss models is a time consuming process as it requires that each model be parameterized calibrated and run prism holds the potential for improved future efficiencies if it is modified to simulate stochastic disturbances with better details this would allow simultaneous evaluation of management and ecological processes in a single prism model multiple prism model runs could produce a range of possible management activities and ecological trajectories some further options worth investigating include stochastic programming models wei et al 2014 that can provide a solution by simultaneously accommodating many possible future scenarios a rolling horizon mechanism savage et al 2010 that evaluates forest management strategy by an adaptive management approach or simply running multiple scenarios of the same model with stochastic disturbances if developed these capabilities would be another example of how prism has improved upon the previous generation of usfs planning models prism has the potential to be maintained and expanded through time it was developed using an open source development platform this form of collaboration allows more flexibility for both the end users and the developers to continuously maintain and improve prism it also encourages participation from other potential users or developers to the software s future development the open source nature of prism enhances its chance to be adopted by managers and experts in the domain of forest management and planning declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the prism project was funded by a challenge cost share agreement 16 cs 11015600 015 between the united states forest service northern region and colorado state university appendix prism goal programming formulation prism formulates the planning problem as a linear programming lp model formulating a lp model requires identifying a set of decision variables for each stratum to represent where when and what management prescriptions may be chosen in prism decision variables are associated with forest strata through information stored in yield tables decision variables can be incorporated into constraints to capture various assumptions relationships and requirements of managing the forest they may also be incorporated into the model s objective function that minimizes the total deviation from multiple forest management goals the solution determined by the prism model indicates a forest management plan that leads to the best objective function value the mathematical expression of the prism lp formulation is provided here objective 1 m i n n w n l n w n u n constraints set 1 non relational constraints 2a y n l n l n n c s o f t 2b y n u n u n n c s o f t 3a y n l n n c h a r d 3b y n u n n c h a r d 4a y n 0 n c f r e e 4b y n n c f r e e constraints set 2 relational constraints flow constraints 5a n c m k 1 y n l m n c m k y n m k 1 k m 1 5b n c m k 1 y n u m n c m k y n m k 1 k m 1 constraints set 3 transitional constraints 6 i i x x s i 1 s i i x x s s i 1 v s s 7 1 p x x s i t x s i t 1 s i i x t 1 t 1 8 1 p x x s i t a x s i t 1 a 1 s i i x t 1 t 1 a 9 i i x p x p s s x x s i t a i i x p x p s s x x s i t a f s s t s s t 1 t 1 10 s f s s t s i i x x s s i t s i i x a x s s i t a i i x x s i t 1 1 s i i x x s s i t 1 1 s t 1 t 1 indexes n index of a non relational constraint m index of a relational i e flow constraint k index of a flow s component s s s index of a stratum based on user defined static attributes such as physical location timber suitability etc i index of a prescription a prescription can capture the effects from various silviculture methods i e natural growth prescribed burning group selection even aged harvest or non lethal natural disturbances i e mixed severity fire insect outbreak t index of a planning period a index of an age class parameters l n lower bound of constraint n l n 0 u n upper bound of constraint n u n 0 w n weight for one unit less than l n w n 0 w n weight for one unit more than u n w n 0 k m number of components of flow m k m 2 l m lower bound of flow m l m 0 u m upper bound of flow m u m l m v s area associated with the existing stratum s at the beginning of planning horizon p x user defined loss rate due to lethal disturbances in the existing stratum area x p x user defined loss rate due to lethal disturbances in the regenerated stratum area x p s s x conversion rate associated with p x where s p s s x 1 s p s s x conversion rate associated with p x where s p s s x 1 s sets c f r e e set of free non relational constraints c h a r d set of hard non relational constraints c s o f t set of soft non relational constraints c m k set of user defined continuous variables that creates the component k of the flow m for example the flow m can be described by 3 components k 1 2 3 where each component may contain multiple continuous variables such as c m 1 y 1 c m 2 y 2 y 3 and c m 3 y 5 y 7 y 9 i x set of prescriptions that are feasible for the existing strata variable x i x set of prescriptions that are feasible for the regenerated strata variable x variables x s i t area of the existing stratum s in the period t which would be managed by prescription i and the activity to be implemented in period t is not clear cut x s s i t area of the existing stratum s in the period t which would be managed by prescription i the activity to be implemented in period t is clear cut and the regenerated landscape attributes identification would be s in the next period t 1 x s i t a area of the regenerated stratum s in the period t at age class a which would be managed by prescription i and the activity to be implemented in period t is not clear cut x s s i t a area of the regenerated stratum s in the period t at age class a which would be managed by prescription i the activity to be implemented in period t is clear cut and the regenerated landscape attributes identification would be s in the next period t 1 f s s t total area of the stratum s subjected to lethal disturbances in period t and regenerated into the forest landscape identification s at age class one in period t 1 y n a user defined continuous variable to calculate a quantified level of either area e g acres or cost e g dollars or any numeric forest attribute e g number of trees harvested volume associated with certain strata in certain time periods l n total units falling short of l n l n 0 l n u n total units exceeding u n u n 0 might not be true for the case when w n 0 where decision variables use their indices to link to a specific row of a specific prescription in the input database fig a1 prism uses modified model i variables x s s i t a n d x s i t a and modified model ii variables x s i t a a n d x s s i t a specifically the time period t index is not necessary for model i variables but prism uses t to capture the area transition due to the effects of lethal disturbances in each planning period note that lethal disturbance conditions proportions and effects are defined in the prism gui instead of using prescriptions while model ii variables often use the starting and ending period indices associated with each prescription prism uses time period t and age class a as alternative indices with an equivalent effect for example a regeneration prescription to regrow vegetation in a bare land in 4 consecutive periods from 2 to 5 would be associated with the set of t a 2 1 3 2 4 3 5 4 fig a1 design of decision variables in prism prism allow using up to six different user defined groups of static attributes integrated into the s index to index a decision variable x s i t x s s i t x s i t a x s s i t a other indices i t a are used to link every variable to an exact row of an exact prescription in the database that single row contains detailed information of the dynamic attributes associated with the variable fig a1 the objective function equation 1 minimizes the sum of penalties across all forest management goals penalties are aggregated from the user defined soft non relational constraints non relational constraints equations 2 4 users define parameters for each constraint including the bounds upper limit and or lower limit and the per unit magnitude of the penalty if the constraint is violated users also define whether a constraint is hard soft or free depending on how the bounds and penalties are applied a hard constraint requires the value of the variable associated with the constraint to be within the specified bounds a soft constraint allows for deviations from the bounds but incurs penalties for deviations users can specify the penalty value so that some constraints are valued more than others note that penalties are applied to the per unit deviation of the constraint and caution should be taken if mixing soft constraints using different units of measurement e g simultaneously considering a soft per volume constraint and a soft per acre constraint soft constraints collectively define the goal objective function where the sum of penalties across all constraints is minimized a free constraint simply tracks the values of the associated variables without incurring penalties or requiring a specific level to be maintained free constraints may be useful for model summarization in prism constraints may also be excluded from the problem formulation without being deleted from the gui this feature makes it convenient for users to find infeasibilities or conduct sensitivity analyses by quickly including or excluding in subsequent model runs relational constraints equation 5 the user defined continuous variables y n and their associated parameters can be considered together to form relational flow constraints for instance one might define a relational constraint that states the average of timber volume harvested in periods 1 5 must be less than or equal to 20 of the harvested volume in period 6 transitional constraints equations 6 10 equation 6 describes the initial landscape condition at the beginning of the planning horizon period 1 equations 7 and 8 respectively capture the transition of existing forest strata areas and regenerated forest strata areas that are not being clear cut and not subjected to lethal disturbances lds in this case age class would increase by one when moving from a period to the next in contrast to the above two equations equations 9 and 10 identify strata areas that are subjected to either clear cuts or lethal disturbances in a planning period and require those areas to become regenerated forest strata areas at age class one in the next period equation 9 identifies the total area subjected to lds in each planning period for each s s transition equation 10 uses the result from 9 plus the total area subjected to clear cuts then transit the whole areas to become regenerated forest strata areas at age class one in the next period illustration of building a simple model in prism here we provide an example to illustrate how prism can be used to evaluate a simple timber harvesting problem for this example we assume 1 users already have a forest database with sufficient information about strata and prescriptions 2 forest strata may be managed with any of the prescriptions associated with each stratum 3 no disturbances will occur during the planning horizon and 4 the cost of management activities is not considered the model is used to schedule timber harvest through a 5 period planning horizon such that the harvest volume in every period is between 5000 and 10 000 cubic feet there are penalties for both under harvest 2 penalty points per cubic foot lower than 5000 and over harvest 1 penalty point for each extra cubic foot greater than 10 000 which are used in the goal programming objective function to minimize the total penalty this problem can be set up in several minutes through the two gui windows illustrated in fig a2 prism automatically builds the complete model formulation based on information provided in the gui including the objective function 11 several user defined constraints 12 15 and the core transitional constraints 6 10 as presented earlier in the appendix 11 m i n i m i z e n 2 l n u n subject to 12 y n s i x s i t s s i x s s i t s i a x s i t a s s i a x s s i t a n 1 5 t n 13 y n l n 5000 n 1 5 14 y n u n 10000 n 1 5 15 y n l n u n 0 n 1 5 note harvest activities include thinning selected cuts clearcuts and other management activities that will contribute to harvest volume and therefore all types of decision variables can be added into the constraint 12 fig a2 the process to build an example timber harvest model in prism that requires using only two prsim gui windows a screenshot of a prism window to import database and set up some general model inputs including the planning horizon b another window screenshot to create user defined constraints here the two arrows represent the required selections of both time period and rmcuft to set up the constraint for timber harvest in the first planning period the rmcuft parameter represents the cubic feet of harvested timber per acre each user defined continuous variable that represents how many acres to harvest in each period will be linked to a specific rmcuft value in the yield table database fig a2 table a1 forestwide comparison of management alternatives in the cgnf s revised forest plan table a1 issue of emphasis alternative a alternative b alternative c alternative d alternative e alternative f recommended wilderness area number 7 9 9 39 0 7 recommended wilderness area acres 33741 113382 145777 711425 0 125675 backcountry area number 3 9 13 1 2 13 backcountry area acres 38414 124980 299522 5937 171326 208959 recreation emphasis area number 0 8 8 4 12 10 recreation emphasis area acres 0 176958 160665 33408 212689 224608 stillwater complex acres 0 101832 101832 0 101832 101832 miles summer motorized trail no longer suitable 0 0 4 172 0 0 miles mechanized trail no longer suitable 0 0 34 264 0 24 acres winter motorized transport no longer suitable 0 0 24885 234431 0 10128 forested acres suitable for timber production percent custer gallatin national forest 664 628 22 573 275 19 549 115 18 545 274 18 593 735 19 565 536 19 forested acres unsuitable for timber production but where timber harvest is suitable for other purposes percent custer gallatin national forest 517 195 17 595 964 20 577 591 19 249 141 8 610 629 20 614 349 20 bison no plan direction proactive bison support proactive bison support most proactive bison support less proactive bison support most proactive bison support bighorn sheep disease prevention permitted grazing of domestic sheep or goats no plan direction risk assessment per policy no in pryor ab or mhg gas yes with risk assessment elsewhere no in pryor ab or mhg gas yes with risk assessment elsewhere no forestwide yes forestwide with risk assessment no in pryor ab bbc or mhg gas yes with risk assessment elsewhere bighorn sheep disease prevention public and outfitter use of recreational pack goats no plan direction no in pryor ab or mgh gas yes with risk assessment elsewhere no in pryor ab or mgh gas yes with risk assessment elsewhere no forestwide yes forestwide with risk assessment yes with conditions in pryor ab and mgh gas yes with risk assessment elsewhere until occupied by bighorn sheep bighorn sheep disease prevention agency use of domestic sheep or goats for weed control no plan direction risk assessment per policy yes forestwide with risk assessment yes forestwide with risk assessment no forestwide yes forestwide with risk assessment yes forestwide with risk assessment connectivity no plan direction plan components and key linkage areas plan components and key linkage areas plan components and key linkage areas plan components plan components and key linkage areas key linkage area acres 0 60834 59528 60834 0 62751 aircraft landing strip acres 1022282 923303 894506 0 924574 920128 note 1 reprinted from final environmental impact statement for the 2020 land management plan custer gallatin national forest by usda forest service 2020 summary table 2 2 ga geographic area ab absaroka beartooth mountains geographic area bbc bridger bangtail crazy mountains geographic area mhg madison henrys lake and gallatin mountains geographic area table a2 relative contributions to social and economic sustainability by alternative table a2 key social and economic benefit from the national forest relative contributions greatest to smallest left to right clean air e a b c d f clean water aquatic ecosystems and flood control d b c f e a conservation of wildlife and rare plants including species for fishing hunting and wildlife viewing d b c f e a designated areas a b c d e f land allocations e g rwa bca b c d e f a educational and volunteer programs b c d e f a fire suppression and fuels management d a b c f e forest products including timber firewood christmas trees berries mushrooms e b c f d a permitted livestock grazing a b c f d e income payments in lieu of taxes secure rural schools labor income in various industries recreation timber grazing etc e b c f d a infrastructure a b c f e d inspiration including spiritual inspiration b c d e f a jobs and induced jobs including recreation timber grazing etc e b c f d a mineral and energy resources a e b f c d preservation of historic cultural tribal or archeological sites d c f b a e sustainable recreation b c d e f a scenery d c f b a e note 1 reprinted from final environmental impact statement for the 2020 land management plan custer gallatin national forest by usda forest service 2020 summary table 9 usda forest service 2020c 2 alternative a represents the current plans in this table alternatives are ordered left to right from greatest to smallest contribution to social and economic sustainability alternatives in parentheses and separated by a slash denote similar contributions table a3 projected timber volume productions and management activities averaged for the first five decades from the six cgnf s alternatives table a3 prism s outputs alt a alt b alt c alt d alt e alt f intermediate acres per year 719 672 674 609 777 668 fuels acres per year 2256 2332 2330 4473 1215 2343 regeneration acres per year 409 398 394 287 523 400 prescribed fire acres per year 2746 2756 2766 2981 2732 2744 total area treated acres per year 6130 6158 6165 8349 5247 6155 timber sale quantity mmbf per year 10 10 10 6 15 10 note 1 reprinted from final environmental impact statement for the 2020 land management plan custer gallatin national forest by usda forest service 2020 volume 3 table 16 usda forest service 2020c 2 mmbf millions of board feet table a4 sensitivity analysis based on first decade harvest volume each run has one set of constraints eliminated a set of constraints is a limiting factor when eliminating them would result in a higher decade one volume table a4 constraint set sr1 sr2 sr3 sr6 sr7 sr8 sr9 sr10 ending inventory yes yes yes yes yes yes yes yes non declining timber volume yes yes yes yes yes yes no yes thinning constraints yes yes yes yes yes no yes yes dispersion of openings yes yes yes yes no yes yes yes lynx wildlife constraints yes yes yes no yes yes yes yes management area volume 80 yes yes no yes yes yes yes yes custer vs gallatin volume 20 yes no yes yes yes yes yes yes prescribed burn constraints no yes yes yes yes yes yes yes decade 1 mmbf volume per year 33 53 28 12 28 12 29 36 28 12 33 95 94 96 28 12 note 1 reprinted from final environmental impact statement for the 2020 land management plan custer gallatin national forest by usda forest service 2020 volume 3 table 17 usda forest service 2020c 2 sr sensitivity run mmbf millions of board feet 
25517,prism a new forest management decision support system was developed for united states national forest planning at the same time existing tools were deprecated prism has a friendly graphical user interface to facilitate model development database management sensitivity analysis and data visualization it integrates sqlite engine for database storage and exploration it supports both classic forest planning model i and ii to schedule management activities and achieve multiple management objectives through the lifecycle of a forest plan multiple models may be created through the prism interface and solved in batch mode for convenient sensitivity analysis various options to visualize model outputs are available through the embedded jfreechart library the open source nature of prism makes it free to use and allows for long term software development and collaboration this paper presents the architecture core functionality and selected applications of prism in the united states national forest planning keywords graphical user interface goal programming optimization forest management open source data availability link to source code is included in the manuscript software availability name of software prism description open source decision support system for forest planning developers dung nguyen eric henderson yu wei david anderson year first available 2016 hardware required 64 bit computer software required java jdk 15 or later versions program language java program size 72 megabytes version 3 2 01 cost and license free under the gnu general public license version 3 availability http bitbucket org prism members prism src master 1 introduction forest planning often includes identifying a long term forest management direction and evaluating effectiveness of forest management practices in achieving the objectives of the management plan historically forest managers have used decision support systems dss to inform the development and implementation of forest management plans in this paper we introduce prism a newly developed dss that has been used to support planning efforts by the united states forest service usfs here we present the architecture core functionality and performance of prism and discuss its utility in real world applications the usfs administers 188 million acres of land organized into 154 national forests hoover 2019 each forest maintains a land management plan i e forest plan that describes how the forest should be managed to achieve recreation livestock grazing timber harvesting watershed protection and wildlife habitat conservation objectives usda forest service 2008 the national forest management act of 1976 16 u s c 1604 requires every national forest to periodically revise its forest plan to address changing management goals forest conditions and public values plan development and revision is often carried out by an interdisciplinary planning team of natural resource experts with input from the public and stakeholders and is a complex process that may take five to seven years to complete haber 2015 a forest plan is comprised of several components including long term aspirational goals called desired conditions shorter term measurable objectives a set of management standards and guidelines to follow and the suitability of lands that defines where certain management activities are permissible 36 cfr 219 7 2 3 moving a forest from its current conditions to the desired conditions is often a complex problem that involves consideration of multiple management goals physical and financial constraints and the interaction of management actions and vegetation conditions through time the best solution to this problem is not usually readily apparent evaluating multiple management strategy options and their associated effects is one way to determine a good management plan united states national forest planning implements the national environmental policy act 42 u s c 4321 et seq where alternative management strategies such as suitability of lands or harvesting levels are analyzed to determine their environmental social and economic effects analysis of land management plan alternatives is often supported by dsss to provide comparative information for decision makers by comparing the beneficial and adverse effects of different alternatives forest managers can evaluate trade offs and make an informed decision about which alternative should be adopted as the forest plan for more than half a century dsss have been used to assist forest managers in making decisions that account for multiple economic ecological social and legal forest planning considerations reynolds et al 2008 segura et al 2014 packalen et al 2013 the development and evolution of dsss are driven by both the increasing demand for solving complex contemporary planning and the availability of advanced computer technologies vacik and lexer 2014 a 2014 synthesis of decision support systems for forest management planning reported 265 prevalent dsss in 26 countries that can address a broad range of forest planning problems borges et al 2014 many of those rely on mathematical formulations that implement quantitative techniques such as decision analysis simulation and optimization power and sharda 2007 nižetić et al 2007 the usfs has a long history with both simulation based and optimization based dsss to support strategic planning borges et al 2014 schuster et al 1993 rauscher 1999 reynolds 2005 in the period from 1979 to 1996 when us forest managers and policy makers emphasized systematic planning and optimal management choices optimization based dsss including forplan johnson and stuart 1987 field 1984 and its successor spectrum greer and meneghin 2000 were widely used the preference of using optimization dsss were driven by the 1982 planning rule 36 cfr 219 that stipulated the use of optimization methods to determine key plan requirements such as sustainable harvest maximum present net value and the suitability of lands beginning in the late 1990s there has been a trend to adopt simulation systems for the us national forest planning typical simulation based dsss for national forest planning in this period were vddt and its current version syncrosim st sim daniel et al 2012 simpplle chew et al 2004 and landis hallett et al 2017 scheller et al 2007 xi et al 2020 optimization models and simulation models have different strengths and associated weaknesses while optimization models are strong at finding good strategies for achieving the desired objective they must be told explicitly the level of disturbance to expect in the future which cannot be exactly known in reality boychuk and martell 1996 simulation models can be used more flexibly to reflect the uncertainty of future disturbances but often must be given the rules for when and how much management to implement gustafson et al 2000 shang et al 2012 simulation models may not achieve the desired objective as efficiently as optimization models forest managers often have to consider these trade offs when deciding which model to use in 2016 the northern region of the usfs and colorado state university began collaborating to develop the optimization model prism as a pilot project to support the custer gallatin national forest plan revision the name prism is an acronym for plan level forest activity scheduling model this tool allows usfs managers to rapidly develop long term forest management strategies and compare the effects of various plan alternatives two main factors contributed to the genesis of the prism project first there was a need to respond to the promulgation of the forest service s 2012 planning rule 36 cfr 219 which emphasizes 1 using the best available scientific information to inform forest planning 2 achieving vegetation desired conditions and ecosystem integrity objectives 3 a financial shift from maximizing present net value to completing projects within budget and 4 completing a planning revision within a shorter length of time usda forest service 2011 nie 2019 second the usfs planning software spectrum was deprecated due to obsolete programming languages and a lack of maintenance capacity the resulting prism dss makes several improvements over its predecessors it incorporates better computer programming technology and contemporary decision science development was done with java and an established long term support plan prism is maintained as an open source project which provides opportunities for long term software maintenance and developer collaboration it incorporates enhanced modelling techniques such as a flexible model i and ii formulation structure prism s design facilitates rapid model development and streamlined debugging the software reduces the complexity of model development by simplifying input requirements the software user interface provides an intuitive platform to include natural disturbances in the model formulation prism allows for virtually limitless attribute tracking that can be used in both the model formulation and back end reporting model runs can be batched to allow for efficient sensitivity analyses and to minimize user set up finally run outputs can be readily analyzed through both predeveloped and customized graphs charts tables or summary queries that can be saved and used in other related model runs the challenges in developing prism were similar to those associated with developing environmental modelling dsss including user engagement application of advanced technologies product adoption and measurement of product success mcintosh et al 2011 therefore we adopted some of the environmental dss development best practices when developing prism mcintosh et al 2006 2011 laniak et al 2013 jakeman et al 2006 pastorella et al 2015 for example to overcome the challenge of engaging end users we participated in several team meetings with the custer gallatin national forest to ensure prism provided the functions required by the forest planning process to ensure technology competitiveness we used a contemporary programming language java to build prism utilized well established methods i e model i and ii in forest management gunn 2007 davis et al 2001 martin et al 2017 mcdill et al 2016 belavenutti et al 2018 martin 2013 to develop the prism optimization model established connections to advanced solvers cplex for better model performance and integrated third party libraries to improve data management sqlite bhosale et al 2015 and data visualization jfreechart jfreechart homepage 2022 we created an easy to use interface for building models and displaying model results at different stages of the planning process this in turn creates opportunities for silviculturists biometricians analysts and planners to participate in model development and validation through the entire planning process finally prism was developed as an open source software package so that it is inexpensive to use update and maintain this paper describes the architecture interface design and key functionality of prism to emphasize the software s utility and ease of use several real world applications are discussed to showcase the software s capabilities and performance while initially developed for national forest planning prism has the potential to support more general forest management and we expect the software will be of interest to forest owners and managers outside of the usfs this software can be freely downloaded utilized and customized with the acceptance of an open source licensing agreement 2 forest planning analysis components and processes developing a prism model requires the user to compile two key datasets including a description of the current state of the forest strata and a menu of options yield tables for managing forest strata through time forest vegetation in each of these strata is grown forward through numerous future management trajectories prescriptions that is compiled in yield tables information about strata and yield tables are stored as tables in a relational database in prism database information is then used to build forest planning models directly through the software interface users must describe any restrictions on management as the model s constraints a measure of solution quality is defined through the model s objective function the goal is then to determine the prescription s to choose for each stratum that stay within the constraints and result in the best objective function value possible the first dataset required by prism is a description of the forest delineated into multiple strata forest areas within each stratum share identical characteristics that represent a unique and meaningful ecological economic and or social class these characteristics also called forest attributes can be static or dynamic depending on whether they would stay as a constant or change across time for example a geographic location on the forest is a static feature that represents a spatial aspect of the forest and does not change through time forest tree size class forest species composition and structural states are all dynamic attributes that are expected to change through time but may be included in the strata definitions that depict the current condition of the forest yield tables are the set of possible vegetation growth trajectories for each stratum resulting from different management strategies i e prescriptions yield tables store dynamic attributes of the strata at each time step or planning period such as timber volume wildlife habitat suitability or species composition some important dynamic attributes including vegetation type size and density are often used together to represent the state of forest strata which in turn can be used to define the vegetation desired conditions each prescription in a yield table is a unique series of forest management activities applied to a stratum through time powell 2013 british columbia and ministry of forests 2000 salwasser management activities may include thinning clearcut planting prescribed burning herbicide and or fertilization etc prescriptions must be designed such that the types and timing of management activities are technically practical and ecologically sound for example a harvest prescription may include a thinning activity to prepare for the final clearcut several decades later prescriptions may also be included to capture the effects of natural disturbances such as wildfire insect outbreaks or wind events although natural disturbances are generally modeled as predetermined proportions of forest impacted rather than management choices a no management prescription option for each stratum is also included to describe the trajectory of vegetation change in the absence of both management and disturbances constraints in a forest planning model typically reflect the limitations on implementing management activities they are often built to comply with the standards guidelines and suitability of lands described in the forest plan usda forest service 2008 for example there may be limits on the amount of prescribed burning that can be implemented in a year or a desirable amount of wildlife habitat to maintain through time another common constraint is the sustainability of timber harvest levels through time called non declining yield under the 2012 rule forests are expected to act within its fiscal capability i e stay within budget therefore the total cost across all management activities in a given time period can be included as a constraint that should not be exceeded the model s objective function is a measure of solution quality and often represents the primary objective of forest management in usfs planning under the older 1982 rule often the objective was the maximum total present net value from all planned forest management activities through a planning horizon under the 2012 rule the objective function value is typically the minimum departure from the vegetation desired conditions in a forest which represents the ecological objectives of the forest plan objective functions can also be used for exploratory information such as to identify the most amount of timber that can be sustainably harvested in the near future once the basic information about a forest is defined managers must then coordinate the management of all forest strata through multiple periods of the planning horizon to achieve the primary objective while meeting all the management constraints this complex problem is why dss models are often needed to inform the selection process 3 prism architecture 3 1 overview prism is an optimization dss built on linear programming lp models lp model formulations have been widely used for the design and analysis of forest management strategies two common lp formulations used in forestry are named model i and model ii gunn 2007 davis et al 2001 martin et al 2017 both formulations are designed to allocate management activities across a forest landscape over multiple periods of a planning horizon a detailed review of both formulations is beyond the scope of this study but can be found in many past publications in forestry literature martin et al 2017 mcdill et al 2016 belavenutti et al 2018 model i and model ii formulations have been applied in forest planning dsss such as forplan spectrum or woodstock www remsoft com prism allows users to implement both model i and model ii formulations descriptions and illustrations of the model i model ii and the prism hybrid model will be described in next sections we developed prism as an open source software and made it accessible under the terms of gnu gpl3 a free copyleft open source software license the gnu gpl3 license allows users to freely download use distribute and modify prism the copyleft clause requires that derivative works from prism i e its modified program and source code must be released to the public under the same gnu gpl3 license the open source nature of prism makes the software inexpensive to use update and maintain the prism software includes three subsystems a database subsystem for managing forest specific data a modeling subsystem for designing forest planning problems as lp models and solving those models and an output subsystem for examining model solutions fig 1 the three subsystems are wrapped into a gui to facilitate interaction between users and the software many open source add ins e g icons tools and libraries are used to build the gui and provide functionality for the subsystems detailed design and functionality of the prism s subsystems are described in the next sections 3 2 the database subsystem the database subsystem facilitates storing manipulating and examining forest planning input data in a relational database databases are displayed hierarchically with a tree structure in the gui of this subsystem fig 2 a a forest database is comprised of two key information strata inventory and yield tables i e prescriptions forest attributes are used to delineate forest strata and project forest condition changes over time for each stratum under the impact of different prescriptions there is no restriction on the number of attributes that can be included in a forest database prism includes an import function that loads information from delimited text files i e csv files and store them as database s tables delimited file specifications are defined in the prism user manual link available in the software availability section a noteworthy data management feature is that users may include multiple sets of strata and yield tables in the database to distinguish different forest plan alternatives sqlite serves as the engine in prism to support many database operations including sql structured query language commands a user friendly feature of prism is it includes prewritten sql commands i e queries designed to manipulate and examine forest data fig 2b so most users do not need to learn sql to do basic data exploration and manipulation for additional flexibility users may write their own sql queries through the database subsystem gui user defined queries may be saved to the prism sql library that will be added to the dropdown query menu fig 2b for future usage 3 3 the modeling subsystem the modeling subsystem reads and validates forest strata and yield tables data from the database subsystem allows users to design forest planning problems and calls an lp solver to determine the optimal management strategy fig 3 a modeling subsystem gui is provided to help set up the following model components planning horizon how many time periods are in the projection and how long is each period goal oriented objective function what is the main objective of the forest and how do you measure success e g movement towards desired conditions model formulation prism automatically decides whether to use a model i a model ii or a combination thereof the decision of which model to use is based on the design of management prescriptions in the input database natural disturbances how many types of natural disturbances will be defined in the model how likely are they to occur and what are their impacts management costs what are the financial costs associated with different management activities user defined constraints what are the management requirements this encompasses a wide range of options including timber production and sustainability targets restrictions on management activities wildlife habitat area requirements budget limitations desired forest species distribution etc solver the lp solver used to identify the optimal management strategy sections from 3 3 1 to 3 3 6 will highlight some of these features in the prism s modeling subsystem more detailed information can be found in the user manual of prism available for download from the website provided in the software availability section 3 3 1 goal oriented objective function in prism management problems are defined using a non preemptive goal programming approach this approach was also used in other software such as a project scale forest planning tool called ned twery et al 2000 and its successor ned 2 twery et al 2005 in this approach forest management goals are optimized simultaneously and weighted according to their importance for example desired future forest species composition can be constructed as a series of goals for each unique vegetation condition each goal specifies a target level as either an absolute value or a numeric range between the minimum and maximum acceptable levels the importance of each goal is defined with a user defined weight which is used to calculate a penalty if the goal is not met when a goal cannot be achieved under achievement or over achievement compared to the target level the weight is multiplied by the deviation from its target level to determine the penalty the objective function of a prism model is to minimize the sum of penalties across all goals prism can also examine single objective problems such as finding maximum timber volume in this case users simply set an arbitrarily large timber volume as the only goal in the formulation with a non zero weight assuming the goal is larger than the volume determined by the solution the solution will represent the maximum possible volume the detailed goal programming formulation implemented in prism is described in the appendix allowing users to easily model unlimited number of goals is a major contribution of prism design that makes it suitable for forest planning with multiple objectives toward desired future conditions 3 3 2 model formulation model i fig 4 a and model ii fig 4b are the two well established lp model formulations used in forest management and planning a major difference between these two models is the consistency through time of forest management strata responding to regeneration events such as timber harvest or lethal disturbance regeneration events result in bare land that has associated management choices such as natural regeneration or planting to a particular tree species a model i formulation allocates the area in each stratum to management prescriptions through the entire model duration or planning horizon in contrast a model ii formulation tracks the area in each stratum assigned to prescriptions that span the length of time between forest regeneration events in contrast to the entire planning horizon each forest stratum used in model i retains a constant area through the planning horizon the area of model ii strata change through time as portions may become bare lands at every period due to harvest or disturbance events bare land areas originated in the same period can be combined to create new strata for regenerating different vegetation species prism uses a hybrid model structure to define management options for forest strata fig 4c this has several advantages 1 users do not have to design all prescriptions to expand through the entire planning horizon typical model i design for example the same model ii harvest prescription may be created and used repeatedly to manage a stratum through any 5 consecutive periods 1 5 2 6 3 7 etc of a planning horizon 2 prism model allows flexible strata management because users can define rules for merging strata that regenerate in the same time period for example users can require that only the mixed conifer and lodgepole pine strata harvested in the same period may be merged they can also choose specific vegetation available from those model ii prescriptions for regeneration such as the merged mixed conifer and lodgepole strata after harvest may be naturally regenerated as mixed conifer or can be planted for a cost to lodgepole 3 model selection requires no user effort because prism automatically implements the appropriate model i or ii or both for each stratum based on its available prescriptions 3 3 3 ability to use forest attributes in the problem formulation prism has a user friendly gui to account for both static and dynamic forest attributes fig 5 in the lp formulation specifically the gui allows users to select forest strata based on static attributes fig 5a specific periods of interest fig 5b and dynamic attributes fig 5c for example the checked boxes in fig 5 would select strata in the montane area with lynx habitat where the initial vegetation is ponderosa with medium and large tree sizes given that in the second period the trees in those strata are at age class 3 and the density is within 300 600 trees per acre the selected strata can then be used to allocate natural disturbances assign management costs and define constraints representing various management restrictions on area or cost or forest attributes e g carbon storage wildlife habitat value associated with those strata across time as described in the next sections of this paper 3 3 4 natural disturbances prism can model both non lethal disturbances nds and lethal disturbances lds nd events are assumed to damage a portion of the trees in a stratum while ld events are lethal to most or all trees in the affected stratum area after an ld event a newly regenerated forest starts a new life cycle non lethal disturbances are modeled at the prescription generation stage with model i prescriptions built to track the effects of predefined sequences of nd events through the entire planning horizon e g mountain pine beetle outbreaks every 30 years prism s gui allows users to assign appropriate prescriptions with nd events to a portion of each stratum in contrast to nds lethal ld disturbances are modeled without using prescriptions ld events are defined directly through the prism gui according to what strata the events occur in occurrence times proportion of each stratum impacted corresponding regenerated vegetation and regeneration trajectories after each ld event i e appropriate model ii prescriptions to manage the new bare land strata after lds prism models the impacts from nds and lds in different ways nds are modeled deterministically as fixed percentages of the total stratum area disturbed while lds are modeled as stochastic percentages the deterministic approach simply uses the user defined average probability in each stratum during each planning period the stochastic approach draws random disturbance levels across the planning horizon based on the user defined probability distributions 3 3 5 management costs prism also has a gui to help users define management activity costs including the per area treatment cost cost associated with a dynamic attribute such as timber volume and regeneration cost that varies by specific vegetation types to plant costs can also be built as fixed values into prescriptions in the forest database however defining costs directly through the prism s gui makes it easier to change and test different assumptions without the need to modify the database for instance two strata managed by the same prescription may be defined in the gui to have different treatment costs if road building is required to access one stratum but not the other 3 3 6 user defined constraints prism is designed to efficiently develop and test constraints in a forest planning model constraints are categorized as non relational and relational each with a separate interface design some relational constraints model the flows of timber cash or other management elements across time and are called flow constraints non relational constraints define direct limitations on area cost or other quantified forest conditions or production levels fig 6 examples include a minimum amount of wildlife habitat area a maximum amount of budget spending or a limit on the timber volume that can be generated from clearcuts etc relational constraints can be used to model other relationship requirements fig 7 for example they can be used to limit timber volume fluctuation between two consecutive time periods control the ratio of even aged to uneven aged forest treatments within the same period or define ending period inventory of standing volume relative to the average standing inventory of the previous several periods the combination of the two constraint categories creates a flexible modeling framework to describe a forest management problem with various management requirements more comprehensive definitions and explanation of these constraints can be found in the prism goal programming formulation section of appendix 3 3 7 solution options prism supports running multiple models in two available batch modes the sequential mode solves models one after another while the parallel mode implements a java concurrency technique multi threads to enable solving many models simultaneously presently the prism interface provides two solver options lpsolve free open source solver or cplex commercial solver requiring a license lpsolve can handle small to medium size lp problems large models such as those with more than one million variables may require the more powerful cplex solver those two options make prism useful for both academic training lower cost and industrial applications 3 4 the output subsystem after an lp model is solved the output subsystem gives users a variety of options to manage and visualize the model solution through both standard and custom output options standard outputs include the following solution metrics such as solution time objective function value number of decision variables and number of constraints information about decision variables in the optimal solution including their values and reduced costs information about constraint levels at the optimal solution including dual and slack values a query framework that includes predefined sql commands to summarize model outputs reports and graphical displays of the model outputs such as selected management activities for each stratum and constraint levels the output subsystem uses an open source chart library jfreechart to visualize the output data by building multiple types of charts e g pie or bar charts in fig 8 the data visualization feature can help users get quick insights from the model results in addition to the standard outputs prism allows for the development and storage of customized sql commands as well as gui enabled query tools to examine the optimal solution without sql outputs from sql queries and the non sql gui tool queries can be exported to external software for supplemental analysis and display options 4 prism application and performance since 2016 prism has been used to support land management planning for several us national forests these include the custer gallatin national forest cgnf in southern montana and western south dakota the helena lewis and clark national forest hlc in north central montana and the nez perce clearwater national forest npc in north central idaho the land management plan for each forest required a detailed analysis of multiple management alternatives to comply with the direction in the 2012 planning rule and the national environmental policy act for each national forest six management alternatives were developed based on the forest s internal assessment and public comments and represent a range of different management strategies and outcomes alternatives may differ by management emphasis appendix table a1 associated with different levels of contribution to social and economic sustainability appendix table a2 differences among alternatives are due to local and national stakeholder group preferences that prioritize certain social and economic benefits over the others for example the cgnf designed alternative d for the greatest contributions to clean water fish and wildlife conservation wildfire and fuel management cultural historic tribal resource protections and scenery while alternative e was designed for the greatest contributions to clean air timber production income and job opportunities alternatives b c and f were aimed at balancing a mix of contributions across all stakeholder groups appendix table a2 prism was used to analyze the long term outcomes and effects of the alternatives for each of these plans each plan alternative is built as a distinct prism model the objective for each model was to schedule vegetation treatments to move the forest toward desired conditions within a 15 decade planning horizon through weighting the benefits and trade offs of each alternative over the life of the forest plan forest managers can make informed decisions on which alternative to implement in the plan comprehensive prism results for each plan are available on the usfs plan revision web pages usda forest service 2020a usda forest service 2021 usda forest service 2019 sections below describe the application and performance of the software 4 1 prism application table 1 summarizes the prism model inputs from the three different national forests for each forest strata were delineated based on various static spatial attributes such as geophysical political and management area designations vegetation attributes were derived from the northern region existing vegetation mapping program usda forest service 2020b prescriptions and associated forest dynamic attributes were derived from projecting forest inventory and analysis data wurtzebach et al 2020 of each forest into the future with the forest vegetation simulator software crookston and dixon 2005 the three forests used a broad range of forest management cost categories including timber sale preparation and administration reforestation timber stand improvement road reconstruction and administration prescribed burning weeds treatment and other surcharges specific cost values varied by management activities static attributes such as physical location and dynamic attributes such as vegetation type and density three types of natural disturbances were modeled on the three forests two disturbances were non lethal mixed severity wildfire and severe insect infestation and the other was lethal wildfire the usfs relied on historical burn severity and insect outbreak data to identify the expected amounts of future natural disturbance events for example the cgnf assumed a 50 year interval for mixed severity wildfire 60 year interval for severe insect outbreaks and between 9000 and 11 500 acres per year of lethal fire disturbance amounts were put into prism as fixed percentages of area that would be disturbed in every decade a wide variety of constraints were built to ensure compliance with laws policy and the management direction of the proposed plan alternatives constraints were classified into several groups including budget constraints budget limits on management activities harvest policy constraints non declining volume and other restrictions on timber harvest dispersion of openings constraints distribution of management activities across strata to prevent habitat fragmentation at any point in time prescribed burning constraints the upper limit of prescribed burning area to meet air quality standards operational capacity and weather limitations wildlife habitat constraints to help ensure sustainable habitat levels and treatment constraints to meet fuel reduction area targets and other management requirements those constraints were designed to meet management requirement targets both under and over achievements of the required management targets are not allowed another group of constraints was built to model the desired area distribution including both the minimum and maximum levels for different forest vegetation types and structure size classes these constraints allow for deviations from the desired vegetation condition levels that were used as the basis for the goal based objective function prism was used to generate the projected acres of management activities and annual timber volume see appendix table a3 for the cgnf reports included directly in each forest s land management plan prism was also used for sensitivity and trade off analyses to evaluate the effect of various plan components expressed as constraints on the forest s ability to meet desired conditions for example by systematically eliminating constraints from the model and measuring the maximum volume achieved in the first decade the cgnf found that the non declining timber volume constraint had the most impact on management results appendix table a4 another analysis was conducted by the hlc where prism was used to quantify trade offs between different budget allocations and identify whether the budget was not a limiting factor 4 2 prism performance to evaluate prism performance models from the three national forests were solved by using ibm s ilog cplex 12 8 on a 64 bit workstation equipped with a dual core 2 5 ghz processor and 32 gb of ram for each national forest six prism models associated with the six plan alternatives developed for that forest were solved prism was able to solve every existing model and reported corresponding results within 30 min the average results are presented for each forest in fig 9 model size and complexity vary for each forest reflected by number of variables and constraints in the model model performance reflected by total run time and memory usage is different depending on model size and complexity models developed for the cgnf hlc and npc respectively have increasing sizes as measured by the number of variables and the number of constraints fig 9a larger models require more computer memory to store and process the model data and solve the associated lp problems fig 9d larger models typically require longer total run times fig 9c here the total run time is the sum of 1 input processing time the time it takes prism to read the user inputs from the gui and formulate the lp problem 2 solution time how long it takes cplex the default solver to solve the lp model and 3 output writing time how long it takes to prepare and import model results to the output subsystem there is a positive relationship between model size fig 9a and the input processing time and the output writing time fig 9c the number of non relational constraints is often much larger than the number of flow constraints for each forest fig 9b both non relational constraints with tighter bounds and flow constraints can make an lp model difficult to solve for example even though the npc models are larger fig 9a they take less time to solve fig 9c likely because they have fewer flow constraints fig 9b sensitivity tests could help define the impacts of different constraints on solution time but this is not our focus in this study 5 discussion and conclusions prism is a new forest planning dss developed through an agreement between the usfs northern region and colorado state university it has been used by several us national forests to support the development of forest plans but its functionality is flexible enough to support planning efforts by other forestland owners and managers as well prism has several strengths that may make it a preferable tool for forest management analysis first it is available through a no cost open source license its database management is supported by the open source sqlite database engine and visualization options are provided by the open source jfreechart library while a powerful commercial solver may incur a licensing fee smaller problems may be solved by the open source solver lpsolve integrated in prism another strength of prism is its architecture to support rapid model development and testing process prism allows users to delineate a forest into many strata incorporate multiple choices for strata management and define various model components natural disturbances management costs management constraints directly through the gui gui features such as the constraints section in particular that allows users to quickly switch constraint types facilitate solution space exploration and pinpointing infeasibilities the ability to run models in batch mode to quickly compare the results is another advantage of prism it saves significant time for analysts to run multiple scenarios and present results to decision makers finally the source code for prism is stored and managed through a git control open source website that is convenient for future software maintenance and expansion natural disturbance and associated uncertainty create challenges for determining an optimal management schedule based solely on using optimization dss to overcome these challenges the three us national forests discussed here integrated prism with other dss software to form an analytical system specifically at the front end the forest vegetation simulator crookston and dixon 2005 was used to determine vegetation growth trajectories for forest strata including the effects of non lethal disturbances this information made the yield tables that served as inputs for prism prism was then used to schedule forest management activities through time under deterministic impacts from both non lethal and lethal disturbances lethal disturbances are defined through prism s gui the management schedule from prism was in turn analyzed with multiple stochastic disturbance scenarios by a state transition model simpplle chew et al 2004 to evaluate the management effectiveness in the context of future uncertainty in such an iterative analytical system parameters of each model can be adjusted based on the outcomes of another to gain a better understanding of the interaction between management and ecological processes chick et al 2019 integrating multiple dss models is a time consuming process as it requires that each model be parameterized calibrated and run prism holds the potential for improved future efficiencies if it is modified to simulate stochastic disturbances with better details this would allow simultaneous evaluation of management and ecological processes in a single prism model multiple prism model runs could produce a range of possible management activities and ecological trajectories some further options worth investigating include stochastic programming models wei et al 2014 that can provide a solution by simultaneously accommodating many possible future scenarios a rolling horizon mechanism savage et al 2010 that evaluates forest management strategy by an adaptive management approach or simply running multiple scenarios of the same model with stochastic disturbances if developed these capabilities would be another example of how prism has improved upon the previous generation of usfs planning models prism has the potential to be maintained and expanded through time it was developed using an open source development platform this form of collaboration allows more flexibility for both the end users and the developers to continuously maintain and improve prism it also encourages participation from other potential users or developers to the software s future development the open source nature of prism enhances its chance to be adopted by managers and experts in the domain of forest management and planning declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the prism project was funded by a challenge cost share agreement 16 cs 11015600 015 between the united states forest service northern region and colorado state university appendix prism goal programming formulation prism formulates the planning problem as a linear programming lp model formulating a lp model requires identifying a set of decision variables for each stratum to represent where when and what management prescriptions may be chosen in prism decision variables are associated with forest strata through information stored in yield tables decision variables can be incorporated into constraints to capture various assumptions relationships and requirements of managing the forest they may also be incorporated into the model s objective function that minimizes the total deviation from multiple forest management goals the solution determined by the prism model indicates a forest management plan that leads to the best objective function value the mathematical expression of the prism lp formulation is provided here objective 1 m i n n w n l n w n u n constraints set 1 non relational constraints 2a y n l n l n n c s o f t 2b y n u n u n n c s o f t 3a y n l n n c h a r d 3b y n u n n c h a r d 4a y n 0 n c f r e e 4b y n n c f r e e constraints set 2 relational constraints flow constraints 5a n c m k 1 y n l m n c m k y n m k 1 k m 1 5b n c m k 1 y n u m n c m k y n m k 1 k m 1 constraints set 3 transitional constraints 6 i i x x s i 1 s i i x x s s i 1 v s s 7 1 p x x s i t x s i t 1 s i i x t 1 t 1 8 1 p x x s i t a x s i t 1 a 1 s i i x t 1 t 1 a 9 i i x p x p s s x x s i t a i i x p x p s s x x s i t a f s s t s s t 1 t 1 10 s f s s t s i i x x s s i t s i i x a x s s i t a i i x x s i t 1 1 s i i x x s s i t 1 1 s t 1 t 1 indexes n index of a non relational constraint m index of a relational i e flow constraint k index of a flow s component s s s index of a stratum based on user defined static attributes such as physical location timber suitability etc i index of a prescription a prescription can capture the effects from various silviculture methods i e natural growth prescribed burning group selection even aged harvest or non lethal natural disturbances i e mixed severity fire insect outbreak t index of a planning period a index of an age class parameters l n lower bound of constraint n l n 0 u n upper bound of constraint n u n 0 w n weight for one unit less than l n w n 0 w n weight for one unit more than u n w n 0 k m number of components of flow m k m 2 l m lower bound of flow m l m 0 u m upper bound of flow m u m l m v s area associated with the existing stratum s at the beginning of planning horizon p x user defined loss rate due to lethal disturbances in the existing stratum area x p x user defined loss rate due to lethal disturbances in the regenerated stratum area x p s s x conversion rate associated with p x where s p s s x 1 s p s s x conversion rate associated with p x where s p s s x 1 s sets c f r e e set of free non relational constraints c h a r d set of hard non relational constraints c s o f t set of soft non relational constraints c m k set of user defined continuous variables that creates the component k of the flow m for example the flow m can be described by 3 components k 1 2 3 where each component may contain multiple continuous variables such as c m 1 y 1 c m 2 y 2 y 3 and c m 3 y 5 y 7 y 9 i x set of prescriptions that are feasible for the existing strata variable x i x set of prescriptions that are feasible for the regenerated strata variable x variables x s i t area of the existing stratum s in the period t which would be managed by prescription i and the activity to be implemented in period t is not clear cut x s s i t area of the existing stratum s in the period t which would be managed by prescription i the activity to be implemented in period t is clear cut and the regenerated landscape attributes identification would be s in the next period t 1 x s i t a area of the regenerated stratum s in the period t at age class a which would be managed by prescription i and the activity to be implemented in period t is not clear cut x s s i t a area of the regenerated stratum s in the period t at age class a which would be managed by prescription i the activity to be implemented in period t is clear cut and the regenerated landscape attributes identification would be s in the next period t 1 f s s t total area of the stratum s subjected to lethal disturbances in period t and regenerated into the forest landscape identification s at age class one in period t 1 y n a user defined continuous variable to calculate a quantified level of either area e g acres or cost e g dollars or any numeric forest attribute e g number of trees harvested volume associated with certain strata in certain time periods l n total units falling short of l n l n 0 l n u n total units exceeding u n u n 0 might not be true for the case when w n 0 where decision variables use their indices to link to a specific row of a specific prescription in the input database fig a1 prism uses modified model i variables x s s i t a n d x s i t a and modified model ii variables x s i t a a n d x s s i t a specifically the time period t index is not necessary for model i variables but prism uses t to capture the area transition due to the effects of lethal disturbances in each planning period note that lethal disturbance conditions proportions and effects are defined in the prism gui instead of using prescriptions while model ii variables often use the starting and ending period indices associated with each prescription prism uses time period t and age class a as alternative indices with an equivalent effect for example a regeneration prescription to regrow vegetation in a bare land in 4 consecutive periods from 2 to 5 would be associated with the set of t a 2 1 3 2 4 3 5 4 fig a1 design of decision variables in prism prism allow using up to six different user defined groups of static attributes integrated into the s index to index a decision variable x s i t x s s i t x s i t a x s s i t a other indices i t a are used to link every variable to an exact row of an exact prescription in the database that single row contains detailed information of the dynamic attributes associated with the variable fig a1 the objective function equation 1 minimizes the sum of penalties across all forest management goals penalties are aggregated from the user defined soft non relational constraints non relational constraints equations 2 4 users define parameters for each constraint including the bounds upper limit and or lower limit and the per unit magnitude of the penalty if the constraint is violated users also define whether a constraint is hard soft or free depending on how the bounds and penalties are applied a hard constraint requires the value of the variable associated with the constraint to be within the specified bounds a soft constraint allows for deviations from the bounds but incurs penalties for deviations users can specify the penalty value so that some constraints are valued more than others note that penalties are applied to the per unit deviation of the constraint and caution should be taken if mixing soft constraints using different units of measurement e g simultaneously considering a soft per volume constraint and a soft per acre constraint soft constraints collectively define the goal objective function where the sum of penalties across all constraints is minimized a free constraint simply tracks the values of the associated variables without incurring penalties or requiring a specific level to be maintained free constraints may be useful for model summarization in prism constraints may also be excluded from the problem formulation without being deleted from the gui this feature makes it convenient for users to find infeasibilities or conduct sensitivity analyses by quickly including or excluding in subsequent model runs relational constraints equation 5 the user defined continuous variables y n and their associated parameters can be considered together to form relational flow constraints for instance one might define a relational constraint that states the average of timber volume harvested in periods 1 5 must be less than or equal to 20 of the harvested volume in period 6 transitional constraints equations 6 10 equation 6 describes the initial landscape condition at the beginning of the planning horizon period 1 equations 7 and 8 respectively capture the transition of existing forest strata areas and regenerated forest strata areas that are not being clear cut and not subjected to lethal disturbances lds in this case age class would increase by one when moving from a period to the next in contrast to the above two equations equations 9 and 10 identify strata areas that are subjected to either clear cuts or lethal disturbances in a planning period and require those areas to become regenerated forest strata areas at age class one in the next period equation 9 identifies the total area subjected to lds in each planning period for each s s transition equation 10 uses the result from 9 plus the total area subjected to clear cuts then transit the whole areas to become regenerated forest strata areas at age class one in the next period illustration of building a simple model in prism here we provide an example to illustrate how prism can be used to evaluate a simple timber harvesting problem for this example we assume 1 users already have a forest database with sufficient information about strata and prescriptions 2 forest strata may be managed with any of the prescriptions associated with each stratum 3 no disturbances will occur during the planning horizon and 4 the cost of management activities is not considered the model is used to schedule timber harvest through a 5 period planning horizon such that the harvest volume in every period is between 5000 and 10 000 cubic feet there are penalties for both under harvest 2 penalty points per cubic foot lower than 5000 and over harvest 1 penalty point for each extra cubic foot greater than 10 000 which are used in the goal programming objective function to minimize the total penalty this problem can be set up in several minutes through the two gui windows illustrated in fig a2 prism automatically builds the complete model formulation based on information provided in the gui including the objective function 11 several user defined constraints 12 15 and the core transitional constraints 6 10 as presented earlier in the appendix 11 m i n i m i z e n 2 l n u n subject to 12 y n s i x s i t s s i x s s i t s i a x s i t a s s i a x s s i t a n 1 5 t n 13 y n l n 5000 n 1 5 14 y n u n 10000 n 1 5 15 y n l n u n 0 n 1 5 note harvest activities include thinning selected cuts clearcuts and other management activities that will contribute to harvest volume and therefore all types of decision variables can be added into the constraint 12 fig a2 the process to build an example timber harvest model in prism that requires using only two prsim gui windows a screenshot of a prism window to import database and set up some general model inputs including the planning horizon b another window screenshot to create user defined constraints here the two arrows represent the required selections of both time period and rmcuft to set up the constraint for timber harvest in the first planning period the rmcuft parameter represents the cubic feet of harvested timber per acre each user defined continuous variable that represents how many acres to harvest in each period will be linked to a specific rmcuft value in the yield table database fig a2 table a1 forestwide comparison of management alternatives in the cgnf s revised forest plan table a1 issue of emphasis alternative a alternative b alternative c alternative d alternative e alternative f recommended wilderness area number 7 9 9 39 0 7 recommended wilderness area acres 33741 113382 145777 711425 0 125675 backcountry area number 3 9 13 1 2 13 backcountry area acres 38414 124980 299522 5937 171326 208959 recreation emphasis area number 0 8 8 4 12 10 recreation emphasis area acres 0 176958 160665 33408 212689 224608 stillwater complex acres 0 101832 101832 0 101832 101832 miles summer motorized trail no longer suitable 0 0 4 172 0 0 miles mechanized trail no longer suitable 0 0 34 264 0 24 acres winter motorized transport no longer suitable 0 0 24885 234431 0 10128 forested acres suitable for timber production percent custer gallatin national forest 664 628 22 573 275 19 549 115 18 545 274 18 593 735 19 565 536 19 forested acres unsuitable for timber production but where timber harvest is suitable for other purposes percent custer gallatin national forest 517 195 17 595 964 20 577 591 19 249 141 8 610 629 20 614 349 20 bison no plan direction proactive bison support proactive bison support most proactive bison support less proactive bison support most proactive bison support bighorn sheep disease prevention permitted grazing of domestic sheep or goats no plan direction risk assessment per policy no in pryor ab or mhg gas yes with risk assessment elsewhere no in pryor ab or mhg gas yes with risk assessment elsewhere no forestwide yes forestwide with risk assessment no in pryor ab bbc or mhg gas yes with risk assessment elsewhere bighorn sheep disease prevention public and outfitter use of recreational pack goats no plan direction no in pryor ab or mgh gas yes with risk assessment elsewhere no in pryor ab or mgh gas yes with risk assessment elsewhere no forestwide yes forestwide with risk assessment yes with conditions in pryor ab and mgh gas yes with risk assessment elsewhere until occupied by bighorn sheep bighorn sheep disease prevention agency use of domestic sheep or goats for weed control no plan direction risk assessment per policy yes forestwide with risk assessment yes forestwide with risk assessment no forestwide yes forestwide with risk assessment yes forestwide with risk assessment connectivity no plan direction plan components and key linkage areas plan components and key linkage areas plan components and key linkage areas plan components plan components and key linkage areas key linkage area acres 0 60834 59528 60834 0 62751 aircraft landing strip acres 1022282 923303 894506 0 924574 920128 note 1 reprinted from final environmental impact statement for the 2020 land management plan custer gallatin national forest by usda forest service 2020 summary table 2 2 ga geographic area ab absaroka beartooth mountains geographic area bbc bridger bangtail crazy mountains geographic area mhg madison henrys lake and gallatin mountains geographic area table a2 relative contributions to social and economic sustainability by alternative table a2 key social and economic benefit from the national forest relative contributions greatest to smallest left to right clean air e a b c d f clean water aquatic ecosystems and flood control d b c f e a conservation of wildlife and rare plants including species for fishing hunting and wildlife viewing d b c f e a designated areas a b c d e f land allocations e g rwa bca b c d e f a educational and volunteer programs b c d e f a fire suppression and fuels management d a b c f e forest products including timber firewood christmas trees berries mushrooms e b c f d a permitted livestock grazing a b c f d e income payments in lieu of taxes secure rural schools labor income in various industries recreation timber grazing etc e b c f d a infrastructure a b c f e d inspiration including spiritual inspiration b c d e f a jobs and induced jobs including recreation timber grazing etc e b c f d a mineral and energy resources a e b f c d preservation of historic cultural tribal or archeological sites d c f b a e sustainable recreation b c d e f a scenery d c f b a e note 1 reprinted from final environmental impact statement for the 2020 land management plan custer gallatin national forest by usda forest service 2020 summary table 9 usda forest service 2020c 2 alternative a represents the current plans in this table alternatives are ordered left to right from greatest to smallest contribution to social and economic sustainability alternatives in parentheses and separated by a slash denote similar contributions table a3 projected timber volume productions and management activities averaged for the first five decades from the six cgnf s alternatives table a3 prism s outputs alt a alt b alt c alt d alt e alt f intermediate acres per year 719 672 674 609 777 668 fuels acres per year 2256 2332 2330 4473 1215 2343 regeneration acres per year 409 398 394 287 523 400 prescribed fire acres per year 2746 2756 2766 2981 2732 2744 total area treated acres per year 6130 6158 6165 8349 5247 6155 timber sale quantity mmbf per year 10 10 10 6 15 10 note 1 reprinted from final environmental impact statement for the 2020 land management plan custer gallatin national forest by usda forest service 2020 volume 3 table 16 usda forest service 2020c 2 mmbf millions of board feet table a4 sensitivity analysis based on first decade harvest volume each run has one set of constraints eliminated a set of constraints is a limiting factor when eliminating them would result in a higher decade one volume table a4 constraint set sr1 sr2 sr3 sr6 sr7 sr8 sr9 sr10 ending inventory yes yes yes yes yes yes yes yes non declining timber volume yes yes yes yes yes yes no yes thinning constraints yes yes yes yes yes no yes yes dispersion of openings yes yes yes yes no yes yes yes lynx wildlife constraints yes yes yes no yes yes yes yes management area volume 80 yes yes no yes yes yes yes yes custer vs gallatin volume 20 yes no yes yes yes yes yes yes prescribed burn constraints no yes yes yes yes yes yes yes decade 1 mmbf volume per year 33 53 28 12 28 12 29 36 28 12 33 95 94 96 28 12 note 1 reprinted from final environmental impact statement for the 2020 land management plan custer gallatin national forest by usda forest service 2020 volume 3 table 17 usda forest service 2020c 2 sr sensitivity run mmbf millions of board feet 
25518,the recording and simulation data of forest landscapes are massive high dimensional and abstract requiring intuitive representation 3 d visualization is an efficient tool to comprehend possible landscape changes generated by real world or forest landscape models based on current advantages of game engines realism and convenience we developed a platform for 3 d visualization of forest landscapes flv flv streamlines multiple software and programming languages to break barrier between geographic data and game engine and transforms outputs of forest landscape models into visualization parameters compared with previous 3 d visualizations flv has better realism efficiency and navigation we used simulation data of post volcanic eruption forest landscape recovery in changbai mountain as a case study and demonstrated functionalities of flv flv can visualize seamlessly from individual tree to forest stand and landscape scales and from individual year to decades and centuries temporal scales it offers potential solutions for the digital representation of complex environmental change keywords forest landscape visualization flv forest landscape model unreal engine 3d visualization tree model multi spatiotemporal scales software availability name forest landscape visualization contact yuhy507 nenu edu cn year first availability 2021 hardware required ibm compatible pc software required unreal engine houdini engine speedtree photoshop system required windows program languages python r availability the flv platform is assembled as free software utility and is available from github https github com flvyhy flv 1 introduction forest landscape visualization plays an important role in forest resource management land use planning and forest landscape change predictions xi et al 2016 traditionally forest landscapes have been described with 2 d maps sheppard 1989 lovett et al 2015 that are widely used birt et al 2009 xi et al 2020 however 3 d landscape visualization presents special abilities that are more effective in describing the consequences of forest landscape change sheppard 2005 expressing richer information and comparing alternative scenarios xi et al 2016 the forest landscape change is driven by complex spatial interactions of ecological processes such as seed dispersal succession and different disturbances mladenoff et al 1996 these processes take place at various scales in space and time mladenoff 2004 3 d visualization can reveal landscape change from multi scale perspectives through zooming and navigating the scene freely appleton et al 2002 it can assist scientists and managers to apprehend the complex and abstract environmental change processes involved li et al 2021 qin et al 2021 and landscape variations and management effects on spatial and temporal scales sheppard 2005 in addition 3 d visualization provide a way to present complex information to people with no experience in map interpretation bishop et al 1994 simulation results from forest landscape models are ideal for 3 d visualization forest landscape models predict changes in spatial characteristics e g distribution shape abundance at tree level he 2008 simulate forest changes driven by species and stand dynamics and landscape processes including dispersal disturbance and management at landscape scales xi et al 2009 and simulate the spatial and temporal characteristics of periodically occurring spatial processes in a spatially interactive manner he 2008 the whole simulation outputs are typically raster based mladenoff 2004 which are ready for most 3d visualization software 3 d forest landscape visualization has evolved over the past decades professional landscape visualization software e g vns visual nature studio emerged song et al 2011 mostly from 2000 to 2010 the software offered solutions to forest landscape visualization and met the expectations of the time however by current standards they did not keep up with new hardware development and were slow in rendering with reduced realism in scenes besides 3 d visualization software for forest landscapes was expensive and difficult to use carbonell carrera et al 2021 meanwhile game industry has emerged with sophisticated visualization software herwig and paar 2002 under the impetus of game commercialization which has potential to extend to 3 d forest landscape visualization lewis and jacobson 2002 game engines are the most widely used and mature visualization software in the game industry they make visualization technology affordable to use carbonell carrera et al 2021 and have great potential for 3 d landscape construction including highly realistic scenes real time rendering real time roaming oakden and kavakli 2022 interaction simulation of terrain and physical environment shiratuddin and thabet 2011 they are also used in the visualization of environmental elements such as water bryceson et al 2022 and wind rafiee et al 2017 moreover components of the visual appearance of a forest landscape also can be simulate using game engines including terrain forests grasslands sky rivers and light etc gang and huang 2011 finally corresponding to the simulation range and time scale of the forest landscape models the 3 d visualization results can show more realistic scenarios at the stand scale and more information at the landscape scale 1 1 content and objectives the aim of this study is 1 to present a platform for 3 d visualizing landis pro forest landscape model output at flexible scales time and space and perspectives spatial location and orientation the platform includes building a library of tree models and a data driven workflow with game engine as core and other ancillary utilities we 2 test the platform using a real set of historical landscapes reconstructed from post volcanic eruption in changbai mountain northeast 2 workflow the platform for our 3 d visualization of forest landscapes was developed by integrating a range of software and programming languages in the design process we specially focused on realistic visualization scenarios forest visualization results consistent with the source data and a user friendly interactive interface we used four pieces of modeling software and a statistical analysis utility in the platform photoshop is image editing software that we used to extract textures from photos of leaves and tree trunks speedtree https store speedtree com virtual foliage generator is commercial software used for modeling trees adjusting parameters to build 3 d models of different tree species based on tree morphology and field data houdini engine https www sidefx com products houdini engine procedural generation software tools available in both open source and commercial versions is a software that reads the raw data and sets them into the actual scene unreal engine https www unrealengine com game engine for building 3 d scenes open source software is the game engine originally for to the 3d game scene creation and here customized to perform the visualization due to its superiority in building 3d scenes the platform of 3 d forest landscape visualization flv contains three components fig 1 photoshop and speedtree that creates all 3 d models of the tree species fig 1c the r code files that transforms the tree species density distribution from forest landscape model output and the houdini engine that generates the tree point map fig 1a and the unreal engine that generates the scene with the operational interface for spatial navigation and switching between scenes with multiple simulated time nodes fig 1b 2 1 tree model high quality tree models are the basis for the realism of forest landscape visualization earlier visualizations typically used 2 d pictures of real trees to save computer memory compromising the 3 d effects concurrent techniques can display 3 d tree models at different levels of detail lod and view distance in our study we use speedtree which is a professional tree model building software this software can export 3 d tree models using pictures of leaves and trunks of each tree species and adjust the parameters according to tree morphology the images of the leaves and trunks we used are from the china plant image library http ppbc iplant cn the main parameters in the software included tree height diameter length and density of branches and size of leaves the tree models of each species were divided into small medium and large size group as surrogates of trees from young to old each size group had multiple forms of tree models to approximate variable tree forms shapes in the study area 2 2 data transmission we chose the r language and houdini engine software for the scope unification format conversion and batch transfer of landscape data the basic elements of the 3 d forest landscape visualization are terrain and trees the topographic data were obtained from the palsar sensors of the alos satellite that has a resolution of 12 5 m 12 5 m the forest elements were based on the output data of the landis pro forest landscape model wu et al 2020 to process the base data we used the raster package in r to crop and stitch the dem data and then standardized the extent of both the dem data and the model output data to the extent of the visualization area a more complex step is needed to process the forest data and therefore we created a new extension to landis pro called distribution and abundance written in r to generate tree density distribution maps and calculate the total number of trees per raster cell the output of the forest landscape model landis pro contains raster images by tree species and age group in a simulation year we needed to summarize the information from all raster images for a given simulation year to generate density maps from these raster images the density maps were then linear stretched into values from 0 to 1 fig 2 a with 0 the white color indicating no trees and 1 the darkest red color indicating highest tree density we calculated the total number of trees to be scattered in the area where the whole raster map is located the number of randomly scattered points in each cell of the raster map is determined by the magnitude of the density value of the density distribution on each cell the location of each tree is not determined but the number and location of all trees within each single cell is determined the points of trees were scattered in houdini engine according to the total number of trees and the density distribution fig 2b then the points were matched to the corresponding tree models in unreal engine driven by the houdini plugin fig 2c at the same time the trees adapted to the undulations of the terrain and are generated close to the ground fig 2d it is important to emphasize that the houdini engine is only used to establish the logical relationships between the elements the final scene construction was done in unreal engine 2 3 visualization scene unreal engine is used to build 3d scenes that creates highly realistic forest scenes and enhanced visual experience for forest landscape analysis the basic elements when building a forest landscape in unreal engine include mainly terrain and forest information at pixel level we placed the created tree model in unreal engine and then drag and drop the automation file created in houdini engine into unreal engine to automatically generate the terrain and forest to achieve the integrity and aesthetics of the forest landscape we also randomly added non tree objects to the surface our recommendation is to do as much as possible to have accessible data available when building each element we considered two parts of non tree elements 1 accessible vector data such as water bodies and roads we obtained data from open source geographic information datasets https www openstreetmap org generation of these elements can rely on accurate location information 2 insufficient survey data are available such as bare soil fallen woods gravels woody debris and forest ground floor that match the study area conditions these elements may not be established from available data sources due to the lack of survey data these object models are from quixel bridge https quixel com bridge an open source library of materials and assets inevitably due to the independence of the various data elements will overlap when they are stacked and thus we used constraints in houdini engine to remove trees from these non tree areas in addition we also used the unreal engine s sky and lighting simulations for the realism 2 4 browse system we used unreal engine s blueprint to add interaction to the platform we added a navigation system to the scene using the blueprint using the w a s d q and e keys on the keyboard to move the position of the camera and use the mouse to move up down left and right to adjust the orientation of the viewport fig 3 the whole scene is a virtual three dimensional space that exists in the computation and the navigation system shows the picture as if it were taken by a camera flying in the air in the real world when we navigate to a high place the scene in the screen is a large scale similarly when the camera flies to a lower place the scene in the screen is a small scale at the same time we created a time toggle assigning the left and right keys on the keyboard to toggle forward one year and backward one year we actually replaced the concept of time switching in disguise and utilized level switching similar to that in games we rendered the simulation results for each time point as a separate level with a scene nominally it is switching time but it is actually switching levels with scenes of different time points such an interactive system provides the convenience of navigating the visualization scene at will at a range of spatial and temporal scales unreal engine provides a set of tools for packaging entire scenes into runnable applications the visualized scene is eventually packaged into an application that can be interacted with and navigated at runtime more trees in the scene require more rendering which requires a higher computer configuration during the process of building the scene we can preview how the whole scene will run if the preview process works perfectly then both the packing process and the packed application are working properly on that computer our application was tested on a workstation 3 00 ghz intel xeon gold 6154 processor 192 gb ram and nvidia quadro p6000 graphic cards in addition to manually controlled interactive browsing we also provide another form of viewing for visual scenes users can render and export from a high performance computer and watch the exported video on a low performance computer by adding a custom video capture path and using unreal engine s virtual camera to capture the visual scene along the specified path fig 4 the resulting video file is saved in local storage this mode of browsing is more focused and easier for the creator to share than the interactive viewing mode 3 case study 3 1 overview our study area is located within the changbai mountain range in northeastern china it spans a wide elevational range from 530 to 2691 m typically there are four vertical vegetation zones on the northern slopes in the changbai mountain a mixed korean pine hardwood forest zone 530 m 1100 m an evergreen coniferous forest zone 1100 m 1700 m a subalpine forest zone 1700 m 2100 m and a tundra zone above 2100 m the dominant tree species include korean pine pinus koraiensis siebold and zucc basswood tilia amurensis rupr asian white birch betula platyphylla suk aspen poplus davidiana dode ash fraxinus mandshurica rupr mongolian oak quercus mongolica fisch ledeb maple acer mono maxim elm ulmus davidiana planch var japonica rehd nakai jezo spruce picea jezoensis siebold and zucc manchurian fir abies nephrolepis trautv maxim mountain birch betula ermanii cham and olga bay larch larix olgensis a henry the volcano eruption of changbai mountain occurred in a d 946 also known as the millennium eruption is one of the largest explosive volcanic eruptions ever recorded on earth in the last 2000 years gill et al 1992 liu et al 1998 the huge amount of ash and pumice ejected completely covered the volcanic cone and destroyed a large area of primary forest with about 50 km radius gill et al 1992 the large amount of volcanic ash covered the soil surface and prevented normal vegetation growth about 300 years ago the volcanic ash became less with water erosion and forest succession restarted driven by residual and matrix forests at lower elevations wu et al 2020 the areas destroyed by the volcanic eruption show different succession processes on different mountain aspects the northern mountain aspect has the most advanced succession and forests are at the late successional stages while the other mountain aspects are in the early succession stages jin et al 2013 the eastern side was most severely affected and forests are still mostly composed of larch jian et al 2016 the eruption also had different effects on each vegetation zone with alpine forest and tundra zone most severe it not only destroyed ceanothus pine liu et al 1993 but also the alpine forest line has not yet recovered to its original height shi and li 2000 du et al 2018 we used the landis pro forest landscape model and reconstructed the post volcano eruption forest landscape from 1710 to 2010 at 10 year time step and 100 m resolution moreover the simulated results were rigorously validated against field data and empirical knowledge wang et al 2014 specifically the reconstructed landscape in 2010 were compared to contemporary forest inventory data and classified remote sensing data the intermediate trajectories of forest dynamics at species and stand levels were validated using traditional stand development theories wu et al 2020 the validation assured the validity of the reconstructed landscape detailed processes and parameters can be found in wu et al 2020 3 2 visualization of output data we used the reconstructed data from this simulation as the source data in flv to demonstrate the functionalities in visualizing the forest landscape model outputs we first produced tree models of the major tree species in changbai mountain for different development stages fig 5 we generated a total of 36 tree models for three age groups of twelve major tree species each model also added different variation shapes to increase the diversity of tree forms in the landscape models of trees of different ages and shapes were imported into unreal engine as soon as they were created we extracted tree density distribution maps fig 6 and the total number of trees table 1 from landis output using the extension number and abundance in this process we summed the number raster plots of the same tree species within the same age group according to the age grouping shown in table 2 to produce maps of the number of trees within each age group the density distribution map and the table of total number of tree species were then output based on these summed tree number maps the age group division requires a combination of forest survey data and human experience the age groups used in this case are based on the average division of the life span of each tree species and then adjusted according to experience the terrain tree road and water datasets were processed in houdini engine to form automated files when these files are dragged into the unreal engine they automatically generate the virtual scene trees in the scene are automatically matched to the corresponding tree models in unreal engine at each tree point while the terrain roads and water bodies are matched to their corresponding entities in unreal engine the water bodies and roads fig 7 a c in the scene were inputted from the actual gis databases https www openstreetmap org also we randomly added fallen wood debris and understories to increase the authenticity of the forest ecosystems fig 7d 3 3 landscape exhibit flv is shown roaming through the visualized forest landscape of changbai mountain allowing us to navigate anywhere in the scene fig 8 this feature expands more usage scenarios in forest management we can zoom in from a landscape scale to a stand scale in smooth and continuous way to observe and navigate the spatial distribution the viewpoint at high altitude fig 8a with a greater viewshed and less detail reveals the distribution of different forest types broadleaf coniferous and mixed coniferous at the landscape level the viewpoint at the medium altitude fig 8b and c with a smaller viewshed and richer detail reveals the locations of individual trees within at the stand scale the viewpoint at the low altitude fig 8d shows a canopy and even organisms e g leaves and trucks at the individual tree scale our time switching program provides user with the ability to view the forest scenes in each simulated year at will over any simulation time step in our case study providing a convenient format to navigate user can fix the location in the navigation system and view the scenes of forest landscape recovery through time figs 9 and 10 for example user can visualize forest succession from broadleaf dominance to conifer dominance fig 9 at a landscape scale as well as tree growth from young to mature trees at a fine site scale fig 10 varying spatial and temporal scale of visualization were achieved by changing the time node and observation distance in the platform recording video along specific paths is another way to present our visualizations making it easier and faster to highlight the paths that researchers would prefer to show we preset the paths and shot video using a virtual camera and put it in the supplementary material the video is edited but the original footage is derived from the virtual camera in the video we show elements of the visualization scene and demonstrate interactive navigation and switching between scenes at different times likewise depending on the focus of the presentation it is also possible to record and produce other aspects that the user intends to display 4 conclusion we have presented a novel approach for forest landscape visualization our visualization platform made key advances in the following areas multi scale and seamless transition between scales our flv platform can perform 3 d visualization from individual trees to forest stand and to landscape or vice versa the transition between scales is seamless at the temporal scale we can switch from the scene at one time point to the scene at the previous or next time point or even jump to the scene at any time point at will we can pick the scenes for different time lengths to perform transition between multiple temporal scales this function enhances the monitoring and tracing ability since information provided to flv is spatially explicit such as tree location and landscape configuration rich information for 3d visualization the flv platform can pack rich information including tree species age height and distribution as well as ancillary information such terrain soil road water and environment to enhance reality and visualization experience flexibility and reusability the flv platform employs manual modeling for individual trees and automatic modeling for the forest landscape the combination of the two features greatly improves the efficiency of the visualization users can easily compile images of different tree species age and stage e g spring fall winter for their study area and the entire workflow is replicable and can be used for visualization of other forest systems realistic scenes and easy navigation the application of the game engine plays a key role in the 3 d visualization of the forest landscape it not only greatly increases the realism and aesthetics of the scene but also provides numerous ways to navigate in space and time at the same time in order to narrow the gap between the virtual landscape and the real landscape we added age group distinctions and morphological variations for each tree species and added non tree elements to the landscape the entire workflow developed based on the game engine can serve as a useful tool to assist scientists and managers in understanding the spatial structure and dynamics of the forest this platform serves as a bridge between forest landscape models and 3d forest landscapes it is dedicated to transforming flat forest landscapes into three dimensional ones it is a new solution of 3 d visualization which has the potential to expand to other elements in the environment the application of game engines to geographic research represents a convergence to real environment presentation and may represent a direction for future development of geographic data presentation the platform is currently limited mainly by the hardware configuration of the computer and the software optimization rendering a large number of trees requires a large processor footprint as well as memory capacity and a gpu for the next few years rendering 3 d scenes will require less hardware footprint with software optimization major geographic vendors e g arcgis cesium are also currently developing geographic analysis plug ins that can run in game engines more geographic information technologies will be integrated with game engines and will be able to provide more ways to 3 d visualization of forest landscapes in the future declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was funded by the joint fund of national natural science foundation of china u19a2023 the national key r d program of china 2017yfa0604403 3 the national natural science foundation of china no 42101107 and the natural science foundation of jilin province china ydzj202201zyts487 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105524 
25518,the recording and simulation data of forest landscapes are massive high dimensional and abstract requiring intuitive representation 3 d visualization is an efficient tool to comprehend possible landscape changes generated by real world or forest landscape models based on current advantages of game engines realism and convenience we developed a platform for 3 d visualization of forest landscapes flv flv streamlines multiple software and programming languages to break barrier between geographic data and game engine and transforms outputs of forest landscape models into visualization parameters compared with previous 3 d visualizations flv has better realism efficiency and navigation we used simulation data of post volcanic eruption forest landscape recovery in changbai mountain as a case study and demonstrated functionalities of flv flv can visualize seamlessly from individual tree to forest stand and landscape scales and from individual year to decades and centuries temporal scales it offers potential solutions for the digital representation of complex environmental change keywords forest landscape visualization flv forest landscape model unreal engine 3d visualization tree model multi spatiotemporal scales software availability name forest landscape visualization contact yuhy507 nenu edu cn year first availability 2021 hardware required ibm compatible pc software required unreal engine houdini engine speedtree photoshop system required windows program languages python r availability the flv platform is assembled as free software utility and is available from github https github com flvyhy flv 1 introduction forest landscape visualization plays an important role in forest resource management land use planning and forest landscape change predictions xi et al 2016 traditionally forest landscapes have been described with 2 d maps sheppard 1989 lovett et al 2015 that are widely used birt et al 2009 xi et al 2020 however 3 d landscape visualization presents special abilities that are more effective in describing the consequences of forest landscape change sheppard 2005 expressing richer information and comparing alternative scenarios xi et al 2016 the forest landscape change is driven by complex spatial interactions of ecological processes such as seed dispersal succession and different disturbances mladenoff et al 1996 these processes take place at various scales in space and time mladenoff 2004 3 d visualization can reveal landscape change from multi scale perspectives through zooming and navigating the scene freely appleton et al 2002 it can assist scientists and managers to apprehend the complex and abstract environmental change processes involved li et al 2021 qin et al 2021 and landscape variations and management effects on spatial and temporal scales sheppard 2005 in addition 3 d visualization provide a way to present complex information to people with no experience in map interpretation bishop et al 1994 simulation results from forest landscape models are ideal for 3 d visualization forest landscape models predict changes in spatial characteristics e g distribution shape abundance at tree level he 2008 simulate forest changes driven by species and stand dynamics and landscape processes including dispersal disturbance and management at landscape scales xi et al 2009 and simulate the spatial and temporal characteristics of periodically occurring spatial processes in a spatially interactive manner he 2008 the whole simulation outputs are typically raster based mladenoff 2004 which are ready for most 3d visualization software 3 d forest landscape visualization has evolved over the past decades professional landscape visualization software e g vns visual nature studio emerged song et al 2011 mostly from 2000 to 2010 the software offered solutions to forest landscape visualization and met the expectations of the time however by current standards they did not keep up with new hardware development and were slow in rendering with reduced realism in scenes besides 3 d visualization software for forest landscapes was expensive and difficult to use carbonell carrera et al 2021 meanwhile game industry has emerged with sophisticated visualization software herwig and paar 2002 under the impetus of game commercialization which has potential to extend to 3 d forest landscape visualization lewis and jacobson 2002 game engines are the most widely used and mature visualization software in the game industry they make visualization technology affordable to use carbonell carrera et al 2021 and have great potential for 3 d landscape construction including highly realistic scenes real time rendering real time roaming oakden and kavakli 2022 interaction simulation of terrain and physical environment shiratuddin and thabet 2011 they are also used in the visualization of environmental elements such as water bryceson et al 2022 and wind rafiee et al 2017 moreover components of the visual appearance of a forest landscape also can be simulate using game engines including terrain forests grasslands sky rivers and light etc gang and huang 2011 finally corresponding to the simulation range and time scale of the forest landscape models the 3 d visualization results can show more realistic scenarios at the stand scale and more information at the landscape scale 1 1 content and objectives the aim of this study is 1 to present a platform for 3 d visualizing landis pro forest landscape model output at flexible scales time and space and perspectives spatial location and orientation the platform includes building a library of tree models and a data driven workflow with game engine as core and other ancillary utilities we 2 test the platform using a real set of historical landscapes reconstructed from post volcanic eruption in changbai mountain northeast 2 workflow the platform for our 3 d visualization of forest landscapes was developed by integrating a range of software and programming languages in the design process we specially focused on realistic visualization scenarios forest visualization results consistent with the source data and a user friendly interactive interface we used four pieces of modeling software and a statistical analysis utility in the platform photoshop is image editing software that we used to extract textures from photos of leaves and tree trunks speedtree https store speedtree com virtual foliage generator is commercial software used for modeling trees adjusting parameters to build 3 d models of different tree species based on tree morphology and field data houdini engine https www sidefx com products houdini engine procedural generation software tools available in both open source and commercial versions is a software that reads the raw data and sets them into the actual scene unreal engine https www unrealengine com game engine for building 3 d scenes open source software is the game engine originally for to the 3d game scene creation and here customized to perform the visualization due to its superiority in building 3d scenes the platform of 3 d forest landscape visualization flv contains three components fig 1 photoshop and speedtree that creates all 3 d models of the tree species fig 1c the r code files that transforms the tree species density distribution from forest landscape model output and the houdini engine that generates the tree point map fig 1a and the unreal engine that generates the scene with the operational interface for spatial navigation and switching between scenes with multiple simulated time nodes fig 1b 2 1 tree model high quality tree models are the basis for the realism of forest landscape visualization earlier visualizations typically used 2 d pictures of real trees to save computer memory compromising the 3 d effects concurrent techniques can display 3 d tree models at different levels of detail lod and view distance in our study we use speedtree which is a professional tree model building software this software can export 3 d tree models using pictures of leaves and trunks of each tree species and adjust the parameters according to tree morphology the images of the leaves and trunks we used are from the china plant image library http ppbc iplant cn the main parameters in the software included tree height diameter length and density of branches and size of leaves the tree models of each species were divided into small medium and large size group as surrogates of trees from young to old each size group had multiple forms of tree models to approximate variable tree forms shapes in the study area 2 2 data transmission we chose the r language and houdini engine software for the scope unification format conversion and batch transfer of landscape data the basic elements of the 3 d forest landscape visualization are terrain and trees the topographic data were obtained from the palsar sensors of the alos satellite that has a resolution of 12 5 m 12 5 m the forest elements were based on the output data of the landis pro forest landscape model wu et al 2020 to process the base data we used the raster package in r to crop and stitch the dem data and then standardized the extent of both the dem data and the model output data to the extent of the visualization area a more complex step is needed to process the forest data and therefore we created a new extension to landis pro called distribution and abundance written in r to generate tree density distribution maps and calculate the total number of trees per raster cell the output of the forest landscape model landis pro contains raster images by tree species and age group in a simulation year we needed to summarize the information from all raster images for a given simulation year to generate density maps from these raster images the density maps were then linear stretched into values from 0 to 1 fig 2 a with 0 the white color indicating no trees and 1 the darkest red color indicating highest tree density we calculated the total number of trees to be scattered in the area where the whole raster map is located the number of randomly scattered points in each cell of the raster map is determined by the magnitude of the density value of the density distribution on each cell the location of each tree is not determined but the number and location of all trees within each single cell is determined the points of trees were scattered in houdini engine according to the total number of trees and the density distribution fig 2b then the points were matched to the corresponding tree models in unreal engine driven by the houdini plugin fig 2c at the same time the trees adapted to the undulations of the terrain and are generated close to the ground fig 2d it is important to emphasize that the houdini engine is only used to establish the logical relationships between the elements the final scene construction was done in unreal engine 2 3 visualization scene unreal engine is used to build 3d scenes that creates highly realistic forest scenes and enhanced visual experience for forest landscape analysis the basic elements when building a forest landscape in unreal engine include mainly terrain and forest information at pixel level we placed the created tree model in unreal engine and then drag and drop the automation file created in houdini engine into unreal engine to automatically generate the terrain and forest to achieve the integrity and aesthetics of the forest landscape we also randomly added non tree objects to the surface our recommendation is to do as much as possible to have accessible data available when building each element we considered two parts of non tree elements 1 accessible vector data such as water bodies and roads we obtained data from open source geographic information datasets https www openstreetmap org generation of these elements can rely on accurate location information 2 insufficient survey data are available such as bare soil fallen woods gravels woody debris and forest ground floor that match the study area conditions these elements may not be established from available data sources due to the lack of survey data these object models are from quixel bridge https quixel com bridge an open source library of materials and assets inevitably due to the independence of the various data elements will overlap when they are stacked and thus we used constraints in houdini engine to remove trees from these non tree areas in addition we also used the unreal engine s sky and lighting simulations for the realism 2 4 browse system we used unreal engine s blueprint to add interaction to the platform we added a navigation system to the scene using the blueprint using the w a s d q and e keys on the keyboard to move the position of the camera and use the mouse to move up down left and right to adjust the orientation of the viewport fig 3 the whole scene is a virtual three dimensional space that exists in the computation and the navigation system shows the picture as if it were taken by a camera flying in the air in the real world when we navigate to a high place the scene in the screen is a large scale similarly when the camera flies to a lower place the scene in the screen is a small scale at the same time we created a time toggle assigning the left and right keys on the keyboard to toggle forward one year and backward one year we actually replaced the concept of time switching in disguise and utilized level switching similar to that in games we rendered the simulation results for each time point as a separate level with a scene nominally it is switching time but it is actually switching levels with scenes of different time points such an interactive system provides the convenience of navigating the visualization scene at will at a range of spatial and temporal scales unreal engine provides a set of tools for packaging entire scenes into runnable applications the visualized scene is eventually packaged into an application that can be interacted with and navigated at runtime more trees in the scene require more rendering which requires a higher computer configuration during the process of building the scene we can preview how the whole scene will run if the preview process works perfectly then both the packing process and the packed application are working properly on that computer our application was tested on a workstation 3 00 ghz intel xeon gold 6154 processor 192 gb ram and nvidia quadro p6000 graphic cards in addition to manually controlled interactive browsing we also provide another form of viewing for visual scenes users can render and export from a high performance computer and watch the exported video on a low performance computer by adding a custom video capture path and using unreal engine s virtual camera to capture the visual scene along the specified path fig 4 the resulting video file is saved in local storage this mode of browsing is more focused and easier for the creator to share than the interactive viewing mode 3 case study 3 1 overview our study area is located within the changbai mountain range in northeastern china it spans a wide elevational range from 530 to 2691 m typically there are four vertical vegetation zones on the northern slopes in the changbai mountain a mixed korean pine hardwood forest zone 530 m 1100 m an evergreen coniferous forest zone 1100 m 1700 m a subalpine forest zone 1700 m 2100 m and a tundra zone above 2100 m the dominant tree species include korean pine pinus koraiensis siebold and zucc basswood tilia amurensis rupr asian white birch betula platyphylla suk aspen poplus davidiana dode ash fraxinus mandshurica rupr mongolian oak quercus mongolica fisch ledeb maple acer mono maxim elm ulmus davidiana planch var japonica rehd nakai jezo spruce picea jezoensis siebold and zucc manchurian fir abies nephrolepis trautv maxim mountain birch betula ermanii cham and olga bay larch larix olgensis a henry the volcano eruption of changbai mountain occurred in a d 946 also known as the millennium eruption is one of the largest explosive volcanic eruptions ever recorded on earth in the last 2000 years gill et al 1992 liu et al 1998 the huge amount of ash and pumice ejected completely covered the volcanic cone and destroyed a large area of primary forest with about 50 km radius gill et al 1992 the large amount of volcanic ash covered the soil surface and prevented normal vegetation growth about 300 years ago the volcanic ash became less with water erosion and forest succession restarted driven by residual and matrix forests at lower elevations wu et al 2020 the areas destroyed by the volcanic eruption show different succession processes on different mountain aspects the northern mountain aspect has the most advanced succession and forests are at the late successional stages while the other mountain aspects are in the early succession stages jin et al 2013 the eastern side was most severely affected and forests are still mostly composed of larch jian et al 2016 the eruption also had different effects on each vegetation zone with alpine forest and tundra zone most severe it not only destroyed ceanothus pine liu et al 1993 but also the alpine forest line has not yet recovered to its original height shi and li 2000 du et al 2018 we used the landis pro forest landscape model and reconstructed the post volcano eruption forest landscape from 1710 to 2010 at 10 year time step and 100 m resolution moreover the simulated results were rigorously validated against field data and empirical knowledge wang et al 2014 specifically the reconstructed landscape in 2010 were compared to contemporary forest inventory data and classified remote sensing data the intermediate trajectories of forest dynamics at species and stand levels were validated using traditional stand development theories wu et al 2020 the validation assured the validity of the reconstructed landscape detailed processes and parameters can be found in wu et al 2020 3 2 visualization of output data we used the reconstructed data from this simulation as the source data in flv to demonstrate the functionalities in visualizing the forest landscape model outputs we first produced tree models of the major tree species in changbai mountain for different development stages fig 5 we generated a total of 36 tree models for three age groups of twelve major tree species each model also added different variation shapes to increase the diversity of tree forms in the landscape models of trees of different ages and shapes were imported into unreal engine as soon as they were created we extracted tree density distribution maps fig 6 and the total number of trees table 1 from landis output using the extension number and abundance in this process we summed the number raster plots of the same tree species within the same age group according to the age grouping shown in table 2 to produce maps of the number of trees within each age group the density distribution map and the table of total number of tree species were then output based on these summed tree number maps the age group division requires a combination of forest survey data and human experience the age groups used in this case are based on the average division of the life span of each tree species and then adjusted according to experience the terrain tree road and water datasets were processed in houdini engine to form automated files when these files are dragged into the unreal engine they automatically generate the virtual scene trees in the scene are automatically matched to the corresponding tree models in unreal engine at each tree point while the terrain roads and water bodies are matched to their corresponding entities in unreal engine the water bodies and roads fig 7 a c in the scene were inputted from the actual gis databases https www openstreetmap org also we randomly added fallen wood debris and understories to increase the authenticity of the forest ecosystems fig 7d 3 3 landscape exhibit flv is shown roaming through the visualized forest landscape of changbai mountain allowing us to navigate anywhere in the scene fig 8 this feature expands more usage scenarios in forest management we can zoom in from a landscape scale to a stand scale in smooth and continuous way to observe and navigate the spatial distribution the viewpoint at high altitude fig 8a with a greater viewshed and less detail reveals the distribution of different forest types broadleaf coniferous and mixed coniferous at the landscape level the viewpoint at the medium altitude fig 8b and c with a smaller viewshed and richer detail reveals the locations of individual trees within at the stand scale the viewpoint at the low altitude fig 8d shows a canopy and even organisms e g leaves and trucks at the individual tree scale our time switching program provides user with the ability to view the forest scenes in each simulated year at will over any simulation time step in our case study providing a convenient format to navigate user can fix the location in the navigation system and view the scenes of forest landscape recovery through time figs 9 and 10 for example user can visualize forest succession from broadleaf dominance to conifer dominance fig 9 at a landscape scale as well as tree growth from young to mature trees at a fine site scale fig 10 varying spatial and temporal scale of visualization were achieved by changing the time node and observation distance in the platform recording video along specific paths is another way to present our visualizations making it easier and faster to highlight the paths that researchers would prefer to show we preset the paths and shot video using a virtual camera and put it in the supplementary material the video is edited but the original footage is derived from the virtual camera in the video we show elements of the visualization scene and demonstrate interactive navigation and switching between scenes at different times likewise depending on the focus of the presentation it is also possible to record and produce other aspects that the user intends to display 4 conclusion we have presented a novel approach for forest landscape visualization our visualization platform made key advances in the following areas multi scale and seamless transition between scales our flv platform can perform 3 d visualization from individual trees to forest stand and to landscape or vice versa the transition between scales is seamless at the temporal scale we can switch from the scene at one time point to the scene at the previous or next time point or even jump to the scene at any time point at will we can pick the scenes for different time lengths to perform transition between multiple temporal scales this function enhances the monitoring and tracing ability since information provided to flv is spatially explicit such as tree location and landscape configuration rich information for 3d visualization the flv platform can pack rich information including tree species age height and distribution as well as ancillary information such terrain soil road water and environment to enhance reality and visualization experience flexibility and reusability the flv platform employs manual modeling for individual trees and automatic modeling for the forest landscape the combination of the two features greatly improves the efficiency of the visualization users can easily compile images of different tree species age and stage e g spring fall winter for their study area and the entire workflow is replicable and can be used for visualization of other forest systems realistic scenes and easy navigation the application of the game engine plays a key role in the 3 d visualization of the forest landscape it not only greatly increases the realism and aesthetics of the scene but also provides numerous ways to navigate in space and time at the same time in order to narrow the gap between the virtual landscape and the real landscape we added age group distinctions and morphological variations for each tree species and added non tree elements to the landscape the entire workflow developed based on the game engine can serve as a useful tool to assist scientists and managers in understanding the spatial structure and dynamics of the forest this platform serves as a bridge between forest landscape models and 3d forest landscapes it is dedicated to transforming flat forest landscapes into three dimensional ones it is a new solution of 3 d visualization which has the potential to expand to other elements in the environment the application of game engines to geographic research represents a convergence to real environment presentation and may represent a direction for future development of geographic data presentation the platform is currently limited mainly by the hardware configuration of the computer and the software optimization rendering a large number of trees requires a large processor footprint as well as memory capacity and a gpu for the next few years rendering 3 d scenes will require less hardware footprint with software optimization major geographic vendors e g arcgis cesium are also currently developing geographic analysis plug ins that can run in game engines more geographic information technologies will be integrated with game engines and will be able to provide more ways to 3 d visualization of forest landscapes in the future declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was funded by the joint fund of national natural science foundation of china u19a2023 the national key r d program of china 2017yfa0604403 3 the national natural science foundation of china no 42101107 and the natural science foundation of jilin province china ydzj202201zyts487 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105524 
25519,complex environmental model outputs used to inform decisions often have systematic errors and are of inappropriate resolution requiring downscaling and bias correction for local applications here we provide a new interpretation of dasymetric modelling dm as a spatial bias correction framework useful in environmental modelling dm is based on areal interpolation where estimates of some variable at target zones are obtained from overlapping source zones using ancillary information we explore dm by downscaling runoff output from a distributed hydrological model using two meta models and describe the properties of the methodology in detail consistent with properties of linear scaling bias correction results show that the methodology 1 reduces errors compared to the source data and meta models 2 improve the spatial structure of the estimates and 3 improve the performance of the downscaled estimates particularly where meta models perform poorly the framework is simple and useful in ensuring spatial coherence of downscaled products keywords bias correction dasymetric modelling downscaling areal interpolation environmental modelling meta modelling data availability the data and code to reproduce the results are shared in an open zenodo repository the link is provided in the data and software availability statement within the manuscript file software and data availability the necessary r code and data to produce the synthetic example table 1 and figs 4 7 are stored in a zenodo repository at https zenodo org record 5857189 last access 16 jan 2022 1 introduction modern environmental decision making depends on the results of various outputs from environmental modelling jakeman et al 2008 schmolke et al 2010 the need for impact modelling to support decision making is particularly important in the face of unprecedented changes the world is currently experiencing in climate land use and other environmental conditions ipcc 2021 the application of complex environmental models is however not trivial and the easily available outputs may be biased or of inappropriate scale or configuration leading to a need for further processing particularly for local applications mismatch between scales of analysis and the source data has been commonly addressed using downscaling upscaling or both downscaling methods attempt to estimate the state of a variable at a finer resolution by relating it to the state of the variable at a coarser resolution von storch and zorita 2019 they are useful when we wish to estimate the internal variability within some temporal or spatial unit in dynamic downscaling a fine resolution model is applied within areas defined by product under downscaling and where the coarse data provides boundary conditions for the fine scale model this is common when e g global climate models are downscaled using regional climate models in empirical downscaling the link between the finer and coarser scales is determined by statistical methods there is a large body of literature on different methods of empirical downscaling for instance using interpolation methods e g mennis 2009 kallio et al 2019 wang et al 2015 lima et al 2021 or different machine learning methods e g latombe et al 2018 stevens et al 2015 bardossy et al 2005 ahmed et al 2013 the outputs from the downscaling methods may exhibit systematic errors i e they may be biased tabari et al 2021 when compared to observations these systematic errors may arise from imperfect model conceptualization and data aggregation teutschbein and seibert 2012 or from errors in input data e g sperna weiland et al 2015 and often need to be corrected ahmed et al 2013 j c chen et al 2021 ibarra et al 2021 teutschbein and seibert 2010 a large number of different approaches to bias correction are available see e g teutschbein and seibert 2012 and are of particular importance for climate change impact modelling e g hempel et al 2013 lange 2019 warszawski et al 2014 which requires unbiased meteorological forcing data tramberend et al 2021 these approaches commonly deal with the temporal dimension with less attention given to correction of spatially autocorrelated errors in distributed environmental models nahar et al 2018 nonetheless some methods to correct spatial bias exist hnilica et al 2017 corrected precipitation with a combination of principal component analysis and quantile mapping nahar et al 2018 used a similar methodology but apply independent component analysis instead of principal component analysis kim et al 2021 developed a bayesian kriging based spatial disaggregation quantile delta mapping method where precipitation distribution parameters are interpolated on to a fine grid with kriging and subsequently used to bias correct downscaled timeseries lange 2019 conducted multivariate bias correction using modified version of cannon 2018 which ensures that downscaled estimates are consistent with the original climate simulations furthermore data assimilation methods are used to optimally combine observations with model outputs in order to reduce their systematic errors reichle 2008 this study presents a novel interpretation of an advanced areal interpolation method dasymetric modelling dm as a method for spatial bias correction of downscaled data areal interpolation consists of all those methods which use values from a set source zones to estimate the same variables at a set of intersecting target zones comber and zeng 2019 goodchild and lam 1980 the target zones are commonly of a higher resolution than source zones in areal interpolation the value of the variable is distributed among target zones within a source zone thus preserving the aggregate value in the area represented in the source zone dm is an extension to areal interpolation where the distribution of the source zone value is guided by some model dm and bias correction share a similar objective of adjusting values to satisfy some constraint e g the distributed values at target zones must match that of their source zone or scaling a daily timeseries to match a monthly reference value we further note that when applying dm to downscale available coarse resolution model outputs the application can be described as meta modelling meta modelling is the process of building a model to emulate another model also known as surrogate modelling e g razavi et al 2012 in meta modelling the output of an original model is emulated by some statistical or process based model from a set of input data with the common aim to reduce computational costs downscaling can be considered meta modelling when the relationship between explanatory variables for instance relating to topography and e g a climate model output is established at a coarser scale and applied at a finer granularity we conduct an experiment in the upper bhima basin ubb central india downscaling local runoff generation i e without accumulation to streamflow several other methods have been developed to handle bias in runoff and streamflow skøien et al 2006 extended kriging interpolation to account for topological relationships within a stream network necessary for the interpolation of streamflow records and further developed a spatio temporal topological kriging method in skøien and blöschl 2007 paiva et al 2015 provided another spatio temporal kriging method to estimate river discharges along a river network loonat et al 2020 used data assimilation of streamflow records to a distributed rainfall runoff model by kriging and global bias correction naz et al 2019 downscaled runoff data using a data assimilation scheme using soil moisture as an ancillary variable bennett et al 2021 developed a mechanism for streamflow which specifically bias correct runoff and apply river routing to make it possible to use streamflow as the reference to correct against in our case study we examine the bias correcting property of dasymetric modelling in detail we train two meta models to emulate the local runoff output of the community water model cwatm burek et al 2020 a distributed physically based hydrological model based on a global model run at 0 5 approx 50 50 km resolution the meta models are then applied to a 5 arc minute approx 10 10 km grid within the same area the 0 5 grid serves as the source zones giving a spatial reference of the magnitude of the values within the basin and the 5 arc minute grid serves as the target zones the meta model outputs at the target zones are then bias corrected using the source zone values as a reference the comparison to cwatm at higher resolution is appropriate because the meta models are trained on cwatm at lower resolution emulating the behavior of the model the experiment we conduct here extends previous work in kallio et al 2021 2019 where global runoff products were downscaled with dasymetric mapping i e dm with a single ancillary variable instead of a model in the earlier studies the bias correcting property is confounded due to a use of irregular polygons which may be influenced by several intersecting source zones b the uncertainties related to river routing modelling of accumulation of runoff in the stream network c the difficulty of assigning a low resolution streamflow value to the more complex higher resolution river network see e g bennett et al 2021 for a discussion of this problem and d the use of a monthly timestep which is large considering the size of the basins the experiment here focuses on the bias correcting property by using a nested grid controls the confounding by areal interpolation comparing local runoff production controlling for uncertainties in routing models and aggregation effects and is applied at a daily rather than a monthly timestep in this way we can isolate the influence of bias correction from the other factors influencing the performance the remainder of the paper is structured as follows section 2 provides the background describing similarities and equivalence of dm to meta modelling and bias correction respectively section 3 describes our case study giving information on cwatm the upper bhima basin meta models and how we assess performance results and detailed discussion of the case study outputs are given in section 4 the bias correction framework and the case study are discussed in section 5 finally we conclude the paper in section 6 2 dasymetric modelling meta modelling and bias correction this section first introduces dm in section 2 1 and proceeds to describe the similarities of the common dm workflow and meta modelling in section 2 2 in the next section 2 3 we establish the equivalence of dm and linear scaling bias correction in section 2 4 we bring them all together in a single framework 2 1 dasymetric modelling areal interpolation a form of spatial interpolation dealing with interpolating values from a source zone to another set of overlapping arbitrary target zones is a standard practice in many fields comber and zeng 2019 in its simplest form area weighted interpolation aw a value from a source zone is assigned to target zones based on their overlapping areas as shown in eq 1 1 y t ˆ s s y s a t s a s where y t ˆ is the estimated value at a target zone t y s denotes the value associated with a source zone s and a is the area of the zone the target zone t is therefore assigned values from all source zones it intersects denoted as the set s according to the proportion of the area of the target zone within each source zone a t s to the area of each overlapping source zone the formulation allows estimation of values in arbitrary target zones which do not neatly conform to the also arbitrary shape of the source zones areal interpolation has a pycnophylactic mass preserving property because it is in essence distributing the source zone values onto the overlapping target zones this procedure leads to the preservation of the source zone values in the target zones mennis 2009 tobler 1979 dasymetric mapping is extension of aw where the interpolation is refined by the use of ancillary information comber and zeng 2019 mennis 2009 dm has originally been developed for improved cartographic presentation of population density mennis 2009 wright 1936 a large body of literature has been written on different applications of dm and its extensions particularly in the context of state of the art population mapping e g stevens et al 2015 tatem 2017 demographic attributes e g tatem et al 2014 and to lesser extent in other fields such as environmental modelling kallio et al 2019 2021 chen et al 2019 in dasymetric mapping the task of the ancillary data is to provide information on the distribution of the interpolated value within a source zone with the ancillary information x t eq 1 becomes 2 y t ˆ s s y s a t s x t t t a t s x t which uses information about target zones t belonging to a set t consisting of all target zones intersecting with source zone s to scale the ancillary data in dasymetric modelling we will use dm to refer to dasymetric modelling for the remainder of the paper the ancillary information x t is given by some model describing the target zone level spatial distribution of the variable given in source zones rather than a single variable as in dasymetric mapping nagle et al 2014 for a review of areal interpolation methods we refer to the review by comber and zeng 2019 2 2 dasymetric modelling and meta modelling in statistical downscaling the relationship between ancillary variable s and the downscaled quantity is commonly regressed at the source zone level von storch and zorita 2019 this is a common workflow in dm as well mennis 2009 nagle et al 2014 the model trained at source zones is subsequently applied at the target zone level to obtain the downscaled estimates in dm the estimates are used as the ancillary information x t the outputs of the trained model are used to derive weights according to which the source zone value is distributed to contributing target zones if the values associated with the source zones are an output of a model e g a climate an impact or some empirical statistical model the workflow can be described as a meta modelling approach a meta model attempts to emulate the output of the original model with the aid of explanatory variables in this case the meta model attempts to estimate what the full model would have output at the target zone level if it was run at that resolution the motivation of using different types of meta models is often to reduce the complexity of a full model to reduce the computational cost and sometimes to reduce input data requirements asher et al 2015 razavi et al 2012 the common workflow in dm can therefore be seen as meta model assisted distribution of the model outputs evaluated at source zones on to the overlapping target zones areal interpolation dm allows this to be done with the objective of upscaling downscaling or representing the values in similar resolution but different zonal arrangement kallio et al 2021 2 3 dasymetric modelling and bias correction in a special case of dm where all target zones are located within a single source zone e g a nested grid a common setup in downscaling earth science data and each target zone is of the same area e g a raster grid with an equal area projection the area terms in eq 2 can be eliminated resulting in eq 3a 3a y t ˆ y s x t t t s x t this equation can be read as solution where the value associated with a source zone y s is distributed to the target zones according to the proportion of some meta model output x t from the sum of meta model results within the source zone if we swap the position of y s and x t the resulting eq 3b is equivalent but with the change in grouping of terms we can now read the equation as the scaling of the meta model outputs x t 3b y t ˆ x t y s t t s x t if the denominator is divided by the number n of target zones within a source zone we obtain the mean output μ x t of the meta model within the source zone using the source zone as our reference the equivalent mean across n target zones within the source zone is μ y t 1 n y s and thus we can simplify eq 3b to eq 5 with the intermediate step given in eq 4 4 y t ˆ x t 1 n y s 1 n t t s x t 5 y t ˆ x t μ y t μ x t where y s is the value associated with the source zone in which the target zone is located and μ x t is the mean value of all meta model outputs within the source zone this formulation is identical to the linear scaling bias correction method shown in eq 6 lenderink et al 2007 teutschbein and seibert 2012 6 y m o d e l y m o d e l μ o b s μ m o d e l where y m o d e l is the bias corrected model output although bias correction is commonly applied to timeseries data it may also be applied to a spatial domain where target zone values are corrected against the source zone data thus when stripped of the area terms under the conditions described above dm is mathematically equivalent to the common linear scaling bias correction method an example of linear scaling bias correction with one dimensional synthetic data is provided in fig 1 showing a cosine function and a hypothetical meta model output x t and its bias corrected counterpart y t ˆ along with goodness of fit statistics for illustration purposes the hypothetical meta model output for each point t was obtained here by adding random variation into the cosine series dividing the values into groups and subtracting a random amount from each group linear scaling is applied group wise for each source s using equation 5 rather than to the dataset as a whole akin to the dm approach having multiple source zones for which to perform bias correction separately from this example we note a few important properties of linear scaling 1 by design bias is eliminated group wise and subsequently for the entire data series 2 pearson correlation coefficient is preserved within group but changes for the entire series 3 the magnitude of standard deviation is changed the reason for preservation of pearson correlation coefficient ρ eq 7 within a group is because ρ depends on how the two data series vary around their respective means 7 ρ x y e x μ x y μ y σ x σ y since each data point in a group is subjected to the same linear transformation the relationship between the standard deviation σ and the mean μ remains unchanged in other words coefficient of variation c v σ μ remains unaltered even when the absolute magnitude of the standard deviation is changed as a consequence bias correction preserves the dynamics provided by the model within each group but because each group is subjected to different scaling operation the overall dynamics correlation is altered in this synthetic example pearson correlation coefficient improves from 0 74 to 0 96 fig 1b shows autocorrelation functions correlation of a variable with itself for the true original the meta model output x t and bias corrected meta model output y t ˆ autocorrelation is significantly improved with the method given the equivalence between linear scaling and dasymetric mapping on nested grids a similar behavior is expected in a spatial bias correction context we note that for the linear scaling bias correction to work as described here all meta model output values x t and the reference values associated with the source zones y s should be strictly finite positive values to prevent erratic behavior 2 4 dasymetric modelling as a combined downscaling bias correction framework the previous sections describe how meta modelling and bias correction relate to dm fig 2 shows workflows for each of these three methods separately and combined together into a single framework each of the three methods have a specific purpose in the workflow the meta model outputs x t evaluated at the target zones is responsible for representing the spatial dynamics of how the value is distributed within a source zone i e the meta model output should have a high correlation with the ground truth values within the source zones bias correction ensures that the spatial information associated with source zones are preserved in the target zones leading to spatially debiased estimates dm provides flexibility in the spatial configuration of source and target zones it describes how non conforming source and target zones are handled how should the meta model output in each target zone be divided during the intersection with source zones and how to combine the values back to the original target zones after bias correction this operation may differ for instance for intensive standardized variables e g population density or water use per capita and extensive variables counts e g the amount of population or volume of water the workflow is agnostic as to how x t is estimated whether given by another similar model a meta model or measured data similarly the method is agnostic as to where the source zone reference values y s are obtained in principle with the redistribution of conventional dm interpreted here as bias correction linear scaling may be replaced by more advanced bias correction methods this leads to the framework being modular consisting of three distinct parts 3 case study we explore the dasymetric modelling framework as interpreted in fig 2 and its properties by a case study where we downscale a coarse resolution 0 5 source zones cwatm simulation with a global extent onto a higher resolution 5 arc minute grid target zones we make use of meta models trained at the source zone level and applied at the target zones the results are compared to a run of cwatm at the target zones in order to evaluate the usefulness of the methodology and the ability to emulate a more spatially detailed model output the experiment is setup so that the source and target zones are nested which reflects the simplification described in section 2 3 this allows us to examine the efficiency of the spatial bias correction without the added complication of non conforming source and target zones in section 3 1 we give a brief introduction to cwatm and section 3 2 contains a description of the study area the upper bhima basin next section 3 3 introduces the two meta models used and section 3 4 describes how we assess the downscaling performance 3 1 community water model the community water model is an open source integrated hydrological and channel model that calculates water availability surface and groundwater environmental flow requirements and socio economic water demands and impacts from water infrastructures such as reservoirs groundwater pumping and irrigation burek et al 2020 the global applications of the model are based on a 0 5 or 5 arc minute resolution grid and are run at a daily temporal resolution the model can be used for regional applications using a daily temporal and 30 arc sec spatial resolution for topography and land cover it uses a sub grid approach and for soil and routing processes it uses sub daily timesteps the conceptual framework and structure of cwatm are similar to that of other hydrological models such as h08 hanasaki et al 2006 2008 2010 watergap alcamo et al 2003 flörke et al 2013 pcr globwb van beek et al 2011 wada et al 2014 and others the model accounts for future water demands based on socio economic change and the impacts of water availability in response to climate change irrigation water demand is estimated for paddy and non paddy crops separately by dynamically linking irrigation water demand with the surface and soil water balance for the upper bhima basin cwatm is forced by the meteorological variables including temperature wind speed relative humidity incoming longwave and shortwave radiation and surface air pressure from w5e5 v2 0 cucchi et al 2020 for precipitation we use data from pai et al 2014 please see burek et al 2020 for a more detailed description of cwatm hydrological processes 3 2 upper bhima basin the bhima basin in india shown in fig 3 is a highly managed and densely populated watershed with important rain fed and irrigated crop production rainfall occurs towards the west and concentrates through 5 months of monsoon with a total annual rainfall of around 35 km3 approx 760 mm per year the bhima basin is an upstream basin of the krishna basin originating in the mountains of the western ghats with a river length of 325 km the bhima basin supports a population of 18 7 million 2015 distributed over 45 800 km2 of largely agricultural land flowing mostly within the state of maharashtra the largest urban agglomeration within the bhima basin is pune with nearly 7 million residents followed by solapur with approximately 1 2 million residents in 2015 cwatm has been successfully applied at the bhima basin guillaumot et al 2022 3 3 meta models two meta models were developed to estimate cwatm modelled runoff output over upper bhima basin during 2001 2010 the first meta model is a standard ordinary least squares regression referred to as the linear model lm the second meta model is based on random forest regression rf breiman 2001 both models were trained at the source zone level 0 5 resolution with the same model equation based on derived variables from a digital elevation model dem daily precipitation p and daily temperature t measurements 8 x t s p t s p t s 1 p t s 2 s u m p t s 1 7 s u m p t s 1 30 s u m p t s 1 180 d s p t t s t t s 1 t t s 2 m e a n t t s 1 7 e l e v a t i o n d u n e where x t s is the output of meta model at timestep t s d s p is the number of days since previous precipitation event and d u n e is a topographic index loritz et al 2019 computed from the hydrosheds 15 arc second dem lehner et al 2008 d u n e describes runoff production through a combination of gravitational potential energy of water height above nearest drainage and dissipation of that potential energy as water moves along the landscape distance to nearest drainage loritz et al 2019 find that it can distinguish between regions with different runoff producing regimes and kallio et al 2019 found it useful in a downscaling context with variable terrains both elevation and d u n e were aggregated from the 15 arc second elevation model to 0 5 degree and 5 arc minute resolution by taking their mean value rf was applied using the ranger package for r wright and ziegler 2017 with 500 trees unlimited tree depth and variance based split rule we chose the lm and rf meta models to have a comparison of a simple linear meta model and a more complex machine learning method which can deal with non linear relationships between the predictand and predictors cutler et al 2007 and which has been successfully used in downscaling studies e g c chen et al 2021 hutengs and vohland 2016 including dm stevens et al 2015 both meta models were trained with the output of cwatm global simulation run at 0 5 resolution using those computational cells which intersect the study area of upper bhima basin fig 3 the source zone dataset was divided so that a random 75 of data points all 30 source zones and all 3652 daily timesteps over 2001 2010 together yields 109 054 data points were used for training and the remaining 25 datapoints were used for testing when applying bias correction we first shift the meta model outputs so that the minimum value within each source zone is 0 reibel and agrawal 2007 if there are any negative values shifting the values is done here because 1 for the linear scaling to work as described all values should be positive and 2 negative runoff values make no physical sense while there may be better alternatives to removing negative values from meta model outputs we shift because shifting preserves the variability provided by the model i e the standard deviation is preserved which is task given to the meta models in section 2 4 we note that as a result cv of any shifted timeseries is not preserved because the mean changes but standard deviation does not bias correction is applied to each daily timestep separately 3 4 assessment we assess the performance of meta models and bias corrected meta models in downscaling coarse resolution cwatm output at the source zone level against a high resolution model run of cwatm at the target zone level using normalized root mean square of error nrmse normalized using the mean and a form of the kling gupta efficiency kge gupta et al 2009 kge is a multi objective function consisting of measures of bias variability and dynamics shown in eq 9 9 k g e 1 r 1 2 α 1 2 β 1 2 where r is the pearson correlation coefficient between simulations s i m and observations o b s α is a measure of variability error σ s i m σ o b s and β is the bias μ s i m μ o b s σ stands for the standard deviation and μ for the mean we use a non parametric variant of kge where the variability α is described using the average error in normalized flow duration curves fdc and pearson correlation coefficient is replaced with spearman rank correlation pool et al 2018 we use the non parametric kge because standard deviation is sensitive to the mean of the timeseries and thus the term is not independent of the bias whereas the normalized fdc is independent of a possible bias in the timeseries further spearman rank correlation is desired here because it is less sensitive to extreme values pool et al 2018 in order to assess an improvement in the overall skill of the downscaling we also compute a kge skill score k g e s s using eq 10 knoben et al 2019 10 k g e s s k g e m o d e l k g e b e n c h m a r k 1 k g e b e n c h m a r k where k g e m o d e l refers to the meta model and k g e b e n c h m a r k refers to the performance of source zone output of cwatm as an estimate of the target zone output of cwatm k g e s s is interpreted so that any positive value means improvement over the benchmark k g e s s 0 signifies equal model performance and negative values indicate poorer model performance k g e s s can take any value between when k g e b e n c h m a r k approaches and when k g e m o d e l approaches for more details we refer the reader to knoben et al 2019 4 results and interpretation we discuss three different aspects of the performance of dm with linear scaling bias correction first overall performance of the meta models benchmark and the results of the bias correction section 4 1 second how dm is affecting the spatial correlation 4 2 and third we consider temporal efficiency 4 3 4 1 improved global and local performance after bias correction both meta models trained at the source zone level show good overall performance and a comparison between a training and testing datasets show no sign of overfitting table 1 of the two meta models rf shows however considerably better performance kgenp 0 96 than lm kgenp 0 71 in the testing dataset when the meta models trained at the source zones are applied to the target zones their performance considerably deteriorates as expected for out of sample prediction error increases and the performance in all components of kgenp decrease a global comparison of the meta models benchmarked against use of the source zone values for the target zones aw y t ˆ shows considerably worse performance for both meta models the deteriorating performance is likely due to 1 the simplicity of the meta models they do not consider land use or soil properties which increase in importance in higher resolutions and thus are not able to represent the heterogenous conditions existing within the basin and 2 the meta model training using a lower resolution spatially averaged model run does not represent all the conditions found when modelling is performed in higher resolution despite the meta models showing inferior performance in the kgenp components than aw y t ˆ overall errors are smaller as shown in table 1 the global goodness of fit statistics reveal that after applying the dm spatial bias correction performance is similar to the benchmark with further reduced errors we note that because the source model runs have 8 higher runoff than target zone model runs reflected in the bias component of aw y t ˆ in table 1 and shown in fig 4 as comparison between source and target zone runoff the maximum attainable kge even when the dynamic and variability terms were perfectly matched is 0 92 that is the assumption that source zone information needs to be preserved means that we have potentially accepted a discrepancy compared to the target zone model runs this level of kge shows a very high match between two timeseries in dm bias is corrected for each source zone separately and consequently for the entire area however the spatial differences in the bias lead to different maximum attainable kge in each area we note here that the two cwatm model runs use different climate forcing data which is a major source of errors in hydrological model runs sperna weiland et al 2015 and that it is known that model runs at different levels of spatial aggregation often result in differences in the simulated water balance wen et al 2021 due to this model to model comparison we do not assess which one more correctly represents the true runoff production within ubb the merits of the presented downscaling method without meta models against observed streamflow records are assessed in kallio et al 2019 2021 using a global evaluation however only provides a partial understanding of the model performance fig 5 shows how local performance is distributed across the basin in small neighborhoods of 3 3 target zones the maps reveal that the meta models perform more poorly than the benchmark in the central and eastern part of the basin fig 5c g areas characterized by low rainfall resulting in low runoff however both meta models improve on the performance in the western basin where runoff production has a high spatial gradient such that downscaling is more important bias correcting the meta model output with dm eliminates nearly all differences between lm y t ˆ and aw y t ˆ in the area where the meta model lm x t was found to be the poorest while still maintaining the improvement of the meta model within the high gradient zone the poor performance artefact found in aw y t ˆ along longitude 74 e becomes almost eliminated in lm y t ˆ the artefact is entirely eliminated in rf y t ˆ but has much higher variability in performance elsewhere compared to aw y t ˆ than lm y t ˆ while the rf meta model shows extremely high performance at the source zone level this finding may indicate that while it performs well on source zone testing data it has still overfitted on the training data and is not as well transferable to the higher resolution target zones as the simpler lm meta model we note here however that the lm meta model predicts nearly half of data points as negative runoff values and the good performance can only be achieved after restricting the outputs to strictly positive values see section 2 3 4 2 improved spatial correlation one of the main benefits of spatial bias correction is that the spatial structure of outputs can substantially improve similarly to the autocorrelation function shown in fig 1b fig 6 shows the median and the 90 distribution of spatial correlation of the benchmark meta models bias corrected meta models and the reference model run of cwatm5min computed between all pairs of target zones the source zone values at target zones aw y t ˆ show a reasonably good agreement with the reference model run both runs are made with the same model albeit with different climate forcing and thus it should be expected that when modelling the same output variable their large scale spatial correlation would be similar however the effect of the coarse correlation can really be seen at the short distances the first two bins in fig 6a since the resolution of the 0 5 source zones is approximately 50 km at ubb a major proportion of all correlation pairs show a perfect match correlation 1 this is not the case for the meta models in fig 6b and c the figures also show that the spatial correlation structure of the meta models in the study area substantially improves in both cases though for rf the improvement is more modest than using lm due to the better structure in rf apart from small differences spatial correlation in aw y t ˆ lm y t ˆ and rf y t ˆ are all highly similar 4 3 assessment of timeseries evaluations based on global and local performance and spatial correlations suggest that using solely source zones seems to be a viable option however it does not accomplish the goal of downscaling as achieved using dm this is highlighted in fig 7 which shows examples of the distribution of runoff values against time within three different source zones the first row in the figure visualizes the fact that aw y t ˆ does not show any distribution of values within the source zone while lm row two rf row three show a large distribution of values the timeseries also illustrates many of the properties of linear scaling bias correction 1 panels a1 to a3 show that when the meta model estimates are reasonably good and with their means close to the reference value of the source zone aw y t ˆ there are only minor adjustments to the distribution 2 in panels b1 to b3 in mid june and mid july 2004 bias correcting meta model outputs improves the distribution estimate however around july 1st source zone value aw y t ˆ is outside the envelope of the cwatm reference model run and thus the distribution of bias corrected meta model outputs have also moved outside the envelope 3 panels c1 to c3 show multiple years of runoff production in the area where the performance of all y t ˆ estimates are poor the lm meta model produces a significant number of negative runoff values and a too wide a range of values bias correcting brings a major improvement in the values although the performance is still poor overall as evidenced by the maps in fig 5 similar improvement is seen also for rf however without the need to shift values 4 the absolute magnitude of the distribution changes according to the direction of the adjustment if the meta model mean is higher than the reference value the distribution shrinks because the adjustment μ o b s μ m o d e l 1 see eqs 5 and 6 the effect is reversed if the mean of meta model outputs is smaller than the reference as the examples in fig 7 portray the final performance of the timeseries depends on the accuracy of the source zone reference values for providing an unbiased estimate and on the ability of the meta model to accurately portray the dynamics within the source zone 5 discussion 5 1 differences to alternative methods and high resolution modelling our study describes spatial bias correction with a method that complements methods found in the literature mainly based on point observations in that it specifically addresses downscaling of data with arbitrary areal spatial support closest areal interpolation alternative area to area kriging results in areal output but require areal features to be first cast to point representation for estimation of the semi variogram hu and huang 2020 this intermediate step is unambiguous additionally kriging is unbiased at the observed point locations but the consequence of using point representation for areal features is that estimates may no longer be unbiased across areal features which is a feature of dm kriging based runoff interpolation furthermore require observations with a high spatial density parajka et al 2015 not generally available in data scarce contexts on the other hand data assimilation methods estimate an optimal combination of model simulations and reference data but do not result in entirely unbiased estimates they also require rigorous a priori uncertainty estimation for both model outputs and observations the standard method of dm is mathematically and conceptually simple and relatively easy to implement statistical meta modelling of arbitrary complexity linear scaling bias correction based on means and spatial intersections between source and target zones furthermore the dm methodology presented herein provides a number of advantages over a full cwatm simulation at the higher resolution 1 substantial computational improvement compared to running the full model in our case study consisting of 30 source zones 567 target zones and 3652 timesteps the unoptimized dm implementation in r language r core team 2020 runs within a few minutes including training of the meta models compared to approximately an hour kallio 2020 for a full cwatm run at 5 arc minute resolution in the area without calibration procedure 2 statistical models such as the lm and rf models used here can use a wide range of explanatory variables while hydrological models have strict requirements for inputs 3 faster calibration to the study area compared to calibrating a distributed hydrological model against observed data or regionalizing parameters from another area and 4 a wider pool of experts is familiar with statistical modelling than with physically based distributed hydrological modelling each of the advantages listed here stack up when uncertainty management requires the use of multiple estimates as is often the case saxe et al 2021 5 2 sensitivities and limitations in light of the presented results source zone values aw y t ˆ despite not being downscaled seem to reflect the runoff outputs of the target zone reasonably well the question therefore arises whether downscaling with the methodology of meta models and bias correction bring any added value our analysis clearly suggests that there is little benefit in downscaling in general when the area within a source zone is homogenous with no large differences in the output in these regions using the coarse resolution data may be justified against the added complexity of training applying and bias correcting a meta model similar concepts have been long explored in hydrological literature e g the concept of hydrological response units in which areas with similar hydrological functioning are lumped together or model applications with variable modelling resolution gharari et al 2020 there is however strong improvement in the performance in areas with a high gradient in the modelled outputs as seen in the western part of the ubb fig 5 furthermore the standard dm methodology used here assumes that the reference values provided by source zones are unbiased and accurate for the areas they describe there are very few variables which can be measured to certainty over large spatial domains or variable spatial units the modifiable area unit problem maup manley 2014 and this uncertainty can affect the regressed model parameters fotheringham and wong 1991 our case study provides a simplified but useful use case with well defined source and target zones as nested grids defined by a cwatm simulations and where the configuration or sizes of target zones play no role this allowed us to have a detailed look at the properties of dm as a downscaling and spatial bias correction method without an added complexity of non conforming target and source zones as an areal interpolation method dm can however deal with arbitrarily shaped source and target zones and therefore can be used to address maup dark and bram 2007 goodchild and lam 1980 kallio et al 2021 performance of the downscaling to non conforming target zones in hydrology has been assessed in kallio et al 2019 2021 and the uncertainty in different realizations of target zones in virkki 2019 we note that modified dm methodologies able to deal with uncertain source and ancillary data are also available leyk et al 2013 nagle et al 2014 however while dm methodology can handle arbitrary overlapping configurations of source and target zones it does not handle spatial dependencies across source zones in environmental modelling this particularly means processes such as streamflow accumulation or air pollution the entire upstream catchment influences streamflow at any particular location and atmospheric processes can transport pollutants from far away regions the meta model used may include these spatial dependencies but independent bias correction at each source zone means that discontinuities and mass balance violations may be introduced at source zone boundaries methods for bias correction of streamflow have been developed which handle the tree like structure of stream networks see e g bennett et al 2021 gottschalk 1993 paiva et al 2015 skøien et al 2006 the standard dm methodology is suitable for any process which may be considered local such as runoff generation without accumulation soil moisture land use and land cover or soil properties or when the source data covers the entire area of generating process see e g the application of the framework to migration data in niva et al 2022 5 3 potential advancements for dm as a bias correction framework in our case study bias correction is carried out individually for each timestep and therefore the distributions of lm y t ˆ and rf y t ˆ closely follow the source zone values aw y t ˆ however extension to spatio temporal variant of dm mennis 2016 means that the reference source zone values could be expressed in a spatio temporal domain e g providing a monthly reference value for the source zone in such a case the daily bias corrected meta model outputs would better retain temporal dynamics provided by the meta model an aspect which is lost with daily adjustments while the linear scaling method used in this study does not always result in the correct distribution of runoff values within source zones other more advanced bias correction methods can be implemented knowledge about the distribution within the source zone say the standard deviation of values found within variance scaling chen et al 2011 could be used to correct for both mean and standard deviation if the actual distribution is known or can be estimated a priori for each source zone more advanced methods like quantile quantile mapping teutschbein and seibert 2012 can be used instead and is recommended if possible teutschbein and seibert 2013 we can expect these methods to improve the spatial distribution of runoff values and thus further improve downscaling performance in principle the used bias correction method is limited by what information is available at the source zone level we note here however that any method which modifies the distribution of the meta model outputs also modifies their spatial correlation in dm the spatial dynamics correlation of the meta model is preserved within source zones and only modified across source zones this behavior is different from methods which apply bias corrections continuously we close the discussion of the presented methodology with a remark that while areal interpolation and as an extension dm are based on area standardization this is not a strict requirement the methodology can extend to other type of variables as well such as water use per capita the modification from standard areal interpolation that needs to be done to support other standardization schemes is to determine how the values are divided when intersecting source and target zones and when combining the target zone intersections back to original i e the areal interpolation component of dm workflow in fig 2d this is straightforward when standardization is by area but can be more complicated for alternative standardizations 6 conclusions in this paper we presented an interpretation of dasymetric modelling as a flexible spatial bias correction framework for downscaling environmental model outputs we showed that the common workflow in dasymetric modelling can be described using concepts of meta modelling and that the distribution of source zone values in dasymetric modelling is in fact equivalent to the common linear scaling bias correction method in a case study we then downscale coarse resolution runoff estimates from cwatm source zones and compare the outputs against a higher resolution model run target zones by exploring the properties of linear scaling in a spatial bias correction setting within the dm framework we find that the dm methodology works well in areas where runoff production exhibits a strong gradient but that there is limited benefit in more homogenous areas the meta models based on precipitation temperature and topographic data have poor performance in most of the study area correcting for the spatially autocorrelated errors spatial bias however corrects most of the problems exhibited by the meta model outputs and significantly improve spatial correlation structure the benefit is strongly dependent on unbiased reference values associated with the source zones and correctly represented dynamics by the meta models dasymetric modelling is a mature technique developed in population studies with multiple variants in the literature it offers great potential for environmental impact studies where large amount of data reflects the future uncertainty space and downscaling to local scales is imperative for policy making as the presented downscaling and spatial bias correction method it offers a flexible simple and effective method which can accommodate arbitrarily shaped areal units for both source and target zones declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements part of the research was developed in the young scientists summer program 2020 at the international institute for applied systems analysis laxenburg austria with financial support from the academy of finland national member organization the work was additionally supported by maa ja vesitekniikan tuki ry the doctoral programme of the school of engineering in aalto university by the academy of finland project watvul grant no 317320 the work on the upper bima basin was conducted as part of the belmont forum sustainable urbanisation global initiative sugi food water energy nexus theme for which coordination was supported by the us national science foundation under grant icer ear 1829999 to stanford university 
25519,complex environmental model outputs used to inform decisions often have systematic errors and are of inappropriate resolution requiring downscaling and bias correction for local applications here we provide a new interpretation of dasymetric modelling dm as a spatial bias correction framework useful in environmental modelling dm is based on areal interpolation where estimates of some variable at target zones are obtained from overlapping source zones using ancillary information we explore dm by downscaling runoff output from a distributed hydrological model using two meta models and describe the properties of the methodology in detail consistent with properties of linear scaling bias correction results show that the methodology 1 reduces errors compared to the source data and meta models 2 improve the spatial structure of the estimates and 3 improve the performance of the downscaled estimates particularly where meta models perform poorly the framework is simple and useful in ensuring spatial coherence of downscaled products keywords bias correction dasymetric modelling downscaling areal interpolation environmental modelling meta modelling data availability the data and code to reproduce the results are shared in an open zenodo repository the link is provided in the data and software availability statement within the manuscript file software and data availability the necessary r code and data to produce the synthetic example table 1 and figs 4 7 are stored in a zenodo repository at https zenodo org record 5857189 last access 16 jan 2022 1 introduction modern environmental decision making depends on the results of various outputs from environmental modelling jakeman et al 2008 schmolke et al 2010 the need for impact modelling to support decision making is particularly important in the face of unprecedented changes the world is currently experiencing in climate land use and other environmental conditions ipcc 2021 the application of complex environmental models is however not trivial and the easily available outputs may be biased or of inappropriate scale or configuration leading to a need for further processing particularly for local applications mismatch between scales of analysis and the source data has been commonly addressed using downscaling upscaling or both downscaling methods attempt to estimate the state of a variable at a finer resolution by relating it to the state of the variable at a coarser resolution von storch and zorita 2019 they are useful when we wish to estimate the internal variability within some temporal or spatial unit in dynamic downscaling a fine resolution model is applied within areas defined by product under downscaling and where the coarse data provides boundary conditions for the fine scale model this is common when e g global climate models are downscaled using regional climate models in empirical downscaling the link between the finer and coarser scales is determined by statistical methods there is a large body of literature on different methods of empirical downscaling for instance using interpolation methods e g mennis 2009 kallio et al 2019 wang et al 2015 lima et al 2021 or different machine learning methods e g latombe et al 2018 stevens et al 2015 bardossy et al 2005 ahmed et al 2013 the outputs from the downscaling methods may exhibit systematic errors i e they may be biased tabari et al 2021 when compared to observations these systematic errors may arise from imperfect model conceptualization and data aggregation teutschbein and seibert 2012 or from errors in input data e g sperna weiland et al 2015 and often need to be corrected ahmed et al 2013 j c chen et al 2021 ibarra et al 2021 teutschbein and seibert 2010 a large number of different approaches to bias correction are available see e g teutschbein and seibert 2012 and are of particular importance for climate change impact modelling e g hempel et al 2013 lange 2019 warszawski et al 2014 which requires unbiased meteorological forcing data tramberend et al 2021 these approaches commonly deal with the temporal dimension with less attention given to correction of spatially autocorrelated errors in distributed environmental models nahar et al 2018 nonetheless some methods to correct spatial bias exist hnilica et al 2017 corrected precipitation with a combination of principal component analysis and quantile mapping nahar et al 2018 used a similar methodology but apply independent component analysis instead of principal component analysis kim et al 2021 developed a bayesian kriging based spatial disaggregation quantile delta mapping method where precipitation distribution parameters are interpolated on to a fine grid with kriging and subsequently used to bias correct downscaled timeseries lange 2019 conducted multivariate bias correction using modified version of cannon 2018 which ensures that downscaled estimates are consistent with the original climate simulations furthermore data assimilation methods are used to optimally combine observations with model outputs in order to reduce their systematic errors reichle 2008 this study presents a novel interpretation of an advanced areal interpolation method dasymetric modelling dm as a method for spatial bias correction of downscaled data areal interpolation consists of all those methods which use values from a set source zones to estimate the same variables at a set of intersecting target zones comber and zeng 2019 goodchild and lam 1980 the target zones are commonly of a higher resolution than source zones in areal interpolation the value of the variable is distributed among target zones within a source zone thus preserving the aggregate value in the area represented in the source zone dm is an extension to areal interpolation where the distribution of the source zone value is guided by some model dm and bias correction share a similar objective of adjusting values to satisfy some constraint e g the distributed values at target zones must match that of their source zone or scaling a daily timeseries to match a monthly reference value we further note that when applying dm to downscale available coarse resolution model outputs the application can be described as meta modelling meta modelling is the process of building a model to emulate another model also known as surrogate modelling e g razavi et al 2012 in meta modelling the output of an original model is emulated by some statistical or process based model from a set of input data with the common aim to reduce computational costs downscaling can be considered meta modelling when the relationship between explanatory variables for instance relating to topography and e g a climate model output is established at a coarser scale and applied at a finer granularity we conduct an experiment in the upper bhima basin ubb central india downscaling local runoff generation i e without accumulation to streamflow several other methods have been developed to handle bias in runoff and streamflow skøien et al 2006 extended kriging interpolation to account for topological relationships within a stream network necessary for the interpolation of streamflow records and further developed a spatio temporal topological kriging method in skøien and blöschl 2007 paiva et al 2015 provided another spatio temporal kriging method to estimate river discharges along a river network loonat et al 2020 used data assimilation of streamflow records to a distributed rainfall runoff model by kriging and global bias correction naz et al 2019 downscaled runoff data using a data assimilation scheme using soil moisture as an ancillary variable bennett et al 2021 developed a mechanism for streamflow which specifically bias correct runoff and apply river routing to make it possible to use streamflow as the reference to correct against in our case study we examine the bias correcting property of dasymetric modelling in detail we train two meta models to emulate the local runoff output of the community water model cwatm burek et al 2020 a distributed physically based hydrological model based on a global model run at 0 5 approx 50 50 km resolution the meta models are then applied to a 5 arc minute approx 10 10 km grid within the same area the 0 5 grid serves as the source zones giving a spatial reference of the magnitude of the values within the basin and the 5 arc minute grid serves as the target zones the meta model outputs at the target zones are then bias corrected using the source zone values as a reference the comparison to cwatm at higher resolution is appropriate because the meta models are trained on cwatm at lower resolution emulating the behavior of the model the experiment we conduct here extends previous work in kallio et al 2021 2019 where global runoff products were downscaled with dasymetric mapping i e dm with a single ancillary variable instead of a model in the earlier studies the bias correcting property is confounded due to a use of irregular polygons which may be influenced by several intersecting source zones b the uncertainties related to river routing modelling of accumulation of runoff in the stream network c the difficulty of assigning a low resolution streamflow value to the more complex higher resolution river network see e g bennett et al 2021 for a discussion of this problem and d the use of a monthly timestep which is large considering the size of the basins the experiment here focuses on the bias correcting property by using a nested grid controls the confounding by areal interpolation comparing local runoff production controlling for uncertainties in routing models and aggregation effects and is applied at a daily rather than a monthly timestep in this way we can isolate the influence of bias correction from the other factors influencing the performance the remainder of the paper is structured as follows section 2 provides the background describing similarities and equivalence of dm to meta modelling and bias correction respectively section 3 describes our case study giving information on cwatm the upper bhima basin meta models and how we assess performance results and detailed discussion of the case study outputs are given in section 4 the bias correction framework and the case study are discussed in section 5 finally we conclude the paper in section 6 2 dasymetric modelling meta modelling and bias correction this section first introduces dm in section 2 1 and proceeds to describe the similarities of the common dm workflow and meta modelling in section 2 2 in the next section 2 3 we establish the equivalence of dm and linear scaling bias correction in section 2 4 we bring them all together in a single framework 2 1 dasymetric modelling areal interpolation a form of spatial interpolation dealing with interpolating values from a source zone to another set of overlapping arbitrary target zones is a standard practice in many fields comber and zeng 2019 in its simplest form area weighted interpolation aw a value from a source zone is assigned to target zones based on their overlapping areas as shown in eq 1 1 y t ˆ s s y s a t s a s where y t ˆ is the estimated value at a target zone t y s denotes the value associated with a source zone s and a is the area of the zone the target zone t is therefore assigned values from all source zones it intersects denoted as the set s according to the proportion of the area of the target zone within each source zone a t s to the area of each overlapping source zone the formulation allows estimation of values in arbitrary target zones which do not neatly conform to the also arbitrary shape of the source zones areal interpolation has a pycnophylactic mass preserving property because it is in essence distributing the source zone values onto the overlapping target zones this procedure leads to the preservation of the source zone values in the target zones mennis 2009 tobler 1979 dasymetric mapping is extension of aw where the interpolation is refined by the use of ancillary information comber and zeng 2019 mennis 2009 dm has originally been developed for improved cartographic presentation of population density mennis 2009 wright 1936 a large body of literature has been written on different applications of dm and its extensions particularly in the context of state of the art population mapping e g stevens et al 2015 tatem 2017 demographic attributes e g tatem et al 2014 and to lesser extent in other fields such as environmental modelling kallio et al 2019 2021 chen et al 2019 in dasymetric mapping the task of the ancillary data is to provide information on the distribution of the interpolated value within a source zone with the ancillary information x t eq 1 becomes 2 y t ˆ s s y s a t s x t t t a t s x t which uses information about target zones t belonging to a set t consisting of all target zones intersecting with source zone s to scale the ancillary data in dasymetric modelling we will use dm to refer to dasymetric modelling for the remainder of the paper the ancillary information x t is given by some model describing the target zone level spatial distribution of the variable given in source zones rather than a single variable as in dasymetric mapping nagle et al 2014 for a review of areal interpolation methods we refer to the review by comber and zeng 2019 2 2 dasymetric modelling and meta modelling in statistical downscaling the relationship between ancillary variable s and the downscaled quantity is commonly regressed at the source zone level von storch and zorita 2019 this is a common workflow in dm as well mennis 2009 nagle et al 2014 the model trained at source zones is subsequently applied at the target zone level to obtain the downscaled estimates in dm the estimates are used as the ancillary information x t the outputs of the trained model are used to derive weights according to which the source zone value is distributed to contributing target zones if the values associated with the source zones are an output of a model e g a climate an impact or some empirical statistical model the workflow can be described as a meta modelling approach a meta model attempts to emulate the output of the original model with the aid of explanatory variables in this case the meta model attempts to estimate what the full model would have output at the target zone level if it was run at that resolution the motivation of using different types of meta models is often to reduce the complexity of a full model to reduce the computational cost and sometimes to reduce input data requirements asher et al 2015 razavi et al 2012 the common workflow in dm can therefore be seen as meta model assisted distribution of the model outputs evaluated at source zones on to the overlapping target zones areal interpolation dm allows this to be done with the objective of upscaling downscaling or representing the values in similar resolution but different zonal arrangement kallio et al 2021 2 3 dasymetric modelling and bias correction in a special case of dm where all target zones are located within a single source zone e g a nested grid a common setup in downscaling earth science data and each target zone is of the same area e g a raster grid with an equal area projection the area terms in eq 2 can be eliminated resulting in eq 3a 3a y t ˆ y s x t t t s x t this equation can be read as solution where the value associated with a source zone y s is distributed to the target zones according to the proportion of some meta model output x t from the sum of meta model results within the source zone if we swap the position of y s and x t the resulting eq 3b is equivalent but with the change in grouping of terms we can now read the equation as the scaling of the meta model outputs x t 3b y t ˆ x t y s t t s x t if the denominator is divided by the number n of target zones within a source zone we obtain the mean output μ x t of the meta model within the source zone using the source zone as our reference the equivalent mean across n target zones within the source zone is μ y t 1 n y s and thus we can simplify eq 3b to eq 5 with the intermediate step given in eq 4 4 y t ˆ x t 1 n y s 1 n t t s x t 5 y t ˆ x t μ y t μ x t where y s is the value associated with the source zone in which the target zone is located and μ x t is the mean value of all meta model outputs within the source zone this formulation is identical to the linear scaling bias correction method shown in eq 6 lenderink et al 2007 teutschbein and seibert 2012 6 y m o d e l y m o d e l μ o b s μ m o d e l where y m o d e l is the bias corrected model output although bias correction is commonly applied to timeseries data it may also be applied to a spatial domain where target zone values are corrected against the source zone data thus when stripped of the area terms under the conditions described above dm is mathematically equivalent to the common linear scaling bias correction method an example of linear scaling bias correction with one dimensional synthetic data is provided in fig 1 showing a cosine function and a hypothetical meta model output x t and its bias corrected counterpart y t ˆ along with goodness of fit statistics for illustration purposes the hypothetical meta model output for each point t was obtained here by adding random variation into the cosine series dividing the values into groups and subtracting a random amount from each group linear scaling is applied group wise for each source s using equation 5 rather than to the dataset as a whole akin to the dm approach having multiple source zones for which to perform bias correction separately from this example we note a few important properties of linear scaling 1 by design bias is eliminated group wise and subsequently for the entire data series 2 pearson correlation coefficient is preserved within group but changes for the entire series 3 the magnitude of standard deviation is changed the reason for preservation of pearson correlation coefficient ρ eq 7 within a group is because ρ depends on how the two data series vary around their respective means 7 ρ x y e x μ x y μ y σ x σ y since each data point in a group is subjected to the same linear transformation the relationship between the standard deviation σ and the mean μ remains unchanged in other words coefficient of variation c v σ μ remains unaltered even when the absolute magnitude of the standard deviation is changed as a consequence bias correction preserves the dynamics provided by the model within each group but because each group is subjected to different scaling operation the overall dynamics correlation is altered in this synthetic example pearson correlation coefficient improves from 0 74 to 0 96 fig 1b shows autocorrelation functions correlation of a variable with itself for the true original the meta model output x t and bias corrected meta model output y t ˆ autocorrelation is significantly improved with the method given the equivalence between linear scaling and dasymetric mapping on nested grids a similar behavior is expected in a spatial bias correction context we note that for the linear scaling bias correction to work as described here all meta model output values x t and the reference values associated with the source zones y s should be strictly finite positive values to prevent erratic behavior 2 4 dasymetric modelling as a combined downscaling bias correction framework the previous sections describe how meta modelling and bias correction relate to dm fig 2 shows workflows for each of these three methods separately and combined together into a single framework each of the three methods have a specific purpose in the workflow the meta model outputs x t evaluated at the target zones is responsible for representing the spatial dynamics of how the value is distributed within a source zone i e the meta model output should have a high correlation with the ground truth values within the source zones bias correction ensures that the spatial information associated with source zones are preserved in the target zones leading to spatially debiased estimates dm provides flexibility in the spatial configuration of source and target zones it describes how non conforming source and target zones are handled how should the meta model output in each target zone be divided during the intersection with source zones and how to combine the values back to the original target zones after bias correction this operation may differ for instance for intensive standardized variables e g population density or water use per capita and extensive variables counts e g the amount of population or volume of water the workflow is agnostic as to how x t is estimated whether given by another similar model a meta model or measured data similarly the method is agnostic as to where the source zone reference values y s are obtained in principle with the redistribution of conventional dm interpreted here as bias correction linear scaling may be replaced by more advanced bias correction methods this leads to the framework being modular consisting of three distinct parts 3 case study we explore the dasymetric modelling framework as interpreted in fig 2 and its properties by a case study where we downscale a coarse resolution 0 5 source zones cwatm simulation with a global extent onto a higher resolution 5 arc minute grid target zones we make use of meta models trained at the source zone level and applied at the target zones the results are compared to a run of cwatm at the target zones in order to evaluate the usefulness of the methodology and the ability to emulate a more spatially detailed model output the experiment is setup so that the source and target zones are nested which reflects the simplification described in section 2 3 this allows us to examine the efficiency of the spatial bias correction without the added complication of non conforming source and target zones in section 3 1 we give a brief introduction to cwatm and section 3 2 contains a description of the study area the upper bhima basin next section 3 3 introduces the two meta models used and section 3 4 describes how we assess the downscaling performance 3 1 community water model the community water model is an open source integrated hydrological and channel model that calculates water availability surface and groundwater environmental flow requirements and socio economic water demands and impacts from water infrastructures such as reservoirs groundwater pumping and irrigation burek et al 2020 the global applications of the model are based on a 0 5 or 5 arc minute resolution grid and are run at a daily temporal resolution the model can be used for regional applications using a daily temporal and 30 arc sec spatial resolution for topography and land cover it uses a sub grid approach and for soil and routing processes it uses sub daily timesteps the conceptual framework and structure of cwatm are similar to that of other hydrological models such as h08 hanasaki et al 2006 2008 2010 watergap alcamo et al 2003 flörke et al 2013 pcr globwb van beek et al 2011 wada et al 2014 and others the model accounts for future water demands based on socio economic change and the impacts of water availability in response to climate change irrigation water demand is estimated for paddy and non paddy crops separately by dynamically linking irrigation water demand with the surface and soil water balance for the upper bhima basin cwatm is forced by the meteorological variables including temperature wind speed relative humidity incoming longwave and shortwave radiation and surface air pressure from w5e5 v2 0 cucchi et al 2020 for precipitation we use data from pai et al 2014 please see burek et al 2020 for a more detailed description of cwatm hydrological processes 3 2 upper bhima basin the bhima basin in india shown in fig 3 is a highly managed and densely populated watershed with important rain fed and irrigated crop production rainfall occurs towards the west and concentrates through 5 months of monsoon with a total annual rainfall of around 35 km3 approx 760 mm per year the bhima basin is an upstream basin of the krishna basin originating in the mountains of the western ghats with a river length of 325 km the bhima basin supports a population of 18 7 million 2015 distributed over 45 800 km2 of largely agricultural land flowing mostly within the state of maharashtra the largest urban agglomeration within the bhima basin is pune with nearly 7 million residents followed by solapur with approximately 1 2 million residents in 2015 cwatm has been successfully applied at the bhima basin guillaumot et al 2022 3 3 meta models two meta models were developed to estimate cwatm modelled runoff output over upper bhima basin during 2001 2010 the first meta model is a standard ordinary least squares regression referred to as the linear model lm the second meta model is based on random forest regression rf breiman 2001 both models were trained at the source zone level 0 5 resolution with the same model equation based on derived variables from a digital elevation model dem daily precipitation p and daily temperature t measurements 8 x t s p t s p t s 1 p t s 2 s u m p t s 1 7 s u m p t s 1 30 s u m p t s 1 180 d s p t t s t t s 1 t t s 2 m e a n t t s 1 7 e l e v a t i o n d u n e where x t s is the output of meta model at timestep t s d s p is the number of days since previous precipitation event and d u n e is a topographic index loritz et al 2019 computed from the hydrosheds 15 arc second dem lehner et al 2008 d u n e describes runoff production through a combination of gravitational potential energy of water height above nearest drainage and dissipation of that potential energy as water moves along the landscape distance to nearest drainage loritz et al 2019 find that it can distinguish between regions with different runoff producing regimes and kallio et al 2019 found it useful in a downscaling context with variable terrains both elevation and d u n e were aggregated from the 15 arc second elevation model to 0 5 degree and 5 arc minute resolution by taking their mean value rf was applied using the ranger package for r wright and ziegler 2017 with 500 trees unlimited tree depth and variance based split rule we chose the lm and rf meta models to have a comparison of a simple linear meta model and a more complex machine learning method which can deal with non linear relationships between the predictand and predictors cutler et al 2007 and which has been successfully used in downscaling studies e g c chen et al 2021 hutengs and vohland 2016 including dm stevens et al 2015 both meta models were trained with the output of cwatm global simulation run at 0 5 resolution using those computational cells which intersect the study area of upper bhima basin fig 3 the source zone dataset was divided so that a random 75 of data points all 30 source zones and all 3652 daily timesteps over 2001 2010 together yields 109 054 data points were used for training and the remaining 25 datapoints were used for testing when applying bias correction we first shift the meta model outputs so that the minimum value within each source zone is 0 reibel and agrawal 2007 if there are any negative values shifting the values is done here because 1 for the linear scaling to work as described all values should be positive and 2 negative runoff values make no physical sense while there may be better alternatives to removing negative values from meta model outputs we shift because shifting preserves the variability provided by the model i e the standard deviation is preserved which is task given to the meta models in section 2 4 we note that as a result cv of any shifted timeseries is not preserved because the mean changes but standard deviation does not bias correction is applied to each daily timestep separately 3 4 assessment we assess the performance of meta models and bias corrected meta models in downscaling coarse resolution cwatm output at the source zone level against a high resolution model run of cwatm at the target zone level using normalized root mean square of error nrmse normalized using the mean and a form of the kling gupta efficiency kge gupta et al 2009 kge is a multi objective function consisting of measures of bias variability and dynamics shown in eq 9 9 k g e 1 r 1 2 α 1 2 β 1 2 where r is the pearson correlation coefficient between simulations s i m and observations o b s α is a measure of variability error σ s i m σ o b s and β is the bias μ s i m μ o b s σ stands for the standard deviation and μ for the mean we use a non parametric variant of kge where the variability α is described using the average error in normalized flow duration curves fdc and pearson correlation coefficient is replaced with spearman rank correlation pool et al 2018 we use the non parametric kge because standard deviation is sensitive to the mean of the timeseries and thus the term is not independent of the bias whereas the normalized fdc is independent of a possible bias in the timeseries further spearman rank correlation is desired here because it is less sensitive to extreme values pool et al 2018 in order to assess an improvement in the overall skill of the downscaling we also compute a kge skill score k g e s s using eq 10 knoben et al 2019 10 k g e s s k g e m o d e l k g e b e n c h m a r k 1 k g e b e n c h m a r k where k g e m o d e l refers to the meta model and k g e b e n c h m a r k refers to the performance of source zone output of cwatm as an estimate of the target zone output of cwatm k g e s s is interpreted so that any positive value means improvement over the benchmark k g e s s 0 signifies equal model performance and negative values indicate poorer model performance k g e s s can take any value between when k g e b e n c h m a r k approaches and when k g e m o d e l approaches for more details we refer the reader to knoben et al 2019 4 results and interpretation we discuss three different aspects of the performance of dm with linear scaling bias correction first overall performance of the meta models benchmark and the results of the bias correction section 4 1 second how dm is affecting the spatial correlation 4 2 and third we consider temporal efficiency 4 3 4 1 improved global and local performance after bias correction both meta models trained at the source zone level show good overall performance and a comparison between a training and testing datasets show no sign of overfitting table 1 of the two meta models rf shows however considerably better performance kgenp 0 96 than lm kgenp 0 71 in the testing dataset when the meta models trained at the source zones are applied to the target zones their performance considerably deteriorates as expected for out of sample prediction error increases and the performance in all components of kgenp decrease a global comparison of the meta models benchmarked against use of the source zone values for the target zones aw y t ˆ shows considerably worse performance for both meta models the deteriorating performance is likely due to 1 the simplicity of the meta models they do not consider land use or soil properties which increase in importance in higher resolutions and thus are not able to represent the heterogenous conditions existing within the basin and 2 the meta model training using a lower resolution spatially averaged model run does not represent all the conditions found when modelling is performed in higher resolution despite the meta models showing inferior performance in the kgenp components than aw y t ˆ overall errors are smaller as shown in table 1 the global goodness of fit statistics reveal that after applying the dm spatial bias correction performance is similar to the benchmark with further reduced errors we note that because the source model runs have 8 higher runoff than target zone model runs reflected in the bias component of aw y t ˆ in table 1 and shown in fig 4 as comparison between source and target zone runoff the maximum attainable kge even when the dynamic and variability terms were perfectly matched is 0 92 that is the assumption that source zone information needs to be preserved means that we have potentially accepted a discrepancy compared to the target zone model runs this level of kge shows a very high match between two timeseries in dm bias is corrected for each source zone separately and consequently for the entire area however the spatial differences in the bias lead to different maximum attainable kge in each area we note here that the two cwatm model runs use different climate forcing data which is a major source of errors in hydrological model runs sperna weiland et al 2015 and that it is known that model runs at different levels of spatial aggregation often result in differences in the simulated water balance wen et al 2021 due to this model to model comparison we do not assess which one more correctly represents the true runoff production within ubb the merits of the presented downscaling method without meta models against observed streamflow records are assessed in kallio et al 2019 2021 using a global evaluation however only provides a partial understanding of the model performance fig 5 shows how local performance is distributed across the basin in small neighborhoods of 3 3 target zones the maps reveal that the meta models perform more poorly than the benchmark in the central and eastern part of the basin fig 5c g areas characterized by low rainfall resulting in low runoff however both meta models improve on the performance in the western basin where runoff production has a high spatial gradient such that downscaling is more important bias correcting the meta model output with dm eliminates nearly all differences between lm y t ˆ and aw y t ˆ in the area where the meta model lm x t was found to be the poorest while still maintaining the improvement of the meta model within the high gradient zone the poor performance artefact found in aw y t ˆ along longitude 74 e becomes almost eliminated in lm y t ˆ the artefact is entirely eliminated in rf y t ˆ but has much higher variability in performance elsewhere compared to aw y t ˆ than lm y t ˆ while the rf meta model shows extremely high performance at the source zone level this finding may indicate that while it performs well on source zone testing data it has still overfitted on the training data and is not as well transferable to the higher resolution target zones as the simpler lm meta model we note here however that the lm meta model predicts nearly half of data points as negative runoff values and the good performance can only be achieved after restricting the outputs to strictly positive values see section 2 3 4 2 improved spatial correlation one of the main benefits of spatial bias correction is that the spatial structure of outputs can substantially improve similarly to the autocorrelation function shown in fig 1b fig 6 shows the median and the 90 distribution of spatial correlation of the benchmark meta models bias corrected meta models and the reference model run of cwatm5min computed between all pairs of target zones the source zone values at target zones aw y t ˆ show a reasonably good agreement with the reference model run both runs are made with the same model albeit with different climate forcing and thus it should be expected that when modelling the same output variable their large scale spatial correlation would be similar however the effect of the coarse correlation can really be seen at the short distances the first two bins in fig 6a since the resolution of the 0 5 source zones is approximately 50 km at ubb a major proportion of all correlation pairs show a perfect match correlation 1 this is not the case for the meta models in fig 6b and c the figures also show that the spatial correlation structure of the meta models in the study area substantially improves in both cases though for rf the improvement is more modest than using lm due to the better structure in rf apart from small differences spatial correlation in aw y t ˆ lm y t ˆ and rf y t ˆ are all highly similar 4 3 assessment of timeseries evaluations based on global and local performance and spatial correlations suggest that using solely source zones seems to be a viable option however it does not accomplish the goal of downscaling as achieved using dm this is highlighted in fig 7 which shows examples of the distribution of runoff values against time within three different source zones the first row in the figure visualizes the fact that aw y t ˆ does not show any distribution of values within the source zone while lm row two rf row three show a large distribution of values the timeseries also illustrates many of the properties of linear scaling bias correction 1 panels a1 to a3 show that when the meta model estimates are reasonably good and with their means close to the reference value of the source zone aw y t ˆ there are only minor adjustments to the distribution 2 in panels b1 to b3 in mid june and mid july 2004 bias correcting meta model outputs improves the distribution estimate however around july 1st source zone value aw y t ˆ is outside the envelope of the cwatm reference model run and thus the distribution of bias corrected meta model outputs have also moved outside the envelope 3 panels c1 to c3 show multiple years of runoff production in the area where the performance of all y t ˆ estimates are poor the lm meta model produces a significant number of negative runoff values and a too wide a range of values bias correcting brings a major improvement in the values although the performance is still poor overall as evidenced by the maps in fig 5 similar improvement is seen also for rf however without the need to shift values 4 the absolute magnitude of the distribution changes according to the direction of the adjustment if the meta model mean is higher than the reference value the distribution shrinks because the adjustment μ o b s μ m o d e l 1 see eqs 5 and 6 the effect is reversed if the mean of meta model outputs is smaller than the reference as the examples in fig 7 portray the final performance of the timeseries depends on the accuracy of the source zone reference values for providing an unbiased estimate and on the ability of the meta model to accurately portray the dynamics within the source zone 5 discussion 5 1 differences to alternative methods and high resolution modelling our study describes spatial bias correction with a method that complements methods found in the literature mainly based on point observations in that it specifically addresses downscaling of data with arbitrary areal spatial support closest areal interpolation alternative area to area kriging results in areal output but require areal features to be first cast to point representation for estimation of the semi variogram hu and huang 2020 this intermediate step is unambiguous additionally kriging is unbiased at the observed point locations but the consequence of using point representation for areal features is that estimates may no longer be unbiased across areal features which is a feature of dm kriging based runoff interpolation furthermore require observations with a high spatial density parajka et al 2015 not generally available in data scarce contexts on the other hand data assimilation methods estimate an optimal combination of model simulations and reference data but do not result in entirely unbiased estimates they also require rigorous a priori uncertainty estimation for both model outputs and observations the standard method of dm is mathematically and conceptually simple and relatively easy to implement statistical meta modelling of arbitrary complexity linear scaling bias correction based on means and spatial intersections between source and target zones furthermore the dm methodology presented herein provides a number of advantages over a full cwatm simulation at the higher resolution 1 substantial computational improvement compared to running the full model in our case study consisting of 30 source zones 567 target zones and 3652 timesteps the unoptimized dm implementation in r language r core team 2020 runs within a few minutes including training of the meta models compared to approximately an hour kallio 2020 for a full cwatm run at 5 arc minute resolution in the area without calibration procedure 2 statistical models such as the lm and rf models used here can use a wide range of explanatory variables while hydrological models have strict requirements for inputs 3 faster calibration to the study area compared to calibrating a distributed hydrological model against observed data or regionalizing parameters from another area and 4 a wider pool of experts is familiar with statistical modelling than with physically based distributed hydrological modelling each of the advantages listed here stack up when uncertainty management requires the use of multiple estimates as is often the case saxe et al 2021 5 2 sensitivities and limitations in light of the presented results source zone values aw y t ˆ despite not being downscaled seem to reflect the runoff outputs of the target zone reasonably well the question therefore arises whether downscaling with the methodology of meta models and bias correction bring any added value our analysis clearly suggests that there is little benefit in downscaling in general when the area within a source zone is homogenous with no large differences in the output in these regions using the coarse resolution data may be justified against the added complexity of training applying and bias correcting a meta model similar concepts have been long explored in hydrological literature e g the concept of hydrological response units in which areas with similar hydrological functioning are lumped together or model applications with variable modelling resolution gharari et al 2020 there is however strong improvement in the performance in areas with a high gradient in the modelled outputs as seen in the western part of the ubb fig 5 furthermore the standard dm methodology used here assumes that the reference values provided by source zones are unbiased and accurate for the areas they describe there are very few variables which can be measured to certainty over large spatial domains or variable spatial units the modifiable area unit problem maup manley 2014 and this uncertainty can affect the regressed model parameters fotheringham and wong 1991 our case study provides a simplified but useful use case with well defined source and target zones as nested grids defined by a cwatm simulations and where the configuration or sizes of target zones play no role this allowed us to have a detailed look at the properties of dm as a downscaling and spatial bias correction method without an added complexity of non conforming target and source zones as an areal interpolation method dm can however deal with arbitrarily shaped source and target zones and therefore can be used to address maup dark and bram 2007 goodchild and lam 1980 kallio et al 2021 performance of the downscaling to non conforming target zones in hydrology has been assessed in kallio et al 2019 2021 and the uncertainty in different realizations of target zones in virkki 2019 we note that modified dm methodologies able to deal with uncertain source and ancillary data are also available leyk et al 2013 nagle et al 2014 however while dm methodology can handle arbitrary overlapping configurations of source and target zones it does not handle spatial dependencies across source zones in environmental modelling this particularly means processes such as streamflow accumulation or air pollution the entire upstream catchment influences streamflow at any particular location and atmospheric processes can transport pollutants from far away regions the meta model used may include these spatial dependencies but independent bias correction at each source zone means that discontinuities and mass balance violations may be introduced at source zone boundaries methods for bias correction of streamflow have been developed which handle the tree like structure of stream networks see e g bennett et al 2021 gottschalk 1993 paiva et al 2015 skøien et al 2006 the standard dm methodology is suitable for any process which may be considered local such as runoff generation without accumulation soil moisture land use and land cover or soil properties or when the source data covers the entire area of generating process see e g the application of the framework to migration data in niva et al 2022 5 3 potential advancements for dm as a bias correction framework in our case study bias correction is carried out individually for each timestep and therefore the distributions of lm y t ˆ and rf y t ˆ closely follow the source zone values aw y t ˆ however extension to spatio temporal variant of dm mennis 2016 means that the reference source zone values could be expressed in a spatio temporal domain e g providing a monthly reference value for the source zone in such a case the daily bias corrected meta model outputs would better retain temporal dynamics provided by the meta model an aspect which is lost with daily adjustments while the linear scaling method used in this study does not always result in the correct distribution of runoff values within source zones other more advanced bias correction methods can be implemented knowledge about the distribution within the source zone say the standard deviation of values found within variance scaling chen et al 2011 could be used to correct for both mean and standard deviation if the actual distribution is known or can be estimated a priori for each source zone more advanced methods like quantile quantile mapping teutschbein and seibert 2012 can be used instead and is recommended if possible teutschbein and seibert 2013 we can expect these methods to improve the spatial distribution of runoff values and thus further improve downscaling performance in principle the used bias correction method is limited by what information is available at the source zone level we note here however that any method which modifies the distribution of the meta model outputs also modifies their spatial correlation in dm the spatial dynamics correlation of the meta model is preserved within source zones and only modified across source zones this behavior is different from methods which apply bias corrections continuously we close the discussion of the presented methodology with a remark that while areal interpolation and as an extension dm are based on area standardization this is not a strict requirement the methodology can extend to other type of variables as well such as water use per capita the modification from standard areal interpolation that needs to be done to support other standardization schemes is to determine how the values are divided when intersecting source and target zones and when combining the target zone intersections back to original i e the areal interpolation component of dm workflow in fig 2d this is straightforward when standardization is by area but can be more complicated for alternative standardizations 6 conclusions in this paper we presented an interpretation of dasymetric modelling as a flexible spatial bias correction framework for downscaling environmental model outputs we showed that the common workflow in dasymetric modelling can be described using concepts of meta modelling and that the distribution of source zone values in dasymetric modelling is in fact equivalent to the common linear scaling bias correction method in a case study we then downscale coarse resolution runoff estimates from cwatm source zones and compare the outputs against a higher resolution model run target zones by exploring the properties of linear scaling in a spatial bias correction setting within the dm framework we find that the dm methodology works well in areas where runoff production exhibits a strong gradient but that there is limited benefit in more homogenous areas the meta models based on precipitation temperature and topographic data have poor performance in most of the study area correcting for the spatially autocorrelated errors spatial bias however corrects most of the problems exhibited by the meta model outputs and significantly improve spatial correlation structure the benefit is strongly dependent on unbiased reference values associated with the source zones and correctly represented dynamics by the meta models dasymetric modelling is a mature technique developed in population studies with multiple variants in the literature it offers great potential for environmental impact studies where large amount of data reflects the future uncertainty space and downscaling to local scales is imperative for policy making as the presented downscaling and spatial bias correction method it offers a flexible simple and effective method which can accommodate arbitrarily shaped areal units for both source and target zones declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements part of the research was developed in the young scientists summer program 2020 at the international institute for applied systems analysis laxenburg austria with financial support from the academy of finland national member organization the work was additionally supported by maa ja vesitekniikan tuki ry the doctoral programme of the school of engineering in aalto university by the academy of finland project watvul grant no 317320 the work on the upper bima basin was conducted as part of the belmont forum sustainable urbanisation global initiative sugi food water energy nexus theme for which coordination was supported by the us national science foundation under grant icer ear 1829999 to stanford university 
