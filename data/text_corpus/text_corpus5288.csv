index,text
26440,numerical models have been widely applied in simulating subsurface non aqueous phase liquid napl contamination processes in order to examine modeling uncertainties and improve simulation performance a new hybrid stochastic design of experiment doe aided parameterization method was developed by using a coupled experimental and modeling approach in a case study an existing commercial groundwater modeling tool biof t 3d was applied to conduct numerical simulations of subsurface contamination processes based on flow cell experiments parameterization results indicated that porosity distribution coefficient and henry s constant were the most significant parameters the result also revealed their interactions the doe predicted responses were found reasonably close to the actual ones from the models simulations monte carlo simulation was applied to conduct uncertainty analysis within the narrowed parameters ranges which were generated by centralizing the doe optimized values and the combinations of parameters were further updated when better responses were found after parameterization r2 valued 0 80 0 91 0 89 and 0 90 for benzene toluene ethylbenzene and xylene btex respectively a good consistency r2 0 76 to 0 90 for btex was also achieved during the model verification which confirmed that after the parameterization processes the simulation model can potentially be used for predictions under similar conditions keywords design of experiment stochastic parameterization subsurface napl contamination uncertainty 1 introduction subsurface non aqueous phase liquids napls contamination from spill and leakage of petroleum products has become a major environmental concern due to the long persistence time and complicated treatment required napls are difficult to be removed from water and soil leading to a high potential risk to human health in a long term timespan yang et al 2012 jácome and van geel 2015 napls leakage and contamination accidents are closely associated with human activities and it can also raise the risk to cause severe environmental and health hazards garcía junco et al 2001 in order to achieve a better understanding of the fate of contaminants in aquifer numerical simulation has been generally accepted as an effective tool and continuously studied kim and corapcioglu 2003 yu et al 2010 the knowledge can be further used to support decision making on monitoring and remediation practices as a simplified representative the numerical model is able to provide the simulated outputs for the modeled system the simulated outputs based on various model settings can reflect different scenarios through numerical simulations more resources energy and time can be saved leading a more efficient decision making process ideally numerical model should be able to reflect the realistic situations however there is often a lack of fit existing due to uncertainty in the modeling system especially the imprecisely defined parameter uncertainties uncertainty is inevitable at the current stage due to limited knowledge and simplification of numerical models in order to increase the reliability of the modeling results it is essential to conduct uncertainty and sensitivity analysis and properly calibrate the model such that discrepancies between simulated and observed data can be minimized parameter uncertainties have been extensively studied particularly integrated with sensitivity analysis and model calibrations sin et al 2011 he et al 2012 shen et al 2012 zhuo et al 2013 houska et al 2014 to consider the parameter uncertainties the optimal results from the different combinations of the parameter settings were analyzed monte carlo simulation is one of the most common stochastic methods involving random sampling within certain types of distributions due to the ease of implementation and generalization monte carlo methods have been widely applied to environmental systems by propagating the parameter uncertainties and evaluating their impacts on the model output huang and loucks 2000 jing et al 2013 li et al 2014 one factor at a time ofat is one of the traditional sensitivity analysis methods this method simply adjusts one parameter at a time while keeping other parameters fixed its applications have been found in multiple studies conducted on various models holvoet et al 2005 jing and chen 2011 garson 1991 introduced another method based on the concept of artificial neural networks ann by assigning the neural net weight matrix this method has been further implemented in many other studies to find the relative importance of input variables in different processes modeling kermani and ebadi 2012 nourani and fard 2012 jing et al 2014 however both of these methods have their limitations ofat is incapable of revealing the interactions between parameters which might lead to the ignorance of the potentially significant variables peeters et al 2014 garson s method is debatable due to the black box nature of ann olden et al 2004 witek krowiak et al 2014 the traditional method of model calibration is essentially a trial and error process which uses iterations to adjust the relevant parameters until the simulated outputs are sufficiently close to the experimental data this method is still popular and has been embedded in commercial modeling tools for automatic calibration sonnenborg et al 2003 mugunthan et al 2005 razavi and tolson 2012 existing studies have focused on optimizing the mathematical algorithms to achieve a more efficient calibration process wu et al 2012 2014 zhao et al 2013 calibration methods have also been improved by applying parameter estimation and global optimization within reasonable predefined intervals kang 2014 plasencia et al 2014 yen et al 2014 zhang et al 2015 despite that a good fit can be expected it should not be ignored that some major limitations such as extensive computational requirements low physical plausibility and over parameterization exist when traditional calibration methods are employed van griensven et al 2006 whittaker et al 2010 okamoto and akella 2012 to address this issue design of experiment doe provides a parameterization option doe is a well known statistical methodology which can unveil the interrelationships between parameters and the corresponding responses by conducting controlled experiments park 2007 strigul et al 2009 by using doe it is possible to simultaneously study several parameters and their interactions veličković et al 2013 sarikaya and güllü 2015 doe was originally developed to guide the planning and setup for physical experiments however considering that the complexity and cost can increase dramatically with the growing number of input variables numerical simulation tools have been extensively involved especially in recent studies wu et al 2012 used a doe aided method to conduct sensitivity analysis and parameterization for a hydrological model slurp and optimized the predicted regression equation which has resulted in a greater goodness of fit value compared to the one achieved by the automatic calibration function within the model zahraee et al 2013 introduced doe in modeling a real world construction process to achieve optimal resource levels and maximize the process productivity in the study of al shalabi et al 2014 seven uncertain design parameters for a low salinity water injection process were screened by using doe method followed by the optimization of cumulative oil recovery using the response surface methodology rsm though doe aided methods have proven advantages in conducting parameterization for numerical models relationships between responses and stochastically distributed parameters are seldom integrated besides it has rarely been used in groundwater and subsurface contamination models in which uncertainties commonly exist and knowledge concerning complicated interactions between each parameter is far from adequate targeting the subsurface hydrocarbon contamination the objective of this study was to develop a new parameterization method to examine modeling uncertainties and improve simulation performance by using a coupled experimental and modeling approach the research tasks entail 1 to conduct a flow cell experiment to physically simulate subsurface hydrocarbon contamination and natural attenuation 2 to employ the biof t 3d model to numerically simulate the contamination and natural attenuation processes 3 to develop a new hybrid stochastic doe parameterization hsdp method for improving the modeling performance by quantifying the significance of modeling parameters and their interactions and evaluating the influence of uncertainties diesel fuel a frequently used liquid fuel has the advantage of easy access strong volatility short test period as well as time and cost saving thus in this study diesel fuel was selected as the appropriate napl contamination source with btex as the targeted compounds for lab analysis as well as numerical modeling 2 methodology 2 1 flow cell experimental setups in this study the physical model was built based on a pre manufactured flow cell which was used to prescribe the simulation domain focused on the longitudinal and vertical directions and provide data for parameterization and verification of the numerical model as shown in fig 1 the flow cell was fabricated with organic glass and was installed on aluminum framed mobile base 20 sampling ports with a space of 15 cm between each were placed on the front panel of the flow cell for aqueous sample collections dimensions of the flow cell are 82 5 cm 55 cm 4 cm water can be introduced from the inlet installed at the top left corner and the effluent can be discharged through the outlet installed with a globe valve at the bottom right corner of the flow cell li et al 2015 li et al 2017 prior to soil loading water proofing of the flow cell was carefully checked for leakages commercial white play sand shaw was purchased and screened with a 2 mm mesh size sieve the soil profile was considered homogenous with the total thickness of 48 cm after the completion of loading a variable speed peristaltic pump was employed to create a continuous water flow in order to maintain constant water levels at both end and to facilitate a continuous water flow in the simulated aquifer sampling port 5 as illustrated in fig 2 was kept open and connected to the drain the water levels were measured at 35 cm and 30 cm for upstream and downstream respectively the 10 ml aqueous samples were collected from specific sampling ports at a certain time interval and then immediately transferred into a 40 ml caped vail which was fulfilled with 30 ml distilled water to remove the headspaces the samples were then processed and the concentrations of btex were analyzed using the stratum ptc in series with the agilent 7890a gas chromatography gc which was equipped with the j w 122 5532 30 m 250 μm 0 25 μm column and the agilent 5975c mass spectrum ms helium was used as the carrying gas for both ptc and gc ms the purge flow was set at 40 m min for 11 min and desorb flow was set at 450 ml min for 4 min the oven temperature for gc was programed and maintained at 40 c for 14 min during the analyzing schedules the detection limits of the analyzing method were determined as 3 ppb for benzene toluene and ethylbenzene and as 5 ppb for xylene respectively 2 2 simulation of subsurface hydrocarbon contamination an existing three dimensional aquifer modeling tool named biof t 3d was employed for numerical simulation in this study the software was developed by scientific software group to model subsurface flow and contaminants transportation in three dimensions using finite element method biof t is able to model biodegradation flow and transport in the saturated and unsaturated zones in 2d or 3d in heterogeneous anisotropic porous media or fractured media biof t has the advantage and ability to simulate the processes including convection dispersion diffusion adsorption desorption and microbial processes ramadas et al 2013 li et al 2017 it has been mainly applied in real world petroleum sites contamination and natural attenuation practices but its application in academic research were dated and not common chen et al 2002 kumar 2002 mulligan and yong 2004 the software package includes a mesh editor to delineate the simulation boundary and domain a pre processor to specify input parameters and configure schedules and a post processor for output presentations biof t 3d applies the galerkin finite element method to approximate the governing equations in three dimensional spaces with the introduction of initial and boundary conditions the simulation domain is discretized into horizontal slices for individual sequential solutions to reduce the matrix size and the picard iterative approach is employed to generate the solutions among slices such that the duration of large domain simulations can be considerably shortened katyal and parker 1992 ak katyal 1997a b 2 3 a new hybrid stochastic design of experiment aided parameterization hsdp method the hsdp method was developed by integrating the doe aided parameterization method with stochastic parameter values using doe methods the optimized parameter set can be obtained to achieve the optimized simulation results in order to consider the uncertainty the small ranges for the optimized value of each parameter were assumed through monte carlo sampling the probabilistic uncertainty was considered and the potential improvement within the uncertainty range can be determined for the best simulation results generally the proposed hsdp method follows the sequence of 1 parameters ranking screening by ofat it is conducted by adjusting one factor at a time within a certain range while keeping the others unchanged thus to investigate if the numerical model is sensitive to the variation of individual parameters in this way the number of parameters involved in the doe model can be reduced by excluding the insignificant parameters which leads to fewer runs during parameterization 2 doe aided parameterization factorial design and rsm are two of the most widely applied doe methodologies factorial design is satisfactory in dealing with linear problems in which interacting effects between parameters are not significant without clear curvatures existing on the 3d response surface however factorial design is not adequate to generate nonlinear doe models in which parameter interactions cannot be neglected and the curvatures are significant in this case rsm should be applied to well fit the doe models the predicted regression equations from the doe models are then optimized to achieve the optimal responses and parameter combinations considering that it is not possible to identify if interactions exist in parameters ranking screening processes using ofat properly selecting doe methods is not guaranteed rsm can be directly applied providing that the number of included parameters is few another alternative is to first try factorial design which serves to further screen parameters for rsm at a later stage if the curvature is significant 3 monte carlo simulations monte carlo simulation is one of the common approaches to deal with stochastic uncertainty problems it is often realized by generating a large number of random data following a certain probability distribution as inputs of the models and further to identify their impacts on the variations of the models outputs traditional monte carlo simulation normally requires huge computational capacity however number of runs can be significantly reduced by performing it within narrowed ranges based on the optimized parameters from the doe aided parameterization processes the overall framework of applying the hsdp method is as illustrated by a flowchart in fig 3 the detailed steps are summarized as follows step 1 to build and preliminarily run the numerical model with all the input properties and boundary schedules specified the input information regarding parameters and boundary schedules should be based on the suggested values from the instructions of the model step 2 to conduct sensitivity analysis for the independent parameters for example using ofat to screen the insensitive parameters and rank the remaining ones based on their relative significance step 3 to determine the upper and lower bounds of the top ranked parameters in reasonable approaches for example from literature and actual experimental measurements step 4 to select and calculate the responses which should be the common criteria that can represent the goodness of fit between experimental observations and numerical simulations step 5 to analyze the relationships between responses and the corresponding parameter combinations using the doe method to collect the outputs such as anova parameter interactions and regression equations for predicted responses step 6 to optimize the doe predicted responses to apply non linear or linear optimization techniques depending on whether parameter interactions are significant or not and record the optimal combination of parameters which are then put back into the original numerical model to achieve actual responses step 7 to compare the actual responses to the optimized responses from doe predictions to continue to step 8 if they are sufficiently close otherwise to update the doe model by transformation or reselection of doe method step 8 to conduct monte carlo simulations and find the relationships between responses and key parameters within narrowed intervals centralized by the optimal parameters from step 6 therefore the number of runs does not have to be as much as traditional stochastic methods step 9 to find the optimal response and the corresponding parameter combinations from step 8 parameterization process ends hereby step 10 to verify the numerical model for potential predictions in addition to the advantages of revealing parameter interactions the proposed hsdp method can also reflect the effects from parameter uncertainties on the performance of numerical models moreover different from conventional methods dealing with stochastic parameters which rigidly apply monte carlo simulations at the beginning of the modeling the hsdp method introduces stochastic parameters at a relatively later stage following the optimization of the doe predicted responses on the one hand the reflections of effects of uncertain parameters would not be compromised by the significantly reduced number of runs which corresponds to the considerable saving of computational requirements on the other hand by introducing iterations of screened significant parameters within narrowed intervals an improvement of calibration results compared with simply using doe aided parameterization method can be expected 3 case study 3 1 data acquisition the volume of 45 ml diesel fuel was gradually injected 4 5 ml min to the same depth but 7 5 cm left to the sampling port 1 to simulate a napl leakage water flow was maintained at 12 ml min for parameterization three batches are chosen for three sampling time points throughout 120 h the 1st batch stood for the first 48 h after oil leaking and no obvious concentrations of btex can be detected in vertical dimension thus the collected data should put emphasis on the sample ports on the first horizontal layer but after 72 h the plume was moving vertically based on the effects of gravity and water flows thus for the 2nd and 3rd both the ports on the first and second layers were considered to ensure the collected data is comprehensive and sufficient to be specific the first batch of aqueous samples 48 h was collected from ports 1 2 3 and 4 ports indexes are indicated in fig 2 the second and third batches of aqueous samples 72 and 120 h were collected from ports 1 2 3 6 and 7 the observed data were given in table 1 for the numerical model the simulation domain reflecting the boundary and setups of flow cell was established by mesh editor flow boundary conditions were defined by assigning water and diesel volumetric flow rates at specific nodes while transport boundary conditions were set with btex relative abundances in the injected diesel were determined as 80 ppm 680 ppm 600 ppm and 3100 ppm for benzene toluene ethyl benzene and xylene respectively the total duration of simulation was set to 120 h with 24 h output intervals through traditional sensitivity analysis using ofat six parameters namely porosity first order decay coefficient dcay distribution coefficient akd henry s constant gama as well as diffusion coefficient in water difw and in air difa ranked top in significance as independent parameters and were represented by factors a to f respectively to be specific porosity indicates the ratio of the volume of voids to the total volume of the soil dcay represents the rate of mass loss in the first order decay degradation process akd shows the equilibrium constant for the distribution of an analyte in two immiscible solvents gama is the constant for henry s law diffusion coefficients act as the proportionality factor in fick s law for diffusion meanwhile lower and upper bounds of these parameters were reasonably determined based on measurements and or literature as given in table 2 the response for this design was root mean square deviation rmsd as given by equation 1 the coefficient of determination r2 which is a common statistical indicator to evaluate the goodness of fit for groundwater models was also calculated during the parameterization processes as given by equation 2 1 rmsd i 1 n y i y ˆ i 2 n 2 r 2 1 i 1 n y i y ˆ i 2 i 1 n y i y 2 in which y i is observed data y ˆ i is simulated data n is the number of observed simulated data y is the mean value of the observed data ideally a perfectly fit model would have rmsd 0 and r2 1 though it is not likely in practice daliakopoulos et al 2005 sun et al 2009 3 2 parameterization there are total 23 groups of simulations conducted by running each simulation model with different combinations of parameters a to f the sequence of simulations was randomly generated by using the minimum run resolution v factorial design with design expert 7 1 the analysis of variance anova results is summarized in table 3 as shown in fig 4 different parameters and their interactions stand out as significant factors for different contaminant species factor a soil porosity factor c distribution coefficient akd and factor d henry s constant gama were the most influential parameters for improving the goodness of fit for numerical simulations it is also important to look at factor b first order decay coefficient dcay for its interactions with other significant parameters factor e diffusion coefficient in water difw and factor f diffusion coefficient in air difa were proven to be insignificant positive or negative effects from individual factors were clearly identified and the interaction effects of two parameters were also presented as 3d surface graphs in fig 5 it also showed that the center point was not far from the doe predicted surface and minimum curvature was observed which suggest the interaction effects were not predominant and the selected factorial design is acceptable in predicting the responses these interactions between parameters cannot be identified by using ofat also minimum run resolution v factorial design uses less number of runs than full factorial design based on anova factor e and f can be excluded due to the insignificances at their corresponding ranges meanwhile the 3d surface graphs indicated that the combination of a lower henry s constant a higher porosity and a higher distribution coefficient would be in favor of improving the performance of the model the predicted regression equation for rmsd in terms of coded factors is given as follows 3 rmsd b 6 76 0 12 a 0 19 c 0 053 d 0 017 ac 4 rmsd t 41 04 1 45 a 0 75 b 4 72 c 1 54 d 0 19 ac 0 14 bc 5 rmsd e 47 74 1 19 a 0 58 b 5 02 c 8 63 d 0 66 cd 6 rmsd x 219 86 6 09 a 2 34 b 30 22 c 36 32 d 2 28 cd as objectives the rmsd predictions are then minimized by using nonlinear optimization lingo was applied in this process the optimized parameters were then put back to the simulation models to achieve actual responses and r2 value the comparisons of predicted and actual responses after optimization were summarized in table 4 as can be noticed the doe predicted responses reasonably match the ones achieved by using the suggested sets of parameters in the simulation model nevertheless it cannot be justified as the best calibration thus further tests involving stochastics parameters were conducted for two purposes firstly to reflect the impact from uncertain parameters on response distributions secondly to minimize the ignorance of parameter combinations for better responses stochastic parameters are introduced into the targeted numerical models by monte carlo simulations after the effective parameterization by doe the computational requirements can be dramatically reduced with fewer parameters and narrower ranges it was found that factors a b c and d were involved in the predicted regression equations however the coefficient of factor b was significantly lower relative to the other three factors and its value was low hence stochastic values were only introduced into factor a c and d in this study uniform distribution was applied to generate 60 groups of data for these three selected parameters their updated intervals were generated by centralizing the optimized parameters and expanding with 20 of their initial intervals which are still within reasonable ranges the total of 60 responses was generated and are shown in fig 6 these figures also represent the impacts from each individual parameter on the rmsd value for btex it can be noticed that only parts of the results akd on benzene toluene and xylene gama on ethylbenzene and xylene reflected that the trends of responses distributions followed the sensitivity analysis on individual parameters it was probably due to the limited number of runs within narrow intervals on the other hand it further manifested the efficiency of the proposed method in reducing computational requirements while dealing with uncertain parameters multiple points were found below the previously achieved rmsd values since the optimized parameters were all found at their upper or lower bound values it is common to find improved responses when half of the new intervals centered by these values were actually beyond the original boundaries only one combination of parameters within the initial intervals was found to be capable of generating a lower rmsd than the previously optimized one that is for ethylbenzene rmsd equals 29 57 ppb when porosity is 0 376 akd is 3 145 and gama is 0 268 all the other improved responses had at least partially involved parameters exceeding the initial ranges which further demonstrated the high accuracy of the predictions from the selected doe method the final calibration results are summarized in table 5 with the comparison of the results achieved without introducing stochastic parameters by monte carlo simulations 3 3 verification for verification of the simulation model another set of flow cell experiments were conducted the diesel injection location was moved right into the sampling port 1 and the quantity of diesel was changed from 45 ml to 35 ml aqueous samples were collected from sampling ports 2 3 4 and 6 at five time stages 12 h 36 h 60 h 72 h and 84 h after the initial diesel injection and analyzed the inputs for model verification are presented in table 6 simulation results at different time stages in the form of btex concentrations contours at the x z plane are shown in fig 7 the migrations of contaminates over time were found reasonably close to practice the comparisons of simulated and observed xylene concentrations from each sampling port were presented in fig 8 the results demonstrated an overall satisfactory level of fit with rmsd value at 5 ppb and r2 value at 0 76 for benzene rmsd value at 26 ppb and r2 value at 0 87 for toluene rmsd value at 31 ppb and r2 value at 0 82 for ethylbenzene and rmsd value at 126 ppb and r2 value at 0 90 for xylene some mismatches exist between the observed and simulated data at specific sampling ports and time stages the spatial and temporal variations of goodness of fit in this study could be caused by the inexact retardation effects and the uncertainties from the applied soil materials which were assumed homogenous but not possible in the practical loading of flow cell additionally the flow rate of water could not be consistently maintained during the experiments without accurate controlling countermeasures which led to the deviation from the defined boundary schedules in the simulations 4 conclusion in this research flow cell experiments were conducted to physically simulate btex contamination and natural attenuation processes in the subsurface a commercial groundwater modeling tool biof t 3d was applied to conduct numerical simulations a new hybrid stochastic doe aided parameterization method was developed to improve the modeling performance and implemented in a case study it was found that the developed hsdp method can efficiently identify key parameters and their interactions for the simulation models after optimizing the regression equations predicted by doe the obtained responses closely followed those achieved from simulations of the numerical models the impacts of individual parameters on the model s overall goodness of fit were reflected by conducting monte carlo simulations within the narrowed intervals based on the doe optimized parameters and the combination of parameters was further updated as better responses were found for verification a good level of fit between the simulated and observed data from flow cell experiments was presented the application of the hsdp method can also be potentially extended to different subsurface models in which parameter uncertainties and interactions need to be identified in a robust and efficient way more complicated spatial and temporal simulation domains such as variation of temperature different recharging schedules and heterogeneous soil profiles can be involved to investigate the generality of the hsdp method in future studies acknowledgements the research is supported by 1 natural sciences and engineering research council of canada nserc 2 canada foundation for innovation cfi and 3 research development corporation of newfoundland and labrador rdc nl 
26440,numerical models have been widely applied in simulating subsurface non aqueous phase liquid napl contamination processes in order to examine modeling uncertainties and improve simulation performance a new hybrid stochastic design of experiment doe aided parameterization method was developed by using a coupled experimental and modeling approach in a case study an existing commercial groundwater modeling tool biof t 3d was applied to conduct numerical simulations of subsurface contamination processes based on flow cell experiments parameterization results indicated that porosity distribution coefficient and henry s constant were the most significant parameters the result also revealed their interactions the doe predicted responses were found reasonably close to the actual ones from the models simulations monte carlo simulation was applied to conduct uncertainty analysis within the narrowed parameters ranges which were generated by centralizing the doe optimized values and the combinations of parameters were further updated when better responses were found after parameterization r2 valued 0 80 0 91 0 89 and 0 90 for benzene toluene ethylbenzene and xylene btex respectively a good consistency r2 0 76 to 0 90 for btex was also achieved during the model verification which confirmed that after the parameterization processes the simulation model can potentially be used for predictions under similar conditions keywords design of experiment stochastic parameterization subsurface napl contamination uncertainty 1 introduction subsurface non aqueous phase liquids napls contamination from spill and leakage of petroleum products has become a major environmental concern due to the long persistence time and complicated treatment required napls are difficult to be removed from water and soil leading to a high potential risk to human health in a long term timespan yang et al 2012 jácome and van geel 2015 napls leakage and contamination accidents are closely associated with human activities and it can also raise the risk to cause severe environmental and health hazards garcía junco et al 2001 in order to achieve a better understanding of the fate of contaminants in aquifer numerical simulation has been generally accepted as an effective tool and continuously studied kim and corapcioglu 2003 yu et al 2010 the knowledge can be further used to support decision making on monitoring and remediation practices as a simplified representative the numerical model is able to provide the simulated outputs for the modeled system the simulated outputs based on various model settings can reflect different scenarios through numerical simulations more resources energy and time can be saved leading a more efficient decision making process ideally numerical model should be able to reflect the realistic situations however there is often a lack of fit existing due to uncertainty in the modeling system especially the imprecisely defined parameter uncertainties uncertainty is inevitable at the current stage due to limited knowledge and simplification of numerical models in order to increase the reliability of the modeling results it is essential to conduct uncertainty and sensitivity analysis and properly calibrate the model such that discrepancies between simulated and observed data can be minimized parameter uncertainties have been extensively studied particularly integrated with sensitivity analysis and model calibrations sin et al 2011 he et al 2012 shen et al 2012 zhuo et al 2013 houska et al 2014 to consider the parameter uncertainties the optimal results from the different combinations of the parameter settings were analyzed monte carlo simulation is one of the most common stochastic methods involving random sampling within certain types of distributions due to the ease of implementation and generalization monte carlo methods have been widely applied to environmental systems by propagating the parameter uncertainties and evaluating their impacts on the model output huang and loucks 2000 jing et al 2013 li et al 2014 one factor at a time ofat is one of the traditional sensitivity analysis methods this method simply adjusts one parameter at a time while keeping other parameters fixed its applications have been found in multiple studies conducted on various models holvoet et al 2005 jing and chen 2011 garson 1991 introduced another method based on the concept of artificial neural networks ann by assigning the neural net weight matrix this method has been further implemented in many other studies to find the relative importance of input variables in different processes modeling kermani and ebadi 2012 nourani and fard 2012 jing et al 2014 however both of these methods have their limitations ofat is incapable of revealing the interactions between parameters which might lead to the ignorance of the potentially significant variables peeters et al 2014 garson s method is debatable due to the black box nature of ann olden et al 2004 witek krowiak et al 2014 the traditional method of model calibration is essentially a trial and error process which uses iterations to adjust the relevant parameters until the simulated outputs are sufficiently close to the experimental data this method is still popular and has been embedded in commercial modeling tools for automatic calibration sonnenborg et al 2003 mugunthan et al 2005 razavi and tolson 2012 existing studies have focused on optimizing the mathematical algorithms to achieve a more efficient calibration process wu et al 2012 2014 zhao et al 2013 calibration methods have also been improved by applying parameter estimation and global optimization within reasonable predefined intervals kang 2014 plasencia et al 2014 yen et al 2014 zhang et al 2015 despite that a good fit can be expected it should not be ignored that some major limitations such as extensive computational requirements low physical plausibility and over parameterization exist when traditional calibration methods are employed van griensven et al 2006 whittaker et al 2010 okamoto and akella 2012 to address this issue design of experiment doe provides a parameterization option doe is a well known statistical methodology which can unveil the interrelationships between parameters and the corresponding responses by conducting controlled experiments park 2007 strigul et al 2009 by using doe it is possible to simultaneously study several parameters and their interactions veličković et al 2013 sarikaya and güllü 2015 doe was originally developed to guide the planning and setup for physical experiments however considering that the complexity and cost can increase dramatically with the growing number of input variables numerical simulation tools have been extensively involved especially in recent studies wu et al 2012 used a doe aided method to conduct sensitivity analysis and parameterization for a hydrological model slurp and optimized the predicted regression equation which has resulted in a greater goodness of fit value compared to the one achieved by the automatic calibration function within the model zahraee et al 2013 introduced doe in modeling a real world construction process to achieve optimal resource levels and maximize the process productivity in the study of al shalabi et al 2014 seven uncertain design parameters for a low salinity water injection process were screened by using doe method followed by the optimization of cumulative oil recovery using the response surface methodology rsm though doe aided methods have proven advantages in conducting parameterization for numerical models relationships between responses and stochastically distributed parameters are seldom integrated besides it has rarely been used in groundwater and subsurface contamination models in which uncertainties commonly exist and knowledge concerning complicated interactions between each parameter is far from adequate targeting the subsurface hydrocarbon contamination the objective of this study was to develop a new parameterization method to examine modeling uncertainties and improve simulation performance by using a coupled experimental and modeling approach the research tasks entail 1 to conduct a flow cell experiment to physically simulate subsurface hydrocarbon contamination and natural attenuation 2 to employ the biof t 3d model to numerically simulate the contamination and natural attenuation processes 3 to develop a new hybrid stochastic doe parameterization hsdp method for improving the modeling performance by quantifying the significance of modeling parameters and their interactions and evaluating the influence of uncertainties diesel fuel a frequently used liquid fuel has the advantage of easy access strong volatility short test period as well as time and cost saving thus in this study diesel fuel was selected as the appropriate napl contamination source with btex as the targeted compounds for lab analysis as well as numerical modeling 2 methodology 2 1 flow cell experimental setups in this study the physical model was built based on a pre manufactured flow cell which was used to prescribe the simulation domain focused on the longitudinal and vertical directions and provide data for parameterization and verification of the numerical model as shown in fig 1 the flow cell was fabricated with organic glass and was installed on aluminum framed mobile base 20 sampling ports with a space of 15 cm between each were placed on the front panel of the flow cell for aqueous sample collections dimensions of the flow cell are 82 5 cm 55 cm 4 cm water can be introduced from the inlet installed at the top left corner and the effluent can be discharged through the outlet installed with a globe valve at the bottom right corner of the flow cell li et al 2015 li et al 2017 prior to soil loading water proofing of the flow cell was carefully checked for leakages commercial white play sand shaw was purchased and screened with a 2 mm mesh size sieve the soil profile was considered homogenous with the total thickness of 48 cm after the completion of loading a variable speed peristaltic pump was employed to create a continuous water flow in order to maintain constant water levels at both end and to facilitate a continuous water flow in the simulated aquifer sampling port 5 as illustrated in fig 2 was kept open and connected to the drain the water levels were measured at 35 cm and 30 cm for upstream and downstream respectively the 10 ml aqueous samples were collected from specific sampling ports at a certain time interval and then immediately transferred into a 40 ml caped vail which was fulfilled with 30 ml distilled water to remove the headspaces the samples were then processed and the concentrations of btex were analyzed using the stratum ptc in series with the agilent 7890a gas chromatography gc which was equipped with the j w 122 5532 30 m 250 μm 0 25 μm column and the agilent 5975c mass spectrum ms helium was used as the carrying gas for both ptc and gc ms the purge flow was set at 40 m min for 11 min and desorb flow was set at 450 ml min for 4 min the oven temperature for gc was programed and maintained at 40 c for 14 min during the analyzing schedules the detection limits of the analyzing method were determined as 3 ppb for benzene toluene and ethylbenzene and as 5 ppb for xylene respectively 2 2 simulation of subsurface hydrocarbon contamination an existing three dimensional aquifer modeling tool named biof t 3d was employed for numerical simulation in this study the software was developed by scientific software group to model subsurface flow and contaminants transportation in three dimensions using finite element method biof t is able to model biodegradation flow and transport in the saturated and unsaturated zones in 2d or 3d in heterogeneous anisotropic porous media or fractured media biof t has the advantage and ability to simulate the processes including convection dispersion diffusion adsorption desorption and microbial processes ramadas et al 2013 li et al 2017 it has been mainly applied in real world petroleum sites contamination and natural attenuation practices but its application in academic research were dated and not common chen et al 2002 kumar 2002 mulligan and yong 2004 the software package includes a mesh editor to delineate the simulation boundary and domain a pre processor to specify input parameters and configure schedules and a post processor for output presentations biof t 3d applies the galerkin finite element method to approximate the governing equations in three dimensional spaces with the introduction of initial and boundary conditions the simulation domain is discretized into horizontal slices for individual sequential solutions to reduce the matrix size and the picard iterative approach is employed to generate the solutions among slices such that the duration of large domain simulations can be considerably shortened katyal and parker 1992 ak katyal 1997a b 2 3 a new hybrid stochastic design of experiment aided parameterization hsdp method the hsdp method was developed by integrating the doe aided parameterization method with stochastic parameter values using doe methods the optimized parameter set can be obtained to achieve the optimized simulation results in order to consider the uncertainty the small ranges for the optimized value of each parameter were assumed through monte carlo sampling the probabilistic uncertainty was considered and the potential improvement within the uncertainty range can be determined for the best simulation results generally the proposed hsdp method follows the sequence of 1 parameters ranking screening by ofat it is conducted by adjusting one factor at a time within a certain range while keeping the others unchanged thus to investigate if the numerical model is sensitive to the variation of individual parameters in this way the number of parameters involved in the doe model can be reduced by excluding the insignificant parameters which leads to fewer runs during parameterization 2 doe aided parameterization factorial design and rsm are two of the most widely applied doe methodologies factorial design is satisfactory in dealing with linear problems in which interacting effects between parameters are not significant without clear curvatures existing on the 3d response surface however factorial design is not adequate to generate nonlinear doe models in which parameter interactions cannot be neglected and the curvatures are significant in this case rsm should be applied to well fit the doe models the predicted regression equations from the doe models are then optimized to achieve the optimal responses and parameter combinations considering that it is not possible to identify if interactions exist in parameters ranking screening processes using ofat properly selecting doe methods is not guaranteed rsm can be directly applied providing that the number of included parameters is few another alternative is to first try factorial design which serves to further screen parameters for rsm at a later stage if the curvature is significant 3 monte carlo simulations monte carlo simulation is one of the common approaches to deal with stochastic uncertainty problems it is often realized by generating a large number of random data following a certain probability distribution as inputs of the models and further to identify their impacts on the variations of the models outputs traditional monte carlo simulation normally requires huge computational capacity however number of runs can be significantly reduced by performing it within narrowed ranges based on the optimized parameters from the doe aided parameterization processes the overall framework of applying the hsdp method is as illustrated by a flowchart in fig 3 the detailed steps are summarized as follows step 1 to build and preliminarily run the numerical model with all the input properties and boundary schedules specified the input information regarding parameters and boundary schedules should be based on the suggested values from the instructions of the model step 2 to conduct sensitivity analysis for the independent parameters for example using ofat to screen the insensitive parameters and rank the remaining ones based on their relative significance step 3 to determine the upper and lower bounds of the top ranked parameters in reasonable approaches for example from literature and actual experimental measurements step 4 to select and calculate the responses which should be the common criteria that can represent the goodness of fit between experimental observations and numerical simulations step 5 to analyze the relationships between responses and the corresponding parameter combinations using the doe method to collect the outputs such as anova parameter interactions and regression equations for predicted responses step 6 to optimize the doe predicted responses to apply non linear or linear optimization techniques depending on whether parameter interactions are significant or not and record the optimal combination of parameters which are then put back into the original numerical model to achieve actual responses step 7 to compare the actual responses to the optimized responses from doe predictions to continue to step 8 if they are sufficiently close otherwise to update the doe model by transformation or reselection of doe method step 8 to conduct monte carlo simulations and find the relationships between responses and key parameters within narrowed intervals centralized by the optimal parameters from step 6 therefore the number of runs does not have to be as much as traditional stochastic methods step 9 to find the optimal response and the corresponding parameter combinations from step 8 parameterization process ends hereby step 10 to verify the numerical model for potential predictions in addition to the advantages of revealing parameter interactions the proposed hsdp method can also reflect the effects from parameter uncertainties on the performance of numerical models moreover different from conventional methods dealing with stochastic parameters which rigidly apply monte carlo simulations at the beginning of the modeling the hsdp method introduces stochastic parameters at a relatively later stage following the optimization of the doe predicted responses on the one hand the reflections of effects of uncertain parameters would not be compromised by the significantly reduced number of runs which corresponds to the considerable saving of computational requirements on the other hand by introducing iterations of screened significant parameters within narrowed intervals an improvement of calibration results compared with simply using doe aided parameterization method can be expected 3 case study 3 1 data acquisition the volume of 45 ml diesel fuel was gradually injected 4 5 ml min to the same depth but 7 5 cm left to the sampling port 1 to simulate a napl leakage water flow was maintained at 12 ml min for parameterization three batches are chosen for three sampling time points throughout 120 h the 1st batch stood for the first 48 h after oil leaking and no obvious concentrations of btex can be detected in vertical dimension thus the collected data should put emphasis on the sample ports on the first horizontal layer but after 72 h the plume was moving vertically based on the effects of gravity and water flows thus for the 2nd and 3rd both the ports on the first and second layers were considered to ensure the collected data is comprehensive and sufficient to be specific the first batch of aqueous samples 48 h was collected from ports 1 2 3 and 4 ports indexes are indicated in fig 2 the second and third batches of aqueous samples 72 and 120 h were collected from ports 1 2 3 6 and 7 the observed data were given in table 1 for the numerical model the simulation domain reflecting the boundary and setups of flow cell was established by mesh editor flow boundary conditions were defined by assigning water and diesel volumetric flow rates at specific nodes while transport boundary conditions were set with btex relative abundances in the injected diesel were determined as 80 ppm 680 ppm 600 ppm and 3100 ppm for benzene toluene ethyl benzene and xylene respectively the total duration of simulation was set to 120 h with 24 h output intervals through traditional sensitivity analysis using ofat six parameters namely porosity first order decay coefficient dcay distribution coefficient akd henry s constant gama as well as diffusion coefficient in water difw and in air difa ranked top in significance as independent parameters and were represented by factors a to f respectively to be specific porosity indicates the ratio of the volume of voids to the total volume of the soil dcay represents the rate of mass loss in the first order decay degradation process akd shows the equilibrium constant for the distribution of an analyte in two immiscible solvents gama is the constant for henry s law diffusion coefficients act as the proportionality factor in fick s law for diffusion meanwhile lower and upper bounds of these parameters were reasonably determined based on measurements and or literature as given in table 2 the response for this design was root mean square deviation rmsd as given by equation 1 the coefficient of determination r2 which is a common statistical indicator to evaluate the goodness of fit for groundwater models was also calculated during the parameterization processes as given by equation 2 1 rmsd i 1 n y i y ˆ i 2 n 2 r 2 1 i 1 n y i y ˆ i 2 i 1 n y i y 2 in which y i is observed data y ˆ i is simulated data n is the number of observed simulated data y is the mean value of the observed data ideally a perfectly fit model would have rmsd 0 and r2 1 though it is not likely in practice daliakopoulos et al 2005 sun et al 2009 3 2 parameterization there are total 23 groups of simulations conducted by running each simulation model with different combinations of parameters a to f the sequence of simulations was randomly generated by using the minimum run resolution v factorial design with design expert 7 1 the analysis of variance anova results is summarized in table 3 as shown in fig 4 different parameters and their interactions stand out as significant factors for different contaminant species factor a soil porosity factor c distribution coefficient akd and factor d henry s constant gama were the most influential parameters for improving the goodness of fit for numerical simulations it is also important to look at factor b first order decay coefficient dcay for its interactions with other significant parameters factor e diffusion coefficient in water difw and factor f diffusion coefficient in air difa were proven to be insignificant positive or negative effects from individual factors were clearly identified and the interaction effects of two parameters were also presented as 3d surface graphs in fig 5 it also showed that the center point was not far from the doe predicted surface and minimum curvature was observed which suggest the interaction effects were not predominant and the selected factorial design is acceptable in predicting the responses these interactions between parameters cannot be identified by using ofat also minimum run resolution v factorial design uses less number of runs than full factorial design based on anova factor e and f can be excluded due to the insignificances at their corresponding ranges meanwhile the 3d surface graphs indicated that the combination of a lower henry s constant a higher porosity and a higher distribution coefficient would be in favor of improving the performance of the model the predicted regression equation for rmsd in terms of coded factors is given as follows 3 rmsd b 6 76 0 12 a 0 19 c 0 053 d 0 017 ac 4 rmsd t 41 04 1 45 a 0 75 b 4 72 c 1 54 d 0 19 ac 0 14 bc 5 rmsd e 47 74 1 19 a 0 58 b 5 02 c 8 63 d 0 66 cd 6 rmsd x 219 86 6 09 a 2 34 b 30 22 c 36 32 d 2 28 cd as objectives the rmsd predictions are then minimized by using nonlinear optimization lingo was applied in this process the optimized parameters were then put back to the simulation models to achieve actual responses and r2 value the comparisons of predicted and actual responses after optimization were summarized in table 4 as can be noticed the doe predicted responses reasonably match the ones achieved by using the suggested sets of parameters in the simulation model nevertheless it cannot be justified as the best calibration thus further tests involving stochastics parameters were conducted for two purposes firstly to reflect the impact from uncertain parameters on response distributions secondly to minimize the ignorance of parameter combinations for better responses stochastic parameters are introduced into the targeted numerical models by monte carlo simulations after the effective parameterization by doe the computational requirements can be dramatically reduced with fewer parameters and narrower ranges it was found that factors a b c and d were involved in the predicted regression equations however the coefficient of factor b was significantly lower relative to the other three factors and its value was low hence stochastic values were only introduced into factor a c and d in this study uniform distribution was applied to generate 60 groups of data for these three selected parameters their updated intervals were generated by centralizing the optimized parameters and expanding with 20 of their initial intervals which are still within reasonable ranges the total of 60 responses was generated and are shown in fig 6 these figures also represent the impacts from each individual parameter on the rmsd value for btex it can be noticed that only parts of the results akd on benzene toluene and xylene gama on ethylbenzene and xylene reflected that the trends of responses distributions followed the sensitivity analysis on individual parameters it was probably due to the limited number of runs within narrow intervals on the other hand it further manifested the efficiency of the proposed method in reducing computational requirements while dealing with uncertain parameters multiple points were found below the previously achieved rmsd values since the optimized parameters were all found at their upper or lower bound values it is common to find improved responses when half of the new intervals centered by these values were actually beyond the original boundaries only one combination of parameters within the initial intervals was found to be capable of generating a lower rmsd than the previously optimized one that is for ethylbenzene rmsd equals 29 57 ppb when porosity is 0 376 akd is 3 145 and gama is 0 268 all the other improved responses had at least partially involved parameters exceeding the initial ranges which further demonstrated the high accuracy of the predictions from the selected doe method the final calibration results are summarized in table 5 with the comparison of the results achieved without introducing stochastic parameters by monte carlo simulations 3 3 verification for verification of the simulation model another set of flow cell experiments were conducted the diesel injection location was moved right into the sampling port 1 and the quantity of diesel was changed from 45 ml to 35 ml aqueous samples were collected from sampling ports 2 3 4 and 6 at five time stages 12 h 36 h 60 h 72 h and 84 h after the initial diesel injection and analyzed the inputs for model verification are presented in table 6 simulation results at different time stages in the form of btex concentrations contours at the x z plane are shown in fig 7 the migrations of contaminates over time were found reasonably close to practice the comparisons of simulated and observed xylene concentrations from each sampling port were presented in fig 8 the results demonstrated an overall satisfactory level of fit with rmsd value at 5 ppb and r2 value at 0 76 for benzene rmsd value at 26 ppb and r2 value at 0 87 for toluene rmsd value at 31 ppb and r2 value at 0 82 for ethylbenzene and rmsd value at 126 ppb and r2 value at 0 90 for xylene some mismatches exist between the observed and simulated data at specific sampling ports and time stages the spatial and temporal variations of goodness of fit in this study could be caused by the inexact retardation effects and the uncertainties from the applied soil materials which were assumed homogenous but not possible in the practical loading of flow cell additionally the flow rate of water could not be consistently maintained during the experiments without accurate controlling countermeasures which led to the deviation from the defined boundary schedules in the simulations 4 conclusion in this research flow cell experiments were conducted to physically simulate btex contamination and natural attenuation processes in the subsurface a commercial groundwater modeling tool biof t 3d was applied to conduct numerical simulations a new hybrid stochastic doe aided parameterization method was developed to improve the modeling performance and implemented in a case study it was found that the developed hsdp method can efficiently identify key parameters and their interactions for the simulation models after optimizing the regression equations predicted by doe the obtained responses closely followed those achieved from simulations of the numerical models the impacts of individual parameters on the model s overall goodness of fit were reflected by conducting monte carlo simulations within the narrowed intervals based on the doe optimized parameters and the combination of parameters was further updated as better responses were found for verification a good level of fit between the simulated and observed data from flow cell experiments was presented the application of the hsdp method can also be potentially extended to different subsurface models in which parameter uncertainties and interactions need to be identified in a robust and efficient way more complicated spatial and temporal simulation domains such as variation of temperature different recharging schedules and heterogeneous soil profiles can be involved to investigate the generality of the hsdp method in future studies acknowledgements the research is supported by 1 natural sciences and engineering research council of canada nserc 2 canada foundation for innovation cfi and 3 research development corporation of newfoundland and labrador rdc nl 
26441,wildfire susceptibility is a measure of land propensity for the occurrence of wildfires based on terrain s intrinsic characteristics in the present study two stochastic approaches i e extreme learning machine and random forest for wildfire susceptibility mapping are compared versus a well established deterministic method the same predisposing variables were combined and used as predictors in all models the portuguese region of dão lafões was selected as a pilot site since it presents national average values of fire incidence and a high heterogeneity in land cover and slope maps representing the susceptibility of the study area to wildfires were finally elaborated two measures were used to compare the different methods namely the location of the pixels with similar standardized susceptibility and total validation burnt area results obtained with the stochastic methods are very alike with the deterministic ones with the advantage of not depending on a priori knowledge of the phenomenon graphical abstract image 1 keywords susceptibility mapping wildfires random forest extreme learning machines portugal 1 introduction wildfires are defined as unwanted fires occurring in countryside or rural area and burning forest and wild lands included abandoned agricultural lands and rural vegetated areas wildfires as undesirable as often uncontrolled events represent a hazardous and harmful phenomena to people and environment natural fires caused by lightning appeared on the earth surface in concomitance with the first plant communities well before the appearance of humans and played a key role in plant adaptation and the ecosystems equilibrium pausas and keeley 2009 nowadays the primary cause of wildfires in populated areas is related to the human activities that voluntary arsonism or involuntary accidental or negligent causes can initiate fire a recent analysis of fire data from the european forest fire information system shows that over 95 of wildfires are human induced san miguel ayanz et al 2012 and this percentage is even higher in the mediterranean regions estimating the probability of wildfire occurrence in a certain area under particular environmental and anthropogenic conditions is a modern tool to support forest protection plans and to reduce fires consequences which can also affect the neighbouring or intermingled urban areas in this context the implementation of wildfire susceptibility maps and the investigation of the main driving factors inducing wildfires is fundamental a good review of these factors can be found in ganteaume et al 2013 they included human factors and related variables such as distance to road or to urban area as well as environmental factors more or less sophisticated models have been applied to combine the predisposing variables into a geographic information systems gis chuvieco et al 2010 chuvieco and salas 1996 bonazountas et al 2005 jaiswal et al 2002 the most reliable analyses applied statistical models to assess the importance of different variables influencing fire occurrences and the obtained results are used to produce the risk maps beverly et al 2009 soto et al 2013 pourtaghi et al 2015 recent analyses compared different statistical models for variable selection pourghasemi 2016 pourghasemi et al 2016 pourtaghi et al 2016 rodrigues et al 2014 eugenio et al 2016 but most of the studies relied on expert knowledge to pre select most important drivers or on the results of linear deterministic statistical models portugal is unequivocally the european country most affected by wildfires due to its favorable climatic conditions topography and vegetation amraoui et al 2015 pereira et al 2013 investigations of driven factors and the elaborations of wildfires density and risk maps were latterly performed for this highly affected country tonini et al 2017 analysed the spatio temporal density distribution of these hazardous events in the last decades and produced a 3d graphical output of the results which highlights areas and frame periods more affected by wildfires nunes et al 2016 used geographically weighted regression to identify relevant municipal drivers of fires it results that topography and population density were significant factors in municipal ignitions while topography and uncultivated land were significant factors in municipal burnt area ba verde and zêzere 2010 assessed forest fire susceptibility testing and using variables of strong spatial correlation i e elevation slope land cover rainfall and temperature and more recently parente and pereira 2016 adopted this method updating the selected variables to map the structural fire risk in the vegetated area of portugal in the present study the authors refer to the wildfire susceptibility mapping as an estimation of the probability that fire occurs in a specific area without considering a temporal scale assessed on the basis of predisposing factors related to terrain s intrinsic characteristics the revised literature misses the use of stochastic models to elaborate accurate susceptibility maps of wildfires which can be compared with the results obtained by applying deterministic approaches these latter methods usually assume a priori knowledge of predisposing factors or they are evaluated by applying linear methods which implies that every set of variable states is uniquely determined by the parameters used in the model and by the sets of previous states therefore a deterministic model always performs the same way for a given set of initial conditions contrary to the deterministic approach the stochastic methods assume that results obtained by the combination of independent factors i e variables affecting the investigated phenomenon can be slightly different due to the randomness of the process this aspect is particularly useful to model environmental and anthropogenic hazard which naturally present a complex behaviours and patterns therefore the objective of the present study is to compare stochastic approaches vs a well established deterministic method for wildfire susceptibility mapping a first assessment of the susceptibility and hazard wildfire performed for portugal verde and zêzere 2010 parente and pereira 2016 is used as benchmarking while extreme learning machine elm and random forest rf are the two applied stochastic methods we restricted our investigation to a pilot area namely the region of dão lafões characterized by a high variability and heterogeneity of environmental features and fire incidence similar to the national average which makes it a good representative of the general characteristics of continental portugal 2 material study area and datasets 2 1 study area portugal is the european country more to the southwest with a mediterranean type of climate but suffering of the influence of the atlantic ocean that bathes its western and southern coasts parente et al 2016 mainland portugal has a total land area of about 90 000 k m 2 which according to the corine land cover clc 2006 inventory is predominantly used for agriculture 47 followed by forests coverings 23 scrub and or herbaceous vegetation associations 23 and open spaces with little or no vegetation 2 pereira et al 2014 according to the planos regionais de ordenamento florestal prof continental portugal is divided into 21 prof regions fig 1 the prof establish specific rules for the use and exploitation of its forest spaces in order to ensure sustainable production of all goods and services associated with them icnf 2016 in the present study dão lafões region was selected as the case study area for the following reasons i it is located in the northern half of portugal which presents by far the highest wildfire incidence parente and pereira 2016 ii this region presents an annual average number of fires and ba very similar to the national average and iii its area is very heterogeneous in terms of topography land use and vegetation cover fig 2 2 2 the datasets raw data used in this study include i digital elevation model dem derived from the shuttle radar topographic mission with a resolution of 1 arc second dem srtm 25 m used to compute elevation and slope gonçalves and morgado 2008 ii clc 2006 inventory produced by the european environment agency which provides the land use and land cover maps and iii the national mapping burnt areas nmba implemented by the institute for the conservation of nature and forests icnf 2016 which provides a detailed description of the shape and the size of the area burnt by fires in each year of occurrence the data pre and post processing as well as the mapping elaboration were performed by quantum gis free software qgis development team 2016 2 2 1 topography topography characterized by the altitude slope and exposure constitutes one of the most important factors to define the type of the climate of a region such as the average weather conditions and the space time variability of the climatic elements e g air temperature precipitation solar radiation these factors control the life cycle of the vegetation cover and land use and have a profound influence on the fire incidence chuvieco and congalton 1989 freire et al 2002 verde and zêzere 2010 parente and pereira 2016 parente et al 2016 in this study we considered the slope as the main topographic variable influencing the susceptibility to wildfires in the study area this value was derived from the dem and categorized in the same 6 classes used by verde and zêzere 2010 namely 0 2 2 5 5 10 10 15 15 20 and 20 2 2 2 land use and vegetation cover the clc consists of an inventory of land cover in 44 classes with a minimum map unit of 25 ha for areal phenomena the main classes are artificial surfaces agricultural forest and semi natural areas wetlands and water bodies büttner 2014 caetano et al 2009 the 2006 version of clc was used in the present study fig 2 since this date is in the middle of the investigated period 2000 2013 in investigated region dão lafões the different classes of land cover and land use are quite homogeneously distributed within the area however it is possible to identify some patterns higher concentration of forest cover may be found in the southwest and middle class slopes agricultural areas are mostly located in the southeast away from the highest slopes while scrubs are predominant in the southeast and northwest borders as well as in high slopes 2 2 3 the fire dataset the nmba is an official portuguese fire dataset based on satellite imagery acquired once per year at the end of the fire season and delivered in vector format as polygons of the ba allowing a detailed description of the location size and shape of the fire scars which is fundamental for the present study this dataset was recently reviewed to correct a minor number of missing values and data inconsistencies it contains 17 903 fire events between 2000 and 2013 where 1 114 of which occurred on dão lafões parente et al 2016 in this region most of the fire incidences are located far in the north and in the southeast fig 2 affecting mostly agricultural areas 10 scrublands 62 and open spaces 13 as well as areas with slopes ranging from 5 to 10 32 and 10 to 15 23 the location and size of the ba for the investigated period 2000 2013 is represented in fig 3 in the form of fire frequency ff which is the number of times each pixel burnt over the fourteen years the year with the highest fire incidence was 2005 with 11 of the total number of fires and 29 of the total ba in the study period followed by 2012 11 of the total number of fires and 2013 8 of the total number of fires and 20 of the total ba only 18 of total number of pixels burnt at least once and the fire frequency is mostly low or very low figs 2 and 3 with 97 of the total number of burnt pixels tnbp with f f 3 14 namely 72 of tnbp with f f 1 14 20 of tnbp with f f 2 14 and 5 of tnbp with f f 3 14 3 methodology both deterministic and stochastic models for wildfire susceptibility mapping were applied in the present study the deterministic model used as benchmark was developed by verde and zêzere 2010 and further adopted and updated by parente and pereira 2016 the model includes the computation of fire occurrence probability and favorability scores for each predisposing variable land cover and slope two stochastic methods from the machine learning field were then applied rf and elm generally speaking stochastic models account for the uncertainty in modelling processes that have some kind of randomness and therefore are useful to represent phenomena with random variability in the case of machine learning algorithms the models produce susceptibility maps based on input data variables without the need of a priori knowledge of the investigated phenomena but simply learning from experience once the model is fitted according to the training data it allows to generate predictions over the entire study area in the present study data were splitted into training 2000 2009 and validation periods 2010 2013 the first was used to fit and calibrate the three models and the second to assess and compare susceptibility maps the susceptibility maps were elaborated by means of gis procedures and organized into 5 classes in agreement with the portuguese law dl 2006 the classes were defined as in the reference works verde and zêzere 2010 parente and pereira 2016 using the quintiles of the susceptibility computed as explained below the applied methods were assessed by computing the matching pixel by pixel between the standardized susceptibility maps obtained for the training period 2000 2009 and the effective ba over the validation period 2010 2013 these values were finally evaluated as a percentage for each susceptibility class the next two subsections are devoted to the brief description of the deterministic and stochastic applied models 3 1 deterministic method in portugal national authorities such as forest service icnf and the meteorological office instituto português do mar e da atmosfera ipma adopted the wildfire susceptibility map proposed by verde and zêzere 2010 which was developed using a deterministic approach and based on just three factors the susceptible values for each regular unit area i e pixel is computed by integrating the favorability scores f a v of the two variables slope and vegetation cover and the fire probability fp as 1 s p f p f a v slope f a v vegetation the favorability scores for each class x f a v x of slope and vegetation cover are computed by 2 f a v x n b p x t n p x 100 where n b p x is the number of burnt pixels in class x and t n p x is the total number of pixels in the class x the fire probability of each pixel is estimated using the fire database and the classic definition of probability according to 3 f p the number of times the pixel burned in the study period in years duration of the study in years 100 it is important to note that due to the yearly temporal acquisition of the fire database nmba each pixel can only burn once in each year in addition due to the multiplicative nature of susceptibility equation all the null favorability scores were reclassified to one thus becoming neutral values in the equation therefore the obtained value in each pixel is a consequence of all the possible combinations of the variables found in that pixel 3 2 machine learning algorithms at present machine learning algorithms are important tools for the analysis modelling and visualization of environmental data kanevski et al 2009 they have good generalization abilities when modelling high dimensional and complex nonlinear phenomena are universal modelling methods and many of them have solid roots in statistical learning theory hastie et al 2009 in predictive learning they focus on modelling the hidden relationship between a set of input and output variables by trying to minimize both the errors and the complexity of the model after a training procedure to calibrate the model s parameters prediction maps of the susceptibility can be computed and displayed with the corresponding uncertainty quantification in this study two machine learning algorithms based on two different concepts are used for comparison purposes rf which is based on decision trees and elm which is based on traditional artificial neural networks detailed application of the rf and elm for environmental data modelling along with the description of consistent methodology are presented in literature micheletti et al 2014 leuenberger and kanevski 2015 analysis were performed using r free software r core team 2016 the packages randomforest and elmnn were employed for rf and elm respectively 3 2 1 random forest developed by breiman 2001 rf is an ensemble machine learning algorithm based on decision trees it contains two hyper parameters the number of decision trees generated nbtree and the number of selected variables for each split node nbvar the random forest algorithm first generates nbtree subsets of the training dataset by bootstrapping i e random sampling with replacement then for each subset it will grow a decision tree by iterating the following rules up to the maximum level when each final node contains less than 5 data points 1 for each split the algorithm selects randomly nbvar variables 2 according to these nbvar variables and the output variable it computes the gini index hastie et al 2009 and selects the best variable with the best threshold in order to minimize the error of the prediction once the nbtree decision trees have been grown prediction of new data points is performed by taking the average value of all decision trees fig 4 4 y pred 1 n b t r e e i 1 n b t r e e y i this procedure leads to a robust mean value of prediction as well as a measure of uncertainty by considering the standard deviation among all trees 3 2 2 extreme learning machine elm is based on artificial neural network concept following the structure of a single hidden layer feedforward neural network slfn it connects all input variables to the hidden layer computes the neurone value and averages all neurons with optimal weights to the output layer huang et al 2006 leuenberger and kanevski 2015 more formally composed of nbnode neurons n and by using an activation function g ℝ ℝ the elm network connecting the inputs x i to the output y i value can be written in the following form 5 j 1 n β j g x i w j b j y i where x i w j is an inner product between the input x i and the weight vector w j which connects the input layer to the jth neuron b j is the bias of the jth neuron and β is a weight vector connecting the hidden layer to the output layer in a more compact way elm can be written as 6 h β y where h i j u j j 1 n g x i w j b j is the output matrix of the hidden layer fig 5 according to this notation elm algorithm applies the following steps 1 randomly generates the input weight w j and the bias b j 2 computes the matrix h 3 computes the output weight β h y where h is the moore penrose generalized inverse of matrix h at the end the only fitted parameter is the number of hidden neuron n n b n o d e 3 2 3 parameter optimization in order to optimize the learning process of rf and elm different pre processing steps must be considered first of all the clc classes were converted into 27 dummy variables one for each class of the clc variable within the study area then the complete dataset which is composed of 28 input variables slope clc variables and 1 output variable presence or absence of forest fire was normalized into the 0 1 interval this transformation was performed in order to fit the functional range where elm works in an optimal way huang et al 2006 after that from the 5 581 522 raster cells covering the study area 100 000 points approximately 1 8 of the total area were randomly selected using a stratified sampling for the construction of the testing subset table 1 namely 6 strata were used by considering the number of time each pixel burnt between 0 and 5 times in this case this process was reiterated 20 times in order to generate 20 training subsets but without considering already selected testing points the optimization of the nbtree and nbvar hyper parameters of rf was performed by using a trial and error process the choice of this method is justified by the fact that both types of rf hyper parameters are not highly sensitive to changes and optimized values are close to the default ones in this study hyper parameters of rf were set to 1000 and 9 for nbtree and nbvar respectively for elm the nbnode hyper parameter which is the number of nodes in the hidden layer a 5 fold cross validation approach was performed minimum mean squared error mse values obtained for the validation sets were retained which lead to an optimal number of 40 nodes for this dataset once each machine learning algorithm was fitted 20 models were built by using the 20 training subsets finally susceptibility maps were generated by averaging the prediction values of the 20 models for the whole study area in addition to the mean prediction values standard deviation maps could be extracted from this process and analysed in order to eventually detect areas with high variability in fire susceptibility a useful by product of rf algorithm is the variable importance ranking breiman 2001 from an internal evaluation of each variable based on random shuffling it computes the percentage of mean square error increase incmse by comparing the difference of rf performance when considering both the raw variables and the shuffled variables breiman 2001 as a result each variable can be ranked according to their incmse score with the following meaning high incmse score indicates an important contribution in the relationship between input and output variables while low incmse score close to 0 indicate that the variable is not a valuable contributor to the model 4 results and discussion the following section 4 1 discusses the selection of clc and slope variables as the only parameters influencing the wildfire susceptibility in our study area then results and comparisons on the susceptibility maps generated by the three proposed methods are presented in section 4 2 finally section 4 3 assesses the different methods by using data from a testing period moreover it presents the variable importance measurement for both the deterministic and the random forest approaches and discusses on the relevance of the obtained results according to the literature in this field 4 1 major variables affecting wildfires occurrence in portugal the deterministic model was first proposed by verde and zêzere 2010 further discussed in verde 2015 and then updated and used by parente and pereira 2016 to map the structural fire risk this model is based on the combination of geographic variables that do not change much in the short period this is in line with the wildfire susceptibility being a measure of the terrain land propensity for the occurrence of wildfires based on the terrain s intrinsic characteristics parente and pereira 2016 although it is a quite simple model parsimoniously based on just two variables it is very robust its robustness was recently assessed verde 2015 in respect to the use of single or multiple clc inventories as well as to rely the calibration and validation on different clc inventories the obtained results point to a relative independence of the model performance in relation to how many or which clc inventories are used to access the favourability scores parente and pereira 2016 test the impacts of using a high resolution dem and besides mapping the susceptibility with higher spatial accuracy the obtained patterns were very similar obviously changes in vegetation cover and different fire history can induce different susceptibility patterns due to changes fire probability and vegetation dynamics many researchers have studied the fire processes mechanisms and tried to identify the underlying factors in portugal including topography land use land cover climate man made features demographic and socio economic information for example nunes et al 2016 found that topography land cover population density and livestock are significant in both ignition density and ba variables such as altitude slope and land cover help to explain the existence of space time clusters of fires in portugal parente and pereira 2016 parente et al 2016 tonini et al 2017 verde and zêzere 2010 tested the usefulness of other variables such as altitude temperature and precipitation in the deterministic model but they did not found any significant increase in the prediction rates this may be due to several reasons first some of these variables can be proxies of each other slope is a measure of the altitude change chang and tsai 1991 parente and pereira 2016 while altitude regulates the rainfall and temperature li et al 2010 neteler et al 2011 parente and pereira 2016 climate weather conditions determines the existence type and state of the vegetation at each location which means that the information about vegetation cover implicitly considers climate information parente and pereira 2016 second all fires tend to occur associated to high air temperature low humidity and relatively long periods of drought amraoui et al 2015 parente and pereira 2016 trigo et al 2006 third vegetation cover can be viewed as a set of different variables instead of just one for example to model fire ignition probabilities vasconcelos et al 2001 test the usefulness of clc related variables such as distance to urban areas distance to agricultural areas distance to forests distance to scrublands etc which can be viewed as a different use of vegetation cover oliveira et al 2012 adopted a similar procedure to study the spatial distribution of large fires considering the proportion of forest area of scrubs of agricultural areas etc finally in a very recent study fernandes et al 2016 identifies fuels and topography as the major determinants of large size ba in portugal and in the western mediterranean basin which is consistent with previous findings on the characterization of wildfires in portugal marques et al 2011 another aspect that must be pointed out in deterministic model is the double use of the ba fire probability namely i to compute the favourability scores to rank clc and slope classes in terms of fire proneness and ii in the expression of susceptibility in the form of fire probability in each pixel i e to discriminate where within the country each class is more or less affected in addition fire probability is also a proxy for the human behaviour since the large majority of the fires are caused by humans parente and pereira 2016 verde and zêzere 2010 4 2 susceptibility maps fig 6 shows the susceptibility maps obtained by applying the three models in a broad sense the three models lead to relatively similar maps the main areas with high very high and low very low susceptibility classes are detected and highlighted on similar locations the very high susceptibility class shows a common pattern for the three models and is mainly located on the north of the region and on the south border in order to evaluate the two stochastic models assuming the deterministic one as reference maps of the differences of susceptibility were generated fig 7 these maps were produced by assigning each class of susceptibility to a unique value very low 0 low 1 medium 2 high 3 and very high 4 and by computing the differences pixel by pixel these maps are predominantly characterized by light colours fig 7 which means that differences when they exist occur between successive classes 1 d i f f 1 the southwest part of the study area shows an apparent and systematic underestimation of the susceptibility classes for rf and elm models compared with the deterministic model fig 7a and b nevertheless this difference is not problematic since it concerns essentially of classifying a pixel in the very low class instead of low or medium classes for the rest of the region differences between the stochastic and deterministic models are insignificant differences between the two machine learning algorithms rf and elm are shown on fig 7c these differences are only slightly present but without significant spatial variations this result is mainly due to the same pre processing and similar methodological procedure featuring the two stochastic methods moreover from the machine learning point of view the use of 100 000 training points contributes to the general stability of both rf and elm models finally it is important to note that standard deviation maps not shown computed from the 20 rf and elm models built by using the 20 training subsets to eventually detect areas with high variability in fire susceptibility reveal on the contrary very low variability of both models in addition to this a general evaluation of both methods was performed by computing the mean squared error mse on the testing set which has never been used during the learning process unsurprisingly elm and rf algorithms show highly similar results with a mse of 0 1115 for elm and 0 1117 for rf 4 3 methods assessment fig 8 table 2 and table 3 show the proportion of ba within each susceptibility class obtained for each method and assessed for the testing period 2010 2013 moreover the ratio between the size of each class and the proportion of ba were also computed by considering the deterministic model as the reference the elm and rf susceptibility maps reveal proportions of ba close to the benchmark model differences in the percentage of total ba in each susceptibility class is always less than 7 2 apparently the percentage of total ba in the two first susceptibility classes very low and low is higher for elm and rf approximately 4 but in the medium and high susceptibility classes this value is higher for the deterministic approach about 3 7 for the last susceptibility class very high both elm and rf algorithms show a percentage of total ba higher than the deterministic one 3 higher as it was already mentioned in section 4 1 the susceptibility maps generated by elm and rf are very similar as shown in fig 8 tables 2 and 3 the maximum difference between the percentage of total ba in each susceptibility class for both methods is 0 5 in low and medium classes and almost zero in the others classes generally and by considering the three approaches the obtained results are promising in the sense that less than 20 of the total ba of the testing period was classified as very low or low susceptibility by summing the very low and low scores this evaluation over the testing period allows to validate the proposed new approach in this field through machine learning algorithms and to compare the stochastic and deterministic approaches on non used dataset the rf algorithm allows an internal evaluation of each input variable which leads to a variable importance ranking breiman 2001 this last result constitutes a significant added value to the understanding of the phenomenon in fig 9 the top 9 variables for rf are listed by decreasing order of their respective incmse score the first six land cover variables clc324 clc322 clc334 clc333 clc312 and clc321 represent the variables that most contribute to model and explain the observed variance higher incmse score these correspond to transitional woodland shrub moors and heathland burnt areas sparsely vegetated areas coniferous forest and natural grasslands this short list is dominated by scrub and or herbaceous vegetation associations level 32 of clc classes followed by open spaces with little or no vegetation level 33 and forests level 31 these results are in accordance with fire selectivity studies performed for portugal where fire selectivity is generally higher for scrublands pine stands and eucalyptus plantations than for evergreen oak woodlands annual and rainfed crops and agroforestry lands barros and pereira 2014 similar findings were recently obtained for fire proneness studies also performed for portugal moreira et al 2009 silva et al 2009 in general agricultural areas are excluded from this list because it includes well managed arable lands both irrigated and non irrigated permanent crops vineyards olive groves fruit trees and berry plantations and even pastures however heterogeneous agricultural areas clc level 24 especially those corresponding to complex cultivation patterns with significant areas of natural vegetation present higher relative importance in rf stochastic models the slope is one of the most important factors of fire spread acting on different aspects of the fuel combustion rothermel 1972 however per se i e without the other aspects of the fire environment controls usually conceptualized in fire triangles e g whitlock et al 2010 the slope is not able to independently determine the terrain land propensity for the occurrence or spread of a wildfire for example terrain parcel with high slope can be free from vegetation therefore it is not surprising that the ranking of the most important variables is dominated by the land cover variables with 9 classes in the top 10 variables on table 4 the top 6 variables for both random forest and deterministic methods are shown for comparison purposes the favorability score of the deterministic model computed based on eq 2 are retained as highlighted 5 of the 6 top variables selected by random forest are also among the most important variables of the deterministic model even if with a different order this fact underlines that in spite of the differences between the methods random forest being able to detect non linear relationship the matching between the most relevant variables is highly satisfactory and validates the use of the new approach based on machine learning algorithm the apparent greater importance of conifers clc312 in relation to the mixed clc313 and broadleaf forest hardwoods clc311 for the rf fig 9 is also worth noting for two reasons 1 these variables present the same relative importance in both methods and 2 it is in good agreement with previous studies for vegetation fire proneness performed for portugal silva and harrison 2010 pereira et al 2014 in fact the increase in conifer tree component tends to increase the difficulties to control the fire rowe and scotter 1973 ba and fire proneness moreira et al 2009 silva et al 2009 silva and harrison 2010 and fire risk in wuis lampin maillet et al 2010 5 conclusion in the present paper susceptibility maps of wildfires obtained by applying stochastic methods namely random forest and extreme learning machine were compared with the correspondent map elaborated by applying a validated standard deterministic method here considered as a benchmark the study was performed for the dão lafões region of portugal which is a representative region of a country highly prone to wildfires the variables implemented into the model considered as favorable factors for wildfires are the slope the land use and vegetation covers provided by the corine land cover 2006 inventory the official dataset of the national mapping ba was considered to train 2000 2009 period and test 2010 2013 period the models comparison of the obtained results clearly suggests that the two stochastic models perform in an equal manner in terms of susceptibility areas and classes as well as that these results are broadly consistent with susceptibility maps obtained with the benchmarking model the main benefit of using stochastic models is that these approaches are data driven meaning that they do not need a priori knowledge of the process moreover random forest directly provides the measurement of the importance of each variable on this respect the rf and the deterministic models present similar top variable importance ranking results of the present analysis are encouraging for further applications of stochastic models to elaborate susceptibility maps considering more variables and larger areas 6 software and data availability the following software and data were used to perform the analysis presented in this paper qgis qgis development team 2016 an open source geospatial software was mainly used for the pre and post processing and the elaboration of maps r language r core team 2016 is an open source statistical software it was used with the packages randomforest and elmnn for computing the random forest and the extreme learning machine algorithms digital elevation model dem derived from the shuttle radar topographic mission strm nasa was used to compute the slope corine land cover clc 2006 is an inventory provided by the european environment agency it was used in order to extract the land use and land cover map national mapping burnt areas nmba is an official portuguese fire dataset and provides a detailed description of the shape and the size of ba it was provided by the institute for the conservation of nature and forests icnf 2016 acknowledgements michael leuenberger wants to thank the institute of earth surface dynamics university of lausanne where he works on this paper during his phd this work was supported by i the herbette foundation of the university of lausanne under the project 2016 2 e 15 ii the firextr project ptdc atpgeo 0462 2014 iii the project interact integrative research in environment agro chain and technology norte 01 0145 feder 000017 research line best co financed by feder norte 2020 and iv european investment funds by feder compete poci operacional competitiveness and internacionalization programme under the project poci 01 0145 feder 006958 and national funds by fct portuguese foundation for science and technology under the project uid agr 04033 2013 we are especially grateful to icnf for providing the fire data and to joão pereira for the final spelling and grammar review of the manuscript 
26441,wildfire susceptibility is a measure of land propensity for the occurrence of wildfires based on terrain s intrinsic characteristics in the present study two stochastic approaches i e extreme learning machine and random forest for wildfire susceptibility mapping are compared versus a well established deterministic method the same predisposing variables were combined and used as predictors in all models the portuguese region of dão lafões was selected as a pilot site since it presents national average values of fire incidence and a high heterogeneity in land cover and slope maps representing the susceptibility of the study area to wildfires were finally elaborated two measures were used to compare the different methods namely the location of the pixels with similar standardized susceptibility and total validation burnt area results obtained with the stochastic methods are very alike with the deterministic ones with the advantage of not depending on a priori knowledge of the phenomenon graphical abstract image 1 keywords susceptibility mapping wildfires random forest extreme learning machines portugal 1 introduction wildfires are defined as unwanted fires occurring in countryside or rural area and burning forest and wild lands included abandoned agricultural lands and rural vegetated areas wildfires as undesirable as often uncontrolled events represent a hazardous and harmful phenomena to people and environment natural fires caused by lightning appeared on the earth surface in concomitance with the first plant communities well before the appearance of humans and played a key role in plant adaptation and the ecosystems equilibrium pausas and keeley 2009 nowadays the primary cause of wildfires in populated areas is related to the human activities that voluntary arsonism or involuntary accidental or negligent causes can initiate fire a recent analysis of fire data from the european forest fire information system shows that over 95 of wildfires are human induced san miguel ayanz et al 2012 and this percentage is even higher in the mediterranean regions estimating the probability of wildfire occurrence in a certain area under particular environmental and anthropogenic conditions is a modern tool to support forest protection plans and to reduce fires consequences which can also affect the neighbouring or intermingled urban areas in this context the implementation of wildfire susceptibility maps and the investigation of the main driving factors inducing wildfires is fundamental a good review of these factors can be found in ganteaume et al 2013 they included human factors and related variables such as distance to road or to urban area as well as environmental factors more or less sophisticated models have been applied to combine the predisposing variables into a geographic information systems gis chuvieco et al 2010 chuvieco and salas 1996 bonazountas et al 2005 jaiswal et al 2002 the most reliable analyses applied statistical models to assess the importance of different variables influencing fire occurrences and the obtained results are used to produce the risk maps beverly et al 2009 soto et al 2013 pourtaghi et al 2015 recent analyses compared different statistical models for variable selection pourghasemi 2016 pourghasemi et al 2016 pourtaghi et al 2016 rodrigues et al 2014 eugenio et al 2016 but most of the studies relied on expert knowledge to pre select most important drivers or on the results of linear deterministic statistical models portugal is unequivocally the european country most affected by wildfires due to its favorable climatic conditions topography and vegetation amraoui et al 2015 pereira et al 2013 investigations of driven factors and the elaborations of wildfires density and risk maps were latterly performed for this highly affected country tonini et al 2017 analysed the spatio temporal density distribution of these hazardous events in the last decades and produced a 3d graphical output of the results which highlights areas and frame periods more affected by wildfires nunes et al 2016 used geographically weighted regression to identify relevant municipal drivers of fires it results that topography and population density were significant factors in municipal ignitions while topography and uncultivated land were significant factors in municipal burnt area ba verde and zêzere 2010 assessed forest fire susceptibility testing and using variables of strong spatial correlation i e elevation slope land cover rainfall and temperature and more recently parente and pereira 2016 adopted this method updating the selected variables to map the structural fire risk in the vegetated area of portugal in the present study the authors refer to the wildfire susceptibility mapping as an estimation of the probability that fire occurs in a specific area without considering a temporal scale assessed on the basis of predisposing factors related to terrain s intrinsic characteristics the revised literature misses the use of stochastic models to elaborate accurate susceptibility maps of wildfires which can be compared with the results obtained by applying deterministic approaches these latter methods usually assume a priori knowledge of predisposing factors or they are evaluated by applying linear methods which implies that every set of variable states is uniquely determined by the parameters used in the model and by the sets of previous states therefore a deterministic model always performs the same way for a given set of initial conditions contrary to the deterministic approach the stochastic methods assume that results obtained by the combination of independent factors i e variables affecting the investigated phenomenon can be slightly different due to the randomness of the process this aspect is particularly useful to model environmental and anthropogenic hazard which naturally present a complex behaviours and patterns therefore the objective of the present study is to compare stochastic approaches vs a well established deterministic method for wildfire susceptibility mapping a first assessment of the susceptibility and hazard wildfire performed for portugal verde and zêzere 2010 parente and pereira 2016 is used as benchmarking while extreme learning machine elm and random forest rf are the two applied stochastic methods we restricted our investigation to a pilot area namely the region of dão lafões characterized by a high variability and heterogeneity of environmental features and fire incidence similar to the national average which makes it a good representative of the general characteristics of continental portugal 2 material study area and datasets 2 1 study area portugal is the european country more to the southwest with a mediterranean type of climate but suffering of the influence of the atlantic ocean that bathes its western and southern coasts parente et al 2016 mainland portugal has a total land area of about 90 000 k m 2 which according to the corine land cover clc 2006 inventory is predominantly used for agriculture 47 followed by forests coverings 23 scrub and or herbaceous vegetation associations 23 and open spaces with little or no vegetation 2 pereira et al 2014 according to the planos regionais de ordenamento florestal prof continental portugal is divided into 21 prof regions fig 1 the prof establish specific rules for the use and exploitation of its forest spaces in order to ensure sustainable production of all goods and services associated with them icnf 2016 in the present study dão lafões region was selected as the case study area for the following reasons i it is located in the northern half of portugal which presents by far the highest wildfire incidence parente and pereira 2016 ii this region presents an annual average number of fires and ba very similar to the national average and iii its area is very heterogeneous in terms of topography land use and vegetation cover fig 2 2 2 the datasets raw data used in this study include i digital elevation model dem derived from the shuttle radar topographic mission with a resolution of 1 arc second dem srtm 25 m used to compute elevation and slope gonçalves and morgado 2008 ii clc 2006 inventory produced by the european environment agency which provides the land use and land cover maps and iii the national mapping burnt areas nmba implemented by the institute for the conservation of nature and forests icnf 2016 which provides a detailed description of the shape and the size of the area burnt by fires in each year of occurrence the data pre and post processing as well as the mapping elaboration were performed by quantum gis free software qgis development team 2016 2 2 1 topography topography characterized by the altitude slope and exposure constitutes one of the most important factors to define the type of the climate of a region such as the average weather conditions and the space time variability of the climatic elements e g air temperature precipitation solar radiation these factors control the life cycle of the vegetation cover and land use and have a profound influence on the fire incidence chuvieco and congalton 1989 freire et al 2002 verde and zêzere 2010 parente and pereira 2016 parente et al 2016 in this study we considered the slope as the main topographic variable influencing the susceptibility to wildfires in the study area this value was derived from the dem and categorized in the same 6 classes used by verde and zêzere 2010 namely 0 2 2 5 5 10 10 15 15 20 and 20 2 2 2 land use and vegetation cover the clc consists of an inventory of land cover in 44 classes with a minimum map unit of 25 ha for areal phenomena the main classes are artificial surfaces agricultural forest and semi natural areas wetlands and water bodies büttner 2014 caetano et al 2009 the 2006 version of clc was used in the present study fig 2 since this date is in the middle of the investigated period 2000 2013 in investigated region dão lafões the different classes of land cover and land use are quite homogeneously distributed within the area however it is possible to identify some patterns higher concentration of forest cover may be found in the southwest and middle class slopes agricultural areas are mostly located in the southeast away from the highest slopes while scrubs are predominant in the southeast and northwest borders as well as in high slopes 2 2 3 the fire dataset the nmba is an official portuguese fire dataset based on satellite imagery acquired once per year at the end of the fire season and delivered in vector format as polygons of the ba allowing a detailed description of the location size and shape of the fire scars which is fundamental for the present study this dataset was recently reviewed to correct a minor number of missing values and data inconsistencies it contains 17 903 fire events between 2000 and 2013 where 1 114 of which occurred on dão lafões parente et al 2016 in this region most of the fire incidences are located far in the north and in the southeast fig 2 affecting mostly agricultural areas 10 scrublands 62 and open spaces 13 as well as areas with slopes ranging from 5 to 10 32 and 10 to 15 23 the location and size of the ba for the investigated period 2000 2013 is represented in fig 3 in the form of fire frequency ff which is the number of times each pixel burnt over the fourteen years the year with the highest fire incidence was 2005 with 11 of the total number of fires and 29 of the total ba in the study period followed by 2012 11 of the total number of fires and 2013 8 of the total number of fires and 20 of the total ba only 18 of total number of pixels burnt at least once and the fire frequency is mostly low or very low figs 2 and 3 with 97 of the total number of burnt pixels tnbp with f f 3 14 namely 72 of tnbp with f f 1 14 20 of tnbp with f f 2 14 and 5 of tnbp with f f 3 14 3 methodology both deterministic and stochastic models for wildfire susceptibility mapping were applied in the present study the deterministic model used as benchmark was developed by verde and zêzere 2010 and further adopted and updated by parente and pereira 2016 the model includes the computation of fire occurrence probability and favorability scores for each predisposing variable land cover and slope two stochastic methods from the machine learning field were then applied rf and elm generally speaking stochastic models account for the uncertainty in modelling processes that have some kind of randomness and therefore are useful to represent phenomena with random variability in the case of machine learning algorithms the models produce susceptibility maps based on input data variables without the need of a priori knowledge of the investigated phenomena but simply learning from experience once the model is fitted according to the training data it allows to generate predictions over the entire study area in the present study data were splitted into training 2000 2009 and validation periods 2010 2013 the first was used to fit and calibrate the three models and the second to assess and compare susceptibility maps the susceptibility maps were elaborated by means of gis procedures and organized into 5 classes in agreement with the portuguese law dl 2006 the classes were defined as in the reference works verde and zêzere 2010 parente and pereira 2016 using the quintiles of the susceptibility computed as explained below the applied methods were assessed by computing the matching pixel by pixel between the standardized susceptibility maps obtained for the training period 2000 2009 and the effective ba over the validation period 2010 2013 these values were finally evaluated as a percentage for each susceptibility class the next two subsections are devoted to the brief description of the deterministic and stochastic applied models 3 1 deterministic method in portugal national authorities such as forest service icnf and the meteorological office instituto português do mar e da atmosfera ipma adopted the wildfire susceptibility map proposed by verde and zêzere 2010 which was developed using a deterministic approach and based on just three factors the susceptible values for each regular unit area i e pixel is computed by integrating the favorability scores f a v of the two variables slope and vegetation cover and the fire probability fp as 1 s p f p f a v slope f a v vegetation the favorability scores for each class x f a v x of slope and vegetation cover are computed by 2 f a v x n b p x t n p x 100 where n b p x is the number of burnt pixels in class x and t n p x is the total number of pixels in the class x the fire probability of each pixel is estimated using the fire database and the classic definition of probability according to 3 f p the number of times the pixel burned in the study period in years duration of the study in years 100 it is important to note that due to the yearly temporal acquisition of the fire database nmba each pixel can only burn once in each year in addition due to the multiplicative nature of susceptibility equation all the null favorability scores were reclassified to one thus becoming neutral values in the equation therefore the obtained value in each pixel is a consequence of all the possible combinations of the variables found in that pixel 3 2 machine learning algorithms at present machine learning algorithms are important tools for the analysis modelling and visualization of environmental data kanevski et al 2009 they have good generalization abilities when modelling high dimensional and complex nonlinear phenomena are universal modelling methods and many of them have solid roots in statistical learning theory hastie et al 2009 in predictive learning they focus on modelling the hidden relationship between a set of input and output variables by trying to minimize both the errors and the complexity of the model after a training procedure to calibrate the model s parameters prediction maps of the susceptibility can be computed and displayed with the corresponding uncertainty quantification in this study two machine learning algorithms based on two different concepts are used for comparison purposes rf which is based on decision trees and elm which is based on traditional artificial neural networks detailed application of the rf and elm for environmental data modelling along with the description of consistent methodology are presented in literature micheletti et al 2014 leuenberger and kanevski 2015 analysis were performed using r free software r core team 2016 the packages randomforest and elmnn were employed for rf and elm respectively 3 2 1 random forest developed by breiman 2001 rf is an ensemble machine learning algorithm based on decision trees it contains two hyper parameters the number of decision trees generated nbtree and the number of selected variables for each split node nbvar the random forest algorithm first generates nbtree subsets of the training dataset by bootstrapping i e random sampling with replacement then for each subset it will grow a decision tree by iterating the following rules up to the maximum level when each final node contains less than 5 data points 1 for each split the algorithm selects randomly nbvar variables 2 according to these nbvar variables and the output variable it computes the gini index hastie et al 2009 and selects the best variable with the best threshold in order to minimize the error of the prediction once the nbtree decision trees have been grown prediction of new data points is performed by taking the average value of all decision trees fig 4 4 y pred 1 n b t r e e i 1 n b t r e e y i this procedure leads to a robust mean value of prediction as well as a measure of uncertainty by considering the standard deviation among all trees 3 2 2 extreme learning machine elm is based on artificial neural network concept following the structure of a single hidden layer feedforward neural network slfn it connects all input variables to the hidden layer computes the neurone value and averages all neurons with optimal weights to the output layer huang et al 2006 leuenberger and kanevski 2015 more formally composed of nbnode neurons n and by using an activation function g ℝ ℝ the elm network connecting the inputs x i to the output y i value can be written in the following form 5 j 1 n β j g x i w j b j y i where x i w j is an inner product between the input x i and the weight vector w j which connects the input layer to the jth neuron b j is the bias of the jth neuron and β is a weight vector connecting the hidden layer to the output layer in a more compact way elm can be written as 6 h β y where h i j u j j 1 n g x i w j b j is the output matrix of the hidden layer fig 5 according to this notation elm algorithm applies the following steps 1 randomly generates the input weight w j and the bias b j 2 computes the matrix h 3 computes the output weight β h y where h is the moore penrose generalized inverse of matrix h at the end the only fitted parameter is the number of hidden neuron n n b n o d e 3 2 3 parameter optimization in order to optimize the learning process of rf and elm different pre processing steps must be considered first of all the clc classes were converted into 27 dummy variables one for each class of the clc variable within the study area then the complete dataset which is composed of 28 input variables slope clc variables and 1 output variable presence or absence of forest fire was normalized into the 0 1 interval this transformation was performed in order to fit the functional range where elm works in an optimal way huang et al 2006 after that from the 5 581 522 raster cells covering the study area 100 000 points approximately 1 8 of the total area were randomly selected using a stratified sampling for the construction of the testing subset table 1 namely 6 strata were used by considering the number of time each pixel burnt between 0 and 5 times in this case this process was reiterated 20 times in order to generate 20 training subsets but without considering already selected testing points the optimization of the nbtree and nbvar hyper parameters of rf was performed by using a trial and error process the choice of this method is justified by the fact that both types of rf hyper parameters are not highly sensitive to changes and optimized values are close to the default ones in this study hyper parameters of rf were set to 1000 and 9 for nbtree and nbvar respectively for elm the nbnode hyper parameter which is the number of nodes in the hidden layer a 5 fold cross validation approach was performed minimum mean squared error mse values obtained for the validation sets were retained which lead to an optimal number of 40 nodes for this dataset once each machine learning algorithm was fitted 20 models were built by using the 20 training subsets finally susceptibility maps were generated by averaging the prediction values of the 20 models for the whole study area in addition to the mean prediction values standard deviation maps could be extracted from this process and analysed in order to eventually detect areas with high variability in fire susceptibility a useful by product of rf algorithm is the variable importance ranking breiman 2001 from an internal evaluation of each variable based on random shuffling it computes the percentage of mean square error increase incmse by comparing the difference of rf performance when considering both the raw variables and the shuffled variables breiman 2001 as a result each variable can be ranked according to their incmse score with the following meaning high incmse score indicates an important contribution in the relationship between input and output variables while low incmse score close to 0 indicate that the variable is not a valuable contributor to the model 4 results and discussion the following section 4 1 discusses the selection of clc and slope variables as the only parameters influencing the wildfire susceptibility in our study area then results and comparisons on the susceptibility maps generated by the three proposed methods are presented in section 4 2 finally section 4 3 assesses the different methods by using data from a testing period moreover it presents the variable importance measurement for both the deterministic and the random forest approaches and discusses on the relevance of the obtained results according to the literature in this field 4 1 major variables affecting wildfires occurrence in portugal the deterministic model was first proposed by verde and zêzere 2010 further discussed in verde 2015 and then updated and used by parente and pereira 2016 to map the structural fire risk this model is based on the combination of geographic variables that do not change much in the short period this is in line with the wildfire susceptibility being a measure of the terrain land propensity for the occurrence of wildfires based on the terrain s intrinsic characteristics parente and pereira 2016 although it is a quite simple model parsimoniously based on just two variables it is very robust its robustness was recently assessed verde 2015 in respect to the use of single or multiple clc inventories as well as to rely the calibration and validation on different clc inventories the obtained results point to a relative independence of the model performance in relation to how many or which clc inventories are used to access the favourability scores parente and pereira 2016 test the impacts of using a high resolution dem and besides mapping the susceptibility with higher spatial accuracy the obtained patterns were very similar obviously changes in vegetation cover and different fire history can induce different susceptibility patterns due to changes fire probability and vegetation dynamics many researchers have studied the fire processes mechanisms and tried to identify the underlying factors in portugal including topography land use land cover climate man made features demographic and socio economic information for example nunes et al 2016 found that topography land cover population density and livestock are significant in both ignition density and ba variables such as altitude slope and land cover help to explain the existence of space time clusters of fires in portugal parente and pereira 2016 parente et al 2016 tonini et al 2017 verde and zêzere 2010 tested the usefulness of other variables such as altitude temperature and precipitation in the deterministic model but they did not found any significant increase in the prediction rates this may be due to several reasons first some of these variables can be proxies of each other slope is a measure of the altitude change chang and tsai 1991 parente and pereira 2016 while altitude regulates the rainfall and temperature li et al 2010 neteler et al 2011 parente and pereira 2016 climate weather conditions determines the existence type and state of the vegetation at each location which means that the information about vegetation cover implicitly considers climate information parente and pereira 2016 second all fires tend to occur associated to high air temperature low humidity and relatively long periods of drought amraoui et al 2015 parente and pereira 2016 trigo et al 2006 third vegetation cover can be viewed as a set of different variables instead of just one for example to model fire ignition probabilities vasconcelos et al 2001 test the usefulness of clc related variables such as distance to urban areas distance to agricultural areas distance to forests distance to scrublands etc which can be viewed as a different use of vegetation cover oliveira et al 2012 adopted a similar procedure to study the spatial distribution of large fires considering the proportion of forest area of scrubs of agricultural areas etc finally in a very recent study fernandes et al 2016 identifies fuels and topography as the major determinants of large size ba in portugal and in the western mediterranean basin which is consistent with previous findings on the characterization of wildfires in portugal marques et al 2011 another aspect that must be pointed out in deterministic model is the double use of the ba fire probability namely i to compute the favourability scores to rank clc and slope classes in terms of fire proneness and ii in the expression of susceptibility in the form of fire probability in each pixel i e to discriminate where within the country each class is more or less affected in addition fire probability is also a proxy for the human behaviour since the large majority of the fires are caused by humans parente and pereira 2016 verde and zêzere 2010 4 2 susceptibility maps fig 6 shows the susceptibility maps obtained by applying the three models in a broad sense the three models lead to relatively similar maps the main areas with high very high and low very low susceptibility classes are detected and highlighted on similar locations the very high susceptibility class shows a common pattern for the three models and is mainly located on the north of the region and on the south border in order to evaluate the two stochastic models assuming the deterministic one as reference maps of the differences of susceptibility were generated fig 7 these maps were produced by assigning each class of susceptibility to a unique value very low 0 low 1 medium 2 high 3 and very high 4 and by computing the differences pixel by pixel these maps are predominantly characterized by light colours fig 7 which means that differences when they exist occur between successive classes 1 d i f f 1 the southwest part of the study area shows an apparent and systematic underestimation of the susceptibility classes for rf and elm models compared with the deterministic model fig 7a and b nevertheless this difference is not problematic since it concerns essentially of classifying a pixel in the very low class instead of low or medium classes for the rest of the region differences between the stochastic and deterministic models are insignificant differences between the two machine learning algorithms rf and elm are shown on fig 7c these differences are only slightly present but without significant spatial variations this result is mainly due to the same pre processing and similar methodological procedure featuring the two stochastic methods moreover from the machine learning point of view the use of 100 000 training points contributes to the general stability of both rf and elm models finally it is important to note that standard deviation maps not shown computed from the 20 rf and elm models built by using the 20 training subsets to eventually detect areas with high variability in fire susceptibility reveal on the contrary very low variability of both models in addition to this a general evaluation of both methods was performed by computing the mean squared error mse on the testing set which has never been used during the learning process unsurprisingly elm and rf algorithms show highly similar results with a mse of 0 1115 for elm and 0 1117 for rf 4 3 methods assessment fig 8 table 2 and table 3 show the proportion of ba within each susceptibility class obtained for each method and assessed for the testing period 2010 2013 moreover the ratio between the size of each class and the proportion of ba were also computed by considering the deterministic model as the reference the elm and rf susceptibility maps reveal proportions of ba close to the benchmark model differences in the percentage of total ba in each susceptibility class is always less than 7 2 apparently the percentage of total ba in the two first susceptibility classes very low and low is higher for elm and rf approximately 4 but in the medium and high susceptibility classes this value is higher for the deterministic approach about 3 7 for the last susceptibility class very high both elm and rf algorithms show a percentage of total ba higher than the deterministic one 3 higher as it was already mentioned in section 4 1 the susceptibility maps generated by elm and rf are very similar as shown in fig 8 tables 2 and 3 the maximum difference between the percentage of total ba in each susceptibility class for both methods is 0 5 in low and medium classes and almost zero in the others classes generally and by considering the three approaches the obtained results are promising in the sense that less than 20 of the total ba of the testing period was classified as very low or low susceptibility by summing the very low and low scores this evaluation over the testing period allows to validate the proposed new approach in this field through machine learning algorithms and to compare the stochastic and deterministic approaches on non used dataset the rf algorithm allows an internal evaluation of each input variable which leads to a variable importance ranking breiman 2001 this last result constitutes a significant added value to the understanding of the phenomenon in fig 9 the top 9 variables for rf are listed by decreasing order of their respective incmse score the first six land cover variables clc324 clc322 clc334 clc333 clc312 and clc321 represent the variables that most contribute to model and explain the observed variance higher incmse score these correspond to transitional woodland shrub moors and heathland burnt areas sparsely vegetated areas coniferous forest and natural grasslands this short list is dominated by scrub and or herbaceous vegetation associations level 32 of clc classes followed by open spaces with little or no vegetation level 33 and forests level 31 these results are in accordance with fire selectivity studies performed for portugal where fire selectivity is generally higher for scrublands pine stands and eucalyptus plantations than for evergreen oak woodlands annual and rainfed crops and agroforestry lands barros and pereira 2014 similar findings were recently obtained for fire proneness studies also performed for portugal moreira et al 2009 silva et al 2009 in general agricultural areas are excluded from this list because it includes well managed arable lands both irrigated and non irrigated permanent crops vineyards olive groves fruit trees and berry plantations and even pastures however heterogeneous agricultural areas clc level 24 especially those corresponding to complex cultivation patterns with significant areas of natural vegetation present higher relative importance in rf stochastic models the slope is one of the most important factors of fire spread acting on different aspects of the fuel combustion rothermel 1972 however per se i e without the other aspects of the fire environment controls usually conceptualized in fire triangles e g whitlock et al 2010 the slope is not able to independently determine the terrain land propensity for the occurrence or spread of a wildfire for example terrain parcel with high slope can be free from vegetation therefore it is not surprising that the ranking of the most important variables is dominated by the land cover variables with 9 classes in the top 10 variables on table 4 the top 6 variables for both random forest and deterministic methods are shown for comparison purposes the favorability score of the deterministic model computed based on eq 2 are retained as highlighted 5 of the 6 top variables selected by random forest are also among the most important variables of the deterministic model even if with a different order this fact underlines that in spite of the differences between the methods random forest being able to detect non linear relationship the matching between the most relevant variables is highly satisfactory and validates the use of the new approach based on machine learning algorithm the apparent greater importance of conifers clc312 in relation to the mixed clc313 and broadleaf forest hardwoods clc311 for the rf fig 9 is also worth noting for two reasons 1 these variables present the same relative importance in both methods and 2 it is in good agreement with previous studies for vegetation fire proneness performed for portugal silva and harrison 2010 pereira et al 2014 in fact the increase in conifer tree component tends to increase the difficulties to control the fire rowe and scotter 1973 ba and fire proneness moreira et al 2009 silva et al 2009 silva and harrison 2010 and fire risk in wuis lampin maillet et al 2010 5 conclusion in the present paper susceptibility maps of wildfires obtained by applying stochastic methods namely random forest and extreme learning machine were compared with the correspondent map elaborated by applying a validated standard deterministic method here considered as a benchmark the study was performed for the dão lafões region of portugal which is a representative region of a country highly prone to wildfires the variables implemented into the model considered as favorable factors for wildfires are the slope the land use and vegetation covers provided by the corine land cover 2006 inventory the official dataset of the national mapping ba was considered to train 2000 2009 period and test 2010 2013 period the models comparison of the obtained results clearly suggests that the two stochastic models perform in an equal manner in terms of susceptibility areas and classes as well as that these results are broadly consistent with susceptibility maps obtained with the benchmarking model the main benefit of using stochastic models is that these approaches are data driven meaning that they do not need a priori knowledge of the process moreover random forest directly provides the measurement of the importance of each variable on this respect the rf and the deterministic models present similar top variable importance ranking results of the present analysis are encouraging for further applications of stochastic models to elaborate susceptibility maps considering more variables and larger areas 6 software and data availability the following software and data were used to perform the analysis presented in this paper qgis qgis development team 2016 an open source geospatial software was mainly used for the pre and post processing and the elaboration of maps r language r core team 2016 is an open source statistical software it was used with the packages randomforest and elmnn for computing the random forest and the extreme learning machine algorithms digital elevation model dem derived from the shuttle radar topographic mission strm nasa was used to compute the slope corine land cover clc 2006 is an inventory provided by the european environment agency it was used in order to extract the land use and land cover map national mapping burnt areas nmba is an official portuguese fire dataset and provides a detailed description of the shape and the size of ba it was provided by the institute for the conservation of nature and forests icnf 2016 acknowledgements michael leuenberger wants to thank the institute of earth surface dynamics university of lausanne where he works on this paper during his phd this work was supported by i the herbette foundation of the university of lausanne under the project 2016 2 e 15 ii the firextr project ptdc atpgeo 0462 2014 iii the project interact integrative research in environment agro chain and technology norte 01 0145 feder 000017 research line best co financed by feder norte 2020 and iv european investment funds by feder compete poci operacional competitiveness and internacionalization programme under the project poci 01 0145 feder 006958 and national funds by fct portuguese foundation for science and technology under the project uid agr 04033 2013 we are especially grateful to icnf for providing the fire data and to joão pereira for the final spelling and grammar review of the manuscript 
26442,this study introduces an end use based system dynamics model to support municipal water planning and management over the medium to long term the calgary water management model cwmm simulates water demand and use to 2040 at a weekly time step for ten municipal end uses as well as the effects of population growth climate change and various water management policies and includes policy implementation costs for assessment of conservation versus economic trade offs the model was validated against historical water demand data for calgary alberta a series of scenario simulations showed 1 potentially large changes to both seasonal and non seasonal water demands with climate change and population growth 2 a need to enhance historical water management policies with new policies such as xeriscaping and greywater reuse to achieve water management goals and 3 the value of an end use based model in simulating management policy effects on municipal water demand and use keywords water demand model municipal water conservation decision support tool scenario analysis system dynamics calgary software availability software name cwmm calgary water management model developers k wang and e davies university of alberta year first available 2017 program language vensim hardware requirements no special requirements software required vensim model reader ventana systems inc availability and cost please contact the corresponding author for a free copy 1 introduction population growth and climate change present challenges for water resources planners and managers mcdonald et al 2011 and water security is increasingly of concern to urban authorities grafton et al 2011 yigzaw and hossain 2016 on the water supply side a variety of studies have investigated effects of changing precipitation patterns glacial retreat and sea level rise on hydrological variables cf dibike et al 2016 eshtawi et al 2016 scalzitti et al 2016 as well as urban hydraulic infrastructure expansions and management padowski and jawitz 2012 mays 2002 water authorities also recognize the value of managing water demand the focus of this research which is less time intensive and more cost effective and environmentally friendly than supply side management house peters and chang 2011 gleick 2003 municipal water systems serve residential industrial commercial institutional and public clients mayer et al 1999 whose demands are affected by both long term impact factors population change economic conditions and water conservation activities and short term impact factors including seasonal weather patterns and the associated summer peak demands march and saurí 2009 in most urban systems of north america the total water demand increases with population growth while per capita water use decreases with water conservation efforts such as adoptions of low flow fixtures and appliances educational campaigns water metering and consumption feedback leak detection programs economic incentives xeriscaping and water treatment and reuse billings and jones 2008 sønderlund et al 2016 deoreo et al 2016 reliable water demand modeling and forecasting provides the basis for both the short term operational and long term planning aspects of urban water management in terms of capital investment infrastructure expansion conflict mitigation policy analysis and system optimization and it can improve understanding of the underlying factors and dynamics that affect water demand and use billings and jones 2008 however accurate demand forecasting and analysis are challenging because of 1 the limited quantity and quality of data brown 2002 2 the numerous variables and drivers that affect demand march and saurí 2009 3 the high uncertainties associated with climate change economic conditions population growth and conservation activities gober et al 2011 4 the complexity of a quantitative analysis of water conservation options and their implementation costs billings and jones 2008 and 5 the different model horizons required for short term and long term purposes both of which affect water security donkor et al 2014 this paper presents a novel end use based model as a decision support tool for municipal water management that addresses many of the challenges above intended for tactical 1 10 years and strategic long term use see table 1 in donkor et al 2014 the model simulates short weekly and long term 10 years municipal water demand and use patterns under various climate change population growth and water conservation scenarios and reveals management trade offs such as the potential water savings and economic costs of alternative water management policies it includes ten specific end uses with seven residential end uses six indoor and one outdoor use and three non residential uses and simulates per capita water demand based on the number of water fixture uses per day and their associated water requirements which are affected by water conservation policies as well as appliance and fixture characteristics called the calgary water management model or cwmm the model runs fast a single simulation on a windows desktop takes a fraction of a second is easy to use requires relatively few input data and matches the historical municipal demands in calgary alberta while also permitting exploration of various plausible water scenarios into the future a system dynamics model cwmm can be adapted to other municipalities by changing model inputs stored in a ms excel file and refined from a whole city scale to represent individual neighborhoods or city regions the paper is structured as follows first municipal water modeling methodologies and models are reviewed then the research area and data availability are presented in terms of water supply demand and management conditions the model is next described and validated using data for calgary and sample results such as per capita water demand management policy conservation effectiveness and policy cost are presented finally the paper closes with conclusions a discussion of model limitations and potential next steps for the research 2 municipal water management and modeling a wide range of methods can be used for municipal water management and modeling with the selection depending on modeler skill available resources and data and accuracy requirements methods such as time series analysis regression analysis stochastic modeling artificial intelligence ai and system dynamics sd are discussed in this section as well as modeling concerns related to water customer disaggregation modeling time step economic considerations and climate change note that many of these methods such as time series and regression models or artificial neural networks and regression models are often used in combination to produce hybrid models that typically improve water demand forecasting performance over the use of the individual methods donkor et al 2014 2 1 available modeling methods water demand projections rely on estimated population growth and per capita water demands and modeling methods differ primarily in their treatment of the latter time series models predict per capita demand based on historical trends using moving averages exponential smoothing or autoregressive integrated moving average methods billings and jones 2008 and with fine temporal scale data can reveal significant temporal trends in water consumption correlated with economic variables as well as weather and climate factors house peters and chang 2011 4 regression models have also been used extensively historically donkor et al 2014 and employ social and economic factors called explanatory variables including water price house and lot size water saving technologies family income education and gender to estimate per capita or family water consumption through linear log linear or exponential models billings and jones 2008 these drivers are analyzed in terms of temporal and spatial scales house peters and chang 2011 direct and indirect impacts on water demand jorgensen et al 2009 and economic and non economic factors march and saurí 2009 both approaches require typically modest computing power house peters and chang 2011 but do not generally account for population growth or water conservation efforts therefore they are usually applied to short term demand prediction typically less than a year for small utilities see qi and chang 2011 house peters and chang 2011 and donkor et al 2014 for examples stochastic models may take several forms stochastic poisson rectangular pulse prp models generate rectangular pulses that represent sub daily scale residential water demands and are typically applied to water quality modeling in drinking water distribution systems they simulate pulse arrival time intensity flow and duration creaco et al 2017 although such models require expensive flow measurements at short time intervals to determine prp model parameters blokker et al 2010 their results have been shown to closely match observed household and aggregated 21 household water demands at short time scales of 1 s to 1 h creaco et al 2017 prp based models can also reproduce specific residential end uses of water blokker et al 2010 and offer the potential of projecting the results of changes both in appliance efficiencies and in human behavior over the longer term creaco et al 2017 stochastic models for long term projections can be generated for example from monte carlo simulations of daily temperature and precipitation values and combined with a deterministic water demand model yung et al 2011 or simpler multiple linear regression models that generate monthly demands over several decades haque et al 2014 artificial intelligence methods include artificial neural networks ann fuzzy inference systems agent based modeling qi and chang 2011 house peters and chang 2011 support vector machine extreme learning machine mouatadid and adamowski 2017 and other approaches ann in particular has been widely used for water demand projections typically for short term forecasts of hours to months ghalehkhondabi et al 2017 qi and chang 2011 a data driven approach anns offer excellent predictive capacity require fewer assumptions than traditional statistical models ghalehkhondabi et al 2017 and can model non linear relationships between water demand consumption and explanatory variables qi and chang 2011 for the projection of future demands for example a set of dynamic ann models developed by ghiassi et al 2008 for urban water demand forecasting at daily weekly and monthly time scales produced accuracies above 99 and multiple studies have concluded that ann models outperform both multiple regression and time series models house peters and chang 2011 ann models are relatively straightforward to develop and validate often simply from historical water demands however their accuracy depends significantly on the quantity and quality of historical data billings and jones 2008 ferraro and price 2011 as well as on the learning algorithm used to train the ann model house peters and chang 2011 ghalehkhondabi et al 2017 further to prepare ann models researchers must select appropriate input variables by pruning their initial selections into a final set of predictors select among available ann network configurations and determine the appropriate number of hidden layers and hidden nodes in the network they must also separate training from testing data yung et al 2011 finally the relationships between explanatory variables and water demand represented as coefficients in the regression equations or ann models are time invariant and are therefore less appropriate for long term planning donkor et al 2014 such models also have limited ability to improve understanding of the components and dynamics of municipal water system or assess alternative management strategies since the explanatory variables are not usually tied to specific water end uses or conservation policies finally system dynamics sd models attempt to replicate real world physical structures and processes in this case to simulate the water end use processes that together produce the total municipal demand system dynamics provides both conceptual and quantitative methods to represent simulate and aid exploration of complex feedback and non linear interactions among system components management actions and performance indicators elsawah et al 2017 and its causal descriptive mathematical models barlas 1994 explicitly model the feedback structures stock and flow processes and delays that characterize real world systems sterman 2000 the approach has been used widely for water resources policy assessment and decision making through alternative scenario building sensitivity analysis and gaming winz et al 2009 mirchi et al 2012 chen and wei 2014 alessi and kopainsky 2015 savic et al 2016 sd models are often developed in a participatory fashion that increases the chance that model solutions will be identified and accepted brown et al 2015 inam et al 2015 and that allows researchers policy makers managers and the public to evaluate proposed management actions identify trade offs and improve their understanding of system behaviour williams et al 2009 municipal water systems have well established end uses such as toilet flushing kitchen uses showering lawn watering commercial uses and so on that are focuses of different demand management programs deoreo et al 2016 system dynamics can model these individual end uses through a structural approach and its simulated end use water demands and their summation to total residential and commercial demands for example can then help managers to plan municipal systems for capacity expansions for specific water users analyze policy environmental and economic trade offs explain root causes of system behavior show consequences of alternative actions clearly and potentially mitigate conflict among various end users stave 2003 municipal water managers usually implement several policies simultaneously making estimation of the effectiveness of individual options difficult especially under population growth and climate change uncertainties tanaka et al 2006 sd models can be used to reveal individual and combined effects of policies through scenario investigations assess effects of new interventions and identify the end uses with the highest consumption hussien et al 2017 and test policy effects over a wide range of plausible futures through sensitivity analyses gober et al 2011 municipal water systems also have important cause and effect interactions for example a water shortage in one year with associated water rationing could also increase water conservation efforts and thereby mitigate the impact of future shortages pacific institute and nrdc 2014 such interactions can be investigated through the incorporation of feedback mechanisms in which the initial cause produces action that ultimately reduces the effects of future causes to support management and planning at the strategic level mirchi et al 2012 thus sd models allow decision makers to investigate both changes in human behavior and technological changes in fixtures and appliances by assembling various changes in policy and model parameters into scenarios and then comparing their results examples of sd applications to municipal water management include qi and chang 2011 gober et al 2011 qaiser et al 2011 ahmad and prashar 2010 and stave 2003 finally despite these advantages it is important to note that sd models can be data intensive when compared with the other methods above for example an end use based model requires specific values for various water end uses and for the effects of various demand management options and its adaptation to other locations could potentially necessitate in field data collection to establish average fixture counts and usage outdoor use patterns and conservation program adoption rates and effectiveness further the stakeholder engagement efforts recommended in the sd literature may be time consuming however once developed an end use based sd model has a relatively transparent structure that is easy to understand and modify and that provides comprehensive and clear results to users and decision makers through alternative scenarios wang and davies 2015 and monte carlo simulations the approach can also produce good results where data are limited for example cwmm performs well as described below using a combination of average north american municipal water use data from deoreo et al 2016 and location specific data for calgary alberta 2 2 model characteristics and capabilities the level of model disaggregation significantly affects the abilities and accuracy of municipal water demand models billings and jones 2008 where models with more customer categories provide greater insight into sources of demands and effects of management programs and can also help to improve model accuracy the level of aggregation depends on utility size modeling method water metering characteristics and data availability for example small utilities may not be able to categorize customers because of potentially excessive volatility within customer categories or inadequate data therefore such systems are usually modeled with time series and regression methods billings and jones 2008 utilities that use customer categories and collect data on water end uses and conservation program effectiveness may be able to model specific water end uses typical data collection approaches include billing data analysis customer interviews surveys home audits retrofit studies flow data recorders and flow trace analysis software deoreo et al 2016 for more information see grafton et al 2011 mayeret al 1999 and deoreo et al 2016 who describe data collection efforts in the areas of demographics attitudinal and consumptive behaviors and household water end uses and the physical characteristics of houses and landscapes in contrast with time series regression and artificial intelligence models which usually focus on general customer categories a variety of sd models include residential industrial commercial and institutional customer categories for example gober et al 2011 simulated both residential and commercial uses and qaiser et al 2011 and stave 2003 included indoor and outdoor residential water uses a more detailed municipal sd model developed by ahmad and prashar 2010 included residential end uses such as kitchen toilet bath laundry and outdoor uses as well as public commercial and industrial uses while hussien et al 2017 projected both a larger number of household water end uses that relied on a detailed water use survey described in hussien et al 2016 as well as annual household energy and food consumption to 2050 note that other recent studies using different methodologies have also applied a residential water end use based structure including a prp based stochastic model at the daily time scale blokker et al 2010 and an agent based model at an annual time scale chu et al 2009 the simulation time step also affects model characteristics and capabilities in general small time steps from hourly to seasonal are used for short less than a year and medium term from one to ten years models these models are developed for system operations and assessment of pumping requirements with focuses on demand changes under a fixed or slowly changing customer base and seasonal variations in demands for example billings and jones 2008 time series regression analysis and artificial neural network model are common methods here see examples provided by donkor et al 2014 in contrast longer time steps up to the annual scale are usually used for long term often a decade or more planning and management to address questions related to infrastructure sizing billings and jones 2008 uncertainties associated with changing socio economic conditions and climate change see for example donkor et al 2014 qi and chang 2011 gober et al 2011 and qaiser et al 2011 historically water demand models have focused on economic factors as driving residential water demand arbués et al 2003 march and saurí 2009 grafton et al 2011 therefore demand models often incorporate economic factors as water use drivers and include their effects on the viability of management options for example water price and family income are widely included in time series and regression models through estimation of price and income elasticities march and saurí 2009 however a variety of studies have shown that residential water demand is rather inelastic arbués et al 2003 and actually responds more to income climate institutional characteristics and demographics than to price palazzo et al 2017 conservation efforts also play an important role and utilities in canada have faced a reduction in revenues di matteo 2016 cbc news 2015 as household water use has decreased over the past several decades across north america deoreo et al 2016 at the same time the utilities must upgrade their water systems interestingly the apparent solution has been to increase the fixed component of the water bill and reduce the component related to variable consumption charges however this reduces the economic incentive for households to conserve water di matteo 2016 several recent studies have analyzed municipal management policies in terms of trade offs between various water supply expansion options system rehabilitation options and water reuse infrastructure for example chung et al 2009 used an optimization model that incorporated system reliability to minimize the economic cost of municipal supply design they found a lower cost for expanding an existing water treatment plant than for building a new canal or pipeline rehan et al 2011 analyzed the effects of rehabilitation strategies on the financial sustainability of water and wastewater services through changing user fees barton and argue 2009 investigated cost implications of water reuse in a residential water planning model and found that reuse infrastructure increased construction costs but that these expenses were largely offset by savings in fees and charges on the demand side olmstead and stavins 2009 compared price and non price measures such as water restrictions on water conservation and generally found price based approaches to be more cost effective and more straightforward for monitoring and enforcement finally climate change presents uncertainties for demand modeling in terms of magnitude timing and possibly even the direction of changes in climate variables house peters and chang 2011 an additional complication is that estimates of changing water availability runoff with climate change do not equal changes in water supply because of complexity of water supply systems which include water storage transmission systems treatment systems and operating rules paton et al 2013 relatively few studies have explicitly modeled climate change effects on municipal water demand and those studies that have included such effects typically found them to produce less significant effects than changes in population or water conservation efforts haque et al 2014 for example haque et al 2014 projected water demands to differ by 4 ml year between three climate scenarios while water use restrictions led to maximum reductions of 38 ml year in 2040 fowler et al 2003 conducted a comprehensive study of the reliability resilience and vulnerability of the yorkshire uk water system to historical and projected water resources droughts they found that climate change is likely to drive increasing vulnerability of yorkshire water resources to severe drought events but their study omitted changing demands over the simulated period to 2080 as well as alternative operating scenarios for the water supply system yung et al 2011 assessed the risk to the ayr ontario canada water supply system from population growth extreme projections of climate change and demand management and or supply expansion to 2025 in contrast to the results above they found system reliability to differ only slightly between the population growth and climate change scenarios while resiliency was better for the population growth scenario and vulnerability was worse demand management had negligible effect while supply expansion significantly improved all risk based measures finally paton et al 2013 assessed relative sources of uncertainty on water security for the southern adelaide australia water supply system they included differences in climate scenarios gcms demand projections and stochastic rainfall time series and found changing demand to be the greatest source of uncertainty 3 study area and data availability calgary is the largest city in alberta and had a population of 1 2 million people about 37 of the provincial population in 2014 city of calgary 2014 further the city has had the highest annual growth rate among all canadian cities over the last five years with a 3 8 growth rate from 2012 to 2013 statistics canada 2016 and is projected to reach a population of 2 4 million by 2041 government of alberta 2015 the city is concerned about water use sustainability city of calgary 2011 since the supply is limited by both its water license and the water treatment plant capacity in terms of the water license future water availability for calgary is limited by the closure of the south saskatchewan river basin to new water allocations in 2006 province of alberta 2007 while municipal consumption has averaged 72 of the peak day production capacity over the past decade boulton chaykowski 2016 at a regional scale water is shared by diverse upstream and downstream users including farmers and irrigation districts industrial users and recreational activities ali and klein 2014 percy 2005 and their water demands are also increasing with population growth adding to the challenges of population growth and limited allocations are potential impacts of climate change calgary depends heavily on consistent river flows primarily from the annual snowpack and glacial meltwater the city has experienced increased temperatures which can significantly increase outdoor demands and decreased river flow over the last 100 years natural resources canada 2007 and such trends are expected to continue rood et al 2016 however climate change impacts on temperature and precipitation are unclear recent studies project changes in seasonal precipitation of 15 to 25 while seasonal temperatures may increase by up to 4 2 c in alberta by the 2050s jiang et al 2015 for the bow river basin changes of monthly mean precipitation could range from 1 to 8 and monthly mean temperature is projected to increase by 2 c 4 5 c by the 2050s tanzeeba and gan 2012 the combined impact of these changes could decrease water availability see section 2 2 as well as increase outdoor water demands during periods of hot dry weather house peters and chang 2011 and lengthen watering seasons 3 1 water supply and demand calgary is located in one of the driest regions in canada with average annual precipitation and evapotranspiration of 413 mm and 405 mm respectively alberta government 2013 calgary relies on two surface water sources the bow and elbow rivers which originate in the rocky mountains west of calgary and flow eastward through the canadian prairies fig 1 from these two water sources the city has an annual licensed withdrawal of 460 million m3 aep 2015 two water treatment plants bearspaw and glenmore plants located along the bow and elbow rivers respectively are capable of providing up to 1 million m3 of drinking water to the city each day city of calgary 2015 in 2013 the average residential water use in calgary was 231 l per capita per day lpcd city of calgary 2013 which is 64 of the north american residential average of 360 lpcd deoreo et al 2016 calgary has reduced its per capita daily demand from an average total per capita demand of 500 lpcd in 2003 to 406 lpcd in 2010 city of calgary 2011 through implementation of water conservation policies and low flow technologies specific data are not available for calgary but table 1 shows the average indoor fixture and appliance usage in 1999 and 2016 for north american cities deoreo et al 2016 despite these per capita decreases in water use calgary s total water demand continues to increase total water demand has almost doubled from 1972 98 mcm or million m3 headwater communication 2007 to 2015 170 mcm boulton chaykowski 2016 as the population has grown from approximately 400 000 in 1971 statistics canada 1977 to 1 4 million in 2015 statistics canada 2016 major water uses in calgary fig 2a include residential industrial commercial and institutional ici non revenue street cleaning firefighting and losses and wholesale supply to nearby communities residential indoor uses are typical for a north american city deoreo et al 2016 as shown in fig 2b outdoor water use accounted for about 12 of total annual use in 2007 headwater communications 2007 a value much lower than the north american average of 50 deoreo et al 2016 mainly because of the short growing season in alberta calgary s outdoor use is known to increase significantly when weekly mean temperatures are higher than 10 c chen et al 2006 and is also affected by weekly rainfall akuoko asibey et al 1993 in normal years summer demand can be 170 of winter demand and up to 250 of winter demand in hot and dry years natural resources canada 2007 3 2 water management and conservation calgary uses the following evaluation criteria to assess its water management options water conservation effectiveness such as per capita demand reduction and peak daily demand reduction cost effectiveness per m3 of water conserved and social impacts of water shortage such as unmet water demand city of calgary 2010 for the past decade calgary has adhered to a 30 in 30 plan headwater communications 2007 which targets the same volume of water withdrawal in 2033 as in 2003 by reducing per capita water demand by 30 percent from 500 lpcd to 350 lpcd over those 30 years a 2015 assessment found that the city was on track to its goal city of calgary 2016 calgary has implemented water metering encouraged adoption of water efficient appliances such as low flow toilets offered economic incentives improved leak detection efforts and educated citizens about water use and conservation for example calgary s water metering rate increased from 44 5 in 1996 to 97 in 2014 and the low flow toilet incentive program awarded 75 000 rebates from 2003 to 2014 boulton chaykowski 2016 the prevalence and water conservation effectiveness of a variety of water management policies are discussed in the supplementary material see table s1 4 calgary water management model cwmm the calgary water management model cwmm is a system dynamics model that simulates municipal water demand and use and treatment plant water withdrawals at a weekly timescale beginning in 1996 and ending in 2040 to aid both near term evaluation of water use characteristics and long term planning and management of regional water resources its novelty lies in its application of a system dynamics framework at a weekly time step to municipal water management its ability to reveal the water requirements of specific end uses and the relative effects of both technological and policy changes and its ability to simulate conservation policy effects and their economic trade offs cwmm includes interactions between increases in municipal water demands with population growth the available water supply and the effects on demand of climate change and water conservation policies including those described in table s1 water metering low flow appliances rain barrels leak management educational programs economic incentives water rationing greywater reuse and xeriscaping the model is described briefly below a detailed description is provided in the supplementary material 4 1 model structures and interface in each simulation run cwmm first simulates the water demands of various end uses in liters per capita per day and then sums them to generate total per capita water demands the model subdivides municipal water demand into ten end use categories based on mayer et al 1999 and coomes et al 2010 toilet shower and bath laundry kitchen leaks other outdoor ici industrial commercial and institutional non revenue for street cleaning and firefighting for example and extra municipal water wholesale uses see fig 3 the per capita daily water demand of each residential indoor end use is calculated according to a combination of the base per use demand number of daily uses and fraction of households equipped with water conserving fixtures and appliances table 1 per capita outdoor demands are based on weekly temperature and rainfall and modify a set of climate based relationships developed for calgary by chen et al 2006 and akuoko asibey et al 1993 see the simulation of the per capita municipal water demand section in the supplementary material for further details to calculate the total municipal demand the model multiplies the per capita demands with the municipal population and adds ici non revenue and wholesale demands this total demand is compared with the available water supply to determine the weekly water withdrawal finally the withdrawn water is used to satisfy water demands and if demands cannot be fully satisfied a water shortage is generated and represented as an unmet water demand as explained in wang and davies 2015 this unmet demand can drive more rapid adoption of low flow fixtures and appliances and the implementation of conservation policies such as rationing the effects of water conservation policies are simulated either by increasing the adoption rates of low flow fixtures and appliances or by decreasing specific end use demands directly policy application costs are modeled using a conservation cost per unit m3 multiplied by the amount of water conserved the adoption cost of each low flow appliance or xeriscaping of each property multiplied by the change in percentage of low flow appliance adoptions or the percentage of properties converted from turf to xeriscaping the model has a user friendly interface fig 4 that facilitates the construction of various population and climate change scenarios as well as policy selection among the available management policies sliders are used to select among policies for example 0 means off and 1 means on or to set policy application intensities to specific values the model interface also displays results such as weekly and per capita water demands for specific water end uses or can display results from multiple scenarios simultaneously to permit comparison among different options finally cwmm supports both a scenario mode where pre set values are used for a single continuous simulation run from 1996 through 2040 and a gaming mode which allows users to refine policy selections at a week by week to longer time step over the 1996 2040 time period 4 2 model validation the following tests were used to validate the model barlas 1994 sterman 2000 1 model structure and parameter tests confirmed that mathematical equations and interrelationships adequately represent the corresponding system in the real world 2 extreme conditions tests ensured that the model generated reasonable results even with extreme values assigned to model parameters 3 sensitivity analyses permitted investigation of the model responsiveness to important uncertainties in model equations and parameters and 4 key model outputs such as per capita water demand weekly water demand and total water withdrawal were compared with historical values to ensure that the model could replicate historical behavior fig 5a compares observed and simulated water demands for the historical period of 2005 2015 the first three tests and key model assumptions are discussed in the model validation and model assumptions sections of the supplementary material further several statistical performance measures were used to quantify differences between the simulated and observed municipal water use at a weekly scale the coefficient of determination r2 and root mean square error rmse were used to evaluate respectively the magnitude of variance explained by the model compared with the total observed variance and the average differences between simulated and observed demands in mcm million m3 the normalized root mean square error nrmse represents the normalized rmse in and the mean bias error mbe indicates over underestimation in mcm by cwmm in particular use of rmse and mbe both measured in mcm would permit utilities to estimate potential effects on costs and revenues associated with model error performance measure results for the 2005 2015 period used an average actual weekly demand of 3 3 mcm and produced values as follows r2 0 80 rmse 0 19 mcm nrmse 5 76 and mbe 0 08 mcm although these values are relatively poorer than those achievable by other methods described in section 2 it is important to recall that these values represent a full decade of simulated water consumption figures at a weekly scale with relatively small error we also calculated performance measures for several representative years to provide more detailed comparisons between simulated and observed data at a weekly time scale see fig 5b and c for the details of 2009 fig 5b and 2014 fig 5c and table 2 for six years of the historical period for 2007 2012 and 2014 the simulated demands were quite close to the observed values in contrast model performance in 2005 2009 and 2015 was relatively poorer the simulated and observed patterns differed in several important ways 1 during winter seasons the simulated water use fluctuated less it is affected only by population growth and low flow fixture or appliance adoption in cwmm than the observed values since neither seasonal behavioral changes nor specific events such as water main breaks street cleaning and sewer line flushing fire event responses social events conferences expositions or sporting events holiday periods or car washing boulton chaykowski 2016 were modeled and 2 during summer seasons simulated weekly water use varied both upwards and downwards from observed values because of the model time step and simplifications and the unpredictability of outdoor water use behavior despite these differences in most simulated years the winter demand as simulated in cwmm is representative of the observed relatively flat pattern see fig 5b and c with notable deviations in 2012 13 and 2013 14 in terms of summer values the effects of sub weekly weather variations on outdoor water use accounted for many of the differences between simulated and observed values for example in the week of june 1 2009 the simulated demand value 3 1 mcm was lower than the observed value 3 7 mcm because weekly mean temperature and rainfall values of 8 9 c and 8 6 mm substantially reduced simulated outdoor use fig 5b however daily weather data show two cold rainy days june 5 and 6 during this period with mean temperatures of 5 9 c and 4 2 c and rainfall of 5 4 mm and 3 2 mm while the preceding several weeks were hot and dry which suggests that outdoor demands were still high during that week prior to june 5 and 6 5 results and discussion 5 1 model forecasting and water demand scenarios four scenario groups with a total of seventeen scenarios were developed for an investigation of calgary s potential future water demands under various degrees of population growth climate change and water conservation policy implementations see table 3 for the scenario details water use which can be lower than water demand under water stress conditions is discussed in some scenarios where appropriate conservation policies are applied from 2018 onwards and scenario comparisons are shown for the period of 2015 2040 scenario group 1 focuses on climate change and population growth effects these are the most commonly assessed variables in water demand studies and provide comparison points for other scenarios first climate change scenarios that included increased temperature decreased streamflow and increased precipitation were simulated while population growth rates were held fixed several recent studies have projected 10 20 increases in temperature and precipitation and decreases in streamflow due to higher evapotranspiration jiang et al 2015 rood et al 2016 tanzeeba and gan 2012 during spring and summer seasons in alberta while another recent study revealed a rate of global warming from 2000 to 2015 as fast as or even exceeding that of the last half of the 20th century noaa 2016 therefore the scenarios here included 10 20 and 30 changes in climate variables under normal population growth conditions second three different population growth rates relative to the normal growth rate were tested under constant climate conditions the normal population growth rate was adapted from historical and projected population values provided by the government of alberta 2015 with an average of 2 6 each year while the low growth scenario applied a 10 reduction in the growth rate a value that actually occurred in 2010 and the higher growth scenarios applied 10 and 20 increases to the normal growth rate under these three scenarios the population of calgary reaches 2 1 2 5 and 2 7 million people by 2040 respectively in scenario group 2 three scenarios with different population growth rates climate change conditions and water management policy adoptions were tested against calgary s 30 in 30 water management goals they were called the high water demand hwd business as usual bau and low water demand lwd scenarios see table 3 these scenarios were intended to represent a realistic range of future water demand conditions where hwd had a high population growth rate moderate climate change and minimal effort to conserve water using only low flow appliances and rain barrels bau had a normal population growth rate without climate change and continued historical management policies and land use trends such as economic incentives and a fixed percentage of households with xeriscaping and lwd also used normal population growth rates without climate change and implemented xeriscaping and greywater reuse policies more broadly scenario group 3 focused on policy trade offs with four policies economic incentives greywater reuse xeriscaping and water rationing adopted separately in 2018 their water use social impacts represented by unmet water demands and application costs were simulated under high population growth and climate change conditions note that the costs of economic incentives programs were based on the city of calgary s toilet rebate program headwater communications 2007 the municipal share of greywater system and water rationing costs were australian values taken from alberta watersmart 2011 and turner et al 2007 and xeriscaping costs were estimated from a xeriscaping program in southern nevada usa sovocool 2005 although all costs were adjusted to the same base year 2016 canadian dollars the simulated policy costs may not be representative of the situation in calgary scenario group 4 focused on greywater reuse in terms of conservation impact and policy cost where the individual scenarios applied various greywater reuse intensities under high population growth and climate change conditions four greywater reuse scenarios represented percentages 20 50 80 and 100 of homes and ici users with greywater reuse further based on assumptions in alberta watersmart 2011 the first three scenarios also imposed water demand reductions of 50 for toilets 20 for outdoor use and 10 for ici purposes while the fourth scenario 100 greywater reuse reduced all end use demands by 50 except kitchen water uses note that the greywater reuse policy is phased in over three years for all scenarios of this group 5 2 model forecasting and scenario results model outputs for the six scenarios of group 1 are shown in fig 6 the applied climate change conditions clearly increased maximum weekly water demands during summer seasons fig 6a by increasing outdoor water demand they also produced longer outdoor watering seasons specifically the changes in climate increased maximum seasonal demands in all three scenarios with a range from a minimum of 4 9 million m3 in summer 2018 to a maximum of 9 3 million m3 in summer 2040 for the ng cc 10 and ng cc 30 scenarios respectively further maximum demand for ng cc 30 was 1 and 9 higher than the ng cc 10 in 2018 and 2040 respectively interestingly during lower water demand seasons ng cc 30 had relatively lower demand than the other scenarios because of a model feedback that altered model behavior in response to repeated water shortages higher unmet summer demands drove relatively greater adoption of water conserving fixtures and appliances such as low flow toilets and washing machines which then resulted in greater conservation indoors during the winter after 2038 all three scenarios were close to their maximum low flow fixture adoption levels and therefore had similar winter demands fig 6a finally at an annual scale outdoor water demand represented only about 10 of the total and water use patterns were assumed not to change under climate change therefore climate change did not change calgary s annual water demand significantly however cwmm does not simulate changes in water supply and at a weekly scale the potential for significant increases in maximum water demands during warmer summers of the future may require further study compared to the climate change scenarios weekly water demand was significantly more sensitive to population growth fig 6b specifically the population growth scenarios significantly increased water demand in all seasons over the simulation period for example the maximum demand of pg 20 nc was greater than pg 10 nc by less than 1 in 2018 and by more than 20 in 2040 for the three scenarios of group 2 the model produced significantly different values for annually averaged per capita daily water demands fig 7 from 2015 onward the decreasing trends of all three scenarios slowed as compared with the historical period since water metering was already 97 in 2014 and low flow shower head and toilet adoption percentages approached their theoretical maxima 85 according to rogers 1995 therefore hwd and bau had annually averaged per capita daily demand values of 426 lpcd and 372 lpcd respectively from 2020 to 2040 to continue to decrease water demands a broader implementation of greywater reuse and xeriscaping policies simulated by lwd reduced the per capita daily demand to 340 lpcd in 2020 below the management goal of 350 lpcd in addition to total municipal demand the model also simulated the water demands of each end use comparing these end uses across scenarios clarifies the impacts of population growth climate change and management policies on the total demands shown in fig 7 see fig 8 which shows the components of the annual peak daily per capita demand for the three scenarios compared to bau hwd had higher leakage losses 50 higher and outdoor water demand about 16 higher because of reduced leak management efforts and gradually increasing temperatures after 2018 with climate change in the lwd scenario toilet outdoor and ici water demands decreased from bau levels by 20 35 and 4 respectively as a result of broader implementation of greywater and xeriscaping policies after 2018 scenario group 3 illustrates weekly water use and unmet demand values for four different conservation options under high population growth and climate change conditions from 2018 to 2040 see fig 9 to compare these policies adoption of each conservation option was simulated individually cwmm simulated similar water use levels for the economic incentives and xeriscaping scenarios but xeriscaping reduced summer season weekly water use by a maximum of 10 a difference of 540 000 m3 in the peak demand week and increased non summer season water use by 0 5 compared with economic incentives the reason for the difference was that xeriscaping only reduced outdoor use during the summer season while economic incentives affected indoor outdoor and ici uses in all seasons by increasing the adoption rate of low flow fixtures and appliances which reduced non summer uses relative to the xeriscaping scenario see fig 9a under water stress conditions after 2022 both scenarios used the maximum water supply during the summer water rationing the third scenario of group 3 had very similar water use 0 2 higher to economic incentives in all seasons however compared to xeriscaping its summer use was 10 higher because water rationing used all the available water during the high demand summer season while use in other seasons was 1 lower as a result of a 2 greater low flow fixture and appliance adoption rate fig 9a this high adoption rate resulted from the high level of unmet demand during summer fig 9b which drove a greater low flow appliance adoption rate through model feedbacks finally greywater reuse the fourth policy of the group was the most effective policy in reducing weekly water use in all seasons it produced a 14 reduction in water use and a low unmet demand under both stress and no stress conditions fig 9b group 3 scenario costs were also simulated based on details provided in the simulation of water policies section of the supplementary material their order ranked from lowest to highest cost was rationing economic incentives xeriscaping and greywater reuse fig 10a compares economic incentives with water rationing costs and shows economic incentive costs decreasing from about 1200 to nearly 0 per week from 2018 to 2040 this decreasing trend resulted from the saturation of households with low flow fixtures and appliances for example low flow toilets were installed in close to 70 and 80 of homes in 2018 and 2040 respectively therefore there was little uptake of the economic subsidy later in the simulation period in contrast water rationing had a relatively low cost when applied during water shortages fig 10a but caused high unmet water demand fig 9b xeriscaping had the highest cost of all four scenarios in the first application year fig 10b approximately 1450 million per year for a 100 application rate for the conversion of all lawns in the city to xeriscaping however costs were relatively lower in the following years approximately 60 million per year to provide 100 of new homes with xeriscaping fig 10b finally greywater reuse had the highest total cost at 570 million per year over three years for a 100 application rate of the normal reuse scenario as well as the longest implementation time fig 10b note that all implementation costs are approximate and that implementation times for the xeriscaping and greywater reuse policies are modeled with delays that can be changed to simulate different conditions in other cities cwmm can also calculate the unit water saving cost m3 saved not shown here for further comparison of policies finally scenario group 4 tested several application intensities of the greywater reuse policy under high population growth and climate change conditions differences among the weekly water demands of the hg cc gr 20 50 and 80 scenarios were not large with a maximum 8 difference fig 11a between them the reasons are that their major effect is on toilet water demand reduced by 50 which represents 14 of the total municipal water demand and outdoor and ici uses which represent about 10 and 28 of the total municipal demand and are reduced by 20 and 10 respectively in contrast full application of the greywater reuse policy hg cc gr 100 dramatically reduced total weekly water demand by 44 compared with the hg cc gr 20 scenario for example fig 11a in reality of course the efficacy of the greywater reuse policy would depend heavily on the quality of treated water and its suitability for different water end uses as well as the implementation cost fig 11b here the municipal share of the application cost for the 100 policy was approximately three times higher at 120 million per year or 80 million per year above the next highest scenario than in the other three scenarios 5 3 discussion application of cwmm to calgary revealed that adoption of several water conservation policies will ensure water availability under future population growth a limited water license and uncertainties from climate change the model showed that weekly water demand was more sensitive to population growth than to climate change further city of calgary is on its way to achieving its 30 in 30 goal and may achieve further demand reductions through a wider application of xeriscaping and greywater treatment and reuse programs at an annual scale these policies could reduce water demand per person per day by 10 relative to the bau scenario in particular xeriscaping reduced the maximum municipal water use during the summer season by an average of 10 while adoption of low flow appliances and greywater reuse affected end uses in all seasons water rationing resulted in high unmet water demands here a maximum of 40 of the total demand however it is a relatively inexpensive conservation approach indeed implementation costs represent the main trade off of conservation policies for both the city and its citizens for example greywater reuse in cwmm reduced water use by 14 during hot and dry conditions as compared with water rationing but required approximately 60 million per year for three years from the city government in one scenario 100 application with normal reuse to implement the required infrastructure note that such economic figures are uncertain and require further research 6 conclusions this paper introduced a system dynamics model for calgary alberta as an alternative to commonly used time series analysis regression analysis stochastic modeling and artificial intelligence ai approaches the calgary water management model cwmm supports water resources management and planning under various population growth and climate change conditions and the implementation of alternative water conservation policies intended for use as a seasonal to decadal scale decision support tool cwmm adopts 1 a process based end use oriented structure with ten individual water end uses 2 a weekly calculation time step and 3 a user friendly data entry system and graphical user interface the end use based structure permits simulation of the impacts of nine water management policies including conservation education leak management economic incentives water rationing low flow appliances rain barrels water metering xeriscaping and greywater reuse on specific municipal end uses further the weekly time step reveals seasonal water demand patterns and their responses to various management policies and facilitates investigation of system capacity in high water demand seasons as well as alternative climate change scenarios several model limitations will be addressed through future research although adoption of low flow fixtures and appliances is at least partially an economic decision the current version of cwmm does not incorporate water price the addition of water pricing would permit simulation of potential changes in demand with changes in price the relative cost effectiveness of alternative system expansion or conservation options comparative policy payback times for options such as xeriscaping and greywater reuse and could improve the simulation of low flow fixture adoption rates in response to price signals however all of these changes would require data collection for example the first model improvement would require data that correlate economic drivers to specific water end uses the second and third would require detailed location specific economic data and projected trends and the last would require customer interviews and surveys as well as the inclusion of other possibly important signals beyond price palazzo et al 2017 the inclusion of water price and water supply infrastructure e g rehan et al 2011 could also permit an examination of the positive feedback between decreasing water consumption falling utility revenue and rising rehabilitation costs and increasing price and possibly show effects of the shift toward a larger fixed cost component in water bills other model improvements could incorporate variable winter demands if correlations could be established between climate variables and water main breaks in cold weather car washing during warm periods residential outdoor use for hockey rinks and spas and institutional commercial and industrial ici uses for example disaggregation of residential uses to detached and multi unit housing and to regions of the city would permit 1 a more detailed analysis of infrastructure needs and 2 more sophisticated model validation approaches such an approach would also allow a more detailed representation of turf and landscape irrigation and potential effects of changing land use over time disaggregation of industrial commercial and institutional ici water end use would allow investigation of alternative industrial and commercial development pathways as well as sector specific policies finally daily or even hourly time steps could be simulated if data at these levels were available the city of calgary was the focus of the model development but the resulting cwmm framework is suitable for other cities facing similar water resources management and planning challenges cwmm is intended to provide comprehensive decision support for municipal water managers planners researchers and modelers a modified model may also assist with city water resources management and assessment drought preparedness and mitigation long term water resources planning and public engagement and education acknowledgements the authors thank a boulton chaykowski and others at the city of calgary for their interest and help and for the data they provided this research was supported by a grant from the alberta land institute at the university of alberta canada appendix a supplementary data the following are the supplementary data related to this article online data online data online data online data appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 12 024 
26442,this study introduces an end use based system dynamics model to support municipal water planning and management over the medium to long term the calgary water management model cwmm simulates water demand and use to 2040 at a weekly time step for ten municipal end uses as well as the effects of population growth climate change and various water management policies and includes policy implementation costs for assessment of conservation versus economic trade offs the model was validated against historical water demand data for calgary alberta a series of scenario simulations showed 1 potentially large changes to both seasonal and non seasonal water demands with climate change and population growth 2 a need to enhance historical water management policies with new policies such as xeriscaping and greywater reuse to achieve water management goals and 3 the value of an end use based model in simulating management policy effects on municipal water demand and use keywords water demand model municipal water conservation decision support tool scenario analysis system dynamics calgary software availability software name cwmm calgary water management model developers k wang and e davies university of alberta year first available 2017 program language vensim hardware requirements no special requirements software required vensim model reader ventana systems inc availability and cost please contact the corresponding author for a free copy 1 introduction population growth and climate change present challenges for water resources planners and managers mcdonald et al 2011 and water security is increasingly of concern to urban authorities grafton et al 2011 yigzaw and hossain 2016 on the water supply side a variety of studies have investigated effects of changing precipitation patterns glacial retreat and sea level rise on hydrological variables cf dibike et al 2016 eshtawi et al 2016 scalzitti et al 2016 as well as urban hydraulic infrastructure expansions and management padowski and jawitz 2012 mays 2002 water authorities also recognize the value of managing water demand the focus of this research which is less time intensive and more cost effective and environmentally friendly than supply side management house peters and chang 2011 gleick 2003 municipal water systems serve residential industrial commercial institutional and public clients mayer et al 1999 whose demands are affected by both long term impact factors population change economic conditions and water conservation activities and short term impact factors including seasonal weather patterns and the associated summer peak demands march and saurí 2009 in most urban systems of north america the total water demand increases with population growth while per capita water use decreases with water conservation efforts such as adoptions of low flow fixtures and appliances educational campaigns water metering and consumption feedback leak detection programs economic incentives xeriscaping and water treatment and reuse billings and jones 2008 sønderlund et al 2016 deoreo et al 2016 reliable water demand modeling and forecasting provides the basis for both the short term operational and long term planning aspects of urban water management in terms of capital investment infrastructure expansion conflict mitigation policy analysis and system optimization and it can improve understanding of the underlying factors and dynamics that affect water demand and use billings and jones 2008 however accurate demand forecasting and analysis are challenging because of 1 the limited quantity and quality of data brown 2002 2 the numerous variables and drivers that affect demand march and saurí 2009 3 the high uncertainties associated with climate change economic conditions population growth and conservation activities gober et al 2011 4 the complexity of a quantitative analysis of water conservation options and their implementation costs billings and jones 2008 and 5 the different model horizons required for short term and long term purposes both of which affect water security donkor et al 2014 this paper presents a novel end use based model as a decision support tool for municipal water management that addresses many of the challenges above intended for tactical 1 10 years and strategic long term use see table 1 in donkor et al 2014 the model simulates short weekly and long term 10 years municipal water demand and use patterns under various climate change population growth and water conservation scenarios and reveals management trade offs such as the potential water savings and economic costs of alternative water management policies it includes ten specific end uses with seven residential end uses six indoor and one outdoor use and three non residential uses and simulates per capita water demand based on the number of water fixture uses per day and their associated water requirements which are affected by water conservation policies as well as appliance and fixture characteristics called the calgary water management model or cwmm the model runs fast a single simulation on a windows desktop takes a fraction of a second is easy to use requires relatively few input data and matches the historical municipal demands in calgary alberta while also permitting exploration of various plausible water scenarios into the future a system dynamics model cwmm can be adapted to other municipalities by changing model inputs stored in a ms excel file and refined from a whole city scale to represent individual neighborhoods or city regions the paper is structured as follows first municipal water modeling methodologies and models are reviewed then the research area and data availability are presented in terms of water supply demand and management conditions the model is next described and validated using data for calgary and sample results such as per capita water demand management policy conservation effectiveness and policy cost are presented finally the paper closes with conclusions a discussion of model limitations and potential next steps for the research 2 municipal water management and modeling a wide range of methods can be used for municipal water management and modeling with the selection depending on modeler skill available resources and data and accuracy requirements methods such as time series analysis regression analysis stochastic modeling artificial intelligence ai and system dynamics sd are discussed in this section as well as modeling concerns related to water customer disaggregation modeling time step economic considerations and climate change note that many of these methods such as time series and regression models or artificial neural networks and regression models are often used in combination to produce hybrid models that typically improve water demand forecasting performance over the use of the individual methods donkor et al 2014 2 1 available modeling methods water demand projections rely on estimated population growth and per capita water demands and modeling methods differ primarily in their treatment of the latter time series models predict per capita demand based on historical trends using moving averages exponential smoothing or autoregressive integrated moving average methods billings and jones 2008 and with fine temporal scale data can reveal significant temporal trends in water consumption correlated with economic variables as well as weather and climate factors house peters and chang 2011 4 regression models have also been used extensively historically donkor et al 2014 and employ social and economic factors called explanatory variables including water price house and lot size water saving technologies family income education and gender to estimate per capita or family water consumption through linear log linear or exponential models billings and jones 2008 these drivers are analyzed in terms of temporal and spatial scales house peters and chang 2011 direct and indirect impacts on water demand jorgensen et al 2009 and economic and non economic factors march and saurí 2009 both approaches require typically modest computing power house peters and chang 2011 but do not generally account for population growth or water conservation efforts therefore they are usually applied to short term demand prediction typically less than a year for small utilities see qi and chang 2011 house peters and chang 2011 and donkor et al 2014 for examples stochastic models may take several forms stochastic poisson rectangular pulse prp models generate rectangular pulses that represent sub daily scale residential water demands and are typically applied to water quality modeling in drinking water distribution systems they simulate pulse arrival time intensity flow and duration creaco et al 2017 although such models require expensive flow measurements at short time intervals to determine prp model parameters blokker et al 2010 their results have been shown to closely match observed household and aggregated 21 household water demands at short time scales of 1 s to 1 h creaco et al 2017 prp based models can also reproduce specific residential end uses of water blokker et al 2010 and offer the potential of projecting the results of changes both in appliance efficiencies and in human behavior over the longer term creaco et al 2017 stochastic models for long term projections can be generated for example from monte carlo simulations of daily temperature and precipitation values and combined with a deterministic water demand model yung et al 2011 or simpler multiple linear regression models that generate monthly demands over several decades haque et al 2014 artificial intelligence methods include artificial neural networks ann fuzzy inference systems agent based modeling qi and chang 2011 house peters and chang 2011 support vector machine extreme learning machine mouatadid and adamowski 2017 and other approaches ann in particular has been widely used for water demand projections typically for short term forecasts of hours to months ghalehkhondabi et al 2017 qi and chang 2011 a data driven approach anns offer excellent predictive capacity require fewer assumptions than traditional statistical models ghalehkhondabi et al 2017 and can model non linear relationships between water demand consumption and explanatory variables qi and chang 2011 for the projection of future demands for example a set of dynamic ann models developed by ghiassi et al 2008 for urban water demand forecasting at daily weekly and monthly time scales produced accuracies above 99 and multiple studies have concluded that ann models outperform both multiple regression and time series models house peters and chang 2011 ann models are relatively straightforward to develop and validate often simply from historical water demands however their accuracy depends significantly on the quantity and quality of historical data billings and jones 2008 ferraro and price 2011 as well as on the learning algorithm used to train the ann model house peters and chang 2011 ghalehkhondabi et al 2017 further to prepare ann models researchers must select appropriate input variables by pruning their initial selections into a final set of predictors select among available ann network configurations and determine the appropriate number of hidden layers and hidden nodes in the network they must also separate training from testing data yung et al 2011 finally the relationships between explanatory variables and water demand represented as coefficients in the regression equations or ann models are time invariant and are therefore less appropriate for long term planning donkor et al 2014 such models also have limited ability to improve understanding of the components and dynamics of municipal water system or assess alternative management strategies since the explanatory variables are not usually tied to specific water end uses or conservation policies finally system dynamics sd models attempt to replicate real world physical structures and processes in this case to simulate the water end use processes that together produce the total municipal demand system dynamics provides both conceptual and quantitative methods to represent simulate and aid exploration of complex feedback and non linear interactions among system components management actions and performance indicators elsawah et al 2017 and its causal descriptive mathematical models barlas 1994 explicitly model the feedback structures stock and flow processes and delays that characterize real world systems sterman 2000 the approach has been used widely for water resources policy assessment and decision making through alternative scenario building sensitivity analysis and gaming winz et al 2009 mirchi et al 2012 chen and wei 2014 alessi and kopainsky 2015 savic et al 2016 sd models are often developed in a participatory fashion that increases the chance that model solutions will be identified and accepted brown et al 2015 inam et al 2015 and that allows researchers policy makers managers and the public to evaluate proposed management actions identify trade offs and improve their understanding of system behaviour williams et al 2009 municipal water systems have well established end uses such as toilet flushing kitchen uses showering lawn watering commercial uses and so on that are focuses of different demand management programs deoreo et al 2016 system dynamics can model these individual end uses through a structural approach and its simulated end use water demands and their summation to total residential and commercial demands for example can then help managers to plan municipal systems for capacity expansions for specific water users analyze policy environmental and economic trade offs explain root causes of system behavior show consequences of alternative actions clearly and potentially mitigate conflict among various end users stave 2003 municipal water managers usually implement several policies simultaneously making estimation of the effectiveness of individual options difficult especially under population growth and climate change uncertainties tanaka et al 2006 sd models can be used to reveal individual and combined effects of policies through scenario investigations assess effects of new interventions and identify the end uses with the highest consumption hussien et al 2017 and test policy effects over a wide range of plausible futures through sensitivity analyses gober et al 2011 municipal water systems also have important cause and effect interactions for example a water shortage in one year with associated water rationing could also increase water conservation efforts and thereby mitigate the impact of future shortages pacific institute and nrdc 2014 such interactions can be investigated through the incorporation of feedback mechanisms in which the initial cause produces action that ultimately reduces the effects of future causes to support management and planning at the strategic level mirchi et al 2012 thus sd models allow decision makers to investigate both changes in human behavior and technological changes in fixtures and appliances by assembling various changes in policy and model parameters into scenarios and then comparing their results examples of sd applications to municipal water management include qi and chang 2011 gober et al 2011 qaiser et al 2011 ahmad and prashar 2010 and stave 2003 finally despite these advantages it is important to note that sd models can be data intensive when compared with the other methods above for example an end use based model requires specific values for various water end uses and for the effects of various demand management options and its adaptation to other locations could potentially necessitate in field data collection to establish average fixture counts and usage outdoor use patterns and conservation program adoption rates and effectiveness further the stakeholder engagement efforts recommended in the sd literature may be time consuming however once developed an end use based sd model has a relatively transparent structure that is easy to understand and modify and that provides comprehensive and clear results to users and decision makers through alternative scenarios wang and davies 2015 and monte carlo simulations the approach can also produce good results where data are limited for example cwmm performs well as described below using a combination of average north american municipal water use data from deoreo et al 2016 and location specific data for calgary alberta 2 2 model characteristics and capabilities the level of model disaggregation significantly affects the abilities and accuracy of municipal water demand models billings and jones 2008 where models with more customer categories provide greater insight into sources of demands and effects of management programs and can also help to improve model accuracy the level of aggregation depends on utility size modeling method water metering characteristics and data availability for example small utilities may not be able to categorize customers because of potentially excessive volatility within customer categories or inadequate data therefore such systems are usually modeled with time series and regression methods billings and jones 2008 utilities that use customer categories and collect data on water end uses and conservation program effectiveness may be able to model specific water end uses typical data collection approaches include billing data analysis customer interviews surveys home audits retrofit studies flow data recorders and flow trace analysis software deoreo et al 2016 for more information see grafton et al 2011 mayeret al 1999 and deoreo et al 2016 who describe data collection efforts in the areas of demographics attitudinal and consumptive behaviors and household water end uses and the physical characteristics of houses and landscapes in contrast with time series regression and artificial intelligence models which usually focus on general customer categories a variety of sd models include residential industrial commercial and institutional customer categories for example gober et al 2011 simulated both residential and commercial uses and qaiser et al 2011 and stave 2003 included indoor and outdoor residential water uses a more detailed municipal sd model developed by ahmad and prashar 2010 included residential end uses such as kitchen toilet bath laundry and outdoor uses as well as public commercial and industrial uses while hussien et al 2017 projected both a larger number of household water end uses that relied on a detailed water use survey described in hussien et al 2016 as well as annual household energy and food consumption to 2050 note that other recent studies using different methodologies have also applied a residential water end use based structure including a prp based stochastic model at the daily time scale blokker et al 2010 and an agent based model at an annual time scale chu et al 2009 the simulation time step also affects model characteristics and capabilities in general small time steps from hourly to seasonal are used for short less than a year and medium term from one to ten years models these models are developed for system operations and assessment of pumping requirements with focuses on demand changes under a fixed or slowly changing customer base and seasonal variations in demands for example billings and jones 2008 time series regression analysis and artificial neural network model are common methods here see examples provided by donkor et al 2014 in contrast longer time steps up to the annual scale are usually used for long term often a decade or more planning and management to address questions related to infrastructure sizing billings and jones 2008 uncertainties associated with changing socio economic conditions and climate change see for example donkor et al 2014 qi and chang 2011 gober et al 2011 and qaiser et al 2011 historically water demand models have focused on economic factors as driving residential water demand arbués et al 2003 march and saurí 2009 grafton et al 2011 therefore demand models often incorporate economic factors as water use drivers and include their effects on the viability of management options for example water price and family income are widely included in time series and regression models through estimation of price and income elasticities march and saurí 2009 however a variety of studies have shown that residential water demand is rather inelastic arbués et al 2003 and actually responds more to income climate institutional characteristics and demographics than to price palazzo et al 2017 conservation efforts also play an important role and utilities in canada have faced a reduction in revenues di matteo 2016 cbc news 2015 as household water use has decreased over the past several decades across north america deoreo et al 2016 at the same time the utilities must upgrade their water systems interestingly the apparent solution has been to increase the fixed component of the water bill and reduce the component related to variable consumption charges however this reduces the economic incentive for households to conserve water di matteo 2016 several recent studies have analyzed municipal management policies in terms of trade offs between various water supply expansion options system rehabilitation options and water reuse infrastructure for example chung et al 2009 used an optimization model that incorporated system reliability to minimize the economic cost of municipal supply design they found a lower cost for expanding an existing water treatment plant than for building a new canal or pipeline rehan et al 2011 analyzed the effects of rehabilitation strategies on the financial sustainability of water and wastewater services through changing user fees barton and argue 2009 investigated cost implications of water reuse in a residential water planning model and found that reuse infrastructure increased construction costs but that these expenses were largely offset by savings in fees and charges on the demand side olmstead and stavins 2009 compared price and non price measures such as water restrictions on water conservation and generally found price based approaches to be more cost effective and more straightforward for monitoring and enforcement finally climate change presents uncertainties for demand modeling in terms of magnitude timing and possibly even the direction of changes in climate variables house peters and chang 2011 an additional complication is that estimates of changing water availability runoff with climate change do not equal changes in water supply because of complexity of water supply systems which include water storage transmission systems treatment systems and operating rules paton et al 2013 relatively few studies have explicitly modeled climate change effects on municipal water demand and those studies that have included such effects typically found them to produce less significant effects than changes in population or water conservation efforts haque et al 2014 for example haque et al 2014 projected water demands to differ by 4 ml year between three climate scenarios while water use restrictions led to maximum reductions of 38 ml year in 2040 fowler et al 2003 conducted a comprehensive study of the reliability resilience and vulnerability of the yorkshire uk water system to historical and projected water resources droughts they found that climate change is likely to drive increasing vulnerability of yorkshire water resources to severe drought events but their study omitted changing demands over the simulated period to 2080 as well as alternative operating scenarios for the water supply system yung et al 2011 assessed the risk to the ayr ontario canada water supply system from population growth extreme projections of climate change and demand management and or supply expansion to 2025 in contrast to the results above they found system reliability to differ only slightly between the population growth and climate change scenarios while resiliency was better for the population growth scenario and vulnerability was worse demand management had negligible effect while supply expansion significantly improved all risk based measures finally paton et al 2013 assessed relative sources of uncertainty on water security for the southern adelaide australia water supply system they included differences in climate scenarios gcms demand projections and stochastic rainfall time series and found changing demand to be the greatest source of uncertainty 3 study area and data availability calgary is the largest city in alberta and had a population of 1 2 million people about 37 of the provincial population in 2014 city of calgary 2014 further the city has had the highest annual growth rate among all canadian cities over the last five years with a 3 8 growth rate from 2012 to 2013 statistics canada 2016 and is projected to reach a population of 2 4 million by 2041 government of alberta 2015 the city is concerned about water use sustainability city of calgary 2011 since the supply is limited by both its water license and the water treatment plant capacity in terms of the water license future water availability for calgary is limited by the closure of the south saskatchewan river basin to new water allocations in 2006 province of alberta 2007 while municipal consumption has averaged 72 of the peak day production capacity over the past decade boulton chaykowski 2016 at a regional scale water is shared by diverse upstream and downstream users including farmers and irrigation districts industrial users and recreational activities ali and klein 2014 percy 2005 and their water demands are also increasing with population growth adding to the challenges of population growth and limited allocations are potential impacts of climate change calgary depends heavily on consistent river flows primarily from the annual snowpack and glacial meltwater the city has experienced increased temperatures which can significantly increase outdoor demands and decreased river flow over the last 100 years natural resources canada 2007 and such trends are expected to continue rood et al 2016 however climate change impacts on temperature and precipitation are unclear recent studies project changes in seasonal precipitation of 15 to 25 while seasonal temperatures may increase by up to 4 2 c in alberta by the 2050s jiang et al 2015 for the bow river basin changes of monthly mean precipitation could range from 1 to 8 and monthly mean temperature is projected to increase by 2 c 4 5 c by the 2050s tanzeeba and gan 2012 the combined impact of these changes could decrease water availability see section 2 2 as well as increase outdoor water demands during periods of hot dry weather house peters and chang 2011 and lengthen watering seasons 3 1 water supply and demand calgary is located in one of the driest regions in canada with average annual precipitation and evapotranspiration of 413 mm and 405 mm respectively alberta government 2013 calgary relies on two surface water sources the bow and elbow rivers which originate in the rocky mountains west of calgary and flow eastward through the canadian prairies fig 1 from these two water sources the city has an annual licensed withdrawal of 460 million m3 aep 2015 two water treatment plants bearspaw and glenmore plants located along the bow and elbow rivers respectively are capable of providing up to 1 million m3 of drinking water to the city each day city of calgary 2015 in 2013 the average residential water use in calgary was 231 l per capita per day lpcd city of calgary 2013 which is 64 of the north american residential average of 360 lpcd deoreo et al 2016 calgary has reduced its per capita daily demand from an average total per capita demand of 500 lpcd in 2003 to 406 lpcd in 2010 city of calgary 2011 through implementation of water conservation policies and low flow technologies specific data are not available for calgary but table 1 shows the average indoor fixture and appliance usage in 1999 and 2016 for north american cities deoreo et al 2016 despite these per capita decreases in water use calgary s total water demand continues to increase total water demand has almost doubled from 1972 98 mcm or million m3 headwater communication 2007 to 2015 170 mcm boulton chaykowski 2016 as the population has grown from approximately 400 000 in 1971 statistics canada 1977 to 1 4 million in 2015 statistics canada 2016 major water uses in calgary fig 2a include residential industrial commercial and institutional ici non revenue street cleaning firefighting and losses and wholesale supply to nearby communities residential indoor uses are typical for a north american city deoreo et al 2016 as shown in fig 2b outdoor water use accounted for about 12 of total annual use in 2007 headwater communications 2007 a value much lower than the north american average of 50 deoreo et al 2016 mainly because of the short growing season in alberta calgary s outdoor use is known to increase significantly when weekly mean temperatures are higher than 10 c chen et al 2006 and is also affected by weekly rainfall akuoko asibey et al 1993 in normal years summer demand can be 170 of winter demand and up to 250 of winter demand in hot and dry years natural resources canada 2007 3 2 water management and conservation calgary uses the following evaluation criteria to assess its water management options water conservation effectiveness such as per capita demand reduction and peak daily demand reduction cost effectiveness per m3 of water conserved and social impacts of water shortage such as unmet water demand city of calgary 2010 for the past decade calgary has adhered to a 30 in 30 plan headwater communications 2007 which targets the same volume of water withdrawal in 2033 as in 2003 by reducing per capita water demand by 30 percent from 500 lpcd to 350 lpcd over those 30 years a 2015 assessment found that the city was on track to its goal city of calgary 2016 calgary has implemented water metering encouraged adoption of water efficient appliances such as low flow toilets offered economic incentives improved leak detection efforts and educated citizens about water use and conservation for example calgary s water metering rate increased from 44 5 in 1996 to 97 in 2014 and the low flow toilet incentive program awarded 75 000 rebates from 2003 to 2014 boulton chaykowski 2016 the prevalence and water conservation effectiveness of a variety of water management policies are discussed in the supplementary material see table s1 4 calgary water management model cwmm the calgary water management model cwmm is a system dynamics model that simulates municipal water demand and use and treatment plant water withdrawals at a weekly timescale beginning in 1996 and ending in 2040 to aid both near term evaluation of water use characteristics and long term planning and management of regional water resources its novelty lies in its application of a system dynamics framework at a weekly time step to municipal water management its ability to reveal the water requirements of specific end uses and the relative effects of both technological and policy changes and its ability to simulate conservation policy effects and their economic trade offs cwmm includes interactions between increases in municipal water demands with population growth the available water supply and the effects on demand of climate change and water conservation policies including those described in table s1 water metering low flow appliances rain barrels leak management educational programs economic incentives water rationing greywater reuse and xeriscaping the model is described briefly below a detailed description is provided in the supplementary material 4 1 model structures and interface in each simulation run cwmm first simulates the water demands of various end uses in liters per capita per day and then sums them to generate total per capita water demands the model subdivides municipal water demand into ten end use categories based on mayer et al 1999 and coomes et al 2010 toilet shower and bath laundry kitchen leaks other outdoor ici industrial commercial and institutional non revenue for street cleaning and firefighting for example and extra municipal water wholesale uses see fig 3 the per capita daily water demand of each residential indoor end use is calculated according to a combination of the base per use demand number of daily uses and fraction of households equipped with water conserving fixtures and appliances table 1 per capita outdoor demands are based on weekly temperature and rainfall and modify a set of climate based relationships developed for calgary by chen et al 2006 and akuoko asibey et al 1993 see the simulation of the per capita municipal water demand section in the supplementary material for further details to calculate the total municipal demand the model multiplies the per capita demands with the municipal population and adds ici non revenue and wholesale demands this total demand is compared with the available water supply to determine the weekly water withdrawal finally the withdrawn water is used to satisfy water demands and if demands cannot be fully satisfied a water shortage is generated and represented as an unmet water demand as explained in wang and davies 2015 this unmet demand can drive more rapid adoption of low flow fixtures and appliances and the implementation of conservation policies such as rationing the effects of water conservation policies are simulated either by increasing the adoption rates of low flow fixtures and appliances or by decreasing specific end use demands directly policy application costs are modeled using a conservation cost per unit m3 multiplied by the amount of water conserved the adoption cost of each low flow appliance or xeriscaping of each property multiplied by the change in percentage of low flow appliance adoptions or the percentage of properties converted from turf to xeriscaping the model has a user friendly interface fig 4 that facilitates the construction of various population and climate change scenarios as well as policy selection among the available management policies sliders are used to select among policies for example 0 means off and 1 means on or to set policy application intensities to specific values the model interface also displays results such as weekly and per capita water demands for specific water end uses or can display results from multiple scenarios simultaneously to permit comparison among different options finally cwmm supports both a scenario mode where pre set values are used for a single continuous simulation run from 1996 through 2040 and a gaming mode which allows users to refine policy selections at a week by week to longer time step over the 1996 2040 time period 4 2 model validation the following tests were used to validate the model barlas 1994 sterman 2000 1 model structure and parameter tests confirmed that mathematical equations and interrelationships adequately represent the corresponding system in the real world 2 extreme conditions tests ensured that the model generated reasonable results even with extreme values assigned to model parameters 3 sensitivity analyses permitted investigation of the model responsiveness to important uncertainties in model equations and parameters and 4 key model outputs such as per capita water demand weekly water demand and total water withdrawal were compared with historical values to ensure that the model could replicate historical behavior fig 5a compares observed and simulated water demands for the historical period of 2005 2015 the first three tests and key model assumptions are discussed in the model validation and model assumptions sections of the supplementary material further several statistical performance measures were used to quantify differences between the simulated and observed municipal water use at a weekly scale the coefficient of determination r2 and root mean square error rmse were used to evaluate respectively the magnitude of variance explained by the model compared with the total observed variance and the average differences between simulated and observed demands in mcm million m3 the normalized root mean square error nrmse represents the normalized rmse in and the mean bias error mbe indicates over underestimation in mcm by cwmm in particular use of rmse and mbe both measured in mcm would permit utilities to estimate potential effects on costs and revenues associated with model error performance measure results for the 2005 2015 period used an average actual weekly demand of 3 3 mcm and produced values as follows r2 0 80 rmse 0 19 mcm nrmse 5 76 and mbe 0 08 mcm although these values are relatively poorer than those achievable by other methods described in section 2 it is important to recall that these values represent a full decade of simulated water consumption figures at a weekly scale with relatively small error we also calculated performance measures for several representative years to provide more detailed comparisons between simulated and observed data at a weekly time scale see fig 5b and c for the details of 2009 fig 5b and 2014 fig 5c and table 2 for six years of the historical period for 2007 2012 and 2014 the simulated demands were quite close to the observed values in contrast model performance in 2005 2009 and 2015 was relatively poorer the simulated and observed patterns differed in several important ways 1 during winter seasons the simulated water use fluctuated less it is affected only by population growth and low flow fixture or appliance adoption in cwmm than the observed values since neither seasonal behavioral changes nor specific events such as water main breaks street cleaning and sewer line flushing fire event responses social events conferences expositions or sporting events holiday periods or car washing boulton chaykowski 2016 were modeled and 2 during summer seasons simulated weekly water use varied both upwards and downwards from observed values because of the model time step and simplifications and the unpredictability of outdoor water use behavior despite these differences in most simulated years the winter demand as simulated in cwmm is representative of the observed relatively flat pattern see fig 5b and c with notable deviations in 2012 13 and 2013 14 in terms of summer values the effects of sub weekly weather variations on outdoor water use accounted for many of the differences between simulated and observed values for example in the week of june 1 2009 the simulated demand value 3 1 mcm was lower than the observed value 3 7 mcm because weekly mean temperature and rainfall values of 8 9 c and 8 6 mm substantially reduced simulated outdoor use fig 5b however daily weather data show two cold rainy days june 5 and 6 during this period with mean temperatures of 5 9 c and 4 2 c and rainfall of 5 4 mm and 3 2 mm while the preceding several weeks were hot and dry which suggests that outdoor demands were still high during that week prior to june 5 and 6 5 results and discussion 5 1 model forecasting and water demand scenarios four scenario groups with a total of seventeen scenarios were developed for an investigation of calgary s potential future water demands under various degrees of population growth climate change and water conservation policy implementations see table 3 for the scenario details water use which can be lower than water demand under water stress conditions is discussed in some scenarios where appropriate conservation policies are applied from 2018 onwards and scenario comparisons are shown for the period of 2015 2040 scenario group 1 focuses on climate change and population growth effects these are the most commonly assessed variables in water demand studies and provide comparison points for other scenarios first climate change scenarios that included increased temperature decreased streamflow and increased precipitation were simulated while population growth rates were held fixed several recent studies have projected 10 20 increases in temperature and precipitation and decreases in streamflow due to higher evapotranspiration jiang et al 2015 rood et al 2016 tanzeeba and gan 2012 during spring and summer seasons in alberta while another recent study revealed a rate of global warming from 2000 to 2015 as fast as or even exceeding that of the last half of the 20th century noaa 2016 therefore the scenarios here included 10 20 and 30 changes in climate variables under normal population growth conditions second three different population growth rates relative to the normal growth rate were tested under constant climate conditions the normal population growth rate was adapted from historical and projected population values provided by the government of alberta 2015 with an average of 2 6 each year while the low growth scenario applied a 10 reduction in the growth rate a value that actually occurred in 2010 and the higher growth scenarios applied 10 and 20 increases to the normal growth rate under these three scenarios the population of calgary reaches 2 1 2 5 and 2 7 million people by 2040 respectively in scenario group 2 three scenarios with different population growth rates climate change conditions and water management policy adoptions were tested against calgary s 30 in 30 water management goals they were called the high water demand hwd business as usual bau and low water demand lwd scenarios see table 3 these scenarios were intended to represent a realistic range of future water demand conditions where hwd had a high population growth rate moderate climate change and minimal effort to conserve water using only low flow appliances and rain barrels bau had a normal population growth rate without climate change and continued historical management policies and land use trends such as economic incentives and a fixed percentage of households with xeriscaping and lwd also used normal population growth rates without climate change and implemented xeriscaping and greywater reuse policies more broadly scenario group 3 focused on policy trade offs with four policies economic incentives greywater reuse xeriscaping and water rationing adopted separately in 2018 their water use social impacts represented by unmet water demands and application costs were simulated under high population growth and climate change conditions note that the costs of economic incentives programs were based on the city of calgary s toilet rebate program headwater communications 2007 the municipal share of greywater system and water rationing costs were australian values taken from alberta watersmart 2011 and turner et al 2007 and xeriscaping costs were estimated from a xeriscaping program in southern nevada usa sovocool 2005 although all costs were adjusted to the same base year 2016 canadian dollars the simulated policy costs may not be representative of the situation in calgary scenario group 4 focused on greywater reuse in terms of conservation impact and policy cost where the individual scenarios applied various greywater reuse intensities under high population growth and climate change conditions four greywater reuse scenarios represented percentages 20 50 80 and 100 of homes and ici users with greywater reuse further based on assumptions in alberta watersmart 2011 the first three scenarios also imposed water demand reductions of 50 for toilets 20 for outdoor use and 10 for ici purposes while the fourth scenario 100 greywater reuse reduced all end use demands by 50 except kitchen water uses note that the greywater reuse policy is phased in over three years for all scenarios of this group 5 2 model forecasting and scenario results model outputs for the six scenarios of group 1 are shown in fig 6 the applied climate change conditions clearly increased maximum weekly water demands during summer seasons fig 6a by increasing outdoor water demand they also produced longer outdoor watering seasons specifically the changes in climate increased maximum seasonal demands in all three scenarios with a range from a minimum of 4 9 million m3 in summer 2018 to a maximum of 9 3 million m3 in summer 2040 for the ng cc 10 and ng cc 30 scenarios respectively further maximum demand for ng cc 30 was 1 and 9 higher than the ng cc 10 in 2018 and 2040 respectively interestingly during lower water demand seasons ng cc 30 had relatively lower demand than the other scenarios because of a model feedback that altered model behavior in response to repeated water shortages higher unmet summer demands drove relatively greater adoption of water conserving fixtures and appliances such as low flow toilets and washing machines which then resulted in greater conservation indoors during the winter after 2038 all three scenarios were close to their maximum low flow fixture adoption levels and therefore had similar winter demands fig 6a finally at an annual scale outdoor water demand represented only about 10 of the total and water use patterns were assumed not to change under climate change therefore climate change did not change calgary s annual water demand significantly however cwmm does not simulate changes in water supply and at a weekly scale the potential for significant increases in maximum water demands during warmer summers of the future may require further study compared to the climate change scenarios weekly water demand was significantly more sensitive to population growth fig 6b specifically the population growth scenarios significantly increased water demand in all seasons over the simulation period for example the maximum demand of pg 20 nc was greater than pg 10 nc by less than 1 in 2018 and by more than 20 in 2040 for the three scenarios of group 2 the model produced significantly different values for annually averaged per capita daily water demands fig 7 from 2015 onward the decreasing trends of all three scenarios slowed as compared with the historical period since water metering was already 97 in 2014 and low flow shower head and toilet adoption percentages approached their theoretical maxima 85 according to rogers 1995 therefore hwd and bau had annually averaged per capita daily demand values of 426 lpcd and 372 lpcd respectively from 2020 to 2040 to continue to decrease water demands a broader implementation of greywater reuse and xeriscaping policies simulated by lwd reduced the per capita daily demand to 340 lpcd in 2020 below the management goal of 350 lpcd in addition to total municipal demand the model also simulated the water demands of each end use comparing these end uses across scenarios clarifies the impacts of population growth climate change and management policies on the total demands shown in fig 7 see fig 8 which shows the components of the annual peak daily per capita demand for the three scenarios compared to bau hwd had higher leakage losses 50 higher and outdoor water demand about 16 higher because of reduced leak management efforts and gradually increasing temperatures after 2018 with climate change in the lwd scenario toilet outdoor and ici water demands decreased from bau levels by 20 35 and 4 respectively as a result of broader implementation of greywater and xeriscaping policies after 2018 scenario group 3 illustrates weekly water use and unmet demand values for four different conservation options under high population growth and climate change conditions from 2018 to 2040 see fig 9 to compare these policies adoption of each conservation option was simulated individually cwmm simulated similar water use levels for the economic incentives and xeriscaping scenarios but xeriscaping reduced summer season weekly water use by a maximum of 10 a difference of 540 000 m3 in the peak demand week and increased non summer season water use by 0 5 compared with economic incentives the reason for the difference was that xeriscaping only reduced outdoor use during the summer season while economic incentives affected indoor outdoor and ici uses in all seasons by increasing the adoption rate of low flow fixtures and appliances which reduced non summer uses relative to the xeriscaping scenario see fig 9a under water stress conditions after 2022 both scenarios used the maximum water supply during the summer water rationing the third scenario of group 3 had very similar water use 0 2 higher to economic incentives in all seasons however compared to xeriscaping its summer use was 10 higher because water rationing used all the available water during the high demand summer season while use in other seasons was 1 lower as a result of a 2 greater low flow fixture and appliance adoption rate fig 9a this high adoption rate resulted from the high level of unmet demand during summer fig 9b which drove a greater low flow appliance adoption rate through model feedbacks finally greywater reuse the fourth policy of the group was the most effective policy in reducing weekly water use in all seasons it produced a 14 reduction in water use and a low unmet demand under both stress and no stress conditions fig 9b group 3 scenario costs were also simulated based on details provided in the simulation of water policies section of the supplementary material their order ranked from lowest to highest cost was rationing economic incentives xeriscaping and greywater reuse fig 10a compares economic incentives with water rationing costs and shows economic incentive costs decreasing from about 1200 to nearly 0 per week from 2018 to 2040 this decreasing trend resulted from the saturation of households with low flow fixtures and appliances for example low flow toilets were installed in close to 70 and 80 of homes in 2018 and 2040 respectively therefore there was little uptake of the economic subsidy later in the simulation period in contrast water rationing had a relatively low cost when applied during water shortages fig 10a but caused high unmet water demand fig 9b xeriscaping had the highest cost of all four scenarios in the first application year fig 10b approximately 1450 million per year for a 100 application rate for the conversion of all lawns in the city to xeriscaping however costs were relatively lower in the following years approximately 60 million per year to provide 100 of new homes with xeriscaping fig 10b finally greywater reuse had the highest total cost at 570 million per year over three years for a 100 application rate of the normal reuse scenario as well as the longest implementation time fig 10b note that all implementation costs are approximate and that implementation times for the xeriscaping and greywater reuse policies are modeled with delays that can be changed to simulate different conditions in other cities cwmm can also calculate the unit water saving cost m3 saved not shown here for further comparison of policies finally scenario group 4 tested several application intensities of the greywater reuse policy under high population growth and climate change conditions differences among the weekly water demands of the hg cc gr 20 50 and 80 scenarios were not large with a maximum 8 difference fig 11a between them the reasons are that their major effect is on toilet water demand reduced by 50 which represents 14 of the total municipal water demand and outdoor and ici uses which represent about 10 and 28 of the total municipal demand and are reduced by 20 and 10 respectively in contrast full application of the greywater reuse policy hg cc gr 100 dramatically reduced total weekly water demand by 44 compared with the hg cc gr 20 scenario for example fig 11a in reality of course the efficacy of the greywater reuse policy would depend heavily on the quality of treated water and its suitability for different water end uses as well as the implementation cost fig 11b here the municipal share of the application cost for the 100 policy was approximately three times higher at 120 million per year or 80 million per year above the next highest scenario than in the other three scenarios 5 3 discussion application of cwmm to calgary revealed that adoption of several water conservation policies will ensure water availability under future population growth a limited water license and uncertainties from climate change the model showed that weekly water demand was more sensitive to population growth than to climate change further city of calgary is on its way to achieving its 30 in 30 goal and may achieve further demand reductions through a wider application of xeriscaping and greywater treatment and reuse programs at an annual scale these policies could reduce water demand per person per day by 10 relative to the bau scenario in particular xeriscaping reduced the maximum municipal water use during the summer season by an average of 10 while adoption of low flow appliances and greywater reuse affected end uses in all seasons water rationing resulted in high unmet water demands here a maximum of 40 of the total demand however it is a relatively inexpensive conservation approach indeed implementation costs represent the main trade off of conservation policies for both the city and its citizens for example greywater reuse in cwmm reduced water use by 14 during hot and dry conditions as compared with water rationing but required approximately 60 million per year for three years from the city government in one scenario 100 application with normal reuse to implement the required infrastructure note that such economic figures are uncertain and require further research 6 conclusions this paper introduced a system dynamics model for calgary alberta as an alternative to commonly used time series analysis regression analysis stochastic modeling and artificial intelligence ai approaches the calgary water management model cwmm supports water resources management and planning under various population growth and climate change conditions and the implementation of alternative water conservation policies intended for use as a seasonal to decadal scale decision support tool cwmm adopts 1 a process based end use oriented structure with ten individual water end uses 2 a weekly calculation time step and 3 a user friendly data entry system and graphical user interface the end use based structure permits simulation of the impacts of nine water management policies including conservation education leak management economic incentives water rationing low flow appliances rain barrels water metering xeriscaping and greywater reuse on specific municipal end uses further the weekly time step reveals seasonal water demand patterns and their responses to various management policies and facilitates investigation of system capacity in high water demand seasons as well as alternative climate change scenarios several model limitations will be addressed through future research although adoption of low flow fixtures and appliances is at least partially an economic decision the current version of cwmm does not incorporate water price the addition of water pricing would permit simulation of potential changes in demand with changes in price the relative cost effectiveness of alternative system expansion or conservation options comparative policy payback times for options such as xeriscaping and greywater reuse and could improve the simulation of low flow fixture adoption rates in response to price signals however all of these changes would require data collection for example the first model improvement would require data that correlate economic drivers to specific water end uses the second and third would require detailed location specific economic data and projected trends and the last would require customer interviews and surveys as well as the inclusion of other possibly important signals beyond price palazzo et al 2017 the inclusion of water price and water supply infrastructure e g rehan et al 2011 could also permit an examination of the positive feedback between decreasing water consumption falling utility revenue and rising rehabilitation costs and increasing price and possibly show effects of the shift toward a larger fixed cost component in water bills other model improvements could incorporate variable winter demands if correlations could be established between climate variables and water main breaks in cold weather car washing during warm periods residential outdoor use for hockey rinks and spas and institutional commercial and industrial ici uses for example disaggregation of residential uses to detached and multi unit housing and to regions of the city would permit 1 a more detailed analysis of infrastructure needs and 2 more sophisticated model validation approaches such an approach would also allow a more detailed representation of turf and landscape irrigation and potential effects of changing land use over time disaggregation of industrial commercial and institutional ici water end use would allow investigation of alternative industrial and commercial development pathways as well as sector specific policies finally daily or even hourly time steps could be simulated if data at these levels were available the city of calgary was the focus of the model development but the resulting cwmm framework is suitable for other cities facing similar water resources management and planning challenges cwmm is intended to provide comprehensive decision support for municipal water managers planners researchers and modelers a modified model may also assist with city water resources management and assessment drought preparedness and mitigation long term water resources planning and public engagement and education acknowledgements the authors thank a boulton chaykowski and others at the city of calgary for their interest and help and for the data they provided this research was supported by a grant from the alberta land institute at the university of alberta canada appendix a supplementary data the following are the supplementary data related to this article online data online data online data online data appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 12 024 
26443,despite various criticisms of glue generalized likelihood uncertainty estimation it is still a widely used uncertainty analysis technique in hydrologic modelling that can give an appreciation of the level and sources of uncertainty we introduce an augmented glue approach based on a gaussian process gp emulator involving gp to conduct a bayesian sensitivity analysis to narrow down the influential factor space and then performing a standard glue uncertainty analysis this approach is demonstrated for a swat soil and water assessment tool application in a watershed in china using a calibration and two validation periods results show 1 the augmented approach led to the screening out of 14 18 unimportant factors effectively narrowing factor space 2 compared to the more standard glue it substantially improved the sampling efficiency and located the optimal factor region at lower computational cost this approach can be used for other uncertainty analysis techniques in hydrologic and non hydrologic models keywords uncertainty analysis hydrologic modelling sensitivity analysis gaussian process emulator glue 1 introduction distributed hydrological modelling is a way to understand site specific hydrology and support operational management planning and decision making in water resources under various scenarios e g water use land use change or climate change arnold et al 2015 prior to its application models generally go through a so called calibration process although the hydrologic and other environmental modeling communities have generally promoted the concept and desirability of uncertainty analysis ua e g beven and freer 2001 todini 2007 jakeman et al 2006 there is still a need for its more widespread general practice especially for distributed hydrologic modelling muleta and nicklow 2005 yen et al 2015 for example most applications are still based on reporting a single optimum parameter set fang et al 2015 liu et al 2011 luo et al 2012 the reasons are multifold for example some uncertainty analysis techniques are difficult to apply e g the need for testing statistical assumptions in bayesian inference see yang et al 2008 for a discussion or only one parameter set is intended or sought for decision making song et al 2015 for complex environmental models as occur in distributed hydrologic modelling a restriction would often be the number of model runs required for the ua which can be a burden even with the ongoing advances in information technology e g increase of cpu speed and parallel computation technology various quantitative ua techniques have been developed or applied in the literature matott et al 2009 and there are now many societies and journals that promote ua in the hydrologic modelling literature such techniques include generalized likelihood uncertainty estimation or glue for short beven and binley 1992 sufi abbaspour et al 2007 first order approximation vrugt and bouten 2002 and bayesian inference kuczera and parent 1998 kavetski et al 2006 yang et al 2007 among all these techniques glue is still by far the most widely applied technique in hydrology shen et al 2012 stedinger et al 2008 due to its simplicity and practicality though it has been criticized for several reasons including its informal statistical basis sampling inefficiency and flat response surface mantovan and todini 2006 yang et al 2008 beven and binley 2014 on the other hand it can be argued that glue warrants use as a guide at least to appreciating the level of various sources of uncertainty when applying glue however one might face a substantial computational burden as captured in beven and binley 2014 page 5905 who state it remains an issue either because of a model that is particularly slow to run so that it is still not possible to sample sufficient realizations or because of a high number of parameter dimensions this often arises in spite of suggestions e g mcmillan and clark 2009 to seek increases in the efficiency of finding behavioral models it has been noted that the sample efficiency i e number of behavioral sets over number of sampling sets of some applications can be lower than 10 4 iorgulescu et al 2005 2007 yang et al 2008 over recent decades emulators or in some literature meta models or surrogate models have been widely applied as a surrogate to deterministic models partly to overcome the high computational cost of the latter but certain types of surrogates like gaussian processes can also be used to assess properties of the model response surface e g see asher et al 2015 for a review in the groundwater domain these emulators include polynomial regression jones 2001 multivariate adaptive regression splines friedman 1991 radial basis functions dyn et al 1986 polynomials chaos wiener 1938 xiu and karniadakis 2002 and gaussian processes kennedy and o hagan 2001 sacks et al 1989 most of these applications especially in hydrologic modelling have been for optimization emmerich et al 2006 jones 2001 and global sensitivity analysis oakley and o hagan 2004 ratto et al 2007 in this paper we propose an emulation augmented glue using gaussian process gp emulation of the original model to help conduct uncertainty analysis this glue gp uses global and local sensitivity analysis arising as a natural byproduct of the gp emulation to screen out unimportant parameters and reduce the ranges of more sensitive ones implemented in the software gem sa www tonyohagan co uk academic gem before application of glue the gp also allows improvement in the sampling efficiency and location of the optimal region for glue sampling at a much lower computational cost than the standard glue the glue gp method is thus akin to glue in the sense that it is an augmentation that applies a glue procedure but only to those factors and their ranges as informed by the initial gp emulation and its inherent sensitivity analysis thus it is an approximation of glue whose differences are numerically investigated here in both calibration and validation modes this approach is demonstrated on the semi distributed hydrologic model swat soil and water assessment tool arnold et al 1998 with an application to the kaidu river basin in xinjiang china which is an important water source for human activity and ecological function in the oasis downstream the remainder of this paper is structured as follows section 2 gives a brief introduction to glue and the gp emulator and then focuses on the proposed emulation based glue approach glue gp and case study section 3 introduces the swat model and case study area section 4 presents and discusses results and finally conclusions are summarized in section 5 2 methodology a large class of hydrologic and environmental models can be formulated as y f x where x x 1 x 2 x m is a vector of m factors and y is either scalar or vector model output e g flow rate time series or objective function e g root mean square error between simulated and observed flows and the notation x i j to indicate the jth realization of the ith factor of x here we distinguish factors from model parameters in that a factor could be a model parameter or a modification to a distributed parameter either in a relative way or with a replacement to their initial values examples are given in table 1 and section 3 2 thus we use the terminology factors instead of model parameters for the gp and uncertainty analyses below 2 1 glue glue beven and binley 1992 2014 is an uncertainty analysis technique inspired by the regional sensitivity analysis of hornberger and spear 1981 in contrast to assuming that there is a single optimal factor set for a model it is based on the concept of equifinality in which different behavioral factor sets lead to similarly good model results in some sense it recognizes that most environmental models used for prediction are non identifiable due largely but not only to the over parameterised structure of the model see shin et al 2015 for an overview of methods to check structural identifiability fig 1 a shows a typical procedure for uncertainty analysis based on glue when applying glue one needs to define an objective function l or generalized likelihood measure and a given threshold value which is used to assess if a sampled factor set is behavioral or non behavioral through a comparison if the corresponding likelihood measure is better or worse than the given threshold value each behavioral factor is then given a likelihood weight according to 1 w i l x i k 1 n l x k where l x i is the objective function value of factor set x i and n is the number of behavioral factor sets from nt total samples then the model predictive uncertainty is described as a prediction band from the cumulative distribution of the model output realized from the weighted behavioral factor sets based on these behavioral and non behavioral factor sets factor sensitivity can also be studied with regionalized sensitivity analysis rsa spear and hornberger 1980 which inspired glue but is not generally part of glue the idea of rsa is if the distributions of a factor in the behavioral and non behavioral factor sets are dissimilar then this factor is considered influential in practice this is performed with the kolmogorov smirnov test to obtain a distance measure d which is the maximum distance between the two empirical cumulative distributions i e behavioral and non behavioral 2 2 gaussian process emulator the emulator we invoke is the gaussian process emulator although other emulators such as those based on polynomial chaos wiener 1938 xiu and karniadakis 2002 have similar advantages mainly sampling efficiency improvements and global and local sensitivity measures as byproducts of the emulation largely because such emulators and their response surfaces are continuous functions which can be differentiated the idea is to construct a simpler and computationally efficient model as a surrogate for the complicated more physically based and less computationally efficient model when applying a gp emulator to a hydrologic model y f x it approximates f x as a gaussian process kennedy and o hagan 2001 2 f ˆ x m x e x h x t β e x where x i s a vector of factors m x is the mean function h x a known regression function and e x a zero mean gaussian process with correlation given by c o v f x f x σ 2 c x x in this study c x x is a correlation function between two points x and x that takes the form exp x x θ x x the quantities β σ 2 and θ are emulator parameters hyperparameters the key assumption is that f x is a smooth function no discontinuities jumps or sharp changes which allows us to predict f x if x is close to x and f x is known emulator hyperparameters i e β σ 2 and θ are estimated based on n design data points x i y i i 1 n as β ˆ σ ˆ 2 and θ ˆ bastos and o hagan 2009 based on bayesian inference 3 f x m x σ ˆ c x x t n m where m x h x t β ˆ t x t a 1 y d h β ˆ c x x c x x t x t a 1 t x h x t t x t a 1 h h t a 1 h 1 h x t t x t a 1 h t t x c x x 1 c x x n h t h x 1 t h x n t y d f x 1 f x n t a c x 1 x 1 c x 1 x n c x n x 1 c x n x n and t n m denotes a student s t distribution with n m degrees of freedom one advantage of a gp emulator is that it not only gives the prediction i e as a surrogate to the original model but also the underlying sensitivity of factors resulting from the emulation process since these types of integrals are easy to be solved analytically it is useful for statistical analysis of the emulated response surface for example variance based sensitivity analysis as explained in the section below 2 3 sensitivity analysis sensitivity analysis sa techniques can be categorized broadly into local sa and global sa based on the factor space of interest saltelli et al 2008 norton 2015 yang 2011 with global sa techniques presently being more popular among the different global sa techniques variance based sa is considered the most reliable saltelli 2002 it is based on anova analysis of variance decomposition i e the original model y f x can be decomposed into m main effects and m interactions 4 y f x g 0 i 1 m g i x i i j m g i j x i x j g 1 2 m x where g 0 e y g i x i e y x i g 0 g i j x i x j e y x i x j g i x i g j x j g 0 and so on the quantities g i x i and g i j x i x j are called the main effect and first order interaction respectively the sensitivities of the m main effects and m interactions sum to unity i e 5 i 1 m s i i j m s i j s 1 2 m 1 where s i v e y x i v y is the main effect index of factor i s i j v e y x i x j v y is the first order interaction index between factor x i and factor x j and so on the notation v denotes variance among these indices in eq 5 s i represents the average output variance reduction that can be achieved when x i is fixed additionally the total effect index s t i 1 v e y x i v y where x i is the subvector of x containing all elements except x i denotes the average output variance that would remain as long as x i stays unknown s t i is used to screen out unimportant factors and s i is used to determine the important factor to be fixed in the calibration process ratto et al 2007 traditional methods to estimate these indices from multiple samplings of the model factors include the sobol method saltelli 2002 sobol 1990 and the extended fast method saltelli et al 1999 however these methods are computationally expensive and emulator based methods have been proposed to reduce the required sampling and associated model runs e g oakley and o hagan 2004 ratto et al 2007 2 4 proposed augmented glue procedure glue gp the proposed uncertainty analysis procedure for combining glue and the gp emulation is shown in fig 1 b gp construction and sensitivity analysis are performed with the software gem sa compared to a typical glue in fig 1 a the main procedure is 1 based on a design dataset of n sampled points x i y i i 1 n i e designed factor sets and corresponding n model simulation results approximate y f x with the gp emulator by estimating its hyperparameters using the software gem sa www tonyohagan co uk academic gem 2 as a product of gem sa a bayesian sensitivity analysis oakley and o hagan 2004 is used to decrease model dimensions i e number of factors based on the total effect index s ti and to narrow factor ranges based on s i s ij and the expectations of e y x i and e y x i with a given cut off the cut off which is a different variable to the threshold in glue to determine behavioral and non behavioral factor sets is used to narrow factor ranges though it is subjective it should be within the range of minimum and maximum of all e y x i s and based on our experience the average of minimum and maximum all e y x i s is a good start though one can always look at values beyond our illustrative choices 3 nm points are sampled from the reduced and range narrowed factors and then the corresponding nm model simulation results are calculated 4 uncertainty analysis is then undertaken similarly to the standard glue it is noted that the final behavioral factor sets include all behavioral factor sets from n design points for gp construction at step 1 and nm points at step 3 to compare the performance of this glue gp procedure with the basic glue on the original model three measures are computed the e factor the percentage of behavioral sets identified p factor the percentage of observations bracketed by 95 prediction uncertainty 95ppu and r factor relative average width of 95ppu are calculated according to 6 e f a c t o r n b s n s 7 r f a c t o r t i t y t i 97 5 m y t i 2 5 m t σ f o b s where n b s and n s are the number of behavioral sets and number of total samples y t i 97 5 m and y t i 2 5 m t i 1 t represent the upper part and lower part of the 95 ppu of the simulated time series in the case of our model application the series is streamflow σ f o b s denotes the standard deviation of the measured time series data t is the number of model outputs in our case streamflow and m indices modelled the e factor quantity ranging from 0 to 100 is used to quantify the sampling efficiency whereas the p factor and r factor quantify the goodness of prediction uncertainty i e closeness of the p factor to 100 i e all observations bracketed by the prediction uncertainty and r factor to 0 i e achievement of rather small uncertainty band these three metrics have been invoked in ua using sufi abbaspour et al 2007 and in a ua technique comparison study e g yang et al 2008 2 5 sampling techniques to apply glue and glue gp one needs to sample from prior factor distributions there are various sampling techniques that have been applied in the literature the most frequently used include simple random sampling monte carlo markov chain monte carlo mcmc importance sampling latin hypercube sampling lhs and sobol sequencing simple random sampling often used with glue is an entirely random sampling technique however it tends to undersample small but possibly very important areas unless the sample size is significantly increased congalton 1991 lhs uses a stratified sampling scheme to improve on the coverage of factor space mckay et al 1979 and sobol sequencing belongs to the family of quasi random low discrepancy sequences sobol et al 2011 it uses a base of two to form successively finer uniform partitions of the unit interval and then reorders the coordinates in each dimension see more details in sobol et al 2011 some studies show that sobol sequences e g kucherenko et al 2016 singhee and rutenbar 2009 and lhs e g aistleitner et al 2012 have faster convergence than simple random sampling in this study we use the sobol sequence for sampling as it typically has a good coverage of factor space and is quite often used to compute sobol indices sobol 2001 however we encourage similar studies on other sampling techniques 3 swat model and study area 3 1 swat model swat soil and water assessment tool arnold et al 1998 is a semi distributed hydrologic model and has been widely used to assess the impact of different management practices and climate change on water resource quantity and quality at a watershed scale e g arnold and fohrer 2005 setegn et al 2011 it divides the watershed into sub basins within which a number of hydrologic response units are generated with a unique combination of landuse soil type and slope the sub basins are linked through a river network for surface water movement its climatic input consists of daily precipitation maximum minimum air temperature solar radiation wind speed and relative humidity in mountainous regions swat can use elevation bands and precipitation temperature lapse rates to account for orographic effects on precipitation and temperature for more details refer to the swat manuals http www brc tamus edu 3 2 study area and model setup the kaidu river basin with a drainage area of 18 634 km2 above the dashankou hydrological station is located in the tianshan mountains in northwest china fig 2 it is characterized as temperate continental with alpine climate as one of the headwaters of the tarim river it provides water resources for agricultural activity and the ecological environment of the oases in the lower reaches fang et al 2015 the altitude ranges from 1342 m to 4796 m above sea level a s l with an average elevation of 2995 m so the orographic effect on precipitation and temperature is significant and is represented here observed flow data are available either at daily or monthly scale from 1980 to 2010 and were used here daily observed meteorological data including precipitation maximum minimum temperature wind speed and relative humidity of two meteorological stations bayanbulak and baluntai locations are denoted by asterisks in fig 2 and are 2458 m and 1740 m a s l respectively are from the china meteorological data sharing service system http cdc cma gov cn the mean annual maximum and minimum temperature at the bayanbulak meteorological station are 3 1 c and 10 6 c and mean annual precipitation is 267 mm generally precipitation falls as rain from may to september and as snow from october to april of the next year the observed streamflow data at the dashankou hydrologic station the triangle in fig 2 are from the xinjiang tarim river basin management bureau the average daily flow is around 110 m3 s 1 equivalent to 185 mm of runoff per year ranging from 15 m3 s 1 to 973 m3 s 1 for more details refer to fang et al 2015 previously fang et al 2015 set up a swat model in this watershed with available dem digital elevation model land use soil and observed climate data and studied the contribution of meteorological inputs to hydrologic modelling results and the impact of climate change on the water resources based on a single optimized factor set the purpose of using the same case is to test our proposed uncertainty analysis approach for its advantages over a typical glue accepting that uncertainties could be reduced and characterized by either approach if better climate and streamflow information were available table 1 lists the initial 25 selected factors for the calibration which are assumed to be uniformly distributed as in fang et al 2015 factors distinct from model parameters can be modifications to model parameters either in a relative way or a replacement for example in table 1 the factor r sol k is a relative change to the distributed parameter sol k saturated hydraulic conductivity while v tlaps is a replacement of initial values of tlaps temperature lapse rate in order to lower the calibration dimension standardized root mean squared error srmse is invoked in the paper for demonstration as the so called likelihood measure objective function 8 s r m s e 1 t y t i m y t i 2 t σ f o b s where y t i is the observed outflow at time t i srmse can be related to the widely used nash sutcliffe coefficient ns through n s 1 s r m s e 2 t t 1 1 s r m s e 2 a value of 0 55 for example for s r m s e is equivalent to an ns value of 0 70 in this study glue and glue gp were first applied to a calibration period from 1986 to 1989 with daily flow data and then to two validation periods one from 1990 to 2002 with daily flow data and the other from 2003 to 2010 with monthly flow data which are the same as the calibration and validation periods in fang et al 2015 4 results based on an initial selected 25 factors table 1 which is a fairly standard level of parameterisation for a swat model and the likelihood measure srmse 600 sobol sequence samples were generated as design data and a corresponding 600 daily model simulations were obtained of these samples the first 300 were used to train the emulator and the remaining 300 for so called validation the emulation result is shown in fig 3 black dots signify training data and grey dots validation points it is not surprising that training data are on the 1 1 line which is characteristic of a gp emulator data for validation were scattered around the 1 1 line with an ns value of 0 82 for the emulated srmse fit to the original modelled srmse a similar result was obtained through a cross validation using the last 300 design samples to train the emulator and first 300 samples for validation as part of the gp construction a bayesian sensitivity analysis was undertaken for these 25 factors with gem sa software which uses the same samples as were generated for the gp construction the results are captured in table 2 and fig 4 in table 2 of the main effect indices s i the most sensitive factor is v tlaps followed by v alpha bf v plaps and v gwqmn while others are not as sensitive for total effect indices s ti the most sensitive factor is v tlaps followed by v alpha bf and then v gw delay v plaps and v gwqmn while others are not as sensitive the difference between the total effect index of a factor and its main effect index measures the interaction between this factor and other factors there are interactions for these mentioned factors especially for v tlaps and v alpha bf however all first order interaction indices i e s ij are around but less than 2 for example s ij between v tlaps and v alpha bf is 1 9 which denotes that first order interactions are low this result is similar to the result of an approach combining the morris method and sdp method applied in fang et al 2015 and their main effect indices and first order interactions are very close to our results here except that the approach here includes total effect indices and requires less model runs based on this analysis insensitive factors whose total effect indices are less than 2 were screened out which led to 11 factors being considered for further analysis as a comparison we also undertook a similar analysis for the 7 most sensitive factors whose s ij s were over 3 5 first 7 factors in first column in table 2 in the following when not specified results are based on these 11 most sensitive factors in contrast regional sensitivity analysis rsa can only detect the first 6 most sensitive factors in table 2 and identifies v ch k1 and v surlag as sensitive factors that are not sensitive in bayesian sensitivity analysis similar results can also be found in yang 2011 sensitivity analysis can also be used to narrow factor ranges based on posterior expectations of e y x i expectation of srmse conditioned on a given factor x i in table 2 and e y x i x j fig 4 shows posterior expectations e y x i of the 11 most sensitive factors and the posterior expectation e y x i x j between v tlaps and v alpha bf other e y x i x j s are not shown here as their corresponding s ij are lower than 2 from the e y x i plots lower srmse values are concentrated in the left side of the initial range for v tlaps from the middle to the right for v alpha bf v plaps and v gw delay while flat for other factors e g v esco the posterior expectation of e y x i x j between v tlaps and v alpha bf also shows that low srmse values are concentrated in the upper corner lower limit of v tlaps and upper limit of v alpha bf by setting a cut off point for the posterior expectation e y x i the ranges of sensitive factors can be reduced e y x i lies in 0 7 1 25 and this gives a suggested cut off of 0 975 and so here we chose three cut offs i e 1 0 0 95 and 0 9 to examine the effects of departures from 0 975 table 3 lists the reduced factor ranges for sensitive factors at these three cut offs for example for a cut off value of 0 95 the new range is 10 6 3 for v tlaps 0 17 1 for v alpha bf and 110 200 for v alpha bf while there was no change in ranges for other factors for the ensuing uncertainty analysis 2000 sobol sequence samples were then generated for each cut off selection and followed by a corresponding 2000 model runs which led to 2000 model simulation results for each cut off when applying the standard glue 5000 sobol sequence samples were generated followed by a corresponding 5000 model runs the larger number being required in order to have a better coverage of the larger factor space the uncertainty analysis was conducted separately for glue and each glue gp application i e for each cut off value of posterior expectation fig 5 compares the histograms of objective function i e srmse in eq 8 values based on glue 5000 simulations top and glue gp with cut off values of 1 0 0 95 and 0 90 each with 2600 simulations in total i e 600 simulations for gp construction initial calibration and validation plus 2000 simulations after range reduction based on 11 factors and 7 factors respectively for glue objective functions are spread out almost uniformly from 0 5 to 1 3 while for glue gp with a cut off of 1 0 objective function values shift to the lower values and are concentrated around 0 6 0 9 based on 11 factors and 0 5 0 9 based on 7 factors and shifts even more to lower values towards the minimum for glue gp with cut offs 0 95 and 0 90 based on both 11 factors and 7 factors compared to the 11 factor based glue gp 7 factor based glue gp has a higher frequency of srmse around 0 5 for illustration we chose a threshold for srmse of 0 55 equivalent to ns of 0 70 below which the behavioral sets were selected for uncertainty analysis fig 6 plots the 95ppu bands and the best simulations corresponding to the lowest srmse for glue and glue gp with each of the three cut off values while table 4 shows their sampling efficiency uncertainty statistics and best smallest srmse in both calibration and validation periods for these 95ppu bands visually there are no obvious differences between the first three plots their relative widths however can be quantified by the r factor results of which are in table 4 in the 1986 89 period as expected the 95ppu bands of glue are the widest 0 83 as it samples from the most expansive prior distributions followed by glue gp with cut offs of posterior expectation 1 0 and 0 95 both 0 78 and that of glue gp with a cut off of 0 90 is the narrowest 0 74 sampling efficiency given by the e factor or percentage of behavioral sets is low 3 for glue and significantly increased by glue gp with expected increasing efficiency for decreasing cut offs 7 9 and 16 for cut offs of 1 0 0 95 and 0 9 respectively table 4 also summarizes that in the calibration the 95ppu bands of glue include 81 of the observed data the 95ppu bands of the 11 factor based glue gp include similar percentages 81 and 80 of observed data when cut offs are 1 0 and 0 95 while this reduces to 77 with a cut off posterior expectation of 0 90 the 7 factor based glue gp gives similar results to the 11 factor based glue gp for all three cut offs the best objective function value from all simulations is lowest and best for the 7 factor based glue gp followed by the 11 factor based glue gp while that of glue has the worst objective function value given that glue gp applies standard glue on a reduced set of factors and factor ranges to glue its computed uncertainties will be somewhat different to that of glue we therefore investigated the level of differences between the two approaches focusing on the validation periods in the two validation periods we found that for both approaches over 95 of the behavioral parameter sets in the calibration period remain behavioral table 4 with glue gp values typically higher than glue similarly uncertainty results in the calibration period carry over for both approaches to the validation periods i e r factor s and p factor s of glue gp validations are either close to or slightly lower than those of glue but objective function values of glue gp are lower than those of glue fig 7 indicates the differences through the validation time periods between the two approaches in terms of their 95 prediction uncertainty bands the main observable differences occur at some of the larger flows otherwise the bands are qualitatively very similar 5 discussion 5 1 comparison with glue compared to glue the 11 factor glue gp has an improved sampling efficiency from 3 to over 7 for all three glue gp applications indeed it is over 9 when only 7 sensitive factors are invoked over factor space while obtaining similar uncertainty coverage for posterior expectation cut offs of 1 0 and 0 95 80 and 81 versus glue s 81 however one needs to be careful not to enforce cut offs that are too stringent in our example a cut off of 0 9 increases efficiency to 16 in the 11 sensitive factor case and 22 if only 7 factors are used but it reduces coverage to 77 in both cases an acceptable cut off should always be guided by appropriate selection of the metric s for the output of interest e g bennett et al 2013 and the accuracy of uncertainty estimates required for it in many cases it might be acceptable to have a stringent cut off that is more efficient computationally yet yields a useful qualitative appreciation of uncertainties thus these results are a consequence of the gp technique excluding insensitive factors and narrowing factor ranges through a bayesian sensitivity analysis that provides sobol indices and expectations of e y x i and e y x i x j glue gp tends to have a stronger focus on optimal factor regions instead of the entire factor space and leads to higher weight on optimal factor regions which further leads to a better optimal factor estimation e g last row in table 4 5 2 comparison with the sobol method generally with the sobol method for sensitivity analysis only sobol indices e g s i s ti and s ij are used these indices can rank the general importance of each factor and measure general interactions among factors therefore it is most useful for studying the average behavior of each factor and factor interaction and excluding insensitive factors the shortcoming is that it cannot provide local information of individual factors and factor interaction in factor space the gp emulation however provides the expectations of e y x i and e y x i x j yielding insight into how different factors affect model behavior along their ranges and their joint effects in their subspaces this can determine how to reduce factor space in model calibration or uncertainty analysis in this study we examined expectations of e y x i and e y x i x j fig 4 however as first order interaction indices s ij were rather small 2 we mainly used expectations of e y x i to narrow factor ranges if first order interactions s ij are high e y x i x j should also be used when reducing factor ranges 5 3 choice of sensitive factors the choice of sensitive factors makes some differences to the performance of glue gp as shown in table 4 and fig 5 compared with the 11 factor glue gp the 7 factor glue gp has a higher sampling efficiency while possessing similar r factors and p factors for the two higher cut offs 5 4 choice of the cut off in the study to illustrate the tradeoffs involved we compared three cut offs in the posterior expectations 1 0 0 95 and 0 90 when the cut off decreases moves towards the optimal region it will cause a larger factor range reduction and involve more factors in the range reduction table 3 and then lead to increasing sampling efficiency e g table 4 in addition to a narrower uncertainty range e g fig 6 and a more skewed histogram of the objective function towards the optimal region e g fig 5 when the cut off is far from the optimal region it will not have too much effect on the factor range and will lead to a similar sampling efficiency and uncertainty result to glue when it is close to the optimal region it will reduce the factor range significantly and therefore increase sampling efficiency but narrow uncertainty a good cut off should be not too far from and not too close to the optimal region an iterative approach could best start with a cut off slightly far from the optimal region and involve several iterations in the gp construction and factor range reduction however this will require more model runs and several gp constructions 6 conclusions this paper introduces glue gp an augmented glue approach based on a gaussian process emulation which is used to undertake a bayesian sensitivity analysis to narrow down the factor space through reduction in the number of factors and the factor range the application involved a semi distributed hydrologic model swat in the kaidu river basin using a standard root mean square error srmse performance or so called glue likelihood measure as demonstration compared to the standard glue it yields significant improvements in behavioral sampling efficiency from 3 to as much as 22 of successfully sampled behaviors but with around half the number of samples and associated model runs i e 2600 versus 5 000 while yielding not substantially different uncertainties to standard glue in validation mode consequently it locates the optimal region at a lower computational cost because the constructed gp assists in narrowing factor ranges towards the optimal factor region in the application of glue gp the critical step is the gp construction which involves an experimental design i e efficient selection of points in the response surface of the original model hyperparameter estimation and assessment of how well the constructed gp reproduces the original model behavior of interest normally if the response surface is smooth a small number of design points is sufficient e g in this case study around 300 points are sufficient however as emulators aim to speed up model simulations it should take less computational cost than the original model the trick of glue gp is to emulate the model just well enough to allow a good sensitivity analysis and select an appropriate cutoff in conditional expectation of the output that reduces the number of factors and their ranges to a computationally manageable level thus glue can then sample where it matters reducing the number of samples and obtain reasonably close uncertainty bounds to the original glue which themselves are approximate in any case but hopefully a useful appreciation of predictive uncertainties this procedure can be adapted for other uncertainty analysis techniques and models that have a smooth response surface it should be particularly suited to models with prohibitive runtimes acknowledgment the research was supported by national natural science foundation of china 41361140361 and state key laboratory of desert and oasis ecology project y471161 the authors wish to express their gratitude to manuscript editor dr joseph guillaume and four anonymous reviewers for insightful and constructive comments that helped improve the manuscript considerably 
26443,despite various criticisms of glue generalized likelihood uncertainty estimation it is still a widely used uncertainty analysis technique in hydrologic modelling that can give an appreciation of the level and sources of uncertainty we introduce an augmented glue approach based on a gaussian process gp emulator involving gp to conduct a bayesian sensitivity analysis to narrow down the influential factor space and then performing a standard glue uncertainty analysis this approach is demonstrated for a swat soil and water assessment tool application in a watershed in china using a calibration and two validation periods results show 1 the augmented approach led to the screening out of 14 18 unimportant factors effectively narrowing factor space 2 compared to the more standard glue it substantially improved the sampling efficiency and located the optimal factor region at lower computational cost this approach can be used for other uncertainty analysis techniques in hydrologic and non hydrologic models keywords uncertainty analysis hydrologic modelling sensitivity analysis gaussian process emulator glue 1 introduction distributed hydrological modelling is a way to understand site specific hydrology and support operational management planning and decision making in water resources under various scenarios e g water use land use change or climate change arnold et al 2015 prior to its application models generally go through a so called calibration process although the hydrologic and other environmental modeling communities have generally promoted the concept and desirability of uncertainty analysis ua e g beven and freer 2001 todini 2007 jakeman et al 2006 there is still a need for its more widespread general practice especially for distributed hydrologic modelling muleta and nicklow 2005 yen et al 2015 for example most applications are still based on reporting a single optimum parameter set fang et al 2015 liu et al 2011 luo et al 2012 the reasons are multifold for example some uncertainty analysis techniques are difficult to apply e g the need for testing statistical assumptions in bayesian inference see yang et al 2008 for a discussion or only one parameter set is intended or sought for decision making song et al 2015 for complex environmental models as occur in distributed hydrologic modelling a restriction would often be the number of model runs required for the ua which can be a burden even with the ongoing advances in information technology e g increase of cpu speed and parallel computation technology various quantitative ua techniques have been developed or applied in the literature matott et al 2009 and there are now many societies and journals that promote ua in the hydrologic modelling literature such techniques include generalized likelihood uncertainty estimation or glue for short beven and binley 1992 sufi abbaspour et al 2007 first order approximation vrugt and bouten 2002 and bayesian inference kuczera and parent 1998 kavetski et al 2006 yang et al 2007 among all these techniques glue is still by far the most widely applied technique in hydrology shen et al 2012 stedinger et al 2008 due to its simplicity and practicality though it has been criticized for several reasons including its informal statistical basis sampling inefficiency and flat response surface mantovan and todini 2006 yang et al 2008 beven and binley 2014 on the other hand it can be argued that glue warrants use as a guide at least to appreciating the level of various sources of uncertainty when applying glue however one might face a substantial computational burden as captured in beven and binley 2014 page 5905 who state it remains an issue either because of a model that is particularly slow to run so that it is still not possible to sample sufficient realizations or because of a high number of parameter dimensions this often arises in spite of suggestions e g mcmillan and clark 2009 to seek increases in the efficiency of finding behavioral models it has been noted that the sample efficiency i e number of behavioral sets over number of sampling sets of some applications can be lower than 10 4 iorgulescu et al 2005 2007 yang et al 2008 over recent decades emulators or in some literature meta models or surrogate models have been widely applied as a surrogate to deterministic models partly to overcome the high computational cost of the latter but certain types of surrogates like gaussian processes can also be used to assess properties of the model response surface e g see asher et al 2015 for a review in the groundwater domain these emulators include polynomial regression jones 2001 multivariate adaptive regression splines friedman 1991 radial basis functions dyn et al 1986 polynomials chaos wiener 1938 xiu and karniadakis 2002 and gaussian processes kennedy and o hagan 2001 sacks et al 1989 most of these applications especially in hydrologic modelling have been for optimization emmerich et al 2006 jones 2001 and global sensitivity analysis oakley and o hagan 2004 ratto et al 2007 in this paper we propose an emulation augmented glue using gaussian process gp emulation of the original model to help conduct uncertainty analysis this glue gp uses global and local sensitivity analysis arising as a natural byproduct of the gp emulation to screen out unimportant parameters and reduce the ranges of more sensitive ones implemented in the software gem sa www tonyohagan co uk academic gem before application of glue the gp also allows improvement in the sampling efficiency and location of the optimal region for glue sampling at a much lower computational cost than the standard glue the glue gp method is thus akin to glue in the sense that it is an augmentation that applies a glue procedure but only to those factors and their ranges as informed by the initial gp emulation and its inherent sensitivity analysis thus it is an approximation of glue whose differences are numerically investigated here in both calibration and validation modes this approach is demonstrated on the semi distributed hydrologic model swat soil and water assessment tool arnold et al 1998 with an application to the kaidu river basin in xinjiang china which is an important water source for human activity and ecological function in the oasis downstream the remainder of this paper is structured as follows section 2 gives a brief introduction to glue and the gp emulator and then focuses on the proposed emulation based glue approach glue gp and case study section 3 introduces the swat model and case study area section 4 presents and discusses results and finally conclusions are summarized in section 5 2 methodology a large class of hydrologic and environmental models can be formulated as y f x where x x 1 x 2 x m is a vector of m factors and y is either scalar or vector model output e g flow rate time series or objective function e g root mean square error between simulated and observed flows and the notation x i j to indicate the jth realization of the ith factor of x here we distinguish factors from model parameters in that a factor could be a model parameter or a modification to a distributed parameter either in a relative way or with a replacement to their initial values examples are given in table 1 and section 3 2 thus we use the terminology factors instead of model parameters for the gp and uncertainty analyses below 2 1 glue glue beven and binley 1992 2014 is an uncertainty analysis technique inspired by the regional sensitivity analysis of hornberger and spear 1981 in contrast to assuming that there is a single optimal factor set for a model it is based on the concept of equifinality in which different behavioral factor sets lead to similarly good model results in some sense it recognizes that most environmental models used for prediction are non identifiable due largely but not only to the over parameterised structure of the model see shin et al 2015 for an overview of methods to check structural identifiability fig 1 a shows a typical procedure for uncertainty analysis based on glue when applying glue one needs to define an objective function l or generalized likelihood measure and a given threshold value which is used to assess if a sampled factor set is behavioral or non behavioral through a comparison if the corresponding likelihood measure is better or worse than the given threshold value each behavioral factor is then given a likelihood weight according to 1 w i l x i k 1 n l x k where l x i is the objective function value of factor set x i and n is the number of behavioral factor sets from nt total samples then the model predictive uncertainty is described as a prediction band from the cumulative distribution of the model output realized from the weighted behavioral factor sets based on these behavioral and non behavioral factor sets factor sensitivity can also be studied with regionalized sensitivity analysis rsa spear and hornberger 1980 which inspired glue but is not generally part of glue the idea of rsa is if the distributions of a factor in the behavioral and non behavioral factor sets are dissimilar then this factor is considered influential in practice this is performed with the kolmogorov smirnov test to obtain a distance measure d which is the maximum distance between the two empirical cumulative distributions i e behavioral and non behavioral 2 2 gaussian process emulator the emulator we invoke is the gaussian process emulator although other emulators such as those based on polynomial chaos wiener 1938 xiu and karniadakis 2002 have similar advantages mainly sampling efficiency improvements and global and local sensitivity measures as byproducts of the emulation largely because such emulators and their response surfaces are continuous functions which can be differentiated the idea is to construct a simpler and computationally efficient model as a surrogate for the complicated more physically based and less computationally efficient model when applying a gp emulator to a hydrologic model y f x it approximates f x as a gaussian process kennedy and o hagan 2001 2 f ˆ x m x e x h x t β e x where x i s a vector of factors m x is the mean function h x a known regression function and e x a zero mean gaussian process with correlation given by c o v f x f x σ 2 c x x in this study c x x is a correlation function between two points x and x that takes the form exp x x θ x x the quantities β σ 2 and θ are emulator parameters hyperparameters the key assumption is that f x is a smooth function no discontinuities jumps or sharp changes which allows us to predict f x if x is close to x and f x is known emulator hyperparameters i e β σ 2 and θ are estimated based on n design data points x i y i i 1 n as β ˆ σ ˆ 2 and θ ˆ bastos and o hagan 2009 based on bayesian inference 3 f x m x σ ˆ c x x t n m where m x h x t β ˆ t x t a 1 y d h β ˆ c x x c x x t x t a 1 t x h x t t x t a 1 h h t a 1 h 1 h x t t x t a 1 h t t x c x x 1 c x x n h t h x 1 t h x n t y d f x 1 f x n t a c x 1 x 1 c x 1 x n c x n x 1 c x n x n and t n m denotes a student s t distribution with n m degrees of freedom one advantage of a gp emulator is that it not only gives the prediction i e as a surrogate to the original model but also the underlying sensitivity of factors resulting from the emulation process since these types of integrals are easy to be solved analytically it is useful for statistical analysis of the emulated response surface for example variance based sensitivity analysis as explained in the section below 2 3 sensitivity analysis sensitivity analysis sa techniques can be categorized broadly into local sa and global sa based on the factor space of interest saltelli et al 2008 norton 2015 yang 2011 with global sa techniques presently being more popular among the different global sa techniques variance based sa is considered the most reliable saltelli 2002 it is based on anova analysis of variance decomposition i e the original model y f x can be decomposed into m main effects and m interactions 4 y f x g 0 i 1 m g i x i i j m g i j x i x j g 1 2 m x where g 0 e y g i x i e y x i g 0 g i j x i x j e y x i x j g i x i g j x j g 0 and so on the quantities g i x i and g i j x i x j are called the main effect and first order interaction respectively the sensitivities of the m main effects and m interactions sum to unity i e 5 i 1 m s i i j m s i j s 1 2 m 1 where s i v e y x i v y is the main effect index of factor i s i j v e y x i x j v y is the first order interaction index between factor x i and factor x j and so on the notation v denotes variance among these indices in eq 5 s i represents the average output variance reduction that can be achieved when x i is fixed additionally the total effect index s t i 1 v e y x i v y where x i is the subvector of x containing all elements except x i denotes the average output variance that would remain as long as x i stays unknown s t i is used to screen out unimportant factors and s i is used to determine the important factor to be fixed in the calibration process ratto et al 2007 traditional methods to estimate these indices from multiple samplings of the model factors include the sobol method saltelli 2002 sobol 1990 and the extended fast method saltelli et al 1999 however these methods are computationally expensive and emulator based methods have been proposed to reduce the required sampling and associated model runs e g oakley and o hagan 2004 ratto et al 2007 2 4 proposed augmented glue procedure glue gp the proposed uncertainty analysis procedure for combining glue and the gp emulation is shown in fig 1 b gp construction and sensitivity analysis are performed with the software gem sa compared to a typical glue in fig 1 a the main procedure is 1 based on a design dataset of n sampled points x i y i i 1 n i e designed factor sets and corresponding n model simulation results approximate y f x with the gp emulator by estimating its hyperparameters using the software gem sa www tonyohagan co uk academic gem 2 as a product of gem sa a bayesian sensitivity analysis oakley and o hagan 2004 is used to decrease model dimensions i e number of factors based on the total effect index s ti and to narrow factor ranges based on s i s ij and the expectations of e y x i and e y x i with a given cut off the cut off which is a different variable to the threshold in glue to determine behavioral and non behavioral factor sets is used to narrow factor ranges though it is subjective it should be within the range of minimum and maximum of all e y x i s and based on our experience the average of minimum and maximum all e y x i s is a good start though one can always look at values beyond our illustrative choices 3 nm points are sampled from the reduced and range narrowed factors and then the corresponding nm model simulation results are calculated 4 uncertainty analysis is then undertaken similarly to the standard glue it is noted that the final behavioral factor sets include all behavioral factor sets from n design points for gp construction at step 1 and nm points at step 3 to compare the performance of this glue gp procedure with the basic glue on the original model three measures are computed the e factor the percentage of behavioral sets identified p factor the percentage of observations bracketed by 95 prediction uncertainty 95ppu and r factor relative average width of 95ppu are calculated according to 6 e f a c t o r n b s n s 7 r f a c t o r t i t y t i 97 5 m y t i 2 5 m t σ f o b s where n b s and n s are the number of behavioral sets and number of total samples y t i 97 5 m and y t i 2 5 m t i 1 t represent the upper part and lower part of the 95 ppu of the simulated time series in the case of our model application the series is streamflow σ f o b s denotes the standard deviation of the measured time series data t is the number of model outputs in our case streamflow and m indices modelled the e factor quantity ranging from 0 to 100 is used to quantify the sampling efficiency whereas the p factor and r factor quantify the goodness of prediction uncertainty i e closeness of the p factor to 100 i e all observations bracketed by the prediction uncertainty and r factor to 0 i e achievement of rather small uncertainty band these three metrics have been invoked in ua using sufi abbaspour et al 2007 and in a ua technique comparison study e g yang et al 2008 2 5 sampling techniques to apply glue and glue gp one needs to sample from prior factor distributions there are various sampling techniques that have been applied in the literature the most frequently used include simple random sampling monte carlo markov chain monte carlo mcmc importance sampling latin hypercube sampling lhs and sobol sequencing simple random sampling often used with glue is an entirely random sampling technique however it tends to undersample small but possibly very important areas unless the sample size is significantly increased congalton 1991 lhs uses a stratified sampling scheme to improve on the coverage of factor space mckay et al 1979 and sobol sequencing belongs to the family of quasi random low discrepancy sequences sobol et al 2011 it uses a base of two to form successively finer uniform partitions of the unit interval and then reorders the coordinates in each dimension see more details in sobol et al 2011 some studies show that sobol sequences e g kucherenko et al 2016 singhee and rutenbar 2009 and lhs e g aistleitner et al 2012 have faster convergence than simple random sampling in this study we use the sobol sequence for sampling as it typically has a good coverage of factor space and is quite often used to compute sobol indices sobol 2001 however we encourage similar studies on other sampling techniques 3 swat model and study area 3 1 swat model swat soil and water assessment tool arnold et al 1998 is a semi distributed hydrologic model and has been widely used to assess the impact of different management practices and climate change on water resource quantity and quality at a watershed scale e g arnold and fohrer 2005 setegn et al 2011 it divides the watershed into sub basins within which a number of hydrologic response units are generated with a unique combination of landuse soil type and slope the sub basins are linked through a river network for surface water movement its climatic input consists of daily precipitation maximum minimum air temperature solar radiation wind speed and relative humidity in mountainous regions swat can use elevation bands and precipitation temperature lapse rates to account for orographic effects on precipitation and temperature for more details refer to the swat manuals http www brc tamus edu 3 2 study area and model setup the kaidu river basin with a drainage area of 18 634 km2 above the dashankou hydrological station is located in the tianshan mountains in northwest china fig 2 it is characterized as temperate continental with alpine climate as one of the headwaters of the tarim river it provides water resources for agricultural activity and the ecological environment of the oases in the lower reaches fang et al 2015 the altitude ranges from 1342 m to 4796 m above sea level a s l with an average elevation of 2995 m so the orographic effect on precipitation and temperature is significant and is represented here observed flow data are available either at daily or monthly scale from 1980 to 2010 and were used here daily observed meteorological data including precipitation maximum minimum temperature wind speed and relative humidity of two meteorological stations bayanbulak and baluntai locations are denoted by asterisks in fig 2 and are 2458 m and 1740 m a s l respectively are from the china meteorological data sharing service system http cdc cma gov cn the mean annual maximum and minimum temperature at the bayanbulak meteorological station are 3 1 c and 10 6 c and mean annual precipitation is 267 mm generally precipitation falls as rain from may to september and as snow from october to april of the next year the observed streamflow data at the dashankou hydrologic station the triangle in fig 2 are from the xinjiang tarim river basin management bureau the average daily flow is around 110 m3 s 1 equivalent to 185 mm of runoff per year ranging from 15 m3 s 1 to 973 m3 s 1 for more details refer to fang et al 2015 previously fang et al 2015 set up a swat model in this watershed with available dem digital elevation model land use soil and observed climate data and studied the contribution of meteorological inputs to hydrologic modelling results and the impact of climate change on the water resources based on a single optimized factor set the purpose of using the same case is to test our proposed uncertainty analysis approach for its advantages over a typical glue accepting that uncertainties could be reduced and characterized by either approach if better climate and streamflow information were available table 1 lists the initial 25 selected factors for the calibration which are assumed to be uniformly distributed as in fang et al 2015 factors distinct from model parameters can be modifications to model parameters either in a relative way or a replacement for example in table 1 the factor r sol k is a relative change to the distributed parameter sol k saturated hydraulic conductivity while v tlaps is a replacement of initial values of tlaps temperature lapse rate in order to lower the calibration dimension standardized root mean squared error srmse is invoked in the paper for demonstration as the so called likelihood measure objective function 8 s r m s e 1 t y t i m y t i 2 t σ f o b s where y t i is the observed outflow at time t i srmse can be related to the widely used nash sutcliffe coefficient ns through n s 1 s r m s e 2 t t 1 1 s r m s e 2 a value of 0 55 for example for s r m s e is equivalent to an ns value of 0 70 in this study glue and glue gp were first applied to a calibration period from 1986 to 1989 with daily flow data and then to two validation periods one from 1990 to 2002 with daily flow data and the other from 2003 to 2010 with monthly flow data which are the same as the calibration and validation periods in fang et al 2015 4 results based on an initial selected 25 factors table 1 which is a fairly standard level of parameterisation for a swat model and the likelihood measure srmse 600 sobol sequence samples were generated as design data and a corresponding 600 daily model simulations were obtained of these samples the first 300 were used to train the emulator and the remaining 300 for so called validation the emulation result is shown in fig 3 black dots signify training data and grey dots validation points it is not surprising that training data are on the 1 1 line which is characteristic of a gp emulator data for validation were scattered around the 1 1 line with an ns value of 0 82 for the emulated srmse fit to the original modelled srmse a similar result was obtained through a cross validation using the last 300 design samples to train the emulator and first 300 samples for validation as part of the gp construction a bayesian sensitivity analysis was undertaken for these 25 factors with gem sa software which uses the same samples as were generated for the gp construction the results are captured in table 2 and fig 4 in table 2 of the main effect indices s i the most sensitive factor is v tlaps followed by v alpha bf v plaps and v gwqmn while others are not as sensitive for total effect indices s ti the most sensitive factor is v tlaps followed by v alpha bf and then v gw delay v plaps and v gwqmn while others are not as sensitive the difference between the total effect index of a factor and its main effect index measures the interaction between this factor and other factors there are interactions for these mentioned factors especially for v tlaps and v alpha bf however all first order interaction indices i e s ij are around but less than 2 for example s ij between v tlaps and v alpha bf is 1 9 which denotes that first order interactions are low this result is similar to the result of an approach combining the morris method and sdp method applied in fang et al 2015 and their main effect indices and first order interactions are very close to our results here except that the approach here includes total effect indices and requires less model runs based on this analysis insensitive factors whose total effect indices are less than 2 were screened out which led to 11 factors being considered for further analysis as a comparison we also undertook a similar analysis for the 7 most sensitive factors whose s ij s were over 3 5 first 7 factors in first column in table 2 in the following when not specified results are based on these 11 most sensitive factors in contrast regional sensitivity analysis rsa can only detect the first 6 most sensitive factors in table 2 and identifies v ch k1 and v surlag as sensitive factors that are not sensitive in bayesian sensitivity analysis similar results can also be found in yang 2011 sensitivity analysis can also be used to narrow factor ranges based on posterior expectations of e y x i expectation of srmse conditioned on a given factor x i in table 2 and e y x i x j fig 4 shows posterior expectations e y x i of the 11 most sensitive factors and the posterior expectation e y x i x j between v tlaps and v alpha bf other e y x i x j s are not shown here as their corresponding s ij are lower than 2 from the e y x i plots lower srmse values are concentrated in the left side of the initial range for v tlaps from the middle to the right for v alpha bf v plaps and v gw delay while flat for other factors e g v esco the posterior expectation of e y x i x j between v tlaps and v alpha bf also shows that low srmse values are concentrated in the upper corner lower limit of v tlaps and upper limit of v alpha bf by setting a cut off point for the posterior expectation e y x i the ranges of sensitive factors can be reduced e y x i lies in 0 7 1 25 and this gives a suggested cut off of 0 975 and so here we chose three cut offs i e 1 0 0 95 and 0 9 to examine the effects of departures from 0 975 table 3 lists the reduced factor ranges for sensitive factors at these three cut offs for example for a cut off value of 0 95 the new range is 10 6 3 for v tlaps 0 17 1 for v alpha bf and 110 200 for v alpha bf while there was no change in ranges for other factors for the ensuing uncertainty analysis 2000 sobol sequence samples were then generated for each cut off selection and followed by a corresponding 2000 model runs which led to 2000 model simulation results for each cut off when applying the standard glue 5000 sobol sequence samples were generated followed by a corresponding 5000 model runs the larger number being required in order to have a better coverage of the larger factor space the uncertainty analysis was conducted separately for glue and each glue gp application i e for each cut off value of posterior expectation fig 5 compares the histograms of objective function i e srmse in eq 8 values based on glue 5000 simulations top and glue gp with cut off values of 1 0 0 95 and 0 90 each with 2600 simulations in total i e 600 simulations for gp construction initial calibration and validation plus 2000 simulations after range reduction based on 11 factors and 7 factors respectively for glue objective functions are spread out almost uniformly from 0 5 to 1 3 while for glue gp with a cut off of 1 0 objective function values shift to the lower values and are concentrated around 0 6 0 9 based on 11 factors and 0 5 0 9 based on 7 factors and shifts even more to lower values towards the minimum for glue gp with cut offs 0 95 and 0 90 based on both 11 factors and 7 factors compared to the 11 factor based glue gp 7 factor based glue gp has a higher frequency of srmse around 0 5 for illustration we chose a threshold for srmse of 0 55 equivalent to ns of 0 70 below which the behavioral sets were selected for uncertainty analysis fig 6 plots the 95ppu bands and the best simulations corresponding to the lowest srmse for glue and glue gp with each of the three cut off values while table 4 shows their sampling efficiency uncertainty statistics and best smallest srmse in both calibration and validation periods for these 95ppu bands visually there are no obvious differences between the first three plots their relative widths however can be quantified by the r factor results of which are in table 4 in the 1986 89 period as expected the 95ppu bands of glue are the widest 0 83 as it samples from the most expansive prior distributions followed by glue gp with cut offs of posterior expectation 1 0 and 0 95 both 0 78 and that of glue gp with a cut off of 0 90 is the narrowest 0 74 sampling efficiency given by the e factor or percentage of behavioral sets is low 3 for glue and significantly increased by glue gp with expected increasing efficiency for decreasing cut offs 7 9 and 16 for cut offs of 1 0 0 95 and 0 9 respectively table 4 also summarizes that in the calibration the 95ppu bands of glue include 81 of the observed data the 95ppu bands of the 11 factor based glue gp include similar percentages 81 and 80 of observed data when cut offs are 1 0 and 0 95 while this reduces to 77 with a cut off posterior expectation of 0 90 the 7 factor based glue gp gives similar results to the 11 factor based glue gp for all three cut offs the best objective function value from all simulations is lowest and best for the 7 factor based glue gp followed by the 11 factor based glue gp while that of glue has the worst objective function value given that glue gp applies standard glue on a reduced set of factors and factor ranges to glue its computed uncertainties will be somewhat different to that of glue we therefore investigated the level of differences between the two approaches focusing on the validation periods in the two validation periods we found that for both approaches over 95 of the behavioral parameter sets in the calibration period remain behavioral table 4 with glue gp values typically higher than glue similarly uncertainty results in the calibration period carry over for both approaches to the validation periods i e r factor s and p factor s of glue gp validations are either close to or slightly lower than those of glue but objective function values of glue gp are lower than those of glue fig 7 indicates the differences through the validation time periods between the two approaches in terms of their 95 prediction uncertainty bands the main observable differences occur at some of the larger flows otherwise the bands are qualitatively very similar 5 discussion 5 1 comparison with glue compared to glue the 11 factor glue gp has an improved sampling efficiency from 3 to over 7 for all three glue gp applications indeed it is over 9 when only 7 sensitive factors are invoked over factor space while obtaining similar uncertainty coverage for posterior expectation cut offs of 1 0 and 0 95 80 and 81 versus glue s 81 however one needs to be careful not to enforce cut offs that are too stringent in our example a cut off of 0 9 increases efficiency to 16 in the 11 sensitive factor case and 22 if only 7 factors are used but it reduces coverage to 77 in both cases an acceptable cut off should always be guided by appropriate selection of the metric s for the output of interest e g bennett et al 2013 and the accuracy of uncertainty estimates required for it in many cases it might be acceptable to have a stringent cut off that is more efficient computationally yet yields a useful qualitative appreciation of uncertainties thus these results are a consequence of the gp technique excluding insensitive factors and narrowing factor ranges through a bayesian sensitivity analysis that provides sobol indices and expectations of e y x i and e y x i x j glue gp tends to have a stronger focus on optimal factor regions instead of the entire factor space and leads to higher weight on optimal factor regions which further leads to a better optimal factor estimation e g last row in table 4 5 2 comparison with the sobol method generally with the sobol method for sensitivity analysis only sobol indices e g s i s ti and s ij are used these indices can rank the general importance of each factor and measure general interactions among factors therefore it is most useful for studying the average behavior of each factor and factor interaction and excluding insensitive factors the shortcoming is that it cannot provide local information of individual factors and factor interaction in factor space the gp emulation however provides the expectations of e y x i and e y x i x j yielding insight into how different factors affect model behavior along their ranges and their joint effects in their subspaces this can determine how to reduce factor space in model calibration or uncertainty analysis in this study we examined expectations of e y x i and e y x i x j fig 4 however as first order interaction indices s ij were rather small 2 we mainly used expectations of e y x i to narrow factor ranges if first order interactions s ij are high e y x i x j should also be used when reducing factor ranges 5 3 choice of sensitive factors the choice of sensitive factors makes some differences to the performance of glue gp as shown in table 4 and fig 5 compared with the 11 factor glue gp the 7 factor glue gp has a higher sampling efficiency while possessing similar r factors and p factors for the two higher cut offs 5 4 choice of the cut off in the study to illustrate the tradeoffs involved we compared three cut offs in the posterior expectations 1 0 0 95 and 0 90 when the cut off decreases moves towards the optimal region it will cause a larger factor range reduction and involve more factors in the range reduction table 3 and then lead to increasing sampling efficiency e g table 4 in addition to a narrower uncertainty range e g fig 6 and a more skewed histogram of the objective function towards the optimal region e g fig 5 when the cut off is far from the optimal region it will not have too much effect on the factor range and will lead to a similar sampling efficiency and uncertainty result to glue when it is close to the optimal region it will reduce the factor range significantly and therefore increase sampling efficiency but narrow uncertainty a good cut off should be not too far from and not too close to the optimal region an iterative approach could best start with a cut off slightly far from the optimal region and involve several iterations in the gp construction and factor range reduction however this will require more model runs and several gp constructions 6 conclusions this paper introduces glue gp an augmented glue approach based on a gaussian process emulation which is used to undertake a bayesian sensitivity analysis to narrow down the factor space through reduction in the number of factors and the factor range the application involved a semi distributed hydrologic model swat in the kaidu river basin using a standard root mean square error srmse performance or so called glue likelihood measure as demonstration compared to the standard glue it yields significant improvements in behavioral sampling efficiency from 3 to as much as 22 of successfully sampled behaviors but with around half the number of samples and associated model runs i e 2600 versus 5 000 while yielding not substantially different uncertainties to standard glue in validation mode consequently it locates the optimal region at a lower computational cost because the constructed gp assists in narrowing factor ranges towards the optimal factor region in the application of glue gp the critical step is the gp construction which involves an experimental design i e efficient selection of points in the response surface of the original model hyperparameter estimation and assessment of how well the constructed gp reproduces the original model behavior of interest normally if the response surface is smooth a small number of design points is sufficient e g in this case study around 300 points are sufficient however as emulators aim to speed up model simulations it should take less computational cost than the original model the trick of glue gp is to emulate the model just well enough to allow a good sensitivity analysis and select an appropriate cutoff in conditional expectation of the output that reduces the number of factors and their ranges to a computationally manageable level thus glue can then sample where it matters reducing the number of samples and obtain reasonably close uncertainty bounds to the original glue which themselves are approximate in any case but hopefully a useful appreciation of predictive uncertainties this procedure can be adapted for other uncertainty analysis techniques and models that have a smooth response surface it should be particularly suited to models with prohibitive runtimes acknowledgment the research was supported by national natural science foundation of china 41361140361 and state key laboratory of desert and oasis ecology project y471161 the authors wish to express their gratitude to manuscript editor dr joseph guillaume and four anonymous reviewers for insightful and constructive comments that helped improve the manuscript considerably 
26444,we assessed whether a complex process based ecohydrological model can be appropriately parameterized to reproduce the key water flux and storage dynamics at a long term research catchment in the scottish highlands we used the fully distributed ecohydrological model ech2o calibrated against long term datasets that encompass hydrologic and energy exchanges and ecological measurements applying diverse combinations of these constraints revealed that calibration against virtually all datasets enabled the model to reproduce streamflow reasonably well however parameterizing the model to adequately capture local flux and storage dynamics such as soil moisture or transpiration required calibration with specific observations this indicates that the footprint of the information contained in observations varies for each type of dataset and that a diverse database informing about the different compartments of the domain is critical to identify consistent model parameterizations these results foster confidence in using ech2o to contribute to understanding current and future ecohydrological couplings in northern catchments keywords catchment hydrology ecohydrology process based modelling multi objective calibration information content ech2o 1 introduction numerical models are crucially important in the environmental sciences models can complement and integrate theory and empirical data by incorporating testable hypotheses and by extending knowledge at spatial and or temporal scales inaccessible to current observation methods in particular process based models seek to explicitly represent the state variables and fluxes that are theoretically observable and can be used in the closure of assumed forms of the laws of conversation of mass energy and momentum at temporal scales characterizing the underlying physical processes adapted from fatichi et al 2016 in contrast to conceptual and empirical approaches physically based models facilitate investigation of specific variables at local process specific scales e g endrizzi et al 2014 manoli et al 2017 niu and phanikumar 2015 pierini et al 2014 additionally a fully distributed description of the simulation domain opens the possibility for tracking intra system patterns and dynamics e g maxwell and condon 2016 pierini et al 2014 a task much less accessible to coarser spatial representations i e lumped or semi distributed models combining these two methodological choices with physically based fully distributed models is thus a way to disentangle feedbacks and non linear dynamics across fundamentally different processes e g drewry et al 2010 tague 2009 and better predict system behaviour outside recorded environmental conditions seibert 2003 uhlenbrook et al 1999 these tools are of particular relevance for the emerging field of critical zone science national research council 2012 which seeks integrated understanding of ecological geological geomorphological and pedological processes within a framework of hydrological partitioning brooks et al 2015 within the field of hydrology the issue of appropriate model complexity is a focus of ongoing discussion the corollary of expanding process based approaches towards an universal model is an inevitable increase in complexity as explicit descriptions of additional system characteristics are added e g topography soil texture tree height canopy density etc band et al 2001 maxwell and condon 2016 arguing that many of these numerous parameters cannot be appropriately measured some fear that evolution of complex multi disciplinary models only layer up unavoidable uncertainty and are prone to equifinality whereby several combinations of parameter values realistic or not yield comparable performance e g beven and binley 1992 beven and freer 2001 mcdonnell et al 2007 the utility of measurements to help constrain the model solution space and identify feasible model configurations has been an increasingly central issue in hydrological model calibration sufficiently informative observations are necessary to ensure that the goodness of model data fit attained effectively translates into physically sound information for the internal model parameters i e getting the right answers for the right reasons beven and binley 1992 kirchner 2006 the problem of equifinality a particular case of underdetermination duhem 1954 is apparent when stream discharge is the only monitored variable available for calibration unfortunately this remains the most common situation the widespread use of streamflow time series to calibrate and validate models has spurred the development of elaborate single and multiple criteria goodness of fit metrics kling et al 2012 krause et al 2005 legates and mccabe 1999 madsen 2003 van werkhoven et al 2009 and calibration algorithms duan et al 1992 gupta et al 1998 sorooshian and dracup 1980 tang et al 2007 tolson and shoemaker 2007 directed toward extracting a maximum of information content from this type of data he et al 2015 rouhani et al 2007 shafii et al 2017 however the information contained in streamflow time series is often insufficient to inform the parameterization of physically based models parameter values that represent physical properties of the catchment are usually poorly identified and become very sensitive to boundary conditions maneta et al 2007 the situation deteriorates as more complex models incorporate increasingly detailed descriptions of catchment functioning to constrain parameters of components associated with different subdomains of the model ecological surface subsurface etc it is desirable but often impractical to diversify data sources fang et al 2013 larsen et al 2016 rajib et al 2016 thorstensen et al 2015 combining different types of observations reduces information redundancy and provides direct insights into the different groups of physical processes represented in the model clark et al 2011 fatichi et al 2016 a data extensive approach to model calibration makes the choice of performance metrics easier because the information contained in observations is more directly related to the model compartment being calibrated e g birkel et al 2014 information diversity however brings other issues related to the assimilation of observations with diverse characteristics during calibration some are technical e g combining spatio temporal scales and associated uncertainties while others are more fundamental to modelling e g parameters compensating for model imperfections clark and vrugt 2006 or overlapping constraints and thus possibly pulling the model in different directions efstratiadis and koutsoyiannis 2010 in other research fields this approach is exemplified by the current efforts and associated challenges in assimilating multiple types of carbon cycle data to optimise earth system models kaminski et al 2013 peylin et al 2016 the ecohydrology of high latitude energy limited landscapes has traditionally been understudied despite the global ecological importance of this region since studies of plant water couplings across disciplines gained momentum in the late 90s bonell 2002 research efforts in ecohydrology have been primarily conducted in environments where water scarcity newman et al 2006 or permanent presence e g wetlands rodriguez iturbe et al 2007 makes hydrology an obvious critical control upon how plants distribute and compete only recently efforts have been directed towards understanding the specific ecohydrological processes of boreal energy limited regions e g cable et al 2014 while there have been process based model developments dedicated to the hydrology of high latitude environments e g endrizzi et al 2014 kuchment et al 2000 lindström et al 1997 pomeroy et al 2007 most model applications in these regions lack an explicit implementation of vegetation dynamics e g ala aho et al 2017a and thus cannot finely capture ecosystem imprints on water partitioning at the catchment scale high latitude regions comprise mixed temperate forests boreal forests and tundra covering nearly 20 of the continental land mass tetzlaff et al 2015a these regions are subject to rapid climate change with significant regional to global scale implications hinzman et al 2013 including shifts in precipitation regime and snow mediated water balance bintanja and andry 2017 jiménez cisneros et al 2014 and associated implications for runoff generation peterson et al 2002 zhang et al 2014 while such environmental change has been observed to alter water pathways and flow regimes dye and tucker 2003 mcclelland et al 2006 tetzlaff et al 2013 and ecosystem dynamics naito and cairns 2015 piao et al 2008 further work is needed to identify the underlying mechanisms reasons for the limited understanding so far lie in the fine scale landscape heterogeneity and the implications for spatial variation in energy inputs as well as the logistical difficulties of collecting data in comparatively remote areas pomeroy et al 2013 tetzlaff et al 2013 and the alarming recent decline in long term monitoring of northern catchments laudon et al 2017 however we need to understand such processes and the related uncertainties of water cycling in these regions while ongoing projected biome shifts e g beck et al 2011 williams et al 2007 call for particular scrutiny of ecosystem influence on water availability law 1956 and vice versa in this study our main aim was to investigate to what extent a data extensive approach to calibration can constrain the range of behavioural configurations of a highly parameterized physically based model such that the achieved parameter sets can be used as falsifiable hypotheses of the internal functioning of the catchment for this we used a distributed ecohydrologic model ech2o see maneta and silverman 2013 that integrates a kinematic hydrologic and energy balance model with a vegetation dynamics model the model is calibrated using several combinations of data types covering a range of ecohydrological variables collected at a long term experimental northern montane catchment we ask the following questions through our modelling experiments 1 what are the physical insights gained across ecohydrological processes 2 how valuable are the information contents brought by the different constraining datasets addressing these questions will help building a robust ecohydrological modelling framework dedicated to critical zone functioning in high latitude environments 2 material and methods 2 1 study site the bruntland burn fig 1 is a small catchment 3 2 km2 located in the eastern scottish highlands 57 8 n 3 20 w it is a headwater of the river dee which provides drinking water for the city of aberdeen 250 000 people ecosystem services such as an atlantic salmon fishery and has eu conservation designations the region receives around 1100 mm of average annual precipitation p distributed quite evenly throughout the year although november february and june august are usually wettest and driest periods respectively less than 5 of p occurs as snowfall the climatic water balance is energy limited with 400 mm of annual potential evapotranspiration pet the mean annual temperature t is 7 c with no monthly averaged t below 0 c in a transitional temperate boreal oceanic climate the local topography reflects glacier retreat with a wide valley bottom 220 m a s l surrounded by steeper slopes reaching up to 560 m a s l fig 1a this slope gradient is reflected by widespread glacial drift deposits 60 of the catchment with depths ranging from 40 m in the valley bottom to 5 m on steeper slopes these deposits are mostly saturated and form significant groundwater reservoirs that sustain stream base flow and maintain wet conditions in the valley bottom soulsby et al 2016 the pedology comprises deep 0 5 4 m organic rich soils histosols peat and gley in the riparian area bordering the stream channel network fig 1b these soils are persistently saturated and rapid overland flow is the dominant runoff generation mechanism following rainfall events tetzlaff et al 2014 hillslopes are characterized by shallower freely draining podzols spodosols overlying moraines and marginal ice deposits while thin regosols rankers dominate above 400 m a s l where the drift is thin or absent fig 1b these hydropedological units are somewhat reflected in the vegetation cover fig 1c f podzols and rankers predominantly support heather shrublands calluna vulgaris and erica spp though this land cover is the result of overgrazing by red deer cervus elaphus and sheep scots pine trees pinus sylvestris the naturally occurring vegetation is confined to the northern steep hillslopes and to plantation stands near the catchment outlet riparian gley soils are characterized by herbaceous cover molinia caerulea the latter being also found as secondary species in the peat where bog mosses sphagnum spp dominate the land cover 2 2 the ecohydrological model ech2o we used a new formulation of the spatially distributed process based model ech2o maneta and silverman 2013 here ech2o couples a two layer canopy and understory vertical energy balance scheme fig 2 a a kinematic hydrologic module solving vertical and lateral water transfers fig 2b and a transpiration based simulator of carbon uptake and allocation for plant growth lozano parra et al 2014 maneta and silverman 2013 the reasons for choosing ech2o lie in its original development aimed at filling a research gap between hydrology focused catchment models and land surface models lsms simulating biophysical and biogeochemical cycles in the critical zone while catchment models provide a fit for purpose conceptualization of water pathways in most cases they lack a process based representation of energy balance and plant water interactions on the other hand most state of art lsms have historically been developed as surface components of climate models to be run over large regions or continents despite the recent advances in representing land processes such as vegetation phenology and carbon nutrient cycles hydrology remained simplistic in most lsms overland flow routing channel routing and lateral subsurface flow are typically neglected or highly simplified see fan 2015 for a further discussion without these components it is not possible to study the ecohydrological effects of upstream water subsidies and the spatial organization of catchments imprinted by the water redistribution network this is especially critical for studies in small catchments at high spatial resolution like the one studied here finally the parsimonious implementation of ech2o was preferred over other recent potentially more sophisticated ecohydrological models e g fatichi et al 2012 maxwell and condon 2016 ech2o s calculation of energy and water fluxes water storage dynamics and vegetation states is made at time steps subordinated to that of the meteorological forcing the atmospheric boundary conditions for each time step are precipitation p incoming shortwave radiation rsw downwelling longwave radiation rlw t maximum ta max minimum ta min and average ta mean during the time step period relative humidity hr and wind speed wxy new developments that we implemented in ech2o are documented in the appendices to complement other model details described elsewhere lozano parra et al 2014 maneta and silverman 2013 here we provide a brief summary of the model philosophy and main features the spatial domain of ech2o is mapped on a regular grid defined by that of the input digital elevation data each cell of the domain can have multiple vegetation covers including bare soil the energy and the water balance is solved for each cover and integrated over the cell area according to the fraction of the cell they occupy the energy balance equations are solved at the top of the canopy see appendix a1 for details and then at the soil or snowpack surface turbulent fluxes momentum heat and vapour are resolved using a first order local closure approximation under gradient similarity theory valid for small eddies and neutral stratification conditions canopy interception is simulated using a linear bucket approach the partition of p and throughfall between the solid snow and liquid components during a time step is done according to the minimum and maximum air t during the time step and to a snow rain transition t threshold snowpack melt and liquid throughfall feed surface ponding which infiltrates following a green and ampt approximation of the richard s equation mays 2010 all ponded water at the end of each time step becomes overland flow i e run on to the downstream cell where it can reinfiltrate or in turn generate further overland flow this calculation cascade follows the local drainage direction until the remaining surface water reaches the outlet or a channel cell flow in a channel is routed using a 1d solution for a kinematic wave see appendix a2 for details the soil is divided into three hydraulic layers the shallow topsoil where soil evaporation takes place the intermediate layer which typically shares the bulk of the roots with the topsoil and the bottom layer where groundwater can be transferred laterally to the downstream cell or seep into the stream vertical water redistribution is based on the theory that only soil moisture in excess of field capacity gravitational water can move under gravity to deeper soil layers or laterally to the next cell downstream diffusive effects driven by local pressure gradients are therefore assumed to be negligible and water below field capacity is retained by the soil and only removed by evapotranspiration simulation of vegetation dynamics are adapted from the 3 pg and treedyn3 models bossel 1996 landsberg and waring 1997 peng et al 2002 with differentiated carbon allocation and growth schemes for ligneous and herbaceous species lozano parra et al 2014 a jarvis type model is used to simulate the response of canopy conductance to environmental drivers cox et al 1998 jarvis 1976 2 3 model setup and landscape characterization all simulations were performed at a 30 30 m2 resolution a lidar derived 1 1 m2 dem lessels et al 2016 was used to delineate the catchment boundary and further processed with the pcraster tool suite http pcraster geo uu nl to obtain local slopes and local drainage direction the latter was determined for each cell using the steepest descent among the eight adjacent cells d8 algorithm fairfield and leymarie 1991 the model was run at daily time steps simulations covered a 64 month long time period from june 2011 to september 2016 with the period from june 2011 to october 2012 used for model spin up and therefore discarded from the analysis see sect 2 5 p ta mean ta min ta max hr and wxy data were collected at three meteorological stations installed in the catchment in different landscape positions valley bottom bog and hilltop fig 1a and used from july 2014 prior to that period p was interpolated using a square elevation inverse distance weighted algorithm applied to five scottish environment protection agency sepa rain gauges located around the bruntland burn catchment within 10 km similarly to birkel et al 2011 while ta mean hr and wxy fields were taken from the balmoral station 5 km nw as available from the centre for environmental data analysis ceda met office 2017 ta min and ta max prior to july 2014 rsw and rlw whole study period were retrieved from the era interim climate reanalysis dee et al 2011 finally we took into account altitudinal effects on p and t by respectively assuming a 5 5 increase of p every 100 m a s l as measured along a hillslope covering 200 m elevation difference ala aho et al 2017b and a decrease of 0 6 c 100 m a s l based on the moist adiabatic temperature lapse rate goody and yung 1995 soil hydrological properties were mapped by splitting the model domain into four hydropedological units aggregated from the soil classes defined by the hydrology of soil types host fig 1b tetzlaff et al 2007 energy related soil characteristics were considered as spatially uniform five land cover types were considered scots pine stands heather moorland peat moss grassland and bare rock scree vegetation fraction of scots pine in each cell fig 1c was estimated by aggregating a 1 1 m2 resolution lidar canopy cover measurements conducted over the catchment to the 30 30 m2 grid used for simulations for the other vegetation fractions we additionally used the soil classification extensive land use mapping and aerial imagery tetzlaff et al 2007 heather was assumed to occupy 95 of the treeless surface of podzols and rankers except for the steep northern rocky hillslopes 40 cover in treeless areas a few sparsely vegetated moorlands on the west hillslopes 20 cover in treeless areas and the gley 5 cover fig 1d peat moss was assumed to occupy 90 of an extensive raised peat bog in the nw parts of the catchment see fig 1 in sprenger et al 2017 and 70 of the remaining peat soils fig 1e in the latter areas molinia grasses compete 30 cover with peat moss while grasslands are dominant on gley soils 95 of the tree and shrub free surface and in patches of managed lands near the catchment outlet fig 1f the list of model parameters calibrated in this study is given in table 1 we selected 16 types of parameters based on a preliminary sensitivity analysis following morris 1991 performed with the calibration dataset described in section 2 4 as shown in table 1 10 parameters are soil dependent and 6 are vegetation specific since we considered 4 soil types and 4 vegetation types the total number of calibrated variables was 64 10 4 6 4 the uncalibrated model parameterization including initial conditions was prescribed based on literature values and expert knowledge supplementary table s1 2 4 calibration and evaluation datasets a specific advantage of the bruntland burn experimental catchment is the length and diversity of data records across ecohydrological processes which is unusual for northern regions we made use of 10 datasets for the calibration as summarized in fig 3 daily discharge at the catchment outlet fig 1a was derived from 15 min stage height records odyssey capacitance probe christchurch new zealand using a rating curve calibrated for a stable stream section soil moisture data was collected at 15 min intervals at four locations three of them along a transect representative of the main hydropedological units podzol gley and peat on heather tetzlaff et al 2014 and one plot in a scots pine forest wang et al 2017a forest site b fig 1a we used time domain reflectometry tdr soil moisture probes model cs616 campbell scientific inc usa located 0 1 0 2 and 0 4 m beneath the surface corresponding to the main soil horizons geris et al 2015 except in the peat where only two probes were present at depths of 0 1 and 0 2 m additionally each group of probes was replicated at the same depths but 2 m apart these tdr sensors were calibrated using laboratory analyses of gravimetric soil water content and bulk density from samples collected at each horizon geris et al 2015 finally a single daily vertically averaged volumetric water content value was used for calibration and evaluation of the ech2o model scots pine transpiration tp was measured between july and september 2015 at forest site a and between april and september 2016 at forest site b fig 1a using 32 sets of granier type thermal dissipation sap flow sensors dynamax inc houston usa installed on 10 and 14 trees in the forest sites a and b respectively with 2 to 4 sensors per tree depending on the stem size 10 32 cm in diameter average stand scale pine transpiration was derived using a sapwood area to tree diameter relationship estimated from incremental wood cores sampling in surrounding trees at the end of the study period see wang et al 2017a for more details and then daily averaged net radiation rn was measured every 15 min at the three meteorological stations fig 1a and then daily averaged finally to provide a novel independent verification of the model s ability to represent seasonal storage dynamics we compared modelled and empirical based estimates of catchment scale saturation area from june 2011 to september 2014 a conceptual rainfall runoff model that linked antecedent wetness and soil moisture to saturation area was used to estimate the extent of saturation in the catchment this model was previously calibrated against maps of measured saturation area extent and isotopic tracers measurements ali et al 2014 birkel et al 2010 these estimates were compared with saturation extent in ech2o which was defined as the proportion of cells in the domain where the volumetric water content in the top hydraulic layer exceeded 99 of soil porosity 2 5 model data fusion method we sampled the parameter space by conducting 100 000 monte carlo simulations using uniform parameter distributions with prescribed bounds based on literature values and prior experience table 1 each run spanned the entire 64 month simulation period but the first 16 months of each run were used to spin up the model and stabilize water storages and flux simulations notably soil moisture and stream discharge after discarding the spin up period the calibration dataset was split whenever possible into two non overlapping periods one for calibration and one for evaluation fig 3 however this was not possible for the transpiration dataset 3 and 6 months long or for the net radiation at the hilltop weather station 15 months long and no split sample evaluation was performed for these variables for each simulation the goodness of fit gof for the calibration and for the evaluation datasets was quantified we used the mean absolute error mae for stream discharge because other metrics based on squared model data difference such as root mean square error rmse and nash sutcliffe efficiency nse nash and sutcliffe 1970 are known to overemphasize the constraint brought by high flow measurements and neglects low flow portions of the dataset krause et al 2005 legates and mccabe 1999 in addition high flow measurements typically carry more uncertainty and in our system flow conditions vary over several orders of magnitude see sect 3 conversely all other observables volumetric water content pine transpiration and net radiation comparatively display a much more compact and symmetric distribution with median values close to mean values rmse has been recommended when no information is given on model error distribution the latter is then conservatively assumed as being gaussian chai and draxler 2014 which is why we chose this metric in this case for cross variable evaluations of model performance after calibration the fit between the respective dispersion of model and data time series was quantified using pearson s correlation coefficient r complemented by the dataset normalized rmse extended to all variables where model data biases are notably taken into account to investigate the extent to which each type of measurement is capable of informing the calibration of a wide range of model parameters we assessed multiple calibration scenarios each using a different subset of the 10 datasets for a multi variable multi site calibration run as described in ala aho et al 2017b in this method the gof functions are used as informal measures of the likelihood the gof for each dataset was calculated for 100 000 monte carlo runs mae for streamflow rmse for the other outputs and the dataset specific empirical cumulative distribution function cdf of these was determined next these cdfs were used to identify the 30 best model runs the method iteratively identifies the quantile threshold common to all corresponding gof s cdfs below which the gof of exactly 30 model runs simultaneously meets the calibration target for the objectives in the case of using only one dataset as a constraint this is equivalent to finding the 30 smallest values for mae or rmse although it remains an aggregative approach to the multi objective problem cohon 1978 this simple method advantageously avoids having to combine the different gofs into a single numerical objective function note also that likelihood estimates are not used to guide the exploration of the model parameter space which eliminates potential search biases if the characteristics of the model error residuals are incorrectly specified in the likelihood function overall all this has five important advantages 1 no need to choose pooling weights to combine the individual objective into a single function 2 results are less sensitive to the choice of factors used to scale observations in dimensional gof functions 3 no compensatory effects between well performing and poorly fitted runs can occur 4 less risk that potentially good sections of the parameters space may be left unexplored if the specification of model errors are incorrect and 5 independence of runs make the process trivially parallelizable the first three of these advantages address some of the classical shortcomings in aggregated objective functions efstratiadis and koutsoyiannis 2010 the last two relax some of the disadvantages associated with more formal markov chain based search methods lastly predictive uncertainty pu was taken as the 90 spread for each simulated daily values across the 30 best runs thus avoiding making assumptions about the output distribution then averaged over the whole simulation period excluding the initial spin up depending on the analysis being carried out see sect 3 and 4 this uncertainty is kept dimensional eq 1a or normalized eq 1b 1a p u 1 n e v a l i 1 n e v a l m 95 t i m 5 t i 1b p u 1 n e v a l i 1 n e v a l m 95 t i m 5 t i m t i best runs where m 95 t i m 5 t i and m t i are respectively the 95th percentile the 5th percentile and average absolute value for the i th time step in the evaluation period 3 results 3 1 simulation of multiple data time series the model captures well the main characteristics of the stream hydrograph for the 45 months period oct 2012 june 2016 shown in fig 4 moderate and high flow conditions are well reproduced with a slight underestimation of low flows during summers especially in 2013 and 2015 the simulation of the hydrograph shows minor differences when the model was calibrated using the entire suite of observations versus using only streamflow data with modified kling gupta efficiency kge scores kling et al 2012 over the evaluation period fig 3 ranging from 0 60 to 0 95 across these best runs not shown in both calibration cases the 90 spread interval shows that the dispersal among the 30 best runs remains similarly small with respective pu values of 0 023 and 0 041 m3 s 1 fig 5 shows the time series of volumetric soil water content θ in the shallow subsurface at the four monitored sites shown in fig 1a at each location the depth averaged measured data see sect 2 4 is compared to thickness weighted averages of simulated θ in the two upper layers of ech2o in three calibration scenarios using the local soil moisture dataset using all four soil moisture datasets and constraining against all datasets the model generally provides consistent results in the peat in terms of timing and amplitude of θ dynamics fig 5a however simulated soil moisture is often too prompt in rewetting the peat in autumn while it displays an unrealistic drying event in the summer of 2015 similar observations can be made about the gley fig 5b where the model tends to underestimate the annual amplitude because simulated soil saturation is reached at lower volumetric water contents than observed in both peat and gley locations the different calibration scenarios display a very similar average behaviour across best runs but the 90 spread interval among time series grows significantly when adding more constraints with predictive uncertainties of 0 04 0 11 and 0 22 m3 m 3 peat and 0 02 0 08 and 0 25 m3 m 3 gley when respectively calibrating using θ measurements in peat all θ data and all datasets at the podzol at the upslope end of the transect the model satisfactorily captures soil moisture dynamics at daily to seasonal time scales fig 5c high frequency peaks of θ tend however to be underestimated by ech2o this discrepancy becoming more marked when adding more constraints in the calibration similar behaviour occurs in the podzolic soil at forest site b fig 5d while the simulated high frequency dynamics are consistent with the measurements the model markedly underestimates long term θ variations in both podzol locations the increase in predictive uncertainty as constraints are diversified is less marked than in the valley bottom peat and gley summer pine transpiration is well simulated by ech2o at both forest sites a and b fig 6 this is particularly true when calibrating the model against local and all transpiration datasets adding all other constraints leads ech2o to underestimate some peak values at forest site b fig 6b while in this configuration the baseline transpiration becomes underestimated at forest site a fig 6a fig 7 compares measured net radiation to the simulated top of canopy value averaged over vegetation and bare soil fractions at the three weather stations in all shown cases the temporal dynamics of the seasonal signal are well reproduced by the model as are the day to day fluctuations with a very small dispersal among best runs pu 5 5 w m 2 however ech2o tends to underestimate net radiation at all three sites which may indicate an overestimation of soil temperatures to compensate for potentially low evaporative losses see eq a1 and discussion in sect 4 1 this feature is especially marked at the heather dominated hillslope location fig 7c where the simulated summer net radiation is only half of the observed values 3 2 overall performance and uncertainty reduction the model performances are summarized using heat maps in the dual space of calibration scenarios and simulated variables fig 8 quantifying model data correlation r m o fig 8a and data average normalized rmse across the evaluation period r m s e m o fig 8b in these plots the columns θ all tp all rn all and all show the metrics averaged over one type of output or all outputs the most notable feature in these plots is that stream discharge is well reproduced regardless of the datasets used for calibration with r m o 0 89 p 001 in all cases and r m s e m o values between 0 53 and 0 68 secondly the temporal dynamics of volumetric water content is generally best captured in the podzolic soils podzol at transect and forest site b fig 8a while the lowest r m s e m o for soil moisture θ are found in the peat fig 8b soil moisture is reasonably simulated in the gley only when the corresponding dataset is included in the calibration although model data correlation there remains low in most cases similarly it is found that accurate simulation of transpiration is achieved only when the observations of transpiration are included in the calibration rows tp foresta tp forestb tp all and all fourth the scores for net radiation are somewhat insensitive to the calibration scenarios with good temporal dynamics but high r m s e m o due to the recurrent underestimation by the model mentioned in 3 1 for most individual observables column 1 to 5 7 to 8 and 10 to 12 the all datasets calibration scenario last row yields the highest scores surpassed only by the scores of each simulated state calibrated against its direct observation diagonals of fig 8 the improved model data fit with more constraints becomes clearer when the average scores over observable types are considered column 1 6 9 and 13 finally using all datasets as a simultaneous constraint yields the lowest overall model data misfit across observables bottom right square in fig 8b as compared to the rest of the last column the impact of different calibration scenarios on individual and overall predictive uncertainty of simulations i e the average 90 spread interval among best runs pu normalized as defined in eq 1b is shown in fig 9 following the same layout of fig 8 when the model is calibrated against individual datasets pu values for corresponding observables remain below 0 5 diagonals in fig 9 and generally below 1 when simulating other variables off diagonal squares even if the latter is not included in the calibration e g uncertainty of θ peat when calibrating against streamflow only 1st row and 2nd column notable exceptions are streamflow where pu remains above 1 whenever discharge is not included in the calibration and simulated pine transpiration for which this feature is even more marked a smaller overall predictive uncertainty is found for the all dataset calibration scenario pu 0 65 as it is the only case where large individual reductions in simulation dispersal are simultaneously achieved for streamflow soil moisture net radiation and to a smaller extent pine transpiration fig 9 last row 3 3 parameter values fig 10 shows the selected parameter values across the best runs within the prescribed sampling interval displaying only 5 calibration scenarios grouping all datasets of a same type and the all datasets constraint case calibrated soil depths show consistent results with deep 4 6 m soils in the valley bottom and shallower podzol and rankers 2 m somewhat reflected in the depth of the two upper hydraulic layers top row porosity takes markedly different values across soil types from 0 85 in the peat 0 5 0 7 in the gley down to 0 35 0 5 on the hillslope this spatial variability is somewhat mirrored by an increasing saturated horizontal hydraulic conductivity k h x over several orders of magnitude from 10 5 m s 1 in the peat to nearly 0 01 m s 1 in the rankers other hydrological parameters such as air entry pressure ψ a e residual soil moisture θ r and anisotropy k r a t i o mostly displays similar values across soil types and calibration scenarios centred in the sampling intervals the same applies for vegetation parameters such as optimal photosynthesis temperature t o p t soil water potential control on stomatal closure ψ d and maximum stomatal conductance g s m a x the light extinction coefficient k b e e r of heather and peat moss displays distinctively higher well constrained values when pine transpiration is used as a constraint for pine trees the sensitivity of stomatal conductance to soil water content c is much lower in the all datasets calibration case while canopy interception capacity c w s m a x becomes much higher than in other cases 3 4 independent evaluation at catchment scale the empirically based estimate of catchment wide saturated area fraction area sat is compared to the simulations provided by the 30 best parameters sets in five calibration scenarios using each data type plus the full suite of measurements in fig 11 in all cases the model broadly reproduces the observed temporal dynamics reasonably well but apart from the peaks the simulated saturation extent is generally overestimated slight differences appear between scenarios with higher area sat values associated with the highest predictive uncertainty 24 when only soil moisture is used as a constraint vwsc all while the stream discharge constraints brings the narrowest range of simulated area sat between best runs pu 17 4 discussion 4 1 insights into ecohydrological processes this study shows the ability of a process based model to consistently simulate not only water storages and fluxes in the critical zone at local to catchment scales but also energy balance and ecohydrological couplings in a comprehensive model evaluation exercise these are very encouraging results for the prospect of explicitly incorporating vegetation dynamics into a mechanistic description of catchment water partitioning and towards improved prediction of the functional changes that catchments in northern latitude are likely to experience in the coming decades the most robustly simulated observable was stream discharge with 90 of all optimisation selected runs 30 for each of the 14 calibrations scenarios 12 of the latter excluding discharge measurements showing modified kge scores between 0 67 and 0 87 across the simulation period 11 2012 06 2016 excluding spin up not shown in particular the model was able to capture well extreme events such as the 200 year return period flood during the winter 2015 2016 note that the kinematic approximation for groundwater and stream routing in ech2o neglects diffusive water redistribution through pressure gradients therefore this model data consistency points at a reasonable adequacy of using a gravity driven conceptualization of the bruntland burn catchment at the spatio temporal scales considered this hypothesis is consistent with strong topographic gradients in the catchment and the wet low energy hydroclimate which both sustain a quasi permanently saturated valley bottom tetzlaff et al 2014 and generally high water tables even on the steeper hillslopes blumstock et al 2015 these settings result in a flashy response of the stream network to run off events soulsby et al 2015 generally driven by saturation overland flow from the peat and gleys but in larger storm events the podzolic soils also connect to the saturated areas tetzlaff et al 2014 the general hydrological behaviour which is broadly representative of other northern boreal catchments tetzlaff et al 2015b contrasts with semiarid regions which are characterized by a more transient hydraulic connection and disconnection within hillslopes and between hillslopes and the channel during dry periods although bedrock topography remains critical to understand shallow subsurface flows in water limited environments e g jobbágy et al 2011 maneta et al 2008 diffusive effects by local pressure gradients are also highly relevant reducing the spatial extent for which local measurements are representative and limiting the propagation of information from the location where fluxes are measured to the location where model parameters need to be identified maneta and wallender 2013 more detailed insights into storage dynamics were provided by including volumetric water content for the upper half meter of soil profile into the calibration at the four monitored locations across the main hydropedological units a good model data fit was achieved in most cases fig 5 ech2o overall managed to capture the very different dynamics between locations across almost 13 data years including the variability within the same podzolic soil unit i e with a common set of parameters at two contrasting sites fig 5c and d further the calibration yields a depth of about 2 m of a hydrologically active profile on the hillslope which is much deeper than typical soil depth estimates from geophysical methods soulsby et al 2016 but consistent with the hypothesis that groundwater recharge on the hillslope and downhill movement actively contributes to saturation overland flow through exfiltration in the valley bottom ala aho et al 2017a birkel et al 2011 we note nonetheless that the riparian areas are the most challenging locations when attempting to capture soil moisture dynamics in part this reflects the very small variability measured though the modelled θ remained too reactive as compared to the damped variability in the measurements fig 5a and b in addition the gley porosity remained underestimated leading to unrealistic saturated conditions outside the summer it might also explain why the simulated gley soil seemed slightly too deep while it should be shallower than in the peat areas as the model likely compensated in order to close the water balance in the valley bottom the other notable model observation mismatch was the underestimation of the seasonal amplitude of net radiation at the hilltop site a location mostly covered by heather shrubs fig 1 using independent estimates of transpiration and total evapotranspiration et on a heather plot near the podzol transect in the same catchment wang et al 2017b a preliminary analysis hinted at an underestimation of transpiration and overestimation of soil evaporation in the ech2o model not shown by contrast net radiation was well simulated in the riparian areas suggesting an accurate estimation of energy conversion from radiative to turbulent fluxes such as soil evaporation and sensible heat further modelling measurement comparison studies focusing on evaporative processes will help better constraining the energy balance on the catchment hillslopes gong et al 2016 larsen et al 2016 it is uncommon in catchment scale calibration studies to use direct measurements of plot scale tree transpiration du et al 2014 wei et al 2016 this is not only because this type of measurements is rarely available but also because most current hydrologic models cannot single out the transpiration fraction of evapotranspiration méndez barroso et al 2014 paniconi and putti 2015 the inclusion of an ecohydrological observable such as plot scale transpiration of scots pine introduced direct knowledge of the exchanges between the physical and ecological components of the catchment and reduced the number of possible internal model configurations that were consistent with observed soil moisture and streamflow the calibrated model reproduced the major features of the transpiration time series surprisingly well which increases our confidence that the internal water and energy exchanges at bruntland burn were adequately captured at forest site b the simulated transpiration remained very similar across calibration scenarios over the summer 2016 fig 6b while soil moisture was more sensitive to the data used to calibrate the model fig 5d this is consistent with results reported using a data oriented approach wang et al 2017a which showed weak controls of soil moisture on pine transpiration outside infrequent dry periods in this humid catchment 4 2 information content brought by the different observations the multiplicity of datasets of this study used in different combinations for model calibration and evaluation brings novel insights in how informative and representative these measured quantities are for improving our modelling approach this may help with the design of more efficient data collection campaigns in the following we first discuss the spatio temporal footprint related to how time and or location specific the measured signal is the issue subsequently discussed is the behavioural footprint i e how specific to some processes the retrieved information content is streamflow was simulated reasonably well in all reported cases fig 8 but with a substantially higher predictive uncertainty whenever discharge data was not included in the calibration fig 9 streamflow is well known to integrate information of many catchment scale processes beven and binley 1992 but this knowledge is too ambiguous to determine the exact catchment configuration that produces the observed signal this is because streamflows integrate downstream following a convergent network towards a unique outlet but the divergent nature of an upstream network makes it impossible to uniquely backtrack the locations where the flow was generated kirchner et al 2001 this has two consequences streamflow can be well simulated with numerous alternative model parameterizations physically consistent or not kirchner 2006 and the spatio temporal and behavioural footprints are large and therefore less informative of individual processes happening at specific locations in the catchment guse et al 2016 this was illustrated in the predictive uncertainties of the simulated catchment states using only discharge as the calibration constraint yielded the most variable results for simulated soil moisture in the gley and net radiation fig 9 some variables were only well simulated when the model was calibrated against observations of that type which indicates a more restricted behavioural and spatio temporal footprint of the information for instance soil moisture in the gley displayed a significantly higher up to ten fold model observation mismatch whenever θ gley was not part of the calibration constraints even if other θ dataset were included in the calibration fig 8b 3rd column therefore exhibiting a very localized spatial footprint moisture in podzols on the other hand displayed slightly more homogeneous performances across calibration scenarios fig 8b 4th and 5th columns indicating that having two different podzolic soil moisture locations in this study additionally increased the spatial footprint of the associated calibration constraints transpiration in scots pine stands tp was also characterized by poor model data fits unless the calibration scenarios involved a transpiration dataset fig 8 performances remained consistent when using data from forest site b to calibrate tp at forest site a and vice versa even when the two sites cover different growing seasons 2015 and 2016 it indicates that in this catchment the tp datasets has a narrow behavioural footprint but a more extensive spatial and temporal footprint conversely this footprint of tp datasets made them ill suited as sole constraints to calibrate the model across processes as seen for example from soil moisture goodness of fit fig 8 and predictive uncertainty fig 9 the above considerations highlight the benefits of combining streamflow observations with other types of information that have a more specific footprint such as measurements of volumetric water content and transpiration du et al 2014 compared to using streamflow alone adding the two latter types of variables improved model calibration by increasing model data fit scores fig 8 and by reducing the dispersion among best runs fig 9 these enhanced performances were moreover generalizable to using a diversified combination of observations in the calibration as seen from the improved overall model data fit and low predictive uncertainty across model outputs it supports a mitigation of the equifinality problem as the increased number of scale and or process specific diagnostics clark et al 2011 helped discarding unfeasible model configurations that may otherwise have given high performance scores however we have also observed that the predictive uncertainty of some outputs the case for peat and gley soil moisture can increase substantially when the model was calibrated with increasing amounts of information this may be an indication that with the catchment functioning hypothesis embedded in the model the datasets have overlapping footprints that inform the calibration process with conflicting or inconsistent information note that such conflicts only occurred for individual outputs as the overall uncertainty across all outputs was indeed lowest when using the full of datasets in the calibration fig 9 bottom right square the monte carlo approach adopted in this study uses the gof as an informal measure of the likelihood of each of these parameters after the best parameters are selected the likelihood measure is not further used and the spread across best runs shown in fig 10 cannot be interpreted as a probability distribution this avoids having to make assumptions about the structure of model residual errors which in more formal statistical frameworks determine how the parameter space is sampled if these assumptions are incorrect some good sections of the parameter space may end up being excluded from the search note that parameters with good performance have been found in the entire range of permitted parameter values also as a result in our methodology all the selected parameter values contribute the same to the predictive spread and their average does not necessarily represent better the hydrologic behaviour of a given catchment unit in fact from fig 10 no single combination of parameters can be picked to represent better the average behaviour of the predictive ensemble for this interpreting parameter means in terms of the hydrologic behaviour of the catchment behaviour can easily be misleading however the fact that the mean value of the parameters fig 10 is often not at the centre of the feasible search space or of the interquartile range indicates that some values in the allowed range are more preferred than others and that the information contained in the alternative calibration datasets informs these preferences differently understanding the mechanisms by which the parameters are nudged in a specific direction when calibrated with a specific dataset is desirable but also difficult and complicates any meaningful interpretation of the differences between mean parameter values 5 conclusions in the growing field of critical zone modelling a process based description of energy plant water relationships is a promising basis for a mechanistic understanding of vegetation influence on water pathways and stores and projecting their responses to environmental change more generally these types of interdisciplinary models are increasingly needed in critical zone studies where water is a fundamental medium for energy and material cycles in a wide range of processes at multiple time scales white et al 2015 likely to be altered over time goddéris and brantley 2013 although the problem of equifinality is exacerbated with the increasing complexity of models using multiple measurements informative of the range of processes implemented in the models can assist in constraining models to a limited subset of feasible configurations when this is achieved newer more integrated models offer an opportunity for deeper process insight we demonstrated this by applying the fully distributed model ech2o to a small northern headwater catchment using different combinations of 10 datasets relative to 4 types of ecohydrological processes discharge soil moisture pine stand transpiration and above canopy net radiation while ech2o was able to perform well for single objectives when calibrated against individual datasets constraining its overall behaviour with multiple datasets in a rigorous multi objective calibration experiment yielded an improved cross output average model performance and smaller overall predictive uncertainty the resulting model configuration also reproduced the main features of the temporal dynamics of an independent estimate of catchment scale saturated area fraction successful comparison against this independent dataset indicated that the internal transient storage dynamics were generally captured correctly by the model we also discussed the informational footprint resulting from each dataset across scales spatio temporal footprint and processes behavioural footprint this modelling experiment increases our confidence that a data intensive calibration approach constrains the set behavioural model configurations in an effective way i e that our approach allows for broadly getting the right answers for the right reasons kirchner 2006 among other approaches ongoing model extensions to include tracking of stable water isotopes 2h and 18o fluxes and water age across ecohydrological compartments will provide means to test this further the experiments to date provide a foundation for using ech2o to project the impact of climate variability on catchment functioning in sensitive high latitude systems acknowledging that we have applied ech2o in a location where snowfall is quite modest a critical next step will be to conduct simulations in snowier catchments more generally we intend to assess the reciprocal links between ecosystem functioning land cover change and the mediating role of vegetation in buffering atmospheric impacts on water fluxes and storage software availability the source code of the ech2o model c programming language is available at https bitbucket org maneta ech2o while the associated documentation compiled binaries and case study files can be found at http hs umt edu regionalhydrologylab software default php the python routine and dataset used for calibration are available upon request to the authors acknowledgements this work was funded by the european research council project ga 335910 vewa m maneta acknowledges support from the u s national science foundation project gss 1461576 and u s national science foundation epscor cooperative agreement eps 1101342 all model runs were performed using the high performance computing hpc cluster of the university of aberdeen and the it service is thanked for its help in installing pcraster and other libraries necessary to run ech2o and post processing python routines on the hpc cluster finally the authors are grateful to the many people who have been involved in establishing and continuing data collection at the bruntland burn particularly christian birkel maria blumstock jon dick josie geris konrad piegat claire tunaley and hailong wang appendixrecent developments in ech2o model in the following equations the parameters calibrated in this study table 1 are highlighted in bold font a1 canopy processes canopy level processes link the radiation budget solar radiation incoming longwave and outgoing longwave radiation to conductive energy transfers sensible heat evaporative losses from the canopy evaporation of intercepted water and plant transpiration soil water availability and soil water potential energy the core of the canopy processes is a set of 3 equations energy balance soil water balance and soil water potential energy and 3 unknowns canopy temperature tc plant available soil moisture st 1 and soil matric potential ψsoil solved for each vegetation type present in the grid cell a 1 s t 1 s η θ r z r 95 δ t l e t t c ψ s o i l ρ w λ v 0 ψ ae s t 1 λ bc ψ s o i l 0 n r t c h t c l e t t c ψ s o i l l e 0 the topmost equation in a 1 is the water balance in the root zone after infiltration has been accounted for in the initial soil moisture where s t is the weighted average degree of soil saturation in the soil over z r95 at the beginning of the time step s t 1 is the weighted average degree of soil saturation over z r95 at the end of the time step in both cases the weights for the saturation averages are given by the fraction of roots in each layer of the soil additionally η is soil porosity θ r is residual moisture content z r95 is the total soil depth of the layers containing 95 of roots δ t is the size of the time step ρ w is density of liquid water λ v is latent heat of vaporization and l e t t c ψ s o i l is a function calculating latent heat consumed for transpiration which is dependent on the temperature of the canopy and the soil water potential the second equation in a 1 is the brooks and corey prognostic equation where ψ a e is the soil air entry pressure ψ s o i l is soil matric potential and λ bc is the pore size index brooks and corey exponent parameter the lower equation in a 1 is the energy balance in the canopy as described in maneta and silverman 2013 but reproduced here for completeness this equation assumes that the available radiative energy nr will be consumed as sensible heat h as latent heat from transpiration let or as latent heat from evaporation of intercepted water le a 2 n r t c r s w 1 α c 1 exp k beer l a i ε c r l w ε c σ t c 4 a 3 h t c ρ a c a t a t c r a a 4 l e t t c ψ s o i l ρ a c a e a e s h r γ r a r s ψ s o i l a 5 l e ρ a c a e a e c h r 1 h r c w s cm s m a x γ r a where r s w is incoming solar radiation α c is the effective canopy albedo k b e e r is a light extinction coefficient as per beer s law l a i is leaf area index ε c is the canopy emissivity r l w is downwelling long wave radiation σ is the stefan boltzmann constant t c is the effective canopy temperature ρ a is density of air at air temperature t a c a is the heat capacity of air e a is air vapour pressure at t a e c is the canopy saturation vapour pressure at t c h r is air relative humidity γ is the psychrometric constant r a is aerodynamic resistance r s is stomatal resistance cws is the current amount of canopy water storage and cws max is the maximum canopy storage allowed by unit lai see maneta and silverman 2013 for additional details on the calculation of these quantities the current calculation of stomatal resistance differs from the original formulation in that the efficiency factor that provided the stomatal dependency on soil moisture has been changed to a dependency on soil water potential the new stomatal resistance formulation and the soil water potential efficiency factor are a 6 r s g s max l a i f r s w f t a f v p d f ψ s o i l 1 f ψ s o i l 1 1 ψ s o i l ψ d c in which ψ d is the soil water potential at which 0 5 efficiency for soil water potential is achieved and c is a function shape parameter other efficiency factors are calculated as in maneta and silverman 2013 a2 soil hydrology the soil hydrology component has been improved over the original formulation in maneta and silverman 2013 by including a vertical soil water redistribution model with three hydraulic layers the topmost layer receives infiltration from the surface its soil moisture content controls infiltration rates in the green and ampt infiltration equation and is also the only layer from which evaporation occurs the middle layer typically contains most of the root system and therefore its moisture controls the hydrologic limitation to transpiration the deepest layer transfers gravitational water laterally using the original kinematic wave formulation the condition at the bottom of the third layer can now be impervious no flow or can leak at a rate given by a bedrock leakance parameter as in the original formulation only water in excess of field capacity can move by gravity when soil water exceeds field capacity in the topmost or second layer the water excess gravitational water moves downward to the next layer at a rate determined by the linearized unsaturated hydraulic conductivity function k θ k hx k hratio l b θ θ r θ f c θ r where k h x is the saturated horizontal hydraulic conductivity k h r a t i o is the anisotropy ratio while l b is a bedrock leakance parameter set to 1 for layers 1 and 2 and that can vary between 0 no flow and 1 free gravitational drainage for soil hydraulic layer 3 water leaking through the bedrock leaves the domain horizontal water transfers to the downstream cell only occur in the third soil hydraulic layer following the linearized kinematic wave formulation described in maneta and silverman 2013 when the storage capacity of the bottom layer is exceeded saturation excess water is transferred to the middle layer increasing the middle layer moisture content if the middle layer saturates saturation excess is transferred to the topmost layer increasing the topmost layer moisture content if the topmost layer saturates saturation excess produces return flow to the surface seepage face return flow is added to the pool of ponded water that generates overland flow the following time step a3 channel routing channel flow is simulated using a 1d solution of the kinematic wave equation the equation is solved for stream discharge using a power function a α q β to relate cross section flow area to stream discharge a 7 q x α β q β 1 q t q b f q o v l 0 where q l3t 1 is stream discharge q b f l2t 1 are groundwater contributions to streamflow per unit length of channel q o v f l2t 1 are overland flow contributions to streamflow per unit length of channel and x l and t t are distance in the flow direction and time respectively using manning s equation to approximate flow velocity and assuming rectangular channel cross sections parameters α and β can be determined to be n p 2 3 s 3 5 and β 3 5 in which n tl 1 3 is manning s roughness coefficient p l is the channel wetted perimeter approximated by channel width and s is the streambed slope equation a 7 is solved using a first order implicit finite difference scheme and is unconditionally stable a4 exponential root profile the new model formulation requires the fraction of roots in each soil layer f root l 1 2 3 to be specified in this paper we have assumed an exponential root profile modulated by a single parameter kroot a 8 f r o o t l 1 1 exp k root d l1 1 exp k root d soil f r o o t l 2 exp k root d l1 exp k root d l1 d l2 1 exp k root d soil f r o o t l 3 1 f r o o t l 1 f r o o t l 2 where d s o i l is the total depth of the soil hydrologically active layer and d l 1 2 are the depth of layers 1 or 2 appendix a supplementary data the following is the supplementary data related to this article supplementarymaterials revised docx supplementarymaterials revised docx appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 01 001 
26444,we assessed whether a complex process based ecohydrological model can be appropriately parameterized to reproduce the key water flux and storage dynamics at a long term research catchment in the scottish highlands we used the fully distributed ecohydrological model ech2o calibrated against long term datasets that encompass hydrologic and energy exchanges and ecological measurements applying diverse combinations of these constraints revealed that calibration against virtually all datasets enabled the model to reproduce streamflow reasonably well however parameterizing the model to adequately capture local flux and storage dynamics such as soil moisture or transpiration required calibration with specific observations this indicates that the footprint of the information contained in observations varies for each type of dataset and that a diverse database informing about the different compartments of the domain is critical to identify consistent model parameterizations these results foster confidence in using ech2o to contribute to understanding current and future ecohydrological couplings in northern catchments keywords catchment hydrology ecohydrology process based modelling multi objective calibration information content ech2o 1 introduction numerical models are crucially important in the environmental sciences models can complement and integrate theory and empirical data by incorporating testable hypotheses and by extending knowledge at spatial and or temporal scales inaccessible to current observation methods in particular process based models seek to explicitly represent the state variables and fluxes that are theoretically observable and can be used in the closure of assumed forms of the laws of conversation of mass energy and momentum at temporal scales characterizing the underlying physical processes adapted from fatichi et al 2016 in contrast to conceptual and empirical approaches physically based models facilitate investigation of specific variables at local process specific scales e g endrizzi et al 2014 manoli et al 2017 niu and phanikumar 2015 pierini et al 2014 additionally a fully distributed description of the simulation domain opens the possibility for tracking intra system patterns and dynamics e g maxwell and condon 2016 pierini et al 2014 a task much less accessible to coarser spatial representations i e lumped or semi distributed models combining these two methodological choices with physically based fully distributed models is thus a way to disentangle feedbacks and non linear dynamics across fundamentally different processes e g drewry et al 2010 tague 2009 and better predict system behaviour outside recorded environmental conditions seibert 2003 uhlenbrook et al 1999 these tools are of particular relevance for the emerging field of critical zone science national research council 2012 which seeks integrated understanding of ecological geological geomorphological and pedological processes within a framework of hydrological partitioning brooks et al 2015 within the field of hydrology the issue of appropriate model complexity is a focus of ongoing discussion the corollary of expanding process based approaches towards an universal model is an inevitable increase in complexity as explicit descriptions of additional system characteristics are added e g topography soil texture tree height canopy density etc band et al 2001 maxwell and condon 2016 arguing that many of these numerous parameters cannot be appropriately measured some fear that evolution of complex multi disciplinary models only layer up unavoidable uncertainty and are prone to equifinality whereby several combinations of parameter values realistic or not yield comparable performance e g beven and binley 1992 beven and freer 2001 mcdonnell et al 2007 the utility of measurements to help constrain the model solution space and identify feasible model configurations has been an increasingly central issue in hydrological model calibration sufficiently informative observations are necessary to ensure that the goodness of model data fit attained effectively translates into physically sound information for the internal model parameters i e getting the right answers for the right reasons beven and binley 1992 kirchner 2006 the problem of equifinality a particular case of underdetermination duhem 1954 is apparent when stream discharge is the only monitored variable available for calibration unfortunately this remains the most common situation the widespread use of streamflow time series to calibrate and validate models has spurred the development of elaborate single and multiple criteria goodness of fit metrics kling et al 2012 krause et al 2005 legates and mccabe 1999 madsen 2003 van werkhoven et al 2009 and calibration algorithms duan et al 1992 gupta et al 1998 sorooshian and dracup 1980 tang et al 2007 tolson and shoemaker 2007 directed toward extracting a maximum of information content from this type of data he et al 2015 rouhani et al 2007 shafii et al 2017 however the information contained in streamflow time series is often insufficient to inform the parameterization of physically based models parameter values that represent physical properties of the catchment are usually poorly identified and become very sensitive to boundary conditions maneta et al 2007 the situation deteriorates as more complex models incorporate increasingly detailed descriptions of catchment functioning to constrain parameters of components associated with different subdomains of the model ecological surface subsurface etc it is desirable but often impractical to diversify data sources fang et al 2013 larsen et al 2016 rajib et al 2016 thorstensen et al 2015 combining different types of observations reduces information redundancy and provides direct insights into the different groups of physical processes represented in the model clark et al 2011 fatichi et al 2016 a data extensive approach to model calibration makes the choice of performance metrics easier because the information contained in observations is more directly related to the model compartment being calibrated e g birkel et al 2014 information diversity however brings other issues related to the assimilation of observations with diverse characteristics during calibration some are technical e g combining spatio temporal scales and associated uncertainties while others are more fundamental to modelling e g parameters compensating for model imperfections clark and vrugt 2006 or overlapping constraints and thus possibly pulling the model in different directions efstratiadis and koutsoyiannis 2010 in other research fields this approach is exemplified by the current efforts and associated challenges in assimilating multiple types of carbon cycle data to optimise earth system models kaminski et al 2013 peylin et al 2016 the ecohydrology of high latitude energy limited landscapes has traditionally been understudied despite the global ecological importance of this region since studies of plant water couplings across disciplines gained momentum in the late 90s bonell 2002 research efforts in ecohydrology have been primarily conducted in environments where water scarcity newman et al 2006 or permanent presence e g wetlands rodriguez iturbe et al 2007 makes hydrology an obvious critical control upon how plants distribute and compete only recently efforts have been directed towards understanding the specific ecohydrological processes of boreal energy limited regions e g cable et al 2014 while there have been process based model developments dedicated to the hydrology of high latitude environments e g endrizzi et al 2014 kuchment et al 2000 lindström et al 1997 pomeroy et al 2007 most model applications in these regions lack an explicit implementation of vegetation dynamics e g ala aho et al 2017a and thus cannot finely capture ecosystem imprints on water partitioning at the catchment scale high latitude regions comprise mixed temperate forests boreal forests and tundra covering nearly 20 of the continental land mass tetzlaff et al 2015a these regions are subject to rapid climate change with significant regional to global scale implications hinzman et al 2013 including shifts in precipitation regime and snow mediated water balance bintanja and andry 2017 jiménez cisneros et al 2014 and associated implications for runoff generation peterson et al 2002 zhang et al 2014 while such environmental change has been observed to alter water pathways and flow regimes dye and tucker 2003 mcclelland et al 2006 tetzlaff et al 2013 and ecosystem dynamics naito and cairns 2015 piao et al 2008 further work is needed to identify the underlying mechanisms reasons for the limited understanding so far lie in the fine scale landscape heterogeneity and the implications for spatial variation in energy inputs as well as the logistical difficulties of collecting data in comparatively remote areas pomeroy et al 2013 tetzlaff et al 2013 and the alarming recent decline in long term monitoring of northern catchments laudon et al 2017 however we need to understand such processes and the related uncertainties of water cycling in these regions while ongoing projected biome shifts e g beck et al 2011 williams et al 2007 call for particular scrutiny of ecosystem influence on water availability law 1956 and vice versa in this study our main aim was to investigate to what extent a data extensive approach to calibration can constrain the range of behavioural configurations of a highly parameterized physically based model such that the achieved parameter sets can be used as falsifiable hypotheses of the internal functioning of the catchment for this we used a distributed ecohydrologic model ech2o see maneta and silverman 2013 that integrates a kinematic hydrologic and energy balance model with a vegetation dynamics model the model is calibrated using several combinations of data types covering a range of ecohydrological variables collected at a long term experimental northern montane catchment we ask the following questions through our modelling experiments 1 what are the physical insights gained across ecohydrological processes 2 how valuable are the information contents brought by the different constraining datasets addressing these questions will help building a robust ecohydrological modelling framework dedicated to critical zone functioning in high latitude environments 2 material and methods 2 1 study site the bruntland burn fig 1 is a small catchment 3 2 km2 located in the eastern scottish highlands 57 8 n 3 20 w it is a headwater of the river dee which provides drinking water for the city of aberdeen 250 000 people ecosystem services such as an atlantic salmon fishery and has eu conservation designations the region receives around 1100 mm of average annual precipitation p distributed quite evenly throughout the year although november february and june august are usually wettest and driest periods respectively less than 5 of p occurs as snowfall the climatic water balance is energy limited with 400 mm of annual potential evapotranspiration pet the mean annual temperature t is 7 c with no monthly averaged t below 0 c in a transitional temperate boreal oceanic climate the local topography reflects glacier retreat with a wide valley bottom 220 m a s l surrounded by steeper slopes reaching up to 560 m a s l fig 1a this slope gradient is reflected by widespread glacial drift deposits 60 of the catchment with depths ranging from 40 m in the valley bottom to 5 m on steeper slopes these deposits are mostly saturated and form significant groundwater reservoirs that sustain stream base flow and maintain wet conditions in the valley bottom soulsby et al 2016 the pedology comprises deep 0 5 4 m organic rich soils histosols peat and gley in the riparian area bordering the stream channel network fig 1b these soils are persistently saturated and rapid overland flow is the dominant runoff generation mechanism following rainfall events tetzlaff et al 2014 hillslopes are characterized by shallower freely draining podzols spodosols overlying moraines and marginal ice deposits while thin regosols rankers dominate above 400 m a s l where the drift is thin or absent fig 1b these hydropedological units are somewhat reflected in the vegetation cover fig 1c f podzols and rankers predominantly support heather shrublands calluna vulgaris and erica spp though this land cover is the result of overgrazing by red deer cervus elaphus and sheep scots pine trees pinus sylvestris the naturally occurring vegetation is confined to the northern steep hillslopes and to plantation stands near the catchment outlet riparian gley soils are characterized by herbaceous cover molinia caerulea the latter being also found as secondary species in the peat where bog mosses sphagnum spp dominate the land cover 2 2 the ecohydrological model ech2o we used a new formulation of the spatially distributed process based model ech2o maneta and silverman 2013 here ech2o couples a two layer canopy and understory vertical energy balance scheme fig 2 a a kinematic hydrologic module solving vertical and lateral water transfers fig 2b and a transpiration based simulator of carbon uptake and allocation for plant growth lozano parra et al 2014 maneta and silverman 2013 the reasons for choosing ech2o lie in its original development aimed at filling a research gap between hydrology focused catchment models and land surface models lsms simulating biophysical and biogeochemical cycles in the critical zone while catchment models provide a fit for purpose conceptualization of water pathways in most cases they lack a process based representation of energy balance and plant water interactions on the other hand most state of art lsms have historically been developed as surface components of climate models to be run over large regions or continents despite the recent advances in representing land processes such as vegetation phenology and carbon nutrient cycles hydrology remained simplistic in most lsms overland flow routing channel routing and lateral subsurface flow are typically neglected or highly simplified see fan 2015 for a further discussion without these components it is not possible to study the ecohydrological effects of upstream water subsidies and the spatial organization of catchments imprinted by the water redistribution network this is especially critical for studies in small catchments at high spatial resolution like the one studied here finally the parsimonious implementation of ech2o was preferred over other recent potentially more sophisticated ecohydrological models e g fatichi et al 2012 maxwell and condon 2016 ech2o s calculation of energy and water fluxes water storage dynamics and vegetation states is made at time steps subordinated to that of the meteorological forcing the atmospheric boundary conditions for each time step are precipitation p incoming shortwave radiation rsw downwelling longwave radiation rlw t maximum ta max minimum ta min and average ta mean during the time step period relative humidity hr and wind speed wxy new developments that we implemented in ech2o are documented in the appendices to complement other model details described elsewhere lozano parra et al 2014 maneta and silverman 2013 here we provide a brief summary of the model philosophy and main features the spatial domain of ech2o is mapped on a regular grid defined by that of the input digital elevation data each cell of the domain can have multiple vegetation covers including bare soil the energy and the water balance is solved for each cover and integrated over the cell area according to the fraction of the cell they occupy the energy balance equations are solved at the top of the canopy see appendix a1 for details and then at the soil or snowpack surface turbulent fluxes momentum heat and vapour are resolved using a first order local closure approximation under gradient similarity theory valid for small eddies and neutral stratification conditions canopy interception is simulated using a linear bucket approach the partition of p and throughfall between the solid snow and liquid components during a time step is done according to the minimum and maximum air t during the time step and to a snow rain transition t threshold snowpack melt and liquid throughfall feed surface ponding which infiltrates following a green and ampt approximation of the richard s equation mays 2010 all ponded water at the end of each time step becomes overland flow i e run on to the downstream cell where it can reinfiltrate or in turn generate further overland flow this calculation cascade follows the local drainage direction until the remaining surface water reaches the outlet or a channel cell flow in a channel is routed using a 1d solution for a kinematic wave see appendix a2 for details the soil is divided into three hydraulic layers the shallow topsoil where soil evaporation takes place the intermediate layer which typically shares the bulk of the roots with the topsoil and the bottom layer where groundwater can be transferred laterally to the downstream cell or seep into the stream vertical water redistribution is based on the theory that only soil moisture in excess of field capacity gravitational water can move under gravity to deeper soil layers or laterally to the next cell downstream diffusive effects driven by local pressure gradients are therefore assumed to be negligible and water below field capacity is retained by the soil and only removed by evapotranspiration simulation of vegetation dynamics are adapted from the 3 pg and treedyn3 models bossel 1996 landsberg and waring 1997 peng et al 2002 with differentiated carbon allocation and growth schemes for ligneous and herbaceous species lozano parra et al 2014 a jarvis type model is used to simulate the response of canopy conductance to environmental drivers cox et al 1998 jarvis 1976 2 3 model setup and landscape characterization all simulations were performed at a 30 30 m2 resolution a lidar derived 1 1 m2 dem lessels et al 2016 was used to delineate the catchment boundary and further processed with the pcraster tool suite http pcraster geo uu nl to obtain local slopes and local drainage direction the latter was determined for each cell using the steepest descent among the eight adjacent cells d8 algorithm fairfield and leymarie 1991 the model was run at daily time steps simulations covered a 64 month long time period from june 2011 to september 2016 with the period from june 2011 to october 2012 used for model spin up and therefore discarded from the analysis see sect 2 5 p ta mean ta min ta max hr and wxy data were collected at three meteorological stations installed in the catchment in different landscape positions valley bottom bog and hilltop fig 1a and used from july 2014 prior to that period p was interpolated using a square elevation inverse distance weighted algorithm applied to five scottish environment protection agency sepa rain gauges located around the bruntland burn catchment within 10 km similarly to birkel et al 2011 while ta mean hr and wxy fields were taken from the balmoral station 5 km nw as available from the centre for environmental data analysis ceda met office 2017 ta min and ta max prior to july 2014 rsw and rlw whole study period were retrieved from the era interim climate reanalysis dee et al 2011 finally we took into account altitudinal effects on p and t by respectively assuming a 5 5 increase of p every 100 m a s l as measured along a hillslope covering 200 m elevation difference ala aho et al 2017b and a decrease of 0 6 c 100 m a s l based on the moist adiabatic temperature lapse rate goody and yung 1995 soil hydrological properties were mapped by splitting the model domain into four hydropedological units aggregated from the soil classes defined by the hydrology of soil types host fig 1b tetzlaff et al 2007 energy related soil characteristics were considered as spatially uniform five land cover types were considered scots pine stands heather moorland peat moss grassland and bare rock scree vegetation fraction of scots pine in each cell fig 1c was estimated by aggregating a 1 1 m2 resolution lidar canopy cover measurements conducted over the catchment to the 30 30 m2 grid used for simulations for the other vegetation fractions we additionally used the soil classification extensive land use mapping and aerial imagery tetzlaff et al 2007 heather was assumed to occupy 95 of the treeless surface of podzols and rankers except for the steep northern rocky hillslopes 40 cover in treeless areas a few sparsely vegetated moorlands on the west hillslopes 20 cover in treeless areas and the gley 5 cover fig 1d peat moss was assumed to occupy 90 of an extensive raised peat bog in the nw parts of the catchment see fig 1 in sprenger et al 2017 and 70 of the remaining peat soils fig 1e in the latter areas molinia grasses compete 30 cover with peat moss while grasslands are dominant on gley soils 95 of the tree and shrub free surface and in patches of managed lands near the catchment outlet fig 1f the list of model parameters calibrated in this study is given in table 1 we selected 16 types of parameters based on a preliminary sensitivity analysis following morris 1991 performed with the calibration dataset described in section 2 4 as shown in table 1 10 parameters are soil dependent and 6 are vegetation specific since we considered 4 soil types and 4 vegetation types the total number of calibrated variables was 64 10 4 6 4 the uncalibrated model parameterization including initial conditions was prescribed based on literature values and expert knowledge supplementary table s1 2 4 calibration and evaluation datasets a specific advantage of the bruntland burn experimental catchment is the length and diversity of data records across ecohydrological processes which is unusual for northern regions we made use of 10 datasets for the calibration as summarized in fig 3 daily discharge at the catchment outlet fig 1a was derived from 15 min stage height records odyssey capacitance probe christchurch new zealand using a rating curve calibrated for a stable stream section soil moisture data was collected at 15 min intervals at four locations three of them along a transect representative of the main hydropedological units podzol gley and peat on heather tetzlaff et al 2014 and one plot in a scots pine forest wang et al 2017a forest site b fig 1a we used time domain reflectometry tdr soil moisture probes model cs616 campbell scientific inc usa located 0 1 0 2 and 0 4 m beneath the surface corresponding to the main soil horizons geris et al 2015 except in the peat where only two probes were present at depths of 0 1 and 0 2 m additionally each group of probes was replicated at the same depths but 2 m apart these tdr sensors were calibrated using laboratory analyses of gravimetric soil water content and bulk density from samples collected at each horizon geris et al 2015 finally a single daily vertically averaged volumetric water content value was used for calibration and evaluation of the ech2o model scots pine transpiration tp was measured between july and september 2015 at forest site a and between april and september 2016 at forest site b fig 1a using 32 sets of granier type thermal dissipation sap flow sensors dynamax inc houston usa installed on 10 and 14 trees in the forest sites a and b respectively with 2 to 4 sensors per tree depending on the stem size 10 32 cm in diameter average stand scale pine transpiration was derived using a sapwood area to tree diameter relationship estimated from incremental wood cores sampling in surrounding trees at the end of the study period see wang et al 2017a for more details and then daily averaged net radiation rn was measured every 15 min at the three meteorological stations fig 1a and then daily averaged finally to provide a novel independent verification of the model s ability to represent seasonal storage dynamics we compared modelled and empirical based estimates of catchment scale saturation area from june 2011 to september 2014 a conceptual rainfall runoff model that linked antecedent wetness and soil moisture to saturation area was used to estimate the extent of saturation in the catchment this model was previously calibrated against maps of measured saturation area extent and isotopic tracers measurements ali et al 2014 birkel et al 2010 these estimates were compared with saturation extent in ech2o which was defined as the proportion of cells in the domain where the volumetric water content in the top hydraulic layer exceeded 99 of soil porosity 2 5 model data fusion method we sampled the parameter space by conducting 100 000 monte carlo simulations using uniform parameter distributions with prescribed bounds based on literature values and prior experience table 1 each run spanned the entire 64 month simulation period but the first 16 months of each run were used to spin up the model and stabilize water storages and flux simulations notably soil moisture and stream discharge after discarding the spin up period the calibration dataset was split whenever possible into two non overlapping periods one for calibration and one for evaluation fig 3 however this was not possible for the transpiration dataset 3 and 6 months long or for the net radiation at the hilltop weather station 15 months long and no split sample evaluation was performed for these variables for each simulation the goodness of fit gof for the calibration and for the evaluation datasets was quantified we used the mean absolute error mae for stream discharge because other metrics based on squared model data difference such as root mean square error rmse and nash sutcliffe efficiency nse nash and sutcliffe 1970 are known to overemphasize the constraint brought by high flow measurements and neglects low flow portions of the dataset krause et al 2005 legates and mccabe 1999 in addition high flow measurements typically carry more uncertainty and in our system flow conditions vary over several orders of magnitude see sect 3 conversely all other observables volumetric water content pine transpiration and net radiation comparatively display a much more compact and symmetric distribution with median values close to mean values rmse has been recommended when no information is given on model error distribution the latter is then conservatively assumed as being gaussian chai and draxler 2014 which is why we chose this metric in this case for cross variable evaluations of model performance after calibration the fit between the respective dispersion of model and data time series was quantified using pearson s correlation coefficient r complemented by the dataset normalized rmse extended to all variables where model data biases are notably taken into account to investigate the extent to which each type of measurement is capable of informing the calibration of a wide range of model parameters we assessed multiple calibration scenarios each using a different subset of the 10 datasets for a multi variable multi site calibration run as described in ala aho et al 2017b in this method the gof functions are used as informal measures of the likelihood the gof for each dataset was calculated for 100 000 monte carlo runs mae for streamflow rmse for the other outputs and the dataset specific empirical cumulative distribution function cdf of these was determined next these cdfs were used to identify the 30 best model runs the method iteratively identifies the quantile threshold common to all corresponding gof s cdfs below which the gof of exactly 30 model runs simultaneously meets the calibration target for the objectives in the case of using only one dataset as a constraint this is equivalent to finding the 30 smallest values for mae or rmse although it remains an aggregative approach to the multi objective problem cohon 1978 this simple method advantageously avoids having to combine the different gofs into a single numerical objective function note also that likelihood estimates are not used to guide the exploration of the model parameter space which eliminates potential search biases if the characteristics of the model error residuals are incorrectly specified in the likelihood function overall all this has five important advantages 1 no need to choose pooling weights to combine the individual objective into a single function 2 results are less sensitive to the choice of factors used to scale observations in dimensional gof functions 3 no compensatory effects between well performing and poorly fitted runs can occur 4 less risk that potentially good sections of the parameters space may be left unexplored if the specification of model errors are incorrect and 5 independence of runs make the process trivially parallelizable the first three of these advantages address some of the classical shortcomings in aggregated objective functions efstratiadis and koutsoyiannis 2010 the last two relax some of the disadvantages associated with more formal markov chain based search methods lastly predictive uncertainty pu was taken as the 90 spread for each simulated daily values across the 30 best runs thus avoiding making assumptions about the output distribution then averaged over the whole simulation period excluding the initial spin up depending on the analysis being carried out see sect 3 and 4 this uncertainty is kept dimensional eq 1a or normalized eq 1b 1a p u 1 n e v a l i 1 n e v a l m 95 t i m 5 t i 1b p u 1 n e v a l i 1 n e v a l m 95 t i m 5 t i m t i best runs where m 95 t i m 5 t i and m t i are respectively the 95th percentile the 5th percentile and average absolute value for the i th time step in the evaluation period 3 results 3 1 simulation of multiple data time series the model captures well the main characteristics of the stream hydrograph for the 45 months period oct 2012 june 2016 shown in fig 4 moderate and high flow conditions are well reproduced with a slight underestimation of low flows during summers especially in 2013 and 2015 the simulation of the hydrograph shows minor differences when the model was calibrated using the entire suite of observations versus using only streamflow data with modified kling gupta efficiency kge scores kling et al 2012 over the evaluation period fig 3 ranging from 0 60 to 0 95 across these best runs not shown in both calibration cases the 90 spread interval shows that the dispersal among the 30 best runs remains similarly small with respective pu values of 0 023 and 0 041 m3 s 1 fig 5 shows the time series of volumetric soil water content θ in the shallow subsurface at the four monitored sites shown in fig 1a at each location the depth averaged measured data see sect 2 4 is compared to thickness weighted averages of simulated θ in the two upper layers of ech2o in three calibration scenarios using the local soil moisture dataset using all four soil moisture datasets and constraining against all datasets the model generally provides consistent results in the peat in terms of timing and amplitude of θ dynamics fig 5a however simulated soil moisture is often too prompt in rewetting the peat in autumn while it displays an unrealistic drying event in the summer of 2015 similar observations can be made about the gley fig 5b where the model tends to underestimate the annual amplitude because simulated soil saturation is reached at lower volumetric water contents than observed in both peat and gley locations the different calibration scenarios display a very similar average behaviour across best runs but the 90 spread interval among time series grows significantly when adding more constraints with predictive uncertainties of 0 04 0 11 and 0 22 m3 m 3 peat and 0 02 0 08 and 0 25 m3 m 3 gley when respectively calibrating using θ measurements in peat all θ data and all datasets at the podzol at the upslope end of the transect the model satisfactorily captures soil moisture dynamics at daily to seasonal time scales fig 5c high frequency peaks of θ tend however to be underestimated by ech2o this discrepancy becoming more marked when adding more constraints in the calibration similar behaviour occurs in the podzolic soil at forest site b fig 5d while the simulated high frequency dynamics are consistent with the measurements the model markedly underestimates long term θ variations in both podzol locations the increase in predictive uncertainty as constraints are diversified is less marked than in the valley bottom peat and gley summer pine transpiration is well simulated by ech2o at both forest sites a and b fig 6 this is particularly true when calibrating the model against local and all transpiration datasets adding all other constraints leads ech2o to underestimate some peak values at forest site b fig 6b while in this configuration the baseline transpiration becomes underestimated at forest site a fig 6a fig 7 compares measured net radiation to the simulated top of canopy value averaged over vegetation and bare soil fractions at the three weather stations in all shown cases the temporal dynamics of the seasonal signal are well reproduced by the model as are the day to day fluctuations with a very small dispersal among best runs pu 5 5 w m 2 however ech2o tends to underestimate net radiation at all three sites which may indicate an overestimation of soil temperatures to compensate for potentially low evaporative losses see eq a1 and discussion in sect 4 1 this feature is especially marked at the heather dominated hillslope location fig 7c where the simulated summer net radiation is only half of the observed values 3 2 overall performance and uncertainty reduction the model performances are summarized using heat maps in the dual space of calibration scenarios and simulated variables fig 8 quantifying model data correlation r m o fig 8a and data average normalized rmse across the evaluation period r m s e m o fig 8b in these plots the columns θ all tp all rn all and all show the metrics averaged over one type of output or all outputs the most notable feature in these plots is that stream discharge is well reproduced regardless of the datasets used for calibration with r m o 0 89 p 001 in all cases and r m s e m o values between 0 53 and 0 68 secondly the temporal dynamics of volumetric water content is generally best captured in the podzolic soils podzol at transect and forest site b fig 8a while the lowest r m s e m o for soil moisture θ are found in the peat fig 8b soil moisture is reasonably simulated in the gley only when the corresponding dataset is included in the calibration although model data correlation there remains low in most cases similarly it is found that accurate simulation of transpiration is achieved only when the observations of transpiration are included in the calibration rows tp foresta tp forestb tp all and all fourth the scores for net radiation are somewhat insensitive to the calibration scenarios with good temporal dynamics but high r m s e m o due to the recurrent underestimation by the model mentioned in 3 1 for most individual observables column 1 to 5 7 to 8 and 10 to 12 the all datasets calibration scenario last row yields the highest scores surpassed only by the scores of each simulated state calibrated against its direct observation diagonals of fig 8 the improved model data fit with more constraints becomes clearer when the average scores over observable types are considered column 1 6 9 and 13 finally using all datasets as a simultaneous constraint yields the lowest overall model data misfit across observables bottom right square in fig 8b as compared to the rest of the last column the impact of different calibration scenarios on individual and overall predictive uncertainty of simulations i e the average 90 spread interval among best runs pu normalized as defined in eq 1b is shown in fig 9 following the same layout of fig 8 when the model is calibrated against individual datasets pu values for corresponding observables remain below 0 5 diagonals in fig 9 and generally below 1 when simulating other variables off diagonal squares even if the latter is not included in the calibration e g uncertainty of θ peat when calibrating against streamflow only 1st row and 2nd column notable exceptions are streamflow where pu remains above 1 whenever discharge is not included in the calibration and simulated pine transpiration for which this feature is even more marked a smaller overall predictive uncertainty is found for the all dataset calibration scenario pu 0 65 as it is the only case where large individual reductions in simulation dispersal are simultaneously achieved for streamflow soil moisture net radiation and to a smaller extent pine transpiration fig 9 last row 3 3 parameter values fig 10 shows the selected parameter values across the best runs within the prescribed sampling interval displaying only 5 calibration scenarios grouping all datasets of a same type and the all datasets constraint case calibrated soil depths show consistent results with deep 4 6 m soils in the valley bottom and shallower podzol and rankers 2 m somewhat reflected in the depth of the two upper hydraulic layers top row porosity takes markedly different values across soil types from 0 85 in the peat 0 5 0 7 in the gley down to 0 35 0 5 on the hillslope this spatial variability is somewhat mirrored by an increasing saturated horizontal hydraulic conductivity k h x over several orders of magnitude from 10 5 m s 1 in the peat to nearly 0 01 m s 1 in the rankers other hydrological parameters such as air entry pressure ψ a e residual soil moisture θ r and anisotropy k r a t i o mostly displays similar values across soil types and calibration scenarios centred in the sampling intervals the same applies for vegetation parameters such as optimal photosynthesis temperature t o p t soil water potential control on stomatal closure ψ d and maximum stomatal conductance g s m a x the light extinction coefficient k b e e r of heather and peat moss displays distinctively higher well constrained values when pine transpiration is used as a constraint for pine trees the sensitivity of stomatal conductance to soil water content c is much lower in the all datasets calibration case while canopy interception capacity c w s m a x becomes much higher than in other cases 3 4 independent evaluation at catchment scale the empirically based estimate of catchment wide saturated area fraction area sat is compared to the simulations provided by the 30 best parameters sets in five calibration scenarios using each data type plus the full suite of measurements in fig 11 in all cases the model broadly reproduces the observed temporal dynamics reasonably well but apart from the peaks the simulated saturation extent is generally overestimated slight differences appear between scenarios with higher area sat values associated with the highest predictive uncertainty 24 when only soil moisture is used as a constraint vwsc all while the stream discharge constraints brings the narrowest range of simulated area sat between best runs pu 17 4 discussion 4 1 insights into ecohydrological processes this study shows the ability of a process based model to consistently simulate not only water storages and fluxes in the critical zone at local to catchment scales but also energy balance and ecohydrological couplings in a comprehensive model evaluation exercise these are very encouraging results for the prospect of explicitly incorporating vegetation dynamics into a mechanistic description of catchment water partitioning and towards improved prediction of the functional changes that catchments in northern latitude are likely to experience in the coming decades the most robustly simulated observable was stream discharge with 90 of all optimisation selected runs 30 for each of the 14 calibrations scenarios 12 of the latter excluding discharge measurements showing modified kge scores between 0 67 and 0 87 across the simulation period 11 2012 06 2016 excluding spin up not shown in particular the model was able to capture well extreme events such as the 200 year return period flood during the winter 2015 2016 note that the kinematic approximation for groundwater and stream routing in ech2o neglects diffusive water redistribution through pressure gradients therefore this model data consistency points at a reasonable adequacy of using a gravity driven conceptualization of the bruntland burn catchment at the spatio temporal scales considered this hypothesis is consistent with strong topographic gradients in the catchment and the wet low energy hydroclimate which both sustain a quasi permanently saturated valley bottom tetzlaff et al 2014 and generally high water tables even on the steeper hillslopes blumstock et al 2015 these settings result in a flashy response of the stream network to run off events soulsby et al 2015 generally driven by saturation overland flow from the peat and gleys but in larger storm events the podzolic soils also connect to the saturated areas tetzlaff et al 2014 the general hydrological behaviour which is broadly representative of other northern boreal catchments tetzlaff et al 2015b contrasts with semiarid regions which are characterized by a more transient hydraulic connection and disconnection within hillslopes and between hillslopes and the channel during dry periods although bedrock topography remains critical to understand shallow subsurface flows in water limited environments e g jobbágy et al 2011 maneta et al 2008 diffusive effects by local pressure gradients are also highly relevant reducing the spatial extent for which local measurements are representative and limiting the propagation of information from the location where fluxes are measured to the location where model parameters need to be identified maneta and wallender 2013 more detailed insights into storage dynamics were provided by including volumetric water content for the upper half meter of soil profile into the calibration at the four monitored locations across the main hydropedological units a good model data fit was achieved in most cases fig 5 ech2o overall managed to capture the very different dynamics between locations across almost 13 data years including the variability within the same podzolic soil unit i e with a common set of parameters at two contrasting sites fig 5c and d further the calibration yields a depth of about 2 m of a hydrologically active profile on the hillslope which is much deeper than typical soil depth estimates from geophysical methods soulsby et al 2016 but consistent with the hypothesis that groundwater recharge on the hillslope and downhill movement actively contributes to saturation overland flow through exfiltration in the valley bottom ala aho et al 2017a birkel et al 2011 we note nonetheless that the riparian areas are the most challenging locations when attempting to capture soil moisture dynamics in part this reflects the very small variability measured though the modelled θ remained too reactive as compared to the damped variability in the measurements fig 5a and b in addition the gley porosity remained underestimated leading to unrealistic saturated conditions outside the summer it might also explain why the simulated gley soil seemed slightly too deep while it should be shallower than in the peat areas as the model likely compensated in order to close the water balance in the valley bottom the other notable model observation mismatch was the underestimation of the seasonal amplitude of net radiation at the hilltop site a location mostly covered by heather shrubs fig 1 using independent estimates of transpiration and total evapotranspiration et on a heather plot near the podzol transect in the same catchment wang et al 2017b a preliminary analysis hinted at an underestimation of transpiration and overestimation of soil evaporation in the ech2o model not shown by contrast net radiation was well simulated in the riparian areas suggesting an accurate estimation of energy conversion from radiative to turbulent fluxes such as soil evaporation and sensible heat further modelling measurement comparison studies focusing on evaporative processes will help better constraining the energy balance on the catchment hillslopes gong et al 2016 larsen et al 2016 it is uncommon in catchment scale calibration studies to use direct measurements of plot scale tree transpiration du et al 2014 wei et al 2016 this is not only because this type of measurements is rarely available but also because most current hydrologic models cannot single out the transpiration fraction of evapotranspiration méndez barroso et al 2014 paniconi and putti 2015 the inclusion of an ecohydrological observable such as plot scale transpiration of scots pine introduced direct knowledge of the exchanges between the physical and ecological components of the catchment and reduced the number of possible internal model configurations that were consistent with observed soil moisture and streamflow the calibrated model reproduced the major features of the transpiration time series surprisingly well which increases our confidence that the internal water and energy exchanges at bruntland burn were adequately captured at forest site b the simulated transpiration remained very similar across calibration scenarios over the summer 2016 fig 6b while soil moisture was more sensitive to the data used to calibrate the model fig 5d this is consistent with results reported using a data oriented approach wang et al 2017a which showed weak controls of soil moisture on pine transpiration outside infrequent dry periods in this humid catchment 4 2 information content brought by the different observations the multiplicity of datasets of this study used in different combinations for model calibration and evaluation brings novel insights in how informative and representative these measured quantities are for improving our modelling approach this may help with the design of more efficient data collection campaigns in the following we first discuss the spatio temporal footprint related to how time and or location specific the measured signal is the issue subsequently discussed is the behavioural footprint i e how specific to some processes the retrieved information content is streamflow was simulated reasonably well in all reported cases fig 8 but with a substantially higher predictive uncertainty whenever discharge data was not included in the calibration fig 9 streamflow is well known to integrate information of many catchment scale processes beven and binley 1992 but this knowledge is too ambiguous to determine the exact catchment configuration that produces the observed signal this is because streamflows integrate downstream following a convergent network towards a unique outlet but the divergent nature of an upstream network makes it impossible to uniquely backtrack the locations where the flow was generated kirchner et al 2001 this has two consequences streamflow can be well simulated with numerous alternative model parameterizations physically consistent or not kirchner 2006 and the spatio temporal and behavioural footprints are large and therefore less informative of individual processes happening at specific locations in the catchment guse et al 2016 this was illustrated in the predictive uncertainties of the simulated catchment states using only discharge as the calibration constraint yielded the most variable results for simulated soil moisture in the gley and net radiation fig 9 some variables were only well simulated when the model was calibrated against observations of that type which indicates a more restricted behavioural and spatio temporal footprint of the information for instance soil moisture in the gley displayed a significantly higher up to ten fold model observation mismatch whenever θ gley was not part of the calibration constraints even if other θ dataset were included in the calibration fig 8b 3rd column therefore exhibiting a very localized spatial footprint moisture in podzols on the other hand displayed slightly more homogeneous performances across calibration scenarios fig 8b 4th and 5th columns indicating that having two different podzolic soil moisture locations in this study additionally increased the spatial footprint of the associated calibration constraints transpiration in scots pine stands tp was also characterized by poor model data fits unless the calibration scenarios involved a transpiration dataset fig 8 performances remained consistent when using data from forest site b to calibrate tp at forest site a and vice versa even when the two sites cover different growing seasons 2015 and 2016 it indicates that in this catchment the tp datasets has a narrow behavioural footprint but a more extensive spatial and temporal footprint conversely this footprint of tp datasets made them ill suited as sole constraints to calibrate the model across processes as seen for example from soil moisture goodness of fit fig 8 and predictive uncertainty fig 9 the above considerations highlight the benefits of combining streamflow observations with other types of information that have a more specific footprint such as measurements of volumetric water content and transpiration du et al 2014 compared to using streamflow alone adding the two latter types of variables improved model calibration by increasing model data fit scores fig 8 and by reducing the dispersion among best runs fig 9 these enhanced performances were moreover generalizable to using a diversified combination of observations in the calibration as seen from the improved overall model data fit and low predictive uncertainty across model outputs it supports a mitigation of the equifinality problem as the increased number of scale and or process specific diagnostics clark et al 2011 helped discarding unfeasible model configurations that may otherwise have given high performance scores however we have also observed that the predictive uncertainty of some outputs the case for peat and gley soil moisture can increase substantially when the model was calibrated with increasing amounts of information this may be an indication that with the catchment functioning hypothesis embedded in the model the datasets have overlapping footprints that inform the calibration process with conflicting or inconsistent information note that such conflicts only occurred for individual outputs as the overall uncertainty across all outputs was indeed lowest when using the full of datasets in the calibration fig 9 bottom right square the monte carlo approach adopted in this study uses the gof as an informal measure of the likelihood of each of these parameters after the best parameters are selected the likelihood measure is not further used and the spread across best runs shown in fig 10 cannot be interpreted as a probability distribution this avoids having to make assumptions about the structure of model residual errors which in more formal statistical frameworks determine how the parameter space is sampled if these assumptions are incorrect some good sections of the parameter space may end up being excluded from the search note that parameters with good performance have been found in the entire range of permitted parameter values also as a result in our methodology all the selected parameter values contribute the same to the predictive spread and their average does not necessarily represent better the hydrologic behaviour of a given catchment unit in fact from fig 10 no single combination of parameters can be picked to represent better the average behaviour of the predictive ensemble for this interpreting parameter means in terms of the hydrologic behaviour of the catchment behaviour can easily be misleading however the fact that the mean value of the parameters fig 10 is often not at the centre of the feasible search space or of the interquartile range indicates that some values in the allowed range are more preferred than others and that the information contained in the alternative calibration datasets informs these preferences differently understanding the mechanisms by which the parameters are nudged in a specific direction when calibrated with a specific dataset is desirable but also difficult and complicates any meaningful interpretation of the differences between mean parameter values 5 conclusions in the growing field of critical zone modelling a process based description of energy plant water relationships is a promising basis for a mechanistic understanding of vegetation influence on water pathways and stores and projecting their responses to environmental change more generally these types of interdisciplinary models are increasingly needed in critical zone studies where water is a fundamental medium for energy and material cycles in a wide range of processes at multiple time scales white et al 2015 likely to be altered over time goddéris and brantley 2013 although the problem of equifinality is exacerbated with the increasing complexity of models using multiple measurements informative of the range of processes implemented in the models can assist in constraining models to a limited subset of feasible configurations when this is achieved newer more integrated models offer an opportunity for deeper process insight we demonstrated this by applying the fully distributed model ech2o to a small northern headwater catchment using different combinations of 10 datasets relative to 4 types of ecohydrological processes discharge soil moisture pine stand transpiration and above canopy net radiation while ech2o was able to perform well for single objectives when calibrated against individual datasets constraining its overall behaviour with multiple datasets in a rigorous multi objective calibration experiment yielded an improved cross output average model performance and smaller overall predictive uncertainty the resulting model configuration also reproduced the main features of the temporal dynamics of an independent estimate of catchment scale saturated area fraction successful comparison against this independent dataset indicated that the internal transient storage dynamics were generally captured correctly by the model we also discussed the informational footprint resulting from each dataset across scales spatio temporal footprint and processes behavioural footprint this modelling experiment increases our confidence that a data intensive calibration approach constrains the set behavioural model configurations in an effective way i e that our approach allows for broadly getting the right answers for the right reasons kirchner 2006 among other approaches ongoing model extensions to include tracking of stable water isotopes 2h and 18o fluxes and water age across ecohydrological compartments will provide means to test this further the experiments to date provide a foundation for using ech2o to project the impact of climate variability on catchment functioning in sensitive high latitude systems acknowledging that we have applied ech2o in a location where snowfall is quite modest a critical next step will be to conduct simulations in snowier catchments more generally we intend to assess the reciprocal links between ecosystem functioning land cover change and the mediating role of vegetation in buffering atmospheric impacts on water fluxes and storage software availability the source code of the ech2o model c programming language is available at https bitbucket org maneta ech2o while the associated documentation compiled binaries and case study files can be found at http hs umt edu regionalhydrologylab software default php the python routine and dataset used for calibration are available upon request to the authors acknowledgements this work was funded by the european research council project ga 335910 vewa m maneta acknowledges support from the u s national science foundation project gss 1461576 and u s national science foundation epscor cooperative agreement eps 1101342 all model runs were performed using the high performance computing hpc cluster of the university of aberdeen and the it service is thanked for its help in installing pcraster and other libraries necessary to run ech2o and post processing python routines on the hpc cluster finally the authors are grateful to the many people who have been involved in establishing and continuing data collection at the bruntland burn particularly christian birkel maria blumstock jon dick josie geris konrad piegat claire tunaley and hailong wang appendixrecent developments in ech2o model in the following equations the parameters calibrated in this study table 1 are highlighted in bold font a1 canopy processes canopy level processes link the radiation budget solar radiation incoming longwave and outgoing longwave radiation to conductive energy transfers sensible heat evaporative losses from the canopy evaporation of intercepted water and plant transpiration soil water availability and soil water potential energy the core of the canopy processes is a set of 3 equations energy balance soil water balance and soil water potential energy and 3 unknowns canopy temperature tc plant available soil moisture st 1 and soil matric potential ψsoil solved for each vegetation type present in the grid cell a 1 s t 1 s η θ r z r 95 δ t l e t t c ψ s o i l ρ w λ v 0 ψ ae s t 1 λ bc ψ s o i l 0 n r t c h t c l e t t c ψ s o i l l e 0 the topmost equation in a 1 is the water balance in the root zone after infiltration has been accounted for in the initial soil moisture where s t is the weighted average degree of soil saturation in the soil over z r95 at the beginning of the time step s t 1 is the weighted average degree of soil saturation over z r95 at the end of the time step in both cases the weights for the saturation averages are given by the fraction of roots in each layer of the soil additionally η is soil porosity θ r is residual moisture content z r95 is the total soil depth of the layers containing 95 of roots δ t is the size of the time step ρ w is density of liquid water λ v is latent heat of vaporization and l e t t c ψ s o i l is a function calculating latent heat consumed for transpiration which is dependent on the temperature of the canopy and the soil water potential the second equation in a 1 is the brooks and corey prognostic equation where ψ a e is the soil air entry pressure ψ s o i l is soil matric potential and λ bc is the pore size index brooks and corey exponent parameter the lower equation in a 1 is the energy balance in the canopy as described in maneta and silverman 2013 but reproduced here for completeness this equation assumes that the available radiative energy nr will be consumed as sensible heat h as latent heat from transpiration let or as latent heat from evaporation of intercepted water le a 2 n r t c r s w 1 α c 1 exp k beer l a i ε c r l w ε c σ t c 4 a 3 h t c ρ a c a t a t c r a a 4 l e t t c ψ s o i l ρ a c a e a e s h r γ r a r s ψ s o i l a 5 l e ρ a c a e a e c h r 1 h r c w s cm s m a x γ r a where r s w is incoming solar radiation α c is the effective canopy albedo k b e e r is a light extinction coefficient as per beer s law l a i is leaf area index ε c is the canopy emissivity r l w is downwelling long wave radiation σ is the stefan boltzmann constant t c is the effective canopy temperature ρ a is density of air at air temperature t a c a is the heat capacity of air e a is air vapour pressure at t a e c is the canopy saturation vapour pressure at t c h r is air relative humidity γ is the psychrometric constant r a is aerodynamic resistance r s is stomatal resistance cws is the current amount of canopy water storage and cws max is the maximum canopy storage allowed by unit lai see maneta and silverman 2013 for additional details on the calculation of these quantities the current calculation of stomatal resistance differs from the original formulation in that the efficiency factor that provided the stomatal dependency on soil moisture has been changed to a dependency on soil water potential the new stomatal resistance formulation and the soil water potential efficiency factor are a 6 r s g s max l a i f r s w f t a f v p d f ψ s o i l 1 f ψ s o i l 1 1 ψ s o i l ψ d c in which ψ d is the soil water potential at which 0 5 efficiency for soil water potential is achieved and c is a function shape parameter other efficiency factors are calculated as in maneta and silverman 2013 a2 soil hydrology the soil hydrology component has been improved over the original formulation in maneta and silverman 2013 by including a vertical soil water redistribution model with three hydraulic layers the topmost layer receives infiltration from the surface its soil moisture content controls infiltration rates in the green and ampt infiltration equation and is also the only layer from which evaporation occurs the middle layer typically contains most of the root system and therefore its moisture controls the hydrologic limitation to transpiration the deepest layer transfers gravitational water laterally using the original kinematic wave formulation the condition at the bottom of the third layer can now be impervious no flow or can leak at a rate given by a bedrock leakance parameter as in the original formulation only water in excess of field capacity can move by gravity when soil water exceeds field capacity in the topmost or second layer the water excess gravitational water moves downward to the next layer at a rate determined by the linearized unsaturated hydraulic conductivity function k θ k hx k hratio l b θ θ r θ f c θ r where k h x is the saturated horizontal hydraulic conductivity k h r a t i o is the anisotropy ratio while l b is a bedrock leakance parameter set to 1 for layers 1 and 2 and that can vary between 0 no flow and 1 free gravitational drainage for soil hydraulic layer 3 water leaking through the bedrock leaves the domain horizontal water transfers to the downstream cell only occur in the third soil hydraulic layer following the linearized kinematic wave formulation described in maneta and silverman 2013 when the storage capacity of the bottom layer is exceeded saturation excess water is transferred to the middle layer increasing the middle layer moisture content if the middle layer saturates saturation excess is transferred to the topmost layer increasing the topmost layer moisture content if the topmost layer saturates saturation excess produces return flow to the surface seepage face return flow is added to the pool of ponded water that generates overland flow the following time step a3 channel routing channel flow is simulated using a 1d solution of the kinematic wave equation the equation is solved for stream discharge using a power function a α q β to relate cross section flow area to stream discharge a 7 q x α β q β 1 q t q b f q o v l 0 where q l3t 1 is stream discharge q b f l2t 1 are groundwater contributions to streamflow per unit length of channel q o v f l2t 1 are overland flow contributions to streamflow per unit length of channel and x l and t t are distance in the flow direction and time respectively using manning s equation to approximate flow velocity and assuming rectangular channel cross sections parameters α and β can be determined to be n p 2 3 s 3 5 and β 3 5 in which n tl 1 3 is manning s roughness coefficient p l is the channel wetted perimeter approximated by channel width and s is the streambed slope equation a 7 is solved using a first order implicit finite difference scheme and is unconditionally stable a4 exponential root profile the new model formulation requires the fraction of roots in each soil layer f root l 1 2 3 to be specified in this paper we have assumed an exponential root profile modulated by a single parameter kroot a 8 f r o o t l 1 1 exp k root d l1 1 exp k root d soil f r o o t l 2 exp k root d l1 exp k root d l1 d l2 1 exp k root d soil f r o o t l 3 1 f r o o t l 1 f r o o t l 2 where d s o i l is the total depth of the soil hydrologically active layer and d l 1 2 are the depth of layers 1 or 2 appendix a supplementary data the following is the supplementary data related to this article supplementarymaterials revised docx supplementarymaterials revised docx appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 01 001 
