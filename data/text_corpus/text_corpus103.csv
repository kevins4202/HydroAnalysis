index,text
515,surface runoff has been recognized as an important component in agricultural water management extensive studies have been developed to measure surface runoff and numerical methods have been applied to simulate surface water dynamics for simulations of surface runoff in agricultural systems three processes should be considered 1 the movement of water along the soil surface i e surface runoff 2 the accumulation of water in depressions and 3 the water fluxes across the soil atmosphere interface i e infiltration evaporation and exfiltration through seepage faces the objective of this study is to develop a physical based surface runoff model that includes all the three processes numerical implementation of the new runoff model was developed and incorporated within 2dsoil a simulation package for soil water energy and solute movement the new model describes the subsurface flow near the soil surface following a unified boundary condition expressed with the heaviside step function this expression enables continuous and automatic changes of boundary conditions between infiltration and runoff the surface water flow is simulated using saint venant equations the ponded water height on the soil surface and the infiltration rate are adjusted based on the runoff flux and topography numerical tests based on an experimental dataset are used to evaluate the accuracy of this model and numerical examples of surface water flow along a variety of topography are used to demonstrate model performance the simulations match the experimental results and the surface water mass balance errors of the numerical examples are less than 1 a practical example of using the surface runoff model to estimate the runoff efficiency in a ridge furrow water harvesting is carried out in conclusion the newly developed surface runoff model can successfully simulate surface water dynamics this model can further support the design and evaluation of agricultural water management strategies and field water budgets keywords finite element runoff infiltration surface water ponding water harvesting heaviside step function saint venant equation 1 introduction water runoff along soil surfaces is a dynamic process in response to precipitation and infiltration runoff occurs when the water input fluxes e g rainfall irrigation or runoff from upstream directions exceed the infiltration capacity of the soil surface runoff is an important component in the field water budget and it is involved in a variety of field management strategies for example nutrients and microorganisms in surface applied manure fertilizer can be transported in surface runoff during rainfall events causing contamination of nearby water bodies kouznetsov et al 2007 runoff which is affected by climate topography and plant cover is recognized as a critical driving force for soil erosion mohamadi and kavian 2015 şensoy and kara 2014 to mitigate the negative effects of runoff surface mulching contour farming and vegetated filter strips are often used to reduce runoff and retain surface water actively increasing runoff at specific locations and capturing the released water at other locations becomes a water management strategy especially in arid or semi arid areas tadmor et al 1961 for example the soil surface can be shaped with repeated ridges and furrows and the ridges mulched to reduce infiltration during rainfall runoff from the ridges is promoted and the surface water flows into the furrow areas where water is captured for crop uptake wang et al 2015 this management practice is referred to as ridge furrow water harvesting which collects the surface runoff to enhance the water use efficiency if crops are planted in the furrows wang et al 2015 zhao et al 2018 a physical description of surface runoff should include three critical processes 1 the water flow along the soil surface 2 the accumulation of water within depressions and 3 the water fluxes across the soil surface i e evaporation infiltration or exfiltration kolditz et al 2008 implementation of such a theoretical framework can be divided into two major categories for the first category in order to describe water flow along the soil surface the kinematic wave or diffusive wave approximation of the surface water flow can be directly combined with soil water movement to simulate the surface runoff this has been implemented in some shallow surface water models including parflow kollet and maxwell 2006 hydrogeosphere brunner and simmons 2011 and surface water chemical movement models based on hydrus liang et al 2017 such designs provide an efficient way to describe surface water movement which can also be generalized for high performance computing in a relatively simple manner fatichi et al 2016 however the assumption of shallow surface water flow in those models was only valid for locally smoothed surfaces and surface boundary conditions and subsurface water flows were usually simplified for example subsurface water flow was restricted as vertical infiltration however the infiltrated water can move laterally as gravity dominated saturated flow and emerge as exfiltration on other portions of soil surface few models fully characterize this process therefore existing models in this category are commonly used in an averaged sense for evaluations of large scale surface water flux such as in a whole catchment area or supporting secondary calculations such as surface chemical movement along with runoff water kolditz et al 2008 liang et al 2017 kollet and maxwell 2006 the second category of surface runoff models pursues a more detailed description of the physical processes for example singh and bhallamudi 1998 applied saint venant equations to explicitly calculate the conservation of mass and conservation of momentum of runoff water in a direction parallel to the soil surface but the study was only validated against experiments performed on a smooth sloping surface smith and woolhiser 1971 appels et al 2011 applied a philip s infiltration model and soil water movement model to study the formation merging and separation of surface ponded water within depressions this study included a relatively complex soil surface topography however the runoff was not routed along the soil surface and subsurface water flow other than infiltration was not considered on a soil surface with hills and depressions physical processes such as evaporation runoff infiltration and exfiltration via surface seepage should be allowed to occur simultaneously and switch automatically due to the changes in weather soil water regimes and surface water distribution e g transient ponded water a flexible and continuous boundary condition on the soil surface can enable the switching among those processes existing methods that simulate runoff with a finite element model switch the soil surface boundary conditions during infiltration between constant flux and constant head depending on the values of surface matrix potential this switching can be presented using some threshold functions such as the heaviside step function camporese et al 2014 bartlett et al 2015 however such switching is usually defined separately from the solver of the surface subsurface water flow thus in numerical simulations with given time increments this switching can only be implemented in discretized time points i e after each time increment of the surface subsurface water flow simulation therefore this type of switching for the boundary conditions was referred to as a discontinuous switching in the time domain in order to embed this switching in a continuous fashion kouznetsov et al 2007 went a step further and fully incorporated a heaviside step function into the governing equations of surface subsurface water movement moreover a comprehensive runoff model must incorporate subsurface hydraulic connectivity when surface runoff occurs in multiple depressions separated by hills on the soil surface this enables the calculation of lateral subsurface water flow and the quantification of exfiltration from the saturated soil surface that can drain into the downstream depressions thus there is a need to develop a comprehensive surface runoff model as these factors have not generally been implemented in existing models and numerical simulations to this time the objective of this study is to develop a physical based surface runoff model based on saint venant equation system which also incorporates soil water movement and boundary conditions for surface water ponding infiltration and exfiltration a numerical implementation of the new surface runoff model is also developed and imbedded into 2dsoil a comprehensive simulation package for soil water heat and chemical redistributions timlin et al 1996 the performance of the proposed runoff model was evaluated with experimental data and numerical examples while a potential application for water harvesting is also proposed 2 theory and model setup 2 1 the governing equations the saint venant equation system is utilized for the simulations of surface water movement it consists of a linear hyperbolic equation for mass conservation and a burgers equation for momentum conservation the momentum equation is an integrated average of the navier stokes equations with respect to flow depth the saint venant equation system can be written as follows 1 h t q l p i q t l q 2 h g h 2 2 g h s 0 s f in these equations h cm is the flow depth which coincides with the ponded water height at each surface computing node here h is constrained as h h 0 0 where h 0 is a small number 1 this prevents the case where q 2 h as h 0 the value q cm2 day 1 is the surface runoff and l represents the partial derivative along the surface slope l s 0 and s f n q 2 h 10 3 are surface slope and friction slope with manning roughness coefficient n cm 1 3 day here h is constrained as h h 0 0 where h 0 is a small number 1 this prevents the case where q 2 h as h 0 the value q cm2 day 1 is the surface runoff and l represents the gravitational acceleration and p cm day 1 is the flux density of precipitation and or irrigation while i cm day 1 represents the flux density of infiltration a common setting for the initial conditions for eq 1 is h l 0 h 0 and q l 0 0 indicating the small surface water depth and zero surface runoff along the slope at t 0 the richards equation derived from darcy buckingham s law and the equation of continuity is a widely used governing model for incompressible fluid movement in soil jury and horton 2004 with a continuous air phase the soil is assumed to be nondeformable and locally isotropic and the vapor transport is negligible because the velocity of soil water fluxes is relatively small the conservation of momentum is generally not involved in a soil water movement model the richards equation in a 2d soil matrix can be approximated as eq 2 2 θ t x k h h x z k h h z 1 s 0 where θ cm3 cm 3 represents the volumetric water content h cm is the water head t day is time x cm and z cm are horizontal and vertical distance s day 1 represents water sources or sinks the water retention curve between θ and h and the hydraulic conductivity model for k h cm day 1 are usually assumed to follow the van genuchten equation van genuchten 1980 vogel and cislerova 1988 šimůnek and suarez 1992 although other representations can be used timlin et al 1996 the soil surface boundary condition for the richards equation i e eq 2 can be written as 3 η h h t p i 0 where p cm day 1 is the precipitation and or irrigation and i cm day 1 is the surface infiltration kouznetsov 1989 surface infiltration is calculated based on the richards equation surface water flux and or the ponded depth of surface water h depending on the concrete boundary conditions timlin et al 1996 η h is the heaviside step function defined as follows 4 η h 1 h 0 0 h 0 the advantage of using this boundary condition is that it can be automatically invoked when h 0 and disabled when h 0 under the net water input flux p thus the model is able to adjust surface subsurface water exchanges between infiltration and evaporation automatically based on weather and soil water conditions in order to apply numerical solvers to eqs 2 and 3 analytic approximations to the heaviside function eq 4 and its derivative the dirac delta function δ h must be applied ηω h and δω h defined in eq 5 are those analytic approximations 5 η ω h 1 π arctan h ω 1 2 η h δ ω h 1 π ω ω 2 h 2 δ h the functions ηω h and δω h respectively converge to the heaviside step function eq 4 and the dirac delta function point wisely as ω 0 we require h h 0 0 in order to use eq 3 where h 0 is a small positive head 0 01 above which surface runoff occurs singh and bhallamudi 1998 for each computing node with associated surface water infiltrability the time it takes for the soil water potential to reach h 0 can be used to determine the ponding time kouznetsov et al 2007 which indicates the time period from the beginning of precipitation to the initiation of runoff refer to the appendix for additional discussion on the usage of small values h 0 and h 0 for the saint venant equation and the heaviside boundary condition an alternate method to calculate overland flow is to express the subsurface and overland flow in single equation system using a pre defined partition function with convex regulators those regulators define the distribution coefficients that quantify how much water belongs to overland flow and how much water stays within the soil matrix marçais et al 2017 however in this study the soil water movement and the surface runoff were explicitly solved via two differential equation systems this enables the use of a single boundary condition eq 3 that links soil water flow and surface runoff 2 2 numerical implementation numerical implementation of the proposed surface runoff model was developed independently and incorporated with 2dsoil 2dsoil is a modular based soil simulator including soil water solute and energy movement with interfaces for climate and plant growth timlin et al 1996 2dsoil has been successfully applied to simulate water energy solute transport and crop growth kemp et al 1997 timlin et al 2002 yang et al 2009 kim et al 2012 timlin et al 2019 the advantage of using 2dsoil is that it already supports simulation of a wide variety of environmental processes and is designed to be easily interfaced with additional modules such as climate models and crop models this model uses a 2d finite element representation of the soil domain which was developed from swms2d šimůnek et al 1994 2dsoil also simulates a wide range of meteorological processes timlin et al 2002 which includes the penman equation penman 1963 for potential evaporation from the soil surface in addition the code for the proposed runoff model i e the saint venant equations and the heaviside boundary condition can be placed following a uniform data input format and a data flowchart pre specified in 2dsoil thus it helps standardize the development of future soil models and simplifies the maintenance of the new surface runoff model numerical methods to solve the saint venant equation system have been extensively studied greco and panattoni 1975 mangeney castelnau et al 2003 especially via a mixed hybrid finite element scheme chavent and roberts 1991 fig 1 illustrates an example of the computing domain the variables x k k 0 n are n 1 computing nodes represented with red circles the subscript k is the index of the computing node the flow depths or ponded water heights h k t k 0 n are evaluated at each xk the surface fluxes marked with solid arrows occur between two adjacent computing nodes thus n flux terms q k t k 1 n are involved in the numerical scheme the superscript t indicates time step numerical computation of heads and fluxes at the left and right boundaries of the soil surface fall into two cases 1 if q points to the interior of the computing domain zero runoff discharge from soil surface is assumed 2 if q points to the exterior of the computing domain the water can freely flow away from the soil surface in fig 1 the runoff discharge will occur at the right boundary of the surface the first equation in eq 1 can be discretized into a conservative form 6 h k t 1 h k t δ t q k 1 t q k t l k p k i k while the discretization of second equation in eq 1 follows the upwind direction i e 7 q k 1 t 1 q k 1 t δ t 2 l k 1 l k q k 1 t 2 h k t g h k t 2 2 q k t 2 h k 1 t g h k 1 t 2 2 g h k t s 0 s f due to the non linearity of this momentum equation picard iteration is applied to achieve the numerical solution please refer to the appendix for the validity of applying picard iteration when the soil surface has depressions the surface water will flow from points of high elevation and accumulate in the valleys possibly forming small transient water pools however direct application of the saint venant equation to characterize the water movement to and from these transient pools is not always the best approach as the water depth increases the soil surface friction between the water and surface soil decreases with respect to the ponded height therefore the water movement near the surface of the ponded water can be approximated as free water flow with relatively large momentum while near the bottom of the transient water pools horizontal momentum of water may vanish and ponded infiltration can be assumed the saint venant equation only provides a vertically averaged result for the momentum thus after solving the saint venant equation system additional procedures are taken for the calculation of water movement within the pools one simple approach is to provide an additional averaging process to adjust the water flow in the ponded area where ponded water of adjacent computing nodes can be exchanged based on the soil surface elevation and the ponded water heights for example given two adjacent computing nodes a and b if the water elevation at computing node a is higher than at node b and the ponded heights at both nodes are relatively large such that ponded water forms a smooth surface and the surface water can be considered as friction free then node a will provide water for node b until both nodes reach the same water elevation however if computing node a has a relatively large water surface elevation but with a relatively small ponded height such that surface water adheres to the soil particles then node a is not able to supply any water for node b and there will be no water exchange between node a and node b these processes will be iterated until the free water surface within each individual pool reaches a stable level the existence of small pools may alter the water infiltration patterns changing the soil surface from an evaporation condition or an unsaturated infiltration condition to a ponded infiltration condition after the rainfall the infiltration at the hilled locations will immediately terminate while the ponded water can maintain saturated infiltration within the depressions for a certain time period in the numerical implementation the ponded depth within the water pools is tracked by the surface runoff model and values of water head are exchanged with the 2dsoil water movement module such that the infiltration process can be adjusted automatically as ponded water is available for the computing nodes within depressions we assume infiltration is the prior first outlet for the ponded water and the ponded height will be decreased or maintained by the incoming surface fluxes from nearby computing nodes by doing that the model simulates infiltration fluxes and surface runoff after rainfall events the infiltrated water may flow laterally within the soil and emerge as exfiltration on the soil surface for example this will occur at locations where the elevation of the soil surface is lower than a nearby transient water pool the infiltration at the bottom of a water pool can elevate the underground water table that lies below the ponded zone thus at those locations when the underground water table reaches the soil surface seepage faces can be formed and exfiltration will be initiated the exfiltration flux that occurs on the soil surface can become surface runoff in the downstream direction thus the calculation of ponded water lateral soil water movement and surface runoff should be combined and the hydraulic connections accounted for in the numerical implementation the 2dsoil water movement module uses a seepage condition to trace the locations of exfiltration fluxes the exfiltration rate calculated by 2dsoil is used by the surface runoff model to calculate surface water flux via eq 1 fig 2 presents a flow diagram for the surface runoff model and the data flow paths the surface runoff module is placed between the weather module and the water movement module because the surface water can be considered as an interface between atmospheric water and soil water the solid arrows indicate the original data paths while the dashed arrows indicate the alternative data paths after the surface runoff module is invoked thus precipitation from the weather module and the soil water infiltration from the water movement module are the input data the three critical factors surface runoff surface water accumulation and water fluxes across the soil surface are all included in this new surface runoff module 3 illustrative examples and applications in this section we first verify the accuracy of the surface runoff model based on an experimental dataset then we explore the effectiveness of the surface runoff model by two numerical examples with a variety of soil surface topography see figs 4 and 5 the goals of applying numerical examples are 1 to test the model performance on complicated soil surface and 2 illustrate how the proposed model handles more than one type of water flux across irregular soil surfaces the variations of surface water fluxes with respect to time including precipitation infiltration and runoff as well as the surface water storage are provided in fig 6 finally we illustrate a potential application of the proposed surface runoff model for water harvesting see figs 7 and 8 all the numerical examples and code presented in this study are available online https github com ars csgcl dt 2dsoilrunoff 3 1 comparison between simulation and experimental data smith and woolhiser 1971 reported a laboratory experiment of coupled surface subsurface water flow in a soil with a linear sloping surface their surface flow data was one of the datasets commonly used for model validation and comparison in the smith and woolhiser 1971 experiment the soil sample was packed into a 12 2 m length 0 051 m width and 1 22 m depth tank the surface slope was 1 a light oil was used as the test fluid with viscosity similar to water the rainfall intensity was 250 mm h 1 over a duration of 15 min the runoff discharge from the sloping surface was collected at the lower edge the packed soil sample contained three layers with thicknesses of 76 5 229 5 and 761 mm the water retention curves and the unsaturated hydraulic conductivity for the three layers are available in singh and bhallamudi 1998 in the numerical simulation an impermeable boundary condition for liquid fluxes was used for the vertical edges and the bottom on the soil surface a flux boundary condition was assumed initially and the surface runoff model would adjust the surface boundary condition based on the soil liquid content and ponded liquid height because the background soil liquid content was not provided in the experiment we assume the initial potential to be 25 cm fig 3 presents the comparison among the measured runoff discharge and the simulated results the simulations by smith and woolhiser 1971 and the proposed runoff model via 2dsoil match the experimental results providing evidence for the accuracy of the newly developed surface runoff model during the initial segment of surface runoff between 8 min and 10 min the simulated results from singh and bhallamudi 1998 are less than the experiment results and the new model simulations indicating some liquid is blocked on the soil surface although both singh and bhallamudi 1998 and the proposed surface runoff model utilized the saint venant equations the surface runoff model in this study adopted an upwind mixed hybrid finite element scheme and had additional adjustments for ponded heights and infiltration those could be the reasons leading to the difference between those two simulation results 3 2 numerical examples following the experimental test we give two numerical examples to demonstrate the ability of this new model to simulate surface water flow with relatively complicated topography the first example is illustrated in fig 4 a which presents a soil profile with one single depression the horizontal distance indicates the width of the soil profile while the vertical distance indicates its height the solid curve indicates the soil surface with red circles marking the computing nodes in the simulations the soil was assumed to be homogeneous and nondeformable the hydraulic properties followed the van genuchten model with the parameters listed in table 1 soil a rainfall was applied for 24 h at the rate of 70 mm h 1 impermeable conditions were applied for the vertical boundaries a seepage condition was adopted for the bottom and a flux condition was initially applied on the soil surface the initial water potential was 100 cm through the soil profile the ponded water heights gray area within the depression on soil surface increased with respect to time during the rainfall while the wetting front propagated downwards fig 4 b and c present the ponded heights at 2 and 4 h after the simulation and rainfall initiated the wetting front was nearly flat in this example because the rainfall was homogeneous on the soil surface the depression was full at 4 h after the rainfall started and then extra surface water flowed to the left edge and discharged from the surface after the rainfall terminated the simulation continued for another 24 h to demonstrate the decreasing ponded heights within the depression fig 4 d e and f present the ponded heights 24 28 and 34 h after the simulation launched i e 0 4 and 10 h after the rainfall stopped soil water content was maintained due to the infiltration of ponded water while soil evaporation occurred at the soil surface beyond the free water surface the evaporation on the free water surface follows the potential evaporation fig 6 a summarizes the fluxes and water storage during this 48 h simulation the large initial infiltration flux was due to the relatively dry soil profile the infiltration rate decreased and reached steady state 7 h after the rainfall initiated the runoff discharge started 4 h after the rainfall started from the left edge of the soil surface which coincided with the time when the ponded water height reached the left edge after the rainfall stopped the runoff discharge stopped immediately because the ponded height became lower than the left edge then the simulation automatically switched to falling head ponded infiltration within the depression for another 5 h until all the water entered the soil profile this example illustrates the ability of the proposed surface runoff model to simulate surface water flux as well as water accumulation and produce plausible results with surface water mass balance error less than 1 along a slope the soil surface may have multiple depressions and those depressions can be separated by hilled soil water infiltrated in higher depressions can flow laterally within the soil profile and emerge from the soil surface as exfiltration through a seepage face in this case a surface subsurface surface hydraulic connection is established a physical based surface runoff model together with soil water movement model should be able to simulate water fluxes in this scenario fig 5 a shows a soil surface with a hill and a depression the soil in this example has the same hydraulic properties as the previous numerical example fig 4 the rainfall rate was 70 mm h 1 only over the depression area for 24 h there was no rainfall to the left of the hilled soil the runoff discharge was collected from both the left and right edges runoff discharge from the left edge is accompanied by lateral subsurface water flow under the hilled soil and exfiltration on the left slope of the soil hill in this example the vertical and bottom boundaries of the soil profile were assumed to be impermeable to water fluxes fig 5 b and c present the ponded water heights and soil water distribution 3 h and 6 h after the rainfall started since the rainfall was only applied on the right side of the soil surface over the depression the wetting front under the depression propagated deeper than the wetting front in the left part of the soil profile thus in contrast to the soil water regimes presented in fig 4 b and c the infiltration front is not uniform as it diffuses downward fig 5 d e and f present the ponded water height and soil water regimes 24 30 and 36 h after the simulation launched or 0 6 and 12 h after the rainfall stopped like fig 4 the water surface within the depression remained flat during the increasing and decreasing periods although the variations of soil water content were not obvious in fig 5 d e and f soil on the top of the hill did not reach saturation during the simulation fig 6 b summarizes the fluxes and water storage during the simulation the net infiltration on the soil surface the net infiltration is equal to infiltration minus exfiltration started with a relatively high value and dropped to zero after the soil approached to saturation because there was no water leakage from the vertical and bottom soil boundaries a relatively complicated decreasing increasing decreasing pattern of infiltration was shown during the first 10 h of the experiment the slight increasing of infiltration was due to the pressure provided by ponded water while the time that infiltration reached a local maximum and started to slowly decrease coincided with the time when exfiltration and left side runoff occurred see fig 6 c when water started to seep from the left part of the soil surface the net infiltration decreased the runoff discharge increased with respect to time and approached the rainfall fluxes at steady state theoretically all the rainfall became runoff discharge the runoff discharge was collected from both edges of the soil surface where exfiltration occurred from x 0 to x 18 along the soil surface and provided runoff discharge on the left edge fig 6 c presents the amount of runoff discharge from the left edge and the right edge during the rainfall the runoff collected at the right edge was twice as large as the runoff at the left edge because the runoff discharge at right edge was directly from the free water surface while the runoff discharge at left edge had to flow across the hilled soil based on the simulation the difference between the runoff discharge and the rainfall i e the mass balance error during the steady stage was less than 1 after the rainfall stopped the runoff discharge at the right edge immediately dropped to zero like the runoff results in the previous example fig 6 a however the runoff flux at the left edge decreased gradually and reached zero after another 20 h one reason is that the ponded water in the depression kept supplying runoff for 10 h when the ponded water fully infiltrated 35 h after the simulation launched the red curve reached 0 in fig 6 b the soil water stored beyond the left edge began to drain due to the assumptions of impermeable boundary conditions this drainage supplied the left edge water discharge for another 10 h further evidence of soil water drainage is shown in the net infiltration curve in fig 6 b a negative net infiltration indicated that drainage from the soil surface between the 35th h and the 45th h this numerical example demonstrates the importance of subsurface water movement in the simulations of surface water movement since the subsurface water movement through the hilled soil serves as a bridge connecting the surface runoff on its left and right sides and this example illustrates that the proposed surface runoff model can include the interactions between surface water flow and subsurface water flow although the soil surfaces in the two numerical examples are chosen arbitrarily they contain some geometrical configurations commonly shared on agriculturally managed soil surfaces by re scaling and connecting the example soil surfaces side by side it is possible to approximate a natural soil surface thus the numerical examples discussed are representative and meaningful please refer to the appendix for performance information 3 3 application example of the surface runoff model a potential application of the surface runoff model in field water management is to simulate the surface water redistribution in a rainfall harvesting management technique that utilizes ridges and furrows this is a method to increase soil water storage water use efficiency and crop yield in arid or semi arid areas a diagram of a ridge furrow construction for water harvesting from a study by wang et al 2015 is shown in fig 7 here only the ridge part is mulched during a large rainfall surface runoff will occur on the ridge and the runoff will flow into the furrow along the ridge surface thus the water input within the furrow can be increased promoting deep water percolation into the soil between rainfall events soil water within the furrow will be partially moved under the ridges and stored for root water uptake due to the soil thermal gradient and mulching zhao et al 2018 for management evaluation two water related quantities can be determined i e the runoff efficiency and the soil water distribution zhao et al 2018 reported numerical and experimental results for soil water storage and soil water distribution surface runoff efficiency which is directly related to the surface water model was studied by wang et al 2015 the runoff efficiency is defined as the ratio between the runoff collected at point a and b fig 7 and the total rainfall received on the ridge the runoff efficiency is a normalized parameter that is strongly related to the soil surface configuration and the mulching materials for example with an ideal impermeable mulch the runoff efficiency should be 1 meaning all the rainfall received on the ridges will flow into the furrows wang et al 2015 reported the ridge furrow designs for optimum runoff efficiency over a range of surface mulching materials in the loess plateau china we chose one of their designs for demonstration proposes fig 7 the horizontal scale of the furrows and ridges was 60 cm and the ridge was 15 cm higher than the soil surface the ridge shape followed a cosine curve weather data from iowa between mar 23th and oct 27th 2006 were used in the simulation and the soil hydraulic properties were those of soil a in table 1 fine textured soil with properties shown in table 1 soil b was used to approximate the soil mulching in wang et al 2015 plastic film mulching was used to approximate the bio degradable or the plastic mulching mentioned in wang et al 2015 fig 8 a presents the simulated runoff efficiency with fine textured soil mulching the data points follow two patterns one is along the horizontal axis and the another one is shown as the upper edge of the shaded angular area with the regression line y 0 27x 0 55 the averaged runoff efficiency is represented with the red regression line y 0 18x 0 88 the slope of the regression line can be considered as the runoff efficiency the upper edge of the shaded area in the figure has a similar slope as the results reported by wang et al 2015 because the mulching is made with soil part of the rainfall received on the ridge becomes infiltration and a small portion of the surface water adheres to the soil or evaporates along the ridge surface thus the runoff efficiency of soil mulching is relatively low the simulated pattern along the horizontal axis was not presented in the experiment in wang et al 2015 however it can represent the scenario when the soil is relatively dry or rainfall rate small in which case most of the rainfall infiltrates into the mulched soil and no surface runoff is produced thus both patterns shown in the simulated results are plausible only a few data points fall between the two patterns because 1 the fine texture soil mulch is relatively thin such that its water content and hydraulic conductivity reach steady state rapidly and 2 the fine texture soil mulch can limit infiltration depending on infiltration rate thus the surface of the ridge parts can shift from infiltration condition to steady runoff condition in a short time period this limits the number of points between high runoff and no runoff fig 8 b presents the simulated runoff efficiency with the plastic mulching only one pattern is shown represented with red regression line y 0 93x 0 11 because the plastic film is assumed to be impermeable to water most of rainfall is changed into surface runoff and supplies the soil water storage in the furrow but a small portion of water may adhere to the plastic surface or evaporate before reaching point a or b a few data points are beyond the 1 1 line because the model counted the rainfall or runoff quantity every hour therefore when a relatively large rainfall stops the surface runoff may be kept for another few minutes such that non zero water discharge can be recorded overall the simulated runoff efficiency values match the experimental results in wang et al 2015 very well the soil properties and the weather datasets used in this simulation are different from the observed measurements reported in wang et al 2015 but the runoff efficiency results are similar thus the runoff efficiency is a well defined parameter for characterizing the property of soil mulch and the effectiveness of the ridge furrow water harvesting configurations for a range of bulk soils and weather conditions the reasonable simulated results also show that the proposed surface runoff model has the potential to solve real world problems 4 summary in this study we developed a physical based surface runoff model and implemented it numerically with 2dsoil to simulate surface runoff water accumulation and water fluxes across the soil surface i e infiltration evaporation and exfiltration through seepage faces the runoff model was based on the saint venant equation and a heaviside expression of the soil surface boundary conditions the model accounts for water accumulation on the soil surface soil water infiltration under flux or ponded water conditions and water flow across the soil profile due to evaporation and seepage the new runoff model was evaluated using experimental runoff datasets and numerical examples for the tests with experimental datasets the simulated results agreed with the results from previous models and matched the experimental results very well the numerical examples demonstrated the model s ability to process surface water flow over a range of topography with mass balance error less than 1 thus the effectiveness and accuracy of the model was demonstrated as a practical example we applied this surface water model to a ridge furrow water harvesting system and evaluated the runoff efficiency the simulated efficiency matched the measured results very well in conclusion the surface runoff model is a reliable method in simulating surface water movement over a variety of soil surface conditions this surface runoff model can be applied in the future to simulate chemical and energy movements on a ponded surface with or without mulching or it can serve as an important supplement in the simulation of soil water management and field water budget credit authorship contribution statement zhuangji wang conceptualization writing original draft writing review editing validation visualization software formal analysis dennis timlin conceptualization writing original draft validation software mikhail kouznetsov conceptualization validation software david fleisher conceptualization validation software sanai li validation software katherine tully supervision conceptualization validation writing original draft vangimalla reddy supervision conceptualization validation writing original draft declaration of competing interests the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this material is based upon work supported by the department of agriculture agricultural research service under agreement no 58 8042 7 067 supplementary materials the source code and example files are on github and can be found online at doi 10 1016 j advwatres 2019 103499 appendix b supplementary materials image application 1 appendix 1 additional discussions for the usage of small values h 0 and h0 a physically the use of h 0 0 and h 0 0 indicates surface runoff will not occur exactly at h 0 0 and h 0 0 due to the friction resistance on soil surface and the surface tension of water thus a shallow ponded water is assumed before runoff occurs b mathematically h h 0 0 and h h 0 0 are stronger than h 0 and h 0 respectively h h 0 0 and h h 0 0 in eqs 1 and 3 ensure lipchitz continuity of the governing equations as well as the boundary conditions that implies the existence and uniqueness of the solutions followed by picard lindelöf theorem for the time domain differentiation as well as enables the application of numerical methods especially the picard s iteration for solution searching 2 some performance results tests was performed in a desktop with intel r core tm i9 7900x cpu 3 30 3 31 ghz and microsoft windows 10 64 bit operation system the time consumed for the two numerical examples i e figs 4 and 5 were less than 10 s and 30 s respectively for multiple runs 
515,surface runoff has been recognized as an important component in agricultural water management extensive studies have been developed to measure surface runoff and numerical methods have been applied to simulate surface water dynamics for simulations of surface runoff in agricultural systems three processes should be considered 1 the movement of water along the soil surface i e surface runoff 2 the accumulation of water in depressions and 3 the water fluxes across the soil atmosphere interface i e infiltration evaporation and exfiltration through seepage faces the objective of this study is to develop a physical based surface runoff model that includes all the three processes numerical implementation of the new runoff model was developed and incorporated within 2dsoil a simulation package for soil water energy and solute movement the new model describes the subsurface flow near the soil surface following a unified boundary condition expressed with the heaviside step function this expression enables continuous and automatic changes of boundary conditions between infiltration and runoff the surface water flow is simulated using saint venant equations the ponded water height on the soil surface and the infiltration rate are adjusted based on the runoff flux and topography numerical tests based on an experimental dataset are used to evaluate the accuracy of this model and numerical examples of surface water flow along a variety of topography are used to demonstrate model performance the simulations match the experimental results and the surface water mass balance errors of the numerical examples are less than 1 a practical example of using the surface runoff model to estimate the runoff efficiency in a ridge furrow water harvesting is carried out in conclusion the newly developed surface runoff model can successfully simulate surface water dynamics this model can further support the design and evaluation of agricultural water management strategies and field water budgets keywords finite element runoff infiltration surface water ponding water harvesting heaviside step function saint venant equation 1 introduction water runoff along soil surfaces is a dynamic process in response to precipitation and infiltration runoff occurs when the water input fluxes e g rainfall irrigation or runoff from upstream directions exceed the infiltration capacity of the soil surface runoff is an important component in the field water budget and it is involved in a variety of field management strategies for example nutrients and microorganisms in surface applied manure fertilizer can be transported in surface runoff during rainfall events causing contamination of nearby water bodies kouznetsov et al 2007 runoff which is affected by climate topography and plant cover is recognized as a critical driving force for soil erosion mohamadi and kavian 2015 şensoy and kara 2014 to mitigate the negative effects of runoff surface mulching contour farming and vegetated filter strips are often used to reduce runoff and retain surface water actively increasing runoff at specific locations and capturing the released water at other locations becomes a water management strategy especially in arid or semi arid areas tadmor et al 1961 for example the soil surface can be shaped with repeated ridges and furrows and the ridges mulched to reduce infiltration during rainfall runoff from the ridges is promoted and the surface water flows into the furrow areas where water is captured for crop uptake wang et al 2015 this management practice is referred to as ridge furrow water harvesting which collects the surface runoff to enhance the water use efficiency if crops are planted in the furrows wang et al 2015 zhao et al 2018 a physical description of surface runoff should include three critical processes 1 the water flow along the soil surface 2 the accumulation of water within depressions and 3 the water fluxes across the soil surface i e evaporation infiltration or exfiltration kolditz et al 2008 implementation of such a theoretical framework can be divided into two major categories for the first category in order to describe water flow along the soil surface the kinematic wave or diffusive wave approximation of the surface water flow can be directly combined with soil water movement to simulate the surface runoff this has been implemented in some shallow surface water models including parflow kollet and maxwell 2006 hydrogeosphere brunner and simmons 2011 and surface water chemical movement models based on hydrus liang et al 2017 such designs provide an efficient way to describe surface water movement which can also be generalized for high performance computing in a relatively simple manner fatichi et al 2016 however the assumption of shallow surface water flow in those models was only valid for locally smoothed surfaces and surface boundary conditions and subsurface water flows were usually simplified for example subsurface water flow was restricted as vertical infiltration however the infiltrated water can move laterally as gravity dominated saturated flow and emerge as exfiltration on other portions of soil surface few models fully characterize this process therefore existing models in this category are commonly used in an averaged sense for evaluations of large scale surface water flux such as in a whole catchment area or supporting secondary calculations such as surface chemical movement along with runoff water kolditz et al 2008 liang et al 2017 kollet and maxwell 2006 the second category of surface runoff models pursues a more detailed description of the physical processes for example singh and bhallamudi 1998 applied saint venant equations to explicitly calculate the conservation of mass and conservation of momentum of runoff water in a direction parallel to the soil surface but the study was only validated against experiments performed on a smooth sloping surface smith and woolhiser 1971 appels et al 2011 applied a philip s infiltration model and soil water movement model to study the formation merging and separation of surface ponded water within depressions this study included a relatively complex soil surface topography however the runoff was not routed along the soil surface and subsurface water flow other than infiltration was not considered on a soil surface with hills and depressions physical processes such as evaporation runoff infiltration and exfiltration via surface seepage should be allowed to occur simultaneously and switch automatically due to the changes in weather soil water regimes and surface water distribution e g transient ponded water a flexible and continuous boundary condition on the soil surface can enable the switching among those processes existing methods that simulate runoff with a finite element model switch the soil surface boundary conditions during infiltration between constant flux and constant head depending on the values of surface matrix potential this switching can be presented using some threshold functions such as the heaviside step function camporese et al 2014 bartlett et al 2015 however such switching is usually defined separately from the solver of the surface subsurface water flow thus in numerical simulations with given time increments this switching can only be implemented in discretized time points i e after each time increment of the surface subsurface water flow simulation therefore this type of switching for the boundary conditions was referred to as a discontinuous switching in the time domain in order to embed this switching in a continuous fashion kouznetsov et al 2007 went a step further and fully incorporated a heaviside step function into the governing equations of surface subsurface water movement moreover a comprehensive runoff model must incorporate subsurface hydraulic connectivity when surface runoff occurs in multiple depressions separated by hills on the soil surface this enables the calculation of lateral subsurface water flow and the quantification of exfiltration from the saturated soil surface that can drain into the downstream depressions thus there is a need to develop a comprehensive surface runoff model as these factors have not generally been implemented in existing models and numerical simulations to this time the objective of this study is to develop a physical based surface runoff model based on saint venant equation system which also incorporates soil water movement and boundary conditions for surface water ponding infiltration and exfiltration a numerical implementation of the new surface runoff model is also developed and imbedded into 2dsoil a comprehensive simulation package for soil water heat and chemical redistributions timlin et al 1996 the performance of the proposed runoff model was evaluated with experimental data and numerical examples while a potential application for water harvesting is also proposed 2 theory and model setup 2 1 the governing equations the saint venant equation system is utilized for the simulations of surface water movement it consists of a linear hyperbolic equation for mass conservation and a burgers equation for momentum conservation the momentum equation is an integrated average of the navier stokes equations with respect to flow depth the saint venant equation system can be written as follows 1 h t q l p i q t l q 2 h g h 2 2 g h s 0 s f in these equations h cm is the flow depth which coincides with the ponded water height at each surface computing node here h is constrained as h h 0 0 where h 0 is a small number 1 this prevents the case where q 2 h as h 0 the value q cm2 day 1 is the surface runoff and l represents the partial derivative along the surface slope l s 0 and s f n q 2 h 10 3 are surface slope and friction slope with manning roughness coefficient n cm 1 3 day here h is constrained as h h 0 0 where h 0 is a small number 1 this prevents the case where q 2 h as h 0 the value q cm2 day 1 is the surface runoff and l represents the gravitational acceleration and p cm day 1 is the flux density of precipitation and or irrigation while i cm day 1 represents the flux density of infiltration a common setting for the initial conditions for eq 1 is h l 0 h 0 and q l 0 0 indicating the small surface water depth and zero surface runoff along the slope at t 0 the richards equation derived from darcy buckingham s law and the equation of continuity is a widely used governing model for incompressible fluid movement in soil jury and horton 2004 with a continuous air phase the soil is assumed to be nondeformable and locally isotropic and the vapor transport is negligible because the velocity of soil water fluxes is relatively small the conservation of momentum is generally not involved in a soil water movement model the richards equation in a 2d soil matrix can be approximated as eq 2 2 θ t x k h h x z k h h z 1 s 0 where θ cm3 cm 3 represents the volumetric water content h cm is the water head t day is time x cm and z cm are horizontal and vertical distance s day 1 represents water sources or sinks the water retention curve between θ and h and the hydraulic conductivity model for k h cm day 1 are usually assumed to follow the van genuchten equation van genuchten 1980 vogel and cislerova 1988 šimůnek and suarez 1992 although other representations can be used timlin et al 1996 the soil surface boundary condition for the richards equation i e eq 2 can be written as 3 η h h t p i 0 where p cm day 1 is the precipitation and or irrigation and i cm day 1 is the surface infiltration kouznetsov 1989 surface infiltration is calculated based on the richards equation surface water flux and or the ponded depth of surface water h depending on the concrete boundary conditions timlin et al 1996 η h is the heaviside step function defined as follows 4 η h 1 h 0 0 h 0 the advantage of using this boundary condition is that it can be automatically invoked when h 0 and disabled when h 0 under the net water input flux p thus the model is able to adjust surface subsurface water exchanges between infiltration and evaporation automatically based on weather and soil water conditions in order to apply numerical solvers to eqs 2 and 3 analytic approximations to the heaviside function eq 4 and its derivative the dirac delta function δ h must be applied ηω h and δω h defined in eq 5 are those analytic approximations 5 η ω h 1 π arctan h ω 1 2 η h δ ω h 1 π ω ω 2 h 2 δ h the functions ηω h and δω h respectively converge to the heaviside step function eq 4 and the dirac delta function point wisely as ω 0 we require h h 0 0 in order to use eq 3 where h 0 is a small positive head 0 01 above which surface runoff occurs singh and bhallamudi 1998 for each computing node with associated surface water infiltrability the time it takes for the soil water potential to reach h 0 can be used to determine the ponding time kouznetsov et al 2007 which indicates the time period from the beginning of precipitation to the initiation of runoff refer to the appendix for additional discussion on the usage of small values h 0 and h 0 for the saint venant equation and the heaviside boundary condition an alternate method to calculate overland flow is to express the subsurface and overland flow in single equation system using a pre defined partition function with convex regulators those regulators define the distribution coefficients that quantify how much water belongs to overland flow and how much water stays within the soil matrix marçais et al 2017 however in this study the soil water movement and the surface runoff were explicitly solved via two differential equation systems this enables the use of a single boundary condition eq 3 that links soil water flow and surface runoff 2 2 numerical implementation numerical implementation of the proposed surface runoff model was developed independently and incorporated with 2dsoil 2dsoil is a modular based soil simulator including soil water solute and energy movement with interfaces for climate and plant growth timlin et al 1996 2dsoil has been successfully applied to simulate water energy solute transport and crop growth kemp et al 1997 timlin et al 2002 yang et al 2009 kim et al 2012 timlin et al 2019 the advantage of using 2dsoil is that it already supports simulation of a wide variety of environmental processes and is designed to be easily interfaced with additional modules such as climate models and crop models this model uses a 2d finite element representation of the soil domain which was developed from swms2d šimůnek et al 1994 2dsoil also simulates a wide range of meteorological processes timlin et al 2002 which includes the penman equation penman 1963 for potential evaporation from the soil surface in addition the code for the proposed runoff model i e the saint venant equations and the heaviside boundary condition can be placed following a uniform data input format and a data flowchart pre specified in 2dsoil thus it helps standardize the development of future soil models and simplifies the maintenance of the new surface runoff model numerical methods to solve the saint venant equation system have been extensively studied greco and panattoni 1975 mangeney castelnau et al 2003 especially via a mixed hybrid finite element scheme chavent and roberts 1991 fig 1 illustrates an example of the computing domain the variables x k k 0 n are n 1 computing nodes represented with red circles the subscript k is the index of the computing node the flow depths or ponded water heights h k t k 0 n are evaluated at each xk the surface fluxes marked with solid arrows occur between two adjacent computing nodes thus n flux terms q k t k 1 n are involved in the numerical scheme the superscript t indicates time step numerical computation of heads and fluxes at the left and right boundaries of the soil surface fall into two cases 1 if q points to the interior of the computing domain zero runoff discharge from soil surface is assumed 2 if q points to the exterior of the computing domain the water can freely flow away from the soil surface in fig 1 the runoff discharge will occur at the right boundary of the surface the first equation in eq 1 can be discretized into a conservative form 6 h k t 1 h k t δ t q k 1 t q k t l k p k i k while the discretization of second equation in eq 1 follows the upwind direction i e 7 q k 1 t 1 q k 1 t δ t 2 l k 1 l k q k 1 t 2 h k t g h k t 2 2 q k t 2 h k 1 t g h k 1 t 2 2 g h k t s 0 s f due to the non linearity of this momentum equation picard iteration is applied to achieve the numerical solution please refer to the appendix for the validity of applying picard iteration when the soil surface has depressions the surface water will flow from points of high elevation and accumulate in the valleys possibly forming small transient water pools however direct application of the saint venant equation to characterize the water movement to and from these transient pools is not always the best approach as the water depth increases the soil surface friction between the water and surface soil decreases with respect to the ponded height therefore the water movement near the surface of the ponded water can be approximated as free water flow with relatively large momentum while near the bottom of the transient water pools horizontal momentum of water may vanish and ponded infiltration can be assumed the saint venant equation only provides a vertically averaged result for the momentum thus after solving the saint venant equation system additional procedures are taken for the calculation of water movement within the pools one simple approach is to provide an additional averaging process to adjust the water flow in the ponded area where ponded water of adjacent computing nodes can be exchanged based on the soil surface elevation and the ponded water heights for example given two adjacent computing nodes a and b if the water elevation at computing node a is higher than at node b and the ponded heights at both nodes are relatively large such that ponded water forms a smooth surface and the surface water can be considered as friction free then node a will provide water for node b until both nodes reach the same water elevation however if computing node a has a relatively large water surface elevation but with a relatively small ponded height such that surface water adheres to the soil particles then node a is not able to supply any water for node b and there will be no water exchange between node a and node b these processes will be iterated until the free water surface within each individual pool reaches a stable level the existence of small pools may alter the water infiltration patterns changing the soil surface from an evaporation condition or an unsaturated infiltration condition to a ponded infiltration condition after the rainfall the infiltration at the hilled locations will immediately terminate while the ponded water can maintain saturated infiltration within the depressions for a certain time period in the numerical implementation the ponded depth within the water pools is tracked by the surface runoff model and values of water head are exchanged with the 2dsoil water movement module such that the infiltration process can be adjusted automatically as ponded water is available for the computing nodes within depressions we assume infiltration is the prior first outlet for the ponded water and the ponded height will be decreased or maintained by the incoming surface fluxes from nearby computing nodes by doing that the model simulates infiltration fluxes and surface runoff after rainfall events the infiltrated water may flow laterally within the soil and emerge as exfiltration on the soil surface for example this will occur at locations where the elevation of the soil surface is lower than a nearby transient water pool the infiltration at the bottom of a water pool can elevate the underground water table that lies below the ponded zone thus at those locations when the underground water table reaches the soil surface seepage faces can be formed and exfiltration will be initiated the exfiltration flux that occurs on the soil surface can become surface runoff in the downstream direction thus the calculation of ponded water lateral soil water movement and surface runoff should be combined and the hydraulic connections accounted for in the numerical implementation the 2dsoil water movement module uses a seepage condition to trace the locations of exfiltration fluxes the exfiltration rate calculated by 2dsoil is used by the surface runoff model to calculate surface water flux via eq 1 fig 2 presents a flow diagram for the surface runoff model and the data flow paths the surface runoff module is placed between the weather module and the water movement module because the surface water can be considered as an interface between atmospheric water and soil water the solid arrows indicate the original data paths while the dashed arrows indicate the alternative data paths after the surface runoff module is invoked thus precipitation from the weather module and the soil water infiltration from the water movement module are the input data the three critical factors surface runoff surface water accumulation and water fluxes across the soil surface are all included in this new surface runoff module 3 illustrative examples and applications in this section we first verify the accuracy of the surface runoff model based on an experimental dataset then we explore the effectiveness of the surface runoff model by two numerical examples with a variety of soil surface topography see figs 4 and 5 the goals of applying numerical examples are 1 to test the model performance on complicated soil surface and 2 illustrate how the proposed model handles more than one type of water flux across irregular soil surfaces the variations of surface water fluxes with respect to time including precipitation infiltration and runoff as well as the surface water storage are provided in fig 6 finally we illustrate a potential application of the proposed surface runoff model for water harvesting see figs 7 and 8 all the numerical examples and code presented in this study are available online https github com ars csgcl dt 2dsoilrunoff 3 1 comparison between simulation and experimental data smith and woolhiser 1971 reported a laboratory experiment of coupled surface subsurface water flow in a soil with a linear sloping surface their surface flow data was one of the datasets commonly used for model validation and comparison in the smith and woolhiser 1971 experiment the soil sample was packed into a 12 2 m length 0 051 m width and 1 22 m depth tank the surface slope was 1 a light oil was used as the test fluid with viscosity similar to water the rainfall intensity was 250 mm h 1 over a duration of 15 min the runoff discharge from the sloping surface was collected at the lower edge the packed soil sample contained three layers with thicknesses of 76 5 229 5 and 761 mm the water retention curves and the unsaturated hydraulic conductivity for the three layers are available in singh and bhallamudi 1998 in the numerical simulation an impermeable boundary condition for liquid fluxes was used for the vertical edges and the bottom on the soil surface a flux boundary condition was assumed initially and the surface runoff model would adjust the surface boundary condition based on the soil liquid content and ponded liquid height because the background soil liquid content was not provided in the experiment we assume the initial potential to be 25 cm fig 3 presents the comparison among the measured runoff discharge and the simulated results the simulations by smith and woolhiser 1971 and the proposed runoff model via 2dsoil match the experimental results providing evidence for the accuracy of the newly developed surface runoff model during the initial segment of surface runoff between 8 min and 10 min the simulated results from singh and bhallamudi 1998 are less than the experiment results and the new model simulations indicating some liquid is blocked on the soil surface although both singh and bhallamudi 1998 and the proposed surface runoff model utilized the saint venant equations the surface runoff model in this study adopted an upwind mixed hybrid finite element scheme and had additional adjustments for ponded heights and infiltration those could be the reasons leading to the difference between those two simulation results 3 2 numerical examples following the experimental test we give two numerical examples to demonstrate the ability of this new model to simulate surface water flow with relatively complicated topography the first example is illustrated in fig 4 a which presents a soil profile with one single depression the horizontal distance indicates the width of the soil profile while the vertical distance indicates its height the solid curve indicates the soil surface with red circles marking the computing nodes in the simulations the soil was assumed to be homogeneous and nondeformable the hydraulic properties followed the van genuchten model with the parameters listed in table 1 soil a rainfall was applied for 24 h at the rate of 70 mm h 1 impermeable conditions were applied for the vertical boundaries a seepage condition was adopted for the bottom and a flux condition was initially applied on the soil surface the initial water potential was 100 cm through the soil profile the ponded water heights gray area within the depression on soil surface increased with respect to time during the rainfall while the wetting front propagated downwards fig 4 b and c present the ponded heights at 2 and 4 h after the simulation and rainfall initiated the wetting front was nearly flat in this example because the rainfall was homogeneous on the soil surface the depression was full at 4 h after the rainfall started and then extra surface water flowed to the left edge and discharged from the surface after the rainfall terminated the simulation continued for another 24 h to demonstrate the decreasing ponded heights within the depression fig 4 d e and f present the ponded heights 24 28 and 34 h after the simulation launched i e 0 4 and 10 h after the rainfall stopped soil water content was maintained due to the infiltration of ponded water while soil evaporation occurred at the soil surface beyond the free water surface the evaporation on the free water surface follows the potential evaporation fig 6 a summarizes the fluxes and water storage during this 48 h simulation the large initial infiltration flux was due to the relatively dry soil profile the infiltration rate decreased and reached steady state 7 h after the rainfall initiated the runoff discharge started 4 h after the rainfall started from the left edge of the soil surface which coincided with the time when the ponded water height reached the left edge after the rainfall stopped the runoff discharge stopped immediately because the ponded height became lower than the left edge then the simulation automatically switched to falling head ponded infiltration within the depression for another 5 h until all the water entered the soil profile this example illustrates the ability of the proposed surface runoff model to simulate surface water flux as well as water accumulation and produce plausible results with surface water mass balance error less than 1 along a slope the soil surface may have multiple depressions and those depressions can be separated by hilled soil water infiltrated in higher depressions can flow laterally within the soil profile and emerge from the soil surface as exfiltration through a seepage face in this case a surface subsurface surface hydraulic connection is established a physical based surface runoff model together with soil water movement model should be able to simulate water fluxes in this scenario fig 5 a shows a soil surface with a hill and a depression the soil in this example has the same hydraulic properties as the previous numerical example fig 4 the rainfall rate was 70 mm h 1 only over the depression area for 24 h there was no rainfall to the left of the hilled soil the runoff discharge was collected from both the left and right edges runoff discharge from the left edge is accompanied by lateral subsurface water flow under the hilled soil and exfiltration on the left slope of the soil hill in this example the vertical and bottom boundaries of the soil profile were assumed to be impermeable to water fluxes fig 5 b and c present the ponded water heights and soil water distribution 3 h and 6 h after the rainfall started since the rainfall was only applied on the right side of the soil surface over the depression the wetting front under the depression propagated deeper than the wetting front in the left part of the soil profile thus in contrast to the soil water regimes presented in fig 4 b and c the infiltration front is not uniform as it diffuses downward fig 5 d e and f present the ponded water height and soil water regimes 24 30 and 36 h after the simulation launched or 0 6 and 12 h after the rainfall stopped like fig 4 the water surface within the depression remained flat during the increasing and decreasing periods although the variations of soil water content were not obvious in fig 5 d e and f soil on the top of the hill did not reach saturation during the simulation fig 6 b summarizes the fluxes and water storage during the simulation the net infiltration on the soil surface the net infiltration is equal to infiltration minus exfiltration started with a relatively high value and dropped to zero after the soil approached to saturation because there was no water leakage from the vertical and bottom soil boundaries a relatively complicated decreasing increasing decreasing pattern of infiltration was shown during the first 10 h of the experiment the slight increasing of infiltration was due to the pressure provided by ponded water while the time that infiltration reached a local maximum and started to slowly decrease coincided with the time when exfiltration and left side runoff occurred see fig 6 c when water started to seep from the left part of the soil surface the net infiltration decreased the runoff discharge increased with respect to time and approached the rainfall fluxes at steady state theoretically all the rainfall became runoff discharge the runoff discharge was collected from both edges of the soil surface where exfiltration occurred from x 0 to x 18 along the soil surface and provided runoff discharge on the left edge fig 6 c presents the amount of runoff discharge from the left edge and the right edge during the rainfall the runoff collected at the right edge was twice as large as the runoff at the left edge because the runoff discharge at right edge was directly from the free water surface while the runoff discharge at left edge had to flow across the hilled soil based on the simulation the difference between the runoff discharge and the rainfall i e the mass balance error during the steady stage was less than 1 after the rainfall stopped the runoff discharge at the right edge immediately dropped to zero like the runoff results in the previous example fig 6 a however the runoff flux at the left edge decreased gradually and reached zero after another 20 h one reason is that the ponded water in the depression kept supplying runoff for 10 h when the ponded water fully infiltrated 35 h after the simulation launched the red curve reached 0 in fig 6 b the soil water stored beyond the left edge began to drain due to the assumptions of impermeable boundary conditions this drainage supplied the left edge water discharge for another 10 h further evidence of soil water drainage is shown in the net infiltration curve in fig 6 b a negative net infiltration indicated that drainage from the soil surface between the 35th h and the 45th h this numerical example demonstrates the importance of subsurface water movement in the simulations of surface water movement since the subsurface water movement through the hilled soil serves as a bridge connecting the surface runoff on its left and right sides and this example illustrates that the proposed surface runoff model can include the interactions between surface water flow and subsurface water flow although the soil surfaces in the two numerical examples are chosen arbitrarily they contain some geometrical configurations commonly shared on agriculturally managed soil surfaces by re scaling and connecting the example soil surfaces side by side it is possible to approximate a natural soil surface thus the numerical examples discussed are representative and meaningful please refer to the appendix for performance information 3 3 application example of the surface runoff model a potential application of the surface runoff model in field water management is to simulate the surface water redistribution in a rainfall harvesting management technique that utilizes ridges and furrows this is a method to increase soil water storage water use efficiency and crop yield in arid or semi arid areas a diagram of a ridge furrow construction for water harvesting from a study by wang et al 2015 is shown in fig 7 here only the ridge part is mulched during a large rainfall surface runoff will occur on the ridge and the runoff will flow into the furrow along the ridge surface thus the water input within the furrow can be increased promoting deep water percolation into the soil between rainfall events soil water within the furrow will be partially moved under the ridges and stored for root water uptake due to the soil thermal gradient and mulching zhao et al 2018 for management evaluation two water related quantities can be determined i e the runoff efficiency and the soil water distribution zhao et al 2018 reported numerical and experimental results for soil water storage and soil water distribution surface runoff efficiency which is directly related to the surface water model was studied by wang et al 2015 the runoff efficiency is defined as the ratio between the runoff collected at point a and b fig 7 and the total rainfall received on the ridge the runoff efficiency is a normalized parameter that is strongly related to the soil surface configuration and the mulching materials for example with an ideal impermeable mulch the runoff efficiency should be 1 meaning all the rainfall received on the ridges will flow into the furrows wang et al 2015 reported the ridge furrow designs for optimum runoff efficiency over a range of surface mulching materials in the loess plateau china we chose one of their designs for demonstration proposes fig 7 the horizontal scale of the furrows and ridges was 60 cm and the ridge was 15 cm higher than the soil surface the ridge shape followed a cosine curve weather data from iowa between mar 23th and oct 27th 2006 were used in the simulation and the soil hydraulic properties were those of soil a in table 1 fine textured soil with properties shown in table 1 soil b was used to approximate the soil mulching in wang et al 2015 plastic film mulching was used to approximate the bio degradable or the plastic mulching mentioned in wang et al 2015 fig 8 a presents the simulated runoff efficiency with fine textured soil mulching the data points follow two patterns one is along the horizontal axis and the another one is shown as the upper edge of the shaded angular area with the regression line y 0 27x 0 55 the averaged runoff efficiency is represented with the red regression line y 0 18x 0 88 the slope of the regression line can be considered as the runoff efficiency the upper edge of the shaded area in the figure has a similar slope as the results reported by wang et al 2015 because the mulching is made with soil part of the rainfall received on the ridge becomes infiltration and a small portion of the surface water adheres to the soil or evaporates along the ridge surface thus the runoff efficiency of soil mulching is relatively low the simulated pattern along the horizontal axis was not presented in the experiment in wang et al 2015 however it can represent the scenario when the soil is relatively dry or rainfall rate small in which case most of the rainfall infiltrates into the mulched soil and no surface runoff is produced thus both patterns shown in the simulated results are plausible only a few data points fall between the two patterns because 1 the fine texture soil mulch is relatively thin such that its water content and hydraulic conductivity reach steady state rapidly and 2 the fine texture soil mulch can limit infiltration depending on infiltration rate thus the surface of the ridge parts can shift from infiltration condition to steady runoff condition in a short time period this limits the number of points between high runoff and no runoff fig 8 b presents the simulated runoff efficiency with the plastic mulching only one pattern is shown represented with red regression line y 0 93x 0 11 because the plastic film is assumed to be impermeable to water most of rainfall is changed into surface runoff and supplies the soil water storage in the furrow but a small portion of water may adhere to the plastic surface or evaporate before reaching point a or b a few data points are beyond the 1 1 line because the model counted the rainfall or runoff quantity every hour therefore when a relatively large rainfall stops the surface runoff may be kept for another few minutes such that non zero water discharge can be recorded overall the simulated runoff efficiency values match the experimental results in wang et al 2015 very well the soil properties and the weather datasets used in this simulation are different from the observed measurements reported in wang et al 2015 but the runoff efficiency results are similar thus the runoff efficiency is a well defined parameter for characterizing the property of soil mulch and the effectiveness of the ridge furrow water harvesting configurations for a range of bulk soils and weather conditions the reasonable simulated results also show that the proposed surface runoff model has the potential to solve real world problems 4 summary in this study we developed a physical based surface runoff model and implemented it numerically with 2dsoil to simulate surface runoff water accumulation and water fluxes across the soil surface i e infiltration evaporation and exfiltration through seepage faces the runoff model was based on the saint venant equation and a heaviside expression of the soil surface boundary conditions the model accounts for water accumulation on the soil surface soil water infiltration under flux or ponded water conditions and water flow across the soil profile due to evaporation and seepage the new runoff model was evaluated using experimental runoff datasets and numerical examples for the tests with experimental datasets the simulated results agreed with the results from previous models and matched the experimental results very well the numerical examples demonstrated the model s ability to process surface water flow over a range of topography with mass balance error less than 1 thus the effectiveness and accuracy of the model was demonstrated as a practical example we applied this surface water model to a ridge furrow water harvesting system and evaluated the runoff efficiency the simulated efficiency matched the measured results very well in conclusion the surface runoff model is a reliable method in simulating surface water movement over a variety of soil surface conditions this surface runoff model can be applied in the future to simulate chemical and energy movements on a ponded surface with or without mulching or it can serve as an important supplement in the simulation of soil water management and field water budget credit authorship contribution statement zhuangji wang conceptualization writing original draft writing review editing validation visualization software formal analysis dennis timlin conceptualization writing original draft validation software mikhail kouznetsov conceptualization validation software david fleisher conceptualization validation software sanai li validation software katherine tully supervision conceptualization validation writing original draft vangimalla reddy supervision conceptualization validation writing original draft declaration of competing interests the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this material is based upon work supported by the department of agriculture agricultural research service under agreement no 58 8042 7 067 supplementary materials the source code and example files are on github and can be found online at doi 10 1016 j advwatres 2019 103499 appendix b supplementary materials image application 1 appendix 1 additional discussions for the usage of small values h 0 and h0 a physically the use of h 0 0 and h 0 0 indicates surface runoff will not occur exactly at h 0 0 and h 0 0 due to the friction resistance on soil surface and the surface tension of water thus a shallow ponded water is assumed before runoff occurs b mathematically h h 0 0 and h h 0 0 are stronger than h 0 and h 0 respectively h h 0 0 and h h 0 0 in eqs 1 and 3 ensure lipchitz continuity of the governing equations as well as the boundary conditions that implies the existence and uniqueness of the solutions followed by picard lindelöf theorem for the time domain differentiation as well as enables the application of numerical methods especially the picard s iteration for solution searching 2 some performance results tests was performed in a desktop with intel r core tm i9 7900x cpu 3 30 3 31 ghz and microsoft windows 10 64 bit operation system the time consumed for the two numerical examples i e figs 4 and 5 were less than 10 s and 30 s respectively for multiple runs 
516,empirical relationships that describe two phase flow in porous media have been largely hysteretic in nature thereby requiring different relationships depending on whether the system is undergoing drainage or imbibition recent studies have suggested using interfacial area to close the well known capillary pressure saturation relationship while others expand upon this by including the euler characteristic for a geometric description of the system with the advancement of fast x ray microtomography at synchrotron facilities three dimensional experiments of two phase quasi and non equilibrium flow experiments were conducted to quantify the uniqueness of constitutive relationships under different flow conditions we find that the state functions that include the euler characteristic provide the most unique prediction of the state of the system for both quasi and non equilibrium flow of these functions those that infer volume fraction from the other state variables are independent of flow condition quasi or non equilibrium this enhances the applicability of new constitutive relationships allowing for more robust models of two phase flow keywords two phase flow non equilibrium flow fast x ray microtomography interfacial area euler characteristic 1 introduction immiscible flow of two fluids in porous media occurs in many environmental and industrial applications some two phase flow processes in the environment include groundwater remediation enhanced oil recovery and geologic carbon sequestration whereas industrial applications include among others fluid separation in fuel cells the adsorption of liquids in absorbing materials and the drying of porous products such as paper pulp and building materials the societal economic and environmental impacts of these systems require well informed two phase flow theories and mathematical models to predict the underlying processes traditionally practitioners have relied on empirical relationships between macroscopic capillary pressure pc and wetting phase saturation sw to model two phase systems defining capillary pressure as 1 p n p w p c s w where pn is the average non wetting phase pressure for the system and pw is the average wetting phase pressure this relationship assumes that pc is solely a function of saturation while processes that affect fluid distribution such as porous medium wettability fluid fluid interfacial configuration and pore morphology are not accounted the pc sw relationship is well known to exhibit hysteretic behavior i e the relationship is process and history dependent and therefore separate functions are needed depending on whether the system is undergoing drainage or imbibition it has been proposed that a geometric description of the system needs to be included to uniquely describe two phase flow systems hassanizadeh and gray 1990 1993 suggested that specific interfacial area between the wetting and non wetting phases anw accounts for variations in fluid configurations that can be present in a porous medium they proposed that when taking into account interfacial area in the constitutive equations such that pc pc sw anw the drainage and imbibition pc sw anw relationships would fall on a unique surface i e the expanded relationship would be non hysteretic and a single relationship could describe the state of the system there has been a concerted effort to validate this theory both numerically and experimentally micromodel cheng et al 2004 chen et al 2007 pyrak nolte et al 2008 karadimitriou et al 2013 and microtomography imaging experiments porter et al 2010 have shown that the pc sw anw relationship is unique within experimental error under quasi equilibrium flow conditions similar results were obtained numerically via pore network modeling reeves and celia 1996 r j held and celia 2001 joekar niasar et al 2008 joekar niasar and hassanizadeh 2011 and a lattice boltzmann model porter et al 2009 however a lingering question is whether these observations hold true under non equilibrium flow conditions non equilibrium flow is a generic term that can cover a broad spectrum of flow in porous media most prominently the term is associated with dynamic or transient flow being effectively reserved for flow with capillary numbers outside of the capillary fingering regime in terms of the lenormand phase diagram lenormand et al 1988 an important facet of non equilibrium flow is the resulting absence of interfacial relaxation towards equilibrium which then affects macroscopic invariants such as saturation average capillary pressure and specific interfacial area gray et al 2015 schlüter et al 2017 in this research we use the term non equilibrium flow for flow experiments that do not allow for interfacial relaxation during drainage imbibition cycles rather than the terms dynamic or transient which connote higher capillary number flows some effort has been made to study the uniqueness of the pc sw anw relationship under transient vs quasi equilibrium conditions in micromodels karadimitriou et al 2014 godinez brizuela et al 2017 and using simulations via dynamic pore network modeling joekar niasar and hassanizadeh 2012 the dynamic pore network model found some differences between the transient and quasi equilibrium pc sw anw surfaces but determined that the differences were negligible the micromodel studies on the other hand showed that the transient and quasi equilibrium pc sw anw surfaces were statistically different to our knowledge no published research exists that compares the pc sw anw relationship under quasi and non equilibrium conditions for a fully three dimensional experimental system an assumption underlying the original theory behind the pc sw anw relationship hassanizadeh and gray 1990 is that each fluid phase is continuous but in most circumstances this condition is not met in two phase flow systems therefore other state variables have been investigated that can describe connectivity and shape of the fluids such as fluid topology herring et al 2013 mcclure et al 2016 schlüter et al 2016 the euler characteristic of the non wetting phase χn has been used as the standard to measure the connectedness of fluid configurations schlüter et al 2016 showed that the euler characteristic conveys complementary information that helps explain the hysteresis present in the anw sw relationship using lattice boltzmann model simulations mcclure et al further demonstrated that the inclusion of χn in a non dimensionalized pc sw anw relationship removes nearly all hysteresis under equilibrium mcclure et al 2016 2018 and dynamic mcclure et al 2018 conditions the ability to visualize non equilibrium multi phase flow in three dimensional porous media has not been possible until very recently with the improvements in imaging hardware detector speed and sensitivity along with increased x ray brilliance capabilities at synchrotron sources x ray computed microtomography µct has become a viable technology to meet this need berg et al 2013 and youssef et al 2013 were among the first to investigate two phase flow using fast µct berg et al determined that a time resolution between 10 and 30 s was sufficient to capture differences in fluid front configurations surrounding a haines jump during two phase flow youssef et al used the technique to determine that saturation of a sandstone followed a linear dependence on the square root of time thereby validating the washburn equation with real time three dimensional data since these initial forays into fast µct there has been a variety of studies looking at drainage and or imbibition processes of two phase flow armstrong et al 2014 leu et al 2014 andrew et al 2015 berg et al 2016 schlüter et al 2016 singh et al 2017 schlüter et al 2016 did collect three dimensional experimental data of a time resolved two phase flow experiment and determined that the pc sw anw relationship was unique in the supporting information however they used external pressure transducer readings rather than pc determined from internal interfacial curvature measurements to establish their pc sw anw curves scanning curves imbibitions and drainages that start at intermediate wetting saturations were not collected in the study resulting in an absence of data points that provide information about the space between the hysteretic main pc sw loop the objective of this study was to generate experimental three dimensional quasi and non equilibrium two phase datasets using fast microtomography to compare an array of state variable constitutive relationships and assess whether any are unique under either flow condition thus determining if a single relationship can describe the state of a system independent of flow condition 2 materials and methods 2 1 experimental setup soda lime glass beads 0 8 1 2 mm were randomly packed and sintered into a borosilicate glass column 6 mm diameter 26 mm height as a simplified porous medium for both two phase flow experiments the porosity ϕ for the quasi and non equilibrium experiments were 0 39 and 0 37 respectively the glass bead columns were contained in a sample holder which was mounted on the x ray µct rotation stage during flow experiments and image acquisition the bottom of the sample holder was connected to a harvard apparatus phd 2000 precision syringe pump controlling fluid volumes and flow rates the pump directly controlled the flow of the wetting phase water inducing the flow of the non wetting phase air the wetting phase was a 1 6 solution by mass of potassium iodide ki to deionized water ki is used as a contrast agent for better differentiation between the fluid phases in x ray images a hydrophilic nylon membrane 1 2 µm pore size compressed between two o rings was used at the bottom of the setup to seal the system and to reach low water saturation within the column a pressure transducer validyne p55 differential pressure transducer was inserted in the water line below the porous medium to track the differential pressure between the fluid phases for comparison to capillary pressures calculated from image analysis differential pressure was measured from the top of the porous medium as the pressure difference between the atmosphere and the bulk wetting phase the top of the sample was open to the atmosphere through a long 2 m tygon tube to prevent evaporation within the column during lengthy experiments 2 2 fast x ray computed microtomography all experiments were performed using synchrotron radiation at the 13 bmd beamline at the advanced photon source aps located at argonne national laboratory in lemont il the fast microtomography method at 13 bmd uses pink beam rather than the more standard monochromatic beam method pink beam is produced by filtering the low and high x ray energies from the full spectrum white beam that is produced by a synchrotron a 1 0 mm copper cu filter was used to filter out the low energy x rays the high energy x rays were removed by reflecting the beam from a platinum pt mirror angled at 1 5 mrad towards the experimental setup all scans were collected with a 12 bit cmos camera with a resolution of 1920 1200 pixels a nikon macro lens and a lutetium aluminum garnet scintillator the pixel size on the sample was 3 2 µm each scan consists of a 180 rotation of the sample with two dimensional radiographic projections taken at 900 angles the exposure time of each projection was 15 ms corresponding to an acquisition time of 13 5 s for a volumetric image ten flat field scans radiographs without the sample present were acquired before and after the scan for white field correction to unwind all fluid lines the sample was rotated back to 0 after each scan before another scan was initiated allowing for scan time flat field acquisitions and sample rotation the temporal resolution between each data point using pink beam was 50 s the x ray beam was turned off when images were not being acquired to reduce x ray exposure 2 3 two phase flow experiments 2 3 1 quasi equilibrium flow experiment the quasi equilibrium flow experiment was carried out using flow conditions similar to previous µct fluid flow experiments e g culligan et al 2004 porter et al 2010 but x ray images were acquired throughout the experiment rather than just at each quasi equilibrium point where the pump was stopped and the fluids were allowed to relax towards equilibrium before the wetting phase was introduced to the glass bead column a dry scan was acquired to facilitate easy separation of the solid phase from the fluid phases in the partially saturated images the dry porous medium was initially saturated with wetting phase at a rate of 5 ml hr fully saturating the field of view fov after saturation a full cycle of primary drainage pd main imbibition mi main drainage md and three scanning curves two starting on the mi curve and one on the md curve were executed at a flow rate of 0 2 ml hr the scanning curves si1 si2 sd1 were used to help fill in the area between the main imbibition drainage pc sw loop the capillary number ca for this flow rate was 2 10 7 which is indicative of a capillary dominated flow regime flow was stopped at different saturation points along each drainage and imbibition curve corresponding to a change of 10 in saturation where the fluids were allowed to relax towards hydrostatic equilibrium before flow was reinitiated scans were also acquired during this relaxation period to study how interfaces relax towards equilibrium although relaxation occurs over multiple hours gray et al 2015 schlüter et al 2017 the majority of saturation change occurs within the first few minutes to be able to compare with other quasi equilibrium experiments culligan et al 2004 porter et al 2010 each quasi equilibrium point was allowed about 15 min for relaxation fig 1 shows an example of how the fluid fluid interfaces retract during this relaxation period 2 3 2 non equilibrium flow experiment the non equilibrium flow experiment was performed in a similarly prepared glass bead column using the same flow rate ca 2 10 7 pd mi md and three scanning curves were collected with image acquisition occurring throughout the experiment the only difference from the quasi equilibrium flow experiments was that the pump was only stopped at the end of each drainage imbibition branch therefore the fluids did not relax during any of the drainage or imbibition experiments 2 4 image processing the x ray projections of each scan were reconstructed into a three dimensional image with the tomorecon software based on gridrec rivers 2012 although there is a ring artifact removal step in the reconstruction algorithm which can handle small artifacts associated with the detector some high frequency ring artifacts associated with pink beam imaging remain these large ring artifacts cause problems during image segmentation as phantom fluid phases are created near the ring artifacts leading to errors during state variable calculation these artifacts are attributable to defects in the pt mirror that is used to filter the high energy x rays details of the method used to remove high frequency ring artifacts is included in the supporting information figure s1 the images were denoised using a non local means filter buades et al 2005 this efficiently denoises the image while preserving image features such as fluid fluid interfaces which are important for accurate calculation of specific interfacial area and capillary pressure from image derived fluid curvatures all images were aligned to the dry scan acquired before the fluid flow experiment using automatic image registration the three phases wetting non wetting solid in each image where separated using a watershed segmentation method beucher and lantuejoul 1979 fig 1 shows a volume rendering of the segmented non wetting phase in the glass bead column during drainage from the segmented image three dimensional isosurfaces were generated using a marching cubes algorithm lorensen and cline 1987 between each phase so that specific interfacial area could be calculated from the images we adopted the surface smoothing approach constrained smoothing smoothing extent 3 of li et al 2018 for images with similar voxel resolution they found that these smoothing parameters preserved the original data while achieving the necessary level of smoothing of the interfaces ring artifact removal was performed using imagej but the remaining image processing steps were completed using avizo 2 5 data processing 2 5 1 porosity saturation and specific interfacial area porosity and saturation values were calculated by counting the number of voxels present for each phase in the segmented images the fluid phases were further separated into connected and disconnected volumes to distinguish the bulk phases for comparison with external pressure transducer measurements it was assumed that the wetting phase regions connected to the bottom of the fov and the non wetting phase regions connected to the top of the fov represented the bulk phases because the outlets of the column were outside of the fov it should be noted that these criteria do not guarantee that the identified regions are actually connected to the bulk phases especially at low saturations isosurfaces between the fluid phases were generated using a marching cubes algorithm in avizo and total fluid fluid interfacial area anw was calculated by measuring the total surface area of the triangles comprising those isosurfaces specific fluid fluid interfacial area anw was then calculated by dividing anw by the total volume a representative elementary volume rev analysis for a subset of saturation and specific interfacial area values are presented in the supporting information figure s2 the analysis shows that the parameters stabilize towards the total fov volume as reported in similar porous media studies e g porter et al 2010 schlüter et al 2016 2 5 2 mean curvature and capillary pressure average mean curvature jnw was calculated using a modified method of li et al 2018 this involves surface modification removal of triangles in close proximity to the solid surface various studies herring et al 2017 singh et al 2017 li et al 2018 have reported that surface modification was required to improve curvature estimation from microtomography images due to partial volume effects near the three phase contact line at the solid surface following the methodology of li et al a distance threshold dmin was set to 20 of the maximum distance between a point on the fluid fluid surface and the nearest point on a solid surface the mean curvature of each remaining triangle jl for every feature i was then weighted by its distance to the solid surface d l assigning more weight to triangles further away and less likely to be affected by the solid surface eq 2 pore scale capillary pressure p i c is calculated using the young laplace equation eq 3 where the interfacial tension between the fluid phases σ is 0 072 n m each feature can then be weighted by its interfacial area a i n w mcclure et al 2016 godinez brizuela et al 2017 to determine the macroscopic average capillary pressure of the system pc 2 j i l n j l d l l n d l for d l d m i n 3 p i c 2 j i σ 4 p c i n p i c a i n w i n a i n w in the supporting information figure s3 the average capillary pressure of the connected fluid phases is compared to the pressure differential measured from external pressure transducers this has previously been used as a basis for the verification of the curvature measurement li et al 2018 schlüter et al 2017 as the connected phases should equilibrate to the bulk fluid pressures and therefore be equal to the pressure differential measured by the external pressure transducers for the quasi equilibrium case although the capillary pressure of the boundary connected fluids has generally been used in previous quasi equilibrium experimental studies these pressures do not necessarily represent the system under non equilibrium flow conditions or more broadly under dynamic conditions for higher capillary numbers due to non equilibrium capillarity effects hassanizadeh et al 2002 these pressure measurements do not include disconnected fluids that contribute to measures of other state variables e g saturation which may suggest that they are measured in a fundamentally inconsistent way mcclure et al 2016 one cannot neglect the disconnected fluid regions without also neglecting the associated internal energy and any definition for the macroscopic average pressure must respect this in order to develop models that conserve energy this study instead uses the area weighted average curvature and capillary pressure of all fluid interfaces which is consistent with other studies testing two phase flow constitutive relationships e g joekar niasar and hassanizadeh 2012 mcclure et al 2016 godinez brizuela et al 2017 2 5 3 euler characteristic the euler characteristic χ is a topological invariant that describes the connectivity of a fluid phase it is described by the following relationship 5 χ β 0 β 1 β 2 where β0 is the number of distinct regions of the fluid phase β1 is the number of redundant connections within each distinct region and β2 is the number of encapsulated voids in each region the β2 value for the non wetting phase can be neglected in this study since an element of water or solid could not be completely suspended in air fundamentally the euler characteristic quantifies the entrapment of fluids and the breaking up of fluid connections through pore throats due to snap off this phenomenon has been established as an important source of hysteresis in multi phase flow r j held and celia 2001 joekar niasar et al 2013 schlüter et al 2016 which makes the euler characteristic an interesting state variable to consider in constitutive relationships schlüter et al 2016 mcclure et al 2018 2 5 4 constitutive state variable relationships the constitutive relationships we are studying are based on a generalized form of the minkowski steiner formula federer 1959 mcclure et al 2018 6 d v n c 1 a n d r c 2 j n d r 2 c 3 χ n d r 3 where ci are coefficients determined by the shape of the non wetting phase domain and dr is the radius of a small ball the state variables are extensive and therefore it is useful to divide the entire equation by the total volume v thus eq 6 provides a geometric state relationship that describes a change in volume of the non wetting phase in terms of the invariant properties of the boundaries of a phase surface area an mean curvature jn and euler characteristic χn the state variables can also be non dimensionalized i e a n j n χ n using the sauter mean diameter d 6ϕ ans aws as a reference length scale where ans and aws are the specific interfacial areas between the non wetting solid surfaces and the wetting solid surfaces respectively mcclure et al 2018 non dimensionalization is important for spline approximation of constitutive relationships section 2 5 5 because the state variables will then tend to range between 0 1 thereby removing any weighting factors that would affect minimization of the residuals between the observed data and the spline fit mcclure et al utilized a modified form of eq 6 to predict the mean curvature of the system j n w s w a n w χ n using geometric measures available in thermodynamically constrained averaging theory tcat models we will analyze the same relationship including lower dimensional variations relationships more closely represented by eq 6 s w a n w j n w χ n and v n a n w j n w χ n will also be considered as it is hypothesized that these relationships can more accurately predict the state of the system the v n a n w j n w χ n relationship should provide the best description of the system because vn is not specific to a porous medium unlike saturation which is based on porosity 2 5 5 estimating hysteresis with generalized additive models gams the hysteresis of a constitutive relationship is evaluated using generalized additive models gams to fit locally smooth spline surfaces to the constitutive relationships and evaluating the associated error wood and shaw 2015 mcclure et al 2016 mcclure et al 2018 which is attributed to the degree of hysteresis associated with the relationship two different error measures are evaluated for each gam to assess the uniqueness of each model the generalized cross validation gcv value represents the error of the gam in predicting each data point if it was not included in the construction of the gam golub et al 1979 therefore as gcv approaches zero the gam predicts the data better the coefficient of determination r2 measures the proportion of variance in the data that is predicted by the gam where a value of 1 0 would indicate a perfect fit gcv and r2 values were calculated for both the quasi and non equilibrium data sets as well as for all the data combined to determine whether the constitutive relationships are unique with respect to fluid flow condition 3 results and discussion we will first quantify the effect of the inclusion of additional state variables in constitutive relationships on gam fitting for the quasi and non equilibrium flow experiments separately secondly we will evaluate whether a single constitutive relationship can uniquely describe two phase flow independent of flow condition 3 1 state variable effect on hysteresis quasi equilibrium flow first the uniqueness of the two variable constitutive relationships of state variables j n w a n w χ n as a function of saturation for the quasi equilibrium case will be assessed three and four variable relationships will also be evaluated to determine whether including additional state variables will better predict the state of the system for quasi equilibrium flow the conventional hysteretic pc sw relationship is observed for the quasi equilibrium flow experiments fig 2 a when the system is allowed to relax towards equilibrium the capillary pressures tend towards the center of the area bounded by the main imbibition mi and main drainage md data schlüter et al 2017 observed similar trends when pressures decreased toward the quasi equilibrium imbibition curve after fast drainage when flow is reinitiated following a relaxation point the capillary pressure rapidly returns to its value before relaxation began this hysteretic behavior is significantly reduced in the anw sw relationship with imbibition located only slightly above drainage fig 2b interestingly the reverse has been observed in many glass bead pack oil water porter et al 2010 schlüter et al 2016 and air water culligan et al 2004 experiments yet many pore scale models e g r j held and celia 2001 joekar niasar et al 2008 of glass bead systems predict that imbibition should fall above drainage corroborating observations reported here it is unclear what is causing this discrepancy in fairly similar glass bead systems the χn sw relationship is also highly hysteretic fig 2c due to different fluid displacement mechanisms controlling drainage and imbibiton schlüter et al 2016 presented the concept that the χ n s n relationship follows a power law relation of the form 7 χ n p s n i ϵ where χ n is the euler characteristic normalized to the χn at 100 non wetting phase saturation p and ε are free parameters and i is the characteristic slope of the relationship they hypothesized that the characteristic slope is indicative of the fluid displacement mechanism of the system the normalized euler characteristic is plotted as a function of non wetting saturation in fig 3 a with each drainage and imbibition curve fitted to eq 7 the characteristic slopes of both drainage and imbibition fall within the same ranges presented by schlüter et al 2016 for a variety of datasets in different porous media and with different fluid pairs interestingly when χ n is plotted against trapped wetting saturation sw t there is a nearly linear dependence with very little separation of the drainage and imbibition curves fig 3b this absence of hysteresis was also observed in oil water data schlüter et al 2016 and is most likely attributable to the causal relation between the production of trapped wetting pendular rings and the invasion of non wetting phase the uniqueness of these two variable relationships are quantified by fitting gams to them and determining the error associated with these fits fig 4 as a reminder curvature j n w is used rather than capillary pressure to keep non dimensionalization of the state variables consistent the j n w s w relationship exhibits large error values with a gcv of 0 15 and r2 of 0 14 which is indicative of significant hysteresis in the relationship as observed in fig 2a also unsurprisingly the χ n s n relationship has large error values gcv 0 0023 and r2 0 62 due to the difference in displacement mechanisms between drainage and imbibition the a n w s w relationship exhibits a much better fit with a low gcv value 0 00032 and high r2 value 0 91 indicating that hysteresis is significantly reduced interestingly the χ n s w t relationship offers a statistically quite unique solution gcv 8 2 10 5 r2 0 99 this simple relationship could be extremely powerful especially for basic experimental and numerical studies because measuring saturation and euler characteristic a function of the location of saturation is much less demanding than measuring pc and anw which require isosurface generation similarly to the two variable constitutive relationships hysteresis in the three variable relationships was investigated by measuring the error values of the gams fits fig 4 in order to assess whether the inclusion of another state variable would result in a less hysteretic relationship unlike the two variable relationships no combination of the state variables j n w s w a n w j n w s w χ n a n d χ n s w a n w stands out in terms of a significantly greater reduction of hysteresis 0 00056 gcv 0 022 and 0 89 r2 0 92 this suggests that each state variable is independent and each combination provides comparable information to describe the system although there is an overall improvement in the ability to predict the system over the two variable relationships it is difficult to determine whether the measured errors in the relationships are real or could possibly be explained by the relatively limited amount of scanning curve data available to fill in the area between the main drainage and imbibition curves this is especially true for the drainage surfaces where only one scanning curve dataset was collected for each experiment but similar results were reported for large data simulations of a glass bead pack by mcclure et al 2018 with a r2 value of 0 93 for the j n w s w a n w relationship godinez brizuela et al 2017 also reported a reduction of hysteresis in micromodels for the pc sw anw relationship finally gams were fitted to the constitutive relationship between all state variables in order to verify that a higher dimensional relationship would provide a unique prediction of the system the j n w s w a n w χ n relationship predicts the state of the system better than the three variable relationships with a gcv value of 0 0043 and a r2 value of 0 98 it is expected that errors associated with measuring interfacial area curvature li et al 2018 and euler characteristic armstrong et al 2018 of experimental µct data contributes to the residual variance in the relationship but this result suggests that the j n w s w a n w χ n relationship is unique for the quasi equilibrium data a finding corroborated by mcclure et al 2018 as hypothesized the s w a n w j n w χ n and v n a n w j n w χ n relationships provide even better fits with the latter being the most accurate gcv 1 5 10 5 and r2 0 998 3 2 state variable effect on hysteresis non equilibrium flow we observed similar results in the two variable relationships for non equilibrium flow as for quasi equilibrium flow for ease of comparison the non equilibrium data is illustrated along with the quasi equilibrium data in fig 2 and in fig 4 for the gam fit errors the non equilibrium j n w s w relationship exhibits the largest magnitude of hysteresis gcv 0 12 r2 0 26 the relaxation points of the quasi equilibrium pc sw relationship tend to be bounded by the non equilibrium relationship fig 2a which is expected due to the earlier observation that capillary pressures tend towards the center of the main drainage imbibition loop when fluids relax towards equilibrium the quasi equilibrium anw sw relationship on the other hand is not bounded by the non equilibrium relationship fig 2b both experiments have a peak anw value around sw 0 24 but the non equilibrium interfacial area generation is much greater than the quasi equilibrium schlüter et al 2017 reported a reduction of interfacial area during interface relaxation following a fast drainage experiment it follows that if the interfaces are allowed to relax multiple times during a quasi equilibrium flow experiment the quasi equilibrium experiment would inherently produce less interfacial area and the anw sw relationship would fall below the non equilibrium experiment as observed here significantly more disconnected interfaces are generated in the non equilibrium experiment than in the quasi equilibrium experiment supporting information figure s4 as suggested by wildenschild et al 2001 2005 when interfaces are not allowed to relax the system is forced further away from equilibrium and the surface energy of the system does not reduce to its minimum value at that saturation this increase in disconnected phase is also represented in the χ n s n relationship fig 3a the normalized euler number for the non equilibrium experiment is slightly more negative than the quasi equilibrium experiment at low non wetting phase saturations indicating a larger number of disconnected non wetting phase blobs the non equilibrium χ n s n relationship has relatively large error values gcv 0 0021 and r2 0 80 whereas hysteresis is significantly reduced in the a n w s w relationship gcv 0 00045 and r2 0 94 as with the quasi equilibrium case the χ n s w t relationship is quite unique with a gcv of 3 6 10 5 and an r2 of 0 98 the non equilibrium χ n s w t relationship has a shallower slope than the quasi equilibrium fig 3a which is attributable to the increased production of disconnected phases in the non equilibrium case similarly to the quasi equilibrium case no three variable combination of the state variables stands out in terms of significantly greater reduction of hysteresis than the others 0 00071 gcv 0 032 and 0 85 r2 0 94 specifically only 85 of variance in the j n w s w a n w relationship is explained this result contradicts transient studies including 3 dimensional dynamic pore network simulations joekar niasar and hassanizadeh 2012 and two dimensional micromodel data karadimitriou et al 2014 where the transient pc sw anw surface was found to be unique for a given capillary number within specific margins or error finally the four variable relationships were investigated for the non equilibrium flow experiment similar to the quasi equilibrium relationship the non equilibrium j n w s w a n w χ n relationship predicts the state of the system better than the three variable relationships with a gcv value of 0 009 and a r2 value of 0 95 but the relationship most representative of eq 6 v n a n w j n w χ n again provides the most accurate prediction of the data gcv 2 1 10 5 r2 0 998 it appears that the inclusion of the euler characteristic to the pc sw anw relationship is needed to produce the most unique relationship this is an important result as these relationships have not previously been explored in non equilibrium multi phase flow experiments although the non equilibrium and quasi equilibrium experiments exhibit many of the same trends in their state variable relationships there is still the question of whether a single constitutive relationship can describe two phase flow independent of flow condition 3 3 effect of flow condition on two phase flow constitutive relationships to determine how flow condition quasi equilibrium vs non equilibrium affects hysteresis in constitutive relationships the quasi and non equilibrium data were merged into a single dataset it follows that if flow condition does not have an effect on a constitutive relationship then the inclusion of all the data would not increase the magnitude of hysteresis the gcv and r2 values of the gam fits for this combined data set are illustrated in fig 5 the two variable relationships produced no surprises based on a cursory look at the comparison between the quasi and non equilibrium data in fig 2 for the pc sw and χn sn relationships the quasi and non equilibrium data overlap fairly well fig 2 and therefore the magnitude of hysteresis is not changed in the combined data with the gcv and r2 values falling between the quasi and non equilibrium values the a n w s w relationship however is highly dependent on flow condition as the error in the gam fit is dramatically increased when the quasi and non equilibrium data are combined also of note the χ n s w t relationship for the combined data explains 94 of the variance in the data compared to 99 and 98 for the quasi and non equilibrium experiments respectively this suggests that the χ n s w t relationship is slightly dependent on flow condition similarly the combined three variable relationships are not unique with respect to flow condition the hysteresis in the j n w s w a n w relationship is notably increased with the combination of the quasi and non equilibrium data gcv 0 068 r2 0 61 in the supporting information figure s5 we include a visual comparison of this separation between the non and quasi equilibrium experiments with the pc s w anw relationship fitted to biquadratic equations as presented in previous works porter et al 2010 based on research using two dimensional micromodels godinez brizuela et al 2017 and karadimitriou et al 2014 similarly concluded that a single pc sw anw relationship does not exist independent of flow condition for a specific porous medium under transient conditions when the quasi and non equilibrium data sets are combined the error present in the four variable relationship j n w s w a n w χ n is increased in relation to the separate data sets with a gcv of 0 014 and a r2 of 0 93 although we cannot conclude that the j n w s w a n w χ n relationship provides a unique solution to the state of the system by including all the state variables hysteresis is drastically reduced the overall j n w s w a n w χ n relationship explains 93 of the variance for fluid configurations in the given porous medium that means that depending on the requirements in terms precision one could use this relationship to predict the state of the system independent of flow condition further non equilibrium experiments with higher capillary number flows would allow for expansion of the validity of this relationship it is also possible that the error associated with this relationship is partially due to the slight pore size distribution differences between the compared porous media fig 2d it follows that the v n a n w j n w χ n relationship should better predict the data as it is less dependent on the porous medium structure this is indeed the case as the v n a n w j n w χ n relationship has a gcv of 6 2 10 5 and r2 of 0 992 unlike the j n w s w a n w χ n relationship we can conclude that for the experimental conditions explored here the v n a n w j n w χ n relationship can uniquely predict the state of the system independent of flow condition 4 conclusions we present a comparison of quasi and non equilibrium two phase flow in a three dimensional porous medium using fast x ray microtomography the rapidly improving methodology of fast microtomography provides the unprecedented ability to track moving and evolving fluid interfaces in three dimensional pore space we have demonstrated with experimental data that the geometric state function j n w s w a n w χ n presented by mcclure et al 2018 uniquely predicts the mean curvature for both quasi and non equilibrium two phase flow this is not the case for many lower dimensional constitutive relationships which have been demonstrated to be hysteretic an interesting exception to this conclusion is the χ n s w t relationship which can uniquely predict the connectivity of a system based on the trapped wetting phase saturation this has significant applications in engineered multi phase flow situations for instance subsurface storage of co2 via capillary trapping yet it is important to acknowledge that while the theoretical value of this finding is significant when it comes to solving a practical field scale problem pc can be measured relatively easily at larger scales when combining the quasi and non equilibrium data the j n w s w a n w χ n relationship only explains 93 of the variance suggesting that it is slightly dependent on the flow condition of the system the v n a n w j n w χ n relationship on the other hand is independent of flow condition this relationship provides a better prediction of the data because it is less dependent on the porous medium and more closely represents the minkowski steiner formula which is the underlying basis of these relationships a few additional points that may need to be addressed in further studies include i non equilibrium flow experiments using higher capillary numbers would address whether these findings apply under more different non equilibrium conditions ii further studies in natural porous media would also be needed to further generalize the validity of the v n a n w j n w χ n relationship predicting the state of the system independent of flow condition it is unclear whether the measured error in the j n w s w a n w χ n relationships between the quasi and non equilibrium cases is merely a function of the smooth surfaces of sintered glass beads allowing for a higher degree of relaxation within the system iii interfacial area development is an important parameter describing mass transfer in multiphase flow operations e g groundwater remediation and therefore it is important to understand how relaxation affects interfacial area generation in a subsequent paper we are finding that the magnitude of interfacial area generation and the shape of the anw sw curves are dependent on the degree of relaxation during drainage and imbibition by better understanding how the extent of relaxation in the system affects interfacial area formation we will be able to develop more robust models and operational control of many industrial and environmental multiphase systems declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the u s national science foundation hydrologic sciences program under award 1344877 this work was performed at geosoilenvirocars the university of chicago sector 13 advanced photon source aps argonne national laboratory geosoilenvirocars is supported by the national science foundation earth sciences ear 1634415 and department of energy geosciences de fg02 94er14466 this research used resources of the advanced photon source a u s department of energy doe office of science user facility operated for the doe office of science by argonne national laboratory under contract no de ac02 06ch11357 supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103506 appendix supplementary materials image application 1 
516,empirical relationships that describe two phase flow in porous media have been largely hysteretic in nature thereby requiring different relationships depending on whether the system is undergoing drainage or imbibition recent studies have suggested using interfacial area to close the well known capillary pressure saturation relationship while others expand upon this by including the euler characteristic for a geometric description of the system with the advancement of fast x ray microtomography at synchrotron facilities three dimensional experiments of two phase quasi and non equilibrium flow experiments were conducted to quantify the uniqueness of constitutive relationships under different flow conditions we find that the state functions that include the euler characteristic provide the most unique prediction of the state of the system for both quasi and non equilibrium flow of these functions those that infer volume fraction from the other state variables are independent of flow condition quasi or non equilibrium this enhances the applicability of new constitutive relationships allowing for more robust models of two phase flow keywords two phase flow non equilibrium flow fast x ray microtomography interfacial area euler characteristic 1 introduction immiscible flow of two fluids in porous media occurs in many environmental and industrial applications some two phase flow processes in the environment include groundwater remediation enhanced oil recovery and geologic carbon sequestration whereas industrial applications include among others fluid separation in fuel cells the adsorption of liquids in absorbing materials and the drying of porous products such as paper pulp and building materials the societal economic and environmental impacts of these systems require well informed two phase flow theories and mathematical models to predict the underlying processes traditionally practitioners have relied on empirical relationships between macroscopic capillary pressure pc and wetting phase saturation sw to model two phase systems defining capillary pressure as 1 p n p w p c s w where pn is the average non wetting phase pressure for the system and pw is the average wetting phase pressure this relationship assumes that pc is solely a function of saturation while processes that affect fluid distribution such as porous medium wettability fluid fluid interfacial configuration and pore morphology are not accounted the pc sw relationship is well known to exhibit hysteretic behavior i e the relationship is process and history dependent and therefore separate functions are needed depending on whether the system is undergoing drainage or imbibition it has been proposed that a geometric description of the system needs to be included to uniquely describe two phase flow systems hassanizadeh and gray 1990 1993 suggested that specific interfacial area between the wetting and non wetting phases anw accounts for variations in fluid configurations that can be present in a porous medium they proposed that when taking into account interfacial area in the constitutive equations such that pc pc sw anw the drainage and imbibition pc sw anw relationships would fall on a unique surface i e the expanded relationship would be non hysteretic and a single relationship could describe the state of the system there has been a concerted effort to validate this theory both numerically and experimentally micromodel cheng et al 2004 chen et al 2007 pyrak nolte et al 2008 karadimitriou et al 2013 and microtomography imaging experiments porter et al 2010 have shown that the pc sw anw relationship is unique within experimental error under quasi equilibrium flow conditions similar results were obtained numerically via pore network modeling reeves and celia 1996 r j held and celia 2001 joekar niasar et al 2008 joekar niasar and hassanizadeh 2011 and a lattice boltzmann model porter et al 2009 however a lingering question is whether these observations hold true under non equilibrium flow conditions non equilibrium flow is a generic term that can cover a broad spectrum of flow in porous media most prominently the term is associated with dynamic or transient flow being effectively reserved for flow with capillary numbers outside of the capillary fingering regime in terms of the lenormand phase diagram lenormand et al 1988 an important facet of non equilibrium flow is the resulting absence of interfacial relaxation towards equilibrium which then affects macroscopic invariants such as saturation average capillary pressure and specific interfacial area gray et al 2015 schlüter et al 2017 in this research we use the term non equilibrium flow for flow experiments that do not allow for interfacial relaxation during drainage imbibition cycles rather than the terms dynamic or transient which connote higher capillary number flows some effort has been made to study the uniqueness of the pc sw anw relationship under transient vs quasi equilibrium conditions in micromodels karadimitriou et al 2014 godinez brizuela et al 2017 and using simulations via dynamic pore network modeling joekar niasar and hassanizadeh 2012 the dynamic pore network model found some differences between the transient and quasi equilibrium pc sw anw surfaces but determined that the differences were negligible the micromodel studies on the other hand showed that the transient and quasi equilibrium pc sw anw surfaces were statistically different to our knowledge no published research exists that compares the pc sw anw relationship under quasi and non equilibrium conditions for a fully three dimensional experimental system an assumption underlying the original theory behind the pc sw anw relationship hassanizadeh and gray 1990 is that each fluid phase is continuous but in most circumstances this condition is not met in two phase flow systems therefore other state variables have been investigated that can describe connectivity and shape of the fluids such as fluid topology herring et al 2013 mcclure et al 2016 schlüter et al 2016 the euler characteristic of the non wetting phase χn has been used as the standard to measure the connectedness of fluid configurations schlüter et al 2016 showed that the euler characteristic conveys complementary information that helps explain the hysteresis present in the anw sw relationship using lattice boltzmann model simulations mcclure et al further demonstrated that the inclusion of χn in a non dimensionalized pc sw anw relationship removes nearly all hysteresis under equilibrium mcclure et al 2016 2018 and dynamic mcclure et al 2018 conditions the ability to visualize non equilibrium multi phase flow in three dimensional porous media has not been possible until very recently with the improvements in imaging hardware detector speed and sensitivity along with increased x ray brilliance capabilities at synchrotron sources x ray computed microtomography µct has become a viable technology to meet this need berg et al 2013 and youssef et al 2013 were among the first to investigate two phase flow using fast µct berg et al determined that a time resolution between 10 and 30 s was sufficient to capture differences in fluid front configurations surrounding a haines jump during two phase flow youssef et al used the technique to determine that saturation of a sandstone followed a linear dependence on the square root of time thereby validating the washburn equation with real time three dimensional data since these initial forays into fast µct there has been a variety of studies looking at drainage and or imbibition processes of two phase flow armstrong et al 2014 leu et al 2014 andrew et al 2015 berg et al 2016 schlüter et al 2016 singh et al 2017 schlüter et al 2016 did collect three dimensional experimental data of a time resolved two phase flow experiment and determined that the pc sw anw relationship was unique in the supporting information however they used external pressure transducer readings rather than pc determined from internal interfacial curvature measurements to establish their pc sw anw curves scanning curves imbibitions and drainages that start at intermediate wetting saturations were not collected in the study resulting in an absence of data points that provide information about the space between the hysteretic main pc sw loop the objective of this study was to generate experimental three dimensional quasi and non equilibrium two phase datasets using fast microtomography to compare an array of state variable constitutive relationships and assess whether any are unique under either flow condition thus determining if a single relationship can describe the state of a system independent of flow condition 2 materials and methods 2 1 experimental setup soda lime glass beads 0 8 1 2 mm were randomly packed and sintered into a borosilicate glass column 6 mm diameter 26 mm height as a simplified porous medium for both two phase flow experiments the porosity ϕ for the quasi and non equilibrium experiments were 0 39 and 0 37 respectively the glass bead columns were contained in a sample holder which was mounted on the x ray µct rotation stage during flow experiments and image acquisition the bottom of the sample holder was connected to a harvard apparatus phd 2000 precision syringe pump controlling fluid volumes and flow rates the pump directly controlled the flow of the wetting phase water inducing the flow of the non wetting phase air the wetting phase was a 1 6 solution by mass of potassium iodide ki to deionized water ki is used as a contrast agent for better differentiation between the fluid phases in x ray images a hydrophilic nylon membrane 1 2 µm pore size compressed between two o rings was used at the bottom of the setup to seal the system and to reach low water saturation within the column a pressure transducer validyne p55 differential pressure transducer was inserted in the water line below the porous medium to track the differential pressure between the fluid phases for comparison to capillary pressures calculated from image analysis differential pressure was measured from the top of the porous medium as the pressure difference between the atmosphere and the bulk wetting phase the top of the sample was open to the atmosphere through a long 2 m tygon tube to prevent evaporation within the column during lengthy experiments 2 2 fast x ray computed microtomography all experiments were performed using synchrotron radiation at the 13 bmd beamline at the advanced photon source aps located at argonne national laboratory in lemont il the fast microtomography method at 13 bmd uses pink beam rather than the more standard monochromatic beam method pink beam is produced by filtering the low and high x ray energies from the full spectrum white beam that is produced by a synchrotron a 1 0 mm copper cu filter was used to filter out the low energy x rays the high energy x rays were removed by reflecting the beam from a platinum pt mirror angled at 1 5 mrad towards the experimental setup all scans were collected with a 12 bit cmos camera with a resolution of 1920 1200 pixels a nikon macro lens and a lutetium aluminum garnet scintillator the pixel size on the sample was 3 2 µm each scan consists of a 180 rotation of the sample with two dimensional radiographic projections taken at 900 angles the exposure time of each projection was 15 ms corresponding to an acquisition time of 13 5 s for a volumetric image ten flat field scans radiographs without the sample present were acquired before and after the scan for white field correction to unwind all fluid lines the sample was rotated back to 0 after each scan before another scan was initiated allowing for scan time flat field acquisitions and sample rotation the temporal resolution between each data point using pink beam was 50 s the x ray beam was turned off when images were not being acquired to reduce x ray exposure 2 3 two phase flow experiments 2 3 1 quasi equilibrium flow experiment the quasi equilibrium flow experiment was carried out using flow conditions similar to previous µct fluid flow experiments e g culligan et al 2004 porter et al 2010 but x ray images were acquired throughout the experiment rather than just at each quasi equilibrium point where the pump was stopped and the fluids were allowed to relax towards equilibrium before the wetting phase was introduced to the glass bead column a dry scan was acquired to facilitate easy separation of the solid phase from the fluid phases in the partially saturated images the dry porous medium was initially saturated with wetting phase at a rate of 5 ml hr fully saturating the field of view fov after saturation a full cycle of primary drainage pd main imbibition mi main drainage md and three scanning curves two starting on the mi curve and one on the md curve were executed at a flow rate of 0 2 ml hr the scanning curves si1 si2 sd1 were used to help fill in the area between the main imbibition drainage pc sw loop the capillary number ca for this flow rate was 2 10 7 which is indicative of a capillary dominated flow regime flow was stopped at different saturation points along each drainage and imbibition curve corresponding to a change of 10 in saturation where the fluids were allowed to relax towards hydrostatic equilibrium before flow was reinitiated scans were also acquired during this relaxation period to study how interfaces relax towards equilibrium although relaxation occurs over multiple hours gray et al 2015 schlüter et al 2017 the majority of saturation change occurs within the first few minutes to be able to compare with other quasi equilibrium experiments culligan et al 2004 porter et al 2010 each quasi equilibrium point was allowed about 15 min for relaxation fig 1 shows an example of how the fluid fluid interfaces retract during this relaxation period 2 3 2 non equilibrium flow experiment the non equilibrium flow experiment was performed in a similarly prepared glass bead column using the same flow rate ca 2 10 7 pd mi md and three scanning curves were collected with image acquisition occurring throughout the experiment the only difference from the quasi equilibrium flow experiments was that the pump was only stopped at the end of each drainage imbibition branch therefore the fluids did not relax during any of the drainage or imbibition experiments 2 4 image processing the x ray projections of each scan were reconstructed into a three dimensional image with the tomorecon software based on gridrec rivers 2012 although there is a ring artifact removal step in the reconstruction algorithm which can handle small artifacts associated with the detector some high frequency ring artifacts associated with pink beam imaging remain these large ring artifacts cause problems during image segmentation as phantom fluid phases are created near the ring artifacts leading to errors during state variable calculation these artifacts are attributable to defects in the pt mirror that is used to filter the high energy x rays details of the method used to remove high frequency ring artifacts is included in the supporting information figure s1 the images were denoised using a non local means filter buades et al 2005 this efficiently denoises the image while preserving image features such as fluid fluid interfaces which are important for accurate calculation of specific interfacial area and capillary pressure from image derived fluid curvatures all images were aligned to the dry scan acquired before the fluid flow experiment using automatic image registration the three phases wetting non wetting solid in each image where separated using a watershed segmentation method beucher and lantuejoul 1979 fig 1 shows a volume rendering of the segmented non wetting phase in the glass bead column during drainage from the segmented image three dimensional isosurfaces were generated using a marching cubes algorithm lorensen and cline 1987 between each phase so that specific interfacial area could be calculated from the images we adopted the surface smoothing approach constrained smoothing smoothing extent 3 of li et al 2018 for images with similar voxel resolution they found that these smoothing parameters preserved the original data while achieving the necessary level of smoothing of the interfaces ring artifact removal was performed using imagej but the remaining image processing steps were completed using avizo 2 5 data processing 2 5 1 porosity saturation and specific interfacial area porosity and saturation values were calculated by counting the number of voxels present for each phase in the segmented images the fluid phases were further separated into connected and disconnected volumes to distinguish the bulk phases for comparison with external pressure transducer measurements it was assumed that the wetting phase regions connected to the bottom of the fov and the non wetting phase regions connected to the top of the fov represented the bulk phases because the outlets of the column were outside of the fov it should be noted that these criteria do not guarantee that the identified regions are actually connected to the bulk phases especially at low saturations isosurfaces between the fluid phases were generated using a marching cubes algorithm in avizo and total fluid fluid interfacial area anw was calculated by measuring the total surface area of the triangles comprising those isosurfaces specific fluid fluid interfacial area anw was then calculated by dividing anw by the total volume a representative elementary volume rev analysis for a subset of saturation and specific interfacial area values are presented in the supporting information figure s2 the analysis shows that the parameters stabilize towards the total fov volume as reported in similar porous media studies e g porter et al 2010 schlüter et al 2016 2 5 2 mean curvature and capillary pressure average mean curvature jnw was calculated using a modified method of li et al 2018 this involves surface modification removal of triangles in close proximity to the solid surface various studies herring et al 2017 singh et al 2017 li et al 2018 have reported that surface modification was required to improve curvature estimation from microtomography images due to partial volume effects near the three phase contact line at the solid surface following the methodology of li et al a distance threshold dmin was set to 20 of the maximum distance between a point on the fluid fluid surface and the nearest point on a solid surface the mean curvature of each remaining triangle jl for every feature i was then weighted by its distance to the solid surface d l assigning more weight to triangles further away and less likely to be affected by the solid surface eq 2 pore scale capillary pressure p i c is calculated using the young laplace equation eq 3 where the interfacial tension between the fluid phases σ is 0 072 n m each feature can then be weighted by its interfacial area a i n w mcclure et al 2016 godinez brizuela et al 2017 to determine the macroscopic average capillary pressure of the system pc 2 j i l n j l d l l n d l for d l d m i n 3 p i c 2 j i σ 4 p c i n p i c a i n w i n a i n w in the supporting information figure s3 the average capillary pressure of the connected fluid phases is compared to the pressure differential measured from external pressure transducers this has previously been used as a basis for the verification of the curvature measurement li et al 2018 schlüter et al 2017 as the connected phases should equilibrate to the bulk fluid pressures and therefore be equal to the pressure differential measured by the external pressure transducers for the quasi equilibrium case although the capillary pressure of the boundary connected fluids has generally been used in previous quasi equilibrium experimental studies these pressures do not necessarily represent the system under non equilibrium flow conditions or more broadly under dynamic conditions for higher capillary numbers due to non equilibrium capillarity effects hassanizadeh et al 2002 these pressure measurements do not include disconnected fluids that contribute to measures of other state variables e g saturation which may suggest that they are measured in a fundamentally inconsistent way mcclure et al 2016 one cannot neglect the disconnected fluid regions without also neglecting the associated internal energy and any definition for the macroscopic average pressure must respect this in order to develop models that conserve energy this study instead uses the area weighted average curvature and capillary pressure of all fluid interfaces which is consistent with other studies testing two phase flow constitutive relationships e g joekar niasar and hassanizadeh 2012 mcclure et al 2016 godinez brizuela et al 2017 2 5 3 euler characteristic the euler characteristic χ is a topological invariant that describes the connectivity of a fluid phase it is described by the following relationship 5 χ β 0 β 1 β 2 where β0 is the number of distinct regions of the fluid phase β1 is the number of redundant connections within each distinct region and β2 is the number of encapsulated voids in each region the β2 value for the non wetting phase can be neglected in this study since an element of water or solid could not be completely suspended in air fundamentally the euler characteristic quantifies the entrapment of fluids and the breaking up of fluid connections through pore throats due to snap off this phenomenon has been established as an important source of hysteresis in multi phase flow r j held and celia 2001 joekar niasar et al 2013 schlüter et al 2016 which makes the euler characteristic an interesting state variable to consider in constitutive relationships schlüter et al 2016 mcclure et al 2018 2 5 4 constitutive state variable relationships the constitutive relationships we are studying are based on a generalized form of the minkowski steiner formula federer 1959 mcclure et al 2018 6 d v n c 1 a n d r c 2 j n d r 2 c 3 χ n d r 3 where ci are coefficients determined by the shape of the non wetting phase domain and dr is the radius of a small ball the state variables are extensive and therefore it is useful to divide the entire equation by the total volume v thus eq 6 provides a geometric state relationship that describes a change in volume of the non wetting phase in terms of the invariant properties of the boundaries of a phase surface area an mean curvature jn and euler characteristic χn the state variables can also be non dimensionalized i e a n j n χ n using the sauter mean diameter d 6ϕ ans aws as a reference length scale where ans and aws are the specific interfacial areas between the non wetting solid surfaces and the wetting solid surfaces respectively mcclure et al 2018 non dimensionalization is important for spline approximation of constitutive relationships section 2 5 5 because the state variables will then tend to range between 0 1 thereby removing any weighting factors that would affect minimization of the residuals between the observed data and the spline fit mcclure et al utilized a modified form of eq 6 to predict the mean curvature of the system j n w s w a n w χ n using geometric measures available in thermodynamically constrained averaging theory tcat models we will analyze the same relationship including lower dimensional variations relationships more closely represented by eq 6 s w a n w j n w χ n and v n a n w j n w χ n will also be considered as it is hypothesized that these relationships can more accurately predict the state of the system the v n a n w j n w χ n relationship should provide the best description of the system because vn is not specific to a porous medium unlike saturation which is based on porosity 2 5 5 estimating hysteresis with generalized additive models gams the hysteresis of a constitutive relationship is evaluated using generalized additive models gams to fit locally smooth spline surfaces to the constitutive relationships and evaluating the associated error wood and shaw 2015 mcclure et al 2016 mcclure et al 2018 which is attributed to the degree of hysteresis associated with the relationship two different error measures are evaluated for each gam to assess the uniqueness of each model the generalized cross validation gcv value represents the error of the gam in predicting each data point if it was not included in the construction of the gam golub et al 1979 therefore as gcv approaches zero the gam predicts the data better the coefficient of determination r2 measures the proportion of variance in the data that is predicted by the gam where a value of 1 0 would indicate a perfect fit gcv and r2 values were calculated for both the quasi and non equilibrium data sets as well as for all the data combined to determine whether the constitutive relationships are unique with respect to fluid flow condition 3 results and discussion we will first quantify the effect of the inclusion of additional state variables in constitutive relationships on gam fitting for the quasi and non equilibrium flow experiments separately secondly we will evaluate whether a single constitutive relationship can uniquely describe two phase flow independent of flow condition 3 1 state variable effect on hysteresis quasi equilibrium flow first the uniqueness of the two variable constitutive relationships of state variables j n w a n w χ n as a function of saturation for the quasi equilibrium case will be assessed three and four variable relationships will also be evaluated to determine whether including additional state variables will better predict the state of the system for quasi equilibrium flow the conventional hysteretic pc sw relationship is observed for the quasi equilibrium flow experiments fig 2 a when the system is allowed to relax towards equilibrium the capillary pressures tend towards the center of the area bounded by the main imbibition mi and main drainage md data schlüter et al 2017 observed similar trends when pressures decreased toward the quasi equilibrium imbibition curve after fast drainage when flow is reinitiated following a relaxation point the capillary pressure rapidly returns to its value before relaxation began this hysteretic behavior is significantly reduced in the anw sw relationship with imbibition located only slightly above drainage fig 2b interestingly the reverse has been observed in many glass bead pack oil water porter et al 2010 schlüter et al 2016 and air water culligan et al 2004 experiments yet many pore scale models e g r j held and celia 2001 joekar niasar et al 2008 of glass bead systems predict that imbibition should fall above drainage corroborating observations reported here it is unclear what is causing this discrepancy in fairly similar glass bead systems the χn sw relationship is also highly hysteretic fig 2c due to different fluid displacement mechanisms controlling drainage and imbibiton schlüter et al 2016 presented the concept that the χ n s n relationship follows a power law relation of the form 7 χ n p s n i ϵ where χ n is the euler characteristic normalized to the χn at 100 non wetting phase saturation p and ε are free parameters and i is the characteristic slope of the relationship they hypothesized that the characteristic slope is indicative of the fluid displacement mechanism of the system the normalized euler characteristic is plotted as a function of non wetting saturation in fig 3 a with each drainage and imbibition curve fitted to eq 7 the characteristic slopes of both drainage and imbibition fall within the same ranges presented by schlüter et al 2016 for a variety of datasets in different porous media and with different fluid pairs interestingly when χ n is plotted against trapped wetting saturation sw t there is a nearly linear dependence with very little separation of the drainage and imbibition curves fig 3b this absence of hysteresis was also observed in oil water data schlüter et al 2016 and is most likely attributable to the causal relation between the production of trapped wetting pendular rings and the invasion of non wetting phase the uniqueness of these two variable relationships are quantified by fitting gams to them and determining the error associated with these fits fig 4 as a reminder curvature j n w is used rather than capillary pressure to keep non dimensionalization of the state variables consistent the j n w s w relationship exhibits large error values with a gcv of 0 15 and r2 of 0 14 which is indicative of significant hysteresis in the relationship as observed in fig 2a also unsurprisingly the χ n s n relationship has large error values gcv 0 0023 and r2 0 62 due to the difference in displacement mechanisms between drainage and imbibition the a n w s w relationship exhibits a much better fit with a low gcv value 0 00032 and high r2 value 0 91 indicating that hysteresis is significantly reduced interestingly the χ n s w t relationship offers a statistically quite unique solution gcv 8 2 10 5 r2 0 99 this simple relationship could be extremely powerful especially for basic experimental and numerical studies because measuring saturation and euler characteristic a function of the location of saturation is much less demanding than measuring pc and anw which require isosurface generation similarly to the two variable constitutive relationships hysteresis in the three variable relationships was investigated by measuring the error values of the gams fits fig 4 in order to assess whether the inclusion of another state variable would result in a less hysteretic relationship unlike the two variable relationships no combination of the state variables j n w s w a n w j n w s w χ n a n d χ n s w a n w stands out in terms of a significantly greater reduction of hysteresis 0 00056 gcv 0 022 and 0 89 r2 0 92 this suggests that each state variable is independent and each combination provides comparable information to describe the system although there is an overall improvement in the ability to predict the system over the two variable relationships it is difficult to determine whether the measured errors in the relationships are real or could possibly be explained by the relatively limited amount of scanning curve data available to fill in the area between the main drainage and imbibition curves this is especially true for the drainage surfaces where only one scanning curve dataset was collected for each experiment but similar results were reported for large data simulations of a glass bead pack by mcclure et al 2018 with a r2 value of 0 93 for the j n w s w a n w relationship godinez brizuela et al 2017 also reported a reduction of hysteresis in micromodels for the pc sw anw relationship finally gams were fitted to the constitutive relationship between all state variables in order to verify that a higher dimensional relationship would provide a unique prediction of the system the j n w s w a n w χ n relationship predicts the state of the system better than the three variable relationships with a gcv value of 0 0043 and a r2 value of 0 98 it is expected that errors associated with measuring interfacial area curvature li et al 2018 and euler characteristic armstrong et al 2018 of experimental µct data contributes to the residual variance in the relationship but this result suggests that the j n w s w a n w χ n relationship is unique for the quasi equilibrium data a finding corroborated by mcclure et al 2018 as hypothesized the s w a n w j n w χ n and v n a n w j n w χ n relationships provide even better fits with the latter being the most accurate gcv 1 5 10 5 and r2 0 998 3 2 state variable effect on hysteresis non equilibrium flow we observed similar results in the two variable relationships for non equilibrium flow as for quasi equilibrium flow for ease of comparison the non equilibrium data is illustrated along with the quasi equilibrium data in fig 2 and in fig 4 for the gam fit errors the non equilibrium j n w s w relationship exhibits the largest magnitude of hysteresis gcv 0 12 r2 0 26 the relaxation points of the quasi equilibrium pc sw relationship tend to be bounded by the non equilibrium relationship fig 2a which is expected due to the earlier observation that capillary pressures tend towards the center of the main drainage imbibition loop when fluids relax towards equilibrium the quasi equilibrium anw sw relationship on the other hand is not bounded by the non equilibrium relationship fig 2b both experiments have a peak anw value around sw 0 24 but the non equilibrium interfacial area generation is much greater than the quasi equilibrium schlüter et al 2017 reported a reduction of interfacial area during interface relaxation following a fast drainage experiment it follows that if the interfaces are allowed to relax multiple times during a quasi equilibrium flow experiment the quasi equilibrium experiment would inherently produce less interfacial area and the anw sw relationship would fall below the non equilibrium experiment as observed here significantly more disconnected interfaces are generated in the non equilibrium experiment than in the quasi equilibrium experiment supporting information figure s4 as suggested by wildenschild et al 2001 2005 when interfaces are not allowed to relax the system is forced further away from equilibrium and the surface energy of the system does not reduce to its minimum value at that saturation this increase in disconnected phase is also represented in the χ n s n relationship fig 3a the normalized euler number for the non equilibrium experiment is slightly more negative than the quasi equilibrium experiment at low non wetting phase saturations indicating a larger number of disconnected non wetting phase blobs the non equilibrium χ n s n relationship has relatively large error values gcv 0 0021 and r2 0 80 whereas hysteresis is significantly reduced in the a n w s w relationship gcv 0 00045 and r2 0 94 as with the quasi equilibrium case the χ n s w t relationship is quite unique with a gcv of 3 6 10 5 and an r2 of 0 98 the non equilibrium χ n s w t relationship has a shallower slope than the quasi equilibrium fig 3a which is attributable to the increased production of disconnected phases in the non equilibrium case similarly to the quasi equilibrium case no three variable combination of the state variables stands out in terms of significantly greater reduction of hysteresis than the others 0 00071 gcv 0 032 and 0 85 r2 0 94 specifically only 85 of variance in the j n w s w a n w relationship is explained this result contradicts transient studies including 3 dimensional dynamic pore network simulations joekar niasar and hassanizadeh 2012 and two dimensional micromodel data karadimitriou et al 2014 where the transient pc sw anw surface was found to be unique for a given capillary number within specific margins or error finally the four variable relationships were investigated for the non equilibrium flow experiment similar to the quasi equilibrium relationship the non equilibrium j n w s w a n w χ n relationship predicts the state of the system better than the three variable relationships with a gcv value of 0 009 and a r2 value of 0 95 but the relationship most representative of eq 6 v n a n w j n w χ n again provides the most accurate prediction of the data gcv 2 1 10 5 r2 0 998 it appears that the inclusion of the euler characteristic to the pc sw anw relationship is needed to produce the most unique relationship this is an important result as these relationships have not previously been explored in non equilibrium multi phase flow experiments although the non equilibrium and quasi equilibrium experiments exhibit many of the same trends in their state variable relationships there is still the question of whether a single constitutive relationship can describe two phase flow independent of flow condition 3 3 effect of flow condition on two phase flow constitutive relationships to determine how flow condition quasi equilibrium vs non equilibrium affects hysteresis in constitutive relationships the quasi and non equilibrium data were merged into a single dataset it follows that if flow condition does not have an effect on a constitutive relationship then the inclusion of all the data would not increase the magnitude of hysteresis the gcv and r2 values of the gam fits for this combined data set are illustrated in fig 5 the two variable relationships produced no surprises based on a cursory look at the comparison between the quasi and non equilibrium data in fig 2 for the pc sw and χn sn relationships the quasi and non equilibrium data overlap fairly well fig 2 and therefore the magnitude of hysteresis is not changed in the combined data with the gcv and r2 values falling between the quasi and non equilibrium values the a n w s w relationship however is highly dependent on flow condition as the error in the gam fit is dramatically increased when the quasi and non equilibrium data are combined also of note the χ n s w t relationship for the combined data explains 94 of the variance in the data compared to 99 and 98 for the quasi and non equilibrium experiments respectively this suggests that the χ n s w t relationship is slightly dependent on flow condition similarly the combined three variable relationships are not unique with respect to flow condition the hysteresis in the j n w s w a n w relationship is notably increased with the combination of the quasi and non equilibrium data gcv 0 068 r2 0 61 in the supporting information figure s5 we include a visual comparison of this separation between the non and quasi equilibrium experiments with the pc s w anw relationship fitted to biquadratic equations as presented in previous works porter et al 2010 based on research using two dimensional micromodels godinez brizuela et al 2017 and karadimitriou et al 2014 similarly concluded that a single pc sw anw relationship does not exist independent of flow condition for a specific porous medium under transient conditions when the quasi and non equilibrium data sets are combined the error present in the four variable relationship j n w s w a n w χ n is increased in relation to the separate data sets with a gcv of 0 014 and a r2 of 0 93 although we cannot conclude that the j n w s w a n w χ n relationship provides a unique solution to the state of the system by including all the state variables hysteresis is drastically reduced the overall j n w s w a n w χ n relationship explains 93 of the variance for fluid configurations in the given porous medium that means that depending on the requirements in terms precision one could use this relationship to predict the state of the system independent of flow condition further non equilibrium experiments with higher capillary number flows would allow for expansion of the validity of this relationship it is also possible that the error associated with this relationship is partially due to the slight pore size distribution differences between the compared porous media fig 2d it follows that the v n a n w j n w χ n relationship should better predict the data as it is less dependent on the porous medium structure this is indeed the case as the v n a n w j n w χ n relationship has a gcv of 6 2 10 5 and r2 of 0 992 unlike the j n w s w a n w χ n relationship we can conclude that for the experimental conditions explored here the v n a n w j n w χ n relationship can uniquely predict the state of the system independent of flow condition 4 conclusions we present a comparison of quasi and non equilibrium two phase flow in a three dimensional porous medium using fast x ray microtomography the rapidly improving methodology of fast microtomography provides the unprecedented ability to track moving and evolving fluid interfaces in three dimensional pore space we have demonstrated with experimental data that the geometric state function j n w s w a n w χ n presented by mcclure et al 2018 uniquely predicts the mean curvature for both quasi and non equilibrium two phase flow this is not the case for many lower dimensional constitutive relationships which have been demonstrated to be hysteretic an interesting exception to this conclusion is the χ n s w t relationship which can uniquely predict the connectivity of a system based on the trapped wetting phase saturation this has significant applications in engineered multi phase flow situations for instance subsurface storage of co2 via capillary trapping yet it is important to acknowledge that while the theoretical value of this finding is significant when it comes to solving a practical field scale problem pc can be measured relatively easily at larger scales when combining the quasi and non equilibrium data the j n w s w a n w χ n relationship only explains 93 of the variance suggesting that it is slightly dependent on the flow condition of the system the v n a n w j n w χ n relationship on the other hand is independent of flow condition this relationship provides a better prediction of the data because it is less dependent on the porous medium and more closely represents the minkowski steiner formula which is the underlying basis of these relationships a few additional points that may need to be addressed in further studies include i non equilibrium flow experiments using higher capillary numbers would address whether these findings apply under more different non equilibrium conditions ii further studies in natural porous media would also be needed to further generalize the validity of the v n a n w j n w χ n relationship predicting the state of the system independent of flow condition it is unclear whether the measured error in the j n w s w a n w χ n relationships between the quasi and non equilibrium cases is merely a function of the smooth surfaces of sintered glass beads allowing for a higher degree of relaxation within the system iii interfacial area development is an important parameter describing mass transfer in multiphase flow operations e g groundwater remediation and therefore it is important to understand how relaxation affects interfacial area generation in a subsequent paper we are finding that the magnitude of interfacial area generation and the shape of the anw sw curves are dependent on the degree of relaxation during drainage and imbibition by better understanding how the extent of relaxation in the system affects interfacial area formation we will be able to develop more robust models and operational control of many industrial and environmental multiphase systems declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the u s national science foundation hydrologic sciences program under award 1344877 this work was performed at geosoilenvirocars the university of chicago sector 13 advanced photon source aps argonne national laboratory geosoilenvirocars is supported by the national science foundation earth sciences ear 1634415 and department of energy geosciences de fg02 94er14466 this research used resources of the advanced photon source a u s department of energy doe office of science user facility operated for the doe office of science by argonne national laboratory under contract no de ac02 06ch11357 supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103506 appendix supplementary materials image application 1 
517,this paper presents a 3 d non hydrostatic reynolds averaged navier stokes rans model using the volume of fluid vof approach to simulate dam break flows good agreement is observed between the 3 d model predictions and results of dam break experiments performed in the laboratory the 3 d model is then applied to predict flood wave propagation induced by the sudden failure of two flood protection dams in iowa usa results are also compared with predictions of 2 d hydrostatic depth averaged models the 2 d model simulations using the precalibrated values of the manning s coefficients underpredict the speed of propagation of the flood wave and the area inundated by the flood compared to the 3 d model predictions a methodology is presented to recalibrate the 2 d model which improves the agreement with the 3 d model predictions simulation results also show that strong 3 d effects are generated in regions of strong curvature of the river channel near sudden constrictions and obstacles and during the times the mean flow direction varies significantly over the flow depth such 3 d effects cannot be captured by the 2 d model even after recalibration pointing toward the need to use 3 d models for detailed flood mapping keywords floods dam break flows numerical simulations flood mitigation 1 introduction dam break flows are floods caused by the structural collapse of a dam they generate rapid increases in the water levels and a flood wave dam break flows are highly unsteady and the flood wave they generate can induce very large forces on all man made structures e g bridges buildings situated in their way and entrain large amounts of bed material molinari et al 2017 some examples include the malpasset dam failure in 1959 in france that caused approximately 70 million in losses and the samarco dam failure in 2015 in brazil that caused the death of 19 people and 5 billion in recovery work one possible way to determine the regions affected by the flood generated by catastrophic failure of a structure e g dam levee investigate the efficiency of flood protection measures and ultimately propose an effective strategy to manage flood hazard is via numerical simulations radice et al 2012 dam break flood modeling is commonly done using either one dimensional 1 d hybrid 1d 2d or 2 d hydrostatic models palu and julien 2019 these models solve the saint venant equations as such they strictly apply only for flows with no significant curvature of the free surface and for flows in which the pressure distribution is close to hydrostatic unfortunately these assumptions are often violated in dam break flows especially near the front of the flood wave and during the times when strong wave structure interactions occur 2d depth averaged models are routinely used to predict dam break flows in idealized conditions and in natural environments using finite differences finite volume and finite elements methods e g see hervouet and petitjean 1999 aureli et al 2000 macchione and morelli 2003 shige eda and akiyama 2003 schwanenberg and harms 2004 liang et al 2007 xia et al 2010 erpicum et al 2010 li et al 2013 cea and blade 2015 another problem is that 3 d effects are generally very important during the initial stages of the dam break flow when the pressure distribution right downstream of the dam breach is highly nonhydrostatic and the flow accelerations are large including in the vertical direction a common consequence reported in many studies is that 1 d and even 2 d hydrostatic models fail to predict the correct flood wave propagation see concerted action on dam break modeling final report cadam 2000 the error propagation in a 2 d simulation due to incorrect prediction of flood wave propagation near the dam is discussed by singh et al 2011 moreover 3 d effects are not always limited only to the initial stages of the dam break flow the presence of regions of strong channel curvature sudden variation in the bathymetry hydraulic structures e g bridges whose decks may become submerged during the passage of the flood wave smaller dams and weirs downstream of which the pressure distribution highly non hydrostatic and obstacles e g buildings induce strong complex 3 d effects that cannot be accurately captured by 2 d hydrostatic models this in turn directly affects the capability of such numerical models to predict the flood extent the peak flood levels and the flow depth at peak flood levels consequently when applied to develop strategies to mitigate flood hazards large safety factors need to be used to account for these possible shortcomings one possible way to alleviate these problems is the use of 2 d non hydrostatic models that partially account for non hydrostatic effects via parametrization of the dynamic pressure and vertical velocity distributions over the flow depth bristeau et al 2011 lu et al 2015 aricó and lo re 2016 still such models cannot fully account for complex 3 d effects and cannot be applied in regions where the flow becomes pressurized a way to overcome the aforementioned challenges to simulate dam break flows over natural terrain containing large scale man made structures is the use of methods that are not based on the shallow water equations the most obvious choice is the use of time accurate reynolds averaged navier stokes rans solvers with free surface tracking capabilities two of the most popular surface tracking algorithms used in 2 d and 3 d rans solvers are the level set method osher and sethian 1998 and the volume of fluid vof method hirt and nichols 1981 level set methods have been applied to dam break problems in simple geometries mostly in 2 d e g liang et al 2011 balabel 2015 the vof method is more popular for simulation of dam break flows 3 d non hydrostatic rans vof simulations of dam break problems were attempted especially for idealized test cases corresponding to laboratory scale experiments biscarini et al 2010 2016 yang et al 2010 ozmen cagatay and kocaman 2010 for example biscarini et al 2010 found that 3 d rans predictions of lab scale dam break flows were more accurate than those of 2 d models e g cche 2d for several test cases that included a dam break flow developing over a flat bed without friction a 2 d dam break flow developing in a channel containing a bottom attached triangular obstacle and a 3 d dam break flow developing in a 900 l shaped channel their study found that the 2 d model predicted lower propagation speeds for the front wave de maio 2004 attributed these differences to the three dimensionality of the flow field during the initial stages of the event these findings are fairly similar to those of yang et al 2010 who found that the 3 d model predicted more accurately the sharp increase of the water levels at the wave front compared to the 2 d model for a dam break flow developing over a non symmetrical floodplain their 3 d results also indicated that the pressure distribution was non hydrostatic in the near field this may explain the larger errors observed in the corresponding 2 d model simulation the application of fully 3 d rans models with a deformable free surface to predict dam break flows over natural terrain at field scale is even scarcer though less accurate than large eddy simulation les based models that resolve the large scale turbulent structures in the flow rodi et al 2013 non hydrostatic 3 d rans models should be able to more accurately capture the interaction of the flood wave with complex bathymetry this is important as such interactions can generate flow regions characterized by significant changes in the mean flow direction with the distance from the bed e g when the flow over the floodplain is not moving in the same direction as the flow inside the main river channel beneath the floodplain level vertical flow separation and strong secondary cross flow currents there is evidence that 3 d rans models can predict with reasonable accuracy flow in natural channels with and without hydraulic structures e g streamwise momentum and secondary flow in meandering channels with flat and naturally deformed bed and gravity driven flows generated between fluids with different densities e g lock exchange gravity currents compared to more sophisticated les based models an et al 2012 constantinescu et al 2011 rodi et al 2013 koken et al 2013 unsteady rans was also found to fairly accurately predict flow and temperature transport in natural channels containing dams haque et al 2007 these types of flows are relevant to the complex flow applications discussed in this paper to the best of our knowledge the only 3 d rans simulations of a dam break in a natural environment is discussed by biscarini et al 2016 who simulated the malpasset dam break flood event the main limiting factor for such simulations is the size of the domain the level of mesh refinement needed to obtain accurate solutions and the time over which the flood event needs to be simulated to be able to evaluate flood hazard the present paper builds on recent work reported by horna munoz and constantinescu 2018 who developed and validated a fully 3 d non hydrostatic rans vof model for prediction of flood related phenomena horna munoz and constantinescu 2018 discuss model validation for several simpler cases that tested the different features of the model and in particular its ability to capture the free surface position for steady and unsteady problems and the redistribution of the streamwise velocity due to strong channel curvature effects the model was previously applied to predict slowly developing flood events in natural environments but was not comprehensively tested for cases characterized by a rapidly developing flood event the first goal of the present paper is to show that the model can accurately predict dam break flows at laboratory scale simulation results are discussed for two complex test cases that were already used to evaluate the predictive capabilities of 2 d and 3 d dam break flow numerical models the second goal of the paper is to show that the 3 d model can be used to predict dam break flows at field scale to quantify differences with simulations performed using commonly used 2 d hydrostatic models and to understand the reasons for these differences 2 numerical model the numerical engine of the present 3 d model is the pressure poisson solver in star ccm the governing rans equations are 1 u i x i 0 2 u i t u i u k x k p x k 1 ρ x k μ μ t u i x k u k x i g δ i 3 where ui is the velocity component along the i direction ρ is the density of the fluid μ is the molecular dynamic viscosity μt is the eddy viscosity p is the pressure g is the gravity δij is kronecker symbol and i 3 corresponds to the vertical direction the realizable k ε turbulence model using the two layer formulation wilcox 1998 and constantinescu and patel 2000 is used to calculate μt the transport equations for the turbulent kinetic energy k and the turbulent dissipation rate ε are 3 k t k u j x j 1 ρ x j μ μ t σ k k x j f c g k ρ ε 4 ε t ε u j x j 1 ρ x j μ μ t σ ε ε x j f c c ε 1 s ε ρ ε k μ ε ρ c ε 2 ε where fc is a curvature correction factor gk is the production of turbulent kinetic energy term s is the rate of strain magnitude cε1 and cε2 are model constants wilcox 1998 close to the walls ε and μt are defined as functions of the wall distance while eq 4 is solved away from the walls the transport equation for k is solved everywhere in the domain more details are given in horna munoz and constantinescu 2018 the position of the deformable free surface is calculated using the vof method a layer of air is present in the upper part of the computational domain at all locations this layer shrinks as water advances over the bathymetry the advection of the volume fraction of water α is tracked by solving a pure advection equation α t u j α x j 0 in cells filled by water α 1 while in cells containing only air α 0 eq 1 4 and the advection equation for α are discretised over each control volume and then integrated in time the simple algorithm is used to integrate the navier stokes equations the advective terms in the momentum equations are discretised using a second order accurate upwind scheme while the diffusive and pressure gradient terms are discretised using the second order central differences scheme the implicit temporal discretization is also second order accurate the first order upwind scheme is used to discretize the convective terms in the transport equations for k and ε the viscous solver in starccm is parallelized using mpi and has shown good scalability on large pc clusters the rans solver was extensively used in our group to calculate steady and unsteady turbulent flows e g see cheng et al 2017 horna munoz and constantinescu 2018 wu et al 2019 a main advantage of star ccm is the very flexible and performant mesh generator that allows generating high quality meshes over highly deformed surfaces e g over irregular bathymetry topography without significant loss of resolution of the smaller scale features and then generating high quality 3 d grids with very different levels of mesh refinement in different parts of the computational domain local bathymetry vertical deformations larger than 3 cm are generally resolved by the mesh in the dam break flow simulations discussed in the present paper for this type of applications the level of mesh refinement is the largest in regions situated close to hydraulic structures and other bathymetry features or obstacles that generate flow separation or strong 3 d effects coarser meshes are used in the remaining parts of the river channels and in regions situated close to the water air interface during the propagation of the flood wave the outlet boundary was treated as a pressure outlet while the pressure and volume fraction of water were extrapolated from the interior cells a pressure outlet boundary condition with a zero volume fraction of water α 0 and a pressure of 0 pa were specified at the top boundary the velocity was set equal to zero on all solid surfaces in the dam break simulations of a laboratory experiment the solid boundaries were assumed to be smooth for dam break flow simulations in natural environments the terrain boundaries were assumed to be rough the roughness height inside the channels and over their floodplains was set equal to two times the mean sediment diameter in each region the implementation of the rough wall boundary condition in the rans solver is similar to that described in zeng et al 2008 3 dam break flows in simplified geometries this section discusses comparison of 3 d and 2 d model results with experiments for two text cases corresponding to dam break flows generated by the sudden release of a volume of water horna munoz and constantinescu 2018 show that the model accurately predicts the temporal evolution of the front surge position and the water column height for the canonical case of a 2 d dam break flow generated by the release of a rectangular column of water in a straight open channel martin and moyce 1952 the simulation conserved the total volume of water with an error of less than 2 during simulated time which is evidence that the vof module is sufficiently accurate for this type of simulations 3 1 dam break flow over a triangular obstacle in a straight channel the first test case considers a dam break flow advancing in a straight channel with a bottom attached triangular obstacle investigated experimentally by soares frazão 2002 2007 the height of the triangular obstacle fig 1 is 0 025 m and its base is 0 9 m long initially water is filled in the left reservoir depth and length of the water column are 0 111 m and 2 39 m respectively and to the right of the triangular obstacle up to a depth of 0 065 m at time t 0 s the water column in the left reservoir collapses despite the fact that the geometry is 2 d strong non hydrostatic effects are present as the flood wave reaches and moves over the triangular obstacle this is the main source for the differences between 2 d and 3 d model predictions for this test case the typical cell size and time step in the present 3 d model simulation conducted on a mesh with close to 6 million cells are about 50 lower than those used in the corresponding 3 d simulation of biscarini et al 2010 the simulation took about 4 hours to complete on a 4 core pc with 16 gb of ram memory per core the 3 d model of biscarini et al 2010 predicts lots of air entrainment downstream of the obstacle e g see frames at t 1 8 s 3 0 s and 3 7 s in fig 1 while air entrainment usually occurs when waves break or when a hydraulic jump occurs it cannot be clearly observed in the pictures taken during the experiment at the corresponding times the present 3 d model does not predict any pockets of air inside the bottom layer of water the same model more accurately captures the position of the free surface upstream of the crest of the obstacle compared to the 3 d model of biscarini et al 2010 this is especially evident in the frame comparing the solutions at t 3 0 s when the front of the backward propagating bore is situated at x 4 05 m in the experiment at x 4 1 m in the present 3 d model simulation and at x 4 25 m in the 3 d model simulation of biscarini et al 2010 at t 3 7 s a second reflected bore forms upstream of the crest of the obstacle while the main wave has hit the channel end wall and is travelling toward the downstream face of the obstacle the crests of the two reflected bores are situated at x 3 9 m and 4 05 m in the experiment at x 3 85 m and 4 1 m in the present 3 d model solution and at x 4 05 m and 4 2 m in the 3 d model solution of biscarini et al 2010 fig 2 compares the free surface elevation profiles from the experiment the present 3 d model predictions and the 3 d and 2 d model predictions of biscarini et al 2010 overall the accuracy of the two 3 d simulations is comparable at t 1 8 s the present 3 d model predicts a fairly flat free surface elevation immediately downstream of the obstacle which is in very good agreement with the experimental data of soares frazão 2002 both 3 d models predictions are overall more accurate than those obtained using a 2 d hydrostatic model by biscarini et al 2010 at t 3 s fig 2 the forward propagating wave has already passed the obstacle in the experiment and the 3 d simulations while the front of the wave moves over the crest of the obstacle and the reflected bore just forms in the 2 d solution no hydraulic jump forms immediately downstream of the obstacle as observed in the experiment and the 3 d simulations meanwhile a hydraulic jump forms at t 3 7 s fig 2 in the 2 d simulation but it is situated over the downstream face of the obstacle rather than downstream of it upstream of the obstacle the free surface elevation is overpredicted by the 2 d simulation the front of the reflected bore is situated close to x 3 7 m in the 2 d simulation which suggests the bore travelled too fast finally the 2 d model underpredicts the free surface position over the downstream face of the obstacle at t 8 4 s fig 2 after the dam break wave has reflected twice so despite the fact that the geometry of the test case is two dimensional 2 d model predictions are overall significantly less accurate than 3 d model predictions because of their limitations to account for strong non hydrostatic flow effects developing over and near the obstacle during the simulation time 3 2 dam break flow in a 90 l shaped channel the second test case considers a dam break flow advancing in a 90 l shaped channel corresponding to the experiment conducted by soares frazão and zech 2002 the l shaped channel is connected to a rectangular reservoir 0 33 m above the bottom surface of the reservoir fig 3 the reservoir is filled up to 0 2 m above the channel bottom the rectangular tank is initially filled up to a height of 0 53 m starting at t 0 s the water in the tank is released into the downstream rectangular channel which is initially dry the interaction of the flood wave with the outer wall of the l shaped channel induces strong backwater effects a strong secondary flow develops in the downstream part of the l shaped channel and 3 d reflections occurs as the flood wave changes direction the 3 d model simulation reported in this paper was conducted on a mesh containing about 9 million cells the mesh was refined near the solid surfaces the time step was 0 02 s the 9 million cell mesh simulation took about 20 h to complete on a 4 core pc with 16 gb of ram memory per core the outer bank free surface elevation profiles predicted by the 3 d and 2 d simulations are compared in fig 4 with experimental measurements the main wave reaches the end of the upstream part of the channel around t 2 s the predictions of the present 3 d model are in good agreement with the experimental data and their level of accuracy is comparable to that of the other 3 d simulation in particular this is the case in the critical region situated close to the sudden change in channel orientation where a backward propagating bore forms the 3 d model of biscarini et al 2010 seems to perform slightly better near the outlet of the domain x 8 5 m where the present 3 d model underpredicts the free surface elevation during the initial stages t 3 0 s fig 4 the 2 d model underestimates significantly the free surface elevation near the region where the flow changes direction compared to the experiment and the two 3 d simulations the peak water depth is underestimated by about 25 30 by the 2 d model compared to both 3 d models this is important as there are the times around which the largest flow depths are predicted in the channel a sharp front of the backwater bore was predicted around x 6 4 m by the 2 d model by contrast the 3 d models predicted a much more gradual increase of the free surface elevation close to the bore with the front of the bore penetrating until x 6 m at later times t 5 0 s and 7 0 s fig 4 these type of differences between the 2 d and the 3 d model simulations continue to be observed experimental data shows that the 3 d model predictions of the free surface elevation are more accurate than the 2 d model predictions especially around the front of the backward propagating bore the edge of the front of the bore is by about 0 4 m closer to the location where the channel changes orientation in the 2 d model simulation compared to the experiment and the two 3 d model simulations however the peak water depth at these times is fairly accurately captured by the 2 d model the underestimation of the free surface elevation by the 2 d model occurs mostly in the upstream part of the channel and is related to the smaller velocity of the backward propagating bore such problems are fairly common for 2 d shallow water flow models cadam 2000 the present 3 d model simulations of the two laboratory test cases were performed using the standard values of the model parameters e g turbulence model no calibration of any model parameter was performed the same is true for the complex domain simulations discussed in the next section in the case of 3 d models calibration is generally needed when the computational domain contains porous regions e g a layer or patches of dense vegetation at the bed where an empirical drag coefficient needs to be specified for the extra drag terms present in the governing equations data from highly resolved les simulations that resolve the individual solid elements e g plant stems can be used to estimate these drag coefficients zhou et al 2017 chang et al 2017 but generally further calibration is needed in computationally much less expensive 3 d models that use a drag force model to account for the effects of the solid elements 4 dam break flows developing over natural bathymetry the test cases discussed in this section simulate the sudden collapse of the coralville dam located upstream of iowa city iowa and of the saylorville dam located upstream of des moines iowa these are 3 d numerical experiments in which the dam break is assumed to occur very rapidly no validation data exists for these cases they serve to illustrate the capability of a fully 3 d model to be used for dam break predictions in natural environments containing large hydraulic structures e g dams something that to the best knowledge of the authors was not done before in both cases 3 d model results are compared with those of a calibrated 2 d depth averaged model that is routinely used for this type of applications to eliminate known deficiencies of 2 d models to predict dam break flows during the initial stages where vertical flow accelerations are high and pressure distribution is far from hydrostatic see discussion in cadam 2000 and biscarini et al 2010 the 2 d simulations are conducted in a domain that does not include the upstream part of the 3 d domain the flow hydrograph predicted by the 3 d simulation at the inlet section of the 2 d domain situated downstream of the location where the dam break occurs is used to specify the upstream boundary condition in the corresponding 2 d simulations thus differences between 2 d and 3 d model simulations are not due to 3 d effects generated during the formation of the flood wave but rather to 3 d effects induced by the interaction of the propagating wave with the bathymetry and hydraulic structures present away from the location where the dam break occurred 4 1 coralville dam break simulations 4 1 1 set up of 3 d and 2 d simulations the coralville dam is a flood protection dam located upstream of iowa city iowa usa fig 5 shows the location of the coralville dam with respect to iowa city together a close up view of the dam and its main features the 18 km river reach starting at the coralville dam contains a river dam 1st dam and an additional river dam 2nd dam close to south limit of iowa city the black arrow points at clear creek the main tributary feeding into the iowa river near iowa city the inclusion of clear creek is of great importance due to the significant backwater effect from the iowa river into clear creek during high flow conditions the domain was split in three main parts the coralville lake the transition part connecting the lake with the iowa river and the 18 km reach of the iowa river and its floodplain near iowa city fig 6 a the coralville lake was meshed with polyhedral cells the average cell size was around 100 m in the horizontal plane a minimum of 30 cells were used to resolve the flow in the vertical direction fig 6 the transition part was meshed with a much higher resolution in the horizontal directions the average cell size near the bed was 0 3125 m the cell size was increasing progressively to about 1 25 m close to the top of the computational domain fig 6b the horizontal mesh resolution in the 18 km reach of the iowa river was about 1 25 m inside the main channel and 20 m over the floodplain the total number of cells was around 18 million the highest water level ever recorded was 219 m on june 15 2008 during the peak of the 2008 flood in the 3 d model simulation the lake was initially filled up to an elevation of 219 m the flow in the 18 km reach of the iowa river was initialized assuming a steady state flowrate of 245 m3 s at the start of the simulation t 0 h the dam was removed assuming cfl 0 2 the time step was estimated to be 0 05 s the simulation was used to study the dam break flow up to t 5 h the simulation took about 1 month to complete on the titan hpc computer using 40 nodes and 4 gb of ram memory per core the srh 2d code lai 2008 2010 was used to perform the 2 d depth averaged simulations the code solves the full saint venant equations with a k ε turbulence model using a finite volume unstructured mesh approach the velocity water surface elevation coupling is achieved using a method similar to the simplec algorithm the code was extensively used for prediction of flood wave propagation over natural bathymetry the hydrograph predicted by the 3 d simulation in a section situated downstream of the dam was used to specify the inlet boundary condition for the 2 d model simulation this approach guarantees that the 2 d models predict the same flowrate as the 3 d model at the inlet of the 2 d domain during the simulation time setting up the 2 d simulation this way increased the chances the 2 d model to accurately predict the flood wave propagation in the regions situated far downstream of the dam where the main assumptions associated with the use of 2 d models should be better satisfied the 2 d model simulations were run on a pc with 4 cores and 16 gb of ram memory it took about 3 weeks to obtain each 2 d solution 4 1 2 comparison between the 3 d model and the precalibrated 2 d model simulations seven different values of the manning s coefficient 0 013 n 0 12 were used in the 2 d model to specify the roughness over the different parts of the computational domain depending on the land use these values were obtained based on preliminary calibration of the 2 d model this 2 d simulation is referred to as version 0 and is denoted 2 d model v0 fig 7 visualizes the flood wave and horizontal extent of the flooded regions predicted by the 3 d simulations at t 0 28 h 0 7 h and 3 22 h after the collapse of the dam t 0 h also shown are the positions of the 22 cross sections where the 2 d and 3 d solutions were compared as expected backwater effects are very strong in the clear creek stream that merges with the iowa river close to section 12 the regions situated around the clear creek stream are entirely flooded for t 1 h in the 3 d simulation fig 7c the flood wave reaches the downstream end of the computational domain around t 3 h the flood wave predicted by the 2 d model v0 simulation progresses in a qualitatively similar way however the wave propagation speed is smaller in the 2 d model v0 simulation as can be observed from comparing the spatial extent of the flooded regions at t 0 7 h in fig 7a and b consistent with this finding the temporal evolution of the total flooded area in fig 8 a shows that the 3 d model simulation floods faster than the 2 d model v0 simulation most of these differences built over the first hour after the collapse of the dam when the predicted temporal rate of increase of the inundated area is 7 km2 h in the 3 d model simulation and 5 9 km2 h in the 2 d model v0 simulation this corresponds to a 15 underprediction by the 2 d model at later times the rate of increase of the flooded area is about the same in both solutions 3 2 3 3 km2 h the peak flood extent is reached at t 2 75 h in the 3 d model simulation and at t 3 25 h in the 2 d model v0 simulation at t 3 22 h the 2 d model v0 simulation predicts a larger flooded area figs 7 and 8 and free surface elevations and smaller flowrates fig 10 downstream of section 11 compared to the 3 d model the difference between the area of the flooded regions predicted by the two simulations ranges from 15 to 6 downstream of the 2nd dam the 3 d model and 2 d model v0 simulations predict very similar free surface profiles at peak flood extent along the centerline of the iowa river fig 9 the differences between the two predictions increase as one moves upstream of the 2nd dam and peak close to section 1 where the differences in the flow depths are close to 3 m comparison at the predicted hydrographs at selected cross sections fig 10 provides a new metric to evaluate the propagation speed of the flood wave the 3 d model predicts a faster increase of the flowrate with time compared to the 2 d model v0 until the peak flood levels are reached at each cross section the more downstream the cross section is situated the larger the difference between the times the peak flow rate is reached in the two simulations a first reason for the observed differences in figs 8 10 is due to 3 d effects that cannot be captured by a 2 d model comparison of the distributions of the unit discharge hu h is the local flow depth and u is the local depth averaged streamwise velocity with respect to the orientation of the cross section at different sections where flooding occurred shows that most of the flow is advected through the deepest parts of the cross section in the 2 d simulation consistent with manning s equation the capacity of depth averaged models to account for phenomena that tend to deviate the flow from following the deepest regions is quite limited given the inability of such models to account for secondary currents e g currents developing at the interface between the main channel and its floodplain or for the different directions of the flow in the main channel below the floodplain level and above the floodplain level this effect is especially strong in the high curvature regions of the iowa river channel e g between sections 11 and 14 local 3 d effects are also very strong as the flood wave arrives and the floodplains start being inundated in 2 d simulations regions of high unit discharge would tend to be situated over the deepest regions which align with the location of the main channel out of the 22 sections analyzed good agreement between the nondimensional unit discharge profiles at peak flood extent predicted by the two simulations was observed at 15 sections fig 11 shows the unit discharge profiles at peak flood extent at three cross sections where the level of agreement between the two simulations was rather poor and at one cross section where the level of agreement was very good also included in fig 11 are the transverse variations of the streamwise depth averaged velocity u at the same cross sections overall the regions where the unit discharge hu and velocity u values are close or are very different in the 3 d model and 2 d model v0 simulations are about the same section 9 is located right downstream from a bridge contraction section 8 in fig 12 in a region where the flow expands and 3 d effects are important at section 9 the 2 d model v0 simulation severely underpredicts the peak unit discharge in the main channel and overpredicts the flow over the two floodplains compared to the 3 d model the 2 d velocity distributions in fig 12e g shows that strong 3 d effects are present upstream of section 9 these 3 d effects generate recirculation regions not only in horizontal planes fig 12a and b but also in vertical streamwise cross sections fig 12f over large regions situated in between cross sections 8 and 9 the 3 d model predicts velocity distributions that are strongly non monotonic in the vertical direction and violate the 2 d model assumption of a close to logarithmic velocity profile in between the channel bottom and the free surface though both models predict the formation a horizontal recirculation region on the right side of the main channel downstream of cross section 8 the size of the recirculation region extending until section b is different in fig 12a and b this eddy is generated by the sudden increase in the bathymetry levels around section 8 where a bridge is present the two ridges present on the two floodplains force an increase of the discharge over the main channel at section 8 the velocity distribution in the contracted cross section 8 fig 12c is fairly consistent with the one expected in a region where 3 d effects are rather small as a result of the higher bed elevation on the right floodplain higher velocities are predicted in section 8 closer to the left bank section c in fig 12a and over the neighboring floodplain the velocity distribution at cross section 9 where the flow is expanding is very different compared to section 8 the small streamwise velocities predicted between a and b in section 9 fig 12d are due to the horizontal recirculation eddy the region of small streamwise velocity magnitude around section c is due to strong 3 d effects induced by the sudden acceleration of the flow past the crest of the ridge on the left floodplain fig 12a the effect of the ridge on the flow can be better understood by comparing the velocity magnitude profiles in the streamwise vertical sections b 1 c and c 1 in fig 12 at section b 1 the flow pattern is similar to the one expected in an open channel flow with variable bathymetry the flow patterns are very different at sections c and c 1 that cut through the ridge on the left floodplain the flow strongly accelerates first over the downslope of the ridge and then as it enters the main channel where the bathymetry levels continue to decay a hydraulic jump occurs associated with the jump is the formation of a vertical recirculation eddy at section c and a sudden raise in the free surface elevation around 8 in section c 1 this kind of 3 d effects and the associated horizontal depth averaged momentum redistribution cannot be correctly captured by a 2 d depth averaged model section 13 is located in the middle of a high curvature reach of the iowa river channel fig 7 the secondary flow leads to the redistribution of the unit discharge in the lateral direction this redistribution is not captured by the 2 d model v0 simulation which predicts that most of the discharge is conveyed over the main channel fig 11 despite large differences between the two models at section 13 the unit discharge distributions at section 14 fig 11 where the channel curvature is relatively low are in excellent agreement which suggest that 3 d channel curvature effects can be strong but also fairly local section 17 is located close to the 2nd river dam fig 5a most of the differences in the unit discharge predictions are observed over part of the left floodplain positive distances in fig 11 where several buildings are located the difference between the 3 d and 2 d model predictions of the free surface elevation at peak flow conditions is less than 0 4 m at section 17 fig 9 this suggests that the differences in the unit discharge predictions are mainly due to 3 d effects induced by the flow as it passes the dam and the buildings present upstream of section 17 as flow is advected on the sides of the buildings and over their top regions of high streamwise momentum are generated and the vertical velocity component becomes significant inside these regions the main modeling assumptions made in 2 d models are violated in general 2 d models predictions of the depth averaged velocity and unit discharge correlate more strongly with the local flow depth than 3 d model predictions because 3 d models can account for secondary flow and vertical transport that can move higher velocity flow originating in the main cannel over shallower regions 4 1 3 recalibration of 2 d model based on 3 d model simulation results at least partially the differences between the 3 d and 2 d models are due to the local values used in the 2 d model for the manning s coefficient varying manning s coefficient is the main way to calibrate 2 d models via an acceleration deceleration of the flood wave in regions where the value of the manning s coefficient is decreased increased next a simple way to improve the predictions of 2 d models based on 3 d model predictions is proposed via recalibration of the manning s coefficient values since the 2 d model v0 simulation underestimated the speed of the flood wave the manning s coefficient needs to be decreased over part or over the whole domain to improve the predictive capabilities of the 2 d model three additional 2 d model simulations denoted v1 v2 and v3 were performed in the 2 d model v1 simulation the manning coefficient in the main channel was decreased from 0 035 to 0 015 while its values over the other parts of the domain were left unchanged with respect to the 2d model v0 simulation in the 2 d model v2 simulation the manning coefficient was set equal to 0 015 over the whole domain to better understand the sensitivity of the 2 d model predictions to the spatial heterogeneity associated with using a different manning coefficient in different regions v0 simulation in the 2 d model v3 simulation the value of the manning coefficient in each of the sub regions defined in the precalibrated 2 d model was multiplied by 0 43 this coefficient was determined based on preliminary simulations that matched the speed of propagation of the flood wave in the main channel of the iowa river predicted by the 3 d model between the inlet section and the 1st dam given that the 2 d model v3 simulation resulted in the closest agreement with the 3 d model simulation most of the discussion focuses on comparison among the 2 d model v0 and v3 and the 3 d model simulations comparison of the flooded regions at t 0 7 h predicted by the 2 d model v0 fig 7b and v3 fig 7c simulations with the 3 d model simulation fig 7a shows that the flood wave advances faster in the 2 d model v3 simulation which results in closer agreement between the recalibrated 2 d model and the 3 d model both models predict the front of the flood wave to be situated close to section 14 at t 0 7 h and significant flooding to occur over the floodplain of the iowa river starting upstream of section 13 and ending around section 14 backwater effects on the clear creek tributary are also stronger at t 0 7 h in these two simulations compared to the 2 d model v0 predictions fig 7b compared to the 2 d model v0 simulation the 2 d model v3 predictions of the temporal evolution of the total flooded area are much closer to the 3 d model fig 8 all 2 d simulations predict a larger time to peak compared to the 3 d model the time averaged difference between the flooded area predicted by the 2 d and 3 d simulations decreases from about 6 for the v0 and v1 simulations to less than 2 for the v3 simulation in terms of the free surface elevation at peak flood extent the 2 d model v3 predictions are again the closest to the 3 d model results fig 9 the difference between the free surface elevation predicted by these two models upstream of the 1st dam are less than 1 3 m significantly less than differences of up to 2 5 m shown by the 2 d model v0 and v1 simulations the comparison of the flow hydrographs at sections 9 and 12 in fig 10 favours the 2 d model v3 simulation that predicts a much closer variation of the flowrate compared to the 3 d model during the first hour of the flood even if the 2 d model v3 simulation gives significantly closer predictions of the flow hydrographs and of the temporal evolution of the total flooded area to those of the 3 d simulation the improvement in the accuracy of the unit discharge profile predictions is only marginal fig 11 these results are not surprising because the differences between the 2 d model v0 and the 3 d model predictions can be mostly attributed to 3 d effects a recalibration of the 2 d model cannot address the fundamental limitations of 2 d depth averaged models to simulate flow in regions where strong 3 d flow effects are present 4 2 saylorville dam break simulations 4 2 1 set up of the 3 d and 2 d simulations saylorville dam is a flood protection dam located upstream of the city of des moines iowa fig 13 shows the location of the saylorville dam with respect to the city of des moines together with a close up view of the dam the length of the des moines river downstream of the saylorville dam included in the computational domain was 34 km two tributaries feeding into the des moines river were included in the model namely an 8 km long reach of the beaver creek and a 27 km long reach of the raccoon river the same type of meshing procedure employed for the coralville dam break simulation was used to generate the 3 d mesh inside the computational domain used to perform the 3 d simulation of the saylorville dam break flow the total number of computational cells was close to 40 million in the 3 d model simulation the lake was initially filled to an elevation of 272 m the areas situated downstream of the dam did not contain any water at the start of the simulation all the other boundary conditions and simulation set up procedures were identical to the ones used in the 3 d simulation of the coralville dam break flow the computation was run for 3 75 h it took about 1 2 months to complete on the titan hpc machine using 40 cores hec ras 2d brunner 2016 was used to perform the 2 d simulations because a calibrated model for this region was already available compared to the srh model hec ras uses a simplified turbulence viscosity approach in which a constant calibrated eddy viscosity mixing coefficient can be specified in different parts of the domain the boundary conditions used in the hec ras 2d simulations are of the same types as the ones used in the 2 d simulations of the coralville dam break flow a hydrograph was recorded based on the 3 d simulation results at a location situated immediately downstream of the dam this hydrograph was then used to specify the water elevation and discharge at the inlet boundary of the computational domain in the 2 d simulations 4 2 2 comparison between the 3 d model and 2 d model simulations the precalibrated 2 d model used 18 different values of the manning s coefficient 0 02 n 0 15 over the simulated region the value of the coefficient was a function of the land usage fig 14 shows the flooded regions generated by the dam break in the 3 d model and 2 d model v0 simulations just before the flood wave arrived at the racoon river t 1 11 h and at t 3 75 h when backwater effects are clearly visible along both tributaries and the front of the main flood wave has left the domain in the 3 d simulation also shown are the positions of the 16 cross sections where the 3 d and 2 d solutions are compared next as for the coralville dam break flow the flood wave predicted by the 2 d model v0 simulation performed using hec ras 2d is lagging behind the one predicted by the 3 d model at t 1 11 h the front of the flood wave in the des moines river is situated around section 11 in the 3 d model simulation and just downstream of section 9 in the 2 d model v0 simulation these differences continue to grow with time e g compare wave front position at t 3 75 h in fig 14 to try to improve the agreement between the 2 d model and the 3 d model predictions one additional 2 d simulation was performed in the 2 d model v3 simulation the values of the manning coefficient used in the 2 d model v0 simulation were multiplied by a constant the value of the constant was determined such that to generate a flood wave that propagates over the upstream part of the des moines river reach at about the same speed as in the 3 d model simulation though the 2 d model v3 simulation also underpredicts the distance travelled by the front wave at t 3 75 h the difference is less than 50 compared to the one observed for the 2 d model v0 simulation fig 14 moreover the lateral extent of the flooded region in between section 14 and the front of the flood wave is in much better agreement with the 3 d model predictions at t 3 75 h the total flooded area predicted by the 2 d model v0 simulation is at all times smaller than the 3 d model predictions the maximum underprediction of the flooded area is of the order of 20 for t 1 5 h fig 15 a by contrast the rates of increase of the inundated area are very close less than 6 difference in the 2 d model v3 simulation and the 3 d model simulation fig 15b over the first 15 km of the des moines river reach the difference between the free surface elevation predicted by the 2 d model v0 simulation and the 3 d model simulation is of the order of 1 m at t 3 75 h fig 16 which corresponds to about 10 of the flow depth inside the main channel these differences drop to only about 0 3 m for the 2 d model v3 simulation all three simulations predict similar hydrographs at section 2 fig 17 by section 6 situated about 15 km downstream of the inlet section the delay in the arrival of the flood wave is about 0 2 hr in the 2 d model v0 simulation the peak discharge rate is also smaller than those predicted by the other two simulations these differences continue to grow as one moves downstream moreover the peak discharge is underpredicted by about 20 25 at sections 8 and 12 by contrast the hydrographs predicted at sections 8 and 12 by the 2 d model v3 simulation are in very good agreement with the 3 d model predictions 5 summary and conclusions a 3 d model developed to predict flood propagation in rivers containing hydraulic structures e g dams bridges was extended and applied to predict propagation of flood waves generated by the sudden collapse of a dam this type of problems is characterized by sudden changes in the free surface elevation especially during the initial stages of the flow and are more challenging for 3 d non hydrostatic rans solvers that use volume of fluid type of techniques to track the deformable free surface simulations of a dam break flow interacting with a triangular bottom mounted obstacle and of a dam break flow propagating in an l shaped channel were performed to test the capability of the present 3 d model to accurately predict dam break flows in controlled laboratory environments and to compare with predictions obtained using other models overall the 3 d model predictions of these lab test cases were found to be closer to experimental results compared to those obtained using a 2 d model two realistic dam break scenarios were simulated and results were compared to those given by 2 d depth averaged models the 2 d simulations using the precalibrated values of the manning s coefficient determined based on slowly varying flood events underpredicted the speed of propagation of the flood wave with respect to the 3 d model predictions as a result the hydrographs predicted by the 2 d model at different cross sections located along the main stream showed a longer time to peak and the total flooded area was smaller compared to the 3 d simulation results until close to peak flood conditions these findings are consistent with those of other studies reported in the literature that found that 2 d models underestimated the speed of propagation of the flood wave e g see discussion in cadam 2000 biscarini et al 2010 the aforementioned differences between 2 d and 3 d model predictions occurred even if the 2 d simulations were set up in a domain starting immediately downstream of the dam that collapsed and the flow hydrograph at the 2 d domain inlet section was that predicted by the 3 d simulation this eliminated from the 2 d model a region where strong 3 d effects and large flow accelerations are present at the start of the dam break flow thus enhancing the chances of such models to accurately predict the further evolution of the dam break flow by contrast the computational domain in the 3 d simulations also contained the lake reservoir upstream of the dam using the 3 d results a methodology was proposed to recalibrate the manning coefficients such that the 2 d predictions can become closer to the 3 d simulation results being able to accurately predict the propagation of the flood wave inside the upstream part of the main stream reach translated into significantly closer predictions of the temporal variation of the total flooded area and of the flow hydrographs at most locations the time to peak predictions at different locations were also much closer to the 3 d model results while the 2 d model predictions of the unit discharge profiles were in many regions in good agreement with the 3 d model predictions significant differences were observed in several regions and these differences did not necessarily diminish for the 2 d simulation that used the recalibrated values of the manning s coefficient this is simply because these differences are mainly due to 3 d effects these effects are induced by strong non hydrostatic effects associated with large curvature of the main river channel sudden channel constrictions presence of hydraulic structures where flow is suddenly accelerated and where the flow can become supercritical e g dams 3 d effects were also found to be important in regions where significant differences between the mean flow direction inside the main channel up to the floodplain level and the mean flow direction over the floodplain level were generated in the 3 d simulation a recalibration of manning s coefficients cannot address any of the basic problems associated with an approach based on solving the depth averaged navier stokes equations for this reason 3 d models should be used for applications where detailed flood mapping is required to evaluate flood risk and flood hazard given that similar non hydrostatic effects and 3 d effects are generally present in realistic dam break cases the 3 d approach should have a better chance to more accurately predict the spatio temporal evolution of the flood wave in such cases especially in domains containing large scale obstructions e g buildings hydraulic dams sudden changes in bathymetry elevation in the vicinity of such obstructions the flow field is highly three dimensional pressure distribution is far from hydrostatic and the core of high streamwise velocities does not always extend up to the free surface in extreme cases flow separation can be generated in vertical planes in such regions most of the assumptions of the 2 d approach are severely violated which is not the case when a fully 3 d non hydrostatic solver is used another advantage of such 3 d models is that they can be used to obtain meaningful predictions for cases when the flow becomes pressurized over part of the domain during the propagation of the flood wave such regime changes occur especially when a flood wave propagates in urban environments containing buildings and large structures 2 d models cannot deal with such situations such a test case was discussed in the companion paper horna munoz and constantinescu 2018 of course the use of 3 d models for predictions of flood propagation in natural environments is computationally much more expensive compared even to complex non hydrostatic 2 d models especially for flooding events occurring over a long time period still the rapid growth in computational power and the development of faster converging algorithms that can be parallelized in an efficient way will make application of 3 d models for real flood problems more and more common and will allow the use of more sophisticated turbulence closures e g les that should result in a further increase of the accuracy of such simulations credit authorship contribution statement daniel horna munoz methodology software validation formal analysis investigation george constantinescu conceptualization methodology resources supervision writing review editing project administration funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to acknowledge the iowa flood center and prof w krajewski for supporting this study and providing guidance on many aspects of the reported research the authors would also like to acknowledge high performance computing support project dd 2015 geo117 from the oak ridge leadership computing facility olcf and in particular dr j wells from the national center for computational science at the oak ridge national laboratory as well as from the high performance computing center at the university of iowa 
517,this paper presents a 3 d non hydrostatic reynolds averaged navier stokes rans model using the volume of fluid vof approach to simulate dam break flows good agreement is observed between the 3 d model predictions and results of dam break experiments performed in the laboratory the 3 d model is then applied to predict flood wave propagation induced by the sudden failure of two flood protection dams in iowa usa results are also compared with predictions of 2 d hydrostatic depth averaged models the 2 d model simulations using the precalibrated values of the manning s coefficients underpredict the speed of propagation of the flood wave and the area inundated by the flood compared to the 3 d model predictions a methodology is presented to recalibrate the 2 d model which improves the agreement with the 3 d model predictions simulation results also show that strong 3 d effects are generated in regions of strong curvature of the river channel near sudden constrictions and obstacles and during the times the mean flow direction varies significantly over the flow depth such 3 d effects cannot be captured by the 2 d model even after recalibration pointing toward the need to use 3 d models for detailed flood mapping keywords floods dam break flows numerical simulations flood mitigation 1 introduction dam break flows are floods caused by the structural collapse of a dam they generate rapid increases in the water levels and a flood wave dam break flows are highly unsteady and the flood wave they generate can induce very large forces on all man made structures e g bridges buildings situated in their way and entrain large amounts of bed material molinari et al 2017 some examples include the malpasset dam failure in 1959 in france that caused approximately 70 million in losses and the samarco dam failure in 2015 in brazil that caused the death of 19 people and 5 billion in recovery work one possible way to determine the regions affected by the flood generated by catastrophic failure of a structure e g dam levee investigate the efficiency of flood protection measures and ultimately propose an effective strategy to manage flood hazard is via numerical simulations radice et al 2012 dam break flood modeling is commonly done using either one dimensional 1 d hybrid 1d 2d or 2 d hydrostatic models palu and julien 2019 these models solve the saint venant equations as such they strictly apply only for flows with no significant curvature of the free surface and for flows in which the pressure distribution is close to hydrostatic unfortunately these assumptions are often violated in dam break flows especially near the front of the flood wave and during the times when strong wave structure interactions occur 2d depth averaged models are routinely used to predict dam break flows in idealized conditions and in natural environments using finite differences finite volume and finite elements methods e g see hervouet and petitjean 1999 aureli et al 2000 macchione and morelli 2003 shige eda and akiyama 2003 schwanenberg and harms 2004 liang et al 2007 xia et al 2010 erpicum et al 2010 li et al 2013 cea and blade 2015 another problem is that 3 d effects are generally very important during the initial stages of the dam break flow when the pressure distribution right downstream of the dam breach is highly nonhydrostatic and the flow accelerations are large including in the vertical direction a common consequence reported in many studies is that 1 d and even 2 d hydrostatic models fail to predict the correct flood wave propagation see concerted action on dam break modeling final report cadam 2000 the error propagation in a 2 d simulation due to incorrect prediction of flood wave propagation near the dam is discussed by singh et al 2011 moreover 3 d effects are not always limited only to the initial stages of the dam break flow the presence of regions of strong channel curvature sudden variation in the bathymetry hydraulic structures e g bridges whose decks may become submerged during the passage of the flood wave smaller dams and weirs downstream of which the pressure distribution highly non hydrostatic and obstacles e g buildings induce strong complex 3 d effects that cannot be accurately captured by 2 d hydrostatic models this in turn directly affects the capability of such numerical models to predict the flood extent the peak flood levels and the flow depth at peak flood levels consequently when applied to develop strategies to mitigate flood hazards large safety factors need to be used to account for these possible shortcomings one possible way to alleviate these problems is the use of 2 d non hydrostatic models that partially account for non hydrostatic effects via parametrization of the dynamic pressure and vertical velocity distributions over the flow depth bristeau et al 2011 lu et al 2015 aricó and lo re 2016 still such models cannot fully account for complex 3 d effects and cannot be applied in regions where the flow becomes pressurized a way to overcome the aforementioned challenges to simulate dam break flows over natural terrain containing large scale man made structures is the use of methods that are not based on the shallow water equations the most obvious choice is the use of time accurate reynolds averaged navier stokes rans solvers with free surface tracking capabilities two of the most popular surface tracking algorithms used in 2 d and 3 d rans solvers are the level set method osher and sethian 1998 and the volume of fluid vof method hirt and nichols 1981 level set methods have been applied to dam break problems in simple geometries mostly in 2 d e g liang et al 2011 balabel 2015 the vof method is more popular for simulation of dam break flows 3 d non hydrostatic rans vof simulations of dam break problems were attempted especially for idealized test cases corresponding to laboratory scale experiments biscarini et al 2010 2016 yang et al 2010 ozmen cagatay and kocaman 2010 for example biscarini et al 2010 found that 3 d rans predictions of lab scale dam break flows were more accurate than those of 2 d models e g cche 2d for several test cases that included a dam break flow developing over a flat bed without friction a 2 d dam break flow developing in a channel containing a bottom attached triangular obstacle and a 3 d dam break flow developing in a 900 l shaped channel their study found that the 2 d model predicted lower propagation speeds for the front wave de maio 2004 attributed these differences to the three dimensionality of the flow field during the initial stages of the event these findings are fairly similar to those of yang et al 2010 who found that the 3 d model predicted more accurately the sharp increase of the water levels at the wave front compared to the 2 d model for a dam break flow developing over a non symmetrical floodplain their 3 d results also indicated that the pressure distribution was non hydrostatic in the near field this may explain the larger errors observed in the corresponding 2 d model simulation the application of fully 3 d rans models with a deformable free surface to predict dam break flows over natural terrain at field scale is even scarcer though less accurate than large eddy simulation les based models that resolve the large scale turbulent structures in the flow rodi et al 2013 non hydrostatic 3 d rans models should be able to more accurately capture the interaction of the flood wave with complex bathymetry this is important as such interactions can generate flow regions characterized by significant changes in the mean flow direction with the distance from the bed e g when the flow over the floodplain is not moving in the same direction as the flow inside the main river channel beneath the floodplain level vertical flow separation and strong secondary cross flow currents there is evidence that 3 d rans models can predict with reasonable accuracy flow in natural channels with and without hydraulic structures e g streamwise momentum and secondary flow in meandering channels with flat and naturally deformed bed and gravity driven flows generated between fluids with different densities e g lock exchange gravity currents compared to more sophisticated les based models an et al 2012 constantinescu et al 2011 rodi et al 2013 koken et al 2013 unsteady rans was also found to fairly accurately predict flow and temperature transport in natural channels containing dams haque et al 2007 these types of flows are relevant to the complex flow applications discussed in this paper to the best of our knowledge the only 3 d rans simulations of a dam break in a natural environment is discussed by biscarini et al 2016 who simulated the malpasset dam break flood event the main limiting factor for such simulations is the size of the domain the level of mesh refinement needed to obtain accurate solutions and the time over which the flood event needs to be simulated to be able to evaluate flood hazard the present paper builds on recent work reported by horna munoz and constantinescu 2018 who developed and validated a fully 3 d non hydrostatic rans vof model for prediction of flood related phenomena horna munoz and constantinescu 2018 discuss model validation for several simpler cases that tested the different features of the model and in particular its ability to capture the free surface position for steady and unsteady problems and the redistribution of the streamwise velocity due to strong channel curvature effects the model was previously applied to predict slowly developing flood events in natural environments but was not comprehensively tested for cases characterized by a rapidly developing flood event the first goal of the present paper is to show that the model can accurately predict dam break flows at laboratory scale simulation results are discussed for two complex test cases that were already used to evaluate the predictive capabilities of 2 d and 3 d dam break flow numerical models the second goal of the paper is to show that the 3 d model can be used to predict dam break flows at field scale to quantify differences with simulations performed using commonly used 2 d hydrostatic models and to understand the reasons for these differences 2 numerical model the numerical engine of the present 3 d model is the pressure poisson solver in star ccm the governing rans equations are 1 u i x i 0 2 u i t u i u k x k p x k 1 ρ x k μ μ t u i x k u k x i g δ i 3 where ui is the velocity component along the i direction ρ is the density of the fluid μ is the molecular dynamic viscosity μt is the eddy viscosity p is the pressure g is the gravity δij is kronecker symbol and i 3 corresponds to the vertical direction the realizable k ε turbulence model using the two layer formulation wilcox 1998 and constantinescu and patel 2000 is used to calculate μt the transport equations for the turbulent kinetic energy k and the turbulent dissipation rate ε are 3 k t k u j x j 1 ρ x j μ μ t σ k k x j f c g k ρ ε 4 ε t ε u j x j 1 ρ x j μ μ t σ ε ε x j f c c ε 1 s ε ρ ε k μ ε ρ c ε 2 ε where fc is a curvature correction factor gk is the production of turbulent kinetic energy term s is the rate of strain magnitude cε1 and cε2 are model constants wilcox 1998 close to the walls ε and μt are defined as functions of the wall distance while eq 4 is solved away from the walls the transport equation for k is solved everywhere in the domain more details are given in horna munoz and constantinescu 2018 the position of the deformable free surface is calculated using the vof method a layer of air is present in the upper part of the computational domain at all locations this layer shrinks as water advances over the bathymetry the advection of the volume fraction of water α is tracked by solving a pure advection equation α t u j α x j 0 in cells filled by water α 1 while in cells containing only air α 0 eq 1 4 and the advection equation for α are discretised over each control volume and then integrated in time the simple algorithm is used to integrate the navier stokes equations the advective terms in the momentum equations are discretised using a second order accurate upwind scheme while the diffusive and pressure gradient terms are discretised using the second order central differences scheme the implicit temporal discretization is also second order accurate the first order upwind scheme is used to discretize the convective terms in the transport equations for k and ε the viscous solver in starccm is parallelized using mpi and has shown good scalability on large pc clusters the rans solver was extensively used in our group to calculate steady and unsteady turbulent flows e g see cheng et al 2017 horna munoz and constantinescu 2018 wu et al 2019 a main advantage of star ccm is the very flexible and performant mesh generator that allows generating high quality meshes over highly deformed surfaces e g over irregular bathymetry topography without significant loss of resolution of the smaller scale features and then generating high quality 3 d grids with very different levels of mesh refinement in different parts of the computational domain local bathymetry vertical deformations larger than 3 cm are generally resolved by the mesh in the dam break flow simulations discussed in the present paper for this type of applications the level of mesh refinement is the largest in regions situated close to hydraulic structures and other bathymetry features or obstacles that generate flow separation or strong 3 d effects coarser meshes are used in the remaining parts of the river channels and in regions situated close to the water air interface during the propagation of the flood wave the outlet boundary was treated as a pressure outlet while the pressure and volume fraction of water were extrapolated from the interior cells a pressure outlet boundary condition with a zero volume fraction of water α 0 and a pressure of 0 pa were specified at the top boundary the velocity was set equal to zero on all solid surfaces in the dam break simulations of a laboratory experiment the solid boundaries were assumed to be smooth for dam break flow simulations in natural environments the terrain boundaries were assumed to be rough the roughness height inside the channels and over their floodplains was set equal to two times the mean sediment diameter in each region the implementation of the rough wall boundary condition in the rans solver is similar to that described in zeng et al 2008 3 dam break flows in simplified geometries this section discusses comparison of 3 d and 2 d model results with experiments for two text cases corresponding to dam break flows generated by the sudden release of a volume of water horna munoz and constantinescu 2018 show that the model accurately predicts the temporal evolution of the front surge position and the water column height for the canonical case of a 2 d dam break flow generated by the release of a rectangular column of water in a straight open channel martin and moyce 1952 the simulation conserved the total volume of water with an error of less than 2 during simulated time which is evidence that the vof module is sufficiently accurate for this type of simulations 3 1 dam break flow over a triangular obstacle in a straight channel the first test case considers a dam break flow advancing in a straight channel with a bottom attached triangular obstacle investigated experimentally by soares frazão 2002 2007 the height of the triangular obstacle fig 1 is 0 025 m and its base is 0 9 m long initially water is filled in the left reservoir depth and length of the water column are 0 111 m and 2 39 m respectively and to the right of the triangular obstacle up to a depth of 0 065 m at time t 0 s the water column in the left reservoir collapses despite the fact that the geometry is 2 d strong non hydrostatic effects are present as the flood wave reaches and moves over the triangular obstacle this is the main source for the differences between 2 d and 3 d model predictions for this test case the typical cell size and time step in the present 3 d model simulation conducted on a mesh with close to 6 million cells are about 50 lower than those used in the corresponding 3 d simulation of biscarini et al 2010 the simulation took about 4 hours to complete on a 4 core pc with 16 gb of ram memory per core the 3 d model of biscarini et al 2010 predicts lots of air entrainment downstream of the obstacle e g see frames at t 1 8 s 3 0 s and 3 7 s in fig 1 while air entrainment usually occurs when waves break or when a hydraulic jump occurs it cannot be clearly observed in the pictures taken during the experiment at the corresponding times the present 3 d model does not predict any pockets of air inside the bottom layer of water the same model more accurately captures the position of the free surface upstream of the crest of the obstacle compared to the 3 d model of biscarini et al 2010 this is especially evident in the frame comparing the solutions at t 3 0 s when the front of the backward propagating bore is situated at x 4 05 m in the experiment at x 4 1 m in the present 3 d model simulation and at x 4 25 m in the 3 d model simulation of biscarini et al 2010 at t 3 7 s a second reflected bore forms upstream of the crest of the obstacle while the main wave has hit the channel end wall and is travelling toward the downstream face of the obstacle the crests of the two reflected bores are situated at x 3 9 m and 4 05 m in the experiment at x 3 85 m and 4 1 m in the present 3 d model solution and at x 4 05 m and 4 2 m in the 3 d model solution of biscarini et al 2010 fig 2 compares the free surface elevation profiles from the experiment the present 3 d model predictions and the 3 d and 2 d model predictions of biscarini et al 2010 overall the accuracy of the two 3 d simulations is comparable at t 1 8 s the present 3 d model predicts a fairly flat free surface elevation immediately downstream of the obstacle which is in very good agreement with the experimental data of soares frazão 2002 both 3 d models predictions are overall more accurate than those obtained using a 2 d hydrostatic model by biscarini et al 2010 at t 3 s fig 2 the forward propagating wave has already passed the obstacle in the experiment and the 3 d simulations while the front of the wave moves over the crest of the obstacle and the reflected bore just forms in the 2 d solution no hydraulic jump forms immediately downstream of the obstacle as observed in the experiment and the 3 d simulations meanwhile a hydraulic jump forms at t 3 7 s fig 2 in the 2 d simulation but it is situated over the downstream face of the obstacle rather than downstream of it upstream of the obstacle the free surface elevation is overpredicted by the 2 d simulation the front of the reflected bore is situated close to x 3 7 m in the 2 d simulation which suggests the bore travelled too fast finally the 2 d model underpredicts the free surface position over the downstream face of the obstacle at t 8 4 s fig 2 after the dam break wave has reflected twice so despite the fact that the geometry of the test case is two dimensional 2 d model predictions are overall significantly less accurate than 3 d model predictions because of their limitations to account for strong non hydrostatic flow effects developing over and near the obstacle during the simulation time 3 2 dam break flow in a 90 l shaped channel the second test case considers a dam break flow advancing in a 90 l shaped channel corresponding to the experiment conducted by soares frazão and zech 2002 the l shaped channel is connected to a rectangular reservoir 0 33 m above the bottom surface of the reservoir fig 3 the reservoir is filled up to 0 2 m above the channel bottom the rectangular tank is initially filled up to a height of 0 53 m starting at t 0 s the water in the tank is released into the downstream rectangular channel which is initially dry the interaction of the flood wave with the outer wall of the l shaped channel induces strong backwater effects a strong secondary flow develops in the downstream part of the l shaped channel and 3 d reflections occurs as the flood wave changes direction the 3 d model simulation reported in this paper was conducted on a mesh containing about 9 million cells the mesh was refined near the solid surfaces the time step was 0 02 s the 9 million cell mesh simulation took about 20 h to complete on a 4 core pc with 16 gb of ram memory per core the outer bank free surface elevation profiles predicted by the 3 d and 2 d simulations are compared in fig 4 with experimental measurements the main wave reaches the end of the upstream part of the channel around t 2 s the predictions of the present 3 d model are in good agreement with the experimental data and their level of accuracy is comparable to that of the other 3 d simulation in particular this is the case in the critical region situated close to the sudden change in channel orientation where a backward propagating bore forms the 3 d model of biscarini et al 2010 seems to perform slightly better near the outlet of the domain x 8 5 m where the present 3 d model underpredicts the free surface elevation during the initial stages t 3 0 s fig 4 the 2 d model underestimates significantly the free surface elevation near the region where the flow changes direction compared to the experiment and the two 3 d simulations the peak water depth is underestimated by about 25 30 by the 2 d model compared to both 3 d models this is important as there are the times around which the largest flow depths are predicted in the channel a sharp front of the backwater bore was predicted around x 6 4 m by the 2 d model by contrast the 3 d models predicted a much more gradual increase of the free surface elevation close to the bore with the front of the bore penetrating until x 6 m at later times t 5 0 s and 7 0 s fig 4 these type of differences between the 2 d and the 3 d model simulations continue to be observed experimental data shows that the 3 d model predictions of the free surface elevation are more accurate than the 2 d model predictions especially around the front of the backward propagating bore the edge of the front of the bore is by about 0 4 m closer to the location where the channel changes orientation in the 2 d model simulation compared to the experiment and the two 3 d model simulations however the peak water depth at these times is fairly accurately captured by the 2 d model the underestimation of the free surface elevation by the 2 d model occurs mostly in the upstream part of the channel and is related to the smaller velocity of the backward propagating bore such problems are fairly common for 2 d shallow water flow models cadam 2000 the present 3 d model simulations of the two laboratory test cases were performed using the standard values of the model parameters e g turbulence model no calibration of any model parameter was performed the same is true for the complex domain simulations discussed in the next section in the case of 3 d models calibration is generally needed when the computational domain contains porous regions e g a layer or patches of dense vegetation at the bed where an empirical drag coefficient needs to be specified for the extra drag terms present in the governing equations data from highly resolved les simulations that resolve the individual solid elements e g plant stems can be used to estimate these drag coefficients zhou et al 2017 chang et al 2017 but generally further calibration is needed in computationally much less expensive 3 d models that use a drag force model to account for the effects of the solid elements 4 dam break flows developing over natural bathymetry the test cases discussed in this section simulate the sudden collapse of the coralville dam located upstream of iowa city iowa and of the saylorville dam located upstream of des moines iowa these are 3 d numerical experiments in which the dam break is assumed to occur very rapidly no validation data exists for these cases they serve to illustrate the capability of a fully 3 d model to be used for dam break predictions in natural environments containing large hydraulic structures e g dams something that to the best knowledge of the authors was not done before in both cases 3 d model results are compared with those of a calibrated 2 d depth averaged model that is routinely used for this type of applications to eliminate known deficiencies of 2 d models to predict dam break flows during the initial stages where vertical flow accelerations are high and pressure distribution is far from hydrostatic see discussion in cadam 2000 and biscarini et al 2010 the 2 d simulations are conducted in a domain that does not include the upstream part of the 3 d domain the flow hydrograph predicted by the 3 d simulation at the inlet section of the 2 d domain situated downstream of the location where the dam break occurs is used to specify the upstream boundary condition in the corresponding 2 d simulations thus differences between 2 d and 3 d model simulations are not due to 3 d effects generated during the formation of the flood wave but rather to 3 d effects induced by the interaction of the propagating wave with the bathymetry and hydraulic structures present away from the location where the dam break occurred 4 1 coralville dam break simulations 4 1 1 set up of 3 d and 2 d simulations the coralville dam is a flood protection dam located upstream of iowa city iowa usa fig 5 shows the location of the coralville dam with respect to iowa city together a close up view of the dam and its main features the 18 km river reach starting at the coralville dam contains a river dam 1st dam and an additional river dam 2nd dam close to south limit of iowa city the black arrow points at clear creek the main tributary feeding into the iowa river near iowa city the inclusion of clear creek is of great importance due to the significant backwater effect from the iowa river into clear creek during high flow conditions the domain was split in three main parts the coralville lake the transition part connecting the lake with the iowa river and the 18 km reach of the iowa river and its floodplain near iowa city fig 6 a the coralville lake was meshed with polyhedral cells the average cell size was around 100 m in the horizontal plane a minimum of 30 cells were used to resolve the flow in the vertical direction fig 6 the transition part was meshed with a much higher resolution in the horizontal directions the average cell size near the bed was 0 3125 m the cell size was increasing progressively to about 1 25 m close to the top of the computational domain fig 6b the horizontal mesh resolution in the 18 km reach of the iowa river was about 1 25 m inside the main channel and 20 m over the floodplain the total number of cells was around 18 million the highest water level ever recorded was 219 m on june 15 2008 during the peak of the 2008 flood in the 3 d model simulation the lake was initially filled up to an elevation of 219 m the flow in the 18 km reach of the iowa river was initialized assuming a steady state flowrate of 245 m3 s at the start of the simulation t 0 h the dam was removed assuming cfl 0 2 the time step was estimated to be 0 05 s the simulation was used to study the dam break flow up to t 5 h the simulation took about 1 month to complete on the titan hpc computer using 40 nodes and 4 gb of ram memory per core the srh 2d code lai 2008 2010 was used to perform the 2 d depth averaged simulations the code solves the full saint venant equations with a k ε turbulence model using a finite volume unstructured mesh approach the velocity water surface elevation coupling is achieved using a method similar to the simplec algorithm the code was extensively used for prediction of flood wave propagation over natural bathymetry the hydrograph predicted by the 3 d simulation in a section situated downstream of the dam was used to specify the inlet boundary condition for the 2 d model simulation this approach guarantees that the 2 d models predict the same flowrate as the 3 d model at the inlet of the 2 d domain during the simulation time setting up the 2 d simulation this way increased the chances the 2 d model to accurately predict the flood wave propagation in the regions situated far downstream of the dam where the main assumptions associated with the use of 2 d models should be better satisfied the 2 d model simulations were run on a pc with 4 cores and 16 gb of ram memory it took about 3 weeks to obtain each 2 d solution 4 1 2 comparison between the 3 d model and the precalibrated 2 d model simulations seven different values of the manning s coefficient 0 013 n 0 12 were used in the 2 d model to specify the roughness over the different parts of the computational domain depending on the land use these values were obtained based on preliminary calibration of the 2 d model this 2 d simulation is referred to as version 0 and is denoted 2 d model v0 fig 7 visualizes the flood wave and horizontal extent of the flooded regions predicted by the 3 d simulations at t 0 28 h 0 7 h and 3 22 h after the collapse of the dam t 0 h also shown are the positions of the 22 cross sections where the 2 d and 3 d solutions were compared as expected backwater effects are very strong in the clear creek stream that merges with the iowa river close to section 12 the regions situated around the clear creek stream are entirely flooded for t 1 h in the 3 d simulation fig 7c the flood wave reaches the downstream end of the computational domain around t 3 h the flood wave predicted by the 2 d model v0 simulation progresses in a qualitatively similar way however the wave propagation speed is smaller in the 2 d model v0 simulation as can be observed from comparing the spatial extent of the flooded regions at t 0 7 h in fig 7a and b consistent with this finding the temporal evolution of the total flooded area in fig 8 a shows that the 3 d model simulation floods faster than the 2 d model v0 simulation most of these differences built over the first hour after the collapse of the dam when the predicted temporal rate of increase of the inundated area is 7 km2 h in the 3 d model simulation and 5 9 km2 h in the 2 d model v0 simulation this corresponds to a 15 underprediction by the 2 d model at later times the rate of increase of the flooded area is about the same in both solutions 3 2 3 3 km2 h the peak flood extent is reached at t 2 75 h in the 3 d model simulation and at t 3 25 h in the 2 d model v0 simulation at t 3 22 h the 2 d model v0 simulation predicts a larger flooded area figs 7 and 8 and free surface elevations and smaller flowrates fig 10 downstream of section 11 compared to the 3 d model the difference between the area of the flooded regions predicted by the two simulations ranges from 15 to 6 downstream of the 2nd dam the 3 d model and 2 d model v0 simulations predict very similar free surface profiles at peak flood extent along the centerline of the iowa river fig 9 the differences between the two predictions increase as one moves upstream of the 2nd dam and peak close to section 1 where the differences in the flow depths are close to 3 m comparison at the predicted hydrographs at selected cross sections fig 10 provides a new metric to evaluate the propagation speed of the flood wave the 3 d model predicts a faster increase of the flowrate with time compared to the 2 d model v0 until the peak flood levels are reached at each cross section the more downstream the cross section is situated the larger the difference between the times the peak flow rate is reached in the two simulations a first reason for the observed differences in figs 8 10 is due to 3 d effects that cannot be captured by a 2 d model comparison of the distributions of the unit discharge hu h is the local flow depth and u is the local depth averaged streamwise velocity with respect to the orientation of the cross section at different sections where flooding occurred shows that most of the flow is advected through the deepest parts of the cross section in the 2 d simulation consistent with manning s equation the capacity of depth averaged models to account for phenomena that tend to deviate the flow from following the deepest regions is quite limited given the inability of such models to account for secondary currents e g currents developing at the interface between the main channel and its floodplain or for the different directions of the flow in the main channel below the floodplain level and above the floodplain level this effect is especially strong in the high curvature regions of the iowa river channel e g between sections 11 and 14 local 3 d effects are also very strong as the flood wave arrives and the floodplains start being inundated in 2 d simulations regions of high unit discharge would tend to be situated over the deepest regions which align with the location of the main channel out of the 22 sections analyzed good agreement between the nondimensional unit discharge profiles at peak flood extent predicted by the two simulations was observed at 15 sections fig 11 shows the unit discharge profiles at peak flood extent at three cross sections where the level of agreement between the two simulations was rather poor and at one cross section where the level of agreement was very good also included in fig 11 are the transverse variations of the streamwise depth averaged velocity u at the same cross sections overall the regions where the unit discharge hu and velocity u values are close or are very different in the 3 d model and 2 d model v0 simulations are about the same section 9 is located right downstream from a bridge contraction section 8 in fig 12 in a region where the flow expands and 3 d effects are important at section 9 the 2 d model v0 simulation severely underpredicts the peak unit discharge in the main channel and overpredicts the flow over the two floodplains compared to the 3 d model the 2 d velocity distributions in fig 12e g shows that strong 3 d effects are present upstream of section 9 these 3 d effects generate recirculation regions not only in horizontal planes fig 12a and b but also in vertical streamwise cross sections fig 12f over large regions situated in between cross sections 8 and 9 the 3 d model predicts velocity distributions that are strongly non monotonic in the vertical direction and violate the 2 d model assumption of a close to logarithmic velocity profile in between the channel bottom and the free surface though both models predict the formation a horizontal recirculation region on the right side of the main channel downstream of cross section 8 the size of the recirculation region extending until section b is different in fig 12a and b this eddy is generated by the sudden increase in the bathymetry levels around section 8 where a bridge is present the two ridges present on the two floodplains force an increase of the discharge over the main channel at section 8 the velocity distribution in the contracted cross section 8 fig 12c is fairly consistent with the one expected in a region where 3 d effects are rather small as a result of the higher bed elevation on the right floodplain higher velocities are predicted in section 8 closer to the left bank section c in fig 12a and over the neighboring floodplain the velocity distribution at cross section 9 where the flow is expanding is very different compared to section 8 the small streamwise velocities predicted between a and b in section 9 fig 12d are due to the horizontal recirculation eddy the region of small streamwise velocity magnitude around section c is due to strong 3 d effects induced by the sudden acceleration of the flow past the crest of the ridge on the left floodplain fig 12a the effect of the ridge on the flow can be better understood by comparing the velocity magnitude profiles in the streamwise vertical sections b 1 c and c 1 in fig 12 at section b 1 the flow pattern is similar to the one expected in an open channel flow with variable bathymetry the flow patterns are very different at sections c and c 1 that cut through the ridge on the left floodplain the flow strongly accelerates first over the downslope of the ridge and then as it enters the main channel where the bathymetry levels continue to decay a hydraulic jump occurs associated with the jump is the formation of a vertical recirculation eddy at section c and a sudden raise in the free surface elevation around 8 in section c 1 this kind of 3 d effects and the associated horizontal depth averaged momentum redistribution cannot be correctly captured by a 2 d depth averaged model section 13 is located in the middle of a high curvature reach of the iowa river channel fig 7 the secondary flow leads to the redistribution of the unit discharge in the lateral direction this redistribution is not captured by the 2 d model v0 simulation which predicts that most of the discharge is conveyed over the main channel fig 11 despite large differences between the two models at section 13 the unit discharge distributions at section 14 fig 11 where the channel curvature is relatively low are in excellent agreement which suggest that 3 d channel curvature effects can be strong but also fairly local section 17 is located close to the 2nd river dam fig 5a most of the differences in the unit discharge predictions are observed over part of the left floodplain positive distances in fig 11 where several buildings are located the difference between the 3 d and 2 d model predictions of the free surface elevation at peak flow conditions is less than 0 4 m at section 17 fig 9 this suggests that the differences in the unit discharge predictions are mainly due to 3 d effects induced by the flow as it passes the dam and the buildings present upstream of section 17 as flow is advected on the sides of the buildings and over their top regions of high streamwise momentum are generated and the vertical velocity component becomes significant inside these regions the main modeling assumptions made in 2 d models are violated in general 2 d models predictions of the depth averaged velocity and unit discharge correlate more strongly with the local flow depth than 3 d model predictions because 3 d models can account for secondary flow and vertical transport that can move higher velocity flow originating in the main cannel over shallower regions 4 1 3 recalibration of 2 d model based on 3 d model simulation results at least partially the differences between the 3 d and 2 d models are due to the local values used in the 2 d model for the manning s coefficient varying manning s coefficient is the main way to calibrate 2 d models via an acceleration deceleration of the flood wave in regions where the value of the manning s coefficient is decreased increased next a simple way to improve the predictions of 2 d models based on 3 d model predictions is proposed via recalibration of the manning s coefficient values since the 2 d model v0 simulation underestimated the speed of the flood wave the manning s coefficient needs to be decreased over part or over the whole domain to improve the predictive capabilities of the 2 d model three additional 2 d model simulations denoted v1 v2 and v3 were performed in the 2 d model v1 simulation the manning coefficient in the main channel was decreased from 0 035 to 0 015 while its values over the other parts of the domain were left unchanged with respect to the 2d model v0 simulation in the 2 d model v2 simulation the manning coefficient was set equal to 0 015 over the whole domain to better understand the sensitivity of the 2 d model predictions to the spatial heterogeneity associated with using a different manning coefficient in different regions v0 simulation in the 2 d model v3 simulation the value of the manning coefficient in each of the sub regions defined in the precalibrated 2 d model was multiplied by 0 43 this coefficient was determined based on preliminary simulations that matched the speed of propagation of the flood wave in the main channel of the iowa river predicted by the 3 d model between the inlet section and the 1st dam given that the 2 d model v3 simulation resulted in the closest agreement with the 3 d model simulation most of the discussion focuses on comparison among the 2 d model v0 and v3 and the 3 d model simulations comparison of the flooded regions at t 0 7 h predicted by the 2 d model v0 fig 7b and v3 fig 7c simulations with the 3 d model simulation fig 7a shows that the flood wave advances faster in the 2 d model v3 simulation which results in closer agreement between the recalibrated 2 d model and the 3 d model both models predict the front of the flood wave to be situated close to section 14 at t 0 7 h and significant flooding to occur over the floodplain of the iowa river starting upstream of section 13 and ending around section 14 backwater effects on the clear creek tributary are also stronger at t 0 7 h in these two simulations compared to the 2 d model v0 predictions fig 7b compared to the 2 d model v0 simulation the 2 d model v3 predictions of the temporal evolution of the total flooded area are much closer to the 3 d model fig 8 all 2 d simulations predict a larger time to peak compared to the 3 d model the time averaged difference between the flooded area predicted by the 2 d and 3 d simulations decreases from about 6 for the v0 and v1 simulations to less than 2 for the v3 simulation in terms of the free surface elevation at peak flood extent the 2 d model v3 predictions are again the closest to the 3 d model results fig 9 the difference between the free surface elevation predicted by these two models upstream of the 1st dam are less than 1 3 m significantly less than differences of up to 2 5 m shown by the 2 d model v0 and v1 simulations the comparison of the flow hydrographs at sections 9 and 12 in fig 10 favours the 2 d model v3 simulation that predicts a much closer variation of the flowrate compared to the 3 d model during the first hour of the flood even if the 2 d model v3 simulation gives significantly closer predictions of the flow hydrographs and of the temporal evolution of the total flooded area to those of the 3 d simulation the improvement in the accuracy of the unit discharge profile predictions is only marginal fig 11 these results are not surprising because the differences between the 2 d model v0 and the 3 d model predictions can be mostly attributed to 3 d effects a recalibration of the 2 d model cannot address the fundamental limitations of 2 d depth averaged models to simulate flow in regions where strong 3 d flow effects are present 4 2 saylorville dam break simulations 4 2 1 set up of the 3 d and 2 d simulations saylorville dam is a flood protection dam located upstream of the city of des moines iowa fig 13 shows the location of the saylorville dam with respect to the city of des moines together with a close up view of the dam the length of the des moines river downstream of the saylorville dam included in the computational domain was 34 km two tributaries feeding into the des moines river were included in the model namely an 8 km long reach of the beaver creek and a 27 km long reach of the raccoon river the same type of meshing procedure employed for the coralville dam break simulation was used to generate the 3 d mesh inside the computational domain used to perform the 3 d simulation of the saylorville dam break flow the total number of computational cells was close to 40 million in the 3 d model simulation the lake was initially filled to an elevation of 272 m the areas situated downstream of the dam did not contain any water at the start of the simulation all the other boundary conditions and simulation set up procedures were identical to the ones used in the 3 d simulation of the coralville dam break flow the computation was run for 3 75 h it took about 1 2 months to complete on the titan hpc machine using 40 cores hec ras 2d brunner 2016 was used to perform the 2 d simulations because a calibrated model for this region was already available compared to the srh model hec ras uses a simplified turbulence viscosity approach in which a constant calibrated eddy viscosity mixing coefficient can be specified in different parts of the domain the boundary conditions used in the hec ras 2d simulations are of the same types as the ones used in the 2 d simulations of the coralville dam break flow a hydrograph was recorded based on the 3 d simulation results at a location situated immediately downstream of the dam this hydrograph was then used to specify the water elevation and discharge at the inlet boundary of the computational domain in the 2 d simulations 4 2 2 comparison between the 3 d model and 2 d model simulations the precalibrated 2 d model used 18 different values of the manning s coefficient 0 02 n 0 15 over the simulated region the value of the coefficient was a function of the land usage fig 14 shows the flooded regions generated by the dam break in the 3 d model and 2 d model v0 simulations just before the flood wave arrived at the racoon river t 1 11 h and at t 3 75 h when backwater effects are clearly visible along both tributaries and the front of the main flood wave has left the domain in the 3 d simulation also shown are the positions of the 16 cross sections where the 3 d and 2 d solutions are compared next as for the coralville dam break flow the flood wave predicted by the 2 d model v0 simulation performed using hec ras 2d is lagging behind the one predicted by the 3 d model at t 1 11 h the front of the flood wave in the des moines river is situated around section 11 in the 3 d model simulation and just downstream of section 9 in the 2 d model v0 simulation these differences continue to grow with time e g compare wave front position at t 3 75 h in fig 14 to try to improve the agreement between the 2 d model and the 3 d model predictions one additional 2 d simulation was performed in the 2 d model v3 simulation the values of the manning coefficient used in the 2 d model v0 simulation were multiplied by a constant the value of the constant was determined such that to generate a flood wave that propagates over the upstream part of the des moines river reach at about the same speed as in the 3 d model simulation though the 2 d model v3 simulation also underpredicts the distance travelled by the front wave at t 3 75 h the difference is less than 50 compared to the one observed for the 2 d model v0 simulation fig 14 moreover the lateral extent of the flooded region in between section 14 and the front of the flood wave is in much better agreement with the 3 d model predictions at t 3 75 h the total flooded area predicted by the 2 d model v0 simulation is at all times smaller than the 3 d model predictions the maximum underprediction of the flooded area is of the order of 20 for t 1 5 h fig 15 a by contrast the rates of increase of the inundated area are very close less than 6 difference in the 2 d model v3 simulation and the 3 d model simulation fig 15b over the first 15 km of the des moines river reach the difference between the free surface elevation predicted by the 2 d model v0 simulation and the 3 d model simulation is of the order of 1 m at t 3 75 h fig 16 which corresponds to about 10 of the flow depth inside the main channel these differences drop to only about 0 3 m for the 2 d model v3 simulation all three simulations predict similar hydrographs at section 2 fig 17 by section 6 situated about 15 km downstream of the inlet section the delay in the arrival of the flood wave is about 0 2 hr in the 2 d model v0 simulation the peak discharge rate is also smaller than those predicted by the other two simulations these differences continue to grow as one moves downstream moreover the peak discharge is underpredicted by about 20 25 at sections 8 and 12 by contrast the hydrographs predicted at sections 8 and 12 by the 2 d model v3 simulation are in very good agreement with the 3 d model predictions 5 summary and conclusions a 3 d model developed to predict flood propagation in rivers containing hydraulic structures e g dams bridges was extended and applied to predict propagation of flood waves generated by the sudden collapse of a dam this type of problems is characterized by sudden changes in the free surface elevation especially during the initial stages of the flow and are more challenging for 3 d non hydrostatic rans solvers that use volume of fluid type of techniques to track the deformable free surface simulations of a dam break flow interacting with a triangular bottom mounted obstacle and of a dam break flow propagating in an l shaped channel were performed to test the capability of the present 3 d model to accurately predict dam break flows in controlled laboratory environments and to compare with predictions obtained using other models overall the 3 d model predictions of these lab test cases were found to be closer to experimental results compared to those obtained using a 2 d model two realistic dam break scenarios were simulated and results were compared to those given by 2 d depth averaged models the 2 d simulations using the precalibrated values of the manning s coefficient determined based on slowly varying flood events underpredicted the speed of propagation of the flood wave with respect to the 3 d model predictions as a result the hydrographs predicted by the 2 d model at different cross sections located along the main stream showed a longer time to peak and the total flooded area was smaller compared to the 3 d simulation results until close to peak flood conditions these findings are consistent with those of other studies reported in the literature that found that 2 d models underestimated the speed of propagation of the flood wave e g see discussion in cadam 2000 biscarini et al 2010 the aforementioned differences between 2 d and 3 d model predictions occurred even if the 2 d simulations were set up in a domain starting immediately downstream of the dam that collapsed and the flow hydrograph at the 2 d domain inlet section was that predicted by the 3 d simulation this eliminated from the 2 d model a region where strong 3 d effects and large flow accelerations are present at the start of the dam break flow thus enhancing the chances of such models to accurately predict the further evolution of the dam break flow by contrast the computational domain in the 3 d simulations also contained the lake reservoir upstream of the dam using the 3 d results a methodology was proposed to recalibrate the manning coefficients such that the 2 d predictions can become closer to the 3 d simulation results being able to accurately predict the propagation of the flood wave inside the upstream part of the main stream reach translated into significantly closer predictions of the temporal variation of the total flooded area and of the flow hydrographs at most locations the time to peak predictions at different locations were also much closer to the 3 d model results while the 2 d model predictions of the unit discharge profiles were in many regions in good agreement with the 3 d model predictions significant differences were observed in several regions and these differences did not necessarily diminish for the 2 d simulation that used the recalibrated values of the manning s coefficient this is simply because these differences are mainly due to 3 d effects these effects are induced by strong non hydrostatic effects associated with large curvature of the main river channel sudden channel constrictions presence of hydraulic structures where flow is suddenly accelerated and where the flow can become supercritical e g dams 3 d effects were also found to be important in regions where significant differences between the mean flow direction inside the main channel up to the floodplain level and the mean flow direction over the floodplain level were generated in the 3 d simulation a recalibration of manning s coefficients cannot address any of the basic problems associated with an approach based on solving the depth averaged navier stokes equations for this reason 3 d models should be used for applications where detailed flood mapping is required to evaluate flood risk and flood hazard given that similar non hydrostatic effects and 3 d effects are generally present in realistic dam break cases the 3 d approach should have a better chance to more accurately predict the spatio temporal evolution of the flood wave in such cases especially in domains containing large scale obstructions e g buildings hydraulic dams sudden changes in bathymetry elevation in the vicinity of such obstructions the flow field is highly three dimensional pressure distribution is far from hydrostatic and the core of high streamwise velocities does not always extend up to the free surface in extreme cases flow separation can be generated in vertical planes in such regions most of the assumptions of the 2 d approach are severely violated which is not the case when a fully 3 d non hydrostatic solver is used another advantage of such 3 d models is that they can be used to obtain meaningful predictions for cases when the flow becomes pressurized over part of the domain during the propagation of the flood wave such regime changes occur especially when a flood wave propagates in urban environments containing buildings and large structures 2 d models cannot deal with such situations such a test case was discussed in the companion paper horna munoz and constantinescu 2018 of course the use of 3 d models for predictions of flood propagation in natural environments is computationally much more expensive compared even to complex non hydrostatic 2 d models especially for flooding events occurring over a long time period still the rapid growth in computational power and the development of faster converging algorithms that can be parallelized in an efficient way will make application of 3 d models for real flood problems more and more common and will allow the use of more sophisticated turbulence closures e g les that should result in a further increase of the accuracy of such simulations credit authorship contribution statement daniel horna munoz methodology software validation formal analysis investigation george constantinescu conceptualization methodology resources supervision writing review editing project administration funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to acknowledge the iowa flood center and prof w krajewski for supporting this study and providing guidance on many aspects of the reported research the authors would also like to acknowledge high performance computing support project dd 2015 geo117 from the oak ridge leadership computing facility olcf and in particular dr j wells from the national center for computational science at the oak ridge national laboratory as well as from the high performance computing center at the university of iowa 
518,traditional random walk particle tracking pt models of advection and dispersion do not track entropy because particle masses remain constant however newer mass transfer particle tracking mtpt models have the ability to do so because masses of all compounds may change along trajectories additionally the probability mass functions pmf of these mtpt models may be compared to continuous solutions with probability density functions when a consistent definition of entropy or similarly the dilution index is constructed this definition reveals that every discretized numerical model incurs a computational entropy similar to akaike s 1974 1992 entropic penalty for larger numbers of adjustable parameters the computational complexity of a model e g number of nodes or particles adds to the entropy and as such must be penalized application of a new computational information criterion reveals that increased accuracy is not always justified relative to increased computational complexity the mtpt method can use a particle collision based kernel or an adaptive kernel derived from smoothed particle hydrodynamics sph the latter is more representative of a locally well mixed system i e one in which the dispersion tensor equally represents mixing and solute spreading while the former better represents the separate processes of mixing versus spreading we use computational means to demonstrate the fitness of each of these methods for simulating 1 d advective dispersive transport with uniform coefficients keywords particle methods entropy mixing dilution index computational penalty aic 1 introduction entropy is a fundamental property possessed by any random variable or process including a plume moving through natural media from a thermodynamic viewpoint an increase in entropy is an increase in mixing dilution for a conservative solute but entropy is also a measure of the information required to describe a system composed of random variables so that the entropy of a computer simulation of a plume can be readily quantified the entropy of a discrete random variable has been defined in a straightforward way by shannon 1948 unfortunately there is not a well defined counterpart for continuous random variables in fact a commonly used definition of the entropy of a continuous random variable rv can take on unphysical negative values as such the kullback leibler divergence or relative entropy which is a measure of the relative difference between two continuous rvs is often used instead kullback and leibler 1951 kullback 1968 similarly this quantity is also used to derive the entropic penalty of over parameterization of models by akaike 1974 where it is assumed that any two random variables have similar measures i e both discrete or continuous when applied to numerical simulations the kullback leibler divergence assumes that different models use the same discretization length which then cancels within the resulting expression if differing discretizations are used then the different entropies information associated with those discretizations must be accounted for this observation also applies to the akaike information criterion which is widely used to assess model fitness akaike s original information criterion the aic simply penalizes different models for their number of adjustable parameters which means that gains in accuracy between a model and a single realization of data may be counteracted by over parameterization in this paper we show that the information penalty for highly discretized models means that minor gains in accuracy may be overwhelmed by losses due to computational complexity in other words a modeler knows intuitively that a good model is 1 accurate 2 parsimonious and 3 computationally efficient the first point has been investigated thoroughly via convergence analysis maximum likelihood estimation etc the second point was addressed by akaike 1974 and later extensions of the aic konishi and kitagawa 2008 the last point has lacked a theoretical foundation so we address it herein we also investigate using the simplest setting possible a 1 d diffusion problem whether both particle tracking and eulerian finite difference solutions display an optimal discretization where small accuracy gains are unsupported by larger computational expenditures i e information requirements we begin by reviewing the definitions of entropy and defining one that is consistent amongst discrete and continuous rvs our definition includes a discretization variable that is similar to the sampling volume of kitanidis 1994a for moving plumes we then review the classical particle tracking algorithm e g labolle et al 1996 which does not directly track entropy because particles do not exchange mass in this case the entropy can only be calculated after a continuous interpolation of concentration is performed which is shown to have an effect on the entropy calculation this is in contrast to newer particle tracking algorithms that do exchange mass between particles during each timestep benson and bolster 2016 sole mari et al 2019 because of the mass transfer entropy automatically and continuously changes during a simulation we investigate the entropy evolution represented by two types of inter particle mass transfer algorithms 1 smoothed particle hydrodynamics gingold and monaghan 1977 monaghan 2012 and 2 a particle collision based algorithm benson and bolster 2016 the first method seeks to optimally solve a given deterministic pde using particles while the second implements a local physics based set of equations for particle behavior typically on the global scale this method solves a stochastically perturbed equation of transport benson et al 2019 because of the subtle differences and similarities of the two methods we investigate the growth of entropy simulated by both for a highly simplified problem one dimensional diffusion we wish to track entropy in the particle tracking algorithms because it is a direct measure of mixing between dissimilar waters additionally mixing is often the primary control on chemical reactions in previous studies benson et al 2017 2019 we were forced to track the creation and destruction of chemical species i e reaction rates to compare numerical methods or upscaling techniques with entropy consistently defined the mixing performance of numerical and analytic techniques can be directly measured 2 mathematical background the classical particle tracking pt method is a way to eliminate numerical dispersion in the simulation of the advection dispersion equation ade given by 1 c t v c d c because the dispersion tensor may have spatial variability and resides inside a spatial derivative one chooses specific values of the drift and pure diffusion terms in a numerical implementation of the associated itô equation of particle motion δ x v d δ t b 2 δ t ζ where x is a particle position vector in d spatial dimensions v x is a known velocity vector and d x is the local dispersion tensor at the position x at the beginning of the timestep bb t d is a cholesky decomposition of the known diffusion tensor and ζ is a d dimensional vector of independent standard normal random variables kitanidis 1994b labolle et al 1996 lichtner et al 2002 øksendal 2003 gardiner 2004 to approximate the solutions of eq 1 a large number of independent particles are transported according to the numerical itô equation and the histogram or other interpolation of these particles is used to recreate the function c x t if all n particles begin at the same location then c x t is a density function and an approximation of the green s function generated by eq 1 because of the random dispersive motions of particles the pt method accurately simulates the spread of a plume following the ade however in its raw form prior to creating the function c x t the pt method does not correctly simulate the mixing of dissimilar waters or dilution of a conservative plume because particles maintain constant mass mixing and dilution can only be taken into account with post processing of particle positions mixing and or dilution are commonly measured by borrowing the definition of the entropy hd of a discrete random variable x see the seminal paper by kitanidis 1994a and recent extensions and applications by chiogna et al 2012 chiogna and rolle 2017 sund et al 2017 entropy is the expectation of the information contained within the probability mass function of that random variable the information i p is a non negative function of an event s probability p that is defined as additive for independent events i e i p 1 i p 2 i p 1 p 2 because of this axiom the functional form of information must be i p ln p so that the expected information is also strictly non negative and defined by 2 h d x e i p x i 1 n p x i ln p x i for a discrete random variable rv with probability mass function p x taking non zero values at points x 1 x n by analogy the expected information for a continuous rv is sometimes listed as 3 h i x f x 0 f x ln f x d x where the pdf of the continuous random variable x is f x l of course f x is not a probability and thus the argument of ln is not a dimensionless quantity for these reasons eq 3 is not well defined on its own in addition because f x may often be greater than unity this usage for a continuous rv can violate the notion of entropy by assuming negative values therefore we use the subscript on hi to represent inconsistent entropy this definition is not without its utility however zero entropy means perfect order zero mixing and negative entropy has no physical meaning in other words this definition 3 for a continuous rv is only a loose analogy see appendix a it does not follow from a riemann integral representation of eq 2 meaning 4 f x 0 f x ln f x d x lim δ x 0 i 1 n f x i δ x ln f x i δ x where x 1 x n is a set of values at which f xi 0 for i 1 n and the grid spacing δ x x i 1 x i is uniform for every i 1 n 1 in fact the limit on the right side does not converge for any valid pdf in practice the evaluation of the entropy of some arbitrary continuous function f x like a plume moving through heterogeneous material that does not have a convenient hand integrable form must impose a sampling interval δv we use this new variable to conform with the usage in kitanidis 1994a with this finite sampling an entropy hc may be defined that is consistent with hd in eq 2 by using the approximation that for small δv 5 p x δ v 2 x x δ v 2 f x δ v so that the argument of the logarithm in eq 3 is once again a dimensionless probability directly related to the sampling interval δv 6 h c x f x 0 f x ln f x δ v d x ln δ v h i additionally to construct a discrete approximation of the consistent entropy we can merely approximate the integral in hi so that 7 h c x ln δ v i 1 n f x i δ x ln f x i now we may identify this sampling volume δv as identical to the volume invoked by kitanidis 1994a to relate the discrete and continuous definitions of entropy so that hd hc most commonly one would let δ v δ x in the sum of eq 7 but in estimation theory this discretization may represent different things appendix a clearly the choice of sampling interval δv both allows for a direct comparison of continuous to discrete processes and imposes some restrictions on how entropy is calculated as we show later kitanidis 1994a also defines the dilution index e as the product of the sampling volume and the exponential of the entropy for discrete and continuous random variables using the consistent entropy provided by eq 7 this can be written as 8 e δ v e h c δ v exp ln δ v i 1 n f x i δ x ln f x i exp i 1 n f x i δ x ln f x i as δx 0 this uses the classical inconsistent definition of entropy for a continuous random variable namely e exp f x ln f x d x e h i for a discrete random variable this becomes 9 e δ v e h d δ v exp i 1 n p x i ln p x i each definition 8 and 9 has the same units as x i e a volume in the number of dimensions of random travel x and has a reasonably well defined physical meaning as the size of the volume occupied by either the ensemble of particles or the pdf f x kitanidis 1994a a real or simulated plume of conservative tracer is often idealized as a pdf of travel distance i e the green s function when the spatial source is a normalized dirac delta function δ x without loss of generality we will only consider plumes that have such a source function so that we may use concentration as a pdf at any fixed time t and thus c x t f x in eq 7 the normalized concentration given by the classical pt method is represented as an interpolation of the n particles namely 10 c n x t 1 m t o t i 1 n ω m i δ z x i t ϕ x z d z 1 m t o t i 1 n m i ϕ x x i t where cn x t l 1 is a reconstructed concentration function mtot is the total mass ω l is the physical domain mi is the mass of the ith particle δ x x i t is a dirac delta function centered at each particle location xi t for i 1 n and ϕ x l 1 is a kernel function the probability of a particle s whereabouts is simply p x i m i m t o t for simplicity here we will use constant m i m 1 n which means that each kernel must integrate to unity and m t o t 1 in general the kernel function is not known or specified in the pt method a common choice uses simple binning of arbitrary size δx which is identified with a generalized kernel that depends not merely upon the distance between particle positions and binning grid points but each separately in particular the binning kernel function ϕ x xi t is defined by 11 ϕ x x i t 1 if x x ℓ x ℓ 1 0 else where ℓ ceil x i t x 1 δ x is the binning gridpoint to the left of the particle position and ceil x is the ceiling function more recent methods recognize that each particle is a random sample with pdf that is the green s function so that the kernel associated with each particle should have the same shape as c x t this should be implemented as an iterative process in which 1 a simple kernel is assumed in eq 10 2 an estimated c x t is constructed 3 a new kernel is estimated ϕ x 1 h c x h t for some h 0 which is then 4 re used in eq 10 to re estimate c x t until closure is reached the closest approximation of this procedure was given by pedretti and fernàndez garcia 2013 in which a specific functional form typically gaussian is chosen for ϕ x and the size or bandwidth h of the kernel is a weighted average of a constant global bandwidth and an adaptive bandwidth based on a single pass estimation of c the weighting is a linear average of particle arrival time rank pre supposing that later arrivals are less dense this method would be difficult to apply for multi dimensional spatial pdfs so more recent methods directly calculate local particle densities that are then used to estimate each particle s unique bandwidth sole mari and fernàndez garcia 2018 because of the convolutional form in eq 10 it is easy to show that the interpolation adds the variance of the kernel to the variance of particle positions so the bandwidth h of the kernel must be kept small to minimize numerical dispersion from the interpolation process it is unclear how the pre choice of kernel function changes estimates of the entropy as we discuss in the following section 3 entropy calculation a problem with previous pt methods is that they do not automatically track dilution as particles move they do so as dirac delta functions i e the kernel itself is a dirac delta and the entropy is based on 12 c n x t 1 m t o t i 1 n m i δ x x i t i 1 n 1 n δ x x i t so that 13 h d x i 1 n m i m t o t ln m i m t o t i 1 n 1 n ln 1 n ln n not only does the entropy depend on the number of particles but it is also constant over all simulation times because mi and n do not change although particle splitting will unnaturally increase entropy this also reveals a key feature of particle tracking algorithms the use of more particles implies greater entropy mixing this effect was shown in the context of chemical reactions benson and meerschaert 2008 and measured via concentration autocovariance functions paster et al 2014 on the other hand if each mi changes due to mass transfer between particles then hd will change the question is does it do so in a manner expected by physical principles for the particle simulations that follow we assume a simple problem that is directly solvable one dimensional 1 d diffusion from an initial condition c x 0 δ x the solution is gaussian with consistent entropy from finite sampling given by 14 h c x e x 2 4 d t 4 π d t ln e x 2 4 d t 4 π d t δ v d x ln δ v 4 π d t 1 2 ln δ v ln 4 π d t 1 2 this reveals a few interesting points regarding entropy calculation first for any finite sampling volume the initial condition has unphysical h c the calculation only makes sense after some setting time t δv 2 4πed 0 03 δv 2 d second for a reliable estimation of entropy the sampling interval for a moving plume must remain constant which means that the sampling volume must be constant in space for instance if an eulerian model possesses finer grids in some areas the plume will appear to have changing entropy if the eulerian grid is used for entropy calculation third the sampling interval must be held constant in time very often pt results are sampled at increasingly larger intervals as a plume spreads out in order to reduce sampling error see chakraborty et al 2009 clearly if the sampling size δ v t then the calculated entropy will remain erroneously constant over time fourth there are two components of the entropy calculation one given by the pdf and one given by the act of sampling or the amount of information used to estimate the probabilities the term inside the logarithm this implies that all other things held equal a finely discretized model has greater consistent entropy typically a model s fitness is penalized by its excess information content but that is only represented currently by adjustable parameters e g akaike 1974 hill and tiedeman 2007 the definition of consistent entropy hc suggests that the number of nodes or total calculations in a model should also contribute to the penalty a simple example and a derivation of a computational information criterion for numerical models is explored in section 6 and appendix a unfortunately a general formula that relates entropy growth with the characteristics of the kernel ϕ x cannot be gained because 15 h x i 1 n m ϕ x x i ln δ v m i 1 n ϕ x x i d x ln δ v m m i 1 n ϕ x x i ln i 1 n ϕ x x i d x and the logarithm of the sum inside the last integral does not expand as a result we will rely on numerical applications of several different kernels in computing the consistent entropy of eq 7 4 mass transfer pt method a recent pt algorithm benson and bolster 2016 implements mass transfer between particles coupled with random walk particle tracking mtpt the mass transfer between particle pairs is based on the conceptualization of mixing as a simple chemical reaction see benson and meerschaert 2008 benson and bolster 2016 benson et al 2019b specifically full mixing between two particles possessing potentially different masses or moles a and b of any species z can be written as the irreversible pseudo reaction a z b z a b 2 z a b 2 z this full mixing only occurs between two particles based on their probability of co location in a time step of size δt and the algorithm is applied to all potentially interacting particle pairs the algorithm has been shown to act as a globally diffusive operator schmidt et al 2018 if the local mixing is modeled as diffusive i e particles move and or collide by brownian motion this means that even if particles are considered dirac delta functions their masses continually change and so the total entropy hd must also change the diffusive nature of the mass transfer may be coupled with random walks to fully flesh out the local hydrodynamic dispersion tensor so between diffusive mass transfer random walks and local advection the mass experiences the green s function of transport which may be complex due to variable velocities e g benson et al 2019 a key feature of this algorithm is that the number of particles encodes the degree of interparticle mixing which is separate from but related to the spreading of a diffusing plume benson et al 2019 schmidt et al 2018 because fewer particles implies greater average separation the mixing lags behind the spreading of particles to a greater degree as n is decreased paster et al 2014 however it remains to be shown that this effect is reflected in the entropy of a conservative plume to briefly review the mass transfer pt method calculates the probability of collision between particles this probability becomes a weight of mass transfer benson and bolster 2016 schmidt et al 2018 with the understanding that co located particles would be well mixed as a result for the ith particle the mass of a given species mi satisfies to first order 16 m i t δ t m i t j 1 n 1 2 m j t m i t p i j for i 1 n for local fickian dispersion each particle pair s collision probability is given by benson and meerschaert 2008 17 p i j δ s 8 π η d i j δ t d 2 exp r 2 8 η d i j δ t where δs is the particle support volume dij is the average d between the i and j particles r is the distance between the i and j particles and 0 η 1 is the fraction of the isotropic diffusion simulated by interparticle mass transfer the remainder 1 η is performed by random walks here we use the arithmetic average d i j d i d j 2 it should be noted that the δs does not actually change the calculation of mass transfer because the probabilities are normalized namely 18 j 1 n p i j 1 for all i 1 n if p is constructed as a matrix this amounts to row normalization which does not guarantee columns summing to unity in practice an average of row and column normalization is used to construct a symmetric and probability mass preserving matrix in which p i j p j i the calculated probabilities are normalized to sum to unity because mass must either move to other particles when i j or stay at the current particle when i j when particle masses are not all the same and particles are close enough to exchange mass then the masses must also change and therefore the entropy h d i 1 n m i ln m i must change as discussed in the introduction in the presence of dispersion gradients particles undergoing random walks must be pseudo advected by the true velocity plus the divergence of dispersion in contrast the probabilities in eq 16 should automatically adjust for these gradients because the probability of mass transfer is not given solely by d at the ith particle transfer is automatically lower in the direction of lower d as opposed to the random walk algorithm which moves a particle with a magnitude given by the value of d at the particle and hence moves it too far into regions of lower d therefore while the mass transfer algorithm has been shown to be diffusive it should also properly solve the ade with its dispersion gradients however this effect has yet to be investigated so we provide evidence via a case study of transport in shear flow in appendix b within eqs 16 18 it appears that the collision probabilities act as a kernel to redistribute mass in other words rather than create a new interpolated concentration function as a convolution of the particle masses the collision probability directly re distributes the particle masses via convolution because the convolution kernel is the collision probability we will refer to eq 17 as the collision kernel several researchers rahbaralam et al 2015 sole mari et al 2017 sole mari and fernàndez garcia 2018 have suggested that the kernel representing the mass transfer should actually be a function of total simulation time and or particle number and local density through the statistics of the particle distribution and not merely the time interval over which the particle undergoes some small scale motions to summarize these authors perform smoothing in order to most closely solve eq 1 i e the case in which mixing and dispersion are both equally modeled by the diffusion term another effect of this operation should be to most closely match the entropy of the perfectly mixed analytic solution of the diffusion equation so we investigate it here recently sole mari et al 2019 showed that mtpt can be generalized so that particles can use a gaussian function kernel other than the particle particle collision probability of eq 17 for the mass transfer in doing so the methodology can be made numerically equivalent to smoothed particle hydrodynamics sph simulations the choice of kernel has an effect on simulation accuracy sole mari et al 2019 which we theorize also changes the entropy or mixing within the simulations specifically for the mixing pseudo reaction we study here sole mari et al 2019 rewrite the mass transfer function 16 in the more general form 19 m i t δ t m i t j 1 n β i j m j t m i t p i j where 20 β i j 2 η d i j δ t h 2 and the expression for pij in eq 17 is also modified by the kernel bandwidth choice 21 p i j δ s 2 π h 2 d 2 exp r 2 2 h 2 the kernel bandwidth h depends at any time on the global statistics of the particle distribution for this reason it is called an adaptive kernel silverman 1986 more specifically we set it as the value that minimizes the asymptotic mean integrated squared error amise of a kernel density estimation the following expression is valid for a density estimation with a gaussian kernel and particles carrying identical masses silverman 1986 22 h de d 2 π d n 2 f 2 d x 1 d 4 where f is the usually unknown true distribution of solute mass for the present diffusion benchmark problem f is a zero mean gaussian with variance 2dt so the density estimation kernel is gaussian with sole mari et al 2017 23 h de 1 06 n 1 5 σ 1 06 n 1 5 2 d t this bandwidth can be used to interpolate the classical pt method for example using a gaussian kernel in eq 10 in the case of mtpt however we do not have a variable density of particles with identical masses but a constant density of particles with variable masses as an approximation we replace the number of particles n in eq 22 with the equivalent value for which the average particle density ρ would be equal in the two cases 24 ρ n f 2 d x which allows us to rewrite expression 22 as an approximation for mtpt 25 h sph d f 2 d x 2 π d ρ 2 f 2 d x 1 d 4 once again because of the simple benchmark problem studied herein there is a very simple solution for the bandwidth because the distribution f at any time is a gaussian with variance σ 2 2 d t furthermore if n particles are placed within an interval of length ω with average spacing ω n 1 ρ which doesn t change significantly during a simulation then the bandwidth reduces to 26 h sph 0 82 σ 4 5 ρ 1 5 0 82 2 d t 2 5 n ω 1 5 we have implemented the adaptive kernels as both the density interpolator ϕ of the classical random walk at any time i e a gaussian kernel with variance h de 2 in eq 10 and also in the mass transfer coefficient 20 and the probability weighting function 21 with bandwidth h sph in the mass transfer algorithm 19 5 results and discussion all simulations use d 10 3 l2t 1 and are run for t f i n a l 1000 arbitrary time units the spatial domain is arbitrary but for the mtpt method we randomly placed particles with zero initial mass uniformly on the interval 5 5 which is approximately 3 5 2 d t f i n a l note that the units are arbitrary but must be internally consistent because of the scale invariance of the solutions to the diffusion equation that follow in 1 d c x t 2 d t 1 2 c x 2 d t 1 2 1 as long as the same units of d say meters and seconds are used for elapsed time and the units of the spatial domain the solutions are universal more extensive discussions of multi scale invariance in 3 d are given by schumer et al 2003 the mtpt method can represent a dirac delta function initial condition by any number of particles here we place one particle at x 0 with unit mass to enable direct comparison of consistent entropy between all methods we chose equivalent average particle spacing and sampling volume of δ v δ x 10 n we investigate the calculation of entropy and dilution indices for 1 the pt method using bins of size δx 2 the pt method using constant size gaussian interpolation kernels 3 the pt method using adaptive kernels with bandwidth given by eq 23 4 the mtpt method using a collision probability kernel size of 4 d δ t and 5 the mtpt method using adaptive kernels with size given by eq 26 with the latter two mass transfer scenarios we also let the proportion of diffusion by mass transfer versus random walks vary and focus on the two cases of η 1 and η 0 1 to see the effect of the collision based versus sph based kernel size 5 1 pt versus collision kernel mtpt first we simulated the classical pt algorithm with concentrations mapped both by binning and by gaussian kernels with fixed variance 2d 1 time unit because the simulations go from t 0 01 to 1000 we chose a kernel size that is too big in the beginning and perhaps too small in the end i e the kernel size is about 1 3 the spread of particles at t 10 the calculated entropies from these simulations were compared to the analytic solution of eq 14 using δ v 10 n and the collision kernel mtpt algorithm outlined in the previous section 4 in these first mtpt simulations we set the proportion of diffusion by mass transfer η 1 in comparison to the other methods the entropy from binned pt concentrations matches the analytical solution very well at early times but significantly diverges later fig 1 the difference between solutions is more obvious when looking at the dilution index e fig 2 the fixed gaussian kernel interpolated concentrations over estimate entropy and mixing at early time because a fixed kernel size is chosen that is typically larger than the actual diffusion distance for small times the mtpt method underestimates entropy at early time relative to the analytic solution of eq 14 because the method by design does not perfectly mix concentrations the random spacings impart regions where the particles are farther apart and in these regions the solutions are imperfectly mixed i e imperfectly diffusive as n gets larger the solution is more perfectly mixed and converges to the analytic diffusion kernel earlier figs 1 and 2 it is also important to note that neither the analytic solution nor the classical pt method represents the entropy of the initial condition correctly the pt method with all n particles placed at the origin still has h d ln n while the entropy of the true dirac delta initial condition is h d 1 ln 1 0 the analytic solution of eq 14 must use a calculation grid with finite δx and sampling volume δv in order for later time entropies to match this must be chosen as the same size as the bins for the pt method i e δ x x max x min n where the extents are chosen to almost surely see all particles in a simulation on the other hand the mtpt method can represent the initial condition in many different ways but here we simply placed one particle at the origin with unit mass while the remaining n 1 particles are placed randomly from the uniform distribution on 5 x 5 with zero mass because of this ic the mtpt method can faithfully represent h d t 0 0 and the effect of this deterministic unmixed ic stays with the simulations for a fair amount of time at later time both the fixed kernel pt and the mtpt methods converge to the analytic solution figs 1 2 at early times however the fixed kernel interpolator overestimates mixing when generating c x t not only with respect to the gaussian solution but also relative to the true initial condition with h d 0 note also that the calculations of consistent entropy hd depend strongly on n but not the dilution index e which accounts for the different sampling support volumes 5 2 adaptive kernel versus collision kernel mtpt we now turn to simulations using adaptive kernels in which the particle particle interaction probability has a time and particle number varying kernel size by placing eq 25 into eq 21 this is predicated on the fact that a finite sampling of independent random variables is often used to create a histogram of those rvs the idea is that a re creation of the histogram should allow each sample to represent a larger domain than just its value and a kernel should be assigned to spread each sample value in the case of independent mass preserving random walks the idea is clearly sound for a delta function initial condition each particle is a sample with a pdf that is the green s function so that each particle s position could be viewed as a rescaled green s function which is approximated by the histogram itself the rescaling depends on the actual green s function which may vary in time and space and the particle numbers for independent particles undergoing brownian motion the green s function is gaussian with variance 2dt and the kernel is shown to be gaussian with zero mean and standard deviation given by eq 25 it is less clear whether this kernel should be used to represent the particle particle interaction probability first the global statistics are not important to local mixing or reactions i e a paucity of a reactant in one location is not informed by a wealth of reactant outside of the diffusion distance in one timestep second the masses present on particles are anything but independent as they depend strongly on their near neighbors third the kernels are designed to create a maximally smooth pdf based on random samples but much research has shown that small scale fluctuations are the most important driver of mixing and reaction rates thus any kernel that smooths the local fluctuations is artificially increasing mixing and resulting reaction rates however much of this discussion is pure speculation so we implement the kernel functions here as both interpolants of independent random walks and as weights in the mixing function for brevity and consistency with the previous results we only show simulations with n 300 and n 30 000 intermediate numbers track the same trends for both particle numbers the kernel interpolated classical pt method has consistent entropy and dilution indices that match the diffusion equation analytic solution quite nicely blue circles figs 3 and 4 the kernels perform exactly as designed for optimally interpolating the pdf of independent randomly walking particles for the lower particle number 300 the adaptive kernels in the mtpt algorithm match the analytic solution more closely than the collision kernel at early times compare figs 1 and 3 the analytic solution assumes perfect mixing i e local mixing and spreading are equal and characterized by the single coefficient d 5 3 partitioning of local mixing and random walk spreading recent studies benson et al 2019 schmidt et al 2018 that employ the collision kernel for mass transfer have shown that mixing can be simulated as a smaller scale and smaller magnitude process than solute spreading this concept relies on the fact that upscaling by volume averaging and or projection of 3 d concentrations to 2 d or 1 d replaces multi valued concentrations with an average e g taylor 1953 the spreading or warping of a concentration interface is a faster process than actual mass transfer across the interface this is exemplified by miscible displacement of one fluid by another in laminar poiseuille flow in a tube where higher velocity in the center warps an initially sharp interface much faster than molecular diffusion actually mixes the fluids when volume averaged to 1 d taylor 1953 the spreading is given by a macro dispersion coefficient that grows in time to an asymptotic value benson et al 2019 showed that taylor s macrodispersion can be performed by random walks which causes particles to spread apart on average while true mixing by molecular diffusion is performed by inter particle mass transfer using the physics based collision kernel it is unclear whether using the adaptive sph kernels as defined in eq 26 can achieve the same effect given that the particle spreading is part of the evaluation of the kernel size for smaller scale mixing to investigate this effect we chose a simple system in which the macrodispersion portion of d was the largest part and also constant over time by setting a constant mixing proportion η 0 1 and re ran the mtpt simulations for n 300 and n 30 000 only the dilution indices are shown here in fig 5 the differences between results for the collision kernel are small while the adaptive kernel shows significantly decreased mixing this increased error for the adaptive kernel when η 1 can be explained as follows expression 26 was obtained from 25 by assuming that the spatial distribution of the solute f is represented by a gaussian function with variance 2dt while this is approximately true for η 1 the micro scale variability generated when η 0 1 see fig 7a suggests that f may not even be continuous and twice differentiable to start with which is a requisite for expression 25 to be valid nevertheless if 2 f 2dx was to be numerically estimated each time step such as in sole mari and fernàndez garcia 2018 it would be much higher than for a gaussian f with variance 2dt because of the strong small scale concentration variations suggesting that the truly optimal adaptive kernel obtained from eq 25 in this case would be much smaller than eq 26 5 4 distributional entropy as noted earlier particle simulations display greater entropy with an increasing number of particles e g figs 1 and 3 in a similar way that the consistent entropy is related to classically defined inconsistent entropy for a continuous rv by adding the sampling portion h c ln δ v h i the portion of the entropy of a discrete rv can be partitioned into particle number and underlying structure of the pmf h pmf ln ω n h d using this adjustment the amount of mixing given by the rate of convergence to the gaussian between simulations with different particle numbers can be compared fig 6 here we ran mtpt simulations using the collision kernel with particle numbers in the set 100 300 1000 3000 10000 30000 for smaller n the ensemble average of up to 20 realizations is used because of differences between individual runs quite clearly the smaller particle numbers experience delayed convergence to the well mixed gaussian this is a feature of the mt algorithm that is usually reflected in reduced reaction rates but a simple measurement of the reduced entropy creation rate with smaller particle numbers is a sufficient demonstration of suppressed mixing it is also instructive to inspect the plots of the calculated pmfs and pdfs from the η 0 1 simulations fig 7 the collision kernel mtpt method is notable because the degree of mixing and the shape of the plume are somewhat independent random walks may place particles with different masses in arbitrarily close proximity and some time must elapse before local mixing equilibrates those masses e g fig 7a the result is the mass or concentration at any single position in 1 d space possesses substantial variability this feature concentration fluctuations at any point in space has been exploited to perform accurate upscaling of transport and reaction in heterogeneous velocity fields benson et al 2019 cirpka and kitanidis 2000a 2000b dentz et al 2000 dentz and carrera 2007 on the other hand the fixed kernel interpolation of classic pt methods replaces this concentration variance at every location with concentration variability in space see blue circles in fig 7b 6 computational entropy penalty numerical models provide discrete estimates of dependent variables that may be continuous functions of time and space often the functions are non negative and can be normalized to unit area so that they are pdfs therefore the underlying true pdf has a certain entropy and the sampling or computational procedure used to approximate these functions adds some artificial entropy because of the information required by the discretization one desirable trait of a model is a parsimonious representation of the true physical process i e fewer model parameters are preferred at the same time a more straightforward and accurate computational process is also preferred considerable attention has been paid to parsimonious few parameter models but less attention has been paid to model computational requirements eq 7 shows that if a true pdf can be estimated via very few sampling points or nodes there is less additional entropy incurred in the calculation that is to say if two models with the same parametric parsimony yield equivalent estimates of the underlying true dependent variable then the model that estimates the pdf with the coarsest sampling or least computationally intensive structure is preferred from an entropic standpoint augmenting the kullback leibler inconsistent representation of model entropy with the consistent entropy appendix a allows us to compare the discrete pmfs obtained from computational approximation with the underlying pdf and ultimately results in the computational information criterion comic as a natural extension of akaike s information criterion akaike 1974 1992 to emphasize the influence of computational entropy we illustrate two examples here by estimating a true diffusion given by a gaussian with variance 2dt by several numerical calculations with zero adjustable parameters i e d is a known parameter 6 1 finite difference example for simplicity we set δ v δ x ω n for a fixed domain ω and n nodes and then compared the numerical estimation of the green s function of the 1 d diffusion equation given by implicit finite difference fd models with different discretizations δx 0 4 0 12 0 04 0 012 0 004 0 0012 0 0004 other numerical parameters were held constant including ω 6 6 d 10 3 and δ t 0 05 we use all of the data from each model to calculate the mean sse i e the mean sse is independent of subsampling so here n n clearly a smaller δx provides a better estimate of the analytic solution of a gaussian with variance 2dt but at what cost do 100 nodes suffice a million because there are no adjustable parameters the aic which is given in terms of the log likelihood function aic 2 ln sse n is a decreasing function of the number of nodes n fig 8 a if however one factors in the penalty of ln δv there is an optimal tradeoff of accuracy and computational entropy at n 3000 at almost every time step fig 8b fewer nodes are not sufficiently accurate and more nodes are superfluous for this particular problem as shown by plotting the relative fitness criteria aic versus comic for each discretization at some time fig 8c four important points regarding the comic immediately arise 1 a model is typically sampled at a finite and fixed number of data measurement locations we also sampled the many fd models and analytic solution at 15 randomly chosen measurement points common to all simulations and found nearly identical albeit more noisy results however we have not yet investigated the effect of additional sample noise on discerning the optimal discretization 2 the aic was derived with the assumption that the number of sample points and computational burden of models is identical and do not contribute to the relative aic often the common factors are eliminated from the aic and some arbitrary constants are also added with no effect on relative aic in considering the comic however the choice of likelihood function and inclusion of constants may change the optimal model so care in the choice of aic is required 3 the numerical solutions at some final time t are actually conditional densities of the joint densities c x t so that increased number of timesteps should also increase computational entropy i e δt contributes to the multidimensional δv see appendix a here we held the time step size constant for all fd models so that the temporal sampling t j δ t has no effect on the relative entropy 4 we used a constant spatial discretization δ v δ x to simplify the comparative kullback leibler measures some models use variably spaced grids so the resulting computational entropy is more complicated than we investigate here 6 2 mass transfer particle tracking examples regarding this last point stated above for finite difference models the main thrust of this paper is the entropy of particle methods the particles are typically randomly spread in space so that a constant δv is not possible however using the inconsistent entropy isolates the correspondence of the n particles to an underlying pmf e g fig 6 in the case of perfectly mixed fickian diffusion this enables a direct comparison of the fitness of the particle methods to simulating diffusion and the correction term ln ω n is the entropy associated with computation we use this correction in analogy with the fd results above to assess the entropic fitness of mtpt methods and test several intuitive hypotheses first prior research has shown that fewer particles in the collision kernel mtpt method represent poorer mixing hence poor fitness when modeling perfectly mixed fickian diffusion in the absence of mixing by random walks i e η 1 we hypothesize that adding more particles will yield an improved average sse but that the overall model entropic fitness measured by a smallest comic reaches a maximum at some point indeed a statistically significant minimum is found between n 1000 and n 10 000 particles with an estimated minimum at 3000 particles fig 9 a on the other hand the adaptive sph kernel is constructed to best match fickian diffusion by everywhere adjusting for particle density and number therefore we hypothesize that the model entropic fitness will be relatively stable across a broad range of particle numbers this is also found to be true in simulations fig 9b and comic fitness only suffers in a significant way for n 100 finally in contrast to the collision kernel for η 1 shown in fig 9a we hypothesize that splitting the diffusion between mass transfer and random walks will improve for this example the fitness of smaller particle number simulations by eliminating persistent mixing gaps where large random distances between particles prevents convergence to a well mixed gaussian however at some point the model sse will not improve with the addition of more particles because the noise of concentrations around the gaussian will be saturated see e g fig 7 a fig 9c reveals exactly this behavior in the comic adding random walks decreases the optimal number of particles to 300 to summarize the mtpt entropic fitness for simulating fickian diffusion 1 for the sph kernel small particle numbers are sufficient and equally fit by design 2 similarly to the fd method the collision kernel has a minimum comic around 3000 particles and 3 with the collision kernel partitioning diffusion by mass transfer and random walks promoted mixing and fitness for smaller particle numbers 300 and clearly shows the superfluous nature of large particle numbers for simulating fickian diffusion 7 conclusions classical pt methods do not track entropy until a concentration function is mapped from particle positions the choice of bins or kernels for this mapping cannot be arbitrary as the choice directly changes the calculated entropy or degree of mixing of a moving plume the newer mass transfer method directly simulates entropy without any such mapping because particle masses continually change and does so with several beneficial features first the zero entropy initial condition and its effect on the early portions of a simulation are accurately tracked second the particle number is an integral part of the mixing rate of a plume higher particle numbers simulate more complete mixing at earlier times as shown by the convergence of entropy to that of a gaussian the mtpt method can use physically based particle collision probabilities for the mixing kernel or adaptive kernels dictated by the sph algorithm these adaptive kernels more closely match the analytic gaussian solution s entropy when solving the diffusion equation in one pass i e all mass transfer given by the diffusion coefficient however when the diffusion dispersion is split between local inter particle mixing and spreading by random walks the adaptive kernel entropies change substantially and do not match the gaussian solution for small particle numbers the collision kernel does not generate the same effect we suggest that the adaptive sph kernels only be used to solve locally well mixed problems i e where the dispersion tensor represents both mixing and dispersion equally whereas the collision kernel may partition mixing and spreading as the physics of the problem dictate benson et al 2019 the fact that discrete or discretized approximations to real continuous functions carry a sampling or computational entropy means that metrics which compare different simulations based on information content must be penalized by that computational information for this purpose we define a computational information criterion comic based on akaike s aic that includes this penalty we show how a finite difference solution of the 1 d diffusion equation has a well defined optimal solution of about 3000 nodes in terms of combined accuracy and computational requirements when the mtpt is used to simulate fickian diffusion these simulations show that the collision kernel also has a minimum comic around 3000 particles but the adaptive sph kernel by design is fit over a large range of particle numbers adding some diffusion by random walks makes the collision kernel a better fit for smaller particle numbers n 300 and shows that simulations of fickian diffusion for large number of particles is computationally superfluous we anticipate that this new entropy based fitness metric may discount some overly computationally intensive models that previously have been deemed optimal in terms of data fit alone credit authorship contribution statement david a benson conceptualization methodology software writing original draft writing review editing stephen pankavich formal analysis writing original draft writing review editing michael j schmidt methodology software writing review editing guillem sole mari methodology software writing original draft writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we thank the editor reviewers daniele pedretti and olaf cirpka and one anonymous reviewer for extremely helpful comments this material is based upon work supported by or in part by the us army research office under contract grant number w911nf 18 1 0338 the authors were also supported by the national science foundation under awards ear 1417145 dms 1614586 dms 1911145 ear 1351625 ear 1417264 ear 1446236 cbet 1705770 and dms 1911145 the first author thanks the students in his class gegn 581 analytic hydrology for inspiring this work two matlab codes for generating all results in this paper one finite difference and one particle tracking are held in the public repository https github com dbenson5225 particle entropy appendix a computational information criterion and maximum likelihood estimators the ultimate goal of this section is to derive an extension of akaike s an information criterion aic akaike 1974 that establishes an objective function to be optimized in order to select a model and a minimal number of parameters that best fits a given set of data such an extension must incorporate the results of section 2 which introduced a consistent notion of entropy that allows one to compare the relative entropies of a continuous pdf with that of a discrete pmf approximation of course this discussion will first require some background knowledge of the kullback leibler divergence the basic formulation of the aic and maximum likelihood estimators each of which we provide below a1 kullback leibler divergence we begin with a review of the kullback leibler divergence and its extension to the inconsistent entropy for continuous rvs following kullback 1968 we first consider the likelihood of two competing hypotheses h 1 and h 2 given some knowledge of the probability of an event x and note that bayes theorem provides a representation for the conditional probability of each hypothesis given x namely 27 p h i x p x h i p h i p x h 1 p h 1 p x h 2 p h 2 for i 1 2 next this statement can be generalized to continuous rvs so that if h 1 and h 2 now represent the events that a random variable x comes from a distribution represented by the pdfs f 1 x and f 2 x respectively then the conditional probability of each hypothesis given that x x is now 28 p h i x f i x p h i f 1 x p h 1 f 2 x p h 2 for i 1 2 taking logarithms and rearranging this expression then yields 29 ln f 1 x f 2 x ln p h 1 x p h 2 x ln p h 1 p h 2 the right side of this equality is a measure of the difference between the logarithm of the odds in favor of hypothesis 1 versus hypothesis 2 after the observation of x x relative to before this observation in other words this difference represents exactly the information contained within the observation that x x and the left side of the equality often referred to as the log likelihood ratio is the information in favor of h 1 and against h 2 to obtain the mean value of this we merely integrate over all possible observations and against the pdf f 1 x which gives 30 i f 1 f 2 f 1 x ln f 1 x f 2 x d x this quantity is defined to be the kullback leibler divergence kld and represents the entropy of f 1 relative to f 2 notice that this expression can also be separated into two integrals so that 31 i f 1 f 2 f 1 x ln f 1 x d x f 1 x ln f 2 x d x and the former of these two integrals is directly related to the inconsistent entropy of eq 3 next we will use the kl divergence to establish the aic a2 akaike information criterion aic the aic was originally established to select a model and associated parameter values that are a best predictor of potential future data based on some set of given data it is well known that adding parameters will reduce data model misfit for a single set of observations but the added parameters will often cause worse fits for newly collected data konishi and kitagawa 2008 in particular consider a variety of different models defined by distinct parameter vectors θ and corresponding pdfs h y θ arising from data values y 1 y n along with a single vector of true parameter values θ 0 with pdf g y h y θ 0 the problem of interest is how to optimally select both a number of model parameters k and their associated values θ to best approximate θ 0 given that we have incomplete knowledge of the latter quantity in fact the information provided to make this decision arises only from the given data which is merely a collection of n independent sample values each representing a realization of a random variable y with pdf g y ultimately the aic yields an approximate criterion for the selection of parameters which entails minimizing the quantity 32 2 i 1 n ln h y i θ 2 k over the number of parameters k where θ is the maximum likelihood estimate for θ furthermore this process corresponds to minimizing the underlying entropy among such models in the context of computing concentrations as in previous sections we could consider a function c x t for which we have a coupled set of observed data say x i c i i 1 n which represents values of the concentration measured at differing spatial points and at a fixed time t t here the function c can be a solution to a pde e g eq 1 and may depend upon some parameters θ for instance d in eq 1 of course the parameter values are unknown and must be inferred from the given data which may contain some noise due to errors in measurement we can then define yi to be the corresponding error between c and the measured data ci for every i 1 n and consider the associated pdf for each of these errors denoted h y θ additionally θ 0 represents the true parameter values so that the underlying pdf can be represented by g y h y θ 0 the aic will then provide a criterion to select the parameter values and number of parameters that is a best predictor of any future concentration data thereby selecting a specific model the selection criterion is based on the entropy maximization principle which states that the optimal model is obtained by maximizing over the given data on which θ depends the expected value of the log likelihood function namely 33 s g h θ g y ln h y θ d y this quantity is not a well defined i e strictly positive counterpart to entropy as discussed in the main text thus it is typically implemented in a relative sense among models using the kullback leibler divergence of g and h given by 34 i g h θ g y ln g y h y θ d y s g g s g h θ as noted in the previous section this can be interpreted as a measurement of the relative similarity between the probability distributions g and h as akaike 1974 notes maximizing the expected log likelihood above is equivalent to minimizing i g h θ over the given data of course since θ 0 is unknown and g y h y θ 0 depends upon knowledge of the true parameter values we cannot directly compute i g h θ instead this quantity must be suitably approximated following akaike 1974 1992 when the number of data points n is sufficiently large an approximation of the kld using the fisher information matrix can be utilized and classical estimation techniques imply 35 i g h θ i 1 n ln g y i i 1 n ln h y i θ k where k is the number of estimated parameters within θ and θ is the maximum likelihood estimate for θ here k appears in order to correct for the bias introduced by approximating the true parameter values with their corresponding maximum likelihood estimates finally since the sum involving g is constant for any choice of model parameters it can be omitted in computing the minimization therefore the aic may be defined with a scaling factor of two as in akaike 1974 by 36 aic 2 ln maximum likelihood 2 k or in the notation described herein 37 aic θ 2 i 1 n ln h y i θ 2 k it is this quantity that one wishes to minimize over k where θ may depend upon k in order to select the best model approximation to g and this is the basis of our departure prior to formulating an extension of the aic we provide a brief review involving maximum likelihood estimators a3 maximum likelihood estimators mles though we have not mentioned the process of obtaining the maximum likelihood estimates θ described above useful discussions of mles for models with unknown structure are provided by hill and tiedeman 2007 and brockwell and davis 2016 as an example consider the scenario in which the errors between model and observations are independent zero mean gaussians in this case the likelihood function is given by 38 l y θ 2 π n σ θ 1 2 exp 1 2 y t σ θ 1 y where n is the number of observation points σ θ is a covariance matrix of errors that depends upon some unknown parameter vector θ and y is a vector of residuals satisfying y i c i c x i t for i 1 n recall that ci is the measured concentration and c xi t represents the concentration at the spatial data point xi and time t given by the pde solution therefore the log likelihood function is 39 ln l n 2 ln 2 π 1 2 ln σ 1 2 y t σ 1 y in practice the observation errors are often assumed to be independent and σ is diagonal furthermore the variance of each observation is often unknown or estimated during the model regression although numerous approximations can be applied see chakraborty et al 2009 for assumed concentration errors in particle tracking so it is assumed that σ depends only upon a single variance parameter denoted by σ 2 and thus satisfies σ σ 2 i the last term in eq 39 is more conveniently given in terms of the sum of squared errors sse y y y 2 so that 40 ln l n 2 ln 2 π n 2 ln σ 2 n 2 σ 2 sse n because this function should be maximized one takes the derivative with respect to σ 2 and sets it to zero to compute the value of σ 2 at which the maximum occurs this provides an estimator of the observation variance namely σ 2 sse n so that the corresponding maximum is 41 ln l n 2 1 ln 2 π ln sse n because the number of observations is usually fixed the n 2 term is canceled from all terms as maximizing ln l also maximizes 2 n ln l hence the mle is σ 2 sse n and the quantity ln sse n provides a relative estimate for the value of the log likelihood function evaluated at the mle a4 computational information criterion returning to the formulation of the aic for a model of concentration we wish to alter the original derivation so that one may compare a variety of discrete computational models to the true continuous model using their relative entropy to evaluate their similarity in such a case we would like to use the kld to measure the relative entropy of the error distribution between the observed data and the solution of the pde model but we must also correct this criterion for the fact that our discrete approximations to the pde solution change with the resolution of the chosen numerical method which is often described by a single parameter n for instance this parameter can represent the number of particles n in a stochastic particle method or identify the spatial grid size δ x ω n in a finite difference method ultimately we wish to establish a criterion to select the best of these approximate models depending upon the value of n as before letting xi ci for i 1 n represent the given pairs of concentration data and c x t denote the true solution to the pde model we can describe the statistical model incorporating measurement error by c i c x i t ϵ for i 1 n where t is a known measurement time and ϵ h y θ is a random variable with distribution h that encodes each of the associated random errors because we often do not possess an analytic solution for c it is necessary to approximate the pde solution at time t t with a number of suitable numerical models the solutions of which we denote by cn x with the model of interest changing with the value of n hence discrete approximation error in addition to measurement error from the data must be incorporated into the model selection criterion because the kld can account for the latter quantity we can establish a computational information criterion merely by correcting the aic by the difference in entropy between the pde solution and the numerical approximation in this way if hrel f 1 f 2 represents the relative entropy between f 1 and f 2 then the new information criterion can be expressed as comic θ n h r e l c c i h r e l c n c the first relative entropy here is given by the aic while the second is merely the difference between the discrete entropy of the approximate solution and the inconsistent entropy of the pde solution or h d h i where these quantities are given by eqs 2 and 3 respectively in section 2 a sampling volume was introduced to relate these terms using the consistent entropy of eq 6 and we found see fig 6 with δ v ω n h d h i ln δ v using this we can finally define an adjusted criterion to the aic which we name comic or the computational information criteria given by 42 comic θ δ v 2 i 1 n ln h y i θ 2 k ln δ v from an information theory perspective this computational penalization can also be seen as a limitation on the information content needed to represent the approximate solution cn x assuming the selected numerical method converges to the underlying pde solution as n the values of c x t can be computed to an arbitrarily large degree of precision by merely choosing n sufficiently large in the computational model however in doing so one must continue to store increasingly large amounts of information to gain smaller and smaller levels of accuracy thus a trade off results between the desired gain in precision and the stored information content and the comic provides an efficient criterion for penalizing such considerations to select a parsimonious and computationally efficient low information content model in order to focus on the computational implications of this adjustment to the model selection criterion we consider the case in which the errors between model and observations are gaussian with variance σ 2 as in the example illustrated within the previous section and assume that no other parameters e g d require estimation in this case the log likelihood function evaluated at the maximum likelihood estimate is proportional to the log of the average sum of squared errors sse given by eq 41 upon removing constants the form of the comic becomes 43 comic δ v 2 ln sse n ln δ v where 44 sse i 1 n c i c n x i 2 for numerical models with equivalent sse their measure of distributional entropy is the same but their computational entropy would be ln δ v so that the model fitness should be adjusted by this difference in information content appendix b effect of d on the mass transfer algorithm we illustrate the effect of spatially variable d in simple 2 d shear flow borrowing the parabolic velocity profile v y 0 and v x y 2 b y of hagen poiseuille flow the domain used here is 0 x 400 0 y 1 with concentrations initially zero everywhere except for a strip 90 x 110 with concentration 1 20 i e initial mass 1 the x domain is periodic so particles that exit at x 400 are re introduced at x 0 we show a scenario with heterogeneous and anisotropic diffusion d α l v x 0 0 α t v x with longitudinal and transverse dispersivities α l 10 2 α t 10 3 dispersive transport was simulated for t 500 with timestep size δ t 1 either solely by mass transfer or solely by random walks because the mass transfer algorithm can move mass among all particles in the domain a total of 20 000 particles were placed in the 400 1 domain with an average of 100 particles in the initial non zero concentration strip this gives plenty of clean particles on either side of the strip pure random walks without the drift correction term migrate all particles including those with mass to the lower d regions fig 10 a the drift correction eliminates the lateral bias fig 10b and e the mass transfer algorithm has no apparent bias or need for d correction fig 10c and f as an aside the mass transfer method quite clearly shows the regions of greatest and least shear and mixing fig 10c supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103509 appendix c supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
518,traditional random walk particle tracking pt models of advection and dispersion do not track entropy because particle masses remain constant however newer mass transfer particle tracking mtpt models have the ability to do so because masses of all compounds may change along trajectories additionally the probability mass functions pmf of these mtpt models may be compared to continuous solutions with probability density functions when a consistent definition of entropy or similarly the dilution index is constructed this definition reveals that every discretized numerical model incurs a computational entropy similar to akaike s 1974 1992 entropic penalty for larger numbers of adjustable parameters the computational complexity of a model e g number of nodes or particles adds to the entropy and as such must be penalized application of a new computational information criterion reveals that increased accuracy is not always justified relative to increased computational complexity the mtpt method can use a particle collision based kernel or an adaptive kernel derived from smoothed particle hydrodynamics sph the latter is more representative of a locally well mixed system i e one in which the dispersion tensor equally represents mixing and solute spreading while the former better represents the separate processes of mixing versus spreading we use computational means to demonstrate the fitness of each of these methods for simulating 1 d advective dispersive transport with uniform coefficients keywords particle methods entropy mixing dilution index computational penalty aic 1 introduction entropy is a fundamental property possessed by any random variable or process including a plume moving through natural media from a thermodynamic viewpoint an increase in entropy is an increase in mixing dilution for a conservative solute but entropy is also a measure of the information required to describe a system composed of random variables so that the entropy of a computer simulation of a plume can be readily quantified the entropy of a discrete random variable has been defined in a straightforward way by shannon 1948 unfortunately there is not a well defined counterpart for continuous random variables in fact a commonly used definition of the entropy of a continuous random variable rv can take on unphysical negative values as such the kullback leibler divergence or relative entropy which is a measure of the relative difference between two continuous rvs is often used instead kullback and leibler 1951 kullback 1968 similarly this quantity is also used to derive the entropic penalty of over parameterization of models by akaike 1974 where it is assumed that any two random variables have similar measures i e both discrete or continuous when applied to numerical simulations the kullback leibler divergence assumes that different models use the same discretization length which then cancels within the resulting expression if differing discretizations are used then the different entropies information associated with those discretizations must be accounted for this observation also applies to the akaike information criterion which is widely used to assess model fitness akaike s original information criterion the aic simply penalizes different models for their number of adjustable parameters which means that gains in accuracy between a model and a single realization of data may be counteracted by over parameterization in this paper we show that the information penalty for highly discretized models means that minor gains in accuracy may be overwhelmed by losses due to computational complexity in other words a modeler knows intuitively that a good model is 1 accurate 2 parsimonious and 3 computationally efficient the first point has been investigated thoroughly via convergence analysis maximum likelihood estimation etc the second point was addressed by akaike 1974 and later extensions of the aic konishi and kitagawa 2008 the last point has lacked a theoretical foundation so we address it herein we also investigate using the simplest setting possible a 1 d diffusion problem whether both particle tracking and eulerian finite difference solutions display an optimal discretization where small accuracy gains are unsupported by larger computational expenditures i e information requirements we begin by reviewing the definitions of entropy and defining one that is consistent amongst discrete and continuous rvs our definition includes a discretization variable that is similar to the sampling volume of kitanidis 1994a for moving plumes we then review the classical particle tracking algorithm e g labolle et al 1996 which does not directly track entropy because particles do not exchange mass in this case the entropy can only be calculated after a continuous interpolation of concentration is performed which is shown to have an effect on the entropy calculation this is in contrast to newer particle tracking algorithms that do exchange mass between particles during each timestep benson and bolster 2016 sole mari et al 2019 because of the mass transfer entropy automatically and continuously changes during a simulation we investigate the entropy evolution represented by two types of inter particle mass transfer algorithms 1 smoothed particle hydrodynamics gingold and monaghan 1977 monaghan 2012 and 2 a particle collision based algorithm benson and bolster 2016 the first method seeks to optimally solve a given deterministic pde using particles while the second implements a local physics based set of equations for particle behavior typically on the global scale this method solves a stochastically perturbed equation of transport benson et al 2019 because of the subtle differences and similarities of the two methods we investigate the growth of entropy simulated by both for a highly simplified problem one dimensional diffusion we wish to track entropy in the particle tracking algorithms because it is a direct measure of mixing between dissimilar waters additionally mixing is often the primary control on chemical reactions in previous studies benson et al 2017 2019 we were forced to track the creation and destruction of chemical species i e reaction rates to compare numerical methods or upscaling techniques with entropy consistently defined the mixing performance of numerical and analytic techniques can be directly measured 2 mathematical background the classical particle tracking pt method is a way to eliminate numerical dispersion in the simulation of the advection dispersion equation ade given by 1 c t v c d c because the dispersion tensor may have spatial variability and resides inside a spatial derivative one chooses specific values of the drift and pure diffusion terms in a numerical implementation of the associated itô equation of particle motion δ x v d δ t b 2 δ t ζ where x is a particle position vector in d spatial dimensions v x is a known velocity vector and d x is the local dispersion tensor at the position x at the beginning of the timestep bb t d is a cholesky decomposition of the known diffusion tensor and ζ is a d dimensional vector of independent standard normal random variables kitanidis 1994b labolle et al 1996 lichtner et al 2002 øksendal 2003 gardiner 2004 to approximate the solutions of eq 1 a large number of independent particles are transported according to the numerical itô equation and the histogram or other interpolation of these particles is used to recreate the function c x t if all n particles begin at the same location then c x t is a density function and an approximation of the green s function generated by eq 1 because of the random dispersive motions of particles the pt method accurately simulates the spread of a plume following the ade however in its raw form prior to creating the function c x t the pt method does not correctly simulate the mixing of dissimilar waters or dilution of a conservative plume because particles maintain constant mass mixing and dilution can only be taken into account with post processing of particle positions mixing and or dilution are commonly measured by borrowing the definition of the entropy hd of a discrete random variable x see the seminal paper by kitanidis 1994a and recent extensions and applications by chiogna et al 2012 chiogna and rolle 2017 sund et al 2017 entropy is the expectation of the information contained within the probability mass function of that random variable the information i p is a non negative function of an event s probability p that is defined as additive for independent events i e i p 1 i p 2 i p 1 p 2 because of this axiom the functional form of information must be i p ln p so that the expected information is also strictly non negative and defined by 2 h d x e i p x i 1 n p x i ln p x i for a discrete random variable rv with probability mass function p x taking non zero values at points x 1 x n by analogy the expected information for a continuous rv is sometimes listed as 3 h i x f x 0 f x ln f x d x where the pdf of the continuous random variable x is f x l of course f x is not a probability and thus the argument of ln is not a dimensionless quantity for these reasons eq 3 is not well defined on its own in addition because f x may often be greater than unity this usage for a continuous rv can violate the notion of entropy by assuming negative values therefore we use the subscript on hi to represent inconsistent entropy this definition is not without its utility however zero entropy means perfect order zero mixing and negative entropy has no physical meaning in other words this definition 3 for a continuous rv is only a loose analogy see appendix a it does not follow from a riemann integral representation of eq 2 meaning 4 f x 0 f x ln f x d x lim δ x 0 i 1 n f x i δ x ln f x i δ x where x 1 x n is a set of values at which f xi 0 for i 1 n and the grid spacing δ x x i 1 x i is uniform for every i 1 n 1 in fact the limit on the right side does not converge for any valid pdf in practice the evaluation of the entropy of some arbitrary continuous function f x like a plume moving through heterogeneous material that does not have a convenient hand integrable form must impose a sampling interval δv we use this new variable to conform with the usage in kitanidis 1994a with this finite sampling an entropy hc may be defined that is consistent with hd in eq 2 by using the approximation that for small δv 5 p x δ v 2 x x δ v 2 f x δ v so that the argument of the logarithm in eq 3 is once again a dimensionless probability directly related to the sampling interval δv 6 h c x f x 0 f x ln f x δ v d x ln δ v h i additionally to construct a discrete approximation of the consistent entropy we can merely approximate the integral in hi so that 7 h c x ln δ v i 1 n f x i δ x ln f x i now we may identify this sampling volume δv as identical to the volume invoked by kitanidis 1994a to relate the discrete and continuous definitions of entropy so that hd hc most commonly one would let δ v δ x in the sum of eq 7 but in estimation theory this discretization may represent different things appendix a clearly the choice of sampling interval δv both allows for a direct comparison of continuous to discrete processes and imposes some restrictions on how entropy is calculated as we show later kitanidis 1994a also defines the dilution index e as the product of the sampling volume and the exponential of the entropy for discrete and continuous random variables using the consistent entropy provided by eq 7 this can be written as 8 e δ v e h c δ v exp ln δ v i 1 n f x i δ x ln f x i exp i 1 n f x i δ x ln f x i as δx 0 this uses the classical inconsistent definition of entropy for a continuous random variable namely e exp f x ln f x d x e h i for a discrete random variable this becomes 9 e δ v e h d δ v exp i 1 n p x i ln p x i each definition 8 and 9 has the same units as x i e a volume in the number of dimensions of random travel x and has a reasonably well defined physical meaning as the size of the volume occupied by either the ensemble of particles or the pdf f x kitanidis 1994a a real or simulated plume of conservative tracer is often idealized as a pdf of travel distance i e the green s function when the spatial source is a normalized dirac delta function δ x without loss of generality we will only consider plumes that have such a source function so that we may use concentration as a pdf at any fixed time t and thus c x t f x in eq 7 the normalized concentration given by the classical pt method is represented as an interpolation of the n particles namely 10 c n x t 1 m t o t i 1 n ω m i δ z x i t ϕ x z d z 1 m t o t i 1 n m i ϕ x x i t where cn x t l 1 is a reconstructed concentration function mtot is the total mass ω l is the physical domain mi is the mass of the ith particle δ x x i t is a dirac delta function centered at each particle location xi t for i 1 n and ϕ x l 1 is a kernel function the probability of a particle s whereabouts is simply p x i m i m t o t for simplicity here we will use constant m i m 1 n which means that each kernel must integrate to unity and m t o t 1 in general the kernel function is not known or specified in the pt method a common choice uses simple binning of arbitrary size δx which is identified with a generalized kernel that depends not merely upon the distance between particle positions and binning grid points but each separately in particular the binning kernel function ϕ x xi t is defined by 11 ϕ x x i t 1 if x x ℓ x ℓ 1 0 else where ℓ ceil x i t x 1 δ x is the binning gridpoint to the left of the particle position and ceil x is the ceiling function more recent methods recognize that each particle is a random sample with pdf that is the green s function so that the kernel associated with each particle should have the same shape as c x t this should be implemented as an iterative process in which 1 a simple kernel is assumed in eq 10 2 an estimated c x t is constructed 3 a new kernel is estimated ϕ x 1 h c x h t for some h 0 which is then 4 re used in eq 10 to re estimate c x t until closure is reached the closest approximation of this procedure was given by pedretti and fernàndez garcia 2013 in which a specific functional form typically gaussian is chosen for ϕ x and the size or bandwidth h of the kernel is a weighted average of a constant global bandwidth and an adaptive bandwidth based on a single pass estimation of c the weighting is a linear average of particle arrival time rank pre supposing that later arrivals are less dense this method would be difficult to apply for multi dimensional spatial pdfs so more recent methods directly calculate local particle densities that are then used to estimate each particle s unique bandwidth sole mari and fernàndez garcia 2018 because of the convolutional form in eq 10 it is easy to show that the interpolation adds the variance of the kernel to the variance of particle positions so the bandwidth h of the kernel must be kept small to minimize numerical dispersion from the interpolation process it is unclear how the pre choice of kernel function changes estimates of the entropy as we discuss in the following section 3 entropy calculation a problem with previous pt methods is that they do not automatically track dilution as particles move they do so as dirac delta functions i e the kernel itself is a dirac delta and the entropy is based on 12 c n x t 1 m t o t i 1 n m i δ x x i t i 1 n 1 n δ x x i t so that 13 h d x i 1 n m i m t o t ln m i m t o t i 1 n 1 n ln 1 n ln n not only does the entropy depend on the number of particles but it is also constant over all simulation times because mi and n do not change although particle splitting will unnaturally increase entropy this also reveals a key feature of particle tracking algorithms the use of more particles implies greater entropy mixing this effect was shown in the context of chemical reactions benson and meerschaert 2008 and measured via concentration autocovariance functions paster et al 2014 on the other hand if each mi changes due to mass transfer between particles then hd will change the question is does it do so in a manner expected by physical principles for the particle simulations that follow we assume a simple problem that is directly solvable one dimensional 1 d diffusion from an initial condition c x 0 δ x the solution is gaussian with consistent entropy from finite sampling given by 14 h c x e x 2 4 d t 4 π d t ln e x 2 4 d t 4 π d t δ v d x ln δ v 4 π d t 1 2 ln δ v ln 4 π d t 1 2 this reveals a few interesting points regarding entropy calculation first for any finite sampling volume the initial condition has unphysical h c the calculation only makes sense after some setting time t δv 2 4πed 0 03 δv 2 d second for a reliable estimation of entropy the sampling interval for a moving plume must remain constant which means that the sampling volume must be constant in space for instance if an eulerian model possesses finer grids in some areas the plume will appear to have changing entropy if the eulerian grid is used for entropy calculation third the sampling interval must be held constant in time very often pt results are sampled at increasingly larger intervals as a plume spreads out in order to reduce sampling error see chakraborty et al 2009 clearly if the sampling size δ v t then the calculated entropy will remain erroneously constant over time fourth there are two components of the entropy calculation one given by the pdf and one given by the act of sampling or the amount of information used to estimate the probabilities the term inside the logarithm this implies that all other things held equal a finely discretized model has greater consistent entropy typically a model s fitness is penalized by its excess information content but that is only represented currently by adjustable parameters e g akaike 1974 hill and tiedeman 2007 the definition of consistent entropy hc suggests that the number of nodes or total calculations in a model should also contribute to the penalty a simple example and a derivation of a computational information criterion for numerical models is explored in section 6 and appendix a unfortunately a general formula that relates entropy growth with the characteristics of the kernel ϕ x cannot be gained because 15 h x i 1 n m ϕ x x i ln δ v m i 1 n ϕ x x i d x ln δ v m m i 1 n ϕ x x i ln i 1 n ϕ x x i d x and the logarithm of the sum inside the last integral does not expand as a result we will rely on numerical applications of several different kernels in computing the consistent entropy of eq 7 4 mass transfer pt method a recent pt algorithm benson and bolster 2016 implements mass transfer between particles coupled with random walk particle tracking mtpt the mass transfer between particle pairs is based on the conceptualization of mixing as a simple chemical reaction see benson and meerschaert 2008 benson and bolster 2016 benson et al 2019b specifically full mixing between two particles possessing potentially different masses or moles a and b of any species z can be written as the irreversible pseudo reaction a z b z a b 2 z a b 2 z this full mixing only occurs between two particles based on their probability of co location in a time step of size δt and the algorithm is applied to all potentially interacting particle pairs the algorithm has been shown to act as a globally diffusive operator schmidt et al 2018 if the local mixing is modeled as diffusive i e particles move and or collide by brownian motion this means that even if particles are considered dirac delta functions their masses continually change and so the total entropy hd must also change the diffusive nature of the mass transfer may be coupled with random walks to fully flesh out the local hydrodynamic dispersion tensor so between diffusive mass transfer random walks and local advection the mass experiences the green s function of transport which may be complex due to variable velocities e g benson et al 2019 a key feature of this algorithm is that the number of particles encodes the degree of interparticle mixing which is separate from but related to the spreading of a diffusing plume benson et al 2019 schmidt et al 2018 because fewer particles implies greater average separation the mixing lags behind the spreading of particles to a greater degree as n is decreased paster et al 2014 however it remains to be shown that this effect is reflected in the entropy of a conservative plume to briefly review the mass transfer pt method calculates the probability of collision between particles this probability becomes a weight of mass transfer benson and bolster 2016 schmidt et al 2018 with the understanding that co located particles would be well mixed as a result for the ith particle the mass of a given species mi satisfies to first order 16 m i t δ t m i t j 1 n 1 2 m j t m i t p i j for i 1 n for local fickian dispersion each particle pair s collision probability is given by benson and meerschaert 2008 17 p i j δ s 8 π η d i j δ t d 2 exp r 2 8 η d i j δ t where δs is the particle support volume dij is the average d between the i and j particles r is the distance between the i and j particles and 0 η 1 is the fraction of the isotropic diffusion simulated by interparticle mass transfer the remainder 1 η is performed by random walks here we use the arithmetic average d i j d i d j 2 it should be noted that the δs does not actually change the calculation of mass transfer because the probabilities are normalized namely 18 j 1 n p i j 1 for all i 1 n if p is constructed as a matrix this amounts to row normalization which does not guarantee columns summing to unity in practice an average of row and column normalization is used to construct a symmetric and probability mass preserving matrix in which p i j p j i the calculated probabilities are normalized to sum to unity because mass must either move to other particles when i j or stay at the current particle when i j when particle masses are not all the same and particles are close enough to exchange mass then the masses must also change and therefore the entropy h d i 1 n m i ln m i must change as discussed in the introduction in the presence of dispersion gradients particles undergoing random walks must be pseudo advected by the true velocity plus the divergence of dispersion in contrast the probabilities in eq 16 should automatically adjust for these gradients because the probability of mass transfer is not given solely by d at the ith particle transfer is automatically lower in the direction of lower d as opposed to the random walk algorithm which moves a particle with a magnitude given by the value of d at the particle and hence moves it too far into regions of lower d therefore while the mass transfer algorithm has been shown to be diffusive it should also properly solve the ade with its dispersion gradients however this effect has yet to be investigated so we provide evidence via a case study of transport in shear flow in appendix b within eqs 16 18 it appears that the collision probabilities act as a kernel to redistribute mass in other words rather than create a new interpolated concentration function as a convolution of the particle masses the collision probability directly re distributes the particle masses via convolution because the convolution kernel is the collision probability we will refer to eq 17 as the collision kernel several researchers rahbaralam et al 2015 sole mari et al 2017 sole mari and fernàndez garcia 2018 have suggested that the kernel representing the mass transfer should actually be a function of total simulation time and or particle number and local density through the statistics of the particle distribution and not merely the time interval over which the particle undergoes some small scale motions to summarize these authors perform smoothing in order to most closely solve eq 1 i e the case in which mixing and dispersion are both equally modeled by the diffusion term another effect of this operation should be to most closely match the entropy of the perfectly mixed analytic solution of the diffusion equation so we investigate it here recently sole mari et al 2019 showed that mtpt can be generalized so that particles can use a gaussian function kernel other than the particle particle collision probability of eq 17 for the mass transfer in doing so the methodology can be made numerically equivalent to smoothed particle hydrodynamics sph simulations the choice of kernel has an effect on simulation accuracy sole mari et al 2019 which we theorize also changes the entropy or mixing within the simulations specifically for the mixing pseudo reaction we study here sole mari et al 2019 rewrite the mass transfer function 16 in the more general form 19 m i t δ t m i t j 1 n β i j m j t m i t p i j where 20 β i j 2 η d i j δ t h 2 and the expression for pij in eq 17 is also modified by the kernel bandwidth choice 21 p i j δ s 2 π h 2 d 2 exp r 2 2 h 2 the kernel bandwidth h depends at any time on the global statistics of the particle distribution for this reason it is called an adaptive kernel silverman 1986 more specifically we set it as the value that minimizes the asymptotic mean integrated squared error amise of a kernel density estimation the following expression is valid for a density estimation with a gaussian kernel and particles carrying identical masses silverman 1986 22 h de d 2 π d n 2 f 2 d x 1 d 4 where f is the usually unknown true distribution of solute mass for the present diffusion benchmark problem f is a zero mean gaussian with variance 2dt so the density estimation kernel is gaussian with sole mari et al 2017 23 h de 1 06 n 1 5 σ 1 06 n 1 5 2 d t this bandwidth can be used to interpolate the classical pt method for example using a gaussian kernel in eq 10 in the case of mtpt however we do not have a variable density of particles with identical masses but a constant density of particles with variable masses as an approximation we replace the number of particles n in eq 22 with the equivalent value for which the average particle density ρ would be equal in the two cases 24 ρ n f 2 d x which allows us to rewrite expression 22 as an approximation for mtpt 25 h sph d f 2 d x 2 π d ρ 2 f 2 d x 1 d 4 once again because of the simple benchmark problem studied herein there is a very simple solution for the bandwidth because the distribution f at any time is a gaussian with variance σ 2 2 d t furthermore if n particles are placed within an interval of length ω with average spacing ω n 1 ρ which doesn t change significantly during a simulation then the bandwidth reduces to 26 h sph 0 82 σ 4 5 ρ 1 5 0 82 2 d t 2 5 n ω 1 5 we have implemented the adaptive kernels as both the density interpolator ϕ of the classical random walk at any time i e a gaussian kernel with variance h de 2 in eq 10 and also in the mass transfer coefficient 20 and the probability weighting function 21 with bandwidth h sph in the mass transfer algorithm 19 5 results and discussion all simulations use d 10 3 l2t 1 and are run for t f i n a l 1000 arbitrary time units the spatial domain is arbitrary but for the mtpt method we randomly placed particles with zero initial mass uniformly on the interval 5 5 which is approximately 3 5 2 d t f i n a l note that the units are arbitrary but must be internally consistent because of the scale invariance of the solutions to the diffusion equation that follow in 1 d c x t 2 d t 1 2 c x 2 d t 1 2 1 as long as the same units of d say meters and seconds are used for elapsed time and the units of the spatial domain the solutions are universal more extensive discussions of multi scale invariance in 3 d are given by schumer et al 2003 the mtpt method can represent a dirac delta function initial condition by any number of particles here we place one particle at x 0 with unit mass to enable direct comparison of consistent entropy between all methods we chose equivalent average particle spacing and sampling volume of δ v δ x 10 n we investigate the calculation of entropy and dilution indices for 1 the pt method using bins of size δx 2 the pt method using constant size gaussian interpolation kernels 3 the pt method using adaptive kernels with bandwidth given by eq 23 4 the mtpt method using a collision probability kernel size of 4 d δ t and 5 the mtpt method using adaptive kernels with size given by eq 26 with the latter two mass transfer scenarios we also let the proportion of diffusion by mass transfer versus random walks vary and focus on the two cases of η 1 and η 0 1 to see the effect of the collision based versus sph based kernel size 5 1 pt versus collision kernel mtpt first we simulated the classical pt algorithm with concentrations mapped both by binning and by gaussian kernels with fixed variance 2d 1 time unit because the simulations go from t 0 01 to 1000 we chose a kernel size that is too big in the beginning and perhaps too small in the end i e the kernel size is about 1 3 the spread of particles at t 10 the calculated entropies from these simulations were compared to the analytic solution of eq 14 using δ v 10 n and the collision kernel mtpt algorithm outlined in the previous section 4 in these first mtpt simulations we set the proportion of diffusion by mass transfer η 1 in comparison to the other methods the entropy from binned pt concentrations matches the analytical solution very well at early times but significantly diverges later fig 1 the difference between solutions is more obvious when looking at the dilution index e fig 2 the fixed gaussian kernel interpolated concentrations over estimate entropy and mixing at early time because a fixed kernel size is chosen that is typically larger than the actual diffusion distance for small times the mtpt method underestimates entropy at early time relative to the analytic solution of eq 14 because the method by design does not perfectly mix concentrations the random spacings impart regions where the particles are farther apart and in these regions the solutions are imperfectly mixed i e imperfectly diffusive as n gets larger the solution is more perfectly mixed and converges to the analytic diffusion kernel earlier figs 1 and 2 it is also important to note that neither the analytic solution nor the classical pt method represents the entropy of the initial condition correctly the pt method with all n particles placed at the origin still has h d ln n while the entropy of the true dirac delta initial condition is h d 1 ln 1 0 the analytic solution of eq 14 must use a calculation grid with finite δx and sampling volume δv in order for later time entropies to match this must be chosen as the same size as the bins for the pt method i e δ x x max x min n where the extents are chosen to almost surely see all particles in a simulation on the other hand the mtpt method can represent the initial condition in many different ways but here we simply placed one particle at the origin with unit mass while the remaining n 1 particles are placed randomly from the uniform distribution on 5 x 5 with zero mass because of this ic the mtpt method can faithfully represent h d t 0 0 and the effect of this deterministic unmixed ic stays with the simulations for a fair amount of time at later time both the fixed kernel pt and the mtpt methods converge to the analytic solution figs 1 2 at early times however the fixed kernel interpolator overestimates mixing when generating c x t not only with respect to the gaussian solution but also relative to the true initial condition with h d 0 note also that the calculations of consistent entropy hd depend strongly on n but not the dilution index e which accounts for the different sampling support volumes 5 2 adaptive kernel versus collision kernel mtpt we now turn to simulations using adaptive kernels in which the particle particle interaction probability has a time and particle number varying kernel size by placing eq 25 into eq 21 this is predicated on the fact that a finite sampling of independent random variables is often used to create a histogram of those rvs the idea is that a re creation of the histogram should allow each sample to represent a larger domain than just its value and a kernel should be assigned to spread each sample value in the case of independent mass preserving random walks the idea is clearly sound for a delta function initial condition each particle is a sample with a pdf that is the green s function so that each particle s position could be viewed as a rescaled green s function which is approximated by the histogram itself the rescaling depends on the actual green s function which may vary in time and space and the particle numbers for independent particles undergoing brownian motion the green s function is gaussian with variance 2dt and the kernel is shown to be gaussian with zero mean and standard deviation given by eq 25 it is less clear whether this kernel should be used to represent the particle particle interaction probability first the global statistics are not important to local mixing or reactions i e a paucity of a reactant in one location is not informed by a wealth of reactant outside of the diffusion distance in one timestep second the masses present on particles are anything but independent as they depend strongly on their near neighbors third the kernels are designed to create a maximally smooth pdf based on random samples but much research has shown that small scale fluctuations are the most important driver of mixing and reaction rates thus any kernel that smooths the local fluctuations is artificially increasing mixing and resulting reaction rates however much of this discussion is pure speculation so we implement the kernel functions here as both interpolants of independent random walks and as weights in the mixing function for brevity and consistency with the previous results we only show simulations with n 300 and n 30 000 intermediate numbers track the same trends for both particle numbers the kernel interpolated classical pt method has consistent entropy and dilution indices that match the diffusion equation analytic solution quite nicely blue circles figs 3 and 4 the kernels perform exactly as designed for optimally interpolating the pdf of independent randomly walking particles for the lower particle number 300 the adaptive kernels in the mtpt algorithm match the analytic solution more closely than the collision kernel at early times compare figs 1 and 3 the analytic solution assumes perfect mixing i e local mixing and spreading are equal and characterized by the single coefficient d 5 3 partitioning of local mixing and random walk spreading recent studies benson et al 2019 schmidt et al 2018 that employ the collision kernel for mass transfer have shown that mixing can be simulated as a smaller scale and smaller magnitude process than solute spreading this concept relies on the fact that upscaling by volume averaging and or projection of 3 d concentrations to 2 d or 1 d replaces multi valued concentrations with an average e g taylor 1953 the spreading or warping of a concentration interface is a faster process than actual mass transfer across the interface this is exemplified by miscible displacement of one fluid by another in laminar poiseuille flow in a tube where higher velocity in the center warps an initially sharp interface much faster than molecular diffusion actually mixes the fluids when volume averaged to 1 d taylor 1953 the spreading is given by a macro dispersion coefficient that grows in time to an asymptotic value benson et al 2019 showed that taylor s macrodispersion can be performed by random walks which causes particles to spread apart on average while true mixing by molecular diffusion is performed by inter particle mass transfer using the physics based collision kernel it is unclear whether using the adaptive sph kernels as defined in eq 26 can achieve the same effect given that the particle spreading is part of the evaluation of the kernel size for smaller scale mixing to investigate this effect we chose a simple system in which the macrodispersion portion of d was the largest part and also constant over time by setting a constant mixing proportion η 0 1 and re ran the mtpt simulations for n 300 and n 30 000 only the dilution indices are shown here in fig 5 the differences between results for the collision kernel are small while the adaptive kernel shows significantly decreased mixing this increased error for the adaptive kernel when η 1 can be explained as follows expression 26 was obtained from 25 by assuming that the spatial distribution of the solute f is represented by a gaussian function with variance 2dt while this is approximately true for η 1 the micro scale variability generated when η 0 1 see fig 7a suggests that f may not even be continuous and twice differentiable to start with which is a requisite for expression 25 to be valid nevertheless if 2 f 2dx was to be numerically estimated each time step such as in sole mari and fernàndez garcia 2018 it would be much higher than for a gaussian f with variance 2dt because of the strong small scale concentration variations suggesting that the truly optimal adaptive kernel obtained from eq 25 in this case would be much smaller than eq 26 5 4 distributional entropy as noted earlier particle simulations display greater entropy with an increasing number of particles e g figs 1 and 3 in a similar way that the consistent entropy is related to classically defined inconsistent entropy for a continuous rv by adding the sampling portion h c ln δ v h i the portion of the entropy of a discrete rv can be partitioned into particle number and underlying structure of the pmf h pmf ln ω n h d using this adjustment the amount of mixing given by the rate of convergence to the gaussian between simulations with different particle numbers can be compared fig 6 here we ran mtpt simulations using the collision kernel with particle numbers in the set 100 300 1000 3000 10000 30000 for smaller n the ensemble average of up to 20 realizations is used because of differences between individual runs quite clearly the smaller particle numbers experience delayed convergence to the well mixed gaussian this is a feature of the mt algorithm that is usually reflected in reduced reaction rates but a simple measurement of the reduced entropy creation rate with smaller particle numbers is a sufficient demonstration of suppressed mixing it is also instructive to inspect the plots of the calculated pmfs and pdfs from the η 0 1 simulations fig 7 the collision kernel mtpt method is notable because the degree of mixing and the shape of the plume are somewhat independent random walks may place particles with different masses in arbitrarily close proximity and some time must elapse before local mixing equilibrates those masses e g fig 7a the result is the mass or concentration at any single position in 1 d space possesses substantial variability this feature concentration fluctuations at any point in space has been exploited to perform accurate upscaling of transport and reaction in heterogeneous velocity fields benson et al 2019 cirpka and kitanidis 2000a 2000b dentz et al 2000 dentz and carrera 2007 on the other hand the fixed kernel interpolation of classic pt methods replaces this concentration variance at every location with concentration variability in space see blue circles in fig 7b 6 computational entropy penalty numerical models provide discrete estimates of dependent variables that may be continuous functions of time and space often the functions are non negative and can be normalized to unit area so that they are pdfs therefore the underlying true pdf has a certain entropy and the sampling or computational procedure used to approximate these functions adds some artificial entropy because of the information required by the discretization one desirable trait of a model is a parsimonious representation of the true physical process i e fewer model parameters are preferred at the same time a more straightforward and accurate computational process is also preferred considerable attention has been paid to parsimonious few parameter models but less attention has been paid to model computational requirements eq 7 shows that if a true pdf can be estimated via very few sampling points or nodes there is less additional entropy incurred in the calculation that is to say if two models with the same parametric parsimony yield equivalent estimates of the underlying true dependent variable then the model that estimates the pdf with the coarsest sampling or least computationally intensive structure is preferred from an entropic standpoint augmenting the kullback leibler inconsistent representation of model entropy with the consistent entropy appendix a allows us to compare the discrete pmfs obtained from computational approximation with the underlying pdf and ultimately results in the computational information criterion comic as a natural extension of akaike s information criterion akaike 1974 1992 to emphasize the influence of computational entropy we illustrate two examples here by estimating a true diffusion given by a gaussian with variance 2dt by several numerical calculations with zero adjustable parameters i e d is a known parameter 6 1 finite difference example for simplicity we set δ v δ x ω n for a fixed domain ω and n nodes and then compared the numerical estimation of the green s function of the 1 d diffusion equation given by implicit finite difference fd models with different discretizations δx 0 4 0 12 0 04 0 012 0 004 0 0012 0 0004 other numerical parameters were held constant including ω 6 6 d 10 3 and δ t 0 05 we use all of the data from each model to calculate the mean sse i e the mean sse is independent of subsampling so here n n clearly a smaller δx provides a better estimate of the analytic solution of a gaussian with variance 2dt but at what cost do 100 nodes suffice a million because there are no adjustable parameters the aic which is given in terms of the log likelihood function aic 2 ln sse n is a decreasing function of the number of nodes n fig 8 a if however one factors in the penalty of ln δv there is an optimal tradeoff of accuracy and computational entropy at n 3000 at almost every time step fig 8b fewer nodes are not sufficiently accurate and more nodes are superfluous for this particular problem as shown by plotting the relative fitness criteria aic versus comic for each discretization at some time fig 8c four important points regarding the comic immediately arise 1 a model is typically sampled at a finite and fixed number of data measurement locations we also sampled the many fd models and analytic solution at 15 randomly chosen measurement points common to all simulations and found nearly identical albeit more noisy results however we have not yet investigated the effect of additional sample noise on discerning the optimal discretization 2 the aic was derived with the assumption that the number of sample points and computational burden of models is identical and do not contribute to the relative aic often the common factors are eliminated from the aic and some arbitrary constants are also added with no effect on relative aic in considering the comic however the choice of likelihood function and inclusion of constants may change the optimal model so care in the choice of aic is required 3 the numerical solutions at some final time t are actually conditional densities of the joint densities c x t so that increased number of timesteps should also increase computational entropy i e δt contributes to the multidimensional δv see appendix a here we held the time step size constant for all fd models so that the temporal sampling t j δ t has no effect on the relative entropy 4 we used a constant spatial discretization δ v δ x to simplify the comparative kullback leibler measures some models use variably spaced grids so the resulting computational entropy is more complicated than we investigate here 6 2 mass transfer particle tracking examples regarding this last point stated above for finite difference models the main thrust of this paper is the entropy of particle methods the particles are typically randomly spread in space so that a constant δv is not possible however using the inconsistent entropy isolates the correspondence of the n particles to an underlying pmf e g fig 6 in the case of perfectly mixed fickian diffusion this enables a direct comparison of the fitness of the particle methods to simulating diffusion and the correction term ln ω n is the entropy associated with computation we use this correction in analogy with the fd results above to assess the entropic fitness of mtpt methods and test several intuitive hypotheses first prior research has shown that fewer particles in the collision kernel mtpt method represent poorer mixing hence poor fitness when modeling perfectly mixed fickian diffusion in the absence of mixing by random walks i e η 1 we hypothesize that adding more particles will yield an improved average sse but that the overall model entropic fitness measured by a smallest comic reaches a maximum at some point indeed a statistically significant minimum is found between n 1000 and n 10 000 particles with an estimated minimum at 3000 particles fig 9 a on the other hand the adaptive sph kernel is constructed to best match fickian diffusion by everywhere adjusting for particle density and number therefore we hypothesize that the model entropic fitness will be relatively stable across a broad range of particle numbers this is also found to be true in simulations fig 9b and comic fitness only suffers in a significant way for n 100 finally in contrast to the collision kernel for η 1 shown in fig 9a we hypothesize that splitting the diffusion between mass transfer and random walks will improve for this example the fitness of smaller particle number simulations by eliminating persistent mixing gaps where large random distances between particles prevents convergence to a well mixed gaussian however at some point the model sse will not improve with the addition of more particles because the noise of concentrations around the gaussian will be saturated see e g fig 7 a fig 9c reveals exactly this behavior in the comic adding random walks decreases the optimal number of particles to 300 to summarize the mtpt entropic fitness for simulating fickian diffusion 1 for the sph kernel small particle numbers are sufficient and equally fit by design 2 similarly to the fd method the collision kernel has a minimum comic around 3000 particles and 3 with the collision kernel partitioning diffusion by mass transfer and random walks promoted mixing and fitness for smaller particle numbers 300 and clearly shows the superfluous nature of large particle numbers for simulating fickian diffusion 7 conclusions classical pt methods do not track entropy until a concentration function is mapped from particle positions the choice of bins or kernels for this mapping cannot be arbitrary as the choice directly changes the calculated entropy or degree of mixing of a moving plume the newer mass transfer method directly simulates entropy without any such mapping because particle masses continually change and does so with several beneficial features first the zero entropy initial condition and its effect on the early portions of a simulation are accurately tracked second the particle number is an integral part of the mixing rate of a plume higher particle numbers simulate more complete mixing at earlier times as shown by the convergence of entropy to that of a gaussian the mtpt method can use physically based particle collision probabilities for the mixing kernel or adaptive kernels dictated by the sph algorithm these adaptive kernels more closely match the analytic gaussian solution s entropy when solving the diffusion equation in one pass i e all mass transfer given by the diffusion coefficient however when the diffusion dispersion is split between local inter particle mixing and spreading by random walks the adaptive kernel entropies change substantially and do not match the gaussian solution for small particle numbers the collision kernel does not generate the same effect we suggest that the adaptive sph kernels only be used to solve locally well mixed problems i e where the dispersion tensor represents both mixing and dispersion equally whereas the collision kernel may partition mixing and spreading as the physics of the problem dictate benson et al 2019 the fact that discrete or discretized approximations to real continuous functions carry a sampling or computational entropy means that metrics which compare different simulations based on information content must be penalized by that computational information for this purpose we define a computational information criterion comic based on akaike s aic that includes this penalty we show how a finite difference solution of the 1 d diffusion equation has a well defined optimal solution of about 3000 nodes in terms of combined accuracy and computational requirements when the mtpt is used to simulate fickian diffusion these simulations show that the collision kernel also has a minimum comic around 3000 particles but the adaptive sph kernel by design is fit over a large range of particle numbers adding some diffusion by random walks makes the collision kernel a better fit for smaller particle numbers n 300 and shows that simulations of fickian diffusion for large number of particles is computationally superfluous we anticipate that this new entropy based fitness metric may discount some overly computationally intensive models that previously have been deemed optimal in terms of data fit alone credit authorship contribution statement david a benson conceptualization methodology software writing original draft writing review editing stephen pankavich formal analysis writing original draft writing review editing michael j schmidt methodology software writing review editing guillem sole mari methodology software writing original draft writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we thank the editor reviewers daniele pedretti and olaf cirpka and one anonymous reviewer for extremely helpful comments this material is based upon work supported by or in part by the us army research office under contract grant number w911nf 18 1 0338 the authors were also supported by the national science foundation under awards ear 1417145 dms 1614586 dms 1911145 ear 1351625 ear 1417264 ear 1446236 cbet 1705770 and dms 1911145 the first author thanks the students in his class gegn 581 analytic hydrology for inspiring this work two matlab codes for generating all results in this paper one finite difference and one particle tracking are held in the public repository https github com dbenson5225 particle entropy appendix a computational information criterion and maximum likelihood estimators the ultimate goal of this section is to derive an extension of akaike s an information criterion aic akaike 1974 that establishes an objective function to be optimized in order to select a model and a minimal number of parameters that best fits a given set of data such an extension must incorporate the results of section 2 which introduced a consistent notion of entropy that allows one to compare the relative entropies of a continuous pdf with that of a discrete pmf approximation of course this discussion will first require some background knowledge of the kullback leibler divergence the basic formulation of the aic and maximum likelihood estimators each of which we provide below a1 kullback leibler divergence we begin with a review of the kullback leibler divergence and its extension to the inconsistent entropy for continuous rvs following kullback 1968 we first consider the likelihood of two competing hypotheses h 1 and h 2 given some knowledge of the probability of an event x and note that bayes theorem provides a representation for the conditional probability of each hypothesis given x namely 27 p h i x p x h i p h i p x h 1 p h 1 p x h 2 p h 2 for i 1 2 next this statement can be generalized to continuous rvs so that if h 1 and h 2 now represent the events that a random variable x comes from a distribution represented by the pdfs f 1 x and f 2 x respectively then the conditional probability of each hypothesis given that x x is now 28 p h i x f i x p h i f 1 x p h 1 f 2 x p h 2 for i 1 2 taking logarithms and rearranging this expression then yields 29 ln f 1 x f 2 x ln p h 1 x p h 2 x ln p h 1 p h 2 the right side of this equality is a measure of the difference between the logarithm of the odds in favor of hypothesis 1 versus hypothesis 2 after the observation of x x relative to before this observation in other words this difference represents exactly the information contained within the observation that x x and the left side of the equality often referred to as the log likelihood ratio is the information in favor of h 1 and against h 2 to obtain the mean value of this we merely integrate over all possible observations and against the pdf f 1 x which gives 30 i f 1 f 2 f 1 x ln f 1 x f 2 x d x this quantity is defined to be the kullback leibler divergence kld and represents the entropy of f 1 relative to f 2 notice that this expression can also be separated into two integrals so that 31 i f 1 f 2 f 1 x ln f 1 x d x f 1 x ln f 2 x d x and the former of these two integrals is directly related to the inconsistent entropy of eq 3 next we will use the kl divergence to establish the aic a2 akaike information criterion aic the aic was originally established to select a model and associated parameter values that are a best predictor of potential future data based on some set of given data it is well known that adding parameters will reduce data model misfit for a single set of observations but the added parameters will often cause worse fits for newly collected data konishi and kitagawa 2008 in particular consider a variety of different models defined by distinct parameter vectors θ and corresponding pdfs h y θ arising from data values y 1 y n along with a single vector of true parameter values θ 0 with pdf g y h y θ 0 the problem of interest is how to optimally select both a number of model parameters k and their associated values θ to best approximate θ 0 given that we have incomplete knowledge of the latter quantity in fact the information provided to make this decision arises only from the given data which is merely a collection of n independent sample values each representing a realization of a random variable y with pdf g y ultimately the aic yields an approximate criterion for the selection of parameters which entails minimizing the quantity 32 2 i 1 n ln h y i θ 2 k over the number of parameters k where θ is the maximum likelihood estimate for θ furthermore this process corresponds to minimizing the underlying entropy among such models in the context of computing concentrations as in previous sections we could consider a function c x t for which we have a coupled set of observed data say x i c i i 1 n which represents values of the concentration measured at differing spatial points and at a fixed time t t here the function c can be a solution to a pde e g eq 1 and may depend upon some parameters θ for instance d in eq 1 of course the parameter values are unknown and must be inferred from the given data which may contain some noise due to errors in measurement we can then define yi to be the corresponding error between c and the measured data ci for every i 1 n and consider the associated pdf for each of these errors denoted h y θ additionally θ 0 represents the true parameter values so that the underlying pdf can be represented by g y h y θ 0 the aic will then provide a criterion to select the parameter values and number of parameters that is a best predictor of any future concentration data thereby selecting a specific model the selection criterion is based on the entropy maximization principle which states that the optimal model is obtained by maximizing over the given data on which θ depends the expected value of the log likelihood function namely 33 s g h θ g y ln h y θ d y this quantity is not a well defined i e strictly positive counterpart to entropy as discussed in the main text thus it is typically implemented in a relative sense among models using the kullback leibler divergence of g and h given by 34 i g h θ g y ln g y h y θ d y s g g s g h θ as noted in the previous section this can be interpreted as a measurement of the relative similarity between the probability distributions g and h as akaike 1974 notes maximizing the expected log likelihood above is equivalent to minimizing i g h θ over the given data of course since θ 0 is unknown and g y h y θ 0 depends upon knowledge of the true parameter values we cannot directly compute i g h θ instead this quantity must be suitably approximated following akaike 1974 1992 when the number of data points n is sufficiently large an approximation of the kld using the fisher information matrix can be utilized and classical estimation techniques imply 35 i g h θ i 1 n ln g y i i 1 n ln h y i θ k where k is the number of estimated parameters within θ and θ is the maximum likelihood estimate for θ here k appears in order to correct for the bias introduced by approximating the true parameter values with their corresponding maximum likelihood estimates finally since the sum involving g is constant for any choice of model parameters it can be omitted in computing the minimization therefore the aic may be defined with a scaling factor of two as in akaike 1974 by 36 aic 2 ln maximum likelihood 2 k or in the notation described herein 37 aic θ 2 i 1 n ln h y i θ 2 k it is this quantity that one wishes to minimize over k where θ may depend upon k in order to select the best model approximation to g and this is the basis of our departure prior to formulating an extension of the aic we provide a brief review involving maximum likelihood estimators a3 maximum likelihood estimators mles though we have not mentioned the process of obtaining the maximum likelihood estimates θ described above useful discussions of mles for models with unknown structure are provided by hill and tiedeman 2007 and brockwell and davis 2016 as an example consider the scenario in which the errors between model and observations are independent zero mean gaussians in this case the likelihood function is given by 38 l y θ 2 π n σ θ 1 2 exp 1 2 y t σ θ 1 y where n is the number of observation points σ θ is a covariance matrix of errors that depends upon some unknown parameter vector θ and y is a vector of residuals satisfying y i c i c x i t for i 1 n recall that ci is the measured concentration and c xi t represents the concentration at the spatial data point xi and time t given by the pde solution therefore the log likelihood function is 39 ln l n 2 ln 2 π 1 2 ln σ 1 2 y t σ 1 y in practice the observation errors are often assumed to be independent and σ is diagonal furthermore the variance of each observation is often unknown or estimated during the model regression although numerous approximations can be applied see chakraborty et al 2009 for assumed concentration errors in particle tracking so it is assumed that σ depends only upon a single variance parameter denoted by σ 2 and thus satisfies σ σ 2 i the last term in eq 39 is more conveniently given in terms of the sum of squared errors sse y y y 2 so that 40 ln l n 2 ln 2 π n 2 ln σ 2 n 2 σ 2 sse n because this function should be maximized one takes the derivative with respect to σ 2 and sets it to zero to compute the value of σ 2 at which the maximum occurs this provides an estimator of the observation variance namely σ 2 sse n so that the corresponding maximum is 41 ln l n 2 1 ln 2 π ln sse n because the number of observations is usually fixed the n 2 term is canceled from all terms as maximizing ln l also maximizes 2 n ln l hence the mle is σ 2 sse n and the quantity ln sse n provides a relative estimate for the value of the log likelihood function evaluated at the mle a4 computational information criterion returning to the formulation of the aic for a model of concentration we wish to alter the original derivation so that one may compare a variety of discrete computational models to the true continuous model using their relative entropy to evaluate their similarity in such a case we would like to use the kld to measure the relative entropy of the error distribution between the observed data and the solution of the pde model but we must also correct this criterion for the fact that our discrete approximations to the pde solution change with the resolution of the chosen numerical method which is often described by a single parameter n for instance this parameter can represent the number of particles n in a stochastic particle method or identify the spatial grid size δ x ω n in a finite difference method ultimately we wish to establish a criterion to select the best of these approximate models depending upon the value of n as before letting xi ci for i 1 n represent the given pairs of concentration data and c x t denote the true solution to the pde model we can describe the statistical model incorporating measurement error by c i c x i t ϵ for i 1 n where t is a known measurement time and ϵ h y θ is a random variable with distribution h that encodes each of the associated random errors because we often do not possess an analytic solution for c it is necessary to approximate the pde solution at time t t with a number of suitable numerical models the solutions of which we denote by cn x with the model of interest changing with the value of n hence discrete approximation error in addition to measurement error from the data must be incorporated into the model selection criterion because the kld can account for the latter quantity we can establish a computational information criterion merely by correcting the aic by the difference in entropy between the pde solution and the numerical approximation in this way if hrel f 1 f 2 represents the relative entropy between f 1 and f 2 then the new information criterion can be expressed as comic θ n h r e l c c i h r e l c n c the first relative entropy here is given by the aic while the second is merely the difference between the discrete entropy of the approximate solution and the inconsistent entropy of the pde solution or h d h i where these quantities are given by eqs 2 and 3 respectively in section 2 a sampling volume was introduced to relate these terms using the consistent entropy of eq 6 and we found see fig 6 with δ v ω n h d h i ln δ v using this we can finally define an adjusted criterion to the aic which we name comic or the computational information criteria given by 42 comic θ δ v 2 i 1 n ln h y i θ 2 k ln δ v from an information theory perspective this computational penalization can also be seen as a limitation on the information content needed to represent the approximate solution cn x assuming the selected numerical method converges to the underlying pde solution as n the values of c x t can be computed to an arbitrarily large degree of precision by merely choosing n sufficiently large in the computational model however in doing so one must continue to store increasingly large amounts of information to gain smaller and smaller levels of accuracy thus a trade off results between the desired gain in precision and the stored information content and the comic provides an efficient criterion for penalizing such considerations to select a parsimonious and computationally efficient low information content model in order to focus on the computational implications of this adjustment to the model selection criterion we consider the case in which the errors between model and observations are gaussian with variance σ 2 as in the example illustrated within the previous section and assume that no other parameters e g d require estimation in this case the log likelihood function evaluated at the maximum likelihood estimate is proportional to the log of the average sum of squared errors sse given by eq 41 upon removing constants the form of the comic becomes 43 comic δ v 2 ln sse n ln δ v where 44 sse i 1 n c i c n x i 2 for numerical models with equivalent sse their measure of distributional entropy is the same but their computational entropy would be ln δ v so that the model fitness should be adjusted by this difference in information content appendix b effect of d on the mass transfer algorithm we illustrate the effect of spatially variable d in simple 2 d shear flow borrowing the parabolic velocity profile v y 0 and v x y 2 b y of hagen poiseuille flow the domain used here is 0 x 400 0 y 1 with concentrations initially zero everywhere except for a strip 90 x 110 with concentration 1 20 i e initial mass 1 the x domain is periodic so particles that exit at x 400 are re introduced at x 0 we show a scenario with heterogeneous and anisotropic diffusion d α l v x 0 0 α t v x with longitudinal and transverse dispersivities α l 10 2 α t 10 3 dispersive transport was simulated for t 500 with timestep size δ t 1 either solely by mass transfer or solely by random walks because the mass transfer algorithm can move mass among all particles in the domain a total of 20 000 particles were placed in the 400 1 domain with an average of 100 particles in the initial non zero concentration strip this gives plenty of clean particles on either side of the strip pure random walks without the drift correction term migrate all particles including those with mass to the lower d regions fig 10 a the drift correction eliminates the lateral bias fig 10b and e the mass transfer algorithm has no apparent bias or need for d correction fig 10c and f as an aside the mass transfer method quite clearly shows the regions of greatest and least shear and mixing fig 10c supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103509 appendix c supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
519,water balance models are commonly employed to improve understanding of drivers behind changes in the hydrologic cycle across multiple space and time scales generally these models are physically based a feature that can lead to unreconciled biases and uncertainties when a model is not encoded to be faithful to changes in water storage over time statistical methods represent one approach to addressing this problem we find however that there are very few historical hydrological modeling studies in which bias correction and uncertainty quantification methods are routinely applied to ensure fidelity to the water balance importantly we know of none aside from preliminary applications of the model we advance in this study applied specifically to large lake systems we fill this gap by developing and applying a bayesian statistical analysis framework for inferring water balance components specifically in large lake systems the model behind this framework which we refer to as the l2swbm large lake statistical water balance model includes a conventional water balance model encoded to iteratively close the water balance over multiple consecutive time periods throughout these iterations the l2swbm can assimilate multiple preliminary estimates of each water balance component from either historical model simulations or interpolated in situ monitoring data for example and it can accommodate those estimates even if they span different time periods the l2swbm can also be executed if data for a particular water balance component are unavailable a feature that underscores its potential utility in data scarce regions here we demonstrate the utility of our new framework through a customized application to the laurentian great lakes the largest system of lakes on earth through this application we find that the l2swbm is able to infer new water balance component estimates that to our are knowledge are the first ever to close the water balance over a multi decadal historical period for this massive lake system more specifically we find that posterior predictive intervals for changes in lake storage are consistent with observed changes in lake storage across this period over simulation time intervals of both 6 and 12 months in additional to introducing a framework for developing definitive long term hydrologic records for large lake systems our study provides important insights into the origins of biases in both legacy and state of the art hydrological models as well as regional and global hydrological data sets keywords hydrologic cycle large lakes statistical modeling bayesian inference water balance 1 introduction hydrological models that simulate and forecast the water balance across a variety of space and time scales are needed to facilitate water resources management planning and ultimately to ensure human and environmental health vörösmarty et al 2000 pekel et al 2016 this need is particularly pronounced in regions where rapid population growth coincides with changes in the spatiotemporal distribution of fresh water and where the sustainability of future water supplies is uncertain schewe et al 2014 to address this need hydrological models need to clearly differentiate components of the hydrologic cycle that are often confounded including for example evapotranspiration and irrigation water demand and to quantify changes in those components over time nijssen et al 2001 kebede et al 2006 raes et al 2006 li et al 2007 gronewold and stow 2014 global continental and basin scale water balance modeling research typically focuses on improving representation of terrestrial and atmospheric physical processes collectively governing precipitation evapotranspiration and streamflow crow et al 2008 kim and stricker 1996 milly and dunne 2017 senay et al 2011 vörösmarty et al 1998 this body of research while providing foundational hydrologic data for much of the planet s land surface rarely explicitly resolves mass and energy fluxes over large freshwater surfaces makhlouf and michel 1994 xu and singh 1998 arnell 1999 guo et al 2002 put differently we find that the primary physical processes driving the water balance of large lakes including over lake evaporation i e turbulent heat fluxes over lake precipitation and predominant channel lake inflows and outflows are represented poorly if at all in large scale terrestrial land surface models and corresponding data sets there are however several state of the art models that represent these processes specifically for lakes and for their interactions with the atmosphere one particular example is wrf lake a one dimensional 1 d physically based lake model that in previous studies gu et al 2013 xiao et al 2016 has been coupled with the weather research and forecasting wrf model a similar one dimensional scheme has been applied to resolve large lakes holman et al 2012 notaro et al 2013 within the abdus salam international center for theoretical physics regional climate model ictp regcm4 there have also been significant evolutions in three dimensional lake models including the transition of the national oceanic and atmospheric administration noaa great lakes operational forecasting system glofs from the princeton ocean model pom to the finite volume community ocean model or fvcom for details see kelley et al 2018 as well as the operationalization of the nucleus for european modelling of the ocean nemo model dupont et al 2012 within environment and climate change canada s water cycle prediction system wcps described in durnford et al 2018 while many of these and other existing lake models have been found to represent key physical processes they are rarely if at all evaluated within the context of the overall hydrologic cycle and reconciliation of changes in lake storage over multiple time periods this missing piece of context in most lake modeling studies limits the extent to which scientists and practitioners along with the general public understand modes of variability in the storage of nearly all of the earth s fresh unfrozen surface water gibson et al 2006 swenson and wahr 2009 xiao et al 2018 filling this gap requires a focused effort on lake water balance models that resolve physical processes governing lake storage at appropriate space and time scales piper et al 1986 nicholson et al 2000 gibson et al 2006 peng et al 2019 here we address this gap by introducing a framework that employs a novel formulation of a lake water balance model in which historical monthly water balance components are estimated through bayesian inference gelman et al 2004 empirical data sets and historical model simulations if available are incorporated into the framework through likelihood functions and prior probability distribution functions this approach leads not only to probabilistic historical water balance component estimates that preserve spatial and temporal correlation across a lake or lake system but also to quantification of bias and uncertainty in the models and data sets that had previously been developed for that lake it is informative to note that a preliminary prototype of our framework was applied in a study assessing rapid water level changes between 2013 and 2015 on lakes superior michigan and huron gronewold et al 2015 2016 an analysis of an evolution of that prototype which focused primarily on model selection was presented in smith and gronewold 2018 the framework we present here is differentiated from the prototype in gronewold et al 2016 by among other features two key improvements the first is a computationally efficient filtering method which we periodically refer to as a rolling inference window that facilitates inference over multi decadal periods the second is a skill assessment that reflects both the relative homoscedasticity of model residuals as well as the extent to which the model closes a lake s water balance over consecutive multi month time periods we also acknowledge that there are previous studies utilizing similar statistical methods such as those that debias continental remote sensing data pan and wood 2006 coccia et al 2015 these studies however are typically focused exclusively on land surface processes and do not adequately resolve large lakes the framework we develop here therefore is further distinguished by its focus on large lakes including its ability to model multiple connected lakes in series our representative application also represents the first time a water balance model has been applied systematically to the entire laurentian great lakes system the largest system of lakes in earth that closes the water balance over multiple time periods while reconciling discrepancies between alternate measurements and model simulations of the same water balance component the results of our application serve as both a solution to a long standing water resources management problem for the great lakes and as a stepping stone towards solving similar problems in large lake systems around the world 2 methods in the following sections we first section 2 1 provide an overview of the generic formulation of our new framework we then section 2 2 describe the customization of our framework to the water balance of the entire laurentian great lakes system section 2 2 also includes a description of our approach to evaluating the new framework using the results from our application to the laurentian great lakes 2 1 the model we developed our modeling framework by first modifying the conventional formulation of a lake water balance model to represent changes in storage over a time window of w months 1 δ h j w h j w h j i j j w 1 p i e i r i i i q i d i ϵ i where δh j w represents the true change in lake storage over a period of w months starting with month j hj and h j w represent true monthly average lake water levels in mm at the beginning of months j and j w respectively j 1 t w 1 and t is the total number of months over which the model is run our use of i and j as month number indices within the context of eq 1 accommodates this rolling multi month window approach the value of the month index j in eq 1 can not by definition exceed t w 1 we index monthly water balance components outside of the context of eq 1 using t 1 t the true values for monthly water balance components expressed in mm over a lake surface area in eq 1 include over lake precipitation p over lake evaporation e lateral tributary runoff r inflow from an upstream channel i discharge through a downstream channel q and the total of interbasin diversions and consumptive uses d the model also includes a process error term ϵ to account for potential sources of water balance variability not explained by components p e r i q and d alone including for example thermal expansion glacial isostatic rebound and groundwater fluxes quinn and guerra 1986 mainville and craymer 2005 these terms could be added to eq 1 on a case by case basis depending on whether they are expected to be significant probabilistic estimates of each water balance component in eq 1 are inferred in a bayesian framework press 2003 gelman et al 2004 in which prior probability distributions and likelihood functions are parameterized using legacy models and data sets as well as expert knowledge and opinion specifically for prior probability distributions 2 1 1 likelihood functions the likelihood function for the change in storage within a given lake over a period of w months is 2 y δ h j w y h j w y h j n δ h j w τ δ h w in which the observed change in storage y δh starting in month j and across a rolling window of length w is the difference between water level measurements yh at the beginning of months j w and j we model this value with a normal distribution with mean δh j w and precision τ δh w this approach allows for an explicit representation of uncertainty in water level data that can be differentiated from uncertainty in water balance component estimates it is informative to note that rather than using variance σ 2 we parameterize normal distributions using precision τ 1 σ 2 following conventional practice for bayesian inference casella and berger 2002 gelman et al 2004 qian et al 2009 we then introduce three new parameters i q and d to represent connecting channel inflows outflows and diversions respectively in units of m3 s we use these units because most water management practitioners are accustomed to recording and assessing these values in m3 s rather than mm over each lake surface we encode the empirical relationship i e the conversion of units between parameters i q and d and respectively i q and d using the surface area of each lake and the number of seconds in a particular month the likelihood functions for water balance components on the right hand side of eq 1 represented collectively by θ p e r i q d is 3 y t θ n n θ t η θ c t n τ t θ n where y t θ n is data source n 1 n for component θ at time step t n is the total number of data sources for that component η θ c t n is the bias of the nth data source in calendar month c and τ t θ n is the data source precision at time step t 2 1 2 prior probability distributions standard formulation in bayesian statistics parameters are frequently modeled with normal probability distributions to support inference across a broad range of potential values alternate probability distribution families can be used however to reflect knowledge or beliefs that a parameter might have a more limited range of values we model e i q and d with normal prior probability distributions 4 π e t n μ e c t τ e c t 2 5 π i t n μ i c t τ i c t 6 π q t n μ q c t τ q c t 7 π d t n μ d c t τ d c t where prior means μ c t and precisions τ c t for each calendar month c are either calculated empirically using historical data records or informed by expert opinion for further reading on objective and subjective prior probability distributions see press 2003 this approach allows for the possibility that lake evaporation can be both positive i e a loss of water from a lake and negative i e when there is warm overlying air and condensation occurs this approach is also suited for relative high values of connecting channel flows q and diversions d future users of our framework could should they choose to do so select different prior probability distribution families such as lognormal for example we divide precision in half i e double the variance for prior probability distributions on over lake evaporation e because for many large lakes evaporation has a very strong historical seasonal cycle with relatively low variability that historical low variability could when quantified in the parameters of a prior probability distribution overly constrain the range of inferred monthly evaporation estimates during a later period particularly in lakes where climate change has led to a systematic increase in evaporation over time milly et al 2008 total lateral tributary runoff values aggregated over a lake basin and over monthly time steps are almost certainly positive and we therefore model r with a log normal prior probability distribution 8 π r t ln μ l n r c t τ l n r c t with prior means μ l n r c t and precisions τ l n r c t these values can be calculated for each calendar month c empirically using historical data records or formulated to represent expert opinion for over lake precipitation we use a gamma prior probability distribution husak et al 2007 9 π p t ga ψ c t 1 ψ c t 2 with shape ψ 1 and rate ψ 2 following thom 1958 defined as ψ c t 1 1 4 ϕ c t 1 1 4 ϕ c t 3 ϕ c t ln μ p c t μ l n p c t ψ c t 2 ψ c t 1 μ p c t where μ p c t is the mean historical precipitation for each month and μ l n p c t is the mean of the logarithm of precipitation for each calendar month c we then model the bias of each contributing data set using normal prior probability distributions 10 π η θ c t n n 0 0 01 with mean 0 and precision 0 01 we note that this precision is equivalent to a standard deviation of 10 and is in units of mm over a lake surface for ηp ηe and ηr while it is in units of m3 s for η q η i and η d users of our framework can customize these prior probability distributions by selecting mean and precision values that are unique to each bias term our representative application in the next section provides an example finally following gelman 2006 we modelled τ δh w and τ t θ using a gamma ga 0 1 0 1 prior probability distribution with shape and scale parameter both equal to 0 1 similarly we constrained water balance model errors to one of 12 values corresponding to each of the 12 calendar months with each error term having a common vague normal prior probability distribution with mean 0 and precision 0 01 11 ϵ t ϵ c t π ϵ c t n 0 0 01 we recognize that bias estimates in our model may be impacted by the classic problem of bias variance tradeoff geman et al 1992 we view implementation of solutions to this problem such as bias variance decomposition valentini and dietterich 2004 as a potential future step in our research 2 2 representative application the laurentian great lakes to demonstrate the utility of our model we customized it to the entire laurentian great lakes system hereafter referred to simply as the great lakes to infer new monthly water balance components for the period 1980 through 2015 the great lakes system fig 1 includes lakes superior michigan huron st clair erie and ontario here we follow conventional practice in great lakes hydrological modeling research at coarse time scales e g monthly by representing lakes michigan and huron as a single lake lake michigan huron given the depth and breadth of the channel i e the straits of mackinac that connects them quinn and edstrom 2000 pietroniro et al 2007 collectively the great lakes represent the largest system of lakes on earth lakes superior and michigan huron alone are the two largest lakes on earth by surface area gronewold et al 2013 we encoded lake to lake connectivity within the great lakes system i e through the st marys st clair detroit and niagara rivers by defining the inflow to each lake through a major connecting channel i as the outflow from the adjacent upstream lake q for example the inflow to lake michigan huron through the st marys river at each monthly time step t is encoded as q s u p t the outflow from lake superior there is no upstream connecting channel flowing into lake superior and therefore in eq 1 for lake superior i s u p 0 we obtained surface areas for each of the great lakes table 1 from the national oceanic and atmospheric administration noaa great lakes environmental research laboratory glerl to calculate relationships between q d q and d for details see hunter et al 2015 2 2 1 model modifications for the water balance of lake st clair we model lake st clair differently from the other great lakes because its surface area is relatively small table 1 and because its hydrologic cycle is dominated by inflow from the st clair river and outflow to the detroit river more specifically rather than differentiating precipitation evaporation and runoff for lake st clair we represented them collectively as a single term commonly referred to as net basin supply n b s p e r we model the nbs for lake st clair using modified versions of eqs 1 and 2 in units of m3 s as follows 12 δ h j w i j j w 1 n b s i q m h u i q i d i ϵ s t c i 13 y δ h j w n δ h j w τ δ h w where q m h u is the outflow from lake michigan huron i e the inflow to lake st clair through the st clair river we model lake st clair nbs values with a normal prior probability distribution 14 π n b s t n μ n b s c t τ n b s c t and a normal likelihood function 15 y t n b s n n n b s t η n b s c t n τ t n b s n where η n b s c t n is the bias of nbs estimate n 1 n in calendar month c n is the total number of nbs data sources and τ t n b s n is the precision of each data source we then adapt the prior probability distributions from eqs 10 and 11 for nbs data bias and lake st clair model error such that the prior precision for each τ 0 0625 1 σ 2 is equivalent to a standard deviation σ of 4 m3 s or roughly 10mm of water on the surface of lake st clair table 1 over the course of a month 16 π η n b s c t n n 0 0 0625 17 π ϵ s t c n 0 0 0625 2 2 2 data for application to the great lakes we obtained beginning of month lakewide average water level data yh for each of the great lakes as well as historical records of interbasin diversions y d channel flows y q and estimates of lake st clair s net basin supply from the coordinating committee on great lakes basic hydraulic and hydrologic data ccglbhhd for further reading on the ccglbhhd see gronewold et al 2018 the water level data we obtained from the ccglbhhd is derived from water level measurements at gauges located around the coastline of each of the great lakes that are maintained by both the noaa national ocean service center for operational oceanographic products and services noaa nos co ops and the canadian department of fisheries and oceans canadian hydrographic service dfo chs we recognize that alternate sources of water level information are available including those based on satellites and other remote sensing products alsdorf et al 2001 crétaux et al 2011 schwatke et al 2015 for this application we utilize the relatively robust network of great lakes water level gauging stations synthesized in the ccglbhhd records and leave assimilation of remotely sensed water levels to future research on either the great lakes or other large lake systems similarly we obtained data on diversions into out of or within each lake basin from the ccglbhhd including the ogoki river and long lac diversions into lake superior the chicago river diversion out of lake michigan huron and the welland canal that runs parallel the niagara river fig 1 we then obtained two sets of connecting channel flow data the first y q 1 includes estimates for each of the great lakes connecting channels derived by the ccglbhhd using a variety of standard methods these methods include the aggregation of discrete flow measurements through dams and marine navigation locks and the application of stage fall discharge equations the second set of connecting channel flow estimates y q 2 is based on acoustic doppler velocity meters advms located at international gauging stations igs maintained through a partnership between the united states geological survey usgs and water survey canada wsc these igs measurements are available only for the st marys st clair and detroit rivers we then obtained a set of data for over lake precipitation over lake evaporation and runoff from the noaa glerl great lakes monthly hydrometeorological database or glm hmd described in hunter et al 2015 the glm hmd utilizes a suite of models and interpolation schemes to generate 1 dimensional estimates of water balance components over the land and lake surfaces of each of the great lakes more specifically over lake precipitation estimates in the glm hmd are based on thiessen weighting croley and hartmann 1985 of land based meteorological station data for further discussion see holman et al 2012 over lake evaporation simulations in the glm hmd are derived from the legacy large lakes thermodynamics model lltm which utilizes wind speed dew point cloud cover and lake surface temperature to simulate heat exchange and ice cover across the lakes croley 1989 1992 finally runoff estimates in the glm hmd are derived from an area ratio based interpolation of usgs and wsc streamflow gages across the basin for further reading see fry et al 2013 we obtained additional data for over lake precipitation over lake evaporation and runoff from two canadian federal government products the canadian precipitation analysis or capa and gem mesh gem mesh is a configuration of the modélisation environmentale surface et hydrologie mesh forced by the canadian global environmental multiscale gem numerical weather prediction model deacu et al 2012 lespinas et al 2015 we utilized each of these data sets during our model inference routine described below in section 2 2 4 for the period 1980 to 2015 we also used a different subset of these data for calculating prior probability distribution hyper parameters as described in the following section a complete summary of the data used for our representative application to the great lakes including an indication of how each data set was used in either prior probability distributions or likelihood functions is included in appendix a 2 2 3 prior probability distributions application to the great lakes for our application to the great lakes we employed prior probability distributions for model parameters p e r q and d prescribed by eqs 4 9 for p e and r we calculated hyper parameters empirically using historical data from 1950 through 1979 from the glm hmd for q and d as well as nbs values for lake st clair we calculated hyper parameters using data from 1950 through 1979 from the ccglbhhd similarly we employed eq 10 as a prior probability distribution for bias in data sources for over lake precipitation over lake evaporation and runoff i e ηp ηe and ηr and eq 16 as a prior probability distribution for bias in data sources for lake st clair nbs however we modified the prior probability distributions for bias in channel flow and diversion data by calculating the prior probability distribution precision as τ 1 σ 2 and σ λ μ where μ is the historical empirical monthly mean of q and d and λ is a coefficient of variation unique to a particular source of data for q and d table 2 reflecting information we obtained from regional experts for further information on expert opinion solicitation see borsuk et al 2001 voinov and bousquet 2010 2 2 4 model inference and analysis we implemented three configurations of our model each with either a 1 month 6 month or 12 month rolling inference window we encoded these configurations in jags just another gibbs sampler plummer 2003 and executed the jags model inference routine through the rjags package in the r statistical software environment r core team 2017 jags is an open source cross platform engine of the bugs bayesian inference using gibbs sampling language lunn et al 2000 which has been applied in numerous bayesian inference studies across a range of disciplines lunn et al 2009 kéry 2010 ntzoufras 2011 parkes and demeritt 2016 jags model code is included for reference in on line supplementary material we ran each model for k 1 000 000 markov chain monte carlo mcmc iterations across three parallel mcmc chains we omitted the first 500 000 iterations as a burn in period and then thinned the remaining 500 000 iterations at even intervals such that each chain had 1000 values the resulting 3000 mcmc samples serve as the basis for our estimates of the posterior probability distributions for each model parameter we evaluated each configuration by first assessing homoscedasticity of model errors i e ϵ and then by assessing the extent to which inferred water balance components closed the water balance over different time horizons this evaluation allowed us to better understand relationships between the length of an inference rolling window and the range of time horizons over which the corresponding model provides results that close the water balance some water management agencies for example need monthly water balance component estimates that are consistent with observed changes in lake storage on only a month to month basis others such as seasonal forecasting authorities may be concerned with changes in the water balance over longer time horizons we then used the inferred water balance component estimates and other model parameters from each model configuration to simulate the posterior predictive distribution gelman et al 2002 kruschke 2013 of observed changes in lake storage i e left side of eq 2 it is informative to note that after inferring monthly water balance components we can use those components to simulate changes in lake storage over any time horizon we are not in other words constrained to simulating over only 1 6 and 12 month windows i e the time windows we used to infer the water balance components to address potential water resources management planning needs over a range of time scales we elected to calculate the posterior predictive distribution for observed changes in storage across consecutive time windows of 1 month 12 months and 60 months all between 1980 and 2015 3 results and discussion 3 1 model diagnostics 3 1 1 process model error distribution i e homoscedasticity our assessment of monthly model process errors indicates that errors in the model configuration with a 1 month inference window left column fig 2 reflect significant seasonality particularly for lakes superior and michigan huron this finding indicates that there is an important mode of variability in the great lakes seasonal cycle that is not represented in water balance component estimates derived from a model with a 1 month inference window the errors in the model configuration with a 6 month inference window also reflect seasonality for lakes superior and michigan huron though not with nearly the same severity as the model with a 1 month inference window it is interesting to note that model errors are relatively uniform for lake erie and ontario for both the 1 month and 6 month configurations this finding most likely reflects the fact that connecting channel flows represent a higher proportion of each lake s water balance moving downstream through the system from lake superior to lake ontario errors in the model configuration with a 12 month inference window right column fig 2 do not follow any noticeable seasonal pattern there does however appear to be some evidence of a positive bias where the mean model error is slightly less than zero in models for lakes superior and michigan huron though this evidence is very weak i e the uncertainty bounds suggest there is no evidence for error values other than zero 3 1 2 long term water balance closure we find that our model configuration with a 1 month inference window only closed the water balance when simulating changes in lake storage over a 1 month period top row fig 3 when used to simulate changes in storage over consecutive 12 and 60 month simulation periods water balance components from the model with a 1 month inference window accumulate severe biases and lead to very wide prediction intervals middle row and bottom row fig 3 in contrast we find that our model configuration with a 12 month inference window fig 4 consistently closes the water balance across consecutive 1 12 and 60 month time horizons furthermore the inferred water balance components and their uncertainties may help identify months in which there is a need for additional information perhaps in the form of expanded or improved monitoring infrastructure knowledge of how to either expand or consolidate monitoring infrastructure is critical to long term understanding of hydrologic response and the l2swbm provides a potential pathway towards that understanding 3 2 inferred water balance component values data bias and data error a visual assessment of a representative time series of inferred values of p e and r for lake erie from our model configuration with a 12 month rolling window fig 5 indicates that while inferred estimates are generally consistent with historical data there are also important differences both among the historical data sets and between those historical data sets and our new estimates for example we find that runoff estimates from the gem mesh system bottom panel fig 5 tend to be systematically lower than those of the glm hmd in late winter and early spring we also note that during periods when only one data source is available for a particular water balance component i e lake erie evaporation in 2015 and lake erie runoff in 2014 and 2015 inferred estimates have a higher degree of uncertainty summary statistics for each water balance component and lake table 3 underscore the relative contribution of each lake s water balance components as well as their magnitudes relative to connecting channel flows it is informative to note that multiple additional models and data products could have been either assimilated into our application of the l2swbm or used as an independent basis for comparison with our new inferred water balance components i e fig 5 the primary goal of this study however was to provide a robust demonstration of how the l2swbm can close the water balance over multiple consecutive multi month time steps this demonstration provides a basis for future comparisons to and perhaps assimilation of those products a recent study using preliminary results from the l2swbm provides a representative example of this potential gronewold et al 2019 it is also worth noting that the larger uncertainty in bias for lake erie and lake ontario channel outflows reflect overall uncertainty in the water balance for both lakes erie and ontario have roughly a quarter of the surface area of superior and michigan huron table 1 thus less water is required to raise and lower the water level for both lakes and uncertainties in other components of their water balances can be magnified in the case of erie and ontario channel flows are the dominant factor in the balance table 3 and therefore any uncertainty in those estimates is magnified in the model in contrast the individual models for lake superior and lake michigan huron can absorb greater amounts of uncertainty in water balance components with respect to the water level and their surface areas an examination of inferred data bias and error estimates fig 6 further underscores the ability of our framework to reconcile disparate historical data sets and to close the water balance of a large lake system for example the bias and error results indicate that capa over lake precipitation estimates tend to be positively biased relative to the overall water balance particularly in winter months these results are interesting in light of previous findings holman et al 2012 suggesting that precipitation estimates across the great lakes based on terrestrial monitoring stations such as those in the glm hmd misrepresent winter atmospheric stability dynamics over large lakes and are therefore expected to show a strong seasonal bias as well we also find that both sources of legacy evaporation data glm hmd lltm and gem mesh have seasonal biases fig 6 relative to the water balance with particularly severe biases in gem mesh evaporation estimates for lakes michigan huron and ontario this is not entirely surprising given the challenge of accurately measuring blanken et al 2011 spence et al 2011 and simulating fujisaki manome et al 2017 charusombat et al 2018 turbulent heat fluxes across the vast surfaces of the great lakes these challenges are particularly pronounced in the fall months a period when evaporation rates increase rapidly and when there can be significant year to year variability lenters 2001 spence et al 2013 4 conclusions we developed tested and applied a new bayesian statistical analysis framework that reconciles the water balance of large lake systems we hereafter propose formally referring to this product as the large lake statistical water balance model or l2swbm significant contributions to hydrological modeling represented by the new l2swbm include explicit closure of the water balance over multiple time horizons through the use of a fixed length rolling window and a formulation for monthly model error distinct from water level measurement error and water balance component estimate uncertainty we have also demonstrated how the l2swbm can incorporate expert opinion through informative prior probability distributions on the bias in historical measurements of certain water balance components it is informative to note that our framework was recently adopted by great lakes regional management authorities including the united states army corps of engineers and environment and climate change canada as a step towards generating a new set of internationally coordinated water balance component estimates for the entire great lakes system it is our understanding that our framework is the first to provide a definitive approach to reconciling differences between water balance estimates for this system and for closing the water balance over multiple time periods moving forward we anticipate applying the l2swbm to other large lakes and large lake systems around the world we recognize that for many global lake systems water balance data sets are based on sparse monitoring networks in some cases monitoring networks are nonexistent and coarse model simulations are used to provide estimates of a lakes water balance components the l2swbm provides an ideal platform for utilizing any available information about a lake s water balance to reconcile changes in storage and to explicitly allocate uncertainty and bias within historical data and water balance component estimates as another potential future step in the evolution of the l2swbm we envision replacing water balance component terms e g p e and r with physically based models potential examples include replacing the term for evaporation e in eq 1 with a lake surface energy balance model based on eddy diffusion hostetler and bartlein 1990 a formulation of penman or priestley taylor equations penman 1948 priestley and taylor 1972 or the surface energy balance system su 2002 this approach would allow direct approximation of parameters for those models that are faithful not only to governing physical processes and environmental observations but to the water balance of a lake or system of lakes as well data availability statement the data that support the findings of this study are openly available from federal agencies including the national oceanic atmospheric administration environment and climate change canada and the coordinating committee on great lakes basic hydraulic and hydrologic data credit authorship contribution statement andrew d gronewold conceptualization methodology writing original draft supervision joeseph p smith data curation validation methodology laura k read conceptualization writing review editing james l crooks software formal analysis writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors thank song qian yves atchade kerby shedden edward ionides vincent fortin bryan tolson freya rowland and craig stow for helpful discussions on bayesian inference and alternative formulations of our water balance model jacob bruxer frank seglenieks tim hunter tim calappi and lauren fry offered expert opinions and facilitated access to publicly available water balance data nicole rice provided graphical and editorial support funding was provided by the international joint commission ijc international watersheds initiative iwi to noaa and the cooperative institute for great lakes research ciglr through a noaa cooperative agreement with the university of michigan na17oar4320152 the use of product names commercial and otherwise in this paper does not imply endorsement by noaa noaa glerl ciglr or any other contributing agency or organization this is noaa glerl contribution number 1935 and ciglr contribution number 1153 appendix a data this appendix includes a summary table a1 of data sources for populating water balance component observations y and for calculating prior probability distribution hyperparameters appendix b notation a summary of notation used in our study is included in table b 1 supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103505 appendix c supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 supplementary data s2 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s2 
519,water balance models are commonly employed to improve understanding of drivers behind changes in the hydrologic cycle across multiple space and time scales generally these models are physically based a feature that can lead to unreconciled biases and uncertainties when a model is not encoded to be faithful to changes in water storage over time statistical methods represent one approach to addressing this problem we find however that there are very few historical hydrological modeling studies in which bias correction and uncertainty quantification methods are routinely applied to ensure fidelity to the water balance importantly we know of none aside from preliminary applications of the model we advance in this study applied specifically to large lake systems we fill this gap by developing and applying a bayesian statistical analysis framework for inferring water balance components specifically in large lake systems the model behind this framework which we refer to as the l2swbm large lake statistical water balance model includes a conventional water balance model encoded to iteratively close the water balance over multiple consecutive time periods throughout these iterations the l2swbm can assimilate multiple preliminary estimates of each water balance component from either historical model simulations or interpolated in situ monitoring data for example and it can accommodate those estimates even if they span different time periods the l2swbm can also be executed if data for a particular water balance component are unavailable a feature that underscores its potential utility in data scarce regions here we demonstrate the utility of our new framework through a customized application to the laurentian great lakes the largest system of lakes on earth through this application we find that the l2swbm is able to infer new water balance component estimates that to our are knowledge are the first ever to close the water balance over a multi decadal historical period for this massive lake system more specifically we find that posterior predictive intervals for changes in lake storage are consistent with observed changes in lake storage across this period over simulation time intervals of both 6 and 12 months in additional to introducing a framework for developing definitive long term hydrologic records for large lake systems our study provides important insights into the origins of biases in both legacy and state of the art hydrological models as well as regional and global hydrological data sets keywords hydrologic cycle large lakes statistical modeling bayesian inference water balance 1 introduction hydrological models that simulate and forecast the water balance across a variety of space and time scales are needed to facilitate water resources management planning and ultimately to ensure human and environmental health vörösmarty et al 2000 pekel et al 2016 this need is particularly pronounced in regions where rapid population growth coincides with changes in the spatiotemporal distribution of fresh water and where the sustainability of future water supplies is uncertain schewe et al 2014 to address this need hydrological models need to clearly differentiate components of the hydrologic cycle that are often confounded including for example evapotranspiration and irrigation water demand and to quantify changes in those components over time nijssen et al 2001 kebede et al 2006 raes et al 2006 li et al 2007 gronewold and stow 2014 global continental and basin scale water balance modeling research typically focuses on improving representation of terrestrial and atmospheric physical processes collectively governing precipitation evapotranspiration and streamflow crow et al 2008 kim and stricker 1996 milly and dunne 2017 senay et al 2011 vörösmarty et al 1998 this body of research while providing foundational hydrologic data for much of the planet s land surface rarely explicitly resolves mass and energy fluxes over large freshwater surfaces makhlouf and michel 1994 xu and singh 1998 arnell 1999 guo et al 2002 put differently we find that the primary physical processes driving the water balance of large lakes including over lake evaporation i e turbulent heat fluxes over lake precipitation and predominant channel lake inflows and outflows are represented poorly if at all in large scale terrestrial land surface models and corresponding data sets there are however several state of the art models that represent these processes specifically for lakes and for their interactions with the atmosphere one particular example is wrf lake a one dimensional 1 d physically based lake model that in previous studies gu et al 2013 xiao et al 2016 has been coupled with the weather research and forecasting wrf model a similar one dimensional scheme has been applied to resolve large lakes holman et al 2012 notaro et al 2013 within the abdus salam international center for theoretical physics regional climate model ictp regcm4 there have also been significant evolutions in three dimensional lake models including the transition of the national oceanic and atmospheric administration noaa great lakes operational forecasting system glofs from the princeton ocean model pom to the finite volume community ocean model or fvcom for details see kelley et al 2018 as well as the operationalization of the nucleus for european modelling of the ocean nemo model dupont et al 2012 within environment and climate change canada s water cycle prediction system wcps described in durnford et al 2018 while many of these and other existing lake models have been found to represent key physical processes they are rarely if at all evaluated within the context of the overall hydrologic cycle and reconciliation of changes in lake storage over multiple time periods this missing piece of context in most lake modeling studies limits the extent to which scientists and practitioners along with the general public understand modes of variability in the storage of nearly all of the earth s fresh unfrozen surface water gibson et al 2006 swenson and wahr 2009 xiao et al 2018 filling this gap requires a focused effort on lake water balance models that resolve physical processes governing lake storage at appropriate space and time scales piper et al 1986 nicholson et al 2000 gibson et al 2006 peng et al 2019 here we address this gap by introducing a framework that employs a novel formulation of a lake water balance model in which historical monthly water balance components are estimated through bayesian inference gelman et al 2004 empirical data sets and historical model simulations if available are incorporated into the framework through likelihood functions and prior probability distribution functions this approach leads not only to probabilistic historical water balance component estimates that preserve spatial and temporal correlation across a lake or lake system but also to quantification of bias and uncertainty in the models and data sets that had previously been developed for that lake it is informative to note that a preliminary prototype of our framework was applied in a study assessing rapid water level changes between 2013 and 2015 on lakes superior michigan and huron gronewold et al 2015 2016 an analysis of an evolution of that prototype which focused primarily on model selection was presented in smith and gronewold 2018 the framework we present here is differentiated from the prototype in gronewold et al 2016 by among other features two key improvements the first is a computationally efficient filtering method which we periodically refer to as a rolling inference window that facilitates inference over multi decadal periods the second is a skill assessment that reflects both the relative homoscedasticity of model residuals as well as the extent to which the model closes a lake s water balance over consecutive multi month time periods we also acknowledge that there are previous studies utilizing similar statistical methods such as those that debias continental remote sensing data pan and wood 2006 coccia et al 2015 these studies however are typically focused exclusively on land surface processes and do not adequately resolve large lakes the framework we develop here therefore is further distinguished by its focus on large lakes including its ability to model multiple connected lakes in series our representative application also represents the first time a water balance model has been applied systematically to the entire laurentian great lakes system the largest system of lakes in earth that closes the water balance over multiple time periods while reconciling discrepancies between alternate measurements and model simulations of the same water balance component the results of our application serve as both a solution to a long standing water resources management problem for the great lakes and as a stepping stone towards solving similar problems in large lake systems around the world 2 methods in the following sections we first section 2 1 provide an overview of the generic formulation of our new framework we then section 2 2 describe the customization of our framework to the water balance of the entire laurentian great lakes system section 2 2 also includes a description of our approach to evaluating the new framework using the results from our application to the laurentian great lakes 2 1 the model we developed our modeling framework by first modifying the conventional formulation of a lake water balance model to represent changes in storage over a time window of w months 1 δ h j w h j w h j i j j w 1 p i e i r i i i q i d i ϵ i where δh j w represents the true change in lake storage over a period of w months starting with month j hj and h j w represent true monthly average lake water levels in mm at the beginning of months j and j w respectively j 1 t w 1 and t is the total number of months over which the model is run our use of i and j as month number indices within the context of eq 1 accommodates this rolling multi month window approach the value of the month index j in eq 1 can not by definition exceed t w 1 we index monthly water balance components outside of the context of eq 1 using t 1 t the true values for monthly water balance components expressed in mm over a lake surface area in eq 1 include over lake precipitation p over lake evaporation e lateral tributary runoff r inflow from an upstream channel i discharge through a downstream channel q and the total of interbasin diversions and consumptive uses d the model also includes a process error term ϵ to account for potential sources of water balance variability not explained by components p e r i q and d alone including for example thermal expansion glacial isostatic rebound and groundwater fluxes quinn and guerra 1986 mainville and craymer 2005 these terms could be added to eq 1 on a case by case basis depending on whether they are expected to be significant probabilistic estimates of each water balance component in eq 1 are inferred in a bayesian framework press 2003 gelman et al 2004 in which prior probability distributions and likelihood functions are parameterized using legacy models and data sets as well as expert knowledge and opinion specifically for prior probability distributions 2 1 1 likelihood functions the likelihood function for the change in storage within a given lake over a period of w months is 2 y δ h j w y h j w y h j n δ h j w τ δ h w in which the observed change in storage y δh starting in month j and across a rolling window of length w is the difference between water level measurements yh at the beginning of months j w and j we model this value with a normal distribution with mean δh j w and precision τ δh w this approach allows for an explicit representation of uncertainty in water level data that can be differentiated from uncertainty in water balance component estimates it is informative to note that rather than using variance σ 2 we parameterize normal distributions using precision τ 1 σ 2 following conventional practice for bayesian inference casella and berger 2002 gelman et al 2004 qian et al 2009 we then introduce three new parameters i q and d to represent connecting channel inflows outflows and diversions respectively in units of m3 s we use these units because most water management practitioners are accustomed to recording and assessing these values in m3 s rather than mm over each lake surface we encode the empirical relationship i e the conversion of units between parameters i q and d and respectively i q and d using the surface area of each lake and the number of seconds in a particular month the likelihood functions for water balance components on the right hand side of eq 1 represented collectively by θ p e r i q d is 3 y t θ n n θ t η θ c t n τ t θ n where y t θ n is data source n 1 n for component θ at time step t n is the total number of data sources for that component η θ c t n is the bias of the nth data source in calendar month c and τ t θ n is the data source precision at time step t 2 1 2 prior probability distributions standard formulation in bayesian statistics parameters are frequently modeled with normal probability distributions to support inference across a broad range of potential values alternate probability distribution families can be used however to reflect knowledge or beliefs that a parameter might have a more limited range of values we model e i q and d with normal prior probability distributions 4 π e t n μ e c t τ e c t 2 5 π i t n μ i c t τ i c t 6 π q t n μ q c t τ q c t 7 π d t n μ d c t τ d c t where prior means μ c t and precisions τ c t for each calendar month c are either calculated empirically using historical data records or informed by expert opinion for further reading on objective and subjective prior probability distributions see press 2003 this approach allows for the possibility that lake evaporation can be both positive i e a loss of water from a lake and negative i e when there is warm overlying air and condensation occurs this approach is also suited for relative high values of connecting channel flows q and diversions d future users of our framework could should they choose to do so select different prior probability distribution families such as lognormal for example we divide precision in half i e double the variance for prior probability distributions on over lake evaporation e because for many large lakes evaporation has a very strong historical seasonal cycle with relatively low variability that historical low variability could when quantified in the parameters of a prior probability distribution overly constrain the range of inferred monthly evaporation estimates during a later period particularly in lakes where climate change has led to a systematic increase in evaporation over time milly et al 2008 total lateral tributary runoff values aggregated over a lake basin and over monthly time steps are almost certainly positive and we therefore model r with a log normal prior probability distribution 8 π r t ln μ l n r c t τ l n r c t with prior means μ l n r c t and precisions τ l n r c t these values can be calculated for each calendar month c empirically using historical data records or formulated to represent expert opinion for over lake precipitation we use a gamma prior probability distribution husak et al 2007 9 π p t ga ψ c t 1 ψ c t 2 with shape ψ 1 and rate ψ 2 following thom 1958 defined as ψ c t 1 1 4 ϕ c t 1 1 4 ϕ c t 3 ϕ c t ln μ p c t μ l n p c t ψ c t 2 ψ c t 1 μ p c t where μ p c t is the mean historical precipitation for each month and μ l n p c t is the mean of the logarithm of precipitation for each calendar month c we then model the bias of each contributing data set using normal prior probability distributions 10 π η θ c t n n 0 0 01 with mean 0 and precision 0 01 we note that this precision is equivalent to a standard deviation of 10 and is in units of mm over a lake surface for ηp ηe and ηr while it is in units of m3 s for η q η i and η d users of our framework can customize these prior probability distributions by selecting mean and precision values that are unique to each bias term our representative application in the next section provides an example finally following gelman 2006 we modelled τ δh w and τ t θ using a gamma ga 0 1 0 1 prior probability distribution with shape and scale parameter both equal to 0 1 similarly we constrained water balance model errors to one of 12 values corresponding to each of the 12 calendar months with each error term having a common vague normal prior probability distribution with mean 0 and precision 0 01 11 ϵ t ϵ c t π ϵ c t n 0 0 01 we recognize that bias estimates in our model may be impacted by the classic problem of bias variance tradeoff geman et al 1992 we view implementation of solutions to this problem such as bias variance decomposition valentini and dietterich 2004 as a potential future step in our research 2 2 representative application the laurentian great lakes to demonstrate the utility of our model we customized it to the entire laurentian great lakes system hereafter referred to simply as the great lakes to infer new monthly water balance components for the period 1980 through 2015 the great lakes system fig 1 includes lakes superior michigan huron st clair erie and ontario here we follow conventional practice in great lakes hydrological modeling research at coarse time scales e g monthly by representing lakes michigan and huron as a single lake lake michigan huron given the depth and breadth of the channel i e the straits of mackinac that connects them quinn and edstrom 2000 pietroniro et al 2007 collectively the great lakes represent the largest system of lakes on earth lakes superior and michigan huron alone are the two largest lakes on earth by surface area gronewold et al 2013 we encoded lake to lake connectivity within the great lakes system i e through the st marys st clair detroit and niagara rivers by defining the inflow to each lake through a major connecting channel i as the outflow from the adjacent upstream lake q for example the inflow to lake michigan huron through the st marys river at each monthly time step t is encoded as q s u p t the outflow from lake superior there is no upstream connecting channel flowing into lake superior and therefore in eq 1 for lake superior i s u p 0 we obtained surface areas for each of the great lakes table 1 from the national oceanic and atmospheric administration noaa great lakes environmental research laboratory glerl to calculate relationships between q d q and d for details see hunter et al 2015 2 2 1 model modifications for the water balance of lake st clair we model lake st clair differently from the other great lakes because its surface area is relatively small table 1 and because its hydrologic cycle is dominated by inflow from the st clair river and outflow to the detroit river more specifically rather than differentiating precipitation evaporation and runoff for lake st clair we represented them collectively as a single term commonly referred to as net basin supply n b s p e r we model the nbs for lake st clair using modified versions of eqs 1 and 2 in units of m3 s as follows 12 δ h j w i j j w 1 n b s i q m h u i q i d i ϵ s t c i 13 y δ h j w n δ h j w τ δ h w where q m h u is the outflow from lake michigan huron i e the inflow to lake st clair through the st clair river we model lake st clair nbs values with a normal prior probability distribution 14 π n b s t n μ n b s c t τ n b s c t and a normal likelihood function 15 y t n b s n n n b s t η n b s c t n τ t n b s n where η n b s c t n is the bias of nbs estimate n 1 n in calendar month c n is the total number of nbs data sources and τ t n b s n is the precision of each data source we then adapt the prior probability distributions from eqs 10 and 11 for nbs data bias and lake st clair model error such that the prior precision for each τ 0 0625 1 σ 2 is equivalent to a standard deviation σ of 4 m3 s or roughly 10mm of water on the surface of lake st clair table 1 over the course of a month 16 π η n b s c t n n 0 0 0625 17 π ϵ s t c n 0 0 0625 2 2 2 data for application to the great lakes we obtained beginning of month lakewide average water level data yh for each of the great lakes as well as historical records of interbasin diversions y d channel flows y q and estimates of lake st clair s net basin supply from the coordinating committee on great lakes basic hydraulic and hydrologic data ccglbhhd for further reading on the ccglbhhd see gronewold et al 2018 the water level data we obtained from the ccglbhhd is derived from water level measurements at gauges located around the coastline of each of the great lakes that are maintained by both the noaa national ocean service center for operational oceanographic products and services noaa nos co ops and the canadian department of fisheries and oceans canadian hydrographic service dfo chs we recognize that alternate sources of water level information are available including those based on satellites and other remote sensing products alsdorf et al 2001 crétaux et al 2011 schwatke et al 2015 for this application we utilize the relatively robust network of great lakes water level gauging stations synthesized in the ccglbhhd records and leave assimilation of remotely sensed water levels to future research on either the great lakes or other large lake systems similarly we obtained data on diversions into out of or within each lake basin from the ccglbhhd including the ogoki river and long lac diversions into lake superior the chicago river diversion out of lake michigan huron and the welland canal that runs parallel the niagara river fig 1 we then obtained two sets of connecting channel flow data the first y q 1 includes estimates for each of the great lakes connecting channels derived by the ccglbhhd using a variety of standard methods these methods include the aggregation of discrete flow measurements through dams and marine navigation locks and the application of stage fall discharge equations the second set of connecting channel flow estimates y q 2 is based on acoustic doppler velocity meters advms located at international gauging stations igs maintained through a partnership between the united states geological survey usgs and water survey canada wsc these igs measurements are available only for the st marys st clair and detroit rivers we then obtained a set of data for over lake precipitation over lake evaporation and runoff from the noaa glerl great lakes monthly hydrometeorological database or glm hmd described in hunter et al 2015 the glm hmd utilizes a suite of models and interpolation schemes to generate 1 dimensional estimates of water balance components over the land and lake surfaces of each of the great lakes more specifically over lake precipitation estimates in the glm hmd are based on thiessen weighting croley and hartmann 1985 of land based meteorological station data for further discussion see holman et al 2012 over lake evaporation simulations in the glm hmd are derived from the legacy large lakes thermodynamics model lltm which utilizes wind speed dew point cloud cover and lake surface temperature to simulate heat exchange and ice cover across the lakes croley 1989 1992 finally runoff estimates in the glm hmd are derived from an area ratio based interpolation of usgs and wsc streamflow gages across the basin for further reading see fry et al 2013 we obtained additional data for over lake precipitation over lake evaporation and runoff from two canadian federal government products the canadian precipitation analysis or capa and gem mesh gem mesh is a configuration of the modélisation environmentale surface et hydrologie mesh forced by the canadian global environmental multiscale gem numerical weather prediction model deacu et al 2012 lespinas et al 2015 we utilized each of these data sets during our model inference routine described below in section 2 2 4 for the period 1980 to 2015 we also used a different subset of these data for calculating prior probability distribution hyper parameters as described in the following section a complete summary of the data used for our representative application to the great lakes including an indication of how each data set was used in either prior probability distributions or likelihood functions is included in appendix a 2 2 3 prior probability distributions application to the great lakes for our application to the great lakes we employed prior probability distributions for model parameters p e r q and d prescribed by eqs 4 9 for p e and r we calculated hyper parameters empirically using historical data from 1950 through 1979 from the glm hmd for q and d as well as nbs values for lake st clair we calculated hyper parameters using data from 1950 through 1979 from the ccglbhhd similarly we employed eq 10 as a prior probability distribution for bias in data sources for over lake precipitation over lake evaporation and runoff i e ηp ηe and ηr and eq 16 as a prior probability distribution for bias in data sources for lake st clair nbs however we modified the prior probability distributions for bias in channel flow and diversion data by calculating the prior probability distribution precision as τ 1 σ 2 and σ λ μ where μ is the historical empirical monthly mean of q and d and λ is a coefficient of variation unique to a particular source of data for q and d table 2 reflecting information we obtained from regional experts for further information on expert opinion solicitation see borsuk et al 2001 voinov and bousquet 2010 2 2 4 model inference and analysis we implemented three configurations of our model each with either a 1 month 6 month or 12 month rolling inference window we encoded these configurations in jags just another gibbs sampler plummer 2003 and executed the jags model inference routine through the rjags package in the r statistical software environment r core team 2017 jags is an open source cross platform engine of the bugs bayesian inference using gibbs sampling language lunn et al 2000 which has been applied in numerous bayesian inference studies across a range of disciplines lunn et al 2009 kéry 2010 ntzoufras 2011 parkes and demeritt 2016 jags model code is included for reference in on line supplementary material we ran each model for k 1 000 000 markov chain monte carlo mcmc iterations across three parallel mcmc chains we omitted the first 500 000 iterations as a burn in period and then thinned the remaining 500 000 iterations at even intervals such that each chain had 1000 values the resulting 3000 mcmc samples serve as the basis for our estimates of the posterior probability distributions for each model parameter we evaluated each configuration by first assessing homoscedasticity of model errors i e ϵ and then by assessing the extent to which inferred water balance components closed the water balance over different time horizons this evaluation allowed us to better understand relationships between the length of an inference rolling window and the range of time horizons over which the corresponding model provides results that close the water balance some water management agencies for example need monthly water balance component estimates that are consistent with observed changes in lake storage on only a month to month basis others such as seasonal forecasting authorities may be concerned with changes in the water balance over longer time horizons we then used the inferred water balance component estimates and other model parameters from each model configuration to simulate the posterior predictive distribution gelman et al 2002 kruschke 2013 of observed changes in lake storage i e left side of eq 2 it is informative to note that after inferring monthly water balance components we can use those components to simulate changes in lake storage over any time horizon we are not in other words constrained to simulating over only 1 6 and 12 month windows i e the time windows we used to infer the water balance components to address potential water resources management planning needs over a range of time scales we elected to calculate the posterior predictive distribution for observed changes in storage across consecutive time windows of 1 month 12 months and 60 months all between 1980 and 2015 3 results and discussion 3 1 model diagnostics 3 1 1 process model error distribution i e homoscedasticity our assessment of monthly model process errors indicates that errors in the model configuration with a 1 month inference window left column fig 2 reflect significant seasonality particularly for lakes superior and michigan huron this finding indicates that there is an important mode of variability in the great lakes seasonal cycle that is not represented in water balance component estimates derived from a model with a 1 month inference window the errors in the model configuration with a 6 month inference window also reflect seasonality for lakes superior and michigan huron though not with nearly the same severity as the model with a 1 month inference window it is interesting to note that model errors are relatively uniform for lake erie and ontario for both the 1 month and 6 month configurations this finding most likely reflects the fact that connecting channel flows represent a higher proportion of each lake s water balance moving downstream through the system from lake superior to lake ontario errors in the model configuration with a 12 month inference window right column fig 2 do not follow any noticeable seasonal pattern there does however appear to be some evidence of a positive bias where the mean model error is slightly less than zero in models for lakes superior and michigan huron though this evidence is very weak i e the uncertainty bounds suggest there is no evidence for error values other than zero 3 1 2 long term water balance closure we find that our model configuration with a 1 month inference window only closed the water balance when simulating changes in lake storage over a 1 month period top row fig 3 when used to simulate changes in storage over consecutive 12 and 60 month simulation periods water balance components from the model with a 1 month inference window accumulate severe biases and lead to very wide prediction intervals middle row and bottom row fig 3 in contrast we find that our model configuration with a 12 month inference window fig 4 consistently closes the water balance across consecutive 1 12 and 60 month time horizons furthermore the inferred water balance components and their uncertainties may help identify months in which there is a need for additional information perhaps in the form of expanded or improved monitoring infrastructure knowledge of how to either expand or consolidate monitoring infrastructure is critical to long term understanding of hydrologic response and the l2swbm provides a potential pathway towards that understanding 3 2 inferred water balance component values data bias and data error a visual assessment of a representative time series of inferred values of p e and r for lake erie from our model configuration with a 12 month rolling window fig 5 indicates that while inferred estimates are generally consistent with historical data there are also important differences both among the historical data sets and between those historical data sets and our new estimates for example we find that runoff estimates from the gem mesh system bottom panel fig 5 tend to be systematically lower than those of the glm hmd in late winter and early spring we also note that during periods when only one data source is available for a particular water balance component i e lake erie evaporation in 2015 and lake erie runoff in 2014 and 2015 inferred estimates have a higher degree of uncertainty summary statistics for each water balance component and lake table 3 underscore the relative contribution of each lake s water balance components as well as their magnitudes relative to connecting channel flows it is informative to note that multiple additional models and data products could have been either assimilated into our application of the l2swbm or used as an independent basis for comparison with our new inferred water balance components i e fig 5 the primary goal of this study however was to provide a robust demonstration of how the l2swbm can close the water balance over multiple consecutive multi month time steps this demonstration provides a basis for future comparisons to and perhaps assimilation of those products a recent study using preliminary results from the l2swbm provides a representative example of this potential gronewold et al 2019 it is also worth noting that the larger uncertainty in bias for lake erie and lake ontario channel outflows reflect overall uncertainty in the water balance for both lakes erie and ontario have roughly a quarter of the surface area of superior and michigan huron table 1 thus less water is required to raise and lower the water level for both lakes and uncertainties in other components of their water balances can be magnified in the case of erie and ontario channel flows are the dominant factor in the balance table 3 and therefore any uncertainty in those estimates is magnified in the model in contrast the individual models for lake superior and lake michigan huron can absorb greater amounts of uncertainty in water balance components with respect to the water level and their surface areas an examination of inferred data bias and error estimates fig 6 further underscores the ability of our framework to reconcile disparate historical data sets and to close the water balance of a large lake system for example the bias and error results indicate that capa over lake precipitation estimates tend to be positively biased relative to the overall water balance particularly in winter months these results are interesting in light of previous findings holman et al 2012 suggesting that precipitation estimates across the great lakes based on terrestrial monitoring stations such as those in the glm hmd misrepresent winter atmospheric stability dynamics over large lakes and are therefore expected to show a strong seasonal bias as well we also find that both sources of legacy evaporation data glm hmd lltm and gem mesh have seasonal biases fig 6 relative to the water balance with particularly severe biases in gem mesh evaporation estimates for lakes michigan huron and ontario this is not entirely surprising given the challenge of accurately measuring blanken et al 2011 spence et al 2011 and simulating fujisaki manome et al 2017 charusombat et al 2018 turbulent heat fluxes across the vast surfaces of the great lakes these challenges are particularly pronounced in the fall months a period when evaporation rates increase rapidly and when there can be significant year to year variability lenters 2001 spence et al 2013 4 conclusions we developed tested and applied a new bayesian statistical analysis framework that reconciles the water balance of large lake systems we hereafter propose formally referring to this product as the large lake statistical water balance model or l2swbm significant contributions to hydrological modeling represented by the new l2swbm include explicit closure of the water balance over multiple time horizons through the use of a fixed length rolling window and a formulation for monthly model error distinct from water level measurement error and water balance component estimate uncertainty we have also demonstrated how the l2swbm can incorporate expert opinion through informative prior probability distributions on the bias in historical measurements of certain water balance components it is informative to note that our framework was recently adopted by great lakes regional management authorities including the united states army corps of engineers and environment and climate change canada as a step towards generating a new set of internationally coordinated water balance component estimates for the entire great lakes system it is our understanding that our framework is the first to provide a definitive approach to reconciling differences between water balance estimates for this system and for closing the water balance over multiple time periods moving forward we anticipate applying the l2swbm to other large lakes and large lake systems around the world we recognize that for many global lake systems water balance data sets are based on sparse monitoring networks in some cases monitoring networks are nonexistent and coarse model simulations are used to provide estimates of a lakes water balance components the l2swbm provides an ideal platform for utilizing any available information about a lake s water balance to reconcile changes in storage and to explicitly allocate uncertainty and bias within historical data and water balance component estimates as another potential future step in the evolution of the l2swbm we envision replacing water balance component terms e g p e and r with physically based models potential examples include replacing the term for evaporation e in eq 1 with a lake surface energy balance model based on eddy diffusion hostetler and bartlein 1990 a formulation of penman or priestley taylor equations penman 1948 priestley and taylor 1972 or the surface energy balance system su 2002 this approach would allow direct approximation of parameters for those models that are faithful not only to governing physical processes and environmental observations but to the water balance of a lake or system of lakes as well data availability statement the data that support the findings of this study are openly available from federal agencies including the national oceanic atmospheric administration environment and climate change canada and the coordinating committee on great lakes basic hydraulic and hydrologic data credit authorship contribution statement andrew d gronewold conceptualization methodology writing original draft supervision joeseph p smith data curation validation methodology laura k read conceptualization writing review editing james l crooks software formal analysis writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors thank song qian yves atchade kerby shedden edward ionides vincent fortin bryan tolson freya rowland and craig stow for helpful discussions on bayesian inference and alternative formulations of our water balance model jacob bruxer frank seglenieks tim hunter tim calappi and lauren fry offered expert opinions and facilitated access to publicly available water balance data nicole rice provided graphical and editorial support funding was provided by the international joint commission ijc international watersheds initiative iwi to noaa and the cooperative institute for great lakes research ciglr through a noaa cooperative agreement with the university of michigan na17oar4320152 the use of product names commercial and otherwise in this paper does not imply endorsement by noaa noaa glerl ciglr or any other contributing agency or organization this is noaa glerl contribution number 1935 and ciglr contribution number 1153 appendix a data this appendix includes a summary table a1 of data sources for populating water balance component observations y and for calculating prior probability distribution hyperparameters appendix b notation a summary of notation used in our study is included in table b 1 supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103505 appendix c supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 supplementary data s2 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s2 
