index,text
280,fluid solid reactions play a key role in a large range of biogeochemical processes transport limitations at the pore scale limit the amount of solute available for reaction so that reaction rates measured under well mixed conditions tend to strongly overestimate rates occurring in natural and engineered systems although different models have been proposed to capture this phenomenon linking pore scale structure flow heterogeneity and local reaction kinetics to upscaled effective kinetics remains a challenging problem we present a new theoretical framework to quantify these dynamics based on the chemical continuous time random walk framework we study a fluid solid reaction with the fluid phase undergoing advective diffusive transport we consider a catalytic degradation reaction a f b s b s where a f is in fluid phase and b s is in solid phase and homogeneous over the fluid solid interface allowing us to focus on the role of transport limitations and medium structure our approach is based on the concept of inter reaction times which result from the times between contacts of transported reactants with the solid phase we use this formulation to quantify the global kinetics of fluid reactant mass and test our predictions against numerical simulations of advective diffusive transport in stratified channel flow and stokes flow through a beadpack the theory captures the decrease of effective reaction rates compared to the well mixed prediction with increasing damköhler number due to transport limitations although we consider simple kinetics and media these findings will contribute to the understanding and modeling of the effect of transport limitations in more complex reactive transport problems keywords reactive transport stochastic modeling chemical continuous time random walk 1 introduction biogeochemical reactions at the interface between fluid and solid medium phases play a central role in a large range of reactive transport problems such as contaminant transport and degradation soil remediation mineral weathering and carbon dioxide sequestration chapelle 2001 appelo and postma 2004 brantley et al 2008 biotic and abiotic reactions at solid fluid interfaces include dissolution precipitation adsorption complexation and redox reactions the kinetics of these reactions on solid surfaces depend directly on the concentration of solutes in the fluid phase which evolve in time and space through flow and transport dynamics therefore much effort has been invested into the development setup and choice of detailed reactive transport models to quantify these processes and their interaction with transport and medium geometry steefel et al 2005 li et al 2017 maher and navarre sitchler 2019 the basic quantification of the kinetics of such reactions is generally performed using well mixed batch experiments yet transport limitations at the pore scale lead to large deviations from these estimates by reducing access of solutes to reactive surfaces compared to fully mixed systems this phenomenon has been observed in resolved numerical simulations of carbonate mineral dissolution in porous media molins et al 2012 numerical simulation and column experiments of calcite dissolution li et al 2007 2008 molins et al 2014 numerical simulations of mineral dissolution in heterogeneous porous media soulaine and tchelepi 2016 soulaine et al 2017 pore scale reactive transport simulations in rough fractures deng et al 2018 and batch experiments and field scale modeling of biodegradation of dissolved organic carbon in aquifers kang et al 2019 pore scale flow and structure have also been found to significantly impact adsorption to mineral surfaces in porous media an effect which has been observed in detailed lattice boltzmann simulations vanson et al 2015 2017b 2017a these studies have consistently found that reaction rates are significantly lower than expected from classical well mixed theories especially when reaction is fast compared to transport processes volume averaging techniques battiato and tartakovsky 2011 battiato et al 2009 have been employed to identify general conditions under which classical macroscopic models of reactive transport break down and transport limitations lead to decreased global reaction rates and or modified rate laws random walk models have been used to investigate the impact of transport on surface reactions for simple geometries such as sinusoidal channels sund et al 2015 sherman et al 2019 the role of available reactive surface area beckingham et al 2016 2017 and surface roughness deng et al 2018 in mineral dissolution in porous and fractured media has been analyzed and quantified through experiments and numerical simulations however a quantitative link between pore scale transport dynamics and effective fluid solid reaction kinetics remains unavailable furthermore in practice highly resolved numerical simulations can be prohibitively expensive and detailed knowledge of the dynamics and spatial distribution of physico chemical properties is often not available stressing the need for upscaled models of reactive transport dentz et al 2011b in well mixed batch reactors reactant concentrations are spatially homogeneous in the lagrangian particle picture this corresponds to every particle being instantaneously available to participate in a reaction with every other particle this deterministic picture can be extended to account for stochastic variability for small particle numbers while retaining the well mixed assumption gillespie 1977 this is achieved through the concept of inter reaction times which represent the time between the occurrence of sequential reaction events amongst sets of reactants in accordance with the chemical reactions in the classical stochastic theory reactants are assumed to be fully mixed in the sense that all sets of reactants allowed by the chemistry have the same probability of reacting this leads to exponentially distributed inter reaction times representing a probability per unit time of reaction that is fully determined by the thermodynamic reaction rate and the available reactant numbers at a given time the classical well mixed rate laws under which reaction rates correspond to products of reactant concentrations with powers determined by the reaction stoichiometry are recovered in the limit of large particle numbers gillespie 1992 in practice this picture holds only if diffusion is sufficiently fast to locally homogenize reactants so that the limiting factor in determining reaction rates lies in the thermodynamic properties of the reaction rather than transport fluid solid reactions involve transported and immobile reactants solid phase reactants are located at the interface between a fluid phase in which fluid phase reactants are transported and a solid phase of the underlying medium the first explicit model of the impact of transport on reaction is due to von smoluchowski 1917 it quantifies contact reactions between a hard sphere and a sea of diffusing particles and it leads to an effective time dependent reaction rate which depends on transport properties namely the diffusion coefficient because there is no fluid flow into or out of the solid interface mass flux of fluid reactants allowing contact with solid phase reactants is ultimately governed by diffusion on the other hand advective transport along streamlines may bring reactants closer or farther from the solid phase thus in the inter reaction time picture discussed above the combined effect of medium heterogeneity advective variability and diffusion introduces reaction delays in terms of the first passage times of reactants to the solid phase quantifying this effect and its impact on reaction rates is therefore fundamental for modeling fluid solid reactions in porous and fractured media recently the chemical continuous time random walk chctrw framework was developed in order to relax the well mixed assumption in stochastic reaction modeling leading to inter reaction times which encode the effect of local transport limitations through additional reaction delays due to transport limitations aquino and dentz 2017 the chctrw hence quantifies the effect of broader distributions of the times required for sequential reaction events to occur despite the formal similarities this differs conceptually from the classical ctrw framework which quantifies the effect of broadly distributed times or distances associated with particle displacements berkowitz et al 2006 such reaction delays can be quantified in terms of the first passage times of reactant particles across each other mcadams and arkin 1997 bénichou et al 2010a b meroz et al 2011 godec and metzler 2016 in the case of fluid solid reactions these are related to the duration of excursions between visits to the solid interface the latter are closely related to the time spent near the interface which can be formally quantified through the so called local time at the boundary which represents the amount of time spent in a thin region near the interface divided by the region thickness in the limit of vanishing thickness karatzas and shreve 1988 takács 1995 grebenkov 2007 2019 the concept of modeling reactive transport in terms of exposure time that is the time that reactants spend in close proximity and so are available for reaction has received some attention over the past decade ginn 1999 2000b 2000a seeboonruang and ginn 2006 sanz prat et al 2016 nonetheless the relationship between exposure time and flow and medium heterogeneity remains little understood the central goal of the present work is to formalize the notion of inter reaction times and their impact on reaction dynamics in the context of fluid solid reactions under advective diffusive transport in order to better undertand and upscale the impact of flow transport and medium structure on global reaction rates we consider here a catalytic degradation reaction a f b s b s a simplified chemical setup which allows us to focus on the role of transport limitations the reactant species b s is taken to be in solid phase immobile and homogeneously distributed over the fluid solid interface whereas the reactant species a f is in fluid phase and undergoes advective diffusive transport the impact of disordered i e random and uncorrelated at different spatial locations distributions of solid phase reactants and residence times on this type of reaction has been studied for diffusive and subdiffusive transport i e transport phenomena where plume variance grows sublinearly in time and trapping using random walk models grassberger and procaccia 1982 kayser and hubbard 1983 klafter et al 1984 weiss 1986 bouchaud and georges 1990 yuste and acedo 2004 lapeyre and dentz 2017 aquino et al 2019 and purely advective transport in a streamtube model using the chctrw framework aquino and dentz 2020 however these models did not consider the joint effect of flow variability and diffusion in porous media the interplay between these processes controls mass fluxes towards the fluid solid interface and therefore the amount of reactant available for reaction as shown here the interplay between medium geometry and transport limitations can lead to effective reaction kinetics that differ from their well mixed counterparts even for this simple chemical setup it should be noted that we disregard for the present more complex effects which may play an important role in reactive transport dynamics such as the coupling of transport and medium evolution due to reaction induced precipitation and dissolution golfier et al 2002 edery et al 2011 garing et al 2015 menke et al 2015 liyanage et al 2019 our simplified setup allows for in depth understanding and quantification of the specific role of transport limitations and medium geometry regarding global reaction dynamics and provides a rigorous upscaling approach to be later extended to more complex reaction chemistry the paper is structured as follows we first formalize fluid solid reaction dynamics under diffusive transport near an interface in section 2 this is followed by a brief review of the fundamental concepts behind inter reaction times and the chctrw formulation in section 3 in section 4 we develop the relationship between return times to the interface and inter reaction times and use this formulation to quantify the time evolution of total fluid reactant mass next in section 5 we illustrate these results by obtaining an analytical formulation of the mass dynamics for advection diffusion under stratified flow in a two dimensional channel section 6 shows how the framework thus developed may be applied to compute the time evolution of total mass from numerical determination of first passage and return times in more general settings in particular we consider advection diffusion under stratified flow in a three dimensional channel and stokes flow in an idealized porous medium specifically a body centered cubic beadpack an overall discussion and conclusions are presented in section 7 and some additional technical details and derivations may be found in appendix 2 fluid solid reaction model we consider a mobile reactant species a f transported by the fluid phase and an immobile solid phase reactant species b s distributed over the fluid solid interface of the medium in order to focus on the effects of transport limitations we assume for simplicity that the distribution of the latter over the interface is homogeneous and that its concentration at a given spatial location does not change appreciably due to reaction assuming further that the reaction is irreversible at the timescale of interest and ignoring the reaction products this corresponds locally to the reaction a f b s b s we thus consider a far from equilibrium situation where the reverse reaction can be neglected mass conservation requires this reaction to give rise to additional products which are ignored here we consider also that the available reactant b s is homogeneous across the solid phase while this assumption should not be expected to hold over large scales it is directly relevant for relatively chemically homogeneous column experiments or over certain regions of larger media the assumption that b s is not consumed holds directly for truly catalytic reactions but along with homogeneity it is also a relevant approximation if b s is locally not consumed appreciably for example under large flow rates and short injections where the fluid phase may be significantly consumed throughout the column but consumption of the solid phase at a particular location is small as mentioned in the introduction the assumption of no consumption of the solid phase also implies that we also disregard more complex effects such as coupling of transport and medium evolution which can occur due to precipitation and or dissolution the resolved simulation method developed in this work can in principle handle more complex chemical setups including multiple reactions and or multicomponent reactions however the theoretical developments become substantially more complex because it is necessary to account for the simultaneous presence and amount of different reactants near the interface the simple chemical reaction a f b s b s along with the assumption of chemical homogeneity of the solid phase reactant allows us to focus on the impact of transport mechanisms and medium geometry on reaction dynamics despite the fact that the chemical kinetics are linear at the fluid solid interface reaction is limited by the available fluid reactant flux toward the latter and transport limitations can lead to modified effective reaction kinetics and significant reaction slowdown while it is important to note that the theory developed here cannot at present be directly applied to multicomponent chemical reactions it provides the first direct link between first passage and return time statistics inter reaction times and fluid solid reaction dynamics and sets the stage for later generalizations in order for reaction to occur physical reactant molecules of the transported phase a f must be in reactive contact with the solid phase component b s this occurs within some distance ℓ s of the fluid solid interface we assume that ℓ s is much smaller than the scale at which transport of reactant a f within the fluid phase may be described through continuous advection diffusion and that reaction then occurs when reactant a f is transported sufficiently close to the interface as we will see this corresponds to the usual concept of surface as opposed to bulk reactions it should be noted that if this spatial scale separation between transport and reaction does not hold a more detailed reaction model is necessary for example if attachment or transport times within a physical reactive layer play a significant role sorption or microporosity models may be needed which we do not consider here from both a theoretical and a numerical perspective it is convenient to adopt a conceptualization of transport in terms of lagrangian tracer particles each lagrangian particle represents a macroscopic number of physical reactant particles undergoing advection diffusion noetinger et al 2016 sund et al 2019 and subject to reaction near the interface disregarding reaction for the moment particle positions x t as a function of time t are described by the langevin equation see e g kampen 1992 1 d x t v x t d t 2 d d t ξ t where d is the diffusion coefficient v is the velocity field as a function of position and for each time t ξ t are independent random vectors whose components are independent unit gaussian random variables under the assumed scale separation between reaction and transport reaction dynamics are controlled by the concentration of fluid reactant a f near the interface subject to both advective diffusive transport and reaction when a discretization is considered for a given time step δ t the support scale ℓ d over which concentrations are well defined must be large compared to the spatial resolution of the model description which is of order 2 d δ t see appendix a for further details it is important to note that ℓ d is not a parameter of the theoretical model but rather a property of the discretization in this sense the dynamics will be shown to converge to a well defined limit when ℓ d 0 which corresponds to taking the continuum limit δ t 0 for a given discretization below the support scale ℓ d the mass represented by a lagrangian particle is taken to be well mixed particles within a region comprising a distance up to ℓ d from the interface are subject to reaction as we will formalize below in what follows we will refer to this region as the reactive region for convenience although it should be remembered that it is associated with a given discretization and different from the physical reactive region associated with the subscale length ℓ s in section 4 we will employ the chctrw inter reaction times to show that the continuum limit ℓ d 0 or equivalently δ t 0 of this conceptualization is well defined and leads to a consistent description of the full reactive transport problem this approach is conceptually convenient for the derivations that follow from a computational standpoint it converges to the correct results as will be discussed in detail below nonetheless we expect that it will prove useful in the future to explore equivalent but computationally more efficient approaches for direct numerical simulations such as kernel smoothing sole mari et al 2017 2019 to determine concentrations near the interface we note that any method for estimating local mass fluxes to the interface from lagrangian particle collisions must address the same conceptual issues since as discussed in detail in what follows the number of collisions with the interface within a given time period is also discretization dependent according to the previous considerations while a lagrangian particle is in the reactive region a fraction ℓ s ℓ d of its mass m p is physically available for reaction assuming the law of mass action holds locally we have 2 d m p d t k c s ℓ s m p ℓ d where k is the usual well mixed reaction rate in units of inverse concentration per time and c s is the solid phase concentration within the reactive region thus 3 d m p d t k d m p k d k c a ℓ d where k d is the effective particle reaction rate in units of inverse time and c a c s ℓ s is the solid phase surface concentration i e mass of solid reactant per unit interface area note that under the scale separation assumption ℓ s is small compared to the scale at which transport can be described by continuous advection diffusion this means that formally the continuum description then corresponds to the scaling limit ℓ d 0 with the physical concentration c a remaining finite whereas ℓ s ℓ d remains small due to the scale separation this scale separation corresponds to the situation where the fluid solid mass action reaction can be treated as a surface reaction the reaction rate k d describing particle mass decay per unit time in the reactive region is independent of ℓ s and k c a is the usual surface reaction rate units of length per time which depends only on the surface concentration c a as will be shown in section 4 this leads to a well defined continuum limit for the evolution of total fluid reactant mass where the results are independent of both the discretization length ℓ d and the subscale length ℓ s similarly we assume here that molecular scale attachment detachment at the interface is fast compared to diffusion near the interface so that it can be considered instantaneous specifically this corresponds to assuming that τ a τ d where τ a is the average duration of an attachment event and τ d ℓ d 2 2 d is the diffusion time associated with the discretization lengthscale remains small we note that the concept of scale separation where a continuum limit is taken while maintaining a subscale length or timescale small is commonly employed in volume averaging see e g whitaker 2013 numerically we implement these dynamics using particle tracking random walk ptrw simulations which discretize the langevin equation 1 see e g noetinger et al 2016 sund et al 2019 and appendix a for further details if a fluid reactant particle is in the reactive region during a time step of duration δ t its mass evolves according to 4 m p t δ t m p t e k d δ t otherwise if the particle is farther from the interface no reaction occurs 3 inter reaction times and the chemical continuous time random walk in this section we provide a brief description of the chctrw framework and the associated concept of inter reaction times which will be used in what follows to obtain a quantitative description of total fluid reactant mass in this framework the inter reaction time is the sum of the delay time due to transport limitations and the intrinsic reaction time necessary for the reaction to occur under well mixed conditions for the present application these correspond to the times between visits of the particle to the reactive region and the time spent in the reactive region these concepts are illustrated in fig 1 a in the sections that follow we will develop a theory of fluid solid reaction under advective diffusive transport and apply it to analyze different two and three dimensional example media fig 1b the chemical ctrw framework treats the inter reaction time τ i e the time between successive reaction events as a stochastic quantity incorporating variability from the chemical kinetics on the one hand and transport and medium structure on the other in what follows we will obtain the pdf ϕ of inter reaction times for our fluid solid reaction disregarding for the moment the first excursion to the interface purple excursion in fig 1 the total inter reaction time in the chctrw formulation can be written as aquino and dentz 2017 5 τ τ r τ g τ r where τ r is the time it would take for a reaction to occur if a particle were confined to the reactive region and τ g is the additional time spent in excursions away from the latter blue excursions in fig 1 the total time τ r that must be spent in the reactive region before the next reaction event impacts the delay time τ g because a longer τ r typically requires more visits to the reactive region corresponding to orange excursions in fig 1 punctuated by excursions when small numbers of reactant particles molecules are considered the inter reaction time refers to the time between two successive reaction events between reactant molecules allowing for capturing fluctuations due to finite particle numbers gillespie 1977 recall that here we consider lagrangian particles representing a certain amount of fluid reactant mass undergoing continuous advection diffusion and corresponding to a macroscopic number of molecules in that case fluctuations due to molecule numbers are not significant gillespie 1977 aquino and dentz 2017 for conceptual and computational reasons it is then more convenient to consider a fixed number of lagrangian particles whose masses evolve in time bolster et al 2016 in this case the rate equation 3 for the evolution of the mass carried by a lagrangian particle within the reactive region corresponds to a constant reaction rate k d k c a ℓ d with units of inverse time according to the classical well mixed theory for stochastic inter reaction times this reaction rate may be interpreted as a constant probability of reaction per unit time which translates into an exponential inter reaction time distribution gillespie 1977 specifically the probability that τ r takes a value in u u d u is given by ϕ r u d u such that its probability density function pdf is given by 6 ϕ r u k d e k d u for a given τ r u τ g u is also a random variable whose pdf ϕ g u reflects the underlying medium heterogeneity and the stochasticity inherent in diffusive motion in the next section we will explicitly relate this pdf to the statistics of excursions away from the interface as we will see although τ r and τ g are discretization dependent τ is well defined in the continuum limit the impact of delay due to return excursions on reaction dynamics is captured by a memory function given in terms of the pdf ϕ of inter reaction times τ as aquino and dentz 2017 7 k ϕ λ λ ϕ λ 1 ϕ λ here and throughout we denote laplace transforms with respect to time by a tilde and the corresponding laplace variable by λ for the a f b s b s reaction considered here the total mass obeys 8 d m ϕ d t k ϕ m ϕ where denotes the convolution product k ϕ m ϕ t 0 d t k ϕ t m ϕ t t and the subscript ϕ in the total mass indicates that the first delay time to reach the interface see fig 1 has not yet been considered so that these dynamics incorporate the impact of the inter reaction time pdf ϕ only this equation may be seen as a generalized rate law governing the time evolution of the total mass under the impact of reaction delay caused by transport limitations in contrast to the classical well mixed rate laws where reaction rates depend only on the current mass in a batch reactor the presence of the convolution with a memory function renders this equation integro differential physically this arises because the reaction rate at a given time depends on past history through the statistics of past excursions excursions away from and back to the reactive interface controlled by transport and medium geometry take the form of reaction delays which lead to memory effects in the large scale mass dynamics broad distributions of excursion times translate into long range memory effects applying these results to fluid solid reactions requires relating the inter reaction times to the statistics of excursions away from and back to the solid interface as well as including the role of the first excursion to the interface fig 1 representing the impact of the initial condition this is the subject of the next section 4 mass dynamics and inter reaction times in this section we first quantify the impact of diffusion near the reactive interface on the distribution of return times to the reactive region and the duration of each visit based on these concepts and the first passage time to the interface from the initial condition we then obtain the inter reaction times and the evolution of total fluid reactant mass based on the chctrw formulation of the previous section 4 1 return and visit times the dominant transport mechanism that controls local reactant mass flux towards the interface is diffusive because there is no fluid flow into or out of the solid phase we assume the interface to be locally flat at the scale of the transport model so that close to the interface it is sufficient to consider one dimensional diffusion in the transverse direction we note that this local flatness assumption may be inappropriate in some systems such as rough fractures where the surface may exhibit fractal i e self similar across scales properties see e g develi and babadagli 1998 such pronounced surface roughness is known to impact reaction rates deng et al 2018 a one dimensional conceptualization of diffusion perpendicular to the surface may then be inaccurate and more detailed modeling of transport and reaction near the interface may be necessary this scenario although important is beyond the scope of the present work and we do not explore it further here for a diffusing particle characterized by the diffusion coefficient d in an unbounded domain in one dimension the pdf of the time to first reach a target located at a distance ℓ to one side of the initial position is given by the lévy 1 2 stable density see e g feller 2008 9 ψ d t ℓ ℓ 4 π d t 3 e ℓ 2 4 d t that is ψ d t ℓ d t is the probability that the first passage time to the target is in the interval t t d t we have 10 ψ d λ ℓ e ℓ 2 λ d in order to apply the chctrw formulation we require the time to return to a target which sets the time between successive visits to the reactive interface and therefore controls the inter reaction times see fig 1 this concept must be treated with care because a particle undergoing continuous diffusive motion in one dimension crosses its original position infinitely many times in any given finite time interval this is reflected in the fact that the limit ℓ 0 of ψ d t ℓ is not well define see also karatzas and shreve 1988 grebenkov 2007 aquino et al 2017 grebenkov 2019 to avoid this problem we will obtain the inter reaction times in the scaling limit of an appropriate discretization associated with the support scale ℓ d see section 2 consider a regular one dimensional discretization into intervals or cells of equal length ℓ d the return times to the interface associated with the discretization are then the first passage times to the center of the cell touching the interface from the center of the adjacent cell the cell centers are a distance ℓ d apart so that the corresponding first passage time pdf is given by ψ d ψ d ℓ d note that the same result is obtained by considering the first passage times to a distance ℓ d from the interface starting from a distance 2 ℓ d this is convenient for numerical determination from particle tracking simulations where particles can be placed at distance 2 ℓ d from the interface and the first passage time determined as the time when distance ℓ d from the interface is crossed before proceeding we may relax the assumption of an unbounded domain and the requirement of one dimensional diffusive transport far from the reactive region first we denote the timescale associated with the discretization support scale ℓ d by 11 τ d ℓ d 2 2 d in order to allow for different effects away from the interface while retaining the diffusive behavior near it we write the return time pdf in the form 12 ψ d λ ψ d λ ℓ d e f λ 2 τ d λ see eq 10 the discretized description can only resolve times t τ d corresponding to λ 1 τ d thus with a view to taking the continuum limit it is sufficient to consider the limit of λ 1 τ d corresponding for a given discretization to times large compared to the discretization time we then find 13 ψ d λ 1 f λ 2 τ d λ it is important to note that this and similar results below do not represent late time expansions but rather lead to results valid for all times or all λ in the continuum limit ℓ d 0 in other words finite ℓ d effects are a product of the discretization which disappear in the continuum limit description of total mass obtained in what follows the form factor f λ which depends on the geometry of the domain and the transport mechanisms involved up to the timescale 1 λ must approach unity for large λ so that the behavior of the return time is dominated by the 2 τ d λ term characterizing the diffusive behavior near the fluid solid interface at short times for small λ corresponding to large times f λ encodes information about transport excursions far from the reactive region as before in an arbitrarily small time interval continuous diffusion in one dimension crosses the origin infinitely many times the factor 2 τ d λ which approaches zero as ℓ d 0 captures the resulting singular character of the return time distribution in this limit whereas the factor f λ whose departure from unity represents additional effects from transport excursions unrelated to the discretization remains finite and nonzero in the continuum limit let ℓ c be a characteristic lengthscale of the medium such as the average pore size note that the choice of ℓ c is arbitrary and simply provides a reference scale based on which nondimensional quantities characterizing the relationship between reaction diffusion and advection processes will be introduced below we denote the corresponding diffusion time as 14 τ d ℓ c 2 2 d and define the damköhler number 15 da k c a τ d ℓ c the latter quantifies the relative importance of reaction and diffusive transport at this scale it is convenient to define the rescaled return time tail probability 16 g t ℓ c ψ d t 2 τ d ℓ d where 17 ψ d t t d t ψ d t is the probability that the return times are greater than a given time t in terms of the rescaled tail probability the return times obey 18 ψ d λ e 2 τ d λ g λ ℓ d ℓ c so that comparing to eq 12 we have the relation g λ f λ 2 τ d λ since f λ remains finite and nonzero in the limit ℓ d 0 so does g λ note that for large λ we have g λ 1 2 τ d λ because f λ 1 as discussed above for small λ corresponding to large times g λ again encodes the behavior of excursions away from the interface these quantities are obtained analytically for the example of diffusion in a bounded one dimensional domain in section 5 note that the statistics of the excursions back to an arbitrary point along the interface may differ depending on the starting point corresponding to different form factors and rescaled tail probabilities depending on the latter here in order to obtain a stationary description where successive return times to the wall have the same statistics we treat any such variability statistically that is the same rescaled tail probability is used to characterize the return time statistics associated with each point on the interface and at all times in other words this method disregards possible non stationarity of the return times as well as possible correlations between subsequent return times arising from different transport and geometry properties at different interface points nonetheless we believe such a statistical description to be appropriate as long as the flow is statistically stationary and the structural characteristics of the medium are statistically homogeneous within the region being considered note that this assumption is similar to that employed in the standard ctrw for transport berkowitz et al 2006 where variability in particle jump sizes and or transit times is treated stochastically but assumed statistically homogeneous and stationary that is the pdfs associated with these quantities do not depend on the current time or particle position in section 6 we show that this approach leads to accurate predictions of mass dynamics in a regular beadpack despite different points on the bead surfaces having different characteristics in terms of the distance to other nearby surface points next we turn to the statistics of the time spent in the reactive region in each visit as shown in appendix b the corresponding pdf is given by 19 ψ v t e t 2 τ d 2 τ d so that the single visit times are exponentially distributed with mean 2 τ d 4 2 inter reaction times we are now in a position to compute the pdf of global delay times τ g which will allow us to obtain the pdf ϕ of the inter reaction time τ see eq 5 given a well mixed reactive time τ r u the delay time due to excursions away from the reactive region is given by 20 τ g u i 1 n u u w i where the w i are independent and identically distributed return times with pdf ψ d and n u u is the random number of visits to the reactive region given well mixed reactive time u for sufficiently fine discretization the residence times per visit to the reactive region are approximately exponential with mean 2 τ d eq 19 this implies that n u u is approximately poisson with mean u 2 τ d see e g kampen 1992 thus the global delay 20 is a compound poisson process so that its pdf has laplace transform benson et al 2007 benson and meerschaert 2009 meerschaert and sikorskii 2012 21 ψ g λ u exp λ ψ d λ u 2 τ d the pdf ϕ of the total inter reaction time τ resulting from a well mixed reaction time τ r with pdf given by eq 6 and a compound poisson delay has been obtained in aquino and dentz 2017 with the form 21 of the delay pdf it reads 22 ϕ λ ϕ r λ 1 ψ d λ 2 τ d in the continuum limit ℓ d 0 using eqs 6 and 12 we obtain 23 ϕ λ da da τ d λ g λ in this form the inter reaction times are manifestly well defined in the continuum limit the effect of diffusion near the interface is implicit in the form of eq 23 and additional effects arising from domain geometry and or velocity variability are encoded in the rescaled tail probability g see eq 16 4 3 dynamics of reactant mass the chctrw formulation can now be used to obtain the evolution of total mass given the inter reaction time pdf together with the additional delay time to first reach the interface see fig 1 this delay is distributed according to a pdf ψ 0 which depends on the transport mechanism and the initial reactant distribution defining 24 ψ 0 t t d t ψ 0 t the tail probability of the first passage time to the interface we obtain as shown in appendix c 25 d m d t k ϕ m m 0 ψ 0 this generalized integro differential rate law encodes the impact of transport and heterogeneity on the effective reaction kinetics through the inter reaction times and the first passage time to the interface in the standard ctrw description of transport advection dispersion at the small scales in the presence of heterogeneity leads to the emergence of memory kernels at larger scales in the advection dispersion equation accounting for statistical variability in particle jump sizes and or transit times berkowitz et al 2006 analogously the chctrw leads to a description of total mass which incorporates the impact of transport limitations through a memory kernel representing statistical variability in first passage and inter reaction times taking the laplace transform of eq 25 we obtain the laplace space solution for the evolution of total mass as 26 m λ m 0 da ψ 0 λ τ d g λ da τ d λ g λ while the number of visits to the reactive region within a given time window increases when the discretization is refined the residence time associated with each visit decreases accordingly and the total mass change due to reaction in the actual subscale reactive region is well defined these results connect the effect of transport limitations on mass dynamics to the inter reaction times of the chctrw under a transport mechanism for which mass exchange near a locally flat interface is controlled by diffusion in particular this is the case for advection diffusion since there is no flux into or out of the solid interface diffusion always dominates returns at sufficiently short times when particles are close to the interface this fact is incorporated into the form of eq 26 the rescaled tail probability g λ captures further effects controlling the statistical variability of the excursion times such as transverse velocity variations across the domain and the size of the latter as will be discussed in more detail and illustrated in the following sections note that this description does not require zero or low velocity regions to occur only near the interface the presence of stagnation regions affects g λ and may lead to broader variability of the return times and therefore of the inter reaction times we note that eq 26 can also be obtained in terms of the statistics of times spent near the interface rather than the statistics of the inter reaction times this alternative formulation is discussed in appendix d 5 reactive transport in a 2d channel in order to illustrate the results of the previous section we consider first the simple example of transport in an infinite two dimensional channel with stratified flow and reactive walls fig 1b i for which fully analytical results can be obtained in laplace space we take the characteristic length ℓ c as the channel half width 5 1 analytical first passage return times and mass dynamics the distribution of times to reach either wall from a given point along the channel cross section depends in this case only on transverse diffusion and not on the velocity profile it is thus sufficient to consider one dimensional diffusive transport along the channel cross section the corresponding pdf in a bounded domain of size l starting from a distance ℓ from one wall has laplace transform aquino and dentz 2018 27 ψ λ ℓ l csch ℓ 2 λ d csch l ℓ 2 λ d coth ℓ 2 λ d coth l ℓ 2 λ d in order to determine the first return time pdf in the sense discussed in the previous section we take ℓ ℓ d and l 2 ℓ c in preparation for taking the continuum limit ℓ d 0 we expand for small ℓ d ℓ c to obtain 28 ψ d λ exp tanh 2 τ d λ 2 τ d λ 1 tanh 2 τ d λ 2 τ d λ which according to eqs 12 and 18 corresponds to the form factor and rescaled tail probability 29 f λ tanh 2 τ d λ g λ tanh 2 τ d λ 2 τ d λ substituting g λ in eq 26 the total mass as a function of time has laplace transform 30 m λ m 0 2 da ψ 0 λ tanh 2 τ d λ 2 τ d λ 2 da tanh 2 τ d λ 2 τ d λ for small da we can expand this result for small λ 1 τ d time t 1 τ d before appreciable reaction occurs and we obtain 31 m λ m 0 da τ d λ m t m 0 e da t τ d hence the reaction kinetics correspond to a fully mixed cross section this arises because for slow reaction compared to the diffusive time scale diffusion homogenizes the fluid reactant to a mass per cross section length 1 2 ℓ c before appreciable reaction occurs since there are two reaction interfaces the fraction of mass in the reactive region is 2 ℓ d 2 ℓ c ℓ d ℓ c the effective reaction rate is thus k d ℓ d ℓ c da τ d k c a ℓ c independent of the diffusion coefficient for large da and an initial condition not fully concentrated at the channel walls so that ψ 0 λ 0 the initial condition dominates and we have 32 m t m 0 ψ 0 t this result can be interpreted as follows in the limit of fast reaction the surviving mass is that which has never reached the channel walls the fraction of the initial mass that has not reached the walls is given by the probability ψ 0 t that the first passage time to the walls from the initial position is greater than t in the particular case where all the mass starts at the channel walls ψ 0 λ 0 the dynamics are fully controlled by the return times for large da and λ 1 τ d early times we obtain 33 m λ m 0 da τ d 2 λ m t m 0 da τ d 2 π t for λ 1 τ d late times we find 34 m λ m 0 da τ d 1 2 τ d λ 3 m t 3 m 0 2 da e 3 t 2 τ d this means that diffusive excursions far from the walls control reaction until the channel cross section is homogenized by diffusion after which we recover exponential behavior but diffusion rather than reaction limited the exponent of the exponential decay in 34 depends only on the diffusion coefficient while that of 31 depends only on the reaction rate while the form of the mass dynamics eq 30 is flow profile independent the actual mass evolution can depend on the profile through the initial condition such as for a flux weighted condition in the low da limit discussed above the initial delay does not play a relevant role and this dependence disappears for definiteness consider two dimensional poiseuille flow with the velocity profile 35 v y v m 1 y 2 ℓ c 2 for y ℓ c ℓ c along the one dimensional cross section with v m the maximum velocity attained at the channel center y 0 the corresponding eulerian mean velocity is given by v 2 v m 3 different initial conditions affect the initial first passage time until the interface is first reached we consider four examples of instantaneous injection with the fluid reactant mass placed a at the middle of the channel b homogeneously along the channel cross section c flux weighted along the channel width and d at the channel walls the laplace transforms of the corresponding first passage time pdfs ψ 0 are given in appendix e 5 2 numerical simulations of reactive transport we performed ptrw simulations of the two dimensional reactive transport problem as described in section 2 and appendix a we nondimensionalize distances by ℓ c time by the diffusion time τ d and mass by the initial mass m 0 note that numerically this can be conveniently achieved by setting the diffusion coefficient to d 1 2 the channel half width to ℓ c 1 and the initial mass to m 0 1 in arbitrary units each of n particles initially carries a fraction m 0 n of the initial mass in order to obtain good statistics the number n of particles used must be such as to permit resolving the transverse direction to within the discretization length meaning n ℓ c ℓ d τ d τ d aside from setting the spatial resolution through ℓ d the time step must be sufficiently small to resolve reaction since the average residence time per visit to the reactive region is 2 τ d this requires 2 k d τ d 1 note that even though the initial condition c depends on the velocity profile the first passage time e 3 is independent of the mean velocity so that we can arbitrarily set v 1 the evolution of total mass under the different initial conditions a d is shown in fig 2 good agreement is observed between simulations and numerical inversion of eq 30 note how the da 10 2 case is identical for all initial conditions it corresponds to the low da limit eq 31 characterized by purely exponential decay and fully controlled by the reaction rate i e independent of the diffusion coefficient in a c the da 10 2 curves have converged to the da independent fast reaction limit eq 32 this regime is the most affected by the initial condition because it is controlled by the initial first passage time to the interface however the characteristic first passage times associated with conditions a c are all on the order of the diffusion time τ d and thus lead to qualitatively similar mass decay for d the evolution of the total mass is fully controlled by the return times to the reactive region eqs 33 and 34 the late time behavior in this case is also exponential but fully controlled by the diffusion coefficient i e independent of the reaction rate constant k and the solid phase surface concentration c a note how for moderate to high damköhler number da 1 the solution 31 corresponding to a well mixed cross section tends to overestimate reaction this is the case even for the uniform initial condition a because fast consumption of solute at the interface inhibits transverse homogeneity at early times in d reaction is faster than the well mixed channel prediction in these regimes due to solute starting in the reactive region but slower at later times as the surviving solute explores the channel cross section 5 3 reaction efficiency these results show that while reaction rates tend to increase with the damköhler number as would be expected the reaction efficiency compared to a well mixed channel cross section characterized by the same da decreases due to transport limitations in order to better understand this phenomenon consider the global or effective reaction rate 36 k e t d ln m t m 0 d t in other words the effective rate k e t is defined so that d m t d t k e t m t note that for pure exponential decay at constant rate m t m 0 exp k e t this definition recovers k e t k e as expected we define the reaction efficiency by comparing k e t to the reaction rate k e wm associated with well mixed conditions in the domain 37 ε t k e t k e wm according to eq 31 the well mixed reaction rate for the two dimensional channel is k e wm da τ d it is important to keep in mind the different physical meanings of the effective well mixed rate k e wm and the thermodynamic well mixed rate k in eq 2 the latter defined in units of inverse concentration per time is the usual batch reactor rate which governs fully well mixed reaction the former defined in units of inverse time represents the effective reaction rate occurring when the fluid phase domain is well mixed it is impacted by medium geometry as it accounts for the fact that part of the fluid reactant remains far from the interface we now obtain a theoretical expression for the asymptotic rate k e lim t k e t and the corresponding asymptotic reaction efficiency ε k e k e wm to this end we introduce the mean first passage time to the wall w 0 and the corresponding second moment 38 w 0 0 d t t ψ 0 t s 0 0 d t t 2 ψ 0 t we define also 39 α 0 s 0 2 τ d w 0 which represents a dimensionless measure of the variability in the first passage times compared to the diffusion time τ d expanding eq 30 for small λ 1 τ d and inverting the laplace transform we find the late time behavior 40 m t m 0 w 0 τ d da k e e k e t k e 2 da da w 0 τ d da 2 s 0 da τ d 2 w 0 4 τ d 3 2 τ d 2 so that 41 ε 1 τ d w 0 da 1 2 τ d 3 w 0 α 0 da τ d w 0 da 1 thus for small da we recover the well mixed rate as predicted by eq 31 and ε 1 for high da and an initial condition not concentrated at the channel walls the asymptotic reaction rate is limited by the first passage times to the interface and k e 1 α 0 τ d for a given initial condition this asymptotic rate cannot exceed a constant value since the well mixed rate is linear in da the asymptotic efficiency ε 1 α 0 da is inversely proportional to da in the special case of the initial condition at the channel walls we recover eq 34 and ε 3 2 da in this case reaction is limited by the return times to the wall and the asymptotic efficiency remains inversely proportional to da these results are illustrated for the case of a mid channel injection initial condition a in fig 3 from eq e 1 we find in this case w 0 τ d s 0 5 τ d 2 3 and α 0 5 6 as predicted the reaction rate becomes asymptotically constant for all damköhler corresponding to exponential decay however the reaction rate is initially variable as the fluid reactant explores the channel cross section for this choice of initial condition the reaction efficiency is initially zero because the solute is far from the interface and subsequently increases to the asymptotic value 6 generalization to other media in this section we will discuss the generalization of our approach to more complex scenarios where fully analytical results for the inter reaction times are not available as long as diffusion is the dominant transport mechanism near the reactive interface and the latter can be assumed flat at the scale of the transport model eq 26 may be used to predict the time evolution of total mass for arbitrary damköhler number given knowledge of the tails ψ 0 and ψ d of the first passage and return times in general analytical expressions for these quantities are not available and they must therefore be determined numerically however some general results about the mean return time if it exists may be obtained and employed to determine asymptotic reaction rates note that the existence of a mean inter reaction time is directly related to the existence of a mean return time to the interface if the excursions away from and back to the interface see fig 1 have sufficiently heavy tailed duration statistics the chctrw framework predicts a similar effect on reaction dynamics aquino and dentz 2017 such situations which we do not consider further here but can be captured in our framework can be important when modeling scenarios where solute may be retained in large regions of low velocity away from the reactive interface or where the distribution of distances between separate reactive regions is very broad and not well characterized by a mean value aquino and dentz 2020 6 1 mean return times and effective reaction rate the asymptotic reaction rate for stratified flow in a 2d channel was obtained in section 5 eq 41 in that case the limit of low da corresponding to slow reaction leads to an effective reaction rate of da τ d eq 31 we begin by generalizing this result by considering the mean return time to the interface 42 w d 0 d t t ψ d t we have for the return time pdf ψ d λ 1 w d λ for λ 1 w d so that the rescaled tail probability eq 16 obeys 43 g 0 0 d t g t w d ℓ c 2 τ d ℓ d taylor expanding eq 26 for small λ and inverting the laplace transform 44 m t m 0 e k e t with the effective reaction rate 45 k e 2 da ℓ d w d ℓ c in order to relate the mean return time w d to medium structure we now determine the effective rate in a well mixed domain consider that the solute is well mixed over a region of volume v within which the volume of the reactive region is v d the reactive region comprises a layer of height ℓ d immediately adjacent to the interface so that v d a i ℓ d where a i is the interface area the mass in the reactive region is then m t v d v and reacts at rate k d see eq 3 so that 46 d m t d t k e wm m t with the well mixed effective reaction rate given by 47 k e wm v d v k d ρ da τ d where we have introduced the dimensionless interface extent coefficient 48 ρ a i ℓ c v encoding the amount of interface area per unit volume note that if a representative elementary volume for interface area exists and the solute is well mixed over a larger volume ρ is given by this ratio over the representative volume note also that for an empty channel of cross section a we have 49 ρ ℓ i ℓ c a where ℓ i is the total interface length intersecting the cross section if the medium is statistically homogeneous along the flow direction this formula may also be used to compute ρ given information about the interface extent over a cross section taking a as the fluid phase area over the latter if the concentration becomes well mixed due to transport before appreciable reaction occurs the effective reaction rates associated with the return time and well mixed domain pictures must coincide k e k e wm using eqs 45 and 47 we find the mean return time and associated effective reaction rate as 50 w d 2 τ d ℓ d ρ ℓ c k e ρ da τ d note that these results agree with the example considered in section 5 in that case ℓ i 2 a 2 ℓ c ρ 1 and k e da τ d note also that w d is independent of the damköhler number so that according to eq 43 51 g 0 ρ 1 independent of da so long as the transport mechanism leads to a well mixed state in the absence of reaction we note that as before for large da fast reaction and an initial condition not concentrated along the interface reaction is controlled by the first passage times to the interface and m t m 0 ψ 0 t in that case the reaction dynamics are thus controlled by the initial reactant distribution 6 2 reaction efficiency we can now generalize the results for the asymptotic reaction efficiency using eqs 16 and 51 we find the small λ 1 τ d expansion for the rescaled tail probability 52 g λ ρ 1 1 α λ 2 where α is defined by 53 α s d 2 τ d w d s d 0 d t t 2 ψ d t quantifying the variability in the return times to the interface relative to the diffusion time using this result to expand eq 26 for λ 1 τ d and inverting the laplace transform we find the late time behavior 54 m t m 0 ρ da w 0 τ d da k e e k e t k e 2 ρ da da w 0 τ d ρ 2 da 2 s 0 ρ da τ d 2 w 0 α 2 τ d 2 we thus conclude that as long as the transport mechanism leads to a well mixed state in the absence of reaction the reaction rate always asymptotes to a constant at late times for arbitrary da using eqs 37 and 47 the corresponding asymptotic reaction efficiency is 55 ε 1 τ d w 0 ρ da 1 α α 0 ρ da τ d w 0 ρ da 1 see also eqs 38 and 39 for the 2d channel case ρ 1 and α 2 3 from eq 28 so that this result reduces to eq 41 in the low da limit of slow reaction we recover eq 47 for the well mixed reaction rate and obtain ε 1 as expected for high da and an initial condition not concentrated at the channel walls we have the initial condition limited results k e 1 α 0 τ d and ε 1 α 0 ρ da in the special case of an initial condition distributed over the interface for which w 0 s 0 0 we find k e 1 α τ d emphasizing that in this case the transport limitations come into play through the return times rather than the first passage times as before ε 1 α ρ da remains inversely proportional to da 6 3 mass dynamics from numerical first passage and return statistics according to eq 16 g λ is proportional to ψ d λ thus mass dynamics can be quantified through eq 26 by numerically sampling first passage and first return times in ptrw simulations of the transport for a given initial condition the laplace transform ψ 0 λ can be found directly from the fraction of first passage time samples to the interface above a given time to determine the first return times associated with discretization length ℓ d for a given medium and flow we take a single particle initial condition chosen uniformly randomly over the surface at perpendicular distance 2 ℓ d from the interface corresponding to a distance ℓ d from the reactive region once the reactive region is reached we record the duration of the excursion place the particle at the closest point at distance 2 ℓ d from the interface and repeat the process for the next excursion a prescribed number of times note that in the cases considered here the system is asymptotically well mixed over a representative region so that different points along the interface are revisited with the same probability this means that equivalent results would be obtained by considering the first passage time to within ℓ d of the interface for a set of particles initially distributed uniformly over the latter a comparison of the rescaled tail probabilities g t obtained in this manner for one dimensional diffusion in a bounded domain of half width ℓ c along with the rescaled first return time pdf ψ d t ℓ c ℓ d with those obtained from numerical inversion of the analytical laplace transform of ψ d λ eq 27 with ℓ ℓ d and l 2 ℓ c is shown in fig 4 for two different values of ℓ d we nondimensionalize distances by ℓ c and times by τ d as before for t δ t the return time statistics converge to the same discretization independent behavior above the discretization timescale τ d the late time scalings follow those of pure diffusion in an semi infinite domain eq 9 until the effect of the far boundary is felt leading to a cutoff on a characteristic timescale of order τ d corresponding to exploring the full domain 6 3 1 reactive transport in a 3d channel as a first example to verify the results for the mass dynamics using numerical estimation of the first passage and return times consider transport in a three dimensional cylindrical channel with the characteristic length ℓ c given by the channel radius fig 1b ii we take a point injection at the center of the channel as the initial condition as before it suffices to consider diffusion along a cross section in order to determine the evolution of total mass independent of the flow field as long as it is assumed to be stratified we again nondimensionalize distances by ℓ c time by τ d and initial mass by m 0 the results obtained from numerical inversion of eq 26 given numerical determination of the first passage and return times are in good agreement with reactive ptrw simulations as shown in fig 5 a in this case the cross section area is given by a π ℓ c 2 and the interface length intersecting a cross section by ℓ i 2 π ℓ c so that the interface extent coefficient is ρ 2 eq 49 for da 10 2 we find good agreement between the numerical simulations and exponential decay according to the effective well mixed reaction rate eq 47 with ρ 2 whereas for higher da deviations from the well mixed behavior are observed because transverse diffusion is incapable of homogenizing the cross section under fast reaction for da 10 2 the numerical simulations agree with the fully transport limited solution m t m 0 ψ 0 t the reaction efficiency ε t corresponding to these dynamics is shown in fig 6b the theoretical asymptotic efficiency ε eq 55 evaluated using the first and second moments of the numerically determined first passage and return times is in good agreement with the simulations although some quantitative differences are discernible in the mass decay compare figs 5a and 2 a the reaction dynamics are remarkably similar to the two dimensional channel case especially regarding the reaction efficiency compare figs 5b and 3 a 6 3 2 reactive transport in a body centered cubic beadpack we now investigate the application of our approach to the case of reactive transport in a crystalline porous medium recall that the inter reaction time approach presented here relies on the assumption that a statistical description of the return times with stochastic return times but with the same statistics applying to each return to the interface is sufficient to characterize reaction we assess this hypothesis for advective diffusive transport in a body centered cubic beadpack turuban et al 2019 where the structure of the medium is periodic but not all points on the interface within a unit cell are equivalent in their positioning relative to the rest of the interface fig 1b iii for details on the numerical simulations see appendix a as shown in appendix f the interface extent coefficient for this system is given by 56 ρ 3 π 2 1 3 π 8 1 14 7 the relative importance of advection compared to diffusion can be quantified through the péclet number which we define in terms of the absolute value v of the eulerian mean velocity vector and the characteristic lengthscale ℓ c as 57 pe ℓ c v d the total mass as a function of time for different damköhler numbers and pe 10 3 is shown in fig 6 a for an initial condition uniformly distributed over a conventional unit cell see fig 1b iii the inter reaction time approach parameterized according to the numerically determined first passage and return tails ψ 0 and ψ d as discussed above is in good agreement with the reactive ptrw simulations for low da the mass evolution agrees with the effective well mixed decay eq 46 this happens because for low reaction rates the fluid reactant remains homogeneous over a representative region of pore volume as for the channel examples deviations from the well mixed behavior become more pronounced as the damköhler number increases and transport is unable to efficiently homogenize the fluid phase concentration the corresponding reaction efficiency ε t is shown in fig 5b the theoretical asymptotic efficiency ε eq 55 evaluated using the first and second moments of the numerically determined first passage and return times is again in good agreement with the simulations in this case the homogeneous character of the initial condition results in a reaction efficiency that is initially unity for high damköhler reaction tends to destroy homogeneity which results in a decrease of the reaction efficiency towards the asymptotic value in fig 7 we show the total mass as a function of time for the same damköhler numbers as in fig 6 and different values of péclet number computed using the inter reaction time approach although we refrain from showing these results in fig 6 to avoid clutter we verified that ptrw simulations show similar very good agreement as for fig 6a across péclet numbers for sufficiently high da the reaction is slower than the effective well mixed prediction but approaches the latter when the péclet number becomes sufficiently high as advection induced transverse dispersion effects become important compared to diffusive mixing in order to better understand the role of the péclet number in the reaction dynamics we examine the first passage and return time statistics arising under different transport regimes see fig 8 the flow considered here is known to induce chaotic mixing lester et al 2016 turuban et al 2019 heyman et al 2020 2021 this means that the flow efficiently homogenizes the concentration on the cross section of the pore space transverse to the mean flow direction over a characteristic advective mixing timescale for sufficiently low péclet number as long as the advective mixing timescale is large compared to the typical diffusive first passage and return times transverse mixing is carried out by diffusion and advection does not have an appreciable effect on reaction thus the first passage and return times along with the mass dynamics are similar for pe up to 10 2 when advective effects become important particles far from the interface are brought towards it faster than by diffusion alone on the other hand particles at intermediate distances can take longer to reach the interface than they would have by diffusion lastly very low return times remain controlled by diffusion this effect on the first passage and return times can be clearly seen for pe 10 3 whichever effect is dominant the average return time reflects the fact that at sufficiently late times concentration is well mixed and remains given by eq 50 thus the low da reaction behavior which depends only on the average return time is unaffected by the mixing mechanism and remains unchanged see fig 7 however the distributions of first passage and return times become less broad with increasing péclet fig 8 and the effective well mixed regime is reached faster for this reason the reaction dynamics approach the effective well mixed behavior at higher values of the damköhler number the strength of this effect increases with increasing pe fig 7 we note that for higher péclet numbers pe 10 4 the corresponding strongly advection dominated transport simulations become particularly sensitive to the underlying flow velocities accurate results for such cases would require more finely resolved flow fields and we refrain from simulating them here the first passage and return time dynamics in the two and three dimensional channel and beadpack examples exhibit some qualitative similarities due to the role of diffusion in transverse mixing and its dominant role near the fluid solid interface indeed the corresponding distributions exhibit clear diffusive scalings followed for long times by cutoffs see figs 4 and 8 the characteristic timescale associated with the cutoff and therefore the characteristic variability in inter reaction times depends on medium geometry and in the case of the beadpack is also impacted by advective mixing for large péclet number correspondingly the reactant mass dynamics in these different systems exhibit similarities such as late time exponential decay but also differences in the onset of transport limitations with damköhler number and the effective reaction rates associated with a well mixed system see figs 2 5 6 and 7 in particular the interface extent coefficient ρ quantifying the available surface area per unit volume increases compared to the 2d channel by a factor of 2 in the 3d channel and about 15 in the beadpack effectively increasing the well mixed reaction rate in these systems for a given value of the damköhler number transport limitation effects become important for ρ da 1 although they are mitigated in the beadpack when the péclet number is large and advection induced mixing plays a role 7 discussion and conclusions in this work we have developed a new framework to quantify the effect of transport limitations on fluid solid reaction dynamics in porous media our approach based on the chemical continuous time random walk theory of inter reaction times relates the statistics of solute excursions away from and back to the fluid solid interface to reaction times we have shown that the dynamics of effective reaction rates relate to the statistics of inter reaction times which are in turn controlled by transport and medium geometry we have illustrated the approach analytically for advection diffusion reaction in stratified flow through a two dimensional channel and provided a generic numerical approach to determine the corresponding dynamics in more complex media and flow fields for fast reactions reactant mass is controlled by the first passage time of solute to the reactive solid interface for slow reactions multiple excursions to the interface are necessary before reaction occurs in the latter case the statistics of the durations of these excursions which are sensitive to diffusive mass transfer near the interface become a dominant control on mass dynamics consistently with numerical simulations our theory predicts that for intermediate and high damköhler number these effects can lead to significant reaction slowdown due to transport limitations even for the simple reaction studied here and even in simple geometries we provide analytical expressions for the late time effective reaction rate as a function of damköhler number which exhibits a transition from the well mixed reaction rate at low da to the inverse of the diffusion time at large da a direct consequence of transport limitations the theory presented here also leads to a useful numerical framework fluid solid reactions pose unique computational and theoretical challenges as they require resolving transport dynamics close to the interface in a consistent manner here we have analyzed a simple fluid solid reaction in detail and shown how the assumption of scale separation between reaction and transport dynamics near the interface leads to a consistent continuum model of surface reaction resolved numerical simulations of these dynamics are computationally demanding the inter reaction time approach presented here opens up new possibilities for efficient numerical computation of mass dynamics by extending existing theoretical frameworks for the determination of first passage times condamin et al 2007 bénichou et al 2010a aquino et al 2019 aquino and dentz 2020 and or numerical techniques for efficient first passage time computation atiya and metwally 2005 shalloway and faradjian 2006 schnoerr et al 2017 this work has been mainly concerned with introducing the concepts and methodology underlying a new approach to upscaling fluid solid reaction dynamics as such we have made significant simplifications regarding the reaction chemistry and porous medium structure the framework developed here brings new perspectives to link the statistical characteristics of medium geometry and flow to reaction dynamics in a broad range of porous media which will be the subject of future work furthermore we expect an approach similar to that presented here to be applicable to the dynamics of mass breakthrough as a function of distance in terms of the statistics of inter reaction distances future work will also aim to generalize the approach to higher order reactions involving multiple transported components multiple simultaneous reactions and heterogeneity spatial and temporal variability in the solid phase reactant distribution along the interface credit authorship contribution statement tomás aquino conceptualization methodology software writing original draft tanguy le borgne conceptualization writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments ta is supported by a marie skłodowska curie individual fellowship funded by the european union s horizon 2020 research and innovation programme under the project chemicalwalks 838426 tlb gratefully acknowledges funding by the erc under the project reactivefronts 648377 the authors thank joris heyman for providing the numerical flow data used in the beadpack ptrw simulations and guillem sole mari for useful comments and suggestions a c implementation of the particle tracking algorithms used to generate the numerical data employed in this work are available under an open source license github github com tcaquino beadpack doi 10 5281 zenodo 4392882 related data is also available doi 10 5281 zenodo 5007336 please contact the authors for further information or requests appendix a particle tracking simulations in this appendix we discuss some details of the ptrw particle tracking random walk simulation method used in this work lagrangian particle tracking methods which simulate particle trajectories based on a random walk have been found to mitigate numerical dispersion issues typically associated with more traditional eulerian methods and their impact on fluid fluid reactions benson et al 2017 however particle tracking methods are less developed and widespread than eulerian methods and we are not aware of an available tool for lagrangian simulation of fluid solid reaction dynamics in the present work a ptrw approach has the added benefit of directly simulating lagrangian trajectories the properties of which are the basis for the theoretical developments this allows us to employ the same methodology to simulate the conservative component of the reactive transport problem and to determine first passage and return times see sections 5 and 6 of the main text the ptrw method used here is based on that employed in aquino and borgne 2021 to simulate conservative transport in a number of stratified flows where it was validated against theoretical predictions for dispersion concentration distributions breakthrough curves first passage times across a plane and lagrangian velocity distributions the conservative transport algorithm consists in discretizing the langevin equation 1 for a set of lagrangian particles or trajectories with prescribed initial conditions the displacement δ x t of a particle in a time step of duration δ t starting at time t is computed as the sum of the advective and diffusive contributions to the change in particle position x t a 1 δ x t x t δ t x t δ x a t δ x d t boundary conditions at the solid interface are implemented as elastic collisions based on the full displacement δ x t the diffusive contribution is obtained from a stochastic forward euler scheme as δ x d t 2 d δ t ξ t where the component of ξ t along each cartesian axis and at each time t is sampled independently from a unit zero mean and unit variance gaussian distribution for the two dimensional channel simulations of section 5 the advective contribution is also computed based on a forward euler scheme δ x a t v x t where the velocity field v is evaluated at particle positions according to the analytical flow field eq 35 here we have extended the algorithm to account for i elastic reflections on bead surfaces representing reflecting boundary conditions ii advection due to an arbitrary velocity field previously obtained and provided at a number of points on an arbitrary possibly unstructured grid and iii fluid solid reaction regarding i we implemented and independently verified a simple trigonometry based computation of perfectly elastic collisions on spherical surfaces valid in arbitrary dimension for circles in the plane etc regarding ii the flow field at a given position is obtained from linear interpolation based on delaunay triangulation and sibson natural neighbor coordinates on an arbitrary grid based on the well established cgal c library project 2020 for the beadpack simulations the advective particle displacement associated with the interpolated flow is computed according to a fourth order explicit runge kutta scheme finally iii is implemented according to the considerations of section 2 as discussed therein the spatial support scale ℓ d should be large compared to 2 d δ t in order to permit resolving fluid reactant concentrations near the interface and the resulting reaction rates we set the support scale a 2 ℓ d 10 2 d δ t it is important to note that ℓ d is not a parameter of the theoretical model but is rather associated with the discretization in this sense taking ℓ d a 2 d δ with a 10 leads to the same results in the continuum limit δ t 0 it is however necessary to take a 10 in order to guarantee that reactant concentrations near the interface are resolved correctly and converge for δ t 0 the diffusion time associated with the support scale ℓ d is τ d ℓ d 2 2 d thus our choice corresponds for a given ℓ d to setting δ t τ d 100 the algorithm employed to compute first passage and return times is validated here by the agreement with theoretical results for the one dimensional channel see section 5 similarly the agreement between theory and simulations found for the dynamics of total mass especially regarding the theoretical results for low and high damköhler number and the assymptotic reaction rates for arbitrary damköhler number discussed in sections 5 and 6 provides validation for the reactive transport algorithm we note that we have implemented only the a f b s b s reaction studied here but the reaction algorithm can be direclty extended to other reactions and heterogeneous distributions of solid phase reactants at the expense of explicitly keeping track of the surface concentrations of the latter across the fluid solid interface a github repository where the code is available under an open source license and regularly updated can be found at github com tcaquino beadpack further benchmarks and extensions to the fluid solid reaction algorithm will be included and discussed thereat the version of the code used in the present work can be found at the doi 10 5281 zenodo 4392882 additional documentation addressing further technical details may also be found in both these repositories in order to simulate reactive transport and determine first passage and return times using the ptrw algorithm we first require the flow field which may be given at a set of points on an arbitrary grid for the beadpack simulations of section 6 the flow field was obtained numerically from stokes flow simulations using a finite volume discretization in openfoam foundation 2020 with n c 1572138 hexahedral cells the flow was computed in a conventional cubic unit cell of side ℓ c 1 cm of the body centered cubic packing considered the associated body cubic centered bead diameter is d 3 ℓ c 2 simon 2013 corresponding to a spatial resolution of about ℓ c n c 1 3 10 2 d periodic boundary conditions were imposed on the cell faces and no slip boundaries at the spherical bead interfaces the flow had kinematic viscosity 5 10 7 m 2 s and was driven by a pressure gradient of 1 pa m obtained by applying a body force in terms of cartesian axes perpendicular to the unit cell faces the mean velocity vector had an orientation corresponding to an angle 5 π 40 with the x axis on the x y plane and an angle π 40 with the z axis this flow configuration has been analyzed in turuban et al 2019 and it is known to induce chaotic mixing as discussed in the same reference the contact points between beads can cause numerical instabilities in the numerical determination of the flow field this issue can be addressed by placing small spheres or cylinders with no slip boundary conditions on the exposed surface at the contact points between beads to avoid numerical issues near the contact points these structures do not have an appreciable impact on flow and transport properties turuban et al 2019 in the present simulations spheres of 20 the bead diameter were employed in this manner these contact spheres are not needed for stability of the ptrw simulations in the conservative and reactive transport simulations no boundary conditions were enforced on their surface and zero velocity points were added at the contact points instead particle positions undergoing advection diffusion were mapped onto the unit cell according to the periodic boundary conditions appendix b statistics of single interface visit times here we obtain the pdf of the time spent in the reactive region in each visit consider again one dimensional diffusion close to the interface the first passage time from the middle of the discretization cell nearest the interface to the middle of the adjacent cell corresponds to the duration of a visit to the reactive region it can be obtained by considering the first passage time to either boundary in a domain of length 3 ℓ d and starting from a distance ℓ d from one boundary as illustrated in fig b 9 the first passage time pdf to reach either boundary in a domain of length l starting from a distance ℓ from one and l ℓ from the other has laplace transform aquino and dentz 2018 b 1 ψ λ ℓ l csch ℓ 2 λ d csch l ℓ 2 λ d coth ℓ 2 λ d coth l ℓ 2 λ d setting l 3 ℓ d and ℓ ℓ d yields the single visit pdf b 2 ψ v λ 2 cosh 2 τ d λ 1 1 expanding for λ 1 τ d and inverting the resulting laplace transform leads to eq 19 recall that λ 1 τ d represents large times compared to the discretization time so that this result constitutes a step in taking the continuum limit rather than a large time approximation as before the same conclusion is reached if we consider the first passage times to within ℓ d of the interface starting from a distance 2 ℓ d from the latter this result remains valid in higher dimensions under the assumption that interface is locally flat at the scale of the transport model appendix c impact of first passage time to the interface on reaction dynamics according to the chctrw framework in the absence of the delay associated with first reaching the interface the generalized rate law governing the evolution of total fluid reactant mass is given by eq 8 when this delay is added the mass is equal to the initial mass for times smaller than the delay and evolves according to the previous equation for later times that is averaging over initial delays i e over an ensemble of lagrangian particles distributed according to the initial condition c 1 m t m 0 0 t d t ψ 0 t 0 t d t ψ 0 t m ϕ t t the first term represents the fact that if the interface has not been reached for the first time by time t which happens with probability 0 t d t ψ 0 t over the ensemble of lagrangian particles the fluid mass is equal to its initial value m 0 m 0 regarding the second term if the interface is reached for the first time at time t t which happens with probability density ψ 0 t the mass at time t is equal to m ϕ t t note that given a first arrival at the interface at time t t m t t m ϕ 0 m 0 then for times t t the mass dynamics proceed according to the inter reaction times with m ϕ obeying the chctrw equation 8 this result may be written in terms of the first passage time tail probability ψ 0 as c 2 m m 0 ψ 0 m ϕ ψ 0 substituting eq 8 for the dynamics of mass m ϕ resulting from the inter reaction times we obtain eq 25 appendix d mass dynamics and reactive time the dynamics of reactant mass may be formulated in terms of the statistics of the total time spent in the reactive region rather than the statistics of the inter reaction times in this appendix we show that the former approach leads to the same results as the latter which is developed in the main text consider the reactive time u d t representing the total time spent within distance ℓ d of the interface we denote its pdf by p u t that is p u u t d u is the probability that given total elapsed time t a particle has spent a time in the interval u u d u within distance ℓ d of the reactive interface for small ℓ d in preparation for taking the continuum limit ℓ d 0 we approximate d 1 u d t 2 τ d n d t where n d t is the random number of visits to the reactive region by time t each of which contributes the mean residence time per visit 2 τ d see eq 19 since u d t is proportional to n d t denoting by p n n t the probability that n d t n we have d 2 p u u t p n 0 t δ u 2 τ d 1 p n u 2 τ d t where δ is the dirac delta the number of visits to the reactive region by a given time depends on the pdf of inter visit delay times ψ d as well as on the pdf ψ 0 of the time of the first visit adapting the results of benson et al 2007 the laplace transform of the distribution of the number of visits to the reactive region with respect to time t in terms of these quantities is given by d 3 p n n λ ψ 0 λ n 0 ψ d λ ψ 0 λ ψ d λ n 1 n 0 in the limit of small ℓ d we obtain according to eq d 2 d 4 p u u λ ψ 0 δ u λ 1 ψ 0 λ e d u λ where d 5 e d u λ λ g λ ℓ c ℓ d exp λ g λ ℓ c ℓ d u with the rescaled tail probability g defined in terms of the return time tail ψ d according to eq 16 to quantify the dynamics of total mass in terms of the reactive time u d t we make use of a subordination type description feller 2008 benson and meerschaert 2009 meerschaert and sikorskii 2012 aquino and dentz 2017 within the reactive region reaction is well mixed in the sense described in section 2 the overall reaction then proceeds according to the time particles spend within this region the amount of mass left by time t is the average of the surviving mass m 0 exp k d u d t over all possible times u d t spent within ℓ d of the interface up to time t which are distributed according to p u t that is d 6 m t m 0 0 d u e k d u p u u t the laplace transform of the total mass 26 obtained using the inter reaction time formulation is then recovered by direct computation in the limit ℓ d 0 in terms of a laplace transform with respect to operational time denoted by a hat keeping time t fixed we may write eq d 6 as d 7 m t m 0 p u k d t which can also be interpreted as the laplace transform of the stochastic process u d ℓ d evaluated at the rate ℓ d k d k c a in the limit ℓ d 0 of fine discretization the stochastic process u d ℓ d is the local time at the boundary mentioned in the introduction karatzas and shreve 1988 takács 1995 grebenkov 2007 2019 note that in accordance with the standard terminology used in the literature the so called local time has units of time per length it is this quantity rather than u d itself that is well defined in the continuum limit this is directly related to the fact that a particle undergoing continuous diffusion in one dimension returns to the initial position infinitely many times within an arbitrarily small time interval despite the fact that this represents a mathematical abstraction it corresponds to the correct behavior when transport is adequately described by continuous diffusion at the scale of interest appendix e first passage times to the wall in a 2d channel in this appendix we provide the laplace transforms of the first passage time pdfs of solute to the channel walls for transport in stratified flow through a two dimensional straight channel as discussed in section 5 these are controlled by diffusion in the transverse direction although the velocity profile can impact the result due to its role in determining the initial distribution for example for flux weighted injections the different first passage times are obtained by using eq 27 for a point injection and weighting according each initial condition thus the mid channel injection a leads to e 1 ψ 0 λ ψ λ ℓ c 2 ℓ c sech 2 τ d λ for the homogeneous injection b we have e 2 ψ 0 λ 1 2 ℓ c 0 2 ℓ c d ℓ ψ λ ℓ 2 ℓ c tanh 2 τ d λ 2 τ d λ for the flux weighted case c we find e 3 ψ 0 λ 1 2 ℓ c 0 2 ℓ c d ℓ v ℓ ℓ c v ψ λ ℓ 2 ℓ c 3 2 τ d λ 1 tanh 2 τ d λ 2 τ d λ and for all mass starting at the channel walls d e 4 ψ 0 λ 1 appendix f interface extent coefficient for the body centered cubic beadpack in order to compute the interface extent coefficient ρ for a body centered cubic beadpack first note that the bead radius is related to the conventional cubic unit cell side by r 3 ℓ c 4 where we have taken the cell side ℓ c as the characteristic length see e g simon 2013 on the theory of crystalline structures within a unit cell there is a full bead at the center and eight bead quarters at each cell corner totaling a solid volume of two full beads since the volume of a bead is v b 4 π r 3 3 the porosity is given by f 1 φ 1 2 v b ℓ c 3 1 3 π 8 0 320 the surface area of a bead is a b 4 π r 2 so that according to eq 48 we have f 2 ρ 2 a b ℓ c φ ℓ c 3 3 π 2 1 3 π 8 1 
280,fluid solid reactions play a key role in a large range of biogeochemical processes transport limitations at the pore scale limit the amount of solute available for reaction so that reaction rates measured under well mixed conditions tend to strongly overestimate rates occurring in natural and engineered systems although different models have been proposed to capture this phenomenon linking pore scale structure flow heterogeneity and local reaction kinetics to upscaled effective kinetics remains a challenging problem we present a new theoretical framework to quantify these dynamics based on the chemical continuous time random walk framework we study a fluid solid reaction with the fluid phase undergoing advective diffusive transport we consider a catalytic degradation reaction a f b s b s where a f is in fluid phase and b s is in solid phase and homogeneous over the fluid solid interface allowing us to focus on the role of transport limitations and medium structure our approach is based on the concept of inter reaction times which result from the times between contacts of transported reactants with the solid phase we use this formulation to quantify the global kinetics of fluid reactant mass and test our predictions against numerical simulations of advective diffusive transport in stratified channel flow and stokes flow through a beadpack the theory captures the decrease of effective reaction rates compared to the well mixed prediction with increasing damköhler number due to transport limitations although we consider simple kinetics and media these findings will contribute to the understanding and modeling of the effect of transport limitations in more complex reactive transport problems keywords reactive transport stochastic modeling chemical continuous time random walk 1 introduction biogeochemical reactions at the interface between fluid and solid medium phases play a central role in a large range of reactive transport problems such as contaminant transport and degradation soil remediation mineral weathering and carbon dioxide sequestration chapelle 2001 appelo and postma 2004 brantley et al 2008 biotic and abiotic reactions at solid fluid interfaces include dissolution precipitation adsorption complexation and redox reactions the kinetics of these reactions on solid surfaces depend directly on the concentration of solutes in the fluid phase which evolve in time and space through flow and transport dynamics therefore much effort has been invested into the development setup and choice of detailed reactive transport models to quantify these processes and their interaction with transport and medium geometry steefel et al 2005 li et al 2017 maher and navarre sitchler 2019 the basic quantification of the kinetics of such reactions is generally performed using well mixed batch experiments yet transport limitations at the pore scale lead to large deviations from these estimates by reducing access of solutes to reactive surfaces compared to fully mixed systems this phenomenon has been observed in resolved numerical simulations of carbonate mineral dissolution in porous media molins et al 2012 numerical simulation and column experiments of calcite dissolution li et al 2007 2008 molins et al 2014 numerical simulations of mineral dissolution in heterogeneous porous media soulaine and tchelepi 2016 soulaine et al 2017 pore scale reactive transport simulations in rough fractures deng et al 2018 and batch experiments and field scale modeling of biodegradation of dissolved organic carbon in aquifers kang et al 2019 pore scale flow and structure have also been found to significantly impact adsorption to mineral surfaces in porous media an effect which has been observed in detailed lattice boltzmann simulations vanson et al 2015 2017b 2017a these studies have consistently found that reaction rates are significantly lower than expected from classical well mixed theories especially when reaction is fast compared to transport processes volume averaging techniques battiato and tartakovsky 2011 battiato et al 2009 have been employed to identify general conditions under which classical macroscopic models of reactive transport break down and transport limitations lead to decreased global reaction rates and or modified rate laws random walk models have been used to investigate the impact of transport on surface reactions for simple geometries such as sinusoidal channels sund et al 2015 sherman et al 2019 the role of available reactive surface area beckingham et al 2016 2017 and surface roughness deng et al 2018 in mineral dissolution in porous and fractured media has been analyzed and quantified through experiments and numerical simulations however a quantitative link between pore scale transport dynamics and effective fluid solid reaction kinetics remains unavailable furthermore in practice highly resolved numerical simulations can be prohibitively expensive and detailed knowledge of the dynamics and spatial distribution of physico chemical properties is often not available stressing the need for upscaled models of reactive transport dentz et al 2011b in well mixed batch reactors reactant concentrations are spatially homogeneous in the lagrangian particle picture this corresponds to every particle being instantaneously available to participate in a reaction with every other particle this deterministic picture can be extended to account for stochastic variability for small particle numbers while retaining the well mixed assumption gillespie 1977 this is achieved through the concept of inter reaction times which represent the time between the occurrence of sequential reaction events amongst sets of reactants in accordance with the chemical reactions in the classical stochastic theory reactants are assumed to be fully mixed in the sense that all sets of reactants allowed by the chemistry have the same probability of reacting this leads to exponentially distributed inter reaction times representing a probability per unit time of reaction that is fully determined by the thermodynamic reaction rate and the available reactant numbers at a given time the classical well mixed rate laws under which reaction rates correspond to products of reactant concentrations with powers determined by the reaction stoichiometry are recovered in the limit of large particle numbers gillespie 1992 in practice this picture holds only if diffusion is sufficiently fast to locally homogenize reactants so that the limiting factor in determining reaction rates lies in the thermodynamic properties of the reaction rather than transport fluid solid reactions involve transported and immobile reactants solid phase reactants are located at the interface between a fluid phase in which fluid phase reactants are transported and a solid phase of the underlying medium the first explicit model of the impact of transport on reaction is due to von smoluchowski 1917 it quantifies contact reactions between a hard sphere and a sea of diffusing particles and it leads to an effective time dependent reaction rate which depends on transport properties namely the diffusion coefficient because there is no fluid flow into or out of the solid interface mass flux of fluid reactants allowing contact with solid phase reactants is ultimately governed by diffusion on the other hand advective transport along streamlines may bring reactants closer or farther from the solid phase thus in the inter reaction time picture discussed above the combined effect of medium heterogeneity advective variability and diffusion introduces reaction delays in terms of the first passage times of reactants to the solid phase quantifying this effect and its impact on reaction rates is therefore fundamental for modeling fluid solid reactions in porous and fractured media recently the chemical continuous time random walk chctrw framework was developed in order to relax the well mixed assumption in stochastic reaction modeling leading to inter reaction times which encode the effect of local transport limitations through additional reaction delays due to transport limitations aquino and dentz 2017 the chctrw hence quantifies the effect of broader distributions of the times required for sequential reaction events to occur despite the formal similarities this differs conceptually from the classical ctrw framework which quantifies the effect of broadly distributed times or distances associated with particle displacements berkowitz et al 2006 such reaction delays can be quantified in terms of the first passage times of reactant particles across each other mcadams and arkin 1997 bénichou et al 2010a b meroz et al 2011 godec and metzler 2016 in the case of fluid solid reactions these are related to the duration of excursions between visits to the solid interface the latter are closely related to the time spent near the interface which can be formally quantified through the so called local time at the boundary which represents the amount of time spent in a thin region near the interface divided by the region thickness in the limit of vanishing thickness karatzas and shreve 1988 takács 1995 grebenkov 2007 2019 the concept of modeling reactive transport in terms of exposure time that is the time that reactants spend in close proximity and so are available for reaction has received some attention over the past decade ginn 1999 2000b 2000a seeboonruang and ginn 2006 sanz prat et al 2016 nonetheless the relationship between exposure time and flow and medium heterogeneity remains little understood the central goal of the present work is to formalize the notion of inter reaction times and their impact on reaction dynamics in the context of fluid solid reactions under advective diffusive transport in order to better undertand and upscale the impact of flow transport and medium structure on global reaction rates we consider here a catalytic degradation reaction a f b s b s a simplified chemical setup which allows us to focus on the role of transport limitations the reactant species b s is taken to be in solid phase immobile and homogeneously distributed over the fluid solid interface whereas the reactant species a f is in fluid phase and undergoes advective diffusive transport the impact of disordered i e random and uncorrelated at different spatial locations distributions of solid phase reactants and residence times on this type of reaction has been studied for diffusive and subdiffusive transport i e transport phenomena where plume variance grows sublinearly in time and trapping using random walk models grassberger and procaccia 1982 kayser and hubbard 1983 klafter et al 1984 weiss 1986 bouchaud and georges 1990 yuste and acedo 2004 lapeyre and dentz 2017 aquino et al 2019 and purely advective transport in a streamtube model using the chctrw framework aquino and dentz 2020 however these models did not consider the joint effect of flow variability and diffusion in porous media the interplay between these processes controls mass fluxes towards the fluid solid interface and therefore the amount of reactant available for reaction as shown here the interplay between medium geometry and transport limitations can lead to effective reaction kinetics that differ from their well mixed counterparts even for this simple chemical setup it should be noted that we disregard for the present more complex effects which may play an important role in reactive transport dynamics such as the coupling of transport and medium evolution due to reaction induced precipitation and dissolution golfier et al 2002 edery et al 2011 garing et al 2015 menke et al 2015 liyanage et al 2019 our simplified setup allows for in depth understanding and quantification of the specific role of transport limitations and medium geometry regarding global reaction dynamics and provides a rigorous upscaling approach to be later extended to more complex reaction chemistry the paper is structured as follows we first formalize fluid solid reaction dynamics under diffusive transport near an interface in section 2 this is followed by a brief review of the fundamental concepts behind inter reaction times and the chctrw formulation in section 3 in section 4 we develop the relationship between return times to the interface and inter reaction times and use this formulation to quantify the time evolution of total fluid reactant mass next in section 5 we illustrate these results by obtaining an analytical formulation of the mass dynamics for advection diffusion under stratified flow in a two dimensional channel section 6 shows how the framework thus developed may be applied to compute the time evolution of total mass from numerical determination of first passage and return times in more general settings in particular we consider advection diffusion under stratified flow in a three dimensional channel and stokes flow in an idealized porous medium specifically a body centered cubic beadpack an overall discussion and conclusions are presented in section 7 and some additional technical details and derivations may be found in appendix 2 fluid solid reaction model we consider a mobile reactant species a f transported by the fluid phase and an immobile solid phase reactant species b s distributed over the fluid solid interface of the medium in order to focus on the effects of transport limitations we assume for simplicity that the distribution of the latter over the interface is homogeneous and that its concentration at a given spatial location does not change appreciably due to reaction assuming further that the reaction is irreversible at the timescale of interest and ignoring the reaction products this corresponds locally to the reaction a f b s b s we thus consider a far from equilibrium situation where the reverse reaction can be neglected mass conservation requires this reaction to give rise to additional products which are ignored here we consider also that the available reactant b s is homogeneous across the solid phase while this assumption should not be expected to hold over large scales it is directly relevant for relatively chemically homogeneous column experiments or over certain regions of larger media the assumption that b s is not consumed holds directly for truly catalytic reactions but along with homogeneity it is also a relevant approximation if b s is locally not consumed appreciably for example under large flow rates and short injections where the fluid phase may be significantly consumed throughout the column but consumption of the solid phase at a particular location is small as mentioned in the introduction the assumption of no consumption of the solid phase also implies that we also disregard more complex effects such as coupling of transport and medium evolution which can occur due to precipitation and or dissolution the resolved simulation method developed in this work can in principle handle more complex chemical setups including multiple reactions and or multicomponent reactions however the theoretical developments become substantially more complex because it is necessary to account for the simultaneous presence and amount of different reactants near the interface the simple chemical reaction a f b s b s along with the assumption of chemical homogeneity of the solid phase reactant allows us to focus on the impact of transport mechanisms and medium geometry on reaction dynamics despite the fact that the chemical kinetics are linear at the fluid solid interface reaction is limited by the available fluid reactant flux toward the latter and transport limitations can lead to modified effective reaction kinetics and significant reaction slowdown while it is important to note that the theory developed here cannot at present be directly applied to multicomponent chemical reactions it provides the first direct link between first passage and return time statistics inter reaction times and fluid solid reaction dynamics and sets the stage for later generalizations in order for reaction to occur physical reactant molecules of the transported phase a f must be in reactive contact with the solid phase component b s this occurs within some distance ℓ s of the fluid solid interface we assume that ℓ s is much smaller than the scale at which transport of reactant a f within the fluid phase may be described through continuous advection diffusion and that reaction then occurs when reactant a f is transported sufficiently close to the interface as we will see this corresponds to the usual concept of surface as opposed to bulk reactions it should be noted that if this spatial scale separation between transport and reaction does not hold a more detailed reaction model is necessary for example if attachment or transport times within a physical reactive layer play a significant role sorption or microporosity models may be needed which we do not consider here from both a theoretical and a numerical perspective it is convenient to adopt a conceptualization of transport in terms of lagrangian tracer particles each lagrangian particle represents a macroscopic number of physical reactant particles undergoing advection diffusion noetinger et al 2016 sund et al 2019 and subject to reaction near the interface disregarding reaction for the moment particle positions x t as a function of time t are described by the langevin equation see e g kampen 1992 1 d x t v x t d t 2 d d t ξ t where d is the diffusion coefficient v is the velocity field as a function of position and for each time t ξ t are independent random vectors whose components are independent unit gaussian random variables under the assumed scale separation between reaction and transport reaction dynamics are controlled by the concentration of fluid reactant a f near the interface subject to both advective diffusive transport and reaction when a discretization is considered for a given time step δ t the support scale ℓ d over which concentrations are well defined must be large compared to the spatial resolution of the model description which is of order 2 d δ t see appendix a for further details it is important to note that ℓ d is not a parameter of the theoretical model but rather a property of the discretization in this sense the dynamics will be shown to converge to a well defined limit when ℓ d 0 which corresponds to taking the continuum limit δ t 0 for a given discretization below the support scale ℓ d the mass represented by a lagrangian particle is taken to be well mixed particles within a region comprising a distance up to ℓ d from the interface are subject to reaction as we will formalize below in what follows we will refer to this region as the reactive region for convenience although it should be remembered that it is associated with a given discretization and different from the physical reactive region associated with the subscale length ℓ s in section 4 we will employ the chctrw inter reaction times to show that the continuum limit ℓ d 0 or equivalently δ t 0 of this conceptualization is well defined and leads to a consistent description of the full reactive transport problem this approach is conceptually convenient for the derivations that follow from a computational standpoint it converges to the correct results as will be discussed in detail below nonetheless we expect that it will prove useful in the future to explore equivalent but computationally more efficient approaches for direct numerical simulations such as kernel smoothing sole mari et al 2017 2019 to determine concentrations near the interface we note that any method for estimating local mass fluxes to the interface from lagrangian particle collisions must address the same conceptual issues since as discussed in detail in what follows the number of collisions with the interface within a given time period is also discretization dependent according to the previous considerations while a lagrangian particle is in the reactive region a fraction ℓ s ℓ d of its mass m p is physically available for reaction assuming the law of mass action holds locally we have 2 d m p d t k c s ℓ s m p ℓ d where k is the usual well mixed reaction rate in units of inverse concentration per time and c s is the solid phase concentration within the reactive region thus 3 d m p d t k d m p k d k c a ℓ d where k d is the effective particle reaction rate in units of inverse time and c a c s ℓ s is the solid phase surface concentration i e mass of solid reactant per unit interface area note that under the scale separation assumption ℓ s is small compared to the scale at which transport can be described by continuous advection diffusion this means that formally the continuum description then corresponds to the scaling limit ℓ d 0 with the physical concentration c a remaining finite whereas ℓ s ℓ d remains small due to the scale separation this scale separation corresponds to the situation where the fluid solid mass action reaction can be treated as a surface reaction the reaction rate k d describing particle mass decay per unit time in the reactive region is independent of ℓ s and k c a is the usual surface reaction rate units of length per time which depends only on the surface concentration c a as will be shown in section 4 this leads to a well defined continuum limit for the evolution of total fluid reactant mass where the results are independent of both the discretization length ℓ d and the subscale length ℓ s similarly we assume here that molecular scale attachment detachment at the interface is fast compared to diffusion near the interface so that it can be considered instantaneous specifically this corresponds to assuming that τ a τ d where τ a is the average duration of an attachment event and τ d ℓ d 2 2 d is the diffusion time associated with the discretization lengthscale remains small we note that the concept of scale separation where a continuum limit is taken while maintaining a subscale length or timescale small is commonly employed in volume averaging see e g whitaker 2013 numerically we implement these dynamics using particle tracking random walk ptrw simulations which discretize the langevin equation 1 see e g noetinger et al 2016 sund et al 2019 and appendix a for further details if a fluid reactant particle is in the reactive region during a time step of duration δ t its mass evolves according to 4 m p t δ t m p t e k d δ t otherwise if the particle is farther from the interface no reaction occurs 3 inter reaction times and the chemical continuous time random walk in this section we provide a brief description of the chctrw framework and the associated concept of inter reaction times which will be used in what follows to obtain a quantitative description of total fluid reactant mass in this framework the inter reaction time is the sum of the delay time due to transport limitations and the intrinsic reaction time necessary for the reaction to occur under well mixed conditions for the present application these correspond to the times between visits of the particle to the reactive region and the time spent in the reactive region these concepts are illustrated in fig 1 a in the sections that follow we will develop a theory of fluid solid reaction under advective diffusive transport and apply it to analyze different two and three dimensional example media fig 1b the chemical ctrw framework treats the inter reaction time τ i e the time between successive reaction events as a stochastic quantity incorporating variability from the chemical kinetics on the one hand and transport and medium structure on the other in what follows we will obtain the pdf ϕ of inter reaction times for our fluid solid reaction disregarding for the moment the first excursion to the interface purple excursion in fig 1 the total inter reaction time in the chctrw formulation can be written as aquino and dentz 2017 5 τ τ r τ g τ r where τ r is the time it would take for a reaction to occur if a particle were confined to the reactive region and τ g is the additional time spent in excursions away from the latter blue excursions in fig 1 the total time τ r that must be spent in the reactive region before the next reaction event impacts the delay time τ g because a longer τ r typically requires more visits to the reactive region corresponding to orange excursions in fig 1 punctuated by excursions when small numbers of reactant particles molecules are considered the inter reaction time refers to the time between two successive reaction events between reactant molecules allowing for capturing fluctuations due to finite particle numbers gillespie 1977 recall that here we consider lagrangian particles representing a certain amount of fluid reactant mass undergoing continuous advection diffusion and corresponding to a macroscopic number of molecules in that case fluctuations due to molecule numbers are not significant gillespie 1977 aquino and dentz 2017 for conceptual and computational reasons it is then more convenient to consider a fixed number of lagrangian particles whose masses evolve in time bolster et al 2016 in this case the rate equation 3 for the evolution of the mass carried by a lagrangian particle within the reactive region corresponds to a constant reaction rate k d k c a ℓ d with units of inverse time according to the classical well mixed theory for stochastic inter reaction times this reaction rate may be interpreted as a constant probability of reaction per unit time which translates into an exponential inter reaction time distribution gillespie 1977 specifically the probability that τ r takes a value in u u d u is given by ϕ r u d u such that its probability density function pdf is given by 6 ϕ r u k d e k d u for a given τ r u τ g u is also a random variable whose pdf ϕ g u reflects the underlying medium heterogeneity and the stochasticity inherent in diffusive motion in the next section we will explicitly relate this pdf to the statistics of excursions away from the interface as we will see although τ r and τ g are discretization dependent τ is well defined in the continuum limit the impact of delay due to return excursions on reaction dynamics is captured by a memory function given in terms of the pdf ϕ of inter reaction times τ as aquino and dentz 2017 7 k ϕ λ λ ϕ λ 1 ϕ λ here and throughout we denote laplace transforms with respect to time by a tilde and the corresponding laplace variable by λ for the a f b s b s reaction considered here the total mass obeys 8 d m ϕ d t k ϕ m ϕ where denotes the convolution product k ϕ m ϕ t 0 d t k ϕ t m ϕ t t and the subscript ϕ in the total mass indicates that the first delay time to reach the interface see fig 1 has not yet been considered so that these dynamics incorporate the impact of the inter reaction time pdf ϕ only this equation may be seen as a generalized rate law governing the time evolution of the total mass under the impact of reaction delay caused by transport limitations in contrast to the classical well mixed rate laws where reaction rates depend only on the current mass in a batch reactor the presence of the convolution with a memory function renders this equation integro differential physically this arises because the reaction rate at a given time depends on past history through the statistics of past excursions excursions away from and back to the reactive interface controlled by transport and medium geometry take the form of reaction delays which lead to memory effects in the large scale mass dynamics broad distributions of excursion times translate into long range memory effects applying these results to fluid solid reactions requires relating the inter reaction times to the statistics of excursions away from and back to the solid interface as well as including the role of the first excursion to the interface fig 1 representing the impact of the initial condition this is the subject of the next section 4 mass dynamics and inter reaction times in this section we first quantify the impact of diffusion near the reactive interface on the distribution of return times to the reactive region and the duration of each visit based on these concepts and the first passage time to the interface from the initial condition we then obtain the inter reaction times and the evolution of total fluid reactant mass based on the chctrw formulation of the previous section 4 1 return and visit times the dominant transport mechanism that controls local reactant mass flux towards the interface is diffusive because there is no fluid flow into or out of the solid phase we assume the interface to be locally flat at the scale of the transport model so that close to the interface it is sufficient to consider one dimensional diffusion in the transverse direction we note that this local flatness assumption may be inappropriate in some systems such as rough fractures where the surface may exhibit fractal i e self similar across scales properties see e g develi and babadagli 1998 such pronounced surface roughness is known to impact reaction rates deng et al 2018 a one dimensional conceptualization of diffusion perpendicular to the surface may then be inaccurate and more detailed modeling of transport and reaction near the interface may be necessary this scenario although important is beyond the scope of the present work and we do not explore it further here for a diffusing particle characterized by the diffusion coefficient d in an unbounded domain in one dimension the pdf of the time to first reach a target located at a distance ℓ to one side of the initial position is given by the lévy 1 2 stable density see e g feller 2008 9 ψ d t ℓ ℓ 4 π d t 3 e ℓ 2 4 d t that is ψ d t ℓ d t is the probability that the first passage time to the target is in the interval t t d t we have 10 ψ d λ ℓ e ℓ 2 λ d in order to apply the chctrw formulation we require the time to return to a target which sets the time between successive visits to the reactive interface and therefore controls the inter reaction times see fig 1 this concept must be treated with care because a particle undergoing continuous diffusive motion in one dimension crosses its original position infinitely many times in any given finite time interval this is reflected in the fact that the limit ℓ 0 of ψ d t ℓ is not well define see also karatzas and shreve 1988 grebenkov 2007 aquino et al 2017 grebenkov 2019 to avoid this problem we will obtain the inter reaction times in the scaling limit of an appropriate discretization associated with the support scale ℓ d see section 2 consider a regular one dimensional discretization into intervals or cells of equal length ℓ d the return times to the interface associated with the discretization are then the first passage times to the center of the cell touching the interface from the center of the adjacent cell the cell centers are a distance ℓ d apart so that the corresponding first passage time pdf is given by ψ d ψ d ℓ d note that the same result is obtained by considering the first passage times to a distance ℓ d from the interface starting from a distance 2 ℓ d this is convenient for numerical determination from particle tracking simulations where particles can be placed at distance 2 ℓ d from the interface and the first passage time determined as the time when distance ℓ d from the interface is crossed before proceeding we may relax the assumption of an unbounded domain and the requirement of one dimensional diffusive transport far from the reactive region first we denote the timescale associated with the discretization support scale ℓ d by 11 τ d ℓ d 2 2 d in order to allow for different effects away from the interface while retaining the diffusive behavior near it we write the return time pdf in the form 12 ψ d λ ψ d λ ℓ d e f λ 2 τ d λ see eq 10 the discretized description can only resolve times t τ d corresponding to λ 1 τ d thus with a view to taking the continuum limit it is sufficient to consider the limit of λ 1 τ d corresponding for a given discretization to times large compared to the discretization time we then find 13 ψ d λ 1 f λ 2 τ d λ it is important to note that this and similar results below do not represent late time expansions but rather lead to results valid for all times or all λ in the continuum limit ℓ d 0 in other words finite ℓ d effects are a product of the discretization which disappear in the continuum limit description of total mass obtained in what follows the form factor f λ which depends on the geometry of the domain and the transport mechanisms involved up to the timescale 1 λ must approach unity for large λ so that the behavior of the return time is dominated by the 2 τ d λ term characterizing the diffusive behavior near the fluid solid interface at short times for small λ corresponding to large times f λ encodes information about transport excursions far from the reactive region as before in an arbitrarily small time interval continuous diffusion in one dimension crosses the origin infinitely many times the factor 2 τ d λ which approaches zero as ℓ d 0 captures the resulting singular character of the return time distribution in this limit whereas the factor f λ whose departure from unity represents additional effects from transport excursions unrelated to the discretization remains finite and nonzero in the continuum limit let ℓ c be a characteristic lengthscale of the medium such as the average pore size note that the choice of ℓ c is arbitrary and simply provides a reference scale based on which nondimensional quantities characterizing the relationship between reaction diffusion and advection processes will be introduced below we denote the corresponding diffusion time as 14 τ d ℓ c 2 2 d and define the damköhler number 15 da k c a τ d ℓ c the latter quantifies the relative importance of reaction and diffusive transport at this scale it is convenient to define the rescaled return time tail probability 16 g t ℓ c ψ d t 2 τ d ℓ d where 17 ψ d t t d t ψ d t is the probability that the return times are greater than a given time t in terms of the rescaled tail probability the return times obey 18 ψ d λ e 2 τ d λ g λ ℓ d ℓ c so that comparing to eq 12 we have the relation g λ f λ 2 τ d λ since f λ remains finite and nonzero in the limit ℓ d 0 so does g λ note that for large λ we have g λ 1 2 τ d λ because f λ 1 as discussed above for small λ corresponding to large times g λ again encodes the behavior of excursions away from the interface these quantities are obtained analytically for the example of diffusion in a bounded one dimensional domain in section 5 note that the statistics of the excursions back to an arbitrary point along the interface may differ depending on the starting point corresponding to different form factors and rescaled tail probabilities depending on the latter here in order to obtain a stationary description where successive return times to the wall have the same statistics we treat any such variability statistically that is the same rescaled tail probability is used to characterize the return time statistics associated with each point on the interface and at all times in other words this method disregards possible non stationarity of the return times as well as possible correlations between subsequent return times arising from different transport and geometry properties at different interface points nonetheless we believe such a statistical description to be appropriate as long as the flow is statistically stationary and the structural characteristics of the medium are statistically homogeneous within the region being considered note that this assumption is similar to that employed in the standard ctrw for transport berkowitz et al 2006 where variability in particle jump sizes and or transit times is treated stochastically but assumed statistically homogeneous and stationary that is the pdfs associated with these quantities do not depend on the current time or particle position in section 6 we show that this approach leads to accurate predictions of mass dynamics in a regular beadpack despite different points on the bead surfaces having different characteristics in terms of the distance to other nearby surface points next we turn to the statistics of the time spent in the reactive region in each visit as shown in appendix b the corresponding pdf is given by 19 ψ v t e t 2 τ d 2 τ d so that the single visit times are exponentially distributed with mean 2 τ d 4 2 inter reaction times we are now in a position to compute the pdf of global delay times τ g which will allow us to obtain the pdf ϕ of the inter reaction time τ see eq 5 given a well mixed reactive time τ r u the delay time due to excursions away from the reactive region is given by 20 τ g u i 1 n u u w i where the w i are independent and identically distributed return times with pdf ψ d and n u u is the random number of visits to the reactive region given well mixed reactive time u for sufficiently fine discretization the residence times per visit to the reactive region are approximately exponential with mean 2 τ d eq 19 this implies that n u u is approximately poisson with mean u 2 τ d see e g kampen 1992 thus the global delay 20 is a compound poisson process so that its pdf has laplace transform benson et al 2007 benson and meerschaert 2009 meerschaert and sikorskii 2012 21 ψ g λ u exp λ ψ d λ u 2 τ d the pdf ϕ of the total inter reaction time τ resulting from a well mixed reaction time τ r with pdf given by eq 6 and a compound poisson delay has been obtained in aquino and dentz 2017 with the form 21 of the delay pdf it reads 22 ϕ λ ϕ r λ 1 ψ d λ 2 τ d in the continuum limit ℓ d 0 using eqs 6 and 12 we obtain 23 ϕ λ da da τ d λ g λ in this form the inter reaction times are manifestly well defined in the continuum limit the effect of diffusion near the interface is implicit in the form of eq 23 and additional effects arising from domain geometry and or velocity variability are encoded in the rescaled tail probability g see eq 16 4 3 dynamics of reactant mass the chctrw formulation can now be used to obtain the evolution of total mass given the inter reaction time pdf together with the additional delay time to first reach the interface see fig 1 this delay is distributed according to a pdf ψ 0 which depends on the transport mechanism and the initial reactant distribution defining 24 ψ 0 t t d t ψ 0 t the tail probability of the first passage time to the interface we obtain as shown in appendix c 25 d m d t k ϕ m m 0 ψ 0 this generalized integro differential rate law encodes the impact of transport and heterogeneity on the effective reaction kinetics through the inter reaction times and the first passage time to the interface in the standard ctrw description of transport advection dispersion at the small scales in the presence of heterogeneity leads to the emergence of memory kernels at larger scales in the advection dispersion equation accounting for statistical variability in particle jump sizes and or transit times berkowitz et al 2006 analogously the chctrw leads to a description of total mass which incorporates the impact of transport limitations through a memory kernel representing statistical variability in first passage and inter reaction times taking the laplace transform of eq 25 we obtain the laplace space solution for the evolution of total mass as 26 m λ m 0 da ψ 0 λ τ d g λ da τ d λ g λ while the number of visits to the reactive region within a given time window increases when the discretization is refined the residence time associated with each visit decreases accordingly and the total mass change due to reaction in the actual subscale reactive region is well defined these results connect the effect of transport limitations on mass dynamics to the inter reaction times of the chctrw under a transport mechanism for which mass exchange near a locally flat interface is controlled by diffusion in particular this is the case for advection diffusion since there is no flux into or out of the solid interface diffusion always dominates returns at sufficiently short times when particles are close to the interface this fact is incorporated into the form of eq 26 the rescaled tail probability g λ captures further effects controlling the statistical variability of the excursion times such as transverse velocity variations across the domain and the size of the latter as will be discussed in more detail and illustrated in the following sections note that this description does not require zero or low velocity regions to occur only near the interface the presence of stagnation regions affects g λ and may lead to broader variability of the return times and therefore of the inter reaction times we note that eq 26 can also be obtained in terms of the statistics of times spent near the interface rather than the statistics of the inter reaction times this alternative formulation is discussed in appendix d 5 reactive transport in a 2d channel in order to illustrate the results of the previous section we consider first the simple example of transport in an infinite two dimensional channel with stratified flow and reactive walls fig 1b i for which fully analytical results can be obtained in laplace space we take the characteristic length ℓ c as the channel half width 5 1 analytical first passage return times and mass dynamics the distribution of times to reach either wall from a given point along the channel cross section depends in this case only on transverse diffusion and not on the velocity profile it is thus sufficient to consider one dimensional diffusive transport along the channel cross section the corresponding pdf in a bounded domain of size l starting from a distance ℓ from one wall has laplace transform aquino and dentz 2018 27 ψ λ ℓ l csch ℓ 2 λ d csch l ℓ 2 λ d coth ℓ 2 λ d coth l ℓ 2 λ d in order to determine the first return time pdf in the sense discussed in the previous section we take ℓ ℓ d and l 2 ℓ c in preparation for taking the continuum limit ℓ d 0 we expand for small ℓ d ℓ c to obtain 28 ψ d λ exp tanh 2 τ d λ 2 τ d λ 1 tanh 2 τ d λ 2 τ d λ which according to eqs 12 and 18 corresponds to the form factor and rescaled tail probability 29 f λ tanh 2 τ d λ g λ tanh 2 τ d λ 2 τ d λ substituting g λ in eq 26 the total mass as a function of time has laplace transform 30 m λ m 0 2 da ψ 0 λ tanh 2 τ d λ 2 τ d λ 2 da tanh 2 τ d λ 2 τ d λ for small da we can expand this result for small λ 1 τ d time t 1 τ d before appreciable reaction occurs and we obtain 31 m λ m 0 da τ d λ m t m 0 e da t τ d hence the reaction kinetics correspond to a fully mixed cross section this arises because for slow reaction compared to the diffusive time scale diffusion homogenizes the fluid reactant to a mass per cross section length 1 2 ℓ c before appreciable reaction occurs since there are two reaction interfaces the fraction of mass in the reactive region is 2 ℓ d 2 ℓ c ℓ d ℓ c the effective reaction rate is thus k d ℓ d ℓ c da τ d k c a ℓ c independent of the diffusion coefficient for large da and an initial condition not fully concentrated at the channel walls so that ψ 0 λ 0 the initial condition dominates and we have 32 m t m 0 ψ 0 t this result can be interpreted as follows in the limit of fast reaction the surviving mass is that which has never reached the channel walls the fraction of the initial mass that has not reached the walls is given by the probability ψ 0 t that the first passage time to the walls from the initial position is greater than t in the particular case where all the mass starts at the channel walls ψ 0 λ 0 the dynamics are fully controlled by the return times for large da and λ 1 τ d early times we obtain 33 m λ m 0 da τ d 2 λ m t m 0 da τ d 2 π t for λ 1 τ d late times we find 34 m λ m 0 da τ d 1 2 τ d λ 3 m t 3 m 0 2 da e 3 t 2 τ d this means that diffusive excursions far from the walls control reaction until the channel cross section is homogenized by diffusion after which we recover exponential behavior but diffusion rather than reaction limited the exponent of the exponential decay in 34 depends only on the diffusion coefficient while that of 31 depends only on the reaction rate while the form of the mass dynamics eq 30 is flow profile independent the actual mass evolution can depend on the profile through the initial condition such as for a flux weighted condition in the low da limit discussed above the initial delay does not play a relevant role and this dependence disappears for definiteness consider two dimensional poiseuille flow with the velocity profile 35 v y v m 1 y 2 ℓ c 2 for y ℓ c ℓ c along the one dimensional cross section with v m the maximum velocity attained at the channel center y 0 the corresponding eulerian mean velocity is given by v 2 v m 3 different initial conditions affect the initial first passage time until the interface is first reached we consider four examples of instantaneous injection with the fluid reactant mass placed a at the middle of the channel b homogeneously along the channel cross section c flux weighted along the channel width and d at the channel walls the laplace transforms of the corresponding first passage time pdfs ψ 0 are given in appendix e 5 2 numerical simulations of reactive transport we performed ptrw simulations of the two dimensional reactive transport problem as described in section 2 and appendix a we nondimensionalize distances by ℓ c time by the diffusion time τ d and mass by the initial mass m 0 note that numerically this can be conveniently achieved by setting the diffusion coefficient to d 1 2 the channel half width to ℓ c 1 and the initial mass to m 0 1 in arbitrary units each of n particles initially carries a fraction m 0 n of the initial mass in order to obtain good statistics the number n of particles used must be such as to permit resolving the transverse direction to within the discretization length meaning n ℓ c ℓ d τ d τ d aside from setting the spatial resolution through ℓ d the time step must be sufficiently small to resolve reaction since the average residence time per visit to the reactive region is 2 τ d this requires 2 k d τ d 1 note that even though the initial condition c depends on the velocity profile the first passage time e 3 is independent of the mean velocity so that we can arbitrarily set v 1 the evolution of total mass under the different initial conditions a d is shown in fig 2 good agreement is observed between simulations and numerical inversion of eq 30 note how the da 10 2 case is identical for all initial conditions it corresponds to the low da limit eq 31 characterized by purely exponential decay and fully controlled by the reaction rate i e independent of the diffusion coefficient in a c the da 10 2 curves have converged to the da independent fast reaction limit eq 32 this regime is the most affected by the initial condition because it is controlled by the initial first passage time to the interface however the characteristic first passage times associated with conditions a c are all on the order of the diffusion time τ d and thus lead to qualitatively similar mass decay for d the evolution of the total mass is fully controlled by the return times to the reactive region eqs 33 and 34 the late time behavior in this case is also exponential but fully controlled by the diffusion coefficient i e independent of the reaction rate constant k and the solid phase surface concentration c a note how for moderate to high damköhler number da 1 the solution 31 corresponding to a well mixed cross section tends to overestimate reaction this is the case even for the uniform initial condition a because fast consumption of solute at the interface inhibits transverse homogeneity at early times in d reaction is faster than the well mixed channel prediction in these regimes due to solute starting in the reactive region but slower at later times as the surviving solute explores the channel cross section 5 3 reaction efficiency these results show that while reaction rates tend to increase with the damköhler number as would be expected the reaction efficiency compared to a well mixed channel cross section characterized by the same da decreases due to transport limitations in order to better understand this phenomenon consider the global or effective reaction rate 36 k e t d ln m t m 0 d t in other words the effective rate k e t is defined so that d m t d t k e t m t note that for pure exponential decay at constant rate m t m 0 exp k e t this definition recovers k e t k e as expected we define the reaction efficiency by comparing k e t to the reaction rate k e wm associated with well mixed conditions in the domain 37 ε t k e t k e wm according to eq 31 the well mixed reaction rate for the two dimensional channel is k e wm da τ d it is important to keep in mind the different physical meanings of the effective well mixed rate k e wm and the thermodynamic well mixed rate k in eq 2 the latter defined in units of inverse concentration per time is the usual batch reactor rate which governs fully well mixed reaction the former defined in units of inverse time represents the effective reaction rate occurring when the fluid phase domain is well mixed it is impacted by medium geometry as it accounts for the fact that part of the fluid reactant remains far from the interface we now obtain a theoretical expression for the asymptotic rate k e lim t k e t and the corresponding asymptotic reaction efficiency ε k e k e wm to this end we introduce the mean first passage time to the wall w 0 and the corresponding second moment 38 w 0 0 d t t ψ 0 t s 0 0 d t t 2 ψ 0 t we define also 39 α 0 s 0 2 τ d w 0 which represents a dimensionless measure of the variability in the first passage times compared to the diffusion time τ d expanding eq 30 for small λ 1 τ d and inverting the laplace transform we find the late time behavior 40 m t m 0 w 0 τ d da k e e k e t k e 2 da da w 0 τ d da 2 s 0 da τ d 2 w 0 4 τ d 3 2 τ d 2 so that 41 ε 1 τ d w 0 da 1 2 τ d 3 w 0 α 0 da τ d w 0 da 1 thus for small da we recover the well mixed rate as predicted by eq 31 and ε 1 for high da and an initial condition not concentrated at the channel walls the asymptotic reaction rate is limited by the first passage times to the interface and k e 1 α 0 τ d for a given initial condition this asymptotic rate cannot exceed a constant value since the well mixed rate is linear in da the asymptotic efficiency ε 1 α 0 da is inversely proportional to da in the special case of the initial condition at the channel walls we recover eq 34 and ε 3 2 da in this case reaction is limited by the return times to the wall and the asymptotic efficiency remains inversely proportional to da these results are illustrated for the case of a mid channel injection initial condition a in fig 3 from eq e 1 we find in this case w 0 τ d s 0 5 τ d 2 3 and α 0 5 6 as predicted the reaction rate becomes asymptotically constant for all damköhler corresponding to exponential decay however the reaction rate is initially variable as the fluid reactant explores the channel cross section for this choice of initial condition the reaction efficiency is initially zero because the solute is far from the interface and subsequently increases to the asymptotic value 6 generalization to other media in this section we will discuss the generalization of our approach to more complex scenarios where fully analytical results for the inter reaction times are not available as long as diffusion is the dominant transport mechanism near the reactive interface and the latter can be assumed flat at the scale of the transport model eq 26 may be used to predict the time evolution of total mass for arbitrary damköhler number given knowledge of the tails ψ 0 and ψ d of the first passage and return times in general analytical expressions for these quantities are not available and they must therefore be determined numerically however some general results about the mean return time if it exists may be obtained and employed to determine asymptotic reaction rates note that the existence of a mean inter reaction time is directly related to the existence of a mean return time to the interface if the excursions away from and back to the interface see fig 1 have sufficiently heavy tailed duration statistics the chctrw framework predicts a similar effect on reaction dynamics aquino and dentz 2017 such situations which we do not consider further here but can be captured in our framework can be important when modeling scenarios where solute may be retained in large regions of low velocity away from the reactive interface or where the distribution of distances between separate reactive regions is very broad and not well characterized by a mean value aquino and dentz 2020 6 1 mean return times and effective reaction rate the asymptotic reaction rate for stratified flow in a 2d channel was obtained in section 5 eq 41 in that case the limit of low da corresponding to slow reaction leads to an effective reaction rate of da τ d eq 31 we begin by generalizing this result by considering the mean return time to the interface 42 w d 0 d t t ψ d t we have for the return time pdf ψ d λ 1 w d λ for λ 1 w d so that the rescaled tail probability eq 16 obeys 43 g 0 0 d t g t w d ℓ c 2 τ d ℓ d taylor expanding eq 26 for small λ and inverting the laplace transform 44 m t m 0 e k e t with the effective reaction rate 45 k e 2 da ℓ d w d ℓ c in order to relate the mean return time w d to medium structure we now determine the effective rate in a well mixed domain consider that the solute is well mixed over a region of volume v within which the volume of the reactive region is v d the reactive region comprises a layer of height ℓ d immediately adjacent to the interface so that v d a i ℓ d where a i is the interface area the mass in the reactive region is then m t v d v and reacts at rate k d see eq 3 so that 46 d m t d t k e wm m t with the well mixed effective reaction rate given by 47 k e wm v d v k d ρ da τ d where we have introduced the dimensionless interface extent coefficient 48 ρ a i ℓ c v encoding the amount of interface area per unit volume note that if a representative elementary volume for interface area exists and the solute is well mixed over a larger volume ρ is given by this ratio over the representative volume note also that for an empty channel of cross section a we have 49 ρ ℓ i ℓ c a where ℓ i is the total interface length intersecting the cross section if the medium is statistically homogeneous along the flow direction this formula may also be used to compute ρ given information about the interface extent over a cross section taking a as the fluid phase area over the latter if the concentration becomes well mixed due to transport before appreciable reaction occurs the effective reaction rates associated with the return time and well mixed domain pictures must coincide k e k e wm using eqs 45 and 47 we find the mean return time and associated effective reaction rate as 50 w d 2 τ d ℓ d ρ ℓ c k e ρ da τ d note that these results agree with the example considered in section 5 in that case ℓ i 2 a 2 ℓ c ρ 1 and k e da τ d note also that w d is independent of the damköhler number so that according to eq 43 51 g 0 ρ 1 independent of da so long as the transport mechanism leads to a well mixed state in the absence of reaction we note that as before for large da fast reaction and an initial condition not concentrated along the interface reaction is controlled by the first passage times to the interface and m t m 0 ψ 0 t in that case the reaction dynamics are thus controlled by the initial reactant distribution 6 2 reaction efficiency we can now generalize the results for the asymptotic reaction efficiency using eqs 16 and 51 we find the small λ 1 τ d expansion for the rescaled tail probability 52 g λ ρ 1 1 α λ 2 where α is defined by 53 α s d 2 τ d w d s d 0 d t t 2 ψ d t quantifying the variability in the return times to the interface relative to the diffusion time using this result to expand eq 26 for λ 1 τ d and inverting the laplace transform we find the late time behavior 54 m t m 0 ρ da w 0 τ d da k e e k e t k e 2 ρ da da w 0 τ d ρ 2 da 2 s 0 ρ da τ d 2 w 0 α 2 τ d 2 we thus conclude that as long as the transport mechanism leads to a well mixed state in the absence of reaction the reaction rate always asymptotes to a constant at late times for arbitrary da using eqs 37 and 47 the corresponding asymptotic reaction efficiency is 55 ε 1 τ d w 0 ρ da 1 α α 0 ρ da τ d w 0 ρ da 1 see also eqs 38 and 39 for the 2d channel case ρ 1 and α 2 3 from eq 28 so that this result reduces to eq 41 in the low da limit of slow reaction we recover eq 47 for the well mixed reaction rate and obtain ε 1 as expected for high da and an initial condition not concentrated at the channel walls we have the initial condition limited results k e 1 α 0 τ d and ε 1 α 0 ρ da in the special case of an initial condition distributed over the interface for which w 0 s 0 0 we find k e 1 α τ d emphasizing that in this case the transport limitations come into play through the return times rather than the first passage times as before ε 1 α ρ da remains inversely proportional to da 6 3 mass dynamics from numerical first passage and return statistics according to eq 16 g λ is proportional to ψ d λ thus mass dynamics can be quantified through eq 26 by numerically sampling first passage and first return times in ptrw simulations of the transport for a given initial condition the laplace transform ψ 0 λ can be found directly from the fraction of first passage time samples to the interface above a given time to determine the first return times associated with discretization length ℓ d for a given medium and flow we take a single particle initial condition chosen uniformly randomly over the surface at perpendicular distance 2 ℓ d from the interface corresponding to a distance ℓ d from the reactive region once the reactive region is reached we record the duration of the excursion place the particle at the closest point at distance 2 ℓ d from the interface and repeat the process for the next excursion a prescribed number of times note that in the cases considered here the system is asymptotically well mixed over a representative region so that different points along the interface are revisited with the same probability this means that equivalent results would be obtained by considering the first passage time to within ℓ d of the interface for a set of particles initially distributed uniformly over the latter a comparison of the rescaled tail probabilities g t obtained in this manner for one dimensional diffusion in a bounded domain of half width ℓ c along with the rescaled first return time pdf ψ d t ℓ c ℓ d with those obtained from numerical inversion of the analytical laplace transform of ψ d λ eq 27 with ℓ ℓ d and l 2 ℓ c is shown in fig 4 for two different values of ℓ d we nondimensionalize distances by ℓ c and times by τ d as before for t δ t the return time statistics converge to the same discretization independent behavior above the discretization timescale τ d the late time scalings follow those of pure diffusion in an semi infinite domain eq 9 until the effect of the far boundary is felt leading to a cutoff on a characteristic timescale of order τ d corresponding to exploring the full domain 6 3 1 reactive transport in a 3d channel as a first example to verify the results for the mass dynamics using numerical estimation of the first passage and return times consider transport in a three dimensional cylindrical channel with the characteristic length ℓ c given by the channel radius fig 1b ii we take a point injection at the center of the channel as the initial condition as before it suffices to consider diffusion along a cross section in order to determine the evolution of total mass independent of the flow field as long as it is assumed to be stratified we again nondimensionalize distances by ℓ c time by τ d and initial mass by m 0 the results obtained from numerical inversion of eq 26 given numerical determination of the first passage and return times are in good agreement with reactive ptrw simulations as shown in fig 5 a in this case the cross section area is given by a π ℓ c 2 and the interface length intersecting a cross section by ℓ i 2 π ℓ c so that the interface extent coefficient is ρ 2 eq 49 for da 10 2 we find good agreement between the numerical simulations and exponential decay according to the effective well mixed reaction rate eq 47 with ρ 2 whereas for higher da deviations from the well mixed behavior are observed because transverse diffusion is incapable of homogenizing the cross section under fast reaction for da 10 2 the numerical simulations agree with the fully transport limited solution m t m 0 ψ 0 t the reaction efficiency ε t corresponding to these dynamics is shown in fig 6b the theoretical asymptotic efficiency ε eq 55 evaluated using the first and second moments of the numerically determined first passage and return times is in good agreement with the simulations although some quantitative differences are discernible in the mass decay compare figs 5a and 2 a the reaction dynamics are remarkably similar to the two dimensional channel case especially regarding the reaction efficiency compare figs 5b and 3 a 6 3 2 reactive transport in a body centered cubic beadpack we now investigate the application of our approach to the case of reactive transport in a crystalline porous medium recall that the inter reaction time approach presented here relies on the assumption that a statistical description of the return times with stochastic return times but with the same statistics applying to each return to the interface is sufficient to characterize reaction we assess this hypothesis for advective diffusive transport in a body centered cubic beadpack turuban et al 2019 where the structure of the medium is periodic but not all points on the interface within a unit cell are equivalent in their positioning relative to the rest of the interface fig 1b iii for details on the numerical simulations see appendix a as shown in appendix f the interface extent coefficient for this system is given by 56 ρ 3 π 2 1 3 π 8 1 14 7 the relative importance of advection compared to diffusion can be quantified through the péclet number which we define in terms of the absolute value v of the eulerian mean velocity vector and the characteristic lengthscale ℓ c as 57 pe ℓ c v d the total mass as a function of time for different damköhler numbers and pe 10 3 is shown in fig 6 a for an initial condition uniformly distributed over a conventional unit cell see fig 1b iii the inter reaction time approach parameterized according to the numerically determined first passage and return tails ψ 0 and ψ d as discussed above is in good agreement with the reactive ptrw simulations for low da the mass evolution agrees with the effective well mixed decay eq 46 this happens because for low reaction rates the fluid reactant remains homogeneous over a representative region of pore volume as for the channel examples deviations from the well mixed behavior become more pronounced as the damköhler number increases and transport is unable to efficiently homogenize the fluid phase concentration the corresponding reaction efficiency ε t is shown in fig 5b the theoretical asymptotic efficiency ε eq 55 evaluated using the first and second moments of the numerically determined first passage and return times is again in good agreement with the simulations in this case the homogeneous character of the initial condition results in a reaction efficiency that is initially unity for high damköhler reaction tends to destroy homogeneity which results in a decrease of the reaction efficiency towards the asymptotic value in fig 7 we show the total mass as a function of time for the same damköhler numbers as in fig 6 and different values of péclet number computed using the inter reaction time approach although we refrain from showing these results in fig 6 to avoid clutter we verified that ptrw simulations show similar very good agreement as for fig 6a across péclet numbers for sufficiently high da the reaction is slower than the effective well mixed prediction but approaches the latter when the péclet number becomes sufficiently high as advection induced transverse dispersion effects become important compared to diffusive mixing in order to better understand the role of the péclet number in the reaction dynamics we examine the first passage and return time statistics arising under different transport regimes see fig 8 the flow considered here is known to induce chaotic mixing lester et al 2016 turuban et al 2019 heyman et al 2020 2021 this means that the flow efficiently homogenizes the concentration on the cross section of the pore space transverse to the mean flow direction over a characteristic advective mixing timescale for sufficiently low péclet number as long as the advective mixing timescale is large compared to the typical diffusive first passage and return times transverse mixing is carried out by diffusion and advection does not have an appreciable effect on reaction thus the first passage and return times along with the mass dynamics are similar for pe up to 10 2 when advective effects become important particles far from the interface are brought towards it faster than by diffusion alone on the other hand particles at intermediate distances can take longer to reach the interface than they would have by diffusion lastly very low return times remain controlled by diffusion this effect on the first passage and return times can be clearly seen for pe 10 3 whichever effect is dominant the average return time reflects the fact that at sufficiently late times concentration is well mixed and remains given by eq 50 thus the low da reaction behavior which depends only on the average return time is unaffected by the mixing mechanism and remains unchanged see fig 7 however the distributions of first passage and return times become less broad with increasing péclet fig 8 and the effective well mixed regime is reached faster for this reason the reaction dynamics approach the effective well mixed behavior at higher values of the damköhler number the strength of this effect increases with increasing pe fig 7 we note that for higher péclet numbers pe 10 4 the corresponding strongly advection dominated transport simulations become particularly sensitive to the underlying flow velocities accurate results for such cases would require more finely resolved flow fields and we refrain from simulating them here the first passage and return time dynamics in the two and three dimensional channel and beadpack examples exhibit some qualitative similarities due to the role of diffusion in transverse mixing and its dominant role near the fluid solid interface indeed the corresponding distributions exhibit clear diffusive scalings followed for long times by cutoffs see figs 4 and 8 the characteristic timescale associated with the cutoff and therefore the characteristic variability in inter reaction times depends on medium geometry and in the case of the beadpack is also impacted by advective mixing for large péclet number correspondingly the reactant mass dynamics in these different systems exhibit similarities such as late time exponential decay but also differences in the onset of transport limitations with damköhler number and the effective reaction rates associated with a well mixed system see figs 2 5 6 and 7 in particular the interface extent coefficient ρ quantifying the available surface area per unit volume increases compared to the 2d channel by a factor of 2 in the 3d channel and about 15 in the beadpack effectively increasing the well mixed reaction rate in these systems for a given value of the damköhler number transport limitation effects become important for ρ da 1 although they are mitigated in the beadpack when the péclet number is large and advection induced mixing plays a role 7 discussion and conclusions in this work we have developed a new framework to quantify the effect of transport limitations on fluid solid reaction dynamics in porous media our approach based on the chemical continuous time random walk theory of inter reaction times relates the statistics of solute excursions away from and back to the fluid solid interface to reaction times we have shown that the dynamics of effective reaction rates relate to the statistics of inter reaction times which are in turn controlled by transport and medium geometry we have illustrated the approach analytically for advection diffusion reaction in stratified flow through a two dimensional channel and provided a generic numerical approach to determine the corresponding dynamics in more complex media and flow fields for fast reactions reactant mass is controlled by the first passage time of solute to the reactive solid interface for slow reactions multiple excursions to the interface are necessary before reaction occurs in the latter case the statistics of the durations of these excursions which are sensitive to diffusive mass transfer near the interface become a dominant control on mass dynamics consistently with numerical simulations our theory predicts that for intermediate and high damköhler number these effects can lead to significant reaction slowdown due to transport limitations even for the simple reaction studied here and even in simple geometries we provide analytical expressions for the late time effective reaction rate as a function of damköhler number which exhibits a transition from the well mixed reaction rate at low da to the inverse of the diffusion time at large da a direct consequence of transport limitations the theory presented here also leads to a useful numerical framework fluid solid reactions pose unique computational and theoretical challenges as they require resolving transport dynamics close to the interface in a consistent manner here we have analyzed a simple fluid solid reaction in detail and shown how the assumption of scale separation between reaction and transport dynamics near the interface leads to a consistent continuum model of surface reaction resolved numerical simulations of these dynamics are computationally demanding the inter reaction time approach presented here opens up new possibilities for efficient numerical computation of mass dynamics by extending existing theoretical frameworks for the determination of first passage times condamin et al 2007 bénichou et al 2010a aquino et al 2019 aquino and dentz 2020 and or numerical techniques for efficient first passage time computation atiya and metwally 2005 shalloway and faradjian 2006 schnoerr et al 2017 this work has been mainly concerned with introducing the concepts and methodology underlying a new approach to upscaling fluid solid reaction dynamics as such we have made significant simplifications regarding the reaction chemistry and porous medium structure the framework developed here brings new perspectives to link the statistical characteristics of medium geometry and flow to reaction dynamics in a broad range of porous media which will be the subject of future work furthermore we expect an approach similar to that presented here to be applicable to the dynamics of mass breakthrough as a function of distance in terms of the statistics of inter reaction distances future work will also aim to generalize the approach to higher order reactions involving multiple transported components multiple simultaneous reactions and heterogeneity spatial and temporal variability in the solid phase reactant distribution along the interface credit authorship contribution statement tomás aquino conceptualization methodology software writing original draft tanguy le borgne conceptualization writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments ta is supported by a marie skłodowska curie individual fellowship funded by the european union s horizon 2020 research and innovation programme under the project chemicalwalks 838426 tlb gratefully acknowledges funding by the erc under the project reactivefronts 648377 the authors thank joris heyman for providing the numerical flow data used in the beadpack ptrw simulations and guillem sole mari for useful comments and suggestions a c implementation of the particle tracking algorithms used to generate the numerical data employed in this work are available under an open source license github github com tcaquino beadpack doi 10 5281 zenodo 4392882 related data is also available doi 10 5281 zenodo 5007336 please contact the authors for further information or requests appendix a particle tracking simulations in this appendix we discuss some details of the ptrw particle tracking random walk simulation method used in this work lagrangian particle tracking methods which simulate particle trajectories based on a random walk have been found to mitigate numerical dispersion issues typically associated with more traditional eulerian methods and their impact on fluid fluid reactions benson et al 2017 however particle tracking methods are less developed and widespread than eulerian methods and we are not aware of an available tool for lagrangian simulation of fluid solid reaction dynamics in the present work a ptrw approach has the added benefit of directly simulating lagrangian trajectories the properties of which are the basis for the theoretical developments this allows us to employ the same methodology to simulate the conservative component of the reactive transport problem and to determine first passage and return times see sections 5 and 6 of the main text the ptrw method used here is based on that employed in aquino and borgne 2021 to simulate conservative transport in a number of stratified flows where it was validated against theoretical predictions for dispersion concentration distributions breakthrough curves first passage times across a plane and lagrangian velocity distributions the conservative transport algorithm consists in discretizing the langevin equation 1 for a set of lagrangian particles or trajectories with prescribed initial conditions the displacement δ x t of a particle in a time step of duration δ t starting at time t is computed as the sum of the advective and diffusive contributions to the change in particle position x t a 1 δ x t x t δ t x t δ x a t δ x d t boundary conditions at the solid interface are implemented as elastic collisions based on the full displacement δ x t the diffusive contribution is obtained from a stochastic forward euler scheme as δ x d t 2 d δ t ξ t where the component of ξ t along each cartesian axis and at each time t is sampled independently from a unit zero mean and unit variance gaussian distribution for the two dimensional channel simulations of section 5 the advective contribution is also computed based on a forward euler scheme δ x a t v x t where the velocity field v is evaluated at particle positions according to the analytical flow field eq 35 here we have extended the algorithm to account for i elastic reflections on bead surfaces representing reflecting boundary conditions ii advection due to an arbitrary velocity field previously obtained and provided at a number of points on an arbitrary possibly unstructured grid and iii fluid solid reaction regarding i we implemented and independently verified a simple trigonometry based computation of perfectly elastic collisions on spherical surfaces valid in arbitrary dimension for circles in the plane etc regarding ii the flow field at a given position is obtained from linear interpolation based on delaunay triangulation and sibson natural neighbor coordinates on an arbitrary grid based on the well established cgal c library project 2020 for the beadpack simulations the advective particle displacement associated with the interpolated flow is computed according to a fourth order explicit runge kutta scheme finally iii is implemented according to the considerations of section 2 as discussed therein the spatial support scale ℓ d should be large compared to 2 d δ t in order to permit resolving fluid reactant concentrations near the interface and the resulting reaction rates we set the support scale a 2 ℓ d 10 2 d δ t it is important to note that ℓ d is not a parameter of the theoretical model but is rather associated with the discretization in this sense taking ℓ d a 2 d δ with a 10 leads to the same results in the continuum limit δ t 0 it is however necessary to take a 10 in order to guarantee that reactant concentrations near the interface are resolved correctly and converge for δ t 0 the diffusion time associated with the support scale ℓ d is τ d ℓ d 2 2 d thus our choice corresponds for a given ℓ d to setting δ t τ d 100 the algorithm employed to compute first passage and return times is validated here by the agreement with theoretical results for the one dimensional channel see section 5 similarly the agreement between theory and simulations found for the dynamics of total mass especially regarding the theoretical results for low and high damköhler number and the assymptotic reaction rates for arbitrary damköhler number discussed in sections 5 and 6 provides validation for the reactive transport algorithm we note that we have implemented only the a f b s b s reaction studied here but the reaction algorithm can be direclty extended to other reactions and heterogeneous distributions of solid phase reactants at the expense of explicitly keeping track of the surface concentrations of the latter across the fluid solid interface a github repository where the code is available under an open source license and regularly updated can be found at github com tcaquino beadpack further benchmarks and extensions to the fluid solid reaction algorithm will be included and discussed thereat the version of the code used in the present work can be found at the doi 10 5281 zenodo 4392882 additional documentation addressing further technical details may also be found in both these repositories in order to simulate reactive transport and determine first passage and return times using the ptrw algorithm we first require the flow field which may be given at a set of points on an arbitrary grid for the beadpack simulations of section 6 the flow field was obtained numerically from stokes flow simulations using a finite volume discretization in openfoam foundation 2020 with n c 1572138 hexahedral cells the flow was computed in a conventional cubic unit cell of side ℓ c 1 cm of the body centered cubic packing considered the associated body cubic centered bead diameter is d 3 ℓ c 2 simon 2013 corresponding to a spatial resolution of about ℓ c n c 1 3 10 2 d periodic boundary conditions were imposed on the cell faces and no slip boundaries at the spherical bead interfaces the flow had kinematic viscosity 5 10 7 m 2 s and was driven by a pressure gradient of 1 pa m obtained by applying a body force in terms of cartesian axes perpendicular to the unit cell faces the mean velocity vector had an orientation corresponding to an angle 5 π 40 with the x axis on the x y plane and an angle π 40 with the z axis this flow configuration has been analyzed in turuban et al 2019 and it is known to induce chaotic mixing as discussed in the same reference the contact points between beads can cause numerical instabilities in the numerical determination of the flow field this issue can be addressed by placing small spheres or cylinders with no slip boundary conditions on the exposed surface at the contact points between beads to avoid numerical issues near the contact points these structures do not have an appreciable impact on flow and transport properties turuban et al 2019 in the present simulations spheres of 20 the bead diameter were employed in this manner these contact spheres are not needed for stability of the ptrw simulations in the conservative and reactive transport simulations no boundary conditions were enforced on their surface and zero velocity points were added at the contact points instead particle positions undergoing advection diffusion were mapped onto the unit cell according to the periodic boundary conditions appendix b statistics of single interface visit times here we obtain the pdf of the time spent in the reactive region in each visit consider again one dimensional diffusion close to the interface the first passage time from the middle of the discretization cell nearest the interface to the middle of the adjacent cell corresponds to the duration of a visit to the reactive region it can be obtained by considering the first passage time to either boundary in a domain of length 3 ℓ d and starting from a distance ℓ d from one boundary as illustrated in fig b 9 the first passage time pdf to reach either boundary in a domain of length l starting from a distance ℓ from one and l ℓ from the other has laplace transform aquino and dentz 2018 b 1 ψ λ ℓ l csch ℓ 2 λ d csch l ℓ 2 λ d coth ℓ 2 λ d coth l ℓ 2 λ d setting l 3 ℓ d and ℓ ℓ d yields the single visit pdf b 2 ψ v λ 2 cosh 2 τ d λ 1 1 expanding for λ 1 τ d and inverting the resulting laplace transform leads to eq 19 recall that λ 1 τ d represents large times compared to the discretization time so that this result constitutes a step in taking the continuum limit rather than a large time approximation as before the same conclusion is reached if we consider the first passage times to within ℓ d of the interface starting from a distance 2 ℓ d from the latter this result remains valid in higher dimensions under the assumption that interface is locally flat at the scale of the transport model appendix c impact of first passage time to the interface on reaction dynamics according to the chctrw framework in the absence of the delay associated with first reaching the interface the generalized rate law governing the evolution of total fluid reactant mass is given by eq 8 when this delay is added the mass is equal to the initial mass for times smaller than the delay and evolves according to the previous equation for later times that is averaging over initial delays i e over an ensemble of lagrangian particles distributed according to the initial condition c 1 m t m 0 0 t d t ψ 0 t 0 t d t ψ 0 t m ϕ t t the first term represents the fact that if the interface has not been reached for the first time by time t which happens with probability 0 t d t ψ 0 t over the ensemble of lagrangian particles the fluid mass is equal to its initial value m 0 m 0 regarding the second term if the interface is reached for the first time at time t t which happens with probability density ψ 0 t the mass at time t is equal to m ϕ t t note that given a first arrival at the interface at time t t m t t m ϕ 0 m 0 then for times t t the mass dynamics proceed according to the inter reaction times with m ϕ obeying the chctrw equation 8 this result may be written in terms of the first passage time tail probability ψ 0 as c 2 m m 0 ψ 0 m ϕ ψ 0 substituting eq 8 for the dynamics of mass m ϕ resulting from the inter reaction times we obtain eq 25 appendix d mass dynamics and reactive time the dynamics of reactant mass may be formulated in terms of the statistics of the total time spent in the reactive region rather than the statistics of the inter reaction times in this appendix we show that the former approach leads to the same results as the latter which is developed in the main text consider the reactive time u d t representing the total time spent within distance ℓ d of the interface we denote its pdf by p u t that is p u u t d u is the probability that given total elapsed time t a particle has spent a time in the interval u u d u within distance ℓ d of the reactive interface for small ℓ d in preparation for taking the continuum limit ℓ d 0 we approximate d 1 u d t 2 τ d n d t where n d t is the random number of visits to the reactive region by time t each of which contributes the mean residence time per visit 2 τ d see eq 19 since u d t is proportional to n d t denoting by p n n t the probability that n d t n we have d 2 p u u t p n 0 t δ u 2 τ d 1 p n u 2 τ d t where δ is the dirac delta the number of visits to the reactive region by a given time depends on the pdf of inter visit delay times ψ d as well as on the pdf ψ 0 of the time of the first visit adapting the results of benson et al 2007 the laplace transform of the distribution of the number of visits to the reactive region with respect to time t in terms of these quantities is given by d 3 p n n λ ψ 0 λ n 0 ψ d λ ψ 0 λ ψ d λ n 1 n 0 in the limit of small ℓ d we obtain according to eq d 2 d 4 p u u λ ψ 0 δ u λ 1 ψ 0 λ e d u λ where d 5 e d u λ λ g λ ℓ c ℓ d exp λ g λ ℓ c ℓ d u with the rescaled tail probability g defined in terms of the return time tail ψ d according to eq 16 to quantify the dynamics of total mass in terms of the reactive time u d t we make use of a subordination type description feller 2008 benson and meerschaert 2009 meerschaert and sikorskii 2012 aquino and dentz 2017 within the reactive region reaction is well mixed in the sense described in section 2 the overall reaction then proceeds according to the time particles spend within this region the amount of mass left by time t is the average of the surviving mass m 0 exp k d u d t over all possible times u d t spent within ℓ d of the interface up to time t which are distributed according to p u t that is d 6 m t m 0 0 d u e k d u p u u t the laplace transform of the total mass 26 obtained using the inter reaction time formulation is then recovered by direct computation in the limit ℓ d 0 in terms of a laplace transform with respect to operational time denoted by a hat keeping time t fixed we may write eq d 6 as d 7 m t m 0 p u k d t which can also be interpreted as the laplace transform of the stochastic process u d ℓ d evaluated at the rate ℓ d k d k c a in the limit ℓ d 0 of fine discretization the stochastic process u d ℓ d is the local time at the boundary mentioned in the introduction karatzas and shreve 1988 takács 1995 grebenkov 2007 2019 note that in accordance with the standard terminology used in the literature the so called local time has units of time per length it is this quantity rather than u d itself that is well defined in the continuum limit this is directly related to the fact that a particle undergoing continuous diffusion in one dimension returns to the initial position infinitely many times within an arbitrarily small time interval despite the fact that this represents a mathematical abstraction it corresponds to the correct behavior when transport is adequately described by continuous diffusion at the scale of interest appendix e first passage times to the wall in a 2d channel in this appendix we provide the laplace transforms of the first passage time pdfs of solute to the channel walls for transport in stratified flow through a two dimensional straight channel as discussed in section 5 these are controlled by diffusion in the transverse direction although the velocity profile can impact the result due to its role in determining the initial distribution for example for flux weighted injections the different first passage times are obtained by using eq 27 for a point injection and weighting according each initial condition thus the mid channel injection a leads to e 1 ψ 0 λ ψ λ ℓ c 2 ℓ c sech 2 τ d λ for the homogeneous injection b we have e 2 ψ 0 λ 1 2 ℓ c 0 2 ℓ c d ℓ ψ λ ℓ 2 ℓ c tanh 2 τ d λ 2 τ d λ for the flux weighted case c we find e 3 ψ 0 λ 1 2 ℓ c 0 2 ℓ c d ℓ v ℓ ℓ c v ψ λ ℓ 2 ℓ c 3 2 τ d λ 1 tanh 2 τ d λ 2 τ d λ and for all mass starting at the channel walls d e 4 ψ 0 λ 1 appendix f interface extent coefficient for the body centered cubic beadpack in order to compute the interface extent coefficient ρ for a body centered cubic beadpack first note that the bead radius is related to the conventional cubic unit cell side by r 3 ℓ c 4 where we have taken the cell side ℓ c as the characteristic length see e g simon 2013 on the theory of crystalline structures within a unit cell there is a full bead at the center and eight bead quarters at each cell corner totaling a solid volume of two full beads since the volume of a bead is v b 4 π r 3 3 the porosity is given by f 1 φ 1 2 v b ℓ c 3 1 3 π 8 0 320 the surface area of a bead is a b 4 π r 2 so that according to eq 48 we have f 2 ρ 2 a b ℓ c φ ℓ c 3 3 π 2 1 3 π 8 1 
281,riverine floods are often caused by prolonged or heavy rainfall these heavy or extreme rainfall events are mainly driven by various climate mechanisms which have often been ignored in flood risk assessments this study presents a climate mechanism based flood frequency analysis approach accommodating a direct linkage between risk metrics for decision making and the dominant causal climate mechanisms driving riverine flooding the approach adopts several techniques including markov chain analysis k nearest neighbor knn resampling and z score based jittering method to mimic historical climate data with the heterogeneous consideration of the climate mechanisms after that the effects of climate change are incorporated through the manipulation of the transition matrix and the utilization of a quantile mapping approach future flood frequencies are quantified using the generated climate sequence and a rainfall runoff model our proposed approach is described as a case study for the nam river basin south korea using two climate mechanisms the tropical cyclone tc and non tcs our results indicate that both mechanisms could potentially contribute to future flood extremes especially when subjected to climate change however tcs have been proven to be the primary driving climate mechanism that shows more significant and immediate effects on the probable extremes in the study area the approach presented in this study can provide new insights into future flood management keywords tropical cyclone flood frequency analysis nam river basin climate change 1 introduction flooding is one of the most damaging and deadly natural disasters in the world hallegatte et al 2013 huang et al 2018 in the 20th century alone it has caused 6 8 million deaths and hundreds of billions of dollars worth of damage doocy et al 2013 with the rising global temperature and sea levels flooding events are also expected to increase in frequency and severity in the coming years quesada montano et al 2018 although substantial improvements have been made in flood management for the past years it still requires further enhanced strategies for more appropriate and timely flood risk assessment van wesemael et al 2019 it motivates this study to explore the methods used for regional flood frequency analyses including their extension to future conditions which could help design proper flood prevention protocols and sustainable flood control infrastructure flood risk analyses are often carried out through flood frequency analysis ffa results of ffa serve as bases for assigning appropriate design standards for flood control infrastructure butler et al 2018 fu and butler 2014 determining coverage of flood insurance programs dawdy et al 2012 gilroy and mccuen 2012 and flood zoning botero and francés 2010 for this method the annual high flows recorded over a long period in a river are fitted to a probability distribution to forecast flood frequencies that correspond to various return periods or probabilities fernandes et al 2010 tanaka et al 2017 the fundamental assumption behind conventional ffa is that the collection of high flows is randomly generated suggesting that they are derived from the random superposition of complex processes in the atmosphere and river system bhat et al 2019 however the occurrence of high flows i e flooding is not only derived from random processes rather it is an output of the combination of several contributing factors such as land covers ahn and merwade 2017 gao et al 2016 soil moisture conditions wanders et al 2014 and various atmospheric phenomena e g rainfall snow gilroy and mccuen 2012 in particular the occurrence of extreme flooding events are mainly caused by prolonged or heavy rainfall armstrong et al 2014 li et al 2015 these extreme events are intensified by various large scale patterns in the atmosphere accordingly the information about large scale patterns and their influence on historical flows have been frequently studied to achieve more reliable flooding management strategies e g muis et al 2018 rueda et al 2017 in line with this studies focusing on the underlying processes that cause the occurrence of precipitation have also emerged e g gimeno et al 2012 wang and chen 2008 yong et al 2016 these studies have shown that precipitation occurrences are driven by several synoptic disturbances or climate mechanisms such as monsoon fronts convection from local heating of the surface and other weather and cloud systems like tropical cyclones these diversities suggest that rainfall driven by the diverse climate processes may have different spatiotemporal properties therefore the individual impacts of the climate processes have been considered in rainfall frequency analyses son et al 2017 wright et al 2013 yoon et al 2013 detection of current and future rainfall trends e g kim and jain 2011 knutson et al 2010 and inspection of the spatial and temporal patterns of rainfall kim and jain 2011 wright et al 2014a for instance yoon et al 2013 found that the annual maximum tropical cyclone rainfall have a more critical contribution to rainfall quantiles than convective rainfall after conducting a nonstationary rainfall frequency analysis the discrete consideration of climate mechanisms is also found in studies dealing with long term variabilities of renewable energy bartos et al 2016 mohammadi and goudarzi 2018 and seasonal flood characteristics carter et al 2018 nakamura et al 2013 acknowledging the necessity of identifying climate mechanisms more comprehensive analyses that consider the individual impacts of climate mechanisms in flood production have been encouraged by hirschboeck 1987 since that seminal paper mixed distributions have been used as an alternative for regular probability distributions to estimate flood extremes alila and mtiraoui 2002 barth et al 2019 2017 smith et al 2011 yang et al 2019 another innovation is the utilization of process based ffa to generate synthetic storm scenarios e g cameron et al 1999 wright et al 2014b yu et al 2019 from this method flood frequency curves are constructed through the combination of observations stochastic models of multi scale temporal variability and hydrologic models that take into account all the flood generation processes e g rainfall and soil moisture sivapalan and samuel 2009 results from past studies show that the approach is useful in considering the complex interactions between flood generating climate processes however despite all these innovations the individual impacts of flood causing mechanisms under climate change have received less attention in the literature the individual consideration of climate mechanisms in ffa could be crucial for future projections since some studies have revealed that these flood generating climate processes will be heterogeneously altered due to climate change e g lee et al 2020 xia et al 2017 many studies exploring the characteristics of rainfall driven by various climate processes have projected substantial changes due to the changing atmospheric system e g lackmann 2013 shields and kiehl 2016 wright et al 2015 moreover due to these alterations substantial changes are expected in future hydrologic conditions e g marsooli and lin 2020 salathé jr et al 2014 these studies have utilized climate models to explore the impacts of climate change on future streamflow projections for instance salathé et al 2014 projected future flood risks from atmospheric rivers and orographic precipitation in the pacific northwest river basin through a regional climate model their results showed that the peak flows in the study area will significantly increase by 30 in 2040 2069 while most of the studies presented results that might likely happen in the future some studies have argued that climate models often underestimate or cannot accurately capture future projections associated with climate mechanisms wang et al 2015 other studies also provided criticisms of climate models since they disregard the differences between the characteristics of climate mechanisms during the model construction e g mukundan et al 2019 and shimura et al 2015 another limitation emerges when the changes in climate mechanisms are explored to project future hydrologic conditions using climate models here the behavior of future results entirely depends on the selected climate scenario for the analysis this suggests that the projected future changes are determined by how the scenario is chosen likely leading to under or over estimations since ffa is one of the crucial topics in climate change assessment almasi and soltani 2017 duan et al 2017 there is a need to use an alternative approach that could explore the possible hydrologic changes caused by the changes in the characteristics of the climate mechanisms based on the aforementioned knowledge gaps a climate mechanism based future ffa may be a desirable tool for water resources management this study tries to incorporate the diverse impacts of the climate mechanisms to the conventional ffa through a bottom up approach the bottom up approach allows decision planners to examine various contributions e g projection and temporal trend to identify future climate conditions that could lead to unacceptable performances of water resource systems the approach is attractive since it explores climate vulnerabilities by introducing a wide range of climate stresses beyond climate model projections stephens et al 2018 therefore several bottom up approaches have been developed to assess how different future climate assumptions affect risk decision planning e g culley et al 2016 knighton et al 2017 ray et al 2019 under this approach we develop synthetic climate scenarios driven by the occurrence of climate mechanisms using a weather generator to sum up this study proposes the climate mechanism based bottom up approach using two main climate mechanisms tailored for the study area a flood frequency analysis that acknowledges the heterogeneous impacts of tropical cyclones tcs and non tc rainfall with the consideration of various future climate change scenarios is conducted in the nam river basin south korea the rest of the paper is organized as follows section 2 presents the study area and the data utilized in this study section 3 presents a statistical framework used to quantify the heterogeneous impacts of tc and non tc rainfall with the consideration of the various effects of climate change on future design floods in the area of study a summary of the results is then presented in section 4 followed by conclusions in section 5 2 study area and data lying in the northern hemisphere south korea has four distinct seasons with a climate that is influenced by the northeastern asian and the western pacific ocean jung et al 2002 the country receives two thirds of its annual precipitation during summer june september while water deficits occasionally occur during winter cha et al 2011 since 65 of south korea is mountainous and most of the river reaches are short and have steep slopes water management problems such as flooding and drought are very common especially in the five major rivers located in different parts of the country including han river nakdong river geum river sumjin and yeongsan river lah et al 2015 the nakdong river basin which has the second largest drainage area in south korea passes major cities such as busan and daegu where almost 6 million people reside in total kim and kim 2018 through the years the basin has suffered from hydrological problems such as drought and flooding eum and simonovic 2010 in particular the basin loses an annual average of two million us dollars from the damages inflicted by flooding events oh 2013 the susceptibility of the basin to tc events and the impending impacts of climate change on the magnitude and intensity of tc rainfall might affect the sustainability and the quality of existing hydrological infrastructures in the area with this motivation the nam river basin which is one of the sub basins in the nakdong river basin is selected as our area of study the nam river basin has a geographical coordinate of 35 54 27 n and 127 46 91 e and a drainage area of 2 285 km2 see fig 1 the basin receives as much as 70 80 of the annual total precipitation during summer and the rainfall distribution is mainly based on the basin topography yoon et al 2014 on average the nam river basin receives a higher amount of rainfall which ranges around 1400 mm to 1600 mm per year in contrast to the mean annual precipitation of south korea which is 1245 mm daily climate data recorded in the nam river basin such as precipitation and maximum and minimum temperature are utilized for this study the weather data are collected from the 60 weather stations spread throughout the country by the korea meteorological administration s automated surface observing system asos and are interpolated to derive daily climate data for the study area that spans from january 01 1973 to december 31 2019 for hydrologic modeling daily observed streamflow data is collected from the water resources management information system http wamis go kr the available streamflow data in the study area starts on january 01 2000 and ends on december 31 2019 to determine tc and non tc rainfall days we follow a methodology recently utilized by alcantara and ahn 2020 from their methodology the radius which could effectively define the rainfall induced by tropical cyclones is determined with the aid of the tc best track data recorded from 1973 to 2018 from the regional specialized meteorological center rsmc tokyo typhoon center of the japan meteorological agency jma http jma go jp to be specific they define tc rainfall as any rainfall that occurs within a 500 km radial distance from the tc center to the weather station during the onset of a tc event more details of how to determine tc and non tc rainfall days can be found in the supplementary material see text s1 3 methodologies this study proposes a climate mechanism based approach for flood frequency analysis we note that the approach can be extended to explore many other climate mechanisms e g tropical and extratropical activities but it is only employed for the heterogeneous impacts of tc and non tc events as a case study the proposed approach comprises five components 1 the generation of synthetic climate data while recognizing the differences between the characteristics of tc and non tc rainfall using several statistical methods namely markov chain analysis the k nearest neighbor knn algorithm and the z score based jittering method 2 incorporation of the future effects of climate change on a synthetic climate data 3 streamflow generation using a rainfall runoff model 4 the execution of flood frequency analysis and lastly 5 the investigation of various potential flood frequencies brought by climate change we briefly illustrate the methodology in fig 2 and a detailed description of each part are further offered in the following sections 3 1 synthetic climate data with the identification of tc and non tc induced rainfall here a synthetic daily time series of meteorological variables x x 1 t x 2 t x j t with length j is desired where x j t indicates the jth climate variable at time t t 1 t this climate data generation starts with the occurrence process or the construction of sequences of states for this approach the three state first order markov chain model is employed with states defined as 1 tc st 0 or rainfall events that occurred within a defined distance from the tc center 2 non tc st 1 or rainfall events which are not considered as tc rainfall and lastly and 3 no rain st 2 or historical records which are less than 0 1 mm after defining all the states and assigning arbitrary labels over the historical records the transition matrix t m i i m for each month m is determined each t m i i has a size of 3 3 with each coordinate showing the probability of a state occurring at time t and transitioning to another state at time t 1 these conditional probabilities cp are computed using the following equation 1 c p i i p s t 1 i s t i i i 0 1 2 where i and i are the state of the present and the next day respectively using the transitional matrices time series containing the sequences of three states are generated for n simulation years hence each state in the simulated time series on a given day on an mth month is based on the state on the previous day and the transitional probabilities for that mth month next a k nearest neighbor knn resampling approach is used to populate each month of the simulated period for all meteorological variables x the approach determines k number of historical records that are most similar to the conditioning vector in terms of their characteristics the conditioning vector then adapts the values of the closest neighbor that the algorithm randomly selects for this study the markov modeled state conditions particularly the total number of monthly tc non tc and no rain occurrences per year are compared to the yearly preexisting historical conditions using the euclidean distance formula for each simulated month in a particular year five nearest historical years i e k 5 are extracted and randomly sampled to construct a daily climate sequence x in this resampling process the months of september may are sampled individually while the months of june august are sampled as one continuous climate sequence since transitional rainfall events during these months are very common here transitional rainfall events are defined by the case when the precipitation state on the last day of the previous month has the same state as the first day of the present month based on the methods described above the synthetic data is resampled from the historical records however the approach limits the extrapolation ability of the model to generate unobserved climate sequences outside the range of the existing records nourani and partoviyan 2018 to address this limitation the z score based jittering method is added to the proposed model for the jittering method noises are added to the simulated time series to allow the generation of synthetic climate data beyond the range of the historical records while primary historical properties are maintained the noise for this study is introduced by adding normally distributed random numbers with mean 0 and optimum standard deviation σ to the quantiles of the non zero tc and non tc rainfall the equivalent precipitation amounts of the two adjusted quantiles are then combined to produce a new time series the optimum standard deviation is determined by comparing the extremes of several adjusted time series having different standard deviations against the extremes of the historical record using the normalized root mean square error nrmse 3 2 incorporating changes in synthetic climate data for flood risk assessments it is important to consider the heterogeneous changes of climate mechanisms under the influence of climate change however employing the proposed approach in section 3 1 alone hinders the generation of climate sequences that might transpire beyond the distribution of the historical records because of unforeseen changes in the climate system therefore this study introduces two adjustments through the modification of the transition matrix tm and the application of a quantile mapping approach to incorporate the effects of climate change on the frequency and magnitude of the climate data respectively in this study we have a transition matrix t m i i for mth month with nine coordinates p 00 p 01 p 02 p 03 p 10 p 11 p 12 p 20 p 21 p 22 each containing a conditional probability of c p i i the probability in each coordinate dictates how the climate mechanisms particularly the tc and non tc events interact and transition from one event to another considering climate change changes in the patterns and behavior of these climate mechanisms are expected to affect and alter the frequency of the rainfall that they induce these changes can be directly reflected in the historical transitional probabilities to factor in these changes a transition matrix with adjusted transitional probabilities t m i i is introduced in the system as the following equation 2 t m i i t m i i a m i i here a m i i is the additive factor matrix formulated to control how the occurrence of the climate mechanisms are affected by climate change t m i i is the original transition matrix and t m i i is the newly adjusted transition matrix when the effects of climate change are considered through the utilization of the additive factor matrix many possible changes in the original transition matrix can be introduced as a simple example to show how to utilize our proposed model occurrences of tc and non tc events are treated as independent from each other in this study we also note that numerous heterogeneous changes of climate mechanisms can be considered beyond this example this analysis intends to explore the primary driving climate mechanism that has the potential to cause more extreme flooding events given similar future climate conditions therefore we assume that the change in the frequency of tc rainfall does not affect the characteristics of non tc rainfall and vice versa when the future effects of climate change are considered in line with this principle the additive factor matrix for tc events a m i i t c only carries adjustment factors λ tc on coordinates involving transitions between tc and no rain events p 00 p 02 p 20 p 22 similarly non tc events are adjusted using the additive factor matrix a m i i n o n t c containing adjustments λ nontc aimed to modify transitional probabilities for non tc and no rain events p 11 p 12 p 21 p 22 since an infinite range of possible adjustment factors can be exercised the adjustment factors for the two conditions are limited and decided based on the yearly historical transition matrices to ensure that the introduced scenarios are realistically feasible presented in section 4 4 this limitation will be further discussed in section 5 the two matrices tm tc and tm nontc are employed to determine how the independent and combined changes in the frequency of the climate mechanisms might affect the behavior of the climate data a quantile mapping approach is used to account for the changes in the magnitudes of the climate mechanisms this is delivered by quantifying the changes in the distribution of the daily precipitation given a generated time series of precipitation data for n number of years the non zero tc and non tc daily precipitation are extracted to represent two individual data sets here we assume that each set of precipitation can be fitted to a theoretical cumulative distribution function cdf with a parameter set p to incorporate the effects of climate change incremental changes can be applied in the mean standard deviation or extremes of the original sequence which are reflected in the new parameter set p the new parameters are then used to achieve the target cumulative distribution function then the quantiles of the target distribution are matched to the quantiles of the original distribution to determine the new daily values in this study the non zero daily tc and non tc precipitation data are individually fitted to a gamma distribution with shape k and scale θ parameters from the method of moments hansen 1982 the parameters are represented through their relationships with the mean μ and the standard deviation δ of the two time series here the scale parameter could take the form of θ δ 2 μ while the shape parameter can be k δ 2 μ 2 in this study the δ is selected as the control variable while the μ is the independent variable which is incrementally altered after modifying the tc and non tc precipitation individually the two altered data sets are then incorporated to represent one data set we also note that changes in other meteorological variables can also be accounted for using the quantile mapping approach for this study temperature changes are manipulated using a gaussian distribution with mean μ and variance δ2 parameters a summary of the adjustment factors and conditions is presented in table 1 3 3 hydrologic modeling after embedding the probable effects of climate change to the driving climate mechanisms the generated synthetic climate data are then converted into runoff this is realized using a parsimonious daily lumped conceptual hydrologic model the simhyd model fig 4 chiew et al 2002 this model is a simplified version of the hydrolog porter and mcmahon 1976 and modhydrolog chiew and mcmahon 1994 models compared to the models mentioned above which use over 15 parameters the simhyd model has seven parameters and only requires daily precipitation and potential evapotranspiration as the input data yu and zhu 2015 a routing model developed based on muskinghum mccarthy 1938 is also included to improve the model performance by providing an additional store to replicate routing of surface flows we note that a lumped model is utilized in the study area since it does not have any significant spatial variabilities however distributed models are more proper in areas where topography plays a significant role the water movement in simhyd starts with the rainfall filling the interception store which is emptied daily by evaporation the excess rainfall that did not evaporate is introduced to an infiltration function which defines the infiltration capacity of the catchment any rainfall which goes above the infiltration capacity then becomes the infiltration excess runoff meanwhile the infiltrating moisture is subjected to a soil moisture function which redirects the water to the interflow stream groundwater store and soil moisture store the simhyd model finally collects the total runoff from different sources which include infiltration excess runoff interflow and baseflow and an additional surface flow which is routed through a storage the overall process of simhyd model with routing is driven by nine parameters including interception store capacity ω1 maximum infiltration loss ω2 infiltration loss exponent ω3 soil moisture store capacity ω4 constant of proportionality in interflow equation ω5 constant of proportionality in groundwater recharge equation ω6 and baseflow linear recession parameter ω7 and storage time constant ω8 and proportionality constant ω9 of a routing model a summary showing the ranges of the probable values for each parameter is shown in table 2 while the model structure of simhyd illustrating the water movement can be seen in fig 3 for more details a theoretical description of simhyd can be found in yu and zhu 2015 and zhang et al 2013 this study also employs the priestley taylor equation priestley and taylor 1972 to estimate the daily potential evapotranspiration pet the equation requires the daily minimum and maximum temperatures days of the year and the latitude of the basin as its input also since it is a closed water balance model the model computes the actual evapotranspiration aet by getting the difference between the rainfall input data and runoff all parameters of the simhyd model are calibrated against the observed streamflow data for the period of january 01 2009 to december 21 2019 11 years by using a bayesian framework to explore parametric uncertainties as an objective function a weighted combination of nash sutcliffe efficiency nse nash and sutcliffe 1970 and a logarithmic function of bias viney 2009 is adopted as follows 3 m a x i m i z e 1 t 1 t q t s i m q t o b s 2 t 1 t q t o b s q t o b s 2 5 ln 1 t 1 t q t s i m q t o b s t 1 t q t o b s 2 5 where q t s i m and q t o b s are the simulated and observed daily streamflow at time t respectively the additional bias constraint in the objective function prevents the unconstrained least square measures such as nse and kling gupta efficiency kge gupta et al 2009 to ignore high biases in events when there are high flow variabilities viney et al 2009 after calibration the accuracies of the computed parameter sets are then validated with the observed streamflow from the period of january 01 2000 to december 31 2008 9 years for the bayesian calibration the dreams markov chain monte carlo algorithm laloy and vrugt 2012 is used to explore the posterior distributions of all parameters three chains are employed for each parameter while convergence is confirmed using the gelman and rubin factor gelman et al 1992 and trace plots of the chains 3 4 execution of flood frequency analysis the generated runoff data is fitted into a probability distribution to estimate the frequency of their occurrence or return period to do so high flows are firstly extracted using the peak over threshold pot method in contrast to the usual annual maximum series ams wherein only the maximum data is extracted per year li et al 2015 the pot method allows the extraction of various extreme flows or floods above a certain threshold the increase in the number of high flows reduces the inherent uncertainties leading to more reliable flood frequency dodangeh et al 2019 here the generalized pareto distribution gpd is employed the distribution has been widely utilized to effectively model floods and to estimate extreme quantiles together with the pot method e g bezak et al 2014 gharib et al 2017 the cdf of the gpd is as follows 4 f q 1 1 ξ q η ζ 1 ξ ξ 0 1 exp q η ζ ξ 0 where q represents the high flow value mm day ξ as the shape parameter and ζ as the scale parameter on the other hand η represents the threshold to define the high flows for this study the high flows are considered as any rainfall that exceeds the 80th percentile of the runoff which is generated with no climate change interventions although a higher percentile is more desirable when determining extremes the 80th percentile is chosen in this study due to relatively low number of tc samples 3 5 identifying potential flood frequency changes under climate change finally future flood frequencies are identified using the bottom up approach brown et al 2012 2011 the main idea for the bottom up approach is that a flooding response map can be developed based on a wide range of plausible future climate conditions in the context of our flood frequency application the frequency analysis is executed by systematically controlling λ and p individually grounded by their plausible future values to explore the possible flood frequencies corresponding to a certain return period this approach returns a response surface that illustrates the possible risks brought by the changes in the climate mechanisms note that while the original bottom up approach explores changes in climate conditions e g changes in temperature the approach is extended to consider the heterogeneous climate mechanisms for this study by comparing diverse changes in λ and p we can map out how the different aspects of projected future hydrologic processes influence flooding metrics relevant for decision making and eventually assist the identification of the dominant climate mechanism which is crucial in future water resources management 4 results 4 1 characterization of tc and non tc rainfall at first the characteristics of rainfall associated with the dominant mechanisms in the study area are examined we note that the rainfall utilized in this analysis is derived by averaging all the available rainfall data in the study area here we present the cumulative distribution function cdf of tc and non tc induced daily fig 4 a and annual maximum precipitation fig 4b the distribution of the non tc precipitation is observed to have a very long tail skewed to the right illustrating that most non tc events bring relatively small amounts of precipitation consistently extreme values i e more than 100 mm are quite infrequent meanwhile the cdf of daily tc precipitation shows a curve emphasizing the frequent occurrence of extreme rainfall compared to non tcs higher ranges starting from the 80th percentile of seasonal tcs are composed of extreme rainfall events inspecting the behavior of their individual annual maxima shows that the 40th percentile of the non tc rainfall 81 91 mm is much higher than that of the tc rainfall 70 51 mm this behavior can be attributed to the annual occurrence of tcs although tc events occurred 140 times in 47 years records reveal that they had been absent for 4 years meanwhile subjecting the two sets of distributions to a kolmogorov smirnov test show similar results both sets derive p values lesser than 0 001 indicating that the distributions of the daily and annual maximum tc precipitation are significantly different from non tcs the results show that rainfall events generated by the climate mechanisms are indeed heterogeneous indicating the need to acknowledge their differences when conducting hydrological analyses 4 2 model evaluations models dealing with the generation of meteorological variables corresponding stream flows and frequencies based on high flows are utilized in this study these include a proposed model which is employed to produce synthetic climate data conditioned based on their driving climate mechanisms a hydrologic model to generate runoff data and a probability distribution for flood frequency analysis the evaluation of the performances of these models is vital to ensure realistic reproductions as mentioned in section 3 3 the hydrologic model utilized in this study requires three meteorological variables precipitation and maximum and minimum temperatures as its input data these meteorological variables are produced using the proposed model for climate simulation presented in fig 5 are the first to fourth order moments of the average daily precipitation data aggregated per 5 days i e pentad we also provide the results for the maximum temperature and minimum temperature in figures s1 and s2 these data are derived from generating a time series of meteorological variables with a length of 47 years similar to the length of the historical data for 50 separate times without imposing changes from λ and p as presented the reproductions can properly capture most historical statistics presented as black points although there are instances of biases e g skewness and kurtosis all historical statistics are included in the 95 confidence interval of the generated data suggesting that the model can effectively reproduce the statistics of all historical variables however we also note that these biases could potentially lead to underestimations especially when representing extreme floods despite the promising results from the processes biases in representing rainfall extremes are still found to limit their efficiencies see fig 6 b hence in this study the jittering method is adopted not only to produce unobserved climate sequences but also to mitigate the abovementioned limitation the method includes the determination of an optimal standard deviation σ for the normally distributed random numbers added as jitters noises in the generated climate data each generated time series from the 50 ensembles utilized before this analysis is subjected to noises containing various standard deviations σ ranging from 0 1 to 0 5 the average extremes derived from each standard deviation are then compared to the historical rainfall extremes see fig 6a using the normalized root mean squared error nrmse compared to other conditions the extremes computed using the σ 0 29 derived values closest to the historical data to confirm this the values of the return periods computed using the optimal standard deviation are compared to the extremes when the z score based jittering method is not employed all the return periods are captured better when noises are introduced in the system showing the additional necessity to incorporate the z score based jittering method in the proposed model fig 7 compares the hydrographs of the observed and simulated daily runoff data for the calibration and validation of the simhyd model results show that low and high flows during the calibration period are within the 95 credible interval of the generated runoff data although there are cases of over and under estimation most of the simulated cases are still in close agreement with the observed data the nash sutcliffe efficiency nse value for the calibration period is 0 65 while the validated data gave an efficiency value of 0 78 suggesting that the rainfall runoff model performs well and provide an acceptable estimation of the historical runoff data based on the performance criteria utilized in past studies li and zhang 2016 yu and zhu 2015 however further evaluation of the performance of the model shows that it cannot properly capture the annual maximum peaks and peaks over threshold of the observed data for the validation period see figure s4 hence for this study the expected changes in future extremes are based on their proportion to the undisturbed values see fig 10 next following the pot method simulated non tc and tc high flows are extracted and compared to the cdfs of the historical non tc and tc high flows respectively fig 8 based on their median quantile estimates both distributions illustrate some overestimations however all the historical records are within the 95 credible intervals of both simulations particularly for higher quantiles e g 95 which is more important when estimating flood frequencies both simulated flows are well matched with the historical ones based on these analyses we confirm that the gpd is acceptable and is used for further analysis 4 3 flow simulations with the heterogeneous changes in the characteristics of climate mechanisms the annual streamflow after employing λ tc is plotted against the result acquired from employing λ nontc fig 9 a also the number of tcs per year with λ tc occupies the upper half and tcs of λ nontc covers the lower half of the figure the λ is introduced to the t m i i of months having both tc and non tc events to account for the impending impacts of climate change to the frequency of the climate mechanisms after assuming an increase of 3 in tc frequencies flows averaging up to more than 820 mm are recorded for most of the years despite having similar lengths the result is significantly greater than the historical average approximately 596 mm also compared to the historical number of tcs that pass through the area annually four tcs per year an increase of 3 in the probability of tcs brings up to 11 tcs in a year on the other hand the 3 increase in the occurrence of non tcs results in an annual average of 755 mm this is slightly lower than the result when tcs are increased also increasing non tc events causes a decrease of tcs in some years similarly 50 ensembles of time series n 47 are subjected to adjustments fig 9b here the average streamflow as well as the average number of tcs per ensemble are presented for most simulations stream flows upon the employment of λ tc are found to be greater than the stream flows when non tcs are adjusted however the averages from the two cases both exceed the mean historical streamflow indicating the sensitivity of all the climate mechanisms to climate change both cases also show similar rainfall days however simulations having an average of 5 tcs per year have a higher streamflow average compared to their counterparts the result implies that more extreme rainfall events occurred in tc altered simulations this hypothesis is explored in the following section 4 4 flood frequency analysis with the consideration of the heterogeneous impacts of tc and non tc events based on various climate change scenarios finally the plausible extreme flood projections are examined through the changes in the frequency and magnitude of the climate mechanisms fig 10 a fig 10b shows the plausible 20 year extremes when the frequencies magnitudes of the tc and non tc induced precipitation are changed in fig 10a we employ the ratio between the streamflow for a certain combination of λ nontc and λ tc and the undisturbed condition λ nontc 0 and λ tc 0 for easy interpretation a similar pattern is presented in fig 10b after employing p first visual assessment readily shows an asymmetric relationship between the changes in tc and non tc frequencies wherein drastic changes seem to be more prevalent when tcs are adjusted the left bottom corner shows the 20 year extreme values when only the non tcs tcs are altered increasing tcs by less than 1 5 provides an immediate shift from the undisturbed extreme in contrast with its counterpart also imposing both changes on the frequencies of climate mechanisms results in a significant increase in the probable values of the design flood to be specific utilizing both the maximum adjustment factors for each climate mechanism yields a 7 increase in the 20 year extreme similar results are also observed for other return periods see fig s3 then we evaluate the extremes due to the changes in the magnitudes of the climate mechanisms while assuming that their frequencies remain unchanged through a quantile mapping approach in this analysis p derived from changing each µ of the available non zero tc and non tc precipitation is introduced in the system here to account for the changes in the distribution of the minimum and maximum temperatures a quantile mapping approach is also used through the modification of the parameters for the gaussian distribution however results have shown that the application of alterations does not show any significant difference in the distribution of the two variables not shown accordingly the change in the magnitude of the climate mechanisms is mainly illustrated by the changes in the magnitude of precipitation as opposed to the result in changing the frequencies the change in the magnitude of the climate mechanisms displays several uniform behaviors from the two conditions see fig 10b for instance increasing the magnitudes of tcs and non tcs individually by 10 and 20 exhibits an almost similar degree of upward shift from the undisturbed extreme also a reduction in the magnitude of non tcs does not show any significant change from the undisturbed extreme which is similar to the change in tcs similar to the result from the preceding analysis imposing both changes on the magnitudes of the two climate mechanisms shows significant changes from the undisturbed extreme overall although both mechanisms provide significant impacts on flood frequency projections the analysis has shown that tc is the primary driving climate mechanism that has the potential to cause more extreme flooding events results indicate that tc induced meteorological variables overpower non tcs in posing flood frequency risks in the area changes in its magnitude and frequency show significant and immediate effects to the probable extremes for various return periods traditionally flood frequencies are conducted without the distinction between the driving climate mechanisms however considering their differences could give decision makers more information to impose more appropriate actions in cases wherein the two dominant processes behave differently 5 summary and conclusions this study demonstrates a climate mechanism based flood frequency analysis approach accommodating a direct linkage between risk metrics for decision making and the dominant causal climate mechanisms driving riverine flooding we believe that the approach presented in this study is a crucial step towards a novel future planning tool based on an understanding of climate flood linkages merz et al 2014 we apply this approach to the nam river basin south korea to support decision makers in identifying two dominant mechanisms responsible for flood frequencies employing the proposed approach reveals that each climate mechanism has distinctive characteristics even though both mechanisms the tc and non tcs play significant roles in high flow events results from tc altered simulations demonstrate that changes in the characteristics of tcs could lead to greater flood frequency risks in the study area compared to non tcs in the future decision makers may require certain measures that would emphasize the effects of tcs to provide more accurate plans to ease flooding problems for instance imposing a climate mechanism based dam release operation policy may significantly reduce flooding and drought problems from this the amount of water to be released will depend heavily on the specific climate mechanism driving the extreme rainfall event to avoid under or over discharging of water albeit the promising results several relevant limitations need to be further addressed which could be suitable for future studies for instance the approach may be limited in regions with insufficient historical records although we have tried to mitigate this limitation by adopting a jittering method the reproductions are ultimately rooted in historical scenarios insufficient historical records make it difficult to precisely measure the performance of the model miao et al 2016 also our case study is illustrated by a watershed in which a snow accumulation melting process is insignificant accordingly the changes in the distribution of temperatures may not provide significant effects on streamflow generations however as described by o gorman 2014 the behavior of snowfall due to the warming climate may be crucial in other areas hence it would also be valuable to explicitly address the contribution of temperature changes in projecting flood frequencies the most significant limitation in this study is linked to the adjustment factors assigned to reflect the future impacts of climate change in the climate mechanisms based on the proposed framework the analyses are bounded by the changes in the historical events during the period 1973 2019 since various external factors contribute to future deep climate change future scenarios could be substantially altered from the historical records hence for the current formation we have no choice but to consult decision makers to assign proper and appropriate values for the adjustment factors to derive more realistic and reliable results to improve the current framework an analysis that is not constricted by historical events should be developed this can be achieved by still adapting the bottom up approach since it allows the consideration of boundless climate stressors to identify the vulnerabilities in a system steinschneider and brown 2013 for future studies it would be beneficial to consider more driving forces that cause the occurrence of extreme hydrological events for instance atmospheric rivers ars have been gaining attention in the field of hydrology since it has a distinct feature from extratropical cyclones and has its own development and life cycle mundhenk et al 2016 studies have shown that the presence of ars contributes to the occurrence of more intense rainfall and flooding events han et al 2020 sodemann and stohl 2013 meanwhile an in depth analysis of the characteristics and classification of heavy precipitation systems can also be executed to improve the evaluation of flood risks in an area on the other hand although tcs are more dominant in this study other areas might present a different result due to the variation in the spatial distribution of the mechanisms raising the need to apply the proposed methodology to other watersheds also the effectiveness of our proposed approach in supporting existing research particularly in the context of predicting future changes can be explored for instance chand et al 2017 reported that the occurrence of tcs in the western and central north pacific could increase from an average of 5 to 7 tcs per year in 2070 2100 to support their claim their projected number of tcs can be used as the base in determining the appropriate adjustment factors to manipulate the transition matrices for the study area we can then evaluate the number of predicted tcs in a year see fig 9 to explore whether the events lead to unacceptable changes in hydrologic projections once the effectiveness of the model in representing future changes in tcs is confirmed the model can then be used to evaluate the changes in flood extremes in the study area which is another promising way of utilizing our proposed model credit authorship contribution statement angelika l alcantara formal analysis investigation methodology writing original draft kuk hyun ahn conceptualization supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the national research foundation of korea nrf grant funded by the korean government msit no 2019r1c1c1002438 
281,riverine floods are often caused by prolonged or heavy rainfall these heavy or extreme rainfall events are mainly driven by various climate mechanisms which have often been ignored in flood risk assessments this study presents a climate mechanism based flood frequency analysis approach accommodating a direct linkage between risk metrics for decision making and the dominant causal climate mechanisms driving riverine flooding the approach adopts several techniques including markov chain analysis k nearest neighbor knn resampling and z score based jittering method to mimic historical climate data with the heterogeneous consideration of the climate mechanisms after that the effects of climate change are incorporated through the manipulation of the transition matrix and the utilization of a quantile mapping approach future flood frequencies are quantified using the generated climate sequence and a rainfall runoff model our proposed approach is described as a case study for the nam river basin south korea using two climate mechanisms the tropical cyclone tc and non tcs our results indicate that both mechanisms could potentially contribute to future flood extremes especially when subjected to climate change however tcs have been proven to be the primary driving climate mechanism that shows more significant and immediate effects on the probable extremes in the study area the approach presented in this study can provide new insights into future flood management keywords tropical cyclone flood frequency analysis nam river basin climate change 1 introduction flooding is one of the most damaging and deadly natural disasters in the world hallegatte et al 2013 huang et al 2018 in the 20th century alone it has caused 6 8 million deaths and hundreds of billions of dollars worth of damage doocy et al 2013 with the rising global temperature and sea levels flooding events are also expected to increase in frequency and severity in the coming years quesada montano et al 2018 although substantial improvements have been made in flood management for the past years it still requires further enhanced strategies for more appropriate and timely flood risk assessment van wesemael et al 2019 it motivates this study to explore the methods used for regional flood frequency analyses including their extension to future conditions which could help design proper flood prevention protocols and sustainable flood control infrastructure flood risk analyses are often carried out through flood frequency analysis ffa results of ffa serve as bases for assigning appropriate design standards for flood control infrastructure butler et al 2018 fu and butler 2014 determining coverage of flood insurance programs dawdy et al 2012 gilroy and mccuen 2012 and flood zoning botero and francés 2010 for this method the annual high flows recorded over a long period in a river are fitted to a probability distribution to forecast flood frequencies that correspond to various return periods or probabilities fernandes et al 2010 tanaka et al 2017 the fundamental assumption behind conventional ffa is that the collection of high flows is randomly generated suggesting that they are derived from the random superposition of complex processes in the atmosphere and river system bhat et al 2019 however the occurrence of high flows i e flooding is not only derived from random processes rather it is an output of the combination of several contributing factors such as land covers ahn and merwade 2017 gao et al 2016 soil moisture conditions wanders et al 2014 and various atmospheric phenomena e g rainfall snow gilroy and mccuen 2012 in particular the occurrence of extreme flooding events are mainly caused by prolonged or heavy rainfall armstrong et al 2014 li et al 2015 these extreme events are intensified by various large scale patterns in the atmosphere accordingly the information about large scale patterns and their influence on historical flows have been frequently studied to achieve more reliable flooding management strategies e g muis et al 2018 rueda et al 2017 in line with this studies focusing on the underlying processes that cause the occurrence of precipitation have also emerged e g gimeno et al 2012 wang and chen 2008 yong et al 2016 these studies have shown that precipitation occurrences are driven by several synoptic disturbances or climate mechanisms such as monsoon fronts convection from local heating of the surface and other weather and cloud systems like tropical cyclones these diversities suggest that rainfall driven by the diverse climate processes may have different spatiotemporal properties therefore the individual impacts of the climate processes have been considered in rainfall frequency analyses son et al 2017 wright et al 2013 yoon et al 2013 detection of current and future rainfall trends e g kim and jain 2011 knutson et al 2010 and inspection of the spatial and temporal patterns of rainfall kim and jain 2011 wright et al 2014a for instance yoon et al 2013 found that the annual maximum tropical cyclone rainfall have a more critical contribution to rainfall quantiles than convective rainfall after conducting a nonstationary rainfall frequency analysis the discrete consideration of climate mechanisms is also found in studies dealing with long term variabilities of renewable energy bartos et al 2016 mohammadi and goudarzi 2018 and seasonal flood characteristics carter et al 2018 nakamura et al 2013 acknowledging the necessity of identifying climate mechanisms more comprehensive analyses that consider the individual impacts of climate mechanisms in flood production have been encouraged by hirschboeck 1987 since that seminal paper mixed distributions have been used as an alternative for regular probability distributions to estimate flood extremes alila and mtiraoui 2002 barth et al 2019 2017 smith et al 2011 yang et al 2019 another innovation is the utilization of process based ffa to generate synthetic storm scenarios e g cameron et al 1999 wright et al 2014b yu et al 2019 from this method flood frequency curves are constructed through the combination of observations stochastic models of multi scale temporal variability and hydrologic models that take into account all the flood generation processes e g rainfall and soil moisture sivapalan and samuel 2009 results from past studies show that the approach is useful in considering the complex interactions between flood generating climate processes however despite all these innovations the individual impacts of flood causing mechanisms under climate change have received less attention in the literature the individual consideration of climate mechanisms in ffa could be crucial for future projections since some studies have revealed that these flood generating climate processes will be heterogeneously altered due to climate change e g lee et al 2020 xia et al 2017 many studies exploring the characteristics of rainfall driven by various climate processes have projected substantial changes due to the changing atmospheric system e g lackmann 2013 shields and kiehl 2016 wright et al 2015 moreover due to these alterations substantial changes are expected in future hydrologic conditions e g marsooli and lin 2020 salathé jr et al 2014 these studies have utilized climate models to explore the impacts of climate change on future streamflow projections for instance salathé et al 2014 projected future flood risks from atmospheric rivers and orographic precipitation in the pacific northwest river basin through a regional climate model their results showed that the peak flows in the study area will significantly increase by 30 in 2040 2069 while most of the studies presented results that might likely happen in the future some studies have argued that climate models often underestimate or cannot accurately capture future projections associated with climate mechanisms wang et al 2015 other studies also provided criticisms of climate models since they disregard the differences between the characteristics of climate mechanisms during the model construction e g mukundan et al 2019 and shimura et al 2015 another limitation emerges when the changes in climate mechanisms are explored to project future hydrologic conditions using climate models here the behavior of future results entirely depends on the selected climate scenario for the analysis this suggests that the projected future changes are determined by how the scenario is chosen likely leading to under or over estimations since ffa is one of the crucial topics in climate change assessment almasi and soltani 2017 duan et al 2017 there is a need to use an alternative approach that could explore the possible hydrologic changes caused by the changes in the characteristics of the climate mechanisms based on the aforementioned knowledge gaps a climate mechanism based future ffa may be a desirable tool for water resources management this study tries to incorporate the diverse impacts of the climate mechanisms to the conventional ffa through a bottom up approach the bottom up approach allows decision planners to examine various contributions e g projection and temporal trend to identify future climate conditions that could lead to unacceptable performances of water resource systems the approach is attractive since it explores climate vulnerabilities by introducing a wide range of climate stresses beyond climate model projections stephens et al 2018 therefore several bottom up approaches have been developed to assess how different future climate assumptions affect risk decision planning e g culley et al 2016 knighton et al 2017 ray et al 2019 under this approach we develop synthetic climate scenarios driven by the occurrence of climate mechanisms using a weather generator to sum up this study proposes the climate mechanism based bottom up approach using two main climate mechanisms tailored for the study area a flood frequency analysis that acknowledges the heterogeneous impacts of tropical cyclones tcs and non tc rainfall with the consideration of various future climate change scenarios is conducted in the nam river basin south korea the rest of the paper is organized as follows section 2 presents the study area and the data utilized in this study section 3 presents a statistical framework used to quantify the heterogeneous impacts of tc and non tc rainfall with the consideration of the various effects of climate change on future design floods in the area of study a summary of the results is then presented in section 4 followed by conclusions in section 5 2 study area and data lying in the northern hemisphere south korea has four distinct seasons with a climate that is influenced by the northeastern asian and the western pacific ocean jung et al 2002 the country receives two thirds of its annual precipitation during summer june september while water deficits occasionally occur during winter cha et al 2011 since 65 of south korea is mountainous and most of the river reaches are short and have steep slopes water management problems such as flooding and drought are very common especially in the five major rivers located in different parts of the country including han river nakdong river geum river sumjin and yeongsan river lah et al 2015 the nakdong river basin which has the second largest drainage area in south korea passes major cities such as busan and daegu where almost 6 million people reside in total kim and kim 2018 through the years the basin has suffered from hydrological problems such as drought and flooding eum and simonovic 2010 in particular the basin loses an annual average of two million us dollars from the damages inflicted by flooding events oh 2013 the susceptibility of the basin to tc events and the impending impacts of climate change on the magnitude and intensity of tc rainfall might affect the sustainability and the quality of existing hydrological infrastructures in the area with this motivation the nam river basin which is one of the sub basins in the nakdong river basin is selected as our area of study the nam river basin has a geographical coordinate of 35 54 27 n and 127 46 91 e and a drainage area of 2 285 km2 see fig 1 the basin receives as much as 70 80 of the annual total precipitation during summer and the rainfall distribution is mainly based on the basin topography yoon et al 2014 on average the nam river basin receives a higher amount of rainfall which ranges around 1400 mm to 1600 mm per year in contrast to the mean annual precipitation of south korea which is 1245 mm daily climate data recorded in the nam river basin such as precipitation and maximum and minimum temperature are utilized for this study the weather data are collected from the 60 weather stations spread throughout the country by the korea meteorological administration s automated surface observing system asos and are interpolated to derive daily climate data for the study area that spans from january 01 1973 to december 31 2019 for hydrologic modeling daily observed streamflow data is collected from the water resources management information system http wamis go kr the available streamflow data in the study area starts on january 01 2000 and ends on december 31 2019 to determine tc and non tc rainfall days we follow a methodology recently utilized by alcantara and ahn 2020 from their methodology the radius which could effectively define the rainfall induced by tropical cyclones is determined with the aid of the tc best track data recorded from 1973 to 2018 from the regional specialized meteorological center rsmc tokyo typhoon center of the japan meteorological agency jma http jma go jp to be specific they define tc rainfall as any rainfall that occurs within a 500 km radial distance from the tc center to the weather station during the onset of a tc event more details of how to determine tc and non tc rainfall days can be found in the supplementary material see text s1 3 methodologies this study proposes a climate mechanism based approach for flood frequency analysis we note that the approach can be extended to explore many other climate mechanisms e g tropical and extratropical activities but it is only employed for the heterogeneous impacts of tc and non tc events as a case study the proposed approach comprises five components 1 the generation of synthetic climate data while recognizing the differences between the characteristics of tc and non tc rainfall using several statistical methods namely markov chain analysis the k nearest neighbor knn algorithm and the z score based jittering method 2 incorporation of the future effects of climate change on a synthetic climate data 3 streamflow generation using a rainfall runoff model 4 the execution of flood frequency analysis and lastly 5 the investigation of various potential flood frequencies brought by climate change we briefly illustrate the methodology in fig 2 and a detailed description of each part are further offered in the following sections 3 1 synthetic climate data with the identification of tc and non tc induced rainfall here a synthetic daily time series of meteorological variables x x 1 t x 2 t x j t with length j is desired where x j t indicates the jth climate variable at time t t 1 t this climate data generation starts with the occurrence process or the construction of sequences of states for this approach the three state first order markov chain model is employed with states defined as 1 tc st 0 or rainfall events that occurred within a defined distance from the tc center 2 non tc st 1 or rainfall events which are not considered as tc rainfall and lastly and 3 no rain st 2 or historical records which are less than 0 1 mm after defining all the states and assigning arbitrary labels over the historical records the transition matrix t m i i m for each month m is determined each t m i i has a size of 3 3 with each coordinate showing the probability of a state occurring at time t and transitioning to another state at time t 1 these conditional probabilities cp are computed using the following equation 1 c p i i p s t 1 i s t i i i 0 1 2 where i and i are the state of the present and the next day respectively using the transitional matrices time series containing the sequences of three states are generated for n simulation years hence each state in the simulated time series on a given day on an mth month is based on the state on the previous day and the transitional probabilities for that mth month next a k nearest neighbor knn resampling approach is used to populate each month of the simulated period for all meteorological variables x the approach determines k number of historical records that are most similar to the conditioning vector in terms of their characteristics the conditioning vector then adapts the values of the closest neighbor that the algorithm randomly selects for this study the markov modeled state conditions particularly the total number of monthly tc non tc and no rain occurrences per year are compared to the yearly preexisting historical conditions using the euclidean distance formula for each simulated month in a particular year five nearest historical years i e k 5 are extracted and randomly sampled to construct a daily climate sequence x in this resampling process the months of september may are sampled individually while the months of june august are sampled as one continuous climate sequence since transitional rainfall events during these months are very common here transitional rainfall events are defined by the case when the precipitation state on the last day of the previous month has the same state as the first day of the present month based on the methods described above the synthetic data is resampled from the historical records however the approach limits the extrapolation ability of the model to generate unobserved climate sequences outside the range of the existing records nourani and partoviyan 2018 to address this limitation the z score based jittering method is added to the proposed model for the jittering method noises are added to the simulated time series to allow the generation of synthetic climate data beyond the range of the historical records while primary historical properties are maintained the noise for this study is introduced by adding normally distributed random numbers with mean 0 and optimum standard deviation σ to the quantiles of the non zero tc and non tc rainfall the equivalent precipitation amounts of the two adjusted quantiles are then combined to produce a new time series the optimum standard deviation is determined by comparing the extremes of several adjusted time series having different standard deviations against the extremes of the historical record using the normalized root mean square error nrmse 3 2 incorporating changes in synthetic climate data for flood risk assessments it is important to consider the heterogeneous changes of climate mechanisms under the influence of climate change however employing the proposed approach in section 3 1 alone hinders the generation of climate sequences that might transpire beyond the distribution of the historical records because of unforeseen changes in the climate system therefore this study introduces two adjustments through the modification of the transition matrix tm and the application of a quantile mapping approach to incorporate the effects of climate change on the frequency and magnitude of the climate data respectively in this study we have a transition matrix t m i i for mth month with nine coordinates p 00 p 01 p 02 p 03 p 10 p 11 p 12 p 20 p 21 p 22 each containing a conditional probability of c p i i the probability in each coordinate dictates how the climate mechanisms particularly the tc and non tc events interact and transition from one event to another considering climate change changes in the patterns and behavior of these climate mechanisms are expected to affect and alter the frequency of the rainfall that they induce these changes can be directly reflected in the historical transitional probabilities to factor in these changes a transition matrix with adjusted transitional probabilities t m i i is introduced in the system as the following equation 2 t m i i t m i i a m i i here a m i i is the additive factor matrix formulated to control how the occurrence of the climate mechanisms are affected by climate change t m i i is the original transition matrix and t m i i is the newly adjusted transition matrix when the effects of climate change are considered through the utilization of the additive factor matrix many possible changes in the original transition matrix can be introduced as a simple example to show how to utilize our proposed model occurrences of tc and non tc events are treated as independent from each other in this study we also note that numerous heterogeneous changes of climate mechanisms can be considered beyond this example this analysis intends to explore the primary driving climate mechanism that has the potential to cause more extreme flooding events given similar future climate conditions therefore we assume that the change in the frequency of tc rainfall does not affect the characteristics of non tc rainfall and vice versa when the future effects of climate change are considered in line with this principle the additive factor matrix for tc events a m i i t c only carries adjustment factors λ tc on coordinates involving transitions between tc and no rain events p 00 p 02 p 20 p 22 similarly non tc events are adjusted using the additive factor matrix a m i i n o n t c containing adjustments λ nontc aimed to modify transitional probabilities for non tc and no rain events p 11 p 12 p 21 p 22 since an infinite range of possible adjustment factors can be exercised the adjustment factors for the two conditions are limited and decided based on the yearly historical transition matrices to ensure that the introduced scenarios are realistically feasible presented in section 4 4 this limitation will be further discussed in section 5 the two matrices tm tc and tm nontc are employed to determine how the independent and combined changes in the frequency of the climate mechanisms might affect the behavior of the climate data a quantile mapping approach is used to account for the changes in the magnitudes of the climate mechanisms this is delivered by quantifying the changes in the distribution of the daily precipitation given a generated time series of precipitation data for n number of years the non zero tc and non tc daily precipitation are extracted to represent two individual data sets here we assume that each set of precipitation can be fitted to a theoretical cumulative distribution function cdf with a parameter set p to incorporate the effects of climate change incremental changes can be applied in the mean standard deviation or extremes of the original sequence which are reflected in the new parameter set p the new parameters are then used to achieve the target cumulative distribution function then the quantiles of the target distribution are matched to the quantiles of the original distribution to determine the new daily values in this study the non zero daily tc and non tc precipitation data are individually fitted to a gamma distribution with shape k and scale θ parameters from the method of moments hansen 1982 the parameters are represented through their relationships with the mean μ and the standard deviation δ of the two time series here the scale parameter could take the form of θ δ 2 μ while the shape parameter can be k δ 2 μ 2 in this study the δ is selected as the control variable while the μ is the independent variable which is incrementally altered after modifying the tc and non tc precipitation individually the two altered data sets are then incorporated to represent one data set we also note that changes in other meteorological variables can also be accounted for using the quantile mapping approach for this study temperature changes are manipulated using a gaussian distribution with mean μ and variance δ2 parameters a summary of the adjustment factors and conditions is presented in table 1 3 3 hydrologic modeling after embedding the probable effects of climate change to the driving climate mechanisms the generated synthetic climate data are then converted into runoff this is realized using a parsimonious daily lumped conceptual hydrologic model the simhyd model fig 4 chiew et al 2002 this model is a simplified version of the hydrolog porter and mcmahon 1976 and modhydrolog chiew and mcmahon 1994 models compared to the models mentioned above which use over 15 parameters the simhyd model has seven parameters and only requires daily precipitation and potential evapotranspiration as the input data yu and zhu 2015 a routing model developed based on muskinghum mccarthy 1938 is also included to improve the model performance by providing an additional store to replicate routing of surface flows we note that a lumped model is utilized in the study area since it does not have any significant spatial variabilities however distributed models are more proper in areas where topography plays a significant role the water movement in simhyd starts with the rainfall filling the interception store which is emptied daily by evaporation the excess rainfall that did not evaporate is introduced to an infiltration function which defines the infiltration capacity of the catchment any rainfall which goes above the infiltration capacity then becomes the infiltration excess runoff meanwhile the infiltrating moisture is subjected to a soil moisture function which redirects the water to the interflow stream groundwater store and soil moisture store the simhyd model finally collects the total runoff from different sources which include infiltration excess runoff interflow and baseflow and an additional surface flow which is routed through a storage the overall process of simhyd model with routing is driven by nine parameters including interception store capacity ω1 maximum infiltration loss ω2 infiltration loss exponent ω3 soil moisture store capacity ω4 constant of proportionality in interflow equation ω5 constant of proportionality in groundwater recharge equation ω6 and baseflow linear recession parameter ω7 and storage time constant ω8 and proportionality constant ω9 of a routing model a summary showing the ranges of the probable values for each parameter is shown in table 2 while the model structure of simhyd illustrating the water movement can be seen in fig 3 for more details a theoretical description of simhyd can be found in yu and zhu 2015 and zhang et al 2013 this study also employs the priestley taylor equation priestley and taylor 1972 to estimate the daily potential evapotranspiration pet the equation requires the daily minimum and maximum temperatures days of the year and the latitude of the basin as its input also since it is a closed water balance model the model computes the actual evapotranspiration aet by getting the difference between the rainfall input data and runoff all parameters of the simhyd model are calibrated against the observed streamflow data for the period of january 01 2009 to december 21 2019 11 years by using a bayesian framework to explore parametric uncertainties as an objective function a weighted combination of nash sutcliffe efficiency nse nash and sutcliffe 1970 and a logarithmic function of bias viney 2009 is adopted as follows 3 m a x i m i z e 1 t 1 t q t s i m q t o b s 2 t 1 t q t o b s q t o b s 2 5 ln 1 t 1 t q t s i m q t o b s t 1 t q t o b s 2 5 where q t s i m and q t o b s are the simulated and observed daily streamflow at time t respectively the additional bias constraint in the objective function prevents the unconstrained least square measures such as nse and kling gupta efficiency kge gupta et al 2009 to ignore high biases in events when there are high flow variabilities viney et al 2009 after calibration the accuracies of the computed parameter sets are then validated with the observed streamflow from the period of january 01 2000 to december 31 2008 9 years for the bayesian calibration the dreams markov chain monte carlo algorithm laloy and vrugt 2012 is used to explore the posterior distributions of all parameters three chains are employed for each parameter while convergence is confirmed using the gelman and rubin factor gelman et al 1992 and trace plots of the chains 3 4 execution of flood frequency analysis the generated runoff data is fitted into a probability distribution to estimate the frequency of their occurrence or return period to do so high flows are firstly extracted using the peak over threshold pot method in contrast to the usual annual maximum series ams wherein only the maximum data is extracted per year li et al 2015 the pot method allows the extraction of various extreme flows or floods above a certain threshold the increase in the number of high flows reduces the inherent uncertainties leading to more reliable flood frequency dodangeh et al 2019 here the generalized pareto distribution gpd is employed the distribution has been widely utilized to effectively model floods and to estimate extreme quantiles together with the pot method e g bezak et al 2014 gharib et al 2017 the cdf of the gpd is as follows 4 f q 1 1 ξ q η ζ 1 ξ ξ 0 1 exp q η ζ ξ 0 where q represents the high flow value mm day ξ as the shape parameter and ζ as the scale parameter on the other hand η represents the threshold to define the high flows for this study the high flows are considered as any rainfall that exceeds the 80th percentile of the runoff which is generated with no climate change interventions although a higher percentile is more desirable when determining extremes the 80th percentile is chosen in this study due to relatively low number of tc samples 3 5 identifying potential flood frequency changes under climate change finally future flood frequencies are identified using the bottom up approach brown et al 2012 2011 the main idea for the bottom up approach is that a flooding response map can be developed based on a wide range of plausible future climate conditions in the context of our flood frequency application the frequency analysis is executed by systematically controlling λ and p individually grounded by their plausible future values to explore the possible flood frequencies corresponding to a certain return period this approach returns a response surface that illustrates the possible risks brought by the changes in the climate mechanisms note that while the original bottom up approach explores changes in climate conditions e g changes in temperature the approach is extended to consider the heterogeneous climate mechanisms for this study by comparing diverse changes in λ and p we can map out how the different aspects of projected future hydrologic processes influence flooding metrics relevant for decision making and eventually assist the identification of the dominant climate mechanism which is crucial in future water resources management 4 results 4 1 characterization of tc and non tc rainfall at first the characteristics of rainfall associated with the dominant mechanisms in the study area are examined we note that the rainfall utilized in this analysis is derived by averaging all the available rainfall data in the study area here we present the cumulative distribution function cdf of tc and non tc induced daily fig 4 a and annual maximum precipitation fig 4b the distribution of the non tc precipitation is observed to have a very long tail skewed to the right illustrating that most non tc events bring relatively small amounts of precipitation consistently extreme values i e more than 100 mm are quite infrequent meanwhile the cdf of daily tc precipitation shows a curve emphasizing the frequent occurrence of extreme rainfall compared to non tcs higher ranges starting from the 80th percentile of seasonal tcs are composed of extreme rainfall events inspecting the behavior of their individual annual maxima shows that the 40th percentile of the non tc rainfall 81 91 mm is much higher than that of the tc rainfall 70 51 mm this behavior can be attributed to the annual occurrence of tcs although tc events occurred 140 times in 47 years records reveal that they had been absent for 4 years meanwhile subjecting the two sets of distributions to a kolmogorov smirnov test show similar results both sets derive p values lesser than 0 001 indicating that the distributions of the daily and annual maximum tc precipitation are significantly different from non tcs the results show that rainfall events generated by the climate mechanisms are indeed heterogeneous indicating the need to acknowledge their differences when conducting hydrological analyses 4 2 model evaluations models dealing with the generation of meteorological variables corresponding stream flows and frequencies based on high flows are utilized in this study these include a proposed model which is employed to produce synthetic climate data conditioned based on their driving climate mechanisms a hydrologic model to generate runoff data and a probability distribution for flood frequency analysis the evaluation of the performances of these models is vital to ensure realistic reproductions as mentioned in section 3 3 the hydrologic model utilized in this study requires three meteorological variables precipitation and maximum and minimum temperatures as its input data these meteorological variables are produced using the proposed model for climate simulation presented in fig 5 are the first to fourth order moments of the average daily precipitation data aggregated per 5 days i e pentad we also provide the results for the maximum temperature and minimum temperature in figures s1 and s2 these data are derived from generating a time series of meteorological variables with a length of 47 years similar to the length of the historical data for 50 separate times without imposing changes from λ and p as presented the reproductions can properly capture most historical statistics presented as black points although there are instances of biases e g skewness and kurtosis all historical statistics are included in the 95 confidence interval of the generated data suggesting that the model can effectively reproduce the statistics of all historical variables however we also note that these biases could potentially lead to underestimations especially when representing extreme floods despite the promising results from the processes biases in representing rainfall extremes are still found to limit their efficiencies see fig 6 b hence in this study the jittering method is adopted not only to produce unobserved climate sequences but also to mitigate the abovementioned limitation the method includes the determination of an optimal standard deviation σ for the normally distributed random numbers added as jitters noises in the generated climate data each generated time series from the 50 ensembles utilized before this analysis is subjected to noises containing various standard deviations σ ranging from 0 1 to 0 5 the average extremes derived from each standard deviation are then compared to the historical rainfall extremes see fig 6a using the normalized root mean squared error nrmse compared to other conditions the extremes computed using the σ 0 29 derived values closest to the historical data to confirm this the values of the return periods computed using the optimal standard deviation are compared to the extremes when the z score based jittering method is not employed all the return periods are captured better when noises are introduced in the system showing the additional necessity to incorporate the z score based jittering method in the proposed model fig 7 compares the hydrographs of the observed and simulated daily runoff data for the calibration and validation of the simhyd model results show that low and high flows during the calibration period are within the 95 credible interval of the generated runoff data although there are cases of over and under estimation most of the simulated cases are still in close agreement with the observed data the nash sutcliffe efficiency nse value for the calibration period is 0 65 while the validated data gave an efficiency value of 0 78 suggesting that the rainfall runoff model performs well and provide an acceptable estimation of the historical runoff data based on the performance criteria utilized in past studies li and zhang 2016 yu and zhu 2015 however further evaluation of the performance of the model shows that it cannot properly capture the annual maximum peaks and peaks over threshold of the observed data for the validation period see figure s4 hence for this study the expected changes in future extremes are based on their proportion to the undisturbed values see fig 10 next following the pot method simulated non tc and tc high flows are extracted and compared to the cdfs of the historical non tc and tc high flows respectively fig 8 based on their median quantile estimates both distributions illustrate some overestimations however all the historical records are within the 95 credible intervals of both simulations particularly for higher quantiles e g 95 which is more important when estimating flood frequencies both simulated flows are well matched with the historical ones based on these analyses we confirm that the gpd is acceptable and is used for further analysis 4 3 flow simulations with the heterogeneous changes in the characteristics of climate mechanisms the annual streamflow after employing λ tc is plotted against the result acquired from employing λ nontc fig 9 a also the number of tcs per year with λ tc occupies the upper half and tcs of λ nontc covers the lower half of the figure the λ is introduced to the t m i i of months having both tc and non tc events to account for the impending impacts of climate change to the frequency of the climate mechanisms after assuming an increase of 3 in tc frequencies flows averaging up to more than 820 mm are recorded for most of the years despite having similar lengths the result is significantly greater than the historical average approximately 596 mm also compared to the historical number of tcs that pass through the area annually four tcs per year an increase of 3 in the probability of tcs brings up to 11 tcs in a year on the other hand the 3 increase in the occurrence of non tcs results in an annual average of 755 mm this is slightly lower than the result when tcs are increased also increasing non tc events causes a decrease of tcs in some years similarly 50 ensembles of time series n 47 are subjected to adjustments fig 9b here the average streamflow as well as the average number of tcs per ensemble are presented for most simulations stream flows upon the employment of λ tc are found to be greater than the stream flows when non tcs are adjusted however the averages from the two cases both exceed the mean historical streamflow indicating the sensitivity of all the climate mechanisms to climate change both cases also show similar rainfall days however simulations having an average of 5 tcs per year have a higher streamflow average compared to their counterparts the result implies that more extreme rainfall events occurred in tc altered simulations this hypothesis is explored in the following section 4 4 flood frequency analysis with the consideration of the heterogeneous impacts of tc and non tc events based on various climate change scenarios finally the plausible extreme flood projections are examined through the changes in the frequency and magnitude of the climate mechanisms fig 10 a fig 10b shows the plausible 20 year extremes when the frequencies magnitudes of the tc and non tc induced precipitation are changed in fig 10a we employ the ratio between the streamflow for a certain combination of λ nontc and λ tc and the undisturbed condition λ nontc 0 and λ tc 0 for easy interpretation a similar pattern is presented in fig 10b after employing p first visual assessment readily shows an asymmetric relationship between the changes in tc and non tc frequencies wherein drastic changes seem to be more prevalent when tcs are adjusted the left bottom corner shows the 20 year extreme values when only the non tcs tcs are altered increasing tcs by less than 1 5 provides an immediate shift from the undisturbed extreme in contrast with its counterpart also imposing both changes on the frequencies of climate mechanisms results in a significant increase in the probable values of the design flood to be specific utilizing both the maximum adjustment factors for each climate mechanism yields a 7 increase in the 20 year extreme similar results are also observed for other return periods see fig s3 then we evaluate the extremes due to the changes in the magnitudes of the climate mechanisms while assuming that their frequencies remain unchanged through a quantile mapping approach in this analysis p derived from changing each µ of the available non zero tc and non tc precipitation is introduced in the system here to account for the changes in the distribution of the minimum and maximum temperatures a quantile mapping approach is also used through the modification of the parameters for the gaussian distribution however results have shown that the application of alterations does not show any significant difference in the distribution of the two variables not shown accordingly the change in the magnitude of the climate mechanisms is mainly illustrated by the changes in the magnitude of precipitation as opposed to the result in changing the frequencies the change in the magnitude of the climate mechanisms displays several uniform behaviors from the two conditions see fig 10b for instance increasing the magnitudes of tcs and non tcs individually by 10 and 20 exhibits an almost similar degree of upward shift from the undisturbed extreme also a reduction in the magnitude of non tcs does not show any significant change from the undisturbed extreme which is similar to the change in tcs similar to the result from the preceding analysis imposing both changes on the magnitudes of the two climate mechanisms shows significant changes from the undisturbed extreme overall although both mechanisms provide significant impacts on flood frequency projections the analysis has shown that tc is the primary driving climate mechanism that has the potential to cause more extreme flooding events results indicate that tc induced meteorological variables overpower non tcs in posing flood frequency risks in the area changes in its magnitude and frequency show significant and immediate effects to the probable extremes for various return periods traditionally flood frequencies are conducted without the distinction between the driving climate mechanisms however considering their differences could give decision makers more information to impose more appropriate actions in cases wherein the two dominant processes behave differently 5 summary and conclusions this study demonstrates a climate mechanism based flood frequency analysis approach accommodating a direct linkage between risk metrics for decision making and the dominant causal climate mechanisms driving riverine flooding we believe that the approach presented in this study is a crucial step towards a novel future planning tool based on an understanding of climate flood linkages merz et al 2014 we apply this approach to the nam river basin south korea to support decision makers in identifying two dominant mechanisms responsible for flood frequencies employing the proposed approach reveals that each climate mechanism has distinctive characteristics even though both mechanisms the tc and non tcs play significant roles in high flow events results from tc altered simulations demonstrate that changes in the characteristics of tcs could lead to greater flood frequency risks in the study area compared to non tcs in the future decision makers may require certain measures that would emphasize the effects of tcs to provide more accurate plans to ease flooding problems for instance imposing a climate mechanism based dam release operation policy may significantly reduce flooding and drought problems from this the amount of water to be released will depend heavily on the specific climate mechanism driving the extreme rainfall event to avoid under or over discharging of water albeit the promising results several relevant limitations need to be further addressed which could be suitable for future studies for instance the approach may be limited in regions with insufficient historical records although we have tried to mitigate this limitation by adopting a jittering method the reproductions are ultimately rooted in historical scenarios insufficient historical records make it difficult to precisely measure the performance of the model miao et al 2016 also our case study is illustrated by a watershed in which a snow accumulation melting process is insignificant accordingly the changes in the distribution of temperatures may not provide significant effects on streamflow generations however as described by o gorman 2014 the behavior of snowfall due to the warming climate may be crucial in other areas hence it would also be valuable to explicitly address the contribution of temperature changes in projecting flood frequencies the most significant limitation in this study is linked to the adjustment factors assigned to reflect the future impacts of climate change in the climate mechanisms based on the proposed framework the analyses are bounded by the changes in the historical events during the period 1973 2019 since various external factors contribute to future deep climate change future scenarios could be substantially altered from the historical records hence for the current formation we have no choice but to consult decision makers to assign proper and appropriate values for the adjustment factors to derive more realistic and reliable results to improve the current framework an analysis that is not constricted by historical events should be developed this can be achieved by still adapting the bottom up approach since it allows the consideration of boundless climate stressors to identify the vulnerabilities in a system steinschneider and brown 2013 for future studies it would be beneficial to consider more driving forces that cause the occurrence of extreme hydrological events for instance atmospheric rivers ars have been gaining attention in the field of hydrology since it has a distinct feature from extratropical cyclones and has its own development and life cycle mundhenk et al 2016 studies have shown that the presence of ars contributes to the occurrence of more intense rainfall and flooding events han et al 2020 sodemann and stohl 2013 meanwhile an in depth analysis of the characteristics and classification of heavy precipitation systems can also be executed to improve the evaluation of flood risks in an area on the other hand although tcs are more dominant in this study other areas might present a different result due to the variation in the spatial distribution of the mechanisms raising the need to apply the proposed methodology to other watersheds also the effectiveness of our proposed approach in supporting existing research particularly in the context of predicting future changes can be explored for instance chand et al 2017 reported that the occurrence of tcs in the western and central north pacific could increase from an average of 5 to 7 tcs per year in 2070 2100 to support their claim their projected number of tcs can be used as the base in determining the appropriate adjustment factors to manipulate the transition matrices for the study area we can then evaluate the number of predicted tcs in a year see fig 9 to explore whether the events lead to unacceptable changes in hydrologic projections once the effectiveness of the model in representing future changes in tcs is confirmed the model can then be used to evaluate the changes in flood extremes in the study area which is another promising way of utilizing our proposed model credit authorship contribution statement angelika l alcantara formal analysis investigation methodology writing original draft kuk hyun ahn conceptualization supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the national research foundation of korea nrf grant funded by the korean government msit no 2019r1c1c1002438 
282,technologies that increase the accuracy of soil moisture monitoring such as in situ sensors have been proposed as a key solution for increasing agricultural water productivity however quantifying how uncertainty in soil moisture estimates lead to irrigation inefficiencies or economic losses has not been explicitly studied we develop a framework that combines a crop simulation model with a rule based irrigation decision making algorithm to assess the impact of soil moisture uncertainty on irrigation use and farm profits we apply this modelling framework to a case study of irrigated maize production in nebraska united states a region where improvements in agricultural water productivity are at the forefront of water policy debates we consider two main sources of uncertainty that result in a divergence between the farmers perception of soil water content and the true water status namely errors in the knowledge of soil texture and measurement of daily soil water flux inflows and outflows even for very large errors in both soil texture and water flux measurements impacts on water use and profits were marginal 11 ha mm increase and 27 ha 1 decrease respectively in contrast farmers choice of irrigation strategy had a much larger impact on water use and profits than uncertainty in soil moisture information used to implement that strategy our findings show that near optimal irrigation decisions can be made without perfect soil moisture information this conclusion suggests that providing farmers with improved irrigation scheduling recommendations utilizing crop water models and optimization techniques would have a larger impact on water use efficiency than simply providing farmers with technologies to more accurately monitor soil moisture conditions keywords aquacrop soil moisture uncertainty irrigation decisions irrigation optimization measurement uncertainty 1 introduction increases in irrigation water demand driven by climate change and population growth are expected to exacerbate water scarcity and conflict over limited freshwater resources in many regions around the world e g molden 2013 to manage these challenges there is a need to identify ways to improve the productivity of agricultural water use commonly referred to as generating more crop per drop one key pathway for raising agricultural water productivity is through improvements to irrigation scheduling which enables farmers to target limited water supply to when the crop is most sensitive to water stress and thus maximize yield returns to limited freshwater inputs morison et al 2008 evans and sadler 2008 adeyemi et al 2017 the majority of the developments in irrigation scheduling come under the label of precision irrigation which broadly encapsulates solutions that aim to vary the temporal and spatial distribution of irrigation to match crop water demands adeyemi et al 2017 associated technologies to support implementation of these strategies include sensors to monitor soil water content or crop growth jones 2004 dobriyal et al 2012 ihuoma and madramootoo 2017 field mapping that highlights heterogeneities in soil texture and other properties daccache et al 2015 and decision support tools that aid in the timing of irrigation events mccarthy et al 2010 the common goal that links these methods is the precise monitoring and management of soil water content and crop water needs currently the uptake of precision irrigation methods and technologies remains low with most farmers even in developed agricultural systems such as the united states relying on proxies such as the visual condition of the crop and soil as well as simple water balance calculations to determine when to irrigate fig 1 much like soil moisture sensors these traditional techniques aim to monitor and manage soil water content in the crop root zone and hence the amount of water available for crop uptake the difference between farmers with and without soil moisture sensors therefore is the accuracy of this monitoring along with the associated rules and heuristics by which this information is used to make irrigation decisions sources of uncertainty in soil moisture monitoring and estimation are similar for both traditional and precision irrigation methods one important source of uncertainty is heterogeneity in soil textural composition which affects estimates of current and potential soil water storage both laterally and vertically within a field nielsen et al 1973 brocca et al 2007 feki et al 2018 heterogeneity in soil texture means that point measurements of soil moisture status are only valid for a specific location and depth potentially leading to errors when this information is used in water balance calculations across large areas e g field or farm additional uncertainty in soil moisture estimation also results from errors in climatological rainfall evapotranspiration etc and irrigation data characterising inflows and outflows from soil water storage differences in infiltration rates due to differences in soil texture and topography as well as large distances between the measurement instruments and cropping area are major contributors to this uncertainty chaubey et al 1999 moreover the accuracy of evapotranspiration estimates are greatly affected by the amount of site specific information used in its calculation sentelhas et al 2010 including in the calculations of reference and actual evapotranspiration allen et al 1998 the result of either soil texture or water flux errors is that a farmer may miscalculate the amount of water stored in the soil on any given day leading to inaccurate estimates of when or how much irrigation should be applied as a result a farmer may apply insufficient or excessive amounts of water with resulting negative impacts on crop yields and or farm profits precision irrigation research has focused on developing tools and techniques to reduce the magnitude of soil moisture error soulis et al 2015 adeyemi et al 2017 kukal et al 2019 alongside this when farmers invest in more accurate soil water monitoring technology such as sensors or weather stations they will often also receive advice about the best trigger levels and irrigation management strategies to maximize yields profits or water efficiency unl 2019 the relative contribution of improved scheduling rules versus improved soil moisture information to farmers irrigation water use practices has not been explicitly studied however this distinction has important implications for farmers and water managers seeking to understand which interventions and technologies will be most effective for delivering desired improvements in the productivity and value of irrigation water use in response to these gaps in the literature this article presents a framework for studying the impact of varying levels of soil moisture uncertainty on irrigation water use decisions and farm profits the framework consists of an optimization module to generate irrigation strategies a crop growth model to assess impacts of these irrigation strategies on water use and crop yields and a mechanism for applying variable soil moisture measurement errors within model simulations we apply this framework a case study of irrigated maize production in nebraska united states using the aquacrop os crop water model to assess how assumptions about irrigation management practices and the accuracy of soil moisture information on which these strategies are implemented affect irrigation decisions and farm profits our findings provide insights on the response of irrigation water productivity to soil moisture measurement uncertainty showing that the impact of inaccurate soil water monitoring on crop yields profits and water use efficiency is low relative to the impacts of using sub optimal irrigation scheduling heuristics we discuss the implications of these results for local policy makers industry and producers and provide recommendations for how our results can be used to make improvements in agricultural water productivity at field to catchment scales 2 methods in this section we describe our proposed framework for modelling farmer irrigation decision making under varying levels of soil moisture measurement uncertainty and for assessing the resulting impacts of this measurement uncertainty on water use crop yields and economic returns we first present our generalised assessment framework in three main subsections determination of farmers irrigation scheduling rules section 2 1 assessment of the performance of these strategies under different sources and magnitudes of measurement error sections 2 2 and the incorporation of a feedback mechanism to represent realistic farmer behavior section 2 3 we then describe the specific case study application of this framework including details of model parameters and choice of crop growth model optimization algorithm crop type and location used as an illustrative example in this study section 2 4 2 1 determining farmers rules in scheduling irrigation despite a large variation in the methods used by farmers to schedule irrigation fig 1 the goal of most methods is to maintain soil moisture levels above a pre specified threshold chosen to meet crop water needs and balance trade offs between costs and yield benefits of water use in line with these assumptions and past research linker et al 2016 foster and brozović 2018 linker et al 2018 we assume that a farmer s irrigation decision making can be characterized by the choice of four soil moisture thresholds throughout the growing season one for each major crop growth stage emergence early season canopy development mid season crop growth and yield formation late season canopy senescence these four soil moisture thresholds determine the allowable root zone soil moisture content expressed as a percentage of total soil water holding capacity which water contents must fall below before irrigation is triggered at the point irrigation is triggered we assume that water will be applied to refill the soil profile back to field capacity subject to a maximum application rate which is determined by the type of irrigation system and water source used by a farmer the choice of soil moisture thresholds which we refer to as the irrigation strategy can have a major impact on water use and crop growth setting these thresholds too high i e irrigating at low levels of soil moisture depletion will result in over irrigation whereas setting thresholds too low i e irrigating only at high levels of soil moisture depletion will result in under irrigation and thus production losses in line with past research showing that farmers input use decisions may be influenced by their risk preferences abdulkadri 2003 menapace et al 2013 we define the optimal set of soil moisture thresholds as those that maximize a farmers certainty equivalent ce the ce is defined as the amount of economic return that would have to be guaranteed in order for an individual to be indifferent towards a higher but less certain return the ce has been used in previous agro economic models of farmer input use decision making lehmann et al 2013 foster et al 2014 in the context of irrigation optimization the ce for a given irrigation strategy s is defined as 1 c e s e p s 0 5 r v a r p s e p s where strategy s t 1 t 2 t 3 t 4 is made up of four soil moisture thresholds the expected profits e p s and variance var p s will be calculated over a given set of seasons that characterise farmers expectations of potential inter annual weather variability and r is the risk coefficient representing farmer s risk preferences if a risk averse farmer r 0 wishes to increase the ce they could either increase the expected average profits or reduce the year to year variability in profits eq 1 a risk neutral farmer r 0 will instead only aim to maximise expected average seasonal profits for a given irrigation strategy s the seasonal profit p s is calculated as 2 p s m y s c i s f where m is a constant crop market price per tonne y s is crop yield tonne per ha c is a constant irrigation cost per ha mm i s is total seasonal irrigation applied ha mm and f is fixed production costs per ha finding a set of soil moisture thresholds that maximize ce is a difficult task due to large number of potential soil moisture threshold combinations and interdependencies between the choice of threshold for each individual growth stage for example irrigating more in the early part of the season will affect the optimal irrigation threshold for the rest of the season as a result the search space of possible threshold combinations and their resulting outcomes is non linear and discontinuous for these types of search spaces swarm intelligence algorithms have become popular in recent decades mavrovouniotis et al 2017 and have been used previously for the purpose of optimal irrigation planning and management de paly and zell 2009 noory et al 2012 these algorithms operate on the principle that a group of agents scattered across the search space can search locally for solutions while communicating with the other agents this combination ensures that the solutions found are close to the global optimum for this analysis we determine farmers optimal i e ce maximizing soil moisture thresholds using the particle swarm optimization algorithm eberhart and kennedy 1995 freitas et al 2020 although in principle any optimization method could be used within our framework this method starts by initializing a population of particles i e possible soil moisture threshold combinations across the search space each particle is then evaluated and updated using the best combinations found by both the particle and the population as a whole this evaluate update process repeats until a stopping criteria has been met mathworks n d particle swarm optimization has been successfully applied in a diverse array of research areas such as control problems biomedical research robotics and distribution networks poli 2008 2 2 quantifying the impact of uncertainty on irrigation decisions and profits as shown in fig 1 and discussed earlier many farmers do not measure soil moisture directly using sensors instead most rely on proxy measures of soil water content such as visual inspection of the soil and simplified checkbook style water balance models each of these approaches carry uncertainties and errors in the measurement of soil moisture which may influence the performance of selected irrigation scheduling strategies in this study we focus on two main sources of error 1 the specification of soil textural characteristics that cause a persistent seasonal bias to water balance estimation and 2 measurements of daily precipitation irrigation and evapotranspiration fluxes that cause daily unbiased errors in soil moisture content our analysis does not consider structural model errors such as simplifications in the water balance equations inherent in traditional soil moisture monitoring approaches used by farmers and our analysis assumes that the underlying soil water balance model is a perfect model of the chosen system focusing on these two sources of error allows us to examine the effects of measurement uncertainty on farmers irrigation decision making the framework created to study the effect of soil moisture uncertainty consists of two identical crop model simulations running in parallel simulation 1 represents the farmers expectations about soil moisture conditions which will be impacted by any errors in soil texture and water flux measurements the sources of uncertainty considered are errors in soil texture inputs added at the beginning of each season and water flux inputs added on each day of the simulation simulation 2 represents the true status of soil moisture conditions based on the actual soil textural properties and water fluxes irrigation decisions are determined based on the soil moisture content given by simulation 1 as well as the farmers ce maximizing irrigation strategy that determines the level of soil moisture at which they intend to trigger irrigation irrigation events estimated in simulation 1 are then applied in simulation 2 to determine resulting impacts on water use and crop yields under perfect information zero errors in soil texture and water flux the farmers perception of soil moisture conditions match the true state and both simulations are identical such that the farmer is able to implement perfectly the ce maximizing irrigation strategy when the magnitudes of soil texture errors water flux errors or both are greater than zero the difference between the farmer s expectations of soil moisture conditions and reality diverge this difference potentially results in sub optimal irrigation scheduling with negative impacts on crop yields or profits for example if the farmer triggers irrigation at a lower level of soil moisture than the intended ce maximizing threshold i e because their estimated soil moisture levels in simulation 1 over estimate true soil moisture levels given in simulation 2 then the crop may experience unintended water stress and a reduction in yields alternatively if the farmer triggers irrigation at a higher soil moisture level than the intended ce maximizing threshold i e because their estimated soil moisture levels in simulation 1 under estimate true soil moisture levels given in simulation 2 then irrigation water may be wasted and profits reduced two examples of how the added soil moisture uncertainty causes the two simulations to diverge are shown in fig 2 where percentage errors drawn from a normal distribution with zero mean and a standard deviation of 30 have been added to the soil texture and daily water flux inputs in simulation 1 the soil texture errors result in seasonal biases towards either over or under estimation of drainage and therefore soil moisture storage whereas the water flux error results in random daily errors in the farmer s estimate of the true daily change in water content when this calculated soil moisture content simulation 1 drops below the pre defined threshold irrigation is triggered this irrigation depth is applied to both fields regardless of whether the true moisture content simulation 2 has crossed the intended irrigation threshold fig 2a shows an example of irrigation being triggered several days after the true water content in simulation 2 had dropped below the threshold resulting in water stress and yield losses in the alternative case irrigation may be triggered too early causing profit losses from excess water application and potentially leading to crop damage from temporary waterlogging fig 2b 2 3 system feedback when incorporating error in farmers estimation of soil moisture content the potential exists for unrealistic irrigation behavior to occur during simulations that may overstate the true effects of measurement error on farmer irrigation decision making crop yields and profits in reality for example the calculated moisture content simulation 1 may recommend no irrigation despite the true water content simulation 2 being sufficiently low that the crop would be showing visible signs of water stress e g wilting or early senescence alternatively the calculated water content may recommend irrigation despite the true soil profile being visibly saturated or water logged in these instances a farmer would likely adjust their irrigation application in response to these clear visual feedbacks from the true field which would be readily observable through day to day crop management activities even if the farmer was no longer personally measuring the soil moisture content in the field this assumption is further supported by the fact that visible inspection of the crop and soil are by far the most common information sources when deciding to schedule irrigation fig 1 to capture these behavioral feedbacks an additional rule is added to our parallel simulation approach to override the daily irrigation recommendation from the farmers calculation simulation 1 this override occurs if on the previous day the soil moisture conditions in the true field simulation 2 were either i low enough to trigger early canopy senescence which is an indication of severe crop water stress or ii higher than the soil s true field capacity which is indicative of visibly moist soil conditions in these instances an irrigation event within our assessment framework would be triggered or blocked respectively the altered irrigation depth is applied to both fields simulations 1 and 2 and the parallel simulations subsequently continue as described in section 2 2 the added feedbacks have the effect of stopping excessive irrigation in the early season while also preventing unrealistically large water stress in the late season figure s1 we hypothesize that incorporating these behavioral feedbacks will minimize the effects of large measurement errors leading to less importance being placed on the quality of improved soil moisture information a hypothesis we test in subsequent sections as part of our illustrative case study application 2 4 illustrative application to illustrate our modelling framework we apply the methods described in sections 2 2 and 2 3 to the example of centre pivot irrigated maize production in nebraska united states maize is the dominant irrigated crop type in nebraska which has the largest number of irrigated acres by state in the united states usda nass 2018 irrigation water is sourced primarily from the underlying high plains aquifer and improving productivity of water use is a key priority for policymakers to address growing issues of aquifer depletion mcguire 2017 streamflow depletion szilagyi 2000 and degradation of freshwater ecosystems palazzo and brozović 2014 perkin et al 2019 financial incentives to farmers to encourage adoption of soil moisture sensors are seen as a key mechanism to improve water productivity and irrigation efficiencies cropwatch 2019b making maize production in nebraska a logical choice for examining the impact of soil moisture uncertainty on water use and farm profits as the basis for soil water balance and crop growth simulations in our illustrative analysis we use the crop water model aquacrop os foster et al 2017 aquacrop os is an open source version of the original aquacrop raes et al 2009 developed by the food and agriculture organization of the united nations to simulate crop response to water stress the model has been successfully applied and calibrated for maize production in a wide variety of environments hsiao et al 2009 abedinpour et al 2012 including in our study area in the central united states foster et al 2015 and within the united states and globally heng et al 2009 foster et al 2015 sandhu and irmak 2019 aquacrop os captures the impacts of water stress on multiple aspects of crop development expansion and senescence transpiration and yield formation through a series of water stress thresholds and coefficients that have been calibrated for maize in our study areas as part of previous studies forster et al 2015 these determine the level of soil moisture depletion at which each process becomes constrained by water stress along with how resulting deficiencies in crop development or transpiration impact on end of season biomass production and crop yields further details about the simulation of water stress on crop growth and yields in aquacrop are given in steduto et al 2009 and raes et al 2009 the first step in our analysis is to identify ce maximizing soil moisture thresholds using particle swarm optimization as described in section 2 1 given the non linear and discontinuous nature of the search space of possible thresholds the same optimization procedure did not always produce the same set of thresholds multiple optimization repetitions were therefore required to capture the full distribution of ce maximizing strategies the number of repetitions was set to 50 after observing that the variance between optimal threshold combinations stabilized for more repetitions than 50 figure s2 each of these 50 sets of thresholds can now be thought of as a separate farmer with their own slightly different ce maximizing irrigation strategy table 1 shows the top five strategies in terms of the resulting 30 year ce subsequently to evaluate the impact of information uncertainty each ce maximizing irrigation strategy was then applied as described in sections 2 2 and 2 3 for each of the 50 ce maximizing irrigation strategies we conducted a 30 year simulation using our framework s two parallel model approach the first model representing the farmer s expectation of soil moisture status perturbs soil texture properties sand and clay content at the start of the simulation by a normally distributed percentage error figure s3 the soil hydraulic properties are then estimated from this soil texture inside aquacrop via pedotransfer functions saxton and rawls 2006 the true soil texture was defined as a silt loam 25 sand 25 clay which is among the common soil types for maize production in nebraska cropwatch 2018 on each day of the simulation water flux inputs rainfall irrigation and evapotranspiration used to simulate farmer s expectations i e simulation 1 were also perturbed by normally distributed percentage errors figure s4 the distributions chosen were unbiased mean 0 with standard deviations of increasing magnitude from 0 to 30 for both water flux and soil texture measurements in the real world measurement errors in these quantities may be on the lower end of this range in particular in the context of quantities such as irrigation depths in more controlled application systems e g centre pivot or drip however we explore variations in errors up to 30 to provide an estimate of how measurement errors affect resulting irrigation water use and crop yields for a diversity of error levels ranging from perfect information 0 error to very large uncertainty 30 error given that the added errors were randomly drawn the process was repeated 1000 times for each combination of irrigation strategy water flux error and soil texture error this number was chosen by finding how many repetitions were required before average ce results stabilized figure s5 simulations were conducted over a 30 year 1989 2018 period using weather data observed at a monitoring station in champion southwest nebraska high plains regional climate center n d where the number of years was chosen to capture the range of potential growing weather conditions experienced by farmers in the region for each 30 year simulation we determine the effects of soil moisture uncertainty by comparing average annual profits and total irrigation water use for simulations with and without water flux and soil texture errors simulations consider a constant crop price used in equation 2 of 180 per tonne 4 57 per bushel based on a 10 year average of us maize grain prices usda 2019 and an irrigation cost of 2 per ha mm 20 55 per acre inch based on typical costs associated with pumping and application of water water 2017 fixed production costs were set to 1728 based on typical average non water production costs for centre pivot irrigated maize production in the 2019 nebraska crop budget report cropwatch 2019a these costs include field operation costs including labour materials and services taxes etc for non irrigation activities and are not likely to change significantly if the irrigation strategy is altered baseline simulations consider a risk neutral farmer i e r 0 in eq 1 to evaluate the sensitivity of our results to model parameter choices we conducted a range of sensitivity tests the first sensitivity analysis was for irrigation cost whereby the same analysis was conducted for two alternative costs of irrigation 1 2 3 per ha mm as this cost can vary greatly due to differences in water source energy price and tariffs and labor costs wichelns 2010 higher irrigation costs will amplify the economic impacts of over irrigation potentially resulting in a corresponding increase in the value a farmer may place on accurate soil moisture information on the other hand when irrigation is cheap to apply there is less incentive to conserve water and risk any reductions in yield that may occur due to delayed irrigation in these circumstances the value of soil moisture information therefore is expected to be lower subsequently we conducted a second sensitivity analysis to evaluate the effects of risk aversion on the value of more accurate soil moisture information to a farmer risk averse farmers were represented by varying the risk coefficient in eq 1 r 0 2 5 5 to reflect the fact that existing literature shows farmers often exhibit risk averse behavior abdulkadri 2003 menapace et al 2013 as more weight is placed on reducing the year to year variance in profits the optimal strategy should aim to apply more water than the risk neutral case in order to reduce water stress in the driest years the desire to reduce year to year variation should also increase the value the farmer places on accurate soil moisture information as greater information accuracy would be expected to result in lower risks of crop failure all else being equal a summary of key model parameters is shown in table 2 3 results 3 1 the effect of soil moisture uncertainty on water use and profits each combination of water flux and soil texture errors averaged over all strategies and repetitions and their resulting impacts on water use and profits is shown in fig 3 for a risk neutral farmer and implementing behavioral feedbacks described in section 2 2 the responsiveness of modelled outputs is variable depending on the underlying source of soil moisture error with water use fig 3a and profits fig 3b showing greater sensitivity to water flux errors than soil texture errors increasing magnitude of soil moisture errors from both sources results in only marginal changes in irrigation water use and profits for example each contour line in fig 3a and b represents a 1 increase and 1 decrease respectively from the case of perfect information these results indicate that a nearly 30 assumed standard error in the water flux and soil texture inputs causes only around a 4 increase in total water use fig 3a and 14 decline in profits fig 3b figure s6 shows that almost no change in crop yields is observed even for extreme levels of error of up to 30 indicating that the primary driver of profit losses is through this excess water application through inefficient irrigation scheduling as a result of soil moisture estimation uncertainty the results presented in this section and following sections are averaged over the 50 ce maximizing strategies as well as 1000 repetitions of the 30 year case study simulation thus the impact of the stochastic measurement uncertainty may deviate from the overall trend in any given simulation figure s7 fig 4 also illustrates the effect of adding the additional behavioral feedback mechanisms described in section 2 3 behavioral feedbacks were designed to prevent unrealistic irrigation behavior caused by measurement error effectively ensuring that irrigation events were triggered when true soil moisture levels were sufficiently low to cause visible plant stress and were not triggered when true soil moisture levels were sufficiently high to cause visible soil saturation introducing the feedback mechanism leads to greater increases in water use as soil moisture errors become larger fig 4a but results in a reduction in the economic losses caused by larger measurement errors fig 4b this result reflects the fact that the behavioral feedbacks most commonly act to trigger irrigation when the farmer has underestimated soil moisture depletion leading to more frequent irrigation events but also reducing the duration and severity of crop water stress relative to simulations without behavioral feedbacks in contrast triggering of behavioral feedbacks for excess irrigation is relatively uncommon as the whole root zone would have to have exceeded field capacity introducing behavioral feedbacks also results in small changes in water use and reductions in ce and profits even under perfect information i e for errors equal to 0 because ce maximizing strategies are designed under the assumption of perfect information without consideration of additional behavioral feedbacks introducing these feedbacks leads to permutations to these ce maximizing strategies that trigger slight adjustments to final simulated outputs at the end of each season these changes however are small representing as little as 0 5 and 0 3 changes in average annual water use and ce respectively 3 2 sensitivity analysis to assess the impact of our choice of risk preference and irrigation cost r 0 and c 2 per ha mm on the estimated impacts of soil moisture uncertainty on irrigation water use and farm profits we repeat the analyses described in sections 2 2 and 2 3 for combinations of three alternative risk coefficients 0 2 5 5 and irrigation costs 1 2 3 per ha mm in the absence of soil moisture uncertainty higher irrigation costs lead to reductions in average water use as expected fig 5a the magnitude of this reduction is also much larger than increases resulting from risk preferences or soil moisture uncertainty fig 5b suggesting that irrigation water costs or prices have a larger effect on water use decisions than risk aversion or uncertainties in underlying data used to determine soil moisture conditions for irrigation scheduling this relationship between irrigation cost and water use is commonly analysed in the literature via the price elasticity of water use also known as the price elasticity of demand which is the percentage change in a farmer s irrigation water use for a percentage change in irrigation cost previous studies have placed the price elasticity within a wide range 0 001 1 97 scheierling et al 2006 calculating the price elasticity of water use using our results in the absence of soil moisture uncertainty yields a mean price elasticity of 0 23 0 12 calculated from the results presented in fig 5a this result is comparable with past estimates that consider only water use adjustments on the intensive margin i e estimates that consider changes to per area irrigation water use but not changes in demand caused by adjustments to irrigated area or crop type similarly calculating the percentage change in water use for a percentage change in soil moisture error or risk coefficient yielded an information elasticity of 0 015 0 016 and a risk elasticity of 0 012 0 016 this comparison further reinforces our finding that water use appears to be much more sensitive to changes in cost than either risk preference or soil moisture error the choice of irrigation cost and risk preference can also alter how soil moisture uncertainty affects water use and profits in the case of farmers with an imperfect ability to estimate or monitor soil moisture conditions high irrigation costs will reduce the optimal amount of irrigation used over the season meaning that farmers have less scope to mis time events while still avoiding damaging water stress or that farmers must apply extra irrigation at significant economic cost a high degree of risk aversion represented by a larger positive value of r will similarly penalize large year to year fluctuations in profit meaning the negative economic impacts of soil moisture errors and hence the value of improved information accuracy may be enhanced due to the increase in the variance in annual profits when both high risk preferences r 5 and high irrigation costs c 3 per ha mm are combined the economic losses resulting from uncertainty are almost doubled compared to the case presented throughout this analysis r 0 and c 2 per ha mm fig 5c 3 3 comparing the impacts of uncertainty with choice of irrigation strategy our analysis in the previous sections has shown that a 30 water flux and soil texture error results in only a 4 increase in water use fig 3a whereas the standard deviation in water use between 50 different optimal strategies was 5 7 fig 4a this comparison indicates that the choice of irrigation management strategy represented in this study by a combination of four soil moisture thresholds could be more important in determining irrigation water use than uncertainty in underlying soil moisture estimates used to implement irrigation strategies since all strategies evaluated so far can be considered optimal one would expect a sub optimal strategy to have a significantly different level of water use and profits if that were the case to explore further the impacts of the choice of irrigation strategy relative to the effects of soil moisture uncertainty we repeated the analysis for our case study allowing the farmer to select only a single optimized soil moisture threshold throughout each season e g the green line in fig 2 would be one straight line as opposed to a set of four optimized thresholds disaggregated by growth stage the top five single threshold strategies in terms of 30 year average ce are displayed in table 3 single thresholds have been applied in past studies of farmer irrigation decision making and is often recommended by extension and agronomic advisories blonquist et al 2006 gutierrez et al 2014 however since they do not not consider the variable sensitivity of crop growth to water deficits over the growing season they are sub optimal compared to the four threshold strategies used in the baseline analysis fig 6 illustrates that the choice of irrigation management strategy i e one versus four soil moisture thresholds per season has a much larger impact on water use fig 6a and profits fig 6b than added soil moisture uncertainty under perfect information zero added soil moisture error the sub optimal strategy one irrigation threshold uses 35 ha mm more water on average and reduces profits by 117 ha 1 compared with the optimal strategy four irrigation thresholds conversely adding 30 water flux and soil texture errors to the optimal strategy results in only 11 ha mm more water being applied and a 27 ha 1 reduction in profits meaning that the accuracy of soil moisture information appears to be much less important to the water use efficiency and profitability of irrigation decision making than the choices of heuristics for irrigation scheduling indeed fig 6 suggests that adding a 30 standard error onto an optimal strategy can still outperform a sub optimal strategy with perfect information 4 discussion our findings demonstrate that the response of irrigation water use and farm profits to soil moisture uncertainty is non linear with soil moisture uncertainty leading to reductions in both efficiency of irrigation water use and farm profits however we show that with the addition of realistic farmer behavior in the form of our feedback mechanism the magnitude of these changes is much smaller than commonly assumed in policy and practice soulis et al 2015 adeyemi et al 2017 in fact our analysis suggests that near optimal irrigation decisions can be made without perfect soil moisture information in contrast we find the choice of irrigation management strategy in this study given by a set of four soil moisture thresholds was a far more important determinant of water efficiency productivity and profitability than the accuracy of soil moisture information used to implement a selected irrigation management strategy to explain this counterintuitive result it is necessary to consider how the choice of irrigation management strategy and the accuracy of the information used to implement that strategy combine to influence risks of crop water stress and resulting yield losses for our baseline analysis the highest performing in terms of ce soil moisture target strategy consisted of irrigation being triggered when soil moisture levels were assumed to be 48 46 38 and 12 of soil water holding capacity in each of the crop s four main growth stages table 1 these assumptions compare with calibrated thresholds for the initiation of crop water stress affecting canopy expansion 86 plant transpiration 31 canopy senescence 31 or pollination 20 thus the optimal soil moisture targets apart from that in the final growth stage canopy senescence are above thresholds needed to trigger more severe crop stress effects stomatal closure early canopy senescence pollination failure these thresholds have been optimized to maximize profit over 30 years of climate data meaning that they must incorporate a degree of headroom in order to cope with unexpected year to year weather variability with exact weather conditions being unknown to farmers apriori in any given season therefore errors in estimates of soil moisture conditions have to be large and persist for multiple days to result in unintended water stress reducing the risks posed by inaccurate soil water monitoring in contrast the highest performing single threshold i e sub optimal irrigation strategy was 38 of soil water holding capacity table 3 this sub optimal threshold is closer to the crop water stress thresholds in particular in earlier stages of crop development and so can result in greater increases in both the frequency and magnitude of water stress occurrence this single seasonal threshold also has the effect of triggering excess water use in the late season explaining how the choice of irrigation strategy has a large impact on water use efficiency and farm profits even in the absence of soil moisture uncertainty this intuition underlying our findings is also critical to understanding how consideration of behavioral feedbacks section 3 1 and variability in irrigation costs section 3 2 influence the impacts of soil moisture uncertainty on water use and farm profits in our analysis when considering the potential for rational behavioral feedbacks made by farmers e g to respond to visible signs of severe water deficits it is logical that this feedback mitigates the effect of measurement uncertainties as these feedbacks further minimize any risks of erroneously falling below the intended irrigation trigger threshold similarly the moderate increase in the sensitivity of water use and profits to soil moisture uncertainty at higher water prices can be explained by the fact that farmers faced by higher water prices will irrigate at lower soil moisture levels independent of any uncertainty in soil moisture measurement in order to reduce total irrigation applied over the season indeed optimal soil moisture targets for higher water price of 3 per ha mm were 0 46 30 2 soil water holding capacity as such farmers operating in regions with higher water prices whatever their cause will have greater risks of triggering unintended water stress for a given level of soil moisture measurement suggesting that the value of improved soil moisture information accuracy will depend in part on local water prices and costs of irrigation access 4 1 policy implications our main finding that perfect soil moisture information is not required to make near optimal irrigation decisions is broadly consistent with wider literature assessing the value of soil and climate information for farmers bosch and eidman 1987 botes et al 1996 fafchamps and minten 2012 for example previous research found that using historical weather data to determine optimal irrigation strategies results in relatively minor irrigation increases and profit losses compared with using perfect weather forecasts linker et al 2018 jamal et al 2019 moreover linker and kisekka 2017 used a similar parallel model approach to the present study to show that perfect real time soil moisture measurements may not be required to implement a deficit irrigation strategy these studies along with our findings show that the value of improved soil moisture measurement data alone may have been overstated by technology providers and water managers instead a meaningful proportion of potential gains in profit or water efficiency may actually be the result of improved advice or guidance about optimal irrigation management practices which are commonly provided by technology providers or extension agents when seeking to introduce new technologies for farm level decision support whether that be a soil moisture sensor or crop water model when compared with estimated profitability gains observed in our analysis this result suggests that the benefits of improved soil moisture data which can cost hundreds of dollars kukal et al 2019 may vary substantially depending on farmers baseline soil moisture uncertainty or irrigation scheduling practices and heuristics with much of the gains in water use productivity potentially achievable through improving irrigation management practices even if soil moisture monitoring remains imperfect the greater impact of irrigation strategy choice over information accuracy identified in our study has important implications for efforts to improve irrigation management decision making for example deficit irrigation whereby irrigation is reduced below full crop water needs while minimizing stress at sensitive growth stages has been widely proposed as a key tool for improving agricultural water productivity kang et al 2000 blonquist et al 2006 fereres and soriano 2007 dukes et al 2010 our findings highlight the importance of these efforts demonstrating that it may be more cost effective to refine irrigation management strategies for example through optimization methods in particular where costs of reducing soil moisture monitoring errors are large e g if multiple sensors need to be installed within a single field a focus on improving irrigation strategies over monitoring data could lead to potentially faster improvements in agricultural water productivity where improvements in monitoring are considered the focus should be on low cost and more scalable solutions as opposed to expensive sensor network arrays that aim for high levels of accuracy and precision in soil moisture monitoring for example a farmer might choose to use a smaller network of low cost sensors to monitor soil moisture or instead rely on remotely observed proxies for soil moisture such as from earth observation satellites our results and those of linker and kisekka 2017 indicate that the lower monitoring accuracy of such approaches would not necessarily be detrimental to water use efficiency or productivity and in some cases may be an economically optimal solution for farmers further research however is required to evaluate the cost effectiveness of different monitoring and sensing approaches that vary in terms of their adoption costs and accuracy in different types of agricultural production systems finally our results contribute to the wider literature on the use of crop water models to estimate irrigation water demand in the context of agricultural water management and planning such studies typically assume the farmer has access to perfect soil moisture information garcía vila and fereres 2012 foster et al 2014 zellner et al 2020 our results suggest that this assumption does not have a major impact on results other than marginal underestimations in water use however our results do highlight that incorrectly parameterizing farmers irrigation strategies is potentially a significant source of uncertainty in irrigation demand projections previous studies have highlighted a large variability in irrigation behavior even after controlling for key inputs to crop models such as crop type soil type and irrigation technology foster et al 2019 our findings thus highlight that the lack of information on the rules that govern farmers irrigation decisions is potentially a major source of uncertainty in irrigation water demand estimation and projection this uncertainty may greatly exceed those introduced by uncertainty in soil climate or crop input parameter data that are more commonly the focus of parameter uncertainty studies 4 2 limitations and future work the analysis presented in this study contains a number of limitations and simplifications that warrant discussion here and consideration in future extensions to this paper in our analysis we have assumed a perfect model that accurately represents both the movement of water in soil and corresponding crop growth this assumption has enabled us to focus on quantifying the effect of measurement errors such as the water flux and soil texture making the results independent of the choice of crop growth model if this assumption does not hold there would be a larger deviation between the farmer s perception and the true soil water conditions potentially placing more importance on having accurate soil moisture information however the inclusion of the feedback mechanism in this study places limits on how wrong these perceptions of soil water conditions can be meaning that the main conclusions of this study should hold even under an imperfect model moreover in reality adoption of sensors is unlikely to guarantee perfect information about soil moisture conditions sensors may give imprecise readings due to faults inaccurate calibration or where extrapolating point measurements to field scales sharma et al 2021 meaning that our comparison with perfect information may be exaggerating the true benefits of technology adoption the major consequences of over irrigating in our framework have been through the excess cost of water applied however in reality an additional impact of over irrigation is waterlogging and associated damage to crops through aeration stress pests diseases and mechanical damage such as lodging recent research has shown that process based crop models poorly capture the occurrence of waterlogging due to assumptions in soil moisture drainage routines and thus underestimate the negative impacts of excess water supply on crop growth and yields li et al 2019 neglecting these negative consequences of over irrigation may mean that our analysis has understated the effects of uncertainty on crop yields and profits in particular for crops that are sensitive to effects of waterlogging and excess irrigation e g enhanced lodging risks for crops such as wheat nonetheless the more severe cases of over irrigation would have been prevented via the behavioral feedback mechanisms used in this analysis and this point of intervention where a farmer feels that further irrigation would be detrimental to the crop chosen to be field capacity in this case study would likely change and reflect the risks of their specific crop based on their own experience this feedback addition means that impacts of model simplifications of over irrigation effects should be minimized under the assumption of rational farmer behavior conversely for systems that apply significant amounts of water in one event e g flood irrigation the potential for crop stress caused by over irrigation is increased compared to our study of pivot production our analysis does not explicitly consider the effects technical or regulatory constraints to irrigation water use on the value of more accurate soil moisture information in reality policies constraining seasonal or intraseasonal rates of water use ifft et al 2018 or physical limits on water use e g due to declining well yields foster et al 2015 may both increase incentives for farmers to avoid over irrigation and thus increase the value of implementing optimal irrigation strategies combined with these limits characteristics of the delivery system will impact which days and how much water can be applied in each event beyond the simple maximum daily limit considered in our analysis o brien et al 1998 such restrictions and extraneous variables e g a seasonal cap on irrigation may increase the value of information as there is an additional cost of exceeding this cap on the other hand if irrigation events are largely determined by well yields water delivery systems energy regulators and weather forecasts then the number of mis timed irrigation events due solely to inaccurate soil moisture measurements will be low in either case we expect the value of improved irrigation strategies to move in the same direction as the value of soil moisture information as more freedom over the timing and quantity of water applied allows the farmer to better adjust irrigation to match crop water needs the results of our illustrative example are predicated on the ability of the underlying crop water model aquacrop os to accurately simulate crop responses to soil water deficits the model has been calibrated and extensively applied to simulate irrigated and rainfed maize production in our study area and wider north american farming systems heng et al 2009 foster et al 2015 sandhu and irmak 2019 however to ensure that our results are not being driven by any remaining uncertainty in either water stress thresholds or response parameters a sensitivity analysis was therefore performed in this sensitivity analysis we artificially altered the stress response parameters in aquacrop os setting each value to 1 to maximize impacts of any short duration periods of water stress on crop growth and yields we then repeated our analysis described in section 3 1 following methods outlined in section 2 water use and profits from this analysis showed the same non linear response to water flux and soil texture uncertainty as reported earlier in section 3 a 10 standard error in both soil texture and water flux inputs led to less than 1 change in water use and less than 5 change in profits figure s8 this sensitivity analysis demonstrates that the results presented in this article are highly unlikely to be an artifact of any inability within aquacrop to simulate the severity of short duration water stress impacts and we expect that qualitatively identical results would therefore be obtained with alternative more biophysically complex crop models e g hybrid maize yang et al 2004 a key reason for this result is that while the model now responds more strongly to water stress optimal soil moisture targets are also adjusted to 64 73 39 0 soil water holding capacity and thereby greatly increasing the headroom above the critical crop stress thresholds finally our results are specific to the particular illustrative application chosen in this study irrigated maize production in the central united states nonetheless we expect that our general qualitative finding that choice of irrigation scheduling rules is more important than the accuracy of data these rules are implemented based on will apply more generally to other crops and regions in particular our sensitivity analysis described above suggests that even for more drought sensitive crops e g soft fruits perfect information may not be necessary to achieve near optimal water efficiency and profits so long as farmers baseline irrigation heuristics are effectively representative of crop sensitivity to water deficits conversely higher crop prices are likely to enhance the value of improved soil moisture information as any yield losses will have a larger economic impact for farmers the limited sensitivity of crop yields to measurement errors shown in sections 3 1 and 3 2 suggest that changes in crop price would have minimal impact on economic losses caused by soil moisture uncertainty this is in contrast with the larger effects of water prices and risk aversion that are directly affected by the greater magnitude of change in irrigation water use with measurement error nonetheless we acknowledge that variations in crop yields may have a larger influence on the value of improved soil moisture information for example where a farmer s baseline irrigation strategy is sub optimal or for crops that respond strongly to either soil water deficits or saturation analyzing such alternate production environments is beyond the scope of this paper but will be an important area for future research building on the results presented here 5 conclusions effective irrigation scheduling is essential for improving agricultural water productivity and managing the negative impacts of agricultural water abstractions on other water users and the environment therefore technology companies and researchers have focused on developing technologies such as soil moisture sensors that improve the quality of farmers soil moisture information implicit to this approach is an assumption that more accurate information is essential to enable efficient near optimal irrigation decisions however this assumption neglects the contribution of farmers choice of irrigation strategy which may have an equally significant or greater impact on overall efficiency and profitability of agricultural water use in this article we have developed a framework to assess the impacts of increasing uncertainty on water use and farm profits measurement errors in water fluxes and soil texture lead to a divergence between the farmers perception of soil moisture and the true soil water status resulting in sub optimal irrigation decisions and reduced profitability however our results show that the magnitude of these impacts are small with a 30 standard error in water flux and soil texture measurements which is much larger than the likely real world measurement uncertainty in these quantities particularly precipitation and irrigation depth resulting in only a 4 increase in average water use meaning that near optimal irrigation decisions can be made without perfect information contrastingly we demonstrate that the choice of irrigation scheduling strategy has a larger impact on water use and profits than soil moisture uncertainty our analysis suggests that efforts to improve irrigation water efficiency should therefore focus primarily on helping farmers to evaluate and develop improved irrigation scheduling strategies for their specific production settings where existing irrigation management strategies are poorly aligned with local agronomic and biophysical conditions the potential gains in water use productivity and profitability may be large and further enhance the potential benefits from complementary efforts to improve quality of soil moisture information using new forms of low cost and scalable sensing technologies credit authorship contribution statement t d kelly conceptualization methodology software formal analysis writing original draft t foster conceptualization methodology supervision software writing review editing visualization david m schultz conceptualization supervision writing review editing t mieno conceptualization methodology writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank the anonymous reviewers for comments that have improved the manuscript the work contained in this article was funded by the national environmental research council s understanding the earth atmosphere and ocean doctoral training programme grant ne l002469 1 data generated by the developed framework and code can be found at http doi org 10 5281 zenodo 4041476 supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2021 103982 appendix supplementary materials image application 1 
282,technologies that increase the accuracy of soil moisture monitoring such as in situ sensors have been proposed as a key solution for increasing agricultural water productivity however quantifying how uncertainty in soil moisture estimates lead to irrigation inefficiencies or economic losses has not been explicitly studied we develop a framework that combines a crop simulation model with a rule based irrigation decision making algorithm to assess the impact of soil moisture uncertainty on irrigation use and farm profits we apply this modelling framework to a case study of irrigated maize production in nebraska united states a region where improvements in agricultural water productivity are at the forefront of water policy debates we consider two main sources of uncertainty that result in a divergence between the farmers perception of soil water content and the true water status namely errors in the knowledge of soil texture and measurement of daily soil water flux inflows and outflows even for very large errors in both soil texture and water flux measurements impacts on water use and profits were marginal 11 ha mm increase and 27 ha 1 decrease respectively in contrast farmers choice of irrigation strategy had a much larger impact on water use and profits than uncertainty in soil moisture information used to implement that strategy our findings show that near optimal irrigation decisions can be made without perfect soil moisture information this conclusion suggests that providing farmers with improved irrigation scheduling recommendations utilizing crop water models and optimization techniques would have a larger impact on water use efficiency than simply providing farmers with technologies to more accurately monitor soil moisture conditions keywords aquacrop soil moisture uncertainty irrigation decisions irrigation optimization measurement uncertainty 1 introduction increases in irrigation water demand driven by climate change and population growth are expected to exacerbate water scarcity and conflict over limited freshwater resources in many regions around the world e g molden 2013 to manage these challenges there is a need to identify ways to improve the productivity of agricultural water use commonly referred to as generating more crop per drop one key pathway for raising agricultural water productivity is through improvements to irrigation scheduling which enables farmers to target limited water supply to when the crop is most sensitive to water stress and thus maximize yield returns to limited freshwater inputs morison et al 2008 evans and sadler 2008 adeyemi et al 2017 the majority of the developments in irrigation scheduling come under the label of precision irrigation which broadly encapsulates solutions that aim to vary the temporal and spatial distribution of irrigation to match crop water demands adeyemi et al 2017 associated technologies to support implementation of these strategies include sensors to monitor soil water content or crop growth jones 2004 dobriyal et al 2012 ihuoma and madramootoo 2017 field mapping that highlights heterogeneities in soil texture and other properties daccache et al 2015 and decision support tools that aid in the timing of irrigation events mccarthy et al 2010 the common goal that links these methods is the precise monitoring and management of soil water content and crop water needs currently the uptake of precision irrigation methods and technologies remains low with most farmers even in developed agricultural systems such as the united states relying on proxies such as the visual condition of the crop and soil as well as simple water balance calculations to determine when to irrigate fig 1 much like soil moisture sensors these traditional techniques aim to monitor and manage soil water content in the crop root zone and hence the amount of water available for crop uptake the difference between farmers with and without soil moisture sensors therefore is the accuracy of this monitoring along with the associated rules and heuristics by which this information is used to make irrigation decisions sources of uncertainty in soil moisture monitoring and estimation are similar for both traditional and precision irrigation methods one important source of uncertainty is heterogeneity in soil textural composition which affects estimates of current and potential soil water storage both laterally and vertically within a field nielsen et al 1973 brocca et al 2007 feki et al 2018 heterogeneity in soil texture means that point measurements of soil moisture status are only valid for a specific location and depth potentially leading to errors when this information is used in water balance calculations across large areas e g field or farm additional uncertainty in soil moisture estimation also results from errors in climatological rainfall evapotranspiration etc and irrigation data characterising inflows and outflows from soil water storage differences in infiltration rates due to differences in soil texture and topography as well as large distances between the measurement instruments and cropping area are major contributors to this uncertainty chaubey et al 1999 moreover the accuracy of evapotranspiration estimates are greatly affected by the amount of site specific information used in its calculation sentelhas et al 2010 including in the calculations of reference and actual evapotranspiration allen et al 1998 the result of either soil texture or water flux errors is that a farmer may miscalculate the amount of water stored in the soil on any given day leading to inaccurate estimates of when or how much irrigation should be applied as a result a farmer may apply insufficient or excessive amounts of water with resulting negative impacts on crop yields and or farm profits precision irrigation research has focused on developing tools and techniques to reduce the magnitude of soil moisture error soulis et al 2015 adeyemi et al 2017 kukal et al 2019 alongside this when farmers invest in more accurate soil water monitoring technology such as sensors or weather stations they will often also receive advice about the best trigger levels and irrigation management strategies to maximize yields profits or water efficiency unl 2019 the relative contribution of improved scheduling rules versus improved soil moisture information to farmers irrigation water use practices has not been explicitly studied however this distinction has important implications for farmers and water managers seeking to understand which interventions and technologies will be most effective for delivering desired improvements in the productivity and value of irrigation water use in response to these gaps in the literature this article presents a framework for studying the impact of varying levels of soil moisture uncertainty on irrigation water use decisions and farm profits the framework consists of an optimization module to generate irrigation strategies a crop growth model to assess impacts of these irrigation strategies on water use and crop yields and a mechanism for applying variable soil moisture measurement errors within model simulations we apply this framework a case study of irrigated maize production in nebraska united states using the aquacrop os crop water model to assess how assumptions about irrigation management practices and the accuracy of soil moisture information on which these strategies are implemented affect irrigation decisions and farm profits our findings provide insights on the response of irrigation water productivity to soil moisture measurement uncertainty showing that the impact of inaccurate soil water monitoring on crop yields profits and water use efficiency is low relative to the impacts of using sub optimal irrigation scheduling heuristics we discuss the implications of these results for local policy makers industry and producers and provide recommendations for how our results can be used to make improvements in agricultural water productivity at field to catchment scales 2 methods in this section we describe our proposed framework for modelling farmer irrigation decision making under varying levels of soil moisture measurement uncertainty and for assessing the resulting impacts of this measurement uncertainty on water use crop yields and economic returns we first present our generalised assessment framework in three main subsections determination of farmers irrigation scheduling rules section 2 1 assessment of the performance of these strategies under different sources and magnitudes of measurement error sections 2 2 and the incorporation of a feedback mechanism to represent realistic farmer behavior section 2 3 we then describe the specific case study application of this framework including details of model parameters and choice of crop growth model optimization algorithm crop type and location used as an illustrative example in this study section 2 4 2 1 determining farmers rules in scheduling irrigation despite a large variation in the methods used by farmers to schedule irrigation fig 1 the goal of most methods is to maintain soil moisture levels above a pre specified threshold chosen to meet crop water needs and balance trade offs between costs and yield benefits of water use in line with these assumptions and past research linker et al 2016 foster and brozović 2018 linker et al 2018 we assume that a farmer s irrigation decision making can be characterized by the choice of four soil moisture thresholds throughout the growing season one for each major crop growth stage emergence early season canopy development mid season crop growth and yield formation late season canopy senescence these four soil moisture thresholds determine the allowable root zone soil moisture content expressed as a percentage of total soil water holding capacity which water contents must fall below before irrigation is triggered at the point irrigation is triggered we assume that water will be applied to refill the soil profile back to field capacity subject to a maximum application rate which is determined by the type of irrigation system and water source used by a farmer the choice of soil moisture thresholds which we refer to as the irrigation strategy can have a major impact on water use and crop growth setting these thresholds too high i e irrigating at low levels of soil moisture depletion will result in over irrigation whereas setting thresholds too low i e irrigating only at high levels of soil moisture depletion will result in under irrigation and thus production losses in line with past research showing that farmers input use decisions may be influenced by their risk preferences abdulkadri 2003 menapace et al 2013 we define the optimal set of soil moisture thresholds as those that maximize a farmers certainty equivalent ce the ce is defined as the amount of economic return that would have to be guaranteed in order for an individual to be indifferent towards a higher but less certain return the ce has been used in previous agro economic models of farmer input use decision making lehmann et al 2013 foster et al 2014 in the context of irrigation optimization the ce for a given irrigation strategy s is defined as 1 c e s e p s 0 5 r v a r p s e p s where strategy s t 1 t 2 t 3 t 4 is made up of four soil moisture thresholds the expected profits e p s and variance var p s will be calculated over a given set of seasons that characterise farmers expectations of potential inter annual weather variability and r is the risk coefficient representing farmer s risk preferences if a risk averse farmer r 0 wishes to increase the ce they could either increase the expected average profits or reduce the year to year variability in profits eq 1 a risk neutral farmer r 0 will instead only aim to maximise expected average seasonal profits for a given irrigation strategy s the seasonal profit p s is calculated as 2 p s m y s c i s f where m is a constant crop market price per tonne y s is crop yield tonne per ha c is a constant irrigation cost per ha mm i s is total seasonal irrigation applied ha mm and f is fixed production costs per ha finding a set of soil moisture thresholds that maximize ce is a difficult task due to large number of potential soil moisture threshold combinations and interdependencies between the choice of threshold for each individual growth stage for example irrigating more in the early part of the season will affect the optimal irrigation threshold for the rest of the season as a result the search space of possible threshold combinations and their resulting outcomes is non linear and discontinuous for these types of search spaces swarm intelligence algorithms have become popular in recent decades mavrovouniotis et al 2017 and have been used previously for the purpose of optimal irrigation planning and management de paly and zell 2009 noory et al 2012 these algorithms operate on the principle that a group of agents scattered across the search space can search locally for solutions while communicating with the other agents this combination ensures that the solutions found are close to the global optimum for this analysis we determine farmers optimal i e ce maximizing soil moisture thresholds using the particle swarm optimization algorithm eberhart and kennedy 1995 freitas et al 2020 although in principle any optimization method could be used within our framework this method starts by initializing a population of particles i e possible soil moisture threshold combinations across the search space each particle is then evaluated and updated using the best combinations found by both the particle and the population as a whole this evaluate update process repeats until a stopping criteria has been met mathworks n d particle swarm optimization has been successfully applied in a diverse array of research areas such as control problems biomedical research robotics and distribution networks poli 2008 2 2 quantifying the impact of uncertainty on irrigation decisions and profits as shown in fig 1 and discussed earlier many farmers do not measure soil moisture directly using sensors instead most rely on proxy measures of soil water content such as visual inspection of the soil and simplified checkbook style water balance models each of these approaches carry uncertainties and errors in the measurement of soil moisture which may influence the performance of selected irrigation scheduling strategies in this study we focus on two main sources of error 1 the specification of soil textural characteristics that cause a persistent seasonal bias to water balance estimation and 2 measurements of daily precipitation irrigation and evapotranspiration fluxes that cause daily unbiased errors in soil moisture content our analysis does not consider structural model errors such as simplifications in the water balance equations inherent in traditional soil moisture monitoring approaches used by farmers and our analysis assumes that the underlying soil water balance model is a perfect model of the chosen system focusing on these two sources of error allows us to examine the effects of measurement uncertainty on farmers irrigation decision making the framework created to study the effect of soil moisture uncertainty consists of two identical crop model simulations running in parallel simulation 1 represents the farmers expectations about soil moisture conditions which will be impacted by any errors in soil texture and water flux measurements the sources of uncertainty considered are errors in soil texture inputs added at the beginning of each season and water flux inputs added on each day of the simulation simulation 2 represents the true status of soil moisture conditions based on the actual soil textural properties and water fluxes irrigation decisions are determined based on the soil moisture content given by simulation 1 as well as the farmers ce maximizing irrigation strategy that determines the level of soil moisture at which they intend to trigger irrigation irrigation events estimated in simulation 1 are then applied in simulation 2 to determine resulting impacts on water use and crop yields under perfect information zero errors in soil texture and water flux the farmers perception of soil moisture conditions match the true state and both simulations are identical such that the farmer is able to implement perfectly the ce maximizing irrigation strategy when the magnitudes of soil texture errors water flux errors or both are greater than zero the difference between the farmer s expectations of soil moisture conditions and reality diverge this difference potentially results in sub optimal irrigation scheduling with negative impacts on crop yields or profits for example if the farmer triggers irrigation at a lower level of soil moisture than the intended ce maximizing threshold i e because their estimated soil moisture levels in simulation 1 over estimate true soil moisture levels given in simulation 2 then the crop may experience unintended water stress and a reduction in yields alternatively if the farmer triggers irrigation at a higher soil moisture level than the intended ce maximizing threshold i e because their estimated soil moisture levels in simulation 1 under estimate true soil moisture levels given in simulation 2 then irrigation water may be wasted and profits reduced two examples of how the added soil moisture uncertainty causes the two simulations to diverge are shown in fig 2 where percentage errors drawn from a normal distribution with zero mean and a standard deviation of 30 have been added to the soil texture and daily water flux inputs in simulation 1 the soil texture errors result in seasonal biases towards either over or under estimation of drainage and therefore soil moisture storage whereas the water flux error results in random daily errors in the farmer s estimate of the true daily change in water content when this calculated soil moisture content simulation 1 drops below the pre defined threshold irrigation is triggered this irrigation depth is applied to both fields regardless of whether the true moisture content simulation 2 has crossed the intended irrigation threshold fig 2a shows an example of irrigation being triggered several days after the true water content in simulation 2 had dropped below the threshold resulting in water stress and yield losses in the alternative case irrigation may be triggered too early causing profit losses from excess water application and potentially leading to crop damage from temporary waterlogging fig 2b 2 3 system feedback when incorporating error in farmers estimation of soil moisture content the potential exists for unrealistic irrigation behavior to occur during simulations that may overstate the true effects of measurement error on farmer irrigation decision making crop yields and profits in reality for example the calculated moisture content simulation 1 may recommend no irrigation despite the true water content simulation 2 being sufficiently low that the crop would be showing visible signs of water stress e g wilting or early senescence alternatively the calculated water content may recommend irrigation despite the true soil profile being visibly saturated or water logged in these instances a farmer would likely adjust their irrigation application in response to these clear visual feedbacks from the true field which would be readily observable through day to day crop management activities even if the farmer was no longer personally measuring the soil moisture content in the field this assumption is further supported by the fact that visible inspection of the crop and soil are by far the most common information sources when deciding to schedule irrigation fig 1 to capture these behavioral feedbacks an additional rule is added to our parallel simulation approach to override the daily irrigation recommendation from the farmers calculation simulation 1 this override occurs if on the previous day the soil moisture conditions in the true field simulation 2 were either i low enough to trigger early canopy senescence which is an indication of severe crop water stress or ii higher than the soil s true field capacity which is indicative of visibly moist soil conditions in these instances an irrigation event within our assessment framework would be triggered or blocked respectively the altered irrigation depth is applied to both fields simulations 1 and 2 and the parallel simulations subsequently continue as described in section 2 2 the added feedbacks have the effect of stopping excessive irrigation in the early season while also preventing unrealistically large water stress in the late season figure s1 we hypothesize that incorporating these behavioral feedbacks will minimize the effects of large measurement errors leading to less importance being placed on the quality of improved soil moisture information a hypothesis we test in subsequent sections as part of our illustrative case study application 2 4 illustrative application to illustrate our modelling framework we apply the methods described in sections 2 2 and 2 3 to the example of centre pivot irrigated maize production in nebraska united states maize is the dominant irrigated crop type in nebraska which has the largest number of irrigated acres by state in the united states usda nass 2018 irrigation water is sourced primarily from the underlying high plains aquifer and improving productivity of water use is a key priority for policymakers to address growing issues of aquifer depletion mcguire 2017 streamflow depletion szilagyi 2000 and degradation of freshwater ecosystems palazzo and brozović 2014 perkin et al 2019 financial incentives to farmers to encourage adoption of soil moisture sensors are seen as a key mechanism to improve water productivity and irrigation efficiencies cropwatch 2019b making maize production in nebraska a logical choice for examining the impact of soil moisture uncertainty on water use and farm profits as the basis for soil water balance and crop growth simulations in our illustrative analysis we use the crop water model aquacrop os foster et al 2017 aquacrop os is an open source version of the original aquacrop raes et al 2009 developed by the food and agriculture organization of the united nations to simulate crop response to water stress the model has been successfully applied and calibrated for maize production in a wide variety of environments hsiao et al 2009 abedinpour et al 2012 including in our study area in the central united states foster et al 2015 and within the united states and globally heng et al 2009 foster et al 2015 sandhu and irmak 2019 aquacrop os captures the impacts of water stress on multiple aspects of crop development expansion and senescence transpiration and yield formation through a series of water stress thresholds and coefficients that have been calibrated for maize in our study areas as part of previous studies forster et al 2015 these determine the level of soil moisture depletion at which each process becomes constrained by water stress along with how resulting deficiencies in crop development or transpiration impact on end of season biomass production and crop yields further details about the simulation of water stress on crop growth and yields in aquacrop are given in steduto et al 2009 and raes et al 2009 the first step in our analysis is to identify ce maximizing soil moisture thresholds using particle swarm optimization as described in section 2 1 given the non linear and discontinuous nature of the search space of possible thresholds the same optimization procedure did not always produce the same set of thresholds multiple optimization repetitions were therefore required to capture the full distribution of ce maximizing strategies the number of repetitions was set to 50 after observing that the variance between optimal threshold combinations stabilized for more repetitions than 50 figure s2 each of these 50 sets of thresholds can now be thought of as a separate farmer with their own slightly different ce maximizing irrigation strategy table 1 shows the top five strategies in terms of the resulting 30 year ce subsequently to evaluate the impact of information uncertainty each ce maximizing irrigation strategy was then applied as described in sections 2 2 and 2 3 for each of the 50 ce maximizing irrigation strategies we conducted a 30 year simulation using our framework s two parallel model approach the first model representing the farmer s expectation of soil moisture status perturbs soil texture properties sand and clay content at the start of the simulation by a normally distributed percentage error figure s3 the soil hydraulic properties are then estimated from this soil texture inside aquacrop via pedotransfer functions saxton and rawls 2006 the true soil texture was defined as a silt loam 25 sand 25 clay which is among the common soil types for maize production in nebraska cropwatch 2018 on each day of the simulation water flux inputs rainfall irrigation and evapotranspiration used to simulate farmer s expectations i e simulation 1 were also perturbed by normally distributed percentage errors figure s4 the distributions chosen were unbiased mean 0 with standard deviations of increasing magnitude from 0 to 30 for both water flux and soil texture measurements in the real world measurement errors in these quantities may be on the lower end of this range in particular in the context of quantities such as irrigation depths in more controlled application systems e g centre pivot or drip however we explore variations in errors up to 30 to provide an estimate of how measurement errors affect resulting irrigation water use and crop yields for a diversity of error levels ranging from perfect information 0 error to very large uncertainty 30 error given that the added errors were randomly drawn the process was repeated 1000 times for each combination of irrigation strategy water flux error and soil texture error this number was chosen by finding how many repetitions were required before average ce results stabilized figure s5 simulations were conducted over a 30 year 1989 2018 period using weather data observed at a monitoring station in champion southwest nebraska high plains regional climate center n d where the number of years was chosen to capture the range of potential growing weather conditions experienced by farmers in the region for each 30 year simulation we determine the effects of soil moisture uncertainty by comparing average annual profits and total irrigation water use for simulations with and without water flux and soil texture errors simulations consider a constant crop price used in equation 2 of 180 per tonne 4 57 per bushel based on a 10 year average of us maize grain prices usda 2019 and an irrigation cost of 2 per ha mm 20 55 per acre inch based on typical costs associated with pumping and application of water water 2017 fixed production costs were set to 1728 based on typical average non water production costs for centre pivot irrigated maize production in the 2019 nebraska crop budget report cropwatch 2019a these costs include field operation costs including labour materials and services taxes etc for non irrigation activities and are not likely to change significantly if the irrigation strategy is altered baseline simulations consider a risk neutral farmer i e r 0 in eq 1 to evaluate the sensitivity of our results to model parameter choices we conducted a range of sensitivity tests the first sensitivity analysis was for irrigation cost whereby the same analysis was conducted for two alternative costs of irrigation 1 2 3 per ha mm as this cost can vary greatly due to differences in water source energy price and tariffs and labor costs wichelns 2010 higher irrigation costs will amplify the economic impacts of over irrigation potentially resulting in a corresponding increase in the value a farmer may place on accurate soil moisture information on the other hand when irrigation is cheap to apply there is less incentive to conserve water and risk any reductions in yield that may occur due to delayed irrigation in these circumstances the value of soil moisture information therefore is expected to be lower subsequently we conducted a second sensitivity analysis to evaluate the effects of risk aversion on the value of more accurate soil moisture information to a farmer risk averse farmers were represented by varying the risk coefficient in eq 1 r 0 2 5 5 to reflect the fact that existing literature shows farmers often exhibit risk averse behavior abdulkadri 2003 menapace et al 2013 as more weight is placed on reducing the year to year variance in profits the optimal strategy should aim to apply more water than the risk neutral case in order to reduce water stress in the driest years the desire to reduce year to year variation should also increase the value the farmer places on accurate soil moisture information as greater information accuracy would be expected to result in lower risks of crop failure all else being equal a summary of key model parameters is shown in table 2 3 results 3 1 the effect of soil moisture uncertainty on water use and profits each combination of water flux and soil texture errors averaged over all strategies and repetitions and their resulting impacts on water use and profits is shown in fig 3 for a risk neutral farmer and implementing behavioral feedbacks described in section 2 2 the responsiveness of modelled outputs is variable depending on the underlying source of soil moisture error with water use fig 3a and profits fig 3b showing greater sensitivity to water flux errors than soil texture errors increasing magnitude of soil moisture errors from both sources results in only marginal changes in irrigation water use and profits for example each contour line in fig 3a and b represents a 1 increase and 1 decrease respectively from the case of perfect information these results indicate that a nearly 30 assumed standard error in the water flux and soil texture inputs causes only around a 4 increase in total water use fig 3a and 14 decline in profits fig 3b figure s6 shows that almost no change in crop yields is observed even for extreme levels of error of up to 30 indicating that the primary driver of profit losses is through this excess water application through inefficient irrigation scheduling as a result of soil moisture estimation uncertainty the results presented in this section and following sections are averaged over the 50 ce maximizing strategies as well as 1000 repetitions of the 30 year case study simulation thus the impact of the stochastic measurement uncertainty may deviate from the overall trend in any given simulation figure s7 fig 4 also illustrates the effect of adding the additional behavioral feedback mechanisms described in section 2 3 behavioral feedbacks were designed to prevent unrealistic irrigation behavior caused by measurement error effectively ensuring that irrigation events were triggered when true soil moisture levels were sufficiently low to cause visible plant stress and were not triggered when true soil moisture levels were sufficiently high to cause visible soil saturation introducing the feedback mechanism leads to greater increases in water use as soil moisture errors become larger fig 4a but results in a reduction in the economic losses caused by larger measurement errors fig 4b this result reflects the fact that the behavioral feedbacks most commonly act to trigger irrigation when the farmer has underestimated soil moisture depletion leading to more frequent irrigation events but also reducing the duration and severity of crop water stress relative to simulations without behavioral feedbacks in contrast triggering of behavioral feedbacks for excess irrigation is relatively uncommon as the whole root zone would have to have exceeded field capacity introducing behavioral feedbacks also results in small changes in water use and reductions in ce and profits even under perfect information i e for errors equal to 0 because ce maximizing strategies are designed under the assumption of perfect information without consideration of additional behavioral feedbacks introducing these feedbacks leads to permutations to these ce maximizing strategies that trigger slight adjustments to final simulated outputs at the end of each season these changes however are small representing as little as 0 5 and 0 3 changes in average annual water use and ce respectively 3 2 sensitivity analysis to assess the impact of our choice of risk preference and irrigation cost r 0 and c 2 per ha mm on the estimated impacts of soil moisture uncertainty on irrigation water use and farm profits we repeat the analyses described in sections 2 2 and 2 3 for combinations of three alternative risk coefficients 0 2 5 5 and irrigation costs 1 2 3 per ha mm in the absence of soil moisture uncertainty higher irrigation costs lead to reductions in average water use as expected fig 5a the magnitude of this reduction is also much larger than increases resulting from risk preferences or soil moisture uncertainty fig 5b suggesting that irrigation water costs or prices have a larger effect on water use decisions than risk aversion or uncertainties in underlying data used to determine soil moisture conditions for irrigation scheduling this relationship between irrigation cost and water use is commonly analysed in the literature via the price elasticity of water use also known as the price elasticity of demand which is the percentage change in a farmer s irrigation water use for a percentage change in irrigation cost previous studies have placed the price elasticity within a wide range 0 001 1 97 scheierling et al 2006 calculating the price elasticity of water use using our results in the absence of soil moisture uncertainty yields a mean price elasticity of 0 23 0 12 calculated from the results presented in fig 5a this result is comparable with past estimates that consider only water use adjustments on the intensive margin i e estimates that consider changes to per area irrigation water use but not changes in demand caused by adjustments to irrigated area or crop type similarly calculating the percentage change in water use for a percentage change in soil moisture error or risk coefficient yielded an information elasticity of 0 015 0 016 and a risk elasticity of 0 012 0 016 this comparison further reinforces our finding that water use appears to be much more sensitive to changes in cost than either risk preference or soil moisture error the choice of irrigation cost and risk preference can also alter how soil moisture uncertainty affects water use and profits in the case of farmers with an imperfect ability to estimate or monitor soil moisture conditions high irrigation costs will reduce the optimal amount of irrigation used over the season meaning that farmers have less scope to mis time events while still avoiding damaging water stress or that farmers must apply extra irrigation at significant economic cost a high degree of risk aversion represented by a larger positive value of r will similarly penalize large year to year fluctuations in profit meaning the negative economic impacts of soil moisture errors and hence the value of improved information accuracy may be enhanced due to the increase in the variance in annual profits when both high risk preferences r 5 and high irrigation costs c 3 per ha mm are combined the economic losses resulting from uncertainty are almost doubled compared to the case presented throughout this analysis r 0 and c 2 per ha mm fig 5c 3 3 comparing the impacts of uncertainty with choice of irrigation strategy our analysis in the previous sections has shown that a 30 water flux and soil texture error results in only a 4 increase in water use fig 3a whereas the standard deviation in water use between 50 different optimal strategies was 5 7 fig 4a this comparison indicates that the choice of irrigation management strategy represented in this study by a combination of four soil moisture thresholds could be more important in determining irrigation water use than uncertainty in underlying soil moisture estimates used to implement irrigation strategies since all strategies evaluated so far can be considered optimal one would expect a sub optimal strategy to have a significantly different level of water use and profits if that were the case to explore further the impacts of the choice of irrigation strategy relative to the effects of soil moisture uncertainty we repeated the analysis for our case study allowing the farmer to select only a single optimized soil moisture threshold throughout each season e g the green line in fig 2 would be one straight line as opposed to a set of four optimized thresholds disaggregated by growth stage the top five single threshold strategies in terms of 30 year average ce are displayed in table 3 single thresholds have been applied in past studies of farmer irrigation decision making and is often recommended by extension and agronomic advisories blonquist et al 2006 gutierrez et al 2014 however since they do not not consider the variable sensitivity of crop growth to water deficits over the growing season they are sub optimal compared to the four threshold strategies used in the baseline analysis fig 6 illustrates that the choice of irrigation management strategy i e one versus four soil moisture thresholds per season has a much larger impact on water use fig 6a and profits fig 6b than added soil moisture uncertainty under perfect information zero added soil moisture error the sub optimal strategy one irrigation threshold uses 35 ha mm more water on average and reduces profits by 117 ha 1 compared with the optimal strategy four irrigation thresholds conversely adding 30 water flux and soil texture errors to the optimal strategy results in only 11 ha mm more water being applied and a 27 ha 1 reduction in profits meaning that the accuracy of soil moisture information appears to be much less important to the water use efficiency and profitability of irrigation decision making than the choices of heuristics for irrigation scheduling indeed fig 6 suggests that adding a 30 standard error onto an optimal strategy can still outperform a sub optimal strategy with perfect information 4 discussion our findings demonstrate that the response of irrigation water use and farm profits to soil moisture uncertainty is non linear with soil moisture uncertainty leading to reductions in both efficiency of irrigation water use and farm profits however we show that with the addition of realistic farmer behavior in the form of our feedback mechanism the magnitude of these changes is much smaller than commonly assumed in policy and practice soulis et al 2015 adeyemi et al 2017 in fact our analysis suggests that near optimal irrigation decisions can be made without perfect soil moisture information in contrast we find the choice of irrigation management strategy in this study given by a set of four soil moisture thresholds was a far more important determinant of water efficiency productivity and profitability than the accuracy of soil moisture information used to implement a selected irrigation management strategy to explain this counterintuitive result it is necessary to consider how the choice of irrigation management strategy and the accuracy of the information used to implement that strategy combine to influence risks of crop water stress and resulting yield losses for our baseline analysis the highest performing in terms of ce soil moisture target strategy consisted of irrigation being triggered when soil moisture levels were assumed to be 48 46 38 and 12 of soil water holding capacity in each of the crop s four main growth stages table 1 these assumptions compare with calibrated thresholds for the initiation of crop water stress affecting canopy expansion 86 plant transpiration 31 canopy senescence 31 or pollination 20 thus the optimal soil moisture targets apart from that in the final growth stage canopy senescence are above thresholds needed to trigger more severe crop stress effects stomatal closure early canopy senescence pollination failure these thresholds have been optimized to maximize profit over 30 years of climate data meaning that they must incorporate a degree of headroom in order to cope with unexpected year to year weather variability with exact weather conditions being unknown to farmers apriori in any given season therefore errors in estimates of soil moisture conditions have to be large and persist for multiple days to result in unintended water stress reducing the risks posed by inaccurate soil water monitoring in contrast the highest performing single threshold i e sub optimal irrigation strategy was 38 of soil water holding capacity table 3 this sub optimal threshold is closer to the crop water stress thresholds in particular in earlier stages of crop development and so can result in greater increases in both the frequency and magnitude of water stress occurrence this single seasonal threshold also has the effect of triggering excess water use in the late season explaining how the choice of irrigation strategy has a large impact on water use efficiency and farm profits even in the absence of soil moisture uncertainty this intuition underlying our findings is also critical to understanding how consideration of behavioral feedbacks section 3 1 and variability in irrigation costs section 3 2 influence the impacts of soil moisture uncertainty on water use and farm profits in our analysis when considering the potential for rational behavioral feedbacks made by farmers e g to respond to visible signs of severe water deficits it is logical that this feedback mitigates the effect of measurement uncertainties as these feedbacks further minimize any risks of erroneously falling below the intended irrigation trigger threshold similarly the moderate increase in the sensitivity of water use and profits to soil moisture uncertainty at higher water prices can be explained by the fact that farmers faced by higher water prices will irrigate at lower soil moisture levels independent of any uncertainty in soil moisture measurement in order to reduce total irrigation applied over the season indeed optimal soil moisture targets for higher water price of 3 per ha mm were 0 46 30 2 soil water holding capacity as such farmers operating in regions with higher water prices whatever their cause will have greater risks of triggering unintended water stress for a given level of soil moisture measurement suggesting that the value of improved soil moisture information accuracy will depend in part on local water prices and costs of irrigation access 4 1 policy implications our main finding that perfect soil moisture information is not required to make near optimal irrigation decisions is broadly consistent with wider literature assessing the value of soil and climate information for farmers bosch and eidman 1987 botes et al 1996 fafchamps and minten 2012 for example previous research found that using historical weather data to determine optimal irrigation strategies results in relatively minor irrigation increases and profit losses compared with using perfect weather forecasts linker et al 2018 jamal et al 2019 moreover linker and kisekka 2017 used a similar parallel model approach to the present study to show that perfect real time soil moisture measurements may not be required to implement a deficit irrigation strategy these studies along with our findings show that the value of improved soil moisture measurement data alone may have been overstated by technology providers and water managers instead a meaningful proportion of potential gains in profit or water efficiency may actually be the result of improved advice or guidance about optimal irrigation management practices which are commonly provided by technology providers or extension agents when seeking to introduce new technologies for farm level decision support whether that be a soil moisture sensor or crop water model when compared with estimated profitability gains observed in our analysis this result suggests that the benefits of improved soil moisture data which can cost hundreds of dollars kukal et al 2019 may vary substantially depending on farmers baseline soil moisture uncertainty or irrigation scheduling practices and heuristics with much of the gains in water use productivity potentially achievable through improving irrigation management practices even if soil moisture monitoring remains imperfect the greater impact of irrigation strategy choice over information accuracy identified in our study has important implications for efforts to improve irrigation management decision making for example deficit irrigation whereby irrigation is reduced below full crop water needs while minimizing stress at sensitive growth stages has been widely proposed as a key tool for improving agricultural water productivity kang et al 2000 blonquist et al 2006 fereres and soriano 2007 dukes et al 2010 our findings highlight the importance of these efforts demonstrating that it may be more cost effective to refine irrigation management strategies for example through optimization methods in particular where costs of reducing soil moisture monitoring errors are large e g if multiple sensors need to be installed within a single field a focus on improving irrigation strategies over monitoring data could lead to potentially faster improvements in agricultural water productivity where improvements in monitoring are considered the focus should be on low cost and more scalable solutions as opposed to expensive sensor network arrays that aim for high levels of accuracy and precision in soil moisture monitoring for example a farmer might choose to use a smaller network of low cost sensors to monitor soil moisture or instead rely on remotely observed proxies for soil moisture such as from earth observation satellites our results and those of linker and kisekka 2017 indicate that the lower monitoring accuracy of such approaches would not necessarily be detrimental to water use efficiency or productivity and in some cases may be an economically optimal solution for farmers further research however is required to evaluate the cost effectiveness of different monitoring and sensing approaches that vary in terms of their adoption costs and accuracy in different types of agricultural production systems finally our results contribute to the wider literature on the use of crop water models to estimate irrigation water demand in the context of agricultural water management and planning such studies typically assume the farmer has access to perfect soil moisture information garcía vila and fereres 2012 foster et al 2014 zellner et al 2020 our results suggest that this assumption does not have a major impact on results other than marginal underestimations in water use however our results do highlight that incorrectly parameterizing farmers irrigation strategies is potentially a significant source of uncertainty in irrigation demand projections previous studies have highlighted a large variability in irrigation behavior even after controlling for key inputs to crop models such as crop type soil type and irrigation technology foster et al 2019 our findings thus highlight that the lack of information on the rules that govern farmers irrigation decisions is potentially a major source of uncertainty in irrigation water demand estimation and projection this uncertainty may greatly exceed those introduced by uncertainty in soil climate or crop input parameter data that are more commonly the focus of parameter uncertainty studies 4 2 limitations and future work the analysis presented in this study contains a number of limitations and simplifications that warrant discussion here and consideration in future extensions to this paper in our analysis we have assumed a perfect model that accurately represents both the movement of water in soil and corresponding crop growth this assumption has enabled us to focus on quantifying the effect of measurement errors such as the water flux and soil texture making the results independent of the choice of crop growth model if this assumption does not hold there would be a larger deviation between the farmer s perception and the true soil water conditions potentially placing more importance on having accurate soil moisture information however the inclusion of the feedback mechanism in this study places limits on how wrong these perceptions of soil water conditions can be meaning that the main conclusions of this study should hold even under an imperfect model moreover in reality adoption of sensors is unlikely to guarantee perfect information about soil moisture conditions sensors may give imprecise readings due to faults inaccurate calibration or where extrapolating point measurements to field scales sharma et al 2021 meaning that our comparison with perfect information may be exaggerating the true benefits of technology adoption the major consequences of over irrigating in our framework have been through the excess cost of water applied however in reality an additional impact of over irrigation is waterlogging and associated damage to crops through aeration stress pests diseases and mechanical damage such as lodging recent research has shown that process based crop models poorly capture the occurrence of waterlogging due to assumptions in soil moisture drainage routines and thus underestimate the negative impacts of excess water supply on crop growth and yields li et al 2019 neglecting these negative consequences of over irrigation may mean that our analysis has understated the effects of uncertainty on crop yields and profits in particular for crops that are sensitive to effects of waterlogging and excess irrigation e g enhanced lodging risks for crops such as wheat nonetheless the more severe cases of over irrigation would have been prevented via the behavioral feedback mechanisms used in this analysis and this point of intervention where a farmer feels that further irrigation would be detrimental to the crop chosen to be field capacity in this case study would likely change and reflect the risks of their specific crop based on their own experience this feedback addition means that impacts of model simplifications of over irrigation effects should be minimized under the assumption of rational farmer behavior conversely for systems that apply significant amounts of water in one event e g flood irrigation the potential for crop stress caused by over irrigation is increased compared to our study of pivot production our analysis does not explicitly consider the effects technical or regulatory constraints to irrigation water use on the value of more accurate soil moisture information in reality policies constraining seasonal or intraseasonal rates of water use ifft et al 2018 or physical limits on water use e g due to declining well yields foster et al 2015 may both increase incentives for farmers to avoid over irrigation and thus increase the value of implementing optimal irrigation strategies combined with these limits characteristics of the delivery system will impact which days and how much water can be applied in each event beyond the simple maximum daily limit considered in our analysis o brien et al 1998 such restrictions and extraneous variables e g a seasonal cap on irrigation may increase the value of information as there is an additional cost of exceeding this cap on the other hand if irrigation events are largely determined by well yields water delivery systems energy regulators and weather forecasts then the number of mis timed irrigation events due solely to inaccurate soil moisture measurements will be low in either case we expect the value of improved irrigation strategies to move in the same direction as the value of soil moisture information as more freedom over the timing and quantity of water applied allows the farmer to better adjust irrigation to match crop water needs the results of our illustrative example are predicated on the ability of the underlying crop water model aquacrop os to accurately simulate crop responses to soil water deficits the model has been calibrated and extensively applied to simulate irrigated and rainfed maize production in our study area and wider north american farming systems heng et al 2009 foster et al 2015 sandhu and irmak 2019 however to ensure that our results are not being driven by any remaining uncertainty in either water stress thresholds or response parameters a sensitivity analysis was therefore performed in this sensitivity analysis we artificially altered the stress response parameters in aquacrop os setting each value to 1 to maximize impacts of any short duration periods of water stress on crop growth and yields we then repeated our analysis described in section 3 1 following methods outlined in section 2 water use and profits from this analysis showed the same non linear response to water flux and soil texture uncertainty as reported earlier in section 3 a 10 standard error in both soil texture and water flux inputs led to less than 1 change in water use and less than 5 change in profits figure s8 this sensitivity analysis demonstrates that the results presented in this article are highly unlikely to be an artifact of any inability within aquacrop to simulate the severity of short duration water stress impacts and we expect that qualitatively identical results would therefore be obtained with alternative more biophysically complex crop models e g hybrid maize yang et al 2004 a key reason for this result is that while the model now responds more strongly to water stress optimal soil moisture targets are also adjusted to 64 73 39 0 soil water holding capacity and thereby greatly increasing the headroom above the critical crop stress thresholds finally our results are specific to the particular illustrative application chosen in this study irrigated maize production in the central united states nonetheless we expect that our general qualitative finding that choice of irrigation scheduling rules is more important than the accuracy of data these rules are implemented based on will apply more generally to other crops and regions in particular our sensitivity analysis described above suggests that even for more drought sensitive crops e g soft fruits perfect information may not be necessary to achieve near optimal water efficiency and profits so long as farmers baseline irrigation heuristics are effectively representative of crop sensitivity to water deficits conversely higher crop prices are likely to enhance the value of improved soil moisture information as any yield losses will have a larger economic impact for farmers the limited sensitivity of crop yields to measurement errors shown in sections 3 1 and 3 2 suggest that changes in crop price would have minimal impact on economic losses caused by soil moisture uncertainty this is in contrast with the larger effects of water prices and risk aversion that are directly affected by the greater magnitude of change in irrigation water use with measurement error nonetheless we acknowledge that variations in crop yields may have a larger influence on the value of improved soil moisture information for example where a farmer s baseline irrigation strategy is sub optimal or for crops that respond strongly to either soil water deficits or saturation analyzing such alternate production environments is beyond the scope of this paper but will be an important area for future research building on the results presented here 5 conclusions effective irrigation scheduling is essential for improving agricultural water productivity and managing the negative impacts of agricultural water abstractions on other water users and the environment therefore technology companies and researchers have focused on developing technologies such as soil moisture sensors that improve the quality of farmers soil moisture information implicit to this approach is an assumption that more accurate information is essential to enable efficient near optimal irrigation decisions however this assumption neglects the contribution of farmers choice of irrigation strategy which may have an equally significant or greater impact on overall efficiency and profitability of agricultural water use in this article we have developed a framework to assess the impacts of increasing uncertainty on water use and farm profits measurement errors in water fluxes and soil texture lead to a divergence between the farmers perception of soil moisture and the true soil water status resulting in sub optimal irrigation decisions and reduced profitability however our results show that the magnitude of these impacts are small with a 30 standard error in water flux and soil texture measurements which is much larger than the likely real world measurement uncertainty in these quantities particularly precipitation and irrigation depth resulting in only a 4 increase in average water use meaning that near optimal irrigation decisions can be made without perfect information contrastingly we demonstrate that the choice of irrigation scheduling strategy has a larger impact on water use and profits than soil moisture uncertainty our analysis suggests that efforts to improve irrigation water efficiency should therefore focus primarily on helping farmers to evaluate and develop improved irrigation scheduling strategies for their specific production settings where existing irrigation management strategies are poorly aligned with local agronomic and biophysical conditions the potential gains in water use productivity and profitability may be large and further enhance the potential benefits from complementary efforts to improve quality of soil moisture information using new forms of low cost and scalable sensing technologies credit authorship contribution statement t d kelly conceptualization methodology software formal analysis writing original draft t foster conceptualization methodology supervision software writing review editing visualization david m schultz conceptualization supervision writing review editing t mieno conceptualization methodology writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank the anonymous reviewers for comments that have improved the manuscript the work contained in this article was funded by the national environmental research council s understanding the earth atmosphere and ocean doctoral training programme grant ne l002469 1 data generated by the developed framework and code can be found at http doi org 10 5281 zenodo 4041476 supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2021 103982 appendix supplementary materials image application 1 
283,hydraulic properties of natural fractures are essential parameters for the modeling of fluid flow and transport in subsurface fractured porous media the cubic law based on the parallel plate concept has been traditionally used to estimate the hydraulic properties of individual fractures this upscaling approach however is known to overestimate the fractures hydraulic properties dozens of methods have been proposed in the literature to improve the accuracy of the cubic law the relative performance of these various methods is not well understood in this work a comprehensive review and benchmark of almost all commonly used cubic law based approaches in the literature covering 43 methods is provided we propose a new corrected cubic law for incompressible single phase laminar flow through rough walled fractures the proposed model incorporates corrections to the hydraulic fracture aperture based on the flow tortuosity and local roughness of the fracture walls we identify geometric rules relative to the local characteristic of the fracture and apply an efficient algorithm to subdivide the fracture into segments accordingly high resolution simulations for navier stokes equations computed in parallel for synthetic fractures with various ranges of surface roughness and apertures are then performed the numerical solutions are used to assess the accuracy of the proposed model and compare it with the other 43 approaches where we demonstrate its superior accuracy the proposed model retains the simplicity and efficiency of the cubic law but with pronounced improvement to its accuracy the data set used in the benchmark including more than 7500 fractures is provided in open access keywords natural fractures modeling fractured reservoirs cubic law hydraulic aperture fracture upscaling 1 introduction modeling fluid flow and solute transport in fractured porous media manifest in a wide spectrum of applications ranging from oil extraction subsurface water management geological carbon storage nuclear waste disposal geothermal energy extraction among others grant 2013 nelson 2001 fractures represent discontinuities in the porous rock and often introduce distinct static properties and flow characteristics the validity of the mathematical models and hydrodynamic properties describing the flow in the continuum is determined by the size of a characteristic volume known as the representative element volume rev bear 2013 bachmat and bear 1987 depending on the nature of the fractured formation and the problem of interest three distinct conceptual models are typically adopted known as the equivalent single porosity dual porosity and discrete fracture models the first model assumes a homogenized fluid continuum in the fractures and the host rock this simple model avoids the need for explicit evaluation of the fracture properties however its applicability is narrow the other two models consider separate fluid continua within the matrix and the intrinsic void space in the fractures and therefore require an explicit assessment of the fracture hydraulic properties chen et al 2018 jiang et al 2017 naimi tajdar et al 2007 rodríguez de castro et al 2020 guo et al 2020 yang et al 2019 huang et al 2018 fractures manifest in 3d space within the host medium but for practical reasons they are simplified to 2d surfaces with conceptual homogenized apertures flemisch et al 2018 hoteit and firoozabadi 2008 glser et al 2017 aene et al 2017 in laminar flow the hydraulic properties of the fractures can be measured and computed from the pressure drop and the corresponding flow rate at steady state using the parallel plate concept also known as the cubic law snow 1965 1969 traditionally discrete rock fractures have been conceptualized as two smooth parallel plates with a constant effective aperture with this idealized assumption an analytical integration of the navier stokes equations results in the cubic law where the volumetric flow rate is proportional to the cube of the fracture aperture basha and el asmar 2003 brown et al 1995 brush and thomson 2003 zimmerman and yeo 2000 the cubic law cl is widely used in modeling flow and transport in fractured formation in many disciplines due to its simplicity and efficiency however real fractures are often formed of surfaces with anisotropic roughness and varying aperture the cl often leads to significant errors in the estimated flow conductivity various methods have been proposed in the literature to modify the cl to account for the deviation caused by the non ideality of the fracture geometric properties konzuk and kueper 2004 novakowski and lapcevic 1994 zimmerman and bodvarsson 1996 these methods can be categorized into two modeling groups the first group is based on a modified cubic law mcl by which correction factors related to the fracture aperture and surface roughness are considered for instance various definitions of the fracture effective aperture have been adopted in the literature including arithmetic geometric harmonic volume average means among others piggott and elsworth 1993 renshaw 1995 unger and mase 1993 waite et al 1999 other correction factors are incorporated to account for the effects of fracture roughness and flow tortuosity loizzo et al 2010 wang et al 2015 xiao et al 2013 most of the mentioned models however neglect local scale flow behavior and therefore introduce inaccuracy when dealing with fractures with significant roughness and aperture variations oron and berkowitz 1998 concluded that the cl assumptions could be valid locally within certain fracture sections as long as both the local roughness and the geometric aspect ratio are significantly less than one wang et al 2018 accounted for flow tortuosity and normal aperture effects using local cell discretization and introduced the concept of the effective normal aperture however their assumption to estimate the flow direction within local cells is not precise when the aspect ratio conditions are not honored moreover the proposed correction factor accounts for the roughness of each fracture wall separately which is inadequate to capture the combined effect of wall roughnesses such as in the case of fracture shearing the second modeling group is based on the local cubic law lcl based this approach accounts for the spatial variability in fracture aperture by assuming local applicability of the cubic law and solving the reynolds equation nicholl et al 1999 ju et al 2019 the accuracy of the lcl models is subject to the non orthogonality effect of local cells reflecting the tortuosity of the fracture real fractures often exhibit a tortuous behavior and abrupt variability in the aperture which restricts the applicability of the lcl based approach brown et al 1998 zimmerman and yeo 2000 oron and berkowitz 1998 many studies discussed the limitations of lcl based models and proposed improvements for instance mourzenko et al 1995 proposed the concept of the normal aperture approximated from the diameter of the largest sphere that can fit within the fracture walls at a given point this method however is sensitive to the roughness of the fracture surfaces which makes it impractical ge 1997 proposed an alternative approach using a local coordinate system to account for the flow tortuosity where the fracture aperture is measured locally from the normal line to the fracture centerline however for rough walled fractures this approach is sensitive to small fluctuations mallikamas and rajaram 2010 presented an improved 2d depth integrated reynolds equation that accounts for the effect of mid surface tortuosity both along and perpendicular to the flow path wang et al 2015 developed a modified lcl model by combining a correction for local tortuosity and normal aperture for the quantification of local roughness however the proposed approach relies on solving nonlinear flow equations which makes it impractical he et al 2016 developed a modified reynolds equation taking into account the 3d effect of roughness by adding correction factors relative to the roughness angles in the transverse and longitudinal directions the lcl based models discussed above are computationally inefficient to different extents because of the requirement to solve the flow problems the mcl based methods on the other hand show inaccuracy when dealing with complex fractures in this work we propose a new corrected cubic law ccl for incompressible single phase laminar flow through rough walled fractures the proposed model incorporates modifications to the hydraulic fracture aperture based on the flow tortuosity normal aperture and local roughness of the fracture walls we identify geometric rules relative to the local characteristic of the fracture and apply an algorithm to subdivide the fracture into segments accordingly we performed a comprehensive benchmark including more than 40 different methods from the literature and checked their accuracy for thousands of fracture cases the numerical solutions of the full navier stokes equations were used as reference solutions the numerical solutions are computed in parallel using high resolution grids to guarantee enough accuracy of the numerical solution the paper is organized as follows we first review the concept of the mcl and lcl based models and then detail the different components of the proposed model we introduce a new procedure to subdivide the fracture into segments of different types to capture the local characteristics of the flow based on fracture roughness and tortuosity we then introduce the proposed model for 2d and 3d fractures in section 3 we demonstrate the model accuracy and compare it to 43 other existing mcl based and lcl based models from the literature finally we conclude with a summary and future work 2 cubic law models the volumetric flux of incompressible fluid flow in a laminar regime within a fracture consisting of two parallel and smooth surfaces is given by the cubic law as follows 1 q w μ l a h 3 12 δ p where q is the volumetric flux normal to the flow direction w is the fracture width μ is the fluid viscosity l is the fracture length p is the fluid pressure and a h is the fracture hydraulic aperture the hydraulic aperture is the same as the mechanical aperture in the case of the idealized parallel plate configuration in the context of darcy s law the fracture permeability becomes k f a h 2 12 for a real rock fracture however fracture surfaces are never smooth nor parallel the fracture aperture is always a variable function of space and the surfaces are nonplanar consequently the hydraulic aperture of a real fracture is not well defined various investigators proposed modifications to the cubic law by introducing correction factors to incorporate the effect of surface roughness and fracture tortuosity the modified hydraulic aperture is written in the following general form 2 a h a x y c r c t where a x y represents an average of the mechanical aperture and c r and c t are correction factors for roughness and tortuosity respectively a review listing 25 different cl based models from the literature is shown in table 1 the local cubic law considers the spatial variability in fracture aperture by assuming the piecewise applicability of the cubic law that is 3 a 3 p 0 where a is the vertical local aperture and a 3 corresponds to the local fracture transmissivity like the cl based models the main limitation of the lcl is in defining the local apertures which depend on the fracture roughness tortuosity and flow direction various investigators proposed modifications to the lcl by introducing correction factors for the aperture defined similarly to the one in eq 2 a review listing 17 different lcl based models from the literature is shown in table 1 3 proposed model the objective of this model is to estimate the hydraulic properties of rock fractures with variable aperture and rough surfaces applicable to incompressible single phase laminar flow in this work we focus on fractures with no contact areas that is fully open fractures the case of partially open fractures will be addressed in future work we first introduce our model for fractures represented by 2d cross sections and then discuss its extension to 3d following the general form of the hydraulic aperture shown in eq 2 our proposed corrected cubic law ccl model for a 2d fracture is expressed by 4 a h 2 d a h i i 1 n i a h i i n i 1 n i n i i in the above equation a 2d cross section of a fracture is subdivided into segments i of two types segments with essentially parallel surfaces type i with effective hydraulic apertures a i i 1 n i and segments with non parallel surfaces type ii with effective apertures a i i n i 1 n i n i i see fig 1 in eq 4 n i 0 and n i i 0 are the number of segments of type i and type ii respectively we denote by an average operator to be defined later the proposed model procedure includes four main steps 1 subdivide the fracture into segments of type i and ii according to specific geometric rules 2 determination of flow tortuosity and normal apertures 3 quantify local roughness and 4 develop the model for 2d fractures and extend it to 3d 3 1 fracture segmentation consider a 2d cross section of a rough walled fracture to determine its hydraulic aperture we first subdivide the fracture into non uniform segments corresponding to two types type i and type ii type i corresponds to the segments whose fracture surfaces are essentially parallel and exhibit specific geometric criteria to be detailed later type ii corresponds to the other segments that are not of type i that is those segments with fracture surfaces that are not parallel fig 1 illustrates an example of a fracture subdivided into four segments from which three are of type i labeled by a c and d and one of type ii labeled by b the subdivision and classification of the local fracture segments are conditioned to some geometric characteristics for which we estimate the local hydraulic apertures we adopt the dimensionless groups based on geometric aspect ratio similar to the ones introduced by oron and berkowitz 1998 to assess the applicability of poiseuille flow for a segment i these criteria given by δ i and ε i are defined in terms of the average fracture aperture a n i its length d f i and the standard deviations of the roughness of the upper and lower surfaces σ u i and σ l i such that 5 δ i a n i 2 d f i δ 6 ε i m a x 2 σ u i a n i 2 σ l i a n i ε oron and berkowitz 1998 showed that when the dimensionless parameters δ and ε are sufficiently less than one δ 1 and ε 1 the navier stokes solution for laminar flow is accurately approximated by the poisseille solution therefore the fracture segment with rough walls can be represented by two parallel surfaces corresponding to average configurations of the upper and lower walls the velocity profile across the fracture aperture is essentially parabolic and the direction of the flow streamlines follows the centerline of the fracture segment nevertheless the selection and classification of such segments require determination of some optimum thresholds for δ and ε in this work we use the constraints in eqs 5 and 6 to subdivide the fracture into type i and type ii segments we investigate the ranges of δ and ε corresponding to the applicability of the poiseuille solution selecting suitable limits is crucial for the accuracy of our proposed algorithm the geometric aspect ratio δ determines the deviation of flow streamlines from the fracture centerline the local roughness controls the effective fracture aperture relative to the mechanical aperture the poiseuille flow holds when δ is much smaller than one therefore it is necessary to determine an appropriate upper limit for δ so that the constraints will not be too tight and the segments in type i can be selected with an acceptable error in this work we found that the optimum upper limit of δ is about 0 4 which is in agreement with the perturbation solutions derived from the ns equations basha and el asmar 2003 as the poiseuille flow is assumed valid within each segment of type i the velocity profile is parabolic across the fracture aperture consequently the contribution of the volumetric flux from the flow streamlines in the vicinity of the fracture wall near wall zone is insignificant compared to the total flux oron and berkowitz 1998 see fig 2 left fig 2 right shows that 25 of the fracture opening next to the walls accounts only for 10 of the total flow therefore the exact form of the surface roughness of the walls is secondary as long as they do not go beyond the near wall zone and approach the fracture center oron and berkowitz 1998 nevertheless a correction factor related to the suppression of local roughness is still required as discussed later in section local roughness our numerical assessment for the impact of δ and ε on the calculation error showed that the optimum selection for δ and ε corresponds to an arc shape region as depicted in fig 3 therefore different combinations for δ and ε within that region give roughly the same accuracy in our model these parameters are fixed at δ 0 4 and ε 0 2 we present a new procedure to subdivide 2d fractures into segments of types i and ii based on the geometric criteria given in eqs 5 and 6 we develop a multi linear regression mlr algorithm to subdivide the fracture and determine the parallel trends approximating the roughed fracture surfaces the algorithm scans the topology of the fracture surfaces defined by their coordinates it detects the longest segments that honor the constraints given in eqs 5 and 6 the details of the algorithm are provided in appendix a fig 4 shows an example of a segmented fracture produced by the mlr algorithm in which the parallel red lines correspond to segments of type i and the dotted blue points are of type ii 3 2 tortuosity and normal aperture in a discrete fracture flow tortuosity is defined as the actual flow path length relative to the nominal fracture length measured in the direction of the pressure gradient walsh and brace 1984 we define the geometric flow tortuosity τ i for a segment i as the ratio of flow direction distance d f i and the straight line distance d s i see fig 5 such that 7 τ i d f i d s i for type i segments d f i is the length of the centerline of the segments parallel sides and a n i is the normal aperture corresponding to the normal distance between the parallel sides see fig 5a for type ii segments however the flow streamlines flow direction are not parallel to the segment sides and therefore d f i does not correspond to the length of the segment centerline fig 5b the selection of the flow direction also impacts the normal aperture in fig 5b we define a n i as the commonly used aperture based on the segment centerline and a n i as the proposed alternative aperture based on a corrected flow direction as detailed below the flow direction in type i segments correspond to the segment centerline we are interested in approximating the flow direction for type ii segments consider a segment i as shown in fig 5b depending on the segment geometrical configuration within its neighboring segments the actual flow streamlines may not be aligned with the fracture centerline for instance in fig 6 b andc the aspect ratio δ i see eq 5 is small enough resulting in streamlines that are aligned with the centerline of the segment in this case the flow direction in i is independent of the configurations of its neighboring segments however in the other two cases fig 6a and d δ i is too large resulting in streamlines that deviate from the centerline therefore the commonly used approximation by ge 1997 of the flow direction by the centerline is not always valid we introduce a flow direction correction for type ii segments based on the aspect ratio δ i for each segment i when δ i 1 2 is satisfied the flow direction defined by γ i is aligned with the centerline otherwise it is corrected based on the geometry of the adjacent segments as follows 8 γ i γ i δ i 1 2 h a r m m e a n γ l l l γ i l i l l l i γ r l r γ i l i l r l i δ i 1 2 in the above equation γ i represents the corrected flow direction based on the geometry of the neighboring segments it is a function of the inclination angles γ l γ i γ r and the straight line distances l l l i l r of the left segment and right segment respectively as depicted in fig 7 following the determination of the corrected flow direction the normal aperture a n i for type i and type ii segments is calculated in terms of the effective vertical aperture a v i see figs 5 and 7 as follows 9 a n i λ i a v i where 10 λ i cos α i i type i 1 1 cos α i 1 cos β i 1 cos γ i α i 1 cos γ i β i i type ii and the effective vertical aperture a v i is defined by 11 a v i a v i i type i 2 a v i 1 2 2 a v i 1 2 2 a v i 1 2 a v i 1 2 1 3 i type ii a detailed derivation for the effective vertical aperture shown in the above equation is given in appendix b 3 3 local roughness as previously discussed when the geometric conditions in eqs 5 and 6 are satisfied type i rough walled segments can be approximated by segments with two parallel straight lines the error of this approximation increases with roughness as expected wang et al 2018 introduced the concept of effective normal aperture by including the effect of local roughness however their proposed correction factor accounts for the roughness of each fracture wall separately which is inadequate to capture the effect of the combined roughness of both walls this issue is highlighted in fig 8 left where we show four fractures with the same normal aperture and same average roughness but exhibit significantly different flow behavior the method of wang et al 2018 predicts the same affective normal apertures for all cases which is inaccurate as shown in fig 8 right in this work we introduce a different correction factor to quantify the roughness effect of the combined fracture walls as follows 12 a h i η i a n i where 13 η i 1 2 s i a n i i type i 1 i type ii in the above equations a h i is the proposed hydraulic aperture of the fracture segment a n i is the normal aperture given in eq 9 and s i is the standard deviation quantifying the overall segment roughness defined by 14 s i 1 n i 3 j 1 n i h j h j where n i is the number of data points defining the wall lines within segment i and h j at a point j is the actual mechanical aperture and h j is the aperture based on the parallel lines representing the fractures walls and shown in fig 8 left fig 8 right demonstrates the improved accuracy of the calculated effective normal aperture from eq 12 against the one obtained from wang et al s method compared to the reference ns simulations 3 4 corrected cubic law we first describe the proposed corrected cubic law for 2d fractures cross sections followed by the extension to 3d we integrate the different procedures described previously which includes the subdivision of a 2d fracture into segments of type i and type ii the flow direction distance d f of a 2d fracture is defined by the sum of local flow direction distances of all segments of type i and type ii such that 15 d f i 1 n i n i i d f i i 1 n i n i i τ i d s i where τ i d s i and d f i are given in eq 7 similarly we define the total fracture length l i 1 n i n i i l i and the macroscopic flow tortuosity τ by 16 τ d f l by combining the effect of roughness and tortuously for each segment the final hydraulic aperture for a 2d fracture cross section a h 2 d is defined by the weighted harmonic average of all segments such that 17 a h 2 d d f i 1 n i n i i d f i a h i 3 1 3 where a h i η i a n i η i λ i a v i is the local hydraulic aperture of each segment i defined in eqs 12 and 9 which incorporates the correction factors η i and λ i for roughness and tortuosity respectively to extend the model for 3d we subdivide the 3d fracture into a set of 2d cross sections following the direction of the streamlines in this work we focus on fractures that are fully open that is fractures with no contact areas therefore the flow streamlines are essentially parallel to the direction of the pressure gradient the proposed model for 3d fractures becomes 18 a h 3 d a h 2 d 3 1 3 where a h 3 d denotes the hydraulic aperture for the 3d fracture and is the arithmetic average operator applied to the cube of a h 2 d relative to all 2d cross sections herein we refer to the hydraulic aperture for discrete individual fracture when it comes to crossing fracture the fracture will be treated individually to calculate the hydraulic aperture for each one following the above procedures 4 model verification 4 1 fracture generation we demonstrate the prediction accuracy and general applicability of the proposed ccl model by applying it to thousands of synthetic fractures we included fractures with variable tortuosity ranging from 1 01 to 1 34 and with relative roughness ranging from 2 2 to 6 9 the tortuosity is defined in eq 16 and the relative roughness is defined as the ratio of arithmetic mean to standard deviation of the aperture field brown 1987 the 3d fractures were generated using synfrac a software designed to mimic natural fractures in terms of geometric properties including roughness tortuosity and aperture ogilvie et al 2006 we used the joint roughness coefficient jrc introduced by jang et al 2014 to rank the fracture surfaces in terms of roughness which varied between 8 and 35 this range was chosen to cover comprehensive scenarios of natural fracture the 2d fractures were selected as cross sections of the 3d fractures random perturbations were also applied on the fracture surfaces to generate cases with an additional range of tortuosity and relative roughness fig 9 left shows the aperture fields of three fractures in 3d with different roughness corresponding to jrc 8 top jrc 20 middle and jrc 35 bottom from each 3d fracture we show fig 9 right three cross sections representing 2d fractures which highlight the degree of roughness 4 2 reference solutions the applicability of the full ns equation and the accuracy of its numerical solution to estimate the hydraulic properties of rough walled fractures are well established brown et al 1995 brush and thomson 2003 oron and berkowitz 1998 zimmerman and yeo 2000 typical ns computation is excessive which renders these simulations impractical in field applications nonetheless they provide useful reference solutions to verify less accurate models brush and thomson 2003 zimmerman and bodvarsson 1996 in our study the solutions from high resolution ns simulations in 2d and 3d are considered as the reference solutions and used for the other models verification and benchmarking we solve the full physics ns equation for single phase fluid flow using the mixed finite element mfe method in fenics a public domain platform logg et al 2012 the mfe method is known for its superiority to other traditional finite difference and finite element methods durlofsky 1994 fahs et al 2015 hoteit and firoozabadi 2008 mosé et al 1994 a brief description of the numerical method is presented in appendix c in single phase flow the gravity effect is irrelevant and no flow and no slip boundary conditions are assigned to the fracture walls a pressure gradient of 1 p a m is applied between the inlet and outlet of the fracture fluid properties for water are given by density ρ 1000 k g m 3 and viscosity μ 0 001 p a s mesh sensitivity was conducted by doubling the number of mesh elements until the difference between two successive simulation results were less than 0 1 the simulation domain for 2d fractures is discretized into approximately 0 25 million triangular cells with a cell size about 0 001 m m fig 10 bottom for 3d fractures the domain is discretized into around 10 million tetrahedral elements where the mesh density is around 800 cells per m m 3 fig 10 top the whole process is developed to run in parallel on a supercomputer the algorithm automates the process to generate the 2d fracture cross sections create the meshes run the steady state ns simulations and calculate the hydraulic apertures we define the relative deviation of the calculated hydraulic apertures d m to assess the accuracy of the proposed model relative to the reference ns solutions and to benchmark it with the other cl based and lcl based models for an approximation model m defined in table 1 d m is defined by 19 d m a m a n s a n s 100 in the above equation a m and a n s are the hydraulic apertures calculated from a model m see table 1 and from the ns solution respectively 5 results and discussions to assess the accuracy of the proposal ccl model we benchmark it with 43 other cl based and lcl based models described in table 1 details of these models are given in the referred papers the accuracy of the models depends on the degree of roughness and tortuosity first we benchmark all the models and compare their prediction accuracy for more than 7500 fractures with various levels of complexity the ns solutions are used as reference solutions the database of the used fracture models and ns simulation results are provided as discussed in appendix e fig 11 shows examples of ns solutions represented by the velocity profiles and flow streamlines for two fractures with low fig 11a and high fig 11b roughness corresponding to jrc 8 and jrc 35 respectively with high roughness cells with stagnant flow may occur fig 11b the deviation of flow pathways from the straight line distance between the fracture inlet and outlet reflects the concept of tortuosity the average velocity varies significantly through the fracture due to the variable apertures fig 12 shows the error distributions of the calculated hydraulic apertures from all models applied to more than 7500 fractures each boxplot displays the distribution of the relative errors represented by the minimum lower quartile p25 mean upper quartile p75 and maximum errors the 43 benchmarked models see table 1 are sorted in descending order by the mean of the absolute relative error and standard deviation the proposed model ccl exhibits the best accuracy with an arithmetic mean of 2 9 and standard deviation of 1 8 among the lcl based models model 43 brown et al 1995 offers the second best results with an average mean error of 4 2 and standard deviation of 3 2 among the cl based models model 42 ge 1997 came third with reasonable accuracy within the range of 5 4 followed by the model of xie et al 2015 model 41 and by louis 1969 model 40 with average errors of 6 7 and 6 8 respectively to further investigate the performance behavior of the different models we analyze how the error d m behaves as the fracture roughness and tortuosity vary fig 13 left depicts the trends of the average error d m obtained from all models applied for about 7500 fractures regrouped and plotted versus the relative roughness it is obvious that the accuracy of many models is influenced by the fracture roughness as expected which includes the effect of the standard deviation of fracture apertures relative to the mean aperture most cl based models tend to introduce higher errors when the relative roughness index decreases from 2 5 to 6 5 indicating high roughness and low roughness respectively see table 1 on the other hand the other models which are mostly lcl based models exhibit more consistency in the prediction accuracy regardless of the fracture roughness similarly in fig 13 right we show the trends of the average error d m versus the effective tortuosity almost all models exhibit a significant dependency on fracture tortuosity some models show increasing accuracy as the tortuosity increases while others show a reverse trend the accuracy of the proposed ccl model on the other hand depicted with the solid lines in fig 13 shows good consistency in the behavior of its order of accuracy regardless of roughness and tortuosity the obtained trends are almost independent of the fractures roughness and tortuosity this important feature in the proposed model enhances its range of applicability for smooth and rough walled fractures fig 14 left shows the histograms of the deviation error distributions from the top 8 models in which the proposed ccl shows deviation error ranging from 5 4 p10 to 0 4 with a median of 2 8 the proposed model shows the smallest mean error and the narrowest p10 to p90 range in fig 14 right we show diagonal plots of the hydraulic apertures for about 7500 fractures from the ns simulations y axis and the predicted values form the top 8 models x axis the prediction values falling at the diagonal match the reference solution while the off diagonal values exhibit deviations the proposed ccl shows good consistency with the reference solution for a wide range of fracture apertures with minor under estimation signaled by the lower diagonal points the other top models however show more sparse predictions and many times encounter major deviations we extensively tested the proposed ccl model for 3d fractures from which we detail four cases fig 15 shows the normalized velocity profiles from ns solutions for four fractures labeled as s1 s2 s3 and s4 the fractures are ranked based on their roughness index jrc which is jrc 8 12 16 and 20 respectively the aperture fields range from 0 08 to 2 7 mm with the arithmetic means from 1 08 to 1 29 mm and standard deviations ranging from 0 25 to 0 35 mm we first show the effective hydraulic apertures along the width of the fractures calculated from the distribution of inlet and outlet fluxes besides the pressure drop fig 16 shows the hydraulic apertures along the fracture width from the ccl model compared to the hydraulic apertures obtained from the 3d ns simulations the results show excellent agreements for all the fractures the overall effective apertures for the 3d fractures calculated based on the total inlet and outlet fluxes are also compared to those obtained from the proposed ccl model which also show a good match with an average error around 3 2 as shown in table 2 6 discussion in modeling fractured reservoirs constructing a detailed geological model with the discrete fracture network is a central step in the simulation workflow where the matrix and fracture properties are often populated using stochastic methods narr et al 2006 various upscaling methods are then used to generate the simulation model that could be based on conceptual dual continua discrete fractured or embedded multiscale approaches hajibeygi et al 2011 rodriguez et al 2006 aene et al 2016 karimi fard and durlofsky 2016 fracture characterization including well logging core plugs and sometimes outcrops are acquired and analyzed to determine the fracture density orientation apertures and other data which are critical to constrain the dfn in current workflows fracture permeabilities are estimated using the cubic law based on the observed fracture characteristics in the core plugs and well logs this model is associated with significant inaccuracies where it may overestimate the fracture permeabilities by orders of magnitude as the estimated fracture properties at the wells are used to constrain the geological model any inaccuracies in fracture permeabilities at the well scale are propagated to the model at the field scale the objective of this work is to improve the mentioned workflow by providing a corrected cubic law to estimate the hydraulic aperture of natural fractures the proposed model retains the simplicity of the cubic law and accounts for measured characteristics of fractures including their geometry mechanical aperture and roughness as discussed in the paper the proposed model has been tested for laminar incompressible single phase fluid flow which could be suitable for most flow conditions in fractured systems however there are some situations where this model requires further investigation and improvements for instance at high reynolds numbers or compressible fluid flow such as gas production or injection at high flow velocities occurring near the wellbore the model may overestimate the hydraulic aperture resulting in a non darcy effect further this model does not take into consideration the impact of stress on altering the fracture hydraulic aperture he et al 2020 on the other hand the 3d fractures are assumed to be fully open in partially open fractures with significant fillings and contact areas flow streamlines may exhibit significantly tortuous behavior and therefore they cannot be approximated by direct 2d cross sections research is currently ongoing to address the mentioned limitations to extend the applicability of the presented model 7 summary approximating the effective hydraulic apertures and permeability of discrete natural fractures is relevant for various geoscience applications the inaccuracy of the traditional cubic law in estimating roughed wall fractures is well known dozens of models have been proposed in the literature to improve the accuracy of the cubic law which are often categorized as modified cubic law mcl based and local cubic law lcl based methods a comparative performance study of these models is still lacking in this work we introduced a new corrected cubic law ccl model the proposed model leverages the key strengths of some existing models and introduces new concepts to fix gaps the main findings in this work include the proposed model predicts the hydraulic aperture of 2d and 3d discrete fractures by accounting for the effects of flow direction normal aperture tortuosity and local roughness the model retains the simplicity of the cubic law but with significant improvement in its accuracy a new algorithm is introduced to subdivide 2d fractures into segments of type i and type ii according to geometric rules reflecting the segment aspect ratio and local roughness a new corrected flow direction is proposed for type ii segments which depend on the orthogonality and geometric aspect ratio of the adjacent segments a new model is used to calculate the normal aperture based on the corrected flow direction a new correction factor to account for the local roughness within type i segments is proposed we presented a comprehensive benchmarking 43 existing mcl based and lcl based models where we show the superiority of the proposed model we offer the raw data of our testing dataset including more than 7500 fractures with the corresponding ns solutions a detailed description for the dataset and the associated codes are detailed in appendix e hoteit 2021 data availability data sets for this research including structures for 7680 fractures geometric properties and a matlab reader are available via hydroshare fracturedata http www hydroshare org resource dbf1e825bb1c468dab5e47e2a8a10260 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper aknowledgments we like to thank kaust supercomputing lab for supporting numerical simulations running on supercomputer shaheen we would also like to thank saudi aramco for the financial support of this project and for authorizing this work to be published appendix a segmentation algorithm the developed segmentation algorithm for subdividing the 2d fracture into type i and type ii is provided in fig a 17 which is detailed as follow 1 input parameters including the values of the constraints δ and ε steps for the outer loop and inner loop denoted as ς o u t e r and ς i n n e r respectively maximum steps that each segment contains and the initial start point i s t a r t 1 2 outer loop aims to check the availability of potential segments based on the fracture topological data mlr algorithm is applied to the potential segment to calculate the values of δ i and ε i the segment is formed if both constraints are honored then we update the i s t a r t with i e n d and resume the checking process for a new potential segment otherwise the process switch into the inner loop 3 inner loop extends the range of potential segments by adding the inner check steps to the end points the new formed potential segment is rechecked in the outer loop when the steps that a potential segment contains exceeds ς m a x we update the start point i s t a r t with i s t a r t 1 to resume the whole process appendix b normal aperture we present the derivation for the normal aperture based on the corrected flow direction the flow direction proposed by ge 1997 follows the orientation of the centerline of segments which has been widely used by many researchers the relationship between the normal aperture a n i blue line and the apparent aperture a v i green line see fig b 18 are given by b 1 a n i a o c a o d 2 a o d 2 a v i cos β i α i 2 1 cos α i 1 cos β i based on eq b1 we have b 2 a o c a o d a v i cos β i α i 2 1 cos α i 1 cos β i the step by step procedure for the calculation of normal aperture a n i see fig b 18 based on the new corrected flow direction is as follow since the normal aperture is perpendicular to the corresponding flow direction in δ o d f b 3 o d f 90 o g d 90 β i α i 2 d o f b o d b o f β i α i 2 γ i interior angles sum up to 180 thus one gets b 4 o f d 180 o d f d o f 90 γ i α i according to the law of sines b 5 a o f sin o d f a o d sin o f d which can be arranged to b 6 a o f a o d sin o d f sin o f d a o d cos β i α i 2 cos γ i α i following the same idea in δ o c e we have b 7 a o e a o c sin o c e sin o e c a o c cos β i α i 2 cos γ i β i then the new normal aperture a n i becomes b 8 a n i a o e a o f a o c cos β i α i 2 cos γ i β i a o d cos β i α i 2 cos γ i α i substituting eq b2 into eq b8 yields b 9 a n i a v i 1 cos α i 1 cos β i 1 cos γ i α i 1 cos γ i β i appendix c mixed finite element formulation consider the steady state of incompressible newtonian laminar flow with no gravity effects the full physics ns equations can be written as c 1 ρ u u p μ 2 u u 0 no slip boundary conditions are assigned to the fracture walls and constant pressures are imposed at the inlet and outlet of fracture creating a pressure gradient see fig c 19 the gradient of velocity in the direction of the pressure gradient is set to zero to guarantee the fully developed flow and to avoid the inlet effect the ns equations can be formulated in a mixed variational from where the velocity and the pressure are approximated simultaneously multiplying eq c1 by the test function v q and integrating the resulting equations over the domain ω yields c 2 ω ρ u u v d x ω p v d x ω μ 2 u v d x ω u q d x 0 applying the integration by parts technique we have c 3 ω p v d x ω p v d x ω p v n d s ω μ 2 u v d x μ ω u v d x μ ω u n v d s using the abstract framework the problem becomes to find u p w such that c 4 a u p v q l v q for all v q w where c 5 a u p v q ω ρ u u v d x ω p v d x μ ω u v d x ω u q d x c 6 l v q ω n p i n l e t v n d s ω n p o u t l e t v n d s the space w should be a mixed function space w v q such that and q q appendix d derivation for hydraulic aperture of the proposed model in this appendix we present the derivation of hydraulic aperture for the proposed ccl model each 2d fracture is divided into a series of segments type i and type ii the original 2d rough fracture is approximated by the segmented fracture fig d 20 analytical solution of the lcl for one dimensional flow in a tapered wedge brown 1984 leading to d 1 a h a 1 3 1 d s 0 d s d x a 3 x 1 3 where d s d s 1 d s 2 d s 3 d s 4 and a x is the vertical aperture function which can be arranged to d 2 a h 0 d s 1 a 3 x d x d s 1 d s 2 a 3 x d x d s 2 d s 3 a 3 x d x d s 3 d s 4 a 3 x d x d s 1 d s 2 d s 3 d s 4 1 3 take the first local cell as an example the geometry of the upper and lower fracture walls become d 3 z u k u x b u z l k l x b l fracture aperture is given by d 4 a x z u z l k u k l x b u b l k x b in first local cell we have d 5 a 1 k 0 b a 2 k d s 1 b integrating a 3 x from 0 to d s 1 yields d 6 0 d s 1 a 3 x d x 0 d s 1 k x b 3 x d x k x b 2 2 k 0 d s 1 a v 1 a v 2 d s 1 2 a v 1 2 a v 2 2 following the same logic eq d2 is derived as d 7 a h a v 1 a v 2 d s 1 2 a v 1 2 a v 2 2 a v 2 a v 3 d s 2 2 a v 2 2 a v 3 2 a v 3 a v 4 d s 3 2 a v 3 2 a v 4 2 a v 4 a v 5 d s 4 2 a v 4 2 a v 5 2 d s 1 d s 2 d s 3 d s 4 1 3 given a fracture with n segments d 8 a h i 1 n a v i a v i 1 d s 1 2 a v i 2 a v i 1 2 i 1 n d s 1 1 3 d s i 1 n a v i a v i 1 d s 1 2 a v i 2 a v i 1 2 1 3 considering the effect of normal aperture local roughness and flow tortuosity effects d 9 a h 2 d d f i 1 n d f i a h i 3 1 3 where a h i η i a n i η i λ i a v i is the local hydraulic aperture of each segment i defined in eqs 12 and 9 which incorporates the correction factors η i and λ i for roughness and tortuosity respectively appendix e fracture data the fractures dataset used in this work is provide in open access dataset to help the community developing and benchmarking other methods and algorithms the data is provided in two matlab files fractureproperties mat and fracturewalls mat hoteit 2021 a reader function reader m with full user description is also provide the fracproperties mat data file includes the geometric properties and the hydraulic apertures for 7680 fractures computed for high resolution ns see fig e 21 while the fracturewalls mat data file contains the top and bottom coordinates of the fracture walls fig e 22 shows eight fractures selected from the database 
283,hydraulic properties of natural fractures are essential parameters for the modeling of fluid flow and transport in subsurface fractured porous media the cubic law based on the parallel plate concept has been traditionally used to estimate the hydraulic properties of individual fractures this upscaling approach however is known to overestimate the fractures hydraulic properties dozens of methods have been proposed in the literature to improve the accuracy of the cubic law the relative performance of these various methods is not well understood in this work a comprehensive review and benchmark of almost all commonly used cubic law based approaches in the literature covering 43 methods is provided we propose a new corrected cubic law for incompressible single phase laminar flow through rough walled fractures the proposed model incorporates corrections to the hydraulic fracture aperture based on the flow tortuosity and local roughness of the fracture walls we identify geometric rules relative to the local characteristic of the fracture and apply an efficient algorithm to subdivide the fracture into segments accordingly high resolution simulations for navier stokes equations computed in parallel for synthetic fractures with various ranges of surface roughness and apertures are then performed the numerical solutions are used to assess the accuracy of the proposed model and compare it with the other 43 approaches where we demonstrate its superior accuracy the proposed model retains the simplicity and efficiency of the cubic law but with pronounced improvement to its accuracy the data set used in the benchmark including more than 7500 fractures is provided in open access keywords natural fractures modeling fractured reservoirs cubic law hydraulic aperture fracture upscaling 1 introduction modeling fluid flow and solute transport in fractured porous media manifest in a wide spectrum of applications ranging from oil extraction subsurface water management geological carbon storage nuclear waste disposal geothermal energy extraction among others grant 2013 nelson 2001 fractures represent discontinuities in the porous rock and often introduce distinct static properties and flow characteristics the validity of the mathematical models and hydrodynamic properties describing the flow in the continuum is determined by the size of a characteristic volume known as the representative element volume rev bear 2013 bachmat and bear 1987 depending on the nature of the fractured formation and the problem of interest three distinct conceptual models are typically adopted known as the equivalent single porosity dual porosity and discrete fracture models the first model assumes a homogenized fluid continuum in the fractures and the host rock this simple model avoids the need for explicit evaluation of the fracture properties however its applicability is narrow the other two models consider separate fluid continua within the matrix and the intrinsic void space in the fractures and therefore require an explicit assessment of the fracture hydraulic properties chen et al 2018 jiang et al 2017 naimi tajdar et al 2007 rodríguez de castro et al 2020 guo et al 2020 yang et al 2019 huang et al 2018 fractures manifest in 3d space within the host medium but for practical reasons they are simplified to 2d surfaces with conceptual homogenized apertures flemisch et al 2018 hoteit and firoozabadi 2008 glser et al 2017 aene et al 2017 in laminar flow the hydraulic properties of the fractures can be measured and computed from the pressure drop and the corresponding flow rate at steady state using the parallel plate concept also known as the cubic law snow 1965 1969 traditionally discrete rock fractures have been conceptualized as two smooth parallel plates with a constant effective aperture with this idealized assumption an analytical integration of the navier stokes equations results in the cubic law where the volumetric flow rate is proportional to the cube of the fracture aperture basha and el asmar 2003 brown et al 1995 brush and thomson 2003 zimmerman and yeo 2000 the cubic law cl is widely used in modeling flow and transport in fractured formation in many disciplines due to its simplicity and efficiency however real fractures are often formed of surfaces with anisotropic roughness and varying aperture the cl often leads to significant errors in the estimated flow conductivity various methods have been proposed in the literature to modify the cl to account for the deviation caused by the non ideality of the fracture geometric properties konzuk and kueper 2004 novakowski and lapcevic 1994 zimmerman and bodvarsson 1996 these methods can be categorized into two modeling groups the first group is based on a modified cubic law mcl by which correction factors related to the fracture aperture and surface roughness are considered for instance various definitions of the fracture effective aperture have been adopted in the literature including arithmetic geometric harmonic volume average means among others piggott and elsworth 1993 renshaw 1995 unger and mase 1993 waite et al 1999 other correction factors are incorporated to account for the effects of fracture roughness and flow tortuosity loizzo et al 2010 wang et al 2015 xiao et al 2013 most of the mentioned models however neglect local scale flow behavior and therefore introduce inaccuracy when dealing with fractures with significant roughness and aperture variations oron and berkowitz 1998 concluded that the cl assumptions could be valid locally within certain fracture sections as long as both the local roughness and the geometric aspect ratio are significantly less than one wang et al 2018 accounted for flow tortuosity and normal aperture effects using local cell discretization and introduced the concept of the effective normal aperture however their assumption to estimate the flow direction within local cells is not precise when the aspect ratio conditions are not honored moreover the proposed correction factor accounts for the roughness of each fracture wall separately which is inadequate to capture the combined effect of wall roughnesses such as in the case of fracture shearing the second modeling group is based on the local cubic law lcl based this approach accounts for the spatial variability in fracture aperture by assuming local applicability of the cubic law and solving the reynolds equation nicholl et al 1999 ju et al 2019 the accuracy of the lcl models is subject to the non orthogonality effect of local cells reflecting the tortuosity of the fracture real fractures often exhibit a tortuous behavior and abrupt variability in the aperture which restricts the applicability of the lcl based approach brown et al 1998 zimmerman and yeo 2000 oron and berkowitz 1998 many studies discussed the limitations of lcl based models and proposed improvements for instance mourzenko et al 1995 proposed the concept of the normal aperture approximated from the diameter of the largest sphere that can fit within the fracture walls at a given point this method however is sensitive to the roughness of the fracture surfaces which makes it impractical ge 1997 proposed an alternative approach using a local coordinate system to account for the flow tortuosity where the fracture aperture is measured locally from the normal line to the fracture centerline however for rough walled fractures this approach is sensitive to small fluctuations mallikamas and rajaram 2010 presented an improved 2d depth integrated reynolds equation that accounts for the effect of mid surface tortuosity both along and perpendicular to the flow path wang et al 2015 developed a modified lcl model by combining a correction for local tortuosity and normal aperture for the quantification of local roughness however the proposed approach relies on solving nonlinear flow equations which makes it impractical he et al 2016 developed a modified reynolds equation taking into account the 3d effect of roughness by adding correction factors relative to the roughness angles in the transverse and longitudinal directions the lcl based models discussed above are computationally inefficient to different extents because of the requirement to solve the flow problems the mcl based methods on the other hand show inaccuracy when dealing with complex fractures in this work we propose a new corrected cubic law ccl for incompressible single phase laminar flow through rough walled fractures the proposed model incorporates modifications to the hydraulic fracture aperture based on the flow tortuosity normal aperture and local roughness of the fracture walls we identify geometric rules relative to the local characteristic of the fracture and apply an algorithm to subdivide the fracture into segments accordingly we performed a comprehensive benchmark including more than 40 different methods from the literature and checked their accuracy for thousands of fracture cases the numerical solutions of the full navier stokes equations were used as reference solutions the numerical solutions are computed in parallel using high resolution grids to guarantee enough accuracy of the numerical solution the paper is organized as follows we first review the concept of the mcl and lcl based models and then detail the different components of the proposed model we introduce a new procedure to subdivide the fracture into segments of different types to capture the local characteristics of the flow based on fracture roughness and tortuosity we then introduce the proposed model for 2d and 3d fractures in section 3 we demonstrate the model accuracy and compare it to 43 other existing mcl based and lcl based models from the literature finally we conclude with a summary and future work 2 cubic law models the volumetric flux of incompressible fluid flow in a laminar regime within a fracture consisting of two parallel and smooth surfaces is given by the cubic law as follows 1 q w μ l a h 3 12 δ p where q is the volumetric flux normal to the flow direction w is the fracture width μ is the fluid viscosity l is the fracture length p is the fluid pressure and a h is the fracture hydraulic aperture the hydraulic aperture is the same as the mechanical aperture in the case of the idealized parallel plate configuration in the context of darcy s law the fracture permeability becomes k f a h 2 12 for a real rock fracture however fracture surfaces are never smooth nor parallel the fracture aperture is always a variable function of space and the surfaces are nonplanar consequently the hydraulic aperture of a real fracture is not well defined various investigators proposed modifications to the cubic law by introducing correction factors to incorporate the effect of surface roughness and fracture tortuosity the modified hydraulic aperture is written in the following general form 2 a h a x y c r c t where a x y represents an average of the mechanical aperture and c r and c t are correction factors for roughness and tortuosity respectively a review listing 25 different cl based models from the literature is shown in table 1 the local cubic law considers the spatial variability in fracture aperture by assuming the piecewise applicability of the cubic law that is 3 a 3 p 0 where a is the vertical local aperture and a 3 corresponds to the local fracture transmissivity like the cl based models the main limitation of the lcl is in defining the local apertures which depend on the fracture roughness tortuosity and flow direction various investigators proposed modifications to the lcl by introducing correction factors for the aperture defined similarly to the one in eq 2 a review listing 17 different lcl based models from the literature is shown in table 1 3 proposed model the objective of this model is to estimate the hydraulic properties of rock fractures with variable aperture and rough surfaces applicable to incompressible single phase laminar flow in this work we focus on fractures with no contact areas that is fully open fractures the case of partially open fractures will be addressed in future work we first introduce our model for fractures represented by 2d cross sections and then discuss its extension to 3d following the general form of the hydraulic aperture shown in eq 2 our proposed corrected cubic law ccl model for a 2d fracture is expressed by 4 a h 2 d a h i i 1 n i a h i i n i 1 n i n i i in the above equation a 2d cross section of a fracture is subdivided into segments i of two types segments with essentially parallel surfaces type i with effective hydraulic apertures a i i 1 n i and segments with non parallel surfaces type ii with effective apertures a i i n i 1 n i n i i see fig 1 in eq 4 n i 0 and n i i 0 are the number of segments of type i and type ii respectively we denote by an average operator to be defined later the proposed model procedure includes four main steps 1 subdivide the fracture into segments of type i and ii according to specific geometric rules 2 determination of flow tortuosity and normal apertures 3 quantify local roughness and 4 develop the model for 2d fractures and extend it to 3d 3 1 fracture segmentation consider a 2d cross section of a rough walled fracture to determine its hydraulic aperture we first subdivide the fracture into non uniform segments corresponding to two types type i and type ii type i corresponds to the segments whose fracture surfaces are essentially parallel and exhibit specific geometric criteria to be detailed later type ii corresponds to the other segments that are not of type i that is those segments with fracture surfaces that are not parallel fig 1 illustrates an example of a fracture subdivided into four segments from which three are of type i labeled by a c and d and one of type ii labeled by b the subdivision and classification of the local fracture segments are conditioned to some geometric characteristics for which we estimate the local hydraulic apertures we adopt the dimensionless groups based on geometric aspect ratio similar to the ones introduced by oron and berkowitz 1998 to assess the applicability of poiseuille flow for a segment i these criteria given by δ i and ε i are defined in terms of the average fracture aperture a n i its length d f i and the standard deviations of the roughness of the upper and lower surfaces σ u i and σ l i such that 5 δ i a n i 2 d f i δ 6 ε i m a x 2 σ u i a n i 2 σ l i a n i ε oron and berkowitz 1998 showed that when the dimensionless parameters δ and ε are sufficiently less than one δ 1 and ε 1 the navier stokes solution for laminar flow is accurately approximated by the poisseille solution therefore the fracture segment with rough walls can be represented by two parallel surfaces corresponding to average configurations of the upper and lower walls the velocity profile across the fracture aperture is essentially parabolic and the direction of the flow streamlines follows the centerline of the fracture segment nevertheless the selection and classification of such segments require determination of some optimum thresholds for δ and ε in this work we use the constraints in eqs 5 and 6 to subdivide the fracture into type i and type ii segments we investigate the ranges of δ and ε corresponding to the applicability of the poiseuille solution selecting suitable limits is crucial for the accuracy of our proposed algorithm the geometric aspect ratio δ determines the deviation of flow streamlines from the fracture centerline the local roughness controls the effective fracture aperture relative to the mechanical aperture the poiseuille flow holds when δ is much smaller than one therefore it is necessary to determine an appropriate upper limit for δ so that the constraints will not be too tight and the segments in type i can be selected with an acceptable error in this work we found that the optimum upper limit of δ is about 0 4 which is in agreement with the perturbation solutions derived from the ns equations basha and el asmar 2003 as the poiseuille flow is assumed valid within each segment of type i the velocity profile is parabolic across the fracture aperture consequently the contribution of the volumetric flux from the flow streamlines in the vicinity of the fracture wall near wall zone is insignificant compared to the total flux oron and berkowitz 1998 see fig 2 left fig 2 right shows that 25 of the fracture opening next to the walls accounts only for 10 of the total flow therefore the exact form of the surface roughness of the walls is secondary as long as they do not go beyond the near wall zone and approach the fracture center oron and berkowitz 1998 nevertheless a correction factor related to the suppression of local roughness is still required as discussed later in section local roughness our numerical assessment for the impact of δ and ε on the calculation error showed that the optimum selection for δ and ε corresponds to an arc shape region as depicted in fig 3 therefore different combinations for δ and ε within that region give roughly the same accuracy in our model these parameters are fixed at δ 0 4 and ε 0 2 we present a new procedure to subdivide 2d fractures into segments of types i and ii based on the geometric criteria given in eqs 5 and 6 we develop a multi linear regression mlr algorithm to subdivide the fracture and determine the parallel trends approximating the roughed fracture surfaces the algorithm scans the topology of the fracture surfaces defined by their coordinates it detects the longest segments that honor the constraints given in eqs 5 and 6 the details of the algorithm are provided in appendix a fig 4 shows an example of a segmented fracture produced by the mlr algorithm in which the parallel red lines correspond to segments of type i and the dotted blue points are of type ii 3 2 tortuosity and normal aperture in a discrete fracture flow tortuosity is defined as the actual flow path length relative to the nominal fracture length measured in the direction of the pressure gradient walsh and brace 1984 we define the geometric flow tortuosity τ i for a segment i as the ratio of flow direction distance d f i and the straight line distance d s i see fig 5 such that 7 τ i d f i d s i for type i segments d f i is the length of the centerline of the segments parallel sides and a n i is the normal aperture corresponding to the normal distance between the parallel sides see fig 5a for type ii segments however the flow streamlines flow direction are not parallel to the segment sides and therefore d f i does not correspond to the length of the segment centerline fig 5b the selection of the flow direction also impacts the normal aperture in fig 5b we define a n i as the commonly used aperture based on the segment centerline and a n i as the proposed alternative aperture based on a corrected flow direction as detailed below the flow direction in type i segments correspond to the segment centerline we are interested in approximating the flow direction for type ii segments consider a segment i as shown in fig 5b depending on the segment geometrical configuration within its neighboring segments the actual flow streamlines may not be aligned with the fracture centerline for instance in fig 6 b andc the aspect ratio δ i see eq 5 is small enough resulting in streamlines that are aligned with the centerline of the segment in this case the flow direction in i is independent of the configurations of its neighboring segments however in the other two cases fig 6a and d δ i is too large resulting in streamlines that deviate from the centerline therefore the commonly used approximation by ge 1997 of the flow direction by the centerline is not always valid we introduce a flow direction correction for type ii segments based on the aspect ratio δ i for each segment i when δ i 1 2 is satisfied the flow direction defined by γ i is aligned with the centerline otherwise it is corrected based on the geometry of the adjacent segments as follows 8 γ i γ i δ i 1 2 h a r m m e a n γ l l l γ i l i l l l i γ r l r γ i l i l r l i δ i 1 2 in the above equation γ i represents the corrected flow direction based on the geometry of the neighboring segments it is a function of the inclination angles γ l γ i γ r and the straight line distances l l l i l r of the left segment and right segment respectively as depicted in fig 7 following the determination of the corrected flow direction the normal aperture a n i for type i and type ii segments is calculated in terms of the effective vertical aperture a v i see figs 5 and 7 as follows 9 a n i λ i a v i where 10 λ i cos α i i type i 1 1 cos α i 1 cos β i 1 cos γ i α i 1 cos γ i β i i type ii and the effective vertical aperture a v i is defined by 11 a v i a v i i type i 2 a v i 1 2 2 a v i 1 2 2 a v i 1 2 a v i 1 2 1 3 i type ii a detailed derivation for the effective vertical aperture shown in the above equation is given in appendix b 3 3 local roughness as previously discussed when the geometric conditions in eqs 5 and 6 are satisfied type i rough walled segments can be approximated by segments with two parallel straight lines the error of this approximation increases with roughness as expected wang et al 2018 introduced the concept of effective normal aperture by including the effect of local roughness however their proposed correction factor accounts for the roughness of each fracture wall separately which is inadequate to capture the effect of the combined roughness of both walls this issue is highlighted in fig 8 left where we show four fractures with the same normal aperture and same average roughness but exhibit significantly different flow behavior the method of wang et al 2018 predicts the same affective normal apertures for all cases which is inaccurate as shown in fig 8 right in this work we introduce a different correction factor to quantify the roughness effect of the combined fracture walls as follows 12 a h i η i a n i where 13 η i 1 2 s i a n i i type i 1 i type ii in the above equations a h i is the proposed hydraulic aperture of the fracture segment a n i is the normal aperture given in eq 9 and s i is the standard deviation quantifying the overall segment roughness defined by 14 s i 1 n i 3 j 1 n i h j h j where n i is the number of data points defining the wall lines within segment i and h j at a point j is the actual mechanical aperture and h j is the aperture based on the parallel lines representing the fractures walls and shown in fig 8 left fig 8 right demonstrates the improved accuracy of the calculated effective normal aperture from eq 12 against the one obtained from wang et al s method compared to the reference ns simulations 3 4 corrected cubic law we first describe the proposed corrected cubic law for 2d fractures cross sections followed by the extension to 3d we integrate the different procedures described previously which includes the subdivision of a 2d fracture into segments of type i and type ii the flow direction distance d f of a 2d fracture is defined by the sum of local flow direction distances of all segments of type i and type ii such that 15 d f i 1 n i n i i d f i i 1 n i n i i τ i d s i where τ i d s i and d f i are given in eq 7 similarly we define the total fracture length l i 1 n i n i i l i and the macroscopic flow tortuosity τ by 16 τ d f l by combining the effect of roughness and tortuously for each segment the final hydraulic aperture for a 2d fracture cross section a h 2 d is defined by the weighted harmonic average of all segments such that 17 a h 2 d d f i 1 n i n i i d f i a h i 3 1 3 where a h i η i a n i η i λ i a v i is the local hydraulic aperture of each segment i defined in eqs 12 and 9 which incorporates the correction factors η i and λ i for roughness and tortuosity respectively to extend the model for 3d we subdivide the 3d fracture into a set of 2d cross sections following the direction of the streamlines in this work we focus on fractures that are fully open that is fractures with no contact areas therefore the flow streamlines are essentially parallel to the direction of the pressure gradient the proposed model for 3d fractures becomes 18 a h 3 d a h 2 d 3 1 3 where a h 3 d denotes the hydraulic aperture for the 3d fracture and is the arithmetic average operator applied to the cube of a h 2 d relative to all 2d cross sections herein we refer to the hydraulic aperture for discrete individual fracture when it comes to crossing fracture the fracture will be treated individually to calculate the hydraulic aperture for each one following the above procedures 4 model verification 4 1 fracture generation we demonstrate the prediction accuracy and general applicability of the proposed ccl model by applying it to thousands of synthetic fractures we included fractures with variable tortuosity ranging from 1 01 to 1 34 and with relative roughness ranging from 2 2 to 6 9 the tortuosity is defined in eq 16 and the relative roughness is defined as the ratio of arithmetic mean to standard deviation of the aperture field brown 1987 the 3d fractures were generated using synfrac a software designed to mimic natural fractures in terms of geometric properties including roughness tortuosity and aperture ogilvie et al 2006 we used the joint roughness coefficient jrc introduced by jang et al 2014 to rank the fracture surfaces in terms of roughness which varied between 8 and 35 this range was chosen to cover comprehensive scenarios of natural fracture the 2d fractures were selected as cross sections of the 3d fractures random perturbations were also applied on the fracture surfaces to generate cases with an additional range of tortuosity and relative roughness fig 9 left shows the aperture fields of three fractures in 3d with different roughness corresponding to jrc 8 top jrc 20 middle and jrc 35 bottom from each 3d fracture we show fig 9 right three cross sections representing 2d fractures which highlight the degree of roughness 4 2 reference solutions the applicability of the full ns equation and the accuracy of its numerical solution to estimate the hydraulic properties of rough walled fractures are well established brown et al 1995 brush and thomson 2003 oron and berkowitz 1998 zimmerman and yeo 2000 typical ns computation is excessive which renders these simulations impractical in field applications nonetheless they provide useful reference solutions to verify less accurate models brush and thomson 2003 zimmerman and bodvarsson 1996 in our study the solutions from high resolution ns simulations in 2d and 3d are considered as the reference solutions and used for the other models verification and benchmarking we solve the full physics ns equation for single phase fluid flow using the mixed finite element mfe method in fenics a public domain platform logg et al 2012 the mfe method is known for its superiority to other traditional finite difference and finite element methods durlofsky 1994 fahs et al 2015 hoteit and firoozabadi 2008 mosé et al 1994 a brief description of the numerical method is presented in appendix c in single phase flow the gravity effect is irrelevant and no flow and no slip boundary conditions are assigned to the fracture walls a pressure gradient of 1 p a m is applied between the inlet and outlet of the fracture fluid properties for water are given by density ρ 1000 k g m 3 and viscosity μ 0 001 p a s mesh sensitivity was conducted by doubling the number of mesh elements until the difference between two successive simulation results were less than 0 1 the simulation domain for 2d fractures is discretized into approximately 0 25 million triangular cells with a cell size about 0 001 m m fig 10 bottom for 3d fractures the domain is discretized into around 10 million tetrahedral elements where the mesh density is around 800 cells per m m 3 fig 10 top the whole process is developed to run in parallel on a supercomputer the algorithm automates the process to generate the 2d fracture cross sections create the meshes run the steady state ns simulations and calculate the hydraulic apertures we define the relative deviation of the calculated hydraulic apertures d m to assess the accuracy of the proposed model relative to the reference ns solutions and to benchmark it with the other cl based and lcl based models for an approximation model m defined in table 1 d m is defined by 19 d m a m a n s a n s 100 in the above equation a m and a n s are the hydraulic apertures calculated from a model m see table 1 and from the ns solution respectively 5 results and discussions to assess the accuracy of the proposal ccl model we benchmark it with 43 other cl based and lcl based models described in table 1 details of these models are given in the referred papers the accuracy of the models depends on the degree of roughness and tortuosity first we benchmark all the models and compare their prediction accuracy for more than 7500 fractures with various levels of complexity the ns solutions are used as reference solutions the database of the used fracture models and ns simulation results are provided as discussed in appendix e fig 11 shows examples of ns solutions represented by the velocity profiles and flow streamlines for two fractures with low fig 11a and high fig 11b roughness corresponding to jrc 8 and jrc 35 respectively with high roughness cells with stagnant flow may occur fig 11b the deviation of flow pathways from the straight line distance between the fracture inlet and outlet reflects the concept of tortuosity the average velocity varies significantly through the fracture due to the variable apertures fig 12 shows the error distributions of the calculated hydraulic apertures from all models applied to more than 7500 fractures each boxplot displays the distribution of the relative errors represented by the minimum lower quartile p25 mean upper quartile p75 and maximum errors the 43 benchmarked models see table 1 are sorted in descending order by the mean of the absolute relative error and standard deviation the proposed model ccl exhibits the best accuracy with an arithmetic mean of 2 9 and standard deviation of 1 8 among the lcl based models model 43 brown et al 1995 offers the second best results with an average mean error of 4 2 and standard deviation of 3 2 among the cl based models model 42 ge 1997 came third with reasonable accuracy within the range of 5 4 followed by the model of xie et al 2015 model 41 and by louis 1969 model 40 with average errors of 6 7 and 6 8 respectively to further investigate the performance behavior of the different models we analyze how the error d m behaves as the fracture roughness and tortuosity vary fig 13 left depicts the trends of the average error d m obtained from all models applied for about 7500 fractures regrouped and plotted versus the relative roughness it is obvious that the accuracy of many models is influenced by the fracture roughness as expected which includes the effect of the standard deviation of fracture apertures relative to the mean aperture most cl based models tend to introduce higher errors when the relative roughness index decreases from 2 5 to 6 5 indicating high roughness and low roughness respectively see table 1 on the other hand the other models which are mostly lcl based models exhibit more consistency in the prediction accuracy regardless of the fracture roughness similarly in fig 13 right we show the trends of the average error d m versus the effective tortuosity almost all models exhibit a significant dependency on fracture tortuosity some models show increasing accuracy as the tortuosity increases while others show a reverse trend the accuracy of the proposed ccl model on the other hand depicted with the solid lines in fig 13 shows good consistency in the behavior of its order of accuracy regardless of roughness and tortuosity the obtained trends are almost independent of the fractures roughness and tortuosity this important feature in the proposed model enhances its range of applicability for smooth and rough walled fractures fig 14 left shows the histograms of the deviation error distributions from the top 8 models in which the proposed ccl shows deviation error ranging from 5 4 p10 to 0 4 with a median of 2 8 the proposed model shows the smallest mean error and the narrowest p10 to p90 range in fig 14 right we show diagonal plots of the hydraulic apertures for about 7500 fractures from the ns simulations y axis and the predicted values form the top 8 models x axis the prediction values falling at the diagonal match the reference solution while the off diagonal values exhibit deviations the proposed ccl shows good consistency with the reference solution for a wide range of fracture apertures with minor under estimation signaled by the lower diagonal points the other top models however show more sparse predictions and many times encounter major deviations we extensively tested the proposed ccl model for 3d fractures from which we detail four cases fig 15 shows the normalized velocity profiles from ns solutions for four fractures labeled as s1 s2 s3 and s4 the fractures are ranked based on their roughness index jrc which is jrc 8 12 16 and 20 respectively the aperture fields range from 0 08 to 2 7 mm with the arithmetic means from 1 08 to 1 29 mm and standard deviations ranging from 0 25 to 0 35 mm we first show the effective hydraulic apertures along the width of the fractures calculated from the distribution of inlet and outlet fluxes besides the pressure drop fig 16 shows the hydraulic apertures along the fracture width from the ccl model compared to the hydraulic apertures obtained from the 3d ns simulations the results show excellent agreements for all the fractures the overall effective apertures for the 3d fractures calculated based on the total inlet and outlet fluxes are also compared to those obtained from the proposed ccl model which also show a good match with an average error around 3 2 as shown in table 2 6 discussion in modeling fractured reservoirs constructing a detailed geological model with the discrete fracture network is a central step in the simulation workflow where the matrix and fracture properties are often populated using stochastic methods narr et al 2006 various upscaling methods are then used to generate the simulation model that could be based on conceptual dual continua discrete fractured or embedded multiscale approaches hajibeygi et al 2011 rodriguez et al 2006 aene et al 2016 karimi fard and durlofsky 2016 fracture characterization including well logging core plugs and sometimes outcrops are acquired and analyzed to determine the fracture density orientation apertures and other data which are critical to constrain the dfn in current workflows fracture permeabilities are estimated using the cubic law based on the observed fracture characteristics in the core plugs and well logs this model is associated with significant inaccuracies where it may overestimate the fracture permeabilities by orders of magnitude as the estimated fracture properties at the wells are used to constrain the geological model any inaccuracies in fracture permeabilities at the well scale are propagated to the model at the field scale the objective of this work is to improve the mentioned workflow by providing a corrected cubic law to estimate the hydraulic aperture of natural fractures the proposed model retains the simplicity of the cubic law and accounts for measured characteristics of fractures including their geometry mechanical aperture and roughness as discussed in the paper the proposed model has been tested for laminar incompressible single phase fluid flow which could be suitable for most flow conditions in fractured systems however there are some situations where this model requires further investigation and improvements for instance at high reynolds numbers or compressible fluid flow such as gas production or injection at high flow velocities occurring near the wellbore the model may overestimate the hydraulic aperture resulting in a non darcy effect further this model does not take into consideration the impact of stress on altering the fracture hydraulic aperture he et al 2020 on the other hand the 3d fractures are assumed to be fully open in partially open fractures with significant fillings and contact areas flow streamlines may exhibit significantly tortuous behavior and therefore they cannot be approximated by direct 2d cross sections research is currently ongoing to address the mentioned limitations to extend the applicability of the presented model 7 summary approximating the effective hydraulic apertures and permeability of discrete natural fractures is relevant for various geoscience applications the inaccuracy of the traditional cubic law in estimating roughed wall fractures is well known dozens of models have been proposed in the literature to improve the accuracy of the cubic law which are often categorized as modified cubic law mcl based and local cubic law lcl based methods a comparative performance study of these models is still lacking in this work we introduced a new corrected cubic law ccl model the proposed model leverages the key strengths of some existing models and introduces new concepts to fix gaps the main findings in this work include the proposed model predicts the hydraulic aperture of 2d and 3d discrete fractures by accounting for the effects of flow direction normal aperture tortuosity and local roughness the model retains the simplicity of the cubic law but with significant improvement in its accuracy a new algorithm is introduced to subdivide 2d fractures into segments of type i and type ii according to geometric rules reflecting the segment aspect ratio and local roughness a new corrected flow direction is proposed for type ii segments which depend on the orthogonality and geometric aspect ratio of the adjacent segments a new model is used to calculate the normal aperture based on the corrected flow direction a new correction factor to account for the local roughness within type i segments is proposed we presented a comprehensive benchmarking 43 existing mcl based and lcl based models where we show the superiority of the proposed model we offer the raw data of our testing dataset including more than 7500 fractures with the corresponding ns solutions a detailed description for the dataset and the associated codes are detailed in appendix e hoteit 2021 data availability data sets for this research including structures for 7680 fractures geometric properties and a matlab reader are available via hydroshare fracturedata http www hydroshare org resource dbf1e825bb1c468dab5e47e2a8a10260 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper aknowledgments we like to thank kaust supercomputing lab for supporting numerical simulations running on supercomputer shaheen we would also like to thank saudi aramco for the financial support of this project and for authorizing this work to be published appendix a segmentation algorithm the developed segmentation algorithm for subdividing the 2d fracture into type i and type ii is provided in fig a 17 which is detailed as follow 1 input parameters including the values of the constraints δ and ε steps for the outer loop and inner loop denoted as ς o u t e r and ς i n n e r respectively maximum steps that each segment contains and the initial start point i s t a r t 1 2 outer loop aims to check the availability of potential segments based on the fracture topological data mlr algorithm is applied to the potential segment to calculate the values of δ i and ε i the segment is formed if both constraints are honored then we update the i s t a r t with i e n d and resume the checking process for a new potential segment otherwise the process switch into the inner loop 3 inner loop extends the range of potential segments by adding the inner check steps to the end points the new formed potential segment is rechecked in the outer loop when the steps that a potential segment contains exceeds ς m a x we update the start point i s t a r t with i s t a r t 1 to resume the whole process appendix b normal aperture we present the derivation for the normal aperture based on the corrected flow direction the flow direction proposed by ge 1997 follows the orientation of the centerline of segments which has been widely used by many researchers the relationship between the normal aperture a n i blue line and the apparent aperture a v i green line see fig b 18 are given by b 1 a n i a o c a o d 2 a o d 2 a v i cos β i α i 2 1 cos α i 1 cos β i based on eq b1 we have b 2 a o c a o d a v i cos β i α i 2 1 cos α i 1 cos β i the step by step procedure for the calculation of normal aperture a n i see fig b 18 based on the new corrected flow direction is as follow since the normal aperture is perpendicular to the corresponding flow direction in δ o d f b 3 o d f 90 o g d 90 β i α i 2 d o f b o d b o f β i α i 2 γ i interior angles sum up to 180 thus one gets b 4 o f d 180 o d f d o f 90 γ i α i according to the law of sines b 5 a o f sin o d f a o d sin o f d which can be arranged to b 6 a o f a o d sin o d f sin o f d a o d cos β i α i 2 cos γ i α i following the same idea in δ o c e we have b 7 a o e a o c sin o c e sin o e c a o c cos β i α i 2 cos γ i β i then the new normal aperture a n i becomes b 8 a n i a o e a o f a o c cos β i α i 2 cos γ i β i a o d cos β i α i 2 cos γ i α i substituting eq b2 into eq b8 yields b 9 a n i a v i 1 cos α i 1 cos β i 1 cos γ i α i 1 cos γ i β i appendix c mixed finite element formulation consider the steady state of incompressible newtonian laminar flow with no gravity effects the full physics ns equations can be written as c 1 ρ u u p μ 2 u u 0 no slip boundary conditions are assigned to the fracture walls and constant pressures are imposed at the inlet and outlet of fracture creating a pressure gradient see fig c 19 the gradient of velocity in the direction of the pressure gradient is set to zero to guarantee the fully developed flow and to avoid the inlet effect the ns equations can be formulated in a mixed variational from where the velocity and the pressure are approximated simultaneously multiplying eq c1 by the test function v q and integrating the resulting equations over the domain ω yields c 2 ω ρ u u v d x ω p v d x ω μ 2 u v d x ω u q d x 0 applying the integration by parts technique we have c 3 ω p v d x ω p v d x ω p v n d s ω μ 2 u v d x μ ω u v d x μ ω u n v d s using the abstract framework the problem becomes to find u p w such that c 4 a u p v q l v q for all v q w where c 5 a u p v q ω ρ u u v d x ω p v d x μ ω u v d x ω u q d x c 6 l v q ω n p i n l e t v n d s ω n p o u t l e t v n d s the space w should be a mixed function space w v q such that and q q appendix d derivation for hydraulic aperture of the proposed model in this appendix we present the derivation of hydraulic aperture for the proposed ccl model each 2d fracture is divided into a series of segments type i and type ii the original 2d rough fracture is approximated by the segmented fracture fig d 20 analytical solution of the lcl for one dimensional flow in a tapered wedge brown 1984 leading to d 1 a h a 1 3 1 d s 0 d s d x a 3 x 1 3 where d s d s 1 d s 2 d s 3 d s 4 and a x is the vertical aperture function which can be arranged to d 2 a h 0 d s 1 a 3 x d x d s 1 d s 2 a 3 x d x d s 2 d s 3 a 3 x d x d s 3 d s 4 a 3 x d x d s 1 d s 2 d s 3 d s 4 1 3 take the first local cell as an example the geometry of the upper and lower fracture walls become d 3 z u k u x b u z l k l x b l fracture aperture is given by d 4 a x z u z l k u k l x b u b l k x b in first local cell we have d 5 a 1 k 0 b a 2 k d s 1 b integrating a 3 x from 0 to d s 1 yields d 6 0 d s 1 a 3 x d x 0 d s 1 k x b 3 x d x k x b 2 2 k 0 d s 1 a v 1 a v 2 d s 1 2 a v 1 2 a v 2 2 following the same logic eq d2 is derived as d 7 a h a v 1 a v 2 d s 1 2 a v 1 2 a v 2 2 a v 2 a v 3 d s 2 2 a v 2 2 a v 3 2 a v 3 a v 4 d s 3 2 a v 3 2 a v 4 2 a v 4 a v 5 d s 4 2 a v 4 2 a v 5 2 d s 1 d s 2 d s 3 d s 4 1 3 given a fracture with n segments d 8 a h i 1 n a v i a v i 1 d s 1 2 a v i 2 a v i 1 2 i 1 n d s 1 1 3 d s i 1 n a v i a v i 1 d s 1 2 a v i 2 a v i 1 2 1 3 considering the effect of normal aperture local roughness and flow tortuosity effects d 9 a h 2 d d f i 1 n d f i a h i 3 1 3 where a h i η i a n i η i λ i a v i is the local hydraulic aperture of each segment i defined in eqs 12 and 9 which incorporates the correction factors η i and λ i for roughness and tortuosity respectively appendix e fracture data the fractures dataset used in this work is provide in open access dataset to help the community developing and benchmarking other methods and algorithms the data is provided in two matlab files fractureproperties mat and fracturewalls mat hoteit 2021 a reader function reader m with full user description is also provide the fracproperties mat data file includes the geometric properties and the hydraulic apertures for 7680 fractures computed for high resolution ns see fig e 21 while the fracturewalls mat data file contains the top and bottom coordinates of the fracture walls fig e 22 shows eight fractures selected from the database 
284,a coupled description of flow and thermal reactive transport is spanning a wide range of scales in space and time which often introduces a significant complexity for the modelling of such processes subsurface reservoir heterogeneity with complex multi scale features increases the modelling complexity even further traditional multiscale techniques are usually focused on the accuracy of the pressure solution and often ignore the transport improving the transport solution can however be quite significant for the performance of the simulation especially in complex applications related to thermal compositional flow the use of an adaptive mesh refinement enables the grid to adapt dynamically during the simulation which facilitates the efficient use of computational resources this is especially important in applications with thermal flow and transport where the region requires high resolution calculations as often localized in space in this work the aim is to develop an adaptive mesh refinement framework for geothermal reservoir simulation the approach uses a multi level connection list and can be applied to fully unstructured grids the adaptivity of the grid in the developed framework is based on a hierarchical connectivity list first the fine scale model is constructed which accurately approximates all reservoir heterogeneity next a global flow based upscaling is applied where an unstructured partitioning of the original grid is created once the full hierarchy of levels is constructed the simulation is started at the coarsest grid grid space refinement criteria is based on the local changes and can be adjusted for specific models and governing physics the multi level connectivity lists are redefined at each timestep and used as an input for the next the developed adaptive mesh refinement framework was implemented in delft advanced research terra simulator which uses the operator based linearization technique the performance of the proposed approach is illustrated for several challenging geothermal applications of practical interest keywords geothermal simulation adaptive mesh refinement unstructured grid darts 1 introduction production development of prospective reservoirs includes the use of various technologies that provide information at many different scales these scales range from core plugs being a few centimeters in size to well logs detecting properties a few meters around the well and to seismic imaging covering a significant volume with limited resolution few meters vertically and 10 s of meters horizontally however time and capital limitations result in sparse direct sampling of reservoir rock and fluid properties this is why the construction of reservoir models through integration of these data using geostatistical reservoir description algorithms has become a crucial step in resource development branets et al 2009 these algorithms conventionally result in fine scale descriptions of reservoir properties porosity permeability on grids of tens of millions of cells christie 1996 an issue of considerable importance is the risk and uncertainty assessment of reservoir performance the uncertainty can be gauged by simulating an ensemble of different geological realizations chen et al 2015 this may require to run thousands of simulations to cover a wide range of parameter variation it is however not computationally feasible or desirable to perform these simulations on the high fidelity fine grid model significantly upscaled models i e the mapping of rock and fluid properties to a coarser resolution are therefore required where these models should ideally be even coarser than typical reservoir simulators which can handle on the order of 10 5 10 6 simulation cells durlofsky 2005 in the presence of more complex physics excessive upscaling may however result in non satisfactory results which necessitate the use of advanced algorithms and solvers to allow for higher resolution grids to be employed cusini et al 2016 traditional multiscale techniques jenny et al 2003 wang et al 2014 developed to solve the elliptic or parabolic pressure equation in sequentially coupled simulations mainly focus on the pressure solution and often ignore the transport however in complex applications related to chemical and compositional eor enhanced oil recovery reservoir storage and geothermal industry the number of conserved chemical species can be large which makes any improvement in transport solution quite significant for the performance and robustness of the simulation a technique called adaptive mesh refinement amr provides an effective means for adapting the resolution of a model to solution requirements this method is well developed in many areas of computational physics e g fluid dynamics and solid mechanics but is however not widely used for practical reservoir simulation karimi fard and durlofsky 2014 in today s literature several researchers have developed and proposed amr procedures to capture the local nature of transport processes bahrainian and dezfuli 2014 have developed a novel unstructured grid generation algorithm which considers the effect of geological features and well locations in the grid resolution this strategy involves the definition and construction of the initial grid based on the geological model geometry adaptation of geological features and grid resolution control trangenstein 2002 used the combination of high resolution discretization methods with dynamically adaptive mesh refinement for a two component single phase model for miscible flooding pau et al 2012 proposed an amr algorithm for compressible two phase flow in porous media the method is implemented within a block structured adaptive mesh refinement framework which allows the grids to dynamically adapt to flow features and enables efficient parallelization of the algorithm the coarse scale permeability was obtained by averaging the fine scale permeability similar techniques have been developed for compositional simulation sammon et al 2003 thermal problems christensen et al 2004 improved enhanced oil recovery processes van batenburg et al 2011 discrete fracture networks berrone et al 2019 and many more applications in this work the aim was to develop a dynamic amr scheme using an unstructured multi level gridding framework for geothermal simulation in complex reservoirs the focus lied particularly on thermal reactive flow and transport formulation which are required for a wide range of subsurface applications relevant to the energy transition including geothermal notice that heterogeneity plays a very important role in geothermal applications shetty et al 2018 babaei and nick 2019 the geothermal doublet lifetime and heat recovery rate usually vary a lot with both reservoir parameters and operational management where uncertainties due to heterogeneity are dominating willems and m nick 2019 heterogeneity in flow path and shale facies play an important role in water heat recharge which directly affects doublet performance at low net to gross ratio crooijmans et al 2016 besides complex heat extraction process and corresponding chemical interactions can also amplify the effect of heterogeneity cui et al 2016 kala and voskov 2020 as a starting point of our framework a fine scale geological model has to be constructed accurately approximating all reservoir heterogeneity in reservoir simulation this model is often represented by an array of volumes depths and a connectivity list lim et al 1995 describing each control volume next a global flow based upscaling was applied and an unstructured partitioning of the original grid was constructed as suggested in karimi fard and durlofsky 2014 this partitioning provides coarser levels of the original model which is also described by an array of volumes depths and a connectivity list a coarser connectivity list includes connections between control volumes at the given level as well as interconnections between the levels once the full hierarchy of levels is constructed the simulation is started at the coarsest grid grid space refinement criterion is developed for particular applications the multi level connection list is reconstructed at each time step and used for the simulation the proposed approach was implemented in delft advanced research terra simulator darts kala and voskov 2020 wang et al 2020 2 methodology 2 1 governing equations general purpose reservoir simulation is based on the solution of governing equations which describe mass and energy transfer of various species in the subsurface the flow of mass and energy in a system with n p phases and n c components are described in this section for this general purpose thermal compositional model n c component mass conservation equations and a single energy conservation equation need to be solved khait and voskov 2018b when chemical reactions occur in the system an additional term describing n k kinetic reactions is added to the mass conservation equation kala and voskov 2020 these governing relations are described as 1 t ϕ p 1 n p x c p ρ p s p div p 1 n p x c p ρ p u p p 1 n p x c p ρ p q p k n k v c k r k c 1 n c k 1 n k 2 t ϕ p 1 n p ρ p s p u p 1 ϕ u r div p 1 n p h p ρ p u p div κ t p 1 n p h p ρ p q p 0 where t is the time v c k is the stoichiometric coefficient associated with kinetic reaction k r k is the rate of kinetic reaction k the right hand side of the mass conservation eq 1 is the kinetic term which describes reactions it is set to zero when no chemical processes are involved in the system the rest of the terms in the system can be described as functions of spatial coordinate ξ and or physical state ω ϕ ξ ω porosity x c p ω the mole fraction of component c in phase p s p ω phase saturation ρ p ω phase molar density u p ξ ω phase velocity q p ξ ω u source of phase p u p ξ phase internal energy u r ξ rock internal energy h p ξ phase enthalpy κ ξ ω thermal conduction an exception is the phase source term which is also dependent on u well control variables the rock internal energy and thermal conduction are assumed to be spatially homogeneous for simplification of the problem meaning that they are characterized as functions of the spatial coordinate ξ only phase flow velocity u p is assumed to follow darcy s law expressed as 3 u p k k r p μ p p p γ p d p 1 n p where k ξ permeability tensor k r p ω relative permeability of phase p μ p ω phase viscosity p p ω pressure in phase p γ p ω gravity vector d ξ depth backward oriented the nonlinear unknowns in this system of equations are the pressure p the overall compositions z c of each component and the enthalpy h 2 2 modeling approach in order to solve the governing eqs 1 and 2 we apply a finite volume discretization on a general unstructured mesh and perform a backward euler approximation in time to both equations where the phase velocities u p are substituted by the darcy relation 3 4 v ϕ p 1 n p x c p ρ p s p n 1 ϕ p 1 n p x c p ρ p s p n δ t l p 1 n p x c p l ρ p l γ p l δ ψ l δ t p 1 n p ρ p x c p q p v δ t k n r v c k r k c 1 n c 5 v ϕ p 1 n p ρ p s p u p 1 ϕ u r n 1 ϕ p 1 n p ρ p s p u p 1 ϕ u r n δ t l p 1 n p h p l ρ p l γ p l δ ψ l γ c l δ t l δ t p 1 n p h p ρ p q p 0 here v is the control volume for which the system is being solved q p q p v is a source of phase p n is the previous time step whereas n 1 is the time step we want to solve for capillarity and gravity are neglected in these equations and a two point flux approximation tpfa with an upstream weighting is applied δ ψ l the phase potential therefore simply becomes the difference in pressure between blocks connected via interface l while δ t l is the temperature difference between these blocks γ p l γ l k r p l μ p l is a phase transmissibility where γ l is the constant geometrical part of the transmissibility involving permeability and geometry of the control volume finally γ c l γ l κ is the thermal conductive transmissibility khait and voskov 2018b this system of equations is solved for each mesh element in time where the unknowns are the composition of the n c components and the pressure for the mass conservation eq 4 and the pressure and enthalpy for the energy eq 5 in general purpose reservoir simulation the solving process requires the linearization of strongly nonlinear governing equations in conventional reservoir simulators a newton raphson based method is typically used for the linearization which solves on each nonlinear iteration a linear system of equations in the following form 6 j ω n ω n 1 ω n r ω n where r is the residual and j is the jacobian which is the derivative of the residual with respect to primary nonlinear unknowns defined at a nonlinear iteration n in this work we use a recently developed approach called operator based linearization obl the main idea of obl is to transform the discretized mass and energy conservation eqs 4 and 5 to an operator form where space dependent ξ and state dependent ω properties of governing equations are separated this provides the opportunity to approximate the representation of the exact physics of a problem through the discretization of the state dependent properties the underlying methodology of obl is explained in more details in voskov 2017 and khait and voskov 2018a b 2 3 connectivity list the proposed amr technique uses the finite volume method fvm for discretization the implementation of the finite volume discretization method to the mass conservation eq 1 requires the evaluation of the flow between two adjacent control volumes in terms of the cell pressures using a two point flux approximation tpfa the flow rate is defined as 7 q i j γ p i j p i p j where q i j flow rate at interface of cells i and j γ p i j phase transmissibility at interface of cells i and j p i pressure of cell i p j pressure of cell j similarly the heat flux between two adjacent control volumes is expressed in terms of thermal transmissibility γ c and is also using a tpfa defined as 8 q i j h γ c i j t i t j where γ c i j is the thermal transmissibility at interface i j t i and t j are the temperatures of cell i and j respectively and q i j h is the heat flux at interface i j to evaluate the flux between two adjacent control volumes a so called connectivity list is constructed i e for each interface between two neighbouring control volumes the indices of these cells are listed together with the transmissibility lim et al 1995 the result is a list with all connection pairs present in the grid a few important points to be noted are each connection consists of only two elements the connection pairs are not repetitive no flow boundaries imply the absence of connections and are hence not listed in the connectivity list fig 1 shows a simple example of a 2d cartesian structured grid with corresponding cell indexing table 1 shows its connectivity list the list is expressed as two arrays cell i and cell j where each column represent a connection pair each pair has an associated interface transmissibility stored in the connectivity list 3 multi level grid generation the adaptivity of the grid in the developed amr scheme is based on a hierarchical representation of connectivity list the simulation grid is composed of several predefined levels representing the same geological properties at different resolutions we start with a fine scale model static geological model which accurately represents all reservoir heterogeneity this grid is defined as level 0 and represents our finest level the modeling grid is defined by a list of control volumes depths reservoir properties all spatially distributed properties required to solve the discretized relations 4 and 5 for each mesh element and a list of connectivity with corresponding transmissibility between neighbouring cells next level 1 is defined where control volumes are constructed by aggregating fine grid cells upscaling is applied to redefine volume depth and reservoir properties at a coarser level a connectivity list with corresponding transmissibility is constructed for this level and inter level connections are defined in addition similarly more levels of coarsening can be constructed a control volume in grid level n always consists of cells from grid level n 1 resulting in a hierarchical relationship karimi fard and durlofsky 2014 the simulation grid is then obtained by combining control volumes from grids of different levels a schematic representation of this procedure is illustrated in fig 2 3 1 cell aggregation a mesh consists of a set of finite control volumes each having vertices with allocated coordinates to conduct cell aggregation the centroid is first computed for each mesh element within the grid fig 3 shows an example 2d unstructured grid to illustrate how cell aggregation is conducted as can be seen in this particular example each cell has 3 vertices and a centroid represented in red with coordinates x c and y c defined as x 1 x 2 x 3 3 y 1 y 2 y 3 3 where x i and y i are the coordinates of the vertices each mesh element has an assigned index number cell aggregation is then carried out by dividing the grid in the x and y direction and in the z direction for 3d models into equidistant intervals δ x and δ y using a predefined scaling factor each interval has coordinates i i δ x in the x direction and j j δ y in the y direction centroids of cells whose coordinates are within a given x y area are aggregated to form one coarse cell to check whether a fine cell f is within a given plane which will form coarse cell f the following algorithm is implemented for the coordinates x c f and y c f of the centroid of fine cell f 9 if i x c f i δ x and j y c f j δ y cell f cell f fig 3 shows the range partitioning illustrated by the white lines for a 2d unstructured grid the x and y range were divided in 5 and 3 equidistant intervals respectively the yellow highlighted 2d plane has range i i δ x in the x direction and j j δ y in the y direction for this given example all cell centroids whose coordinates fall within this plane are aggregated to form one coarse cell for example cells 41 46 68 77 84 92 106 111 and 118 form coarse cell 0 for the given 2d unstructured grid example in fig 3 the so called level 1 i e the next level of coarsening is shown in fig 4 the numbers represent the assigned indices of the newly constructed coarse cells if one wants to construct an additional level the same procedure can be followed with a larger x and y range partitioning where grid cells of level 1 are aggregated to form level 2 for further steps into the generation of the levels a list fines in coarse is constructed where the corresponding indices of the aggregated fine cells are listed for each coarse cell table 2 tabulates this list for the example above figs 3 to 4 this type of list is generated for each coarse level level 0 in the hierarchical grid these lists are stored for the construction of the cell properties e g volume porosity of the coarse levels where the cell data from the fine level is needed during upscaling note that cell aggregation can also be conducted while taking care of highlighting geological features e g fractures and different facies in the model for example cell aggregation can be conducted by grouping domains with the same facies together into one coarse cell or in fractured reservoirs by aggregating cells by isobar contours similar to karimi fard and durlofsky 2014 after cell aggregation is conducted the connectivity list is then constructed describing all connections within each level and the inter level connections to illustrate the methodology we use the simple structured grid from fig 1 where cell aggregation was performed to form one coarse level in the proposed amr scheme the connectivity list of each level is determined systematically each mesh element consists of a set of vertices x e g a triangular mesh element comprises 3 vertices and a cartesian grid comprises 4 vertices these vertices are numbered uniquely the vertices x comprised in a cell i are stored in a list this is done for each mesh element in level 0 to determine whether two control volumes i and j are adjacent we take the intersection of both sets of vertices that is 10 x x i x x j each geometry has a different criterion for 2d shaped mesh elements the interface is a line for 3d shaped cells the interface is a plane hence the criterion is that the intersection length should equal 2 for 2d shapes and 3 or more for 3d shapes this methodology is applied to the finest level of refinement level 0 the result is a connectivity list representing all the unique connection pairs within level 0 the interface area is subsequently computed and stored for transmissibility computation in further steps for each connection for the construction of the coarse level connectivity list we first store for each cell i connection pairs interfaces which consist of cell i describing its faces table 3 illustrates this methodology for the fine grid in fig 1 a similar list is constructed for coarser levels level 0 which is constructed by aggregating the faces f of the fine grid cells i listed in table 3 in this example contained in each coarse cell i inner fine interfaces are unaccounted for as they are not contained in the coarse interface for the example above fig 5 this results in the list shown in table 4 next the common faces between each coarse cell are determined this is implemented by evaluating the intersection between the set of faces f belonging to coarse cell i and the set of faces f forming coarse cell j this is expressed as 11 f f i f f j if a given coarse cell i has one or multiple common faces f with another coarse cell j these two cells form neighbouring blocks for transmissibility computation in further steps the area of the connecting interface is stored which is here expressed as the sum of the intersecting fine grid faces for inter level connections a similar method is implemented for each coarse cell i in level n the intersection of its set of faces f with the set of faces f of a given fine cell i is determined this operation is conducted for every fine cell i in level n 1 except for the fine cells comprised in the evaluated coarse cell i i i this is expressed mathematically as follows 12 f f i f f i where i i i similarly if a given coarse cell i has a common face with a fine cell i the two cells are connected this procedure is applied between all levels n and n 1 the result is a list of connections within level 0 a list of connections for each level n and an inter level connectivity list which describe the full hierarchical grid 3 2 transmissibility and upscaling in this work the amr method is implemented for unstructured grids of any geometry the definition of the transmissibility for unstructured grids is expressed as 13 γ p 12 γ 12 λ with γ 12 α 1 α 2 α 1 α 2 and α i a k i d i n d i where γ p 12 transmissibility between cells 1 and 2 γ 12 constant geometrical part of the transmissibility λ mobility of a given phase p a interface area k i permeability of cell i d i distance between centroid of cell i to interface area a n unit vector normal to the interface d i unit vector along the line joining centroid of cell i to the center of interface a here the directional permeability of each cell is expressed as the magnitude of the cell s k x k y k z coordinates multiplied by the unit vector d i to solve the mass conservation eq 1 the flow rate must be computed for the interface of every neighbouring cells it is therefore necessary to compute the transmissibility for each dual connection listed in the connectivity list the result is a list consisting of all connections with their corresponding transmissibility this methodology is applied at the finest level of refinement level 0 for thermal problems another type of transmissibility γ c l must be computed to approximate thermal conductive flux in the energy eq 2 since thermal rock conduction is not as heterogeneous as permeability the thermal transmissibility is defined as the geometric coefficient that is the area of the interface l divided by the sum of the distances d 1 and d 2 from centroids to interface l multiplied by the average conduction κ 12 14 γ c 12 κ 12 a d 1 d 2 as mentioned earlier level 0 is represented by an array of volumes depths and reservoir properties which are derived from the static geological model once the hierarchical grid is constructed all cell properties must be redefined for the coarser levels level 0 this is done by upscaling the properties of the corresponding fine grid cells the volume is upscaled by simply summing the volumes of the aggregated fine grid cells ν i 15 v i i i ν i depth upscaling is done by taking the average of the fine scale depths the porosity thermal conductivity and rock heat capacity are upscaled using a volumetric averaging for example the sum of the porosity ϕ i multiplied by the corresponding cell volume ν i of each fine cell i is taken over the total volume of the coarse cell v i 16 ϕ i 1 v i i i ν i ϕ i in this study for the upscaling of permeability we use the flow based upscaling technique developed by karimi fard et al 2006 gong et al 2008 karimi fard and durlofsky 2012 this technique uses the pressure solution when the system has reached steady state to compute the flow across each interface the transmissibility can then be derived by rearranging the flow eq 7 these approaches can be applied to unstructured coarse grids with generally shaped control volumes karimi fard and durlofsky 2014 the coarsening technique defines the coarse transmissibility γ p i j between two adjacent control volumes i and j this is expressed as 17 γ p i j q i j p i p j the coarse grid average pressures p i and p j and the coarse grid flow rate q i j are computed using a fine grid pressure solution these quantities are given by 18 p i 1 v i i i ν i p i p j 1 v j j j ν j p j q i j i i j j q i j i i j j γ p i j p i p j where p i and p j define the fine scale pressures in the corresponding coarse blocks in the flow rate expression q i j i j indicates the interface between fine cells i and j and γ p i j denotes the transmissibility for this interface this i j interface comprises a portion of the interface between coarse blocks i and j for inter level connections a similar approach is used for a given fine cell i and coarse cell j with interface i j eq 18 is used with p i p i the pressure of fine cell i and p j the pressure of coarse cell j this procedure is conducted for each inter level connection found within the hierarchical grid for thermal problems a similar method can be implemented but is however not computationally efficient as temperature takes significantly longer to reach a steady state we therefore use eq 14 to compute the upscaled thermal transmissibility where the area is expressed as the sum of the fine scale faces which compose interface i j and the distances d i and d j represent the distances between the cell centroid and the centroid of the coarse interface 3 2 1 global indexing once all needed parameters at every hierarchical level are evaluated which include cell properties and a connectivity list with associated transmissibility for each level and between levels it is necessary to combine the levels in order to form a global hierarchical set of grids to combine the levels it is however necessary to assign a unique indexing to each and every mesh element contained in the multi level grid this is where global indexing plays a role for convenience indexing is ordered starting from the coarsest level an example of global indexing is shown in fig 6 the procedure used to assign global indexing is to simply add the number of cells of the previous level s to the current level e g for level 1 from fig 6 numbering starts at the total number of cells of level 2 n 2 for level 0 numbering starts at n 2 n 1 this procedure is applied to the bookkeeping lists such as fines in coarse and to the connectivity list of the corresponding levels after the global indexing is applied to the connectivity lists the connectivity list at each level and the inter level connection lists are combined into one list this is conducted by concatenating these lists to form one list describing all existing connections within the hierarchical grid regarding the array of cell properties global indexing is applied by simply concatenating the arrays together in the right order i e from the coarsest level to the finest level this way indexing is done in the same order as the global indexing the result is a global array of volumes depths and relevant reservoir properties describing each mesh element within the hierarchical grid having constructed the hierarchical grid and assigned it global indexing the pre processing stage is complete and the simulation with dynamic adaptivity can be performed 4 dynamic adaptivity framework to determine whether grid adaptivity is necessary we define refinement and coarsening criteria which are dependent on the application used in this study we adopted an approach where the difference in solution variable is analysed between neighbouring blocks therefore the difference in the solution variable of interest x is computed between each pair of cells active in the simulation grid if this difference is higher than a given threshold both neighbouring blocks are refined for the coarsening of a set of fine cells belonging to a given coarse cell the difference between all the corresponding fine cells and their neighbouring cells is computed if each and every one of these connections have a difference in solution variable below a given threshold the fine cells are coarsened to the next consecutive level for cells marked for refinement the corresponding fine cells from the level below are added to the array of active blocks which is used for implementation of the next time step while the indices of the coarse cells in question are suppressed similarly the cells marked for coarsening are suppressed from the active cells and the corresponding coarse blocks are added fig 7 shows an example of a two level hierarchical grid the current time step simulation grid is represented on the bottom left after a check for adaptivity was conducted cells 1 and 2 were marked for refinement hence as explained above the cell indices 1 and 2 are suppressed from the array of active blocks and their corresponding fine cell indices are added 6 7 10 and 11 for coarse cell 1 and 12 13 16 and 17 for coarse cell 2 the scheme at the bottom right of the figure shows the simulation grid which will be used for the next time step as can be seen cell adaptivity results in an unstructured indexing once the simulation grid is redefined and the array of active cells is updated the connectivity list and corresponding transmissibility must be redefined this is done by copying the list of connections for the whole hierarchical grid where only the connections and corresponding transmissibility involving the active cells are kept while connections involving non active cells and their corresponding transmissibility are suppressed similarly the same holds for the array of volume depth and relevant properties only the cell properties of the active blocks are stored for computation of the next time step solution x n 1 the solution of the previous time step x n is required see eqs 4 and 5 however x n doesn t have the same grid configuration as the next time step t n 1 it is therefore necessary to convert the grid of solution x n to the same configuration as the simulation grid at t n 1 to do so we use simple mapping techniques a prolongation operator is firstly used to redefine the solution variable x at each cell of the finest level of refinement level 0 a so called constant prolongation is implemented i e all sub domain values x i are set to the coarse value solution variable x i 19 x i x i i i subsequently restriction to the new simulation grid is conducted on the prolongated solution i e for cells already at the finest level the solution stays the same when several control volumes are grouped into a single coarser control volume the coarse value x i is set to the volume weighted average of all sub domain values x i karimi fard and durlofsky 2014 20 x i 1 v i i i ν i x i a schematic representation of this procedure for the 2 level hierarchical grid and for the new simulation grid of fig 7 t n 1 is shown in fig 8 the model however necessitates sequential numbering for mesh generation it can be seen in fig 7 that indexing is non consecutive when grid adaptivity is applied this is where local indexing comes in play that is the active blocks indices are re numbered in a sequential order to prevent undefined indices in the mesh the global indexing is stored in a so called global to local array for conversion back to the global indices for adaptivity check in the next time step the described procedure which redefines the grid configuration for the next simulation is repeated at each time t it is also important to note that all previously computed operators in the obl method are re used after each successive timestep this is possible since the parameter space for each state dependent operator in the obl method is decoupled from any spatial property or discretization this provides a significant speeds up of the computation especially when simulation property is expensive to evaluate in the synthetic examples used to illustrate the performance of the amr framework the first time step simulation is started at the coarsest level for improved accuracy the cells containing the wells are kept at the finest level of refinement level 0 5 applications for geothermal reservoirs geothermal technology has recently received substantial attention as an alternative source of energy however geothermal production systems have a relatively low return on investment where uncertainties related to lack of detailed information about subsurface formations can significantly affect the quantification of the economic planning and feasibility of geothermal projects willems 2017 it is therefore important to reduce the uncertainty and produce a high accuracy solution while keeping the computational costs low geothermal systems therefore represent a good candidate for implementation of our amr framework since it keeps the accuracy of simulation process close to the fine scale while the performance is close to coarse scale models simulation of geothermal reservoirs implicates the solution of both mass 1 and energy 2 conservation equations where pressure and enthalpy are the solution variables we are mostly interested in the accurate prediction of the temperature displacement front and resulting thermal breakthrough time dynamic adaptivity will be illustrated for 2 synthetic geothermal examples a homogeneous reservoir with unstructured meshing a heterogeneous fluvial system from shetty et al 2018 with low net to gross ratio in darts the enthalpy is used as nonlinear unknown instead of the temperature the adaptivity criteria are therefore applied to the enthalpy solution where the difference in enthalpy between two adjacent control volumes is analysed this is done for each pair of connection within the simulation grid here we applied the following adaptivity criteria 21 if δ h i j 70 k j mark cells i and j for refinement if δ h l 40 k j l i mark cells i i for coarsening this adaptivity criteria is a simple heuristic and serves a practical purpose in this work the proposed amr method would greatly benefit from a more sophisticated criteria for example a criteria based on a posteriori error estimates similar to vohralík and wheeler 2013 vohralík and yousef 2018 the geothermal examples are illustrated by showing the fine scale solution at different time steps versus the amr solution and the coarse scale solution each synthetic example was analyzed quantitatively by conducting an error analysis where the error of both amr and coarse solution are computed relative to the fine scale solution both the l2 norm and l infinity norm were calculated for each time step throughout the simulation moreover to define the performance of the amr method in terms of computational resources the percentage of grid cells utilized in the simulation using the amr grid relative to the total number of cells in the fine scale model was plotted for each example 5 1 case 1 homogeneous model the first model is a simple 2d homogeneous reservoir constant permeability with unstructured triangular mesh we consider a single injector i and a single producer p configuration a two level hierarchical grid is used with 1420 cells in level 0 and 75 cells in level 1 fig 9 illustrates both levels along with the permeability field constant permeability of 2000 md and the well locations the simulation parameters for this model are specified in tables 6 and 7 in the appendix the level 1 is illustrated in fig 9 where each color represents a coarse cell as can be seen cell aggregation was conducted by dividing the x and y axes into 5 and 15 equidistant intervals the cells at the well locations are kept fine at all times the simulation was conducted for a period of 5500 days the temperature solution at three different times is shown in fig 10 figure a represents the temperature solution at fine scale figure b the solution on the amr grid figure c shows the coarse scale solution and figure d shows the node distribution for the amr simulation run the solution on the amr grid demonstrates a particularly good match with the fine scale solution the node distribution shows high concentration along the front and at the well locations and low concentration behind and ahead of the front where no significant changes are observed this considerably lowers the computational time as compared to running the fine scale model the coarse scale solution differs notably from the amr and fine scale solution with a faster cold front propagation at the coarse grid which is more pronounced in comparison at late times t d 0 3 and 1 the relative error of the amr solution is significantly lower than the coarse solution in both the l2 and l infinity norm fig 11 moreover the number of cells is considerably reduced see fig 12 ranging from 8 to 60 the trend shows an overall increase as the front propagates and a decrease when the cold front has reached the producing well which results in coarsening at locations where no more thermal variations are detected this considerably improves the performance of simulation since the amr approach is much more favourable in terms of efficient use of computational resources see table 5 in section 6 5 2 case 2 sugar cube shale model shales are often neglected in conventional reservoir simulation as the convective flow is never acquired in shales due to low permeability for geothermal applications they represent an important source of heat for thermal recharge of the cold water front modelling of the shales however significantly increases computational time since shales often occupy a significant amount of computational grid here we test an application of our amr approach to a sugar cube model where cubes represents shale bodies and space between them fluvial channels we use a simple 2d setup with in total a 5 by 6 shale block configuration shale blocks have a permeability of 10 2 md while the sand bodies have a permeability of 10 3 md the injector and producer are placed at opposite corners of the reservoir as shown in the fig 13 level 0 consists of 4588 fine cells level 1 is constructed differently from the conventional amr approach with the sand channels kept at fine level and only the shale blocks are coarsened by a ratio of 100 10 10 the coarse grid level 1 contains 1618 cells the simulation parameters for this model are specified in tables 6 and 7 in the appendix fig 14 depicts the temperature solution at three different times throughout the simulation t d 0 1 0 3 and 0 7 similarly to the previous example the solution is shown for a the fine grid b the amr grid and c the coarse grid as can be seen on the amr figure b the grid refines as soon as the cold front arrives at proximity to a shale body the cold front is accurately represented on the amr grid and there are no differences compared to the fine grid on the coarse grid however the cold front propagates further than for the fine and amr model which is clearly visible at the late time recording t d 0 7 when the cold front passes part of the shales blocks and these have cooled down coarsening occurs as observed at t d 0 7 fig 15 depicts the error distribution through time of both the amr and coarse model relative to the fine model as can be seen in fig 15 the error of the coarse model is significantly larger than for the amr model where the error is close to zero the high frequency changes in the error especially observed in the l norm seem to correlate with refinement and coarsening of the mesh in between timesteps similar to what was observed in berrone 2010 the percentage of cells used in the amr grid relative to the number of cells used in the fine grid is shown in fig 16 as can be seen the percentage of cells ranges from roughly 35 to around 90 halfway through the simulation when the cold front reaches the producing well and then lowers to 65 when shale blocks proximal to the injector wells have cooled down to injection temperature and hence coarsening occurs as observed the computational time and effort was considerably reduced throughout the simulation and the amr solution outcome shows a very accurate representation of the fine scale model see table 5 in section 6 5 3 case 3 fluvial heterogeneous model our amr framework was tested for a heterogeneous reservoir with a low net to gross ratio n g 35 the permeability field ranges from 5 to 3400 md with a significant amount of shale regions present the hierarchical grid for this example is a structured grid and it comprises two levels the finest grid level 0 consists of 2400 grid cells with 40 cells in the x direction and 60 cells in the y direction level 1 was reduced to 150 mesh elements where aggregation was done using 4x4 fine mesh elements resulting in 10 grid cells in the x direction and 15 grid cells in the y direction the permeability field along with the hierarchical grid for this example is shown in fig 17 the location of the injector i and producer p are depicted in yellow on the permeability distribution the simulation parameters for this model are specified in tables 6 and 7 in the appendix the simulation was conducted until cold water breakthrough reached the producing well fig 18 illustrates the temperature solution at different times throughout the simulation for each time shown figure a represents the fine scale solution figure b is the amr solution and figure c is the coarse scale solution the grid is kept at its finest level at well locations the amr mesh exhibits a significant improvement in temperature solution compared to the solution on the coarse grid refinement is mainly focused at the front and slightly beyond the front while areas where insignificant changes occur remain coarse important details such as fingering effects at the cold water front which are neglected on the coarse grid are clearly visible in both fine and amr solutions which results in a more accurate representation of this physical phenomenon the relative error throughout the simulation run was recorded where the fine model is taken as reference solution for comparison between the coarse and amr model fig 19 shows the l2 norm and the l infinity norm error in time as can be seen the marked improvement is also recorded in the error analysis where the error between the coarse and fine model is notably larger than the error between the amr and fine model the l2 norm remains relatively constant for the amr solution whereas it increases slightly in time for the coarse solution the number of grid cells used in the simulation ranges from 8 to 70 throughout the simulation see fig 20 this represents a significant improvement in computational effort and time while still capturing important features see table 5 in section 6 5 4 case 4 reactive transport carbonate reservoirs host a major part of the world s hydrocarbon reserves but besides hydrocarbon reserves the ongoing energy transition has resulted in an increase interest in geothermal systems where many are hosted by carbonate rocks these reservoirs can have heavily fractured and karstified intervals resulting in unforeseen hazards during drilling furthermore naturally fractured carbonate reservoirs contain a large uncertainty in flow response due to the poor ability to predict the spatial distribution of discontinuity networks at reservoir scale another important process related to dissolution is well acidization used to increase the production this process involves the dissolution of reservoir rock to stimulate flow towards the wells these chemical reactions are localized and form important features for accurate representation of the flow response furthermore reaction rates which occur during dissolution are high resulting in a sharp front in the flow response moreover during dissolution formation and development of an unstable dissolution front with multiple wormholes can occur and its modeling is quite sensitive to the resolution shaik et al 2018 in near well acidization processes the regime which forms a single dominating wormhole is the most preferable it is therefore important to accurately predict this unstable dissolution while keeping the computational time reasonable amr is therefore a good solution to model these reservoirs and chemical processes to solution requirements in the flow example analyzed in this study dissolution involves the following simple reaction where carbonate is dissolved 22 caco 3 s ca 2 co 3 2 the kinetic reaction rate for this reaction is 23 r k a k k 1 q k s p s s where a is the mineral surface area k k is the kinetic reaction constant q is the ion activity product k s p is the equilibrium product and s s is the solid saturation permeability is updated using a power law relationship defined as follows 24 k k 0 ϕ ϕ 0 n where k 0 and ϕ 0 are the initial permeability and porosity and n is the power law exponent the proposed model simulates the phenomenon of unstable wormhole formation triggered by small perturbations in permeability on one side of the reservoir an injector well is placed which is perforated throughout the whole thickness on the other side the producer well is placed also spanning the entire thickness of the reservoir the model described in this example has dimensions of 100 by 100 meters a constant permeability of 1 md is used with 5 of random noise the left illustration in fig 21 shows the well locations along with the permeability of the reservoir the hierarchical grid consists of two levels where level 0 is an unstructured grid containing 2194 triangular cells cell aggregation was conducted to construct level 1 where the x and y axes were divided in 10 equidistant intervals resulting in a grid with only 100 cells level 0 and 1 are shown in fig 21 the simulation parameters for this model are specified in tables 8 and 9 in the appendix the amr simulation was started at the coarse level while keeping the well cells at the finest level throughout the entire simulation run for this application the adaptivity criterion is based on the solid composition x c a c o 3 the adaptivity criteria used in this example are defined as follows 25 if δ x c a c o 3 i j 0 02 mark cells i and j for refinement if δ x c a c o 3 l 0 01 l i mark cells i i for coarsening where δ x c a c o 3 is the difference in composition of the calcium carbonate c a c o 3 component the simulation was recorded at three different times 0 13 0 25 and at the final time expressed in dimensionless time fig 22 depicts the solid composition the composition of c a c o 3 in time where figure a is the fine scale solution figure b is the amr solution figure c is the coarse scale solution and figure d represents the node distribution of the amr grid as can be seen the amr solution is considerably more accurate than the coarse scale solution the far propagating wormhole at t d 1 which is present in both the fine scale solution and the amr solution is not well represented on the coarse scale solution where two extensive wormholes are present the amr solution however shows a very good representation of the fine scale solution throughout time the most extensive wormhole exhibits slight differences in thickness and some minor variations are observed at the other smaller wormholes the node distribution follows the front which is in this example quite dispersed resulting in refinement spanning a wide area especially at the last time step however considerable computational resources are saved at the early stage of the simulation to quantify the differences between fine scale amr and coarse scale solutions an error analysis was conducted here again both the l2 norm and the l infinity norm were computed for the amr and coarse model relative to the fine scale solution the graphs in fig 23 depict the outcome as can be seen the amr error is once more significantly less than the coarse model for both norms for the l2 norm the coarse fine relative error is three times greater than the amr fine error at the final time step the l infinity norm of the coarse fine error starts low at the first time step where no extensive propagation is observed and where the model is close to the initial conditions but then rapidly increases to 0 8 and remains more or less constant throughout the l infinity norm of the amr fine error seems to increase in time this is due to the propagation of initially small errors in the solution note however that the relatively big error for both the amr and coarse scale model are not representative for this example and are related to another type of instability in the solution similarly to the previous example we have analyzed the total number of cells used in the amr model relative to the total number of cells contained in the fine scale grid the graph in fig 24 shows this quantity expressed in percentage as can be seen the number of cells used during the amr simulation is overall less than the number of cells present in the fine scale model initially the number of cells starts at 20 which represents the use of the coarsest level with both left and right boundaries kept at the finest level it then increases fairly steeply to around 95 due to the high injection velocity which corresponds to the dominating wormhole regime around the end of the simulation the number of cells starts to decrease which indicates coarsening at some locations see table 5 in the next section for the actual computational time although almost 100 of the cells is used at two thirds of the simulation which is computationally expensive considerable resources are saved in the beginning moreover this problem is sensitive to the resolution which requires refinement at many locations in order to accurately capture the wormhole propagation 6 discussion in the results section all computational speed up was indicated in terms of of active cells w r t fine scale in this section the actual cpu times are highlighted for all cases in order to have a fair comparison the same nonlinear solver newton s based update with a fixed number of iterations linear solver direct one and time stepping strategy is used for the fine coarse and amr runs all these results are shown in table 5 even though the overhead of the amr method is substantial this can easily be explained by the non vectorized python implementation of the amr procedure vs highly optimized python and c implementation of the conventional simulation used in the coarse and fine simulation since the scope of this work is a proof of concept of the proposed amr procedure prototyped outside of the simulation loop our amr code has not been optimized yet this can be solved by either an application of numba just in time compiler for python or rewriting the procedure in c an expected speed up in our experience is around two orders of magnitude when compared to the original python implementation thereby reducing the overhead to around 1 5 of the runtime of the amr method and making it a viable strategy for geoscience applications the framework presented in this paper is developed in the darts platform which can be used for a more general set of applications related to the energy transition however it is important to note the major differences in various energy applications for example in two types of applications shown in this study geothermal and chemical dissolution cases the coarser representation is still capable of accurately capturing important features of the geothermal dynamic behaviour the coarse scale simulation in chemical dissolution however completely fails to represent the same dynamic behaviour dissolution pattern and effective characterization of the process e g effective rock dissolution it is therefore evident as is also pointed out in the literature that problems contained localized sharp gradients can greatly benefit from the amr technique 7 conclusions this study aimed at developing an adaptive mesh refinement amr technique in delft advanced research terra simulator darts for general purpose reservoir simulation the developed amr framework consists of a multi level hierarchical grid where levels are constructed through a mesh partitioning of the fine scale model the static geological model which is represented by an array of properties e g volume and porosity the framework consists of the construction of the coarse levels through cell aggregation of the next consecutive fine level at the pre processing stage the method used to aggregate fine cells includes the grouping of subdomains whose centroids are found within a predefined 3d domain in this study domains are grouped by the partitioning of the x y and z axes into equidistant intervals however this strategy can easily be changed and improved the aggregation of the subdomains to form a coarser level is stored as an array of indices for the next stages which consists of the indices of the fine cells comprised in its coarse control volume for each coarse cell next in order to solve the relevant governing equations the flow must be computed at each interface present in the mesh we therefore generate a list called a connectivity list describing all neighbouring cells within each level and between levels the fine scale transmissibility is then computed using the permeability field hereafter a flow based upscaling is applied in order to acquire the transmissibility of coarser levels and the inter level transmissibility each control volume has defined parameters that are relevant for solving the system volume porosity depth etc once the hierarchy of levels is complete the simulation can be started adaptivity check is performed at every time step using criteria specific to the application once the regions for coarsening and refinement are defined the solution is prolongated to the finest meshing level and subsequently restricted from fine to the adaptive simulation grid a new connection list and grid properties are constructed for the new coarsened schema once it is completed the simulation runs for the next time step using the constructed simulation model the accuracy of the method was demonstrated for geothermal applications two models were tested including a homogeneous model with unstructured gridding a synthetic sugar cube like model with high permeability channels surrounded by shale blocks and a heterogeneous fluvial system model with a low net to gross ratio high levels of solution accuracy relative to the reference fine scale results are observed for both cases an error analysis was conducted to record the differences between the amr and the coarse solution relative to the reference fine scale solution the error resulting from the amr model is significantly lower than for the coarse model for all tested problems the overall percentage of grid cells used in the amr model relative to the fine scale model is considerably decreased for most problems to conclude the developed amr method shows high levels of accuracy for both homogeneous and heterogeneous models and can be used for geothermal applications as well as for other applications implemented in darts the number of cells in the amr simulation relative to the total number of cells of the finest level is considerably reduced which is very favourable in terms of efficient use of computational resources the framework is applicable to two and three dimensional models and for unstructured as well as structured meshes the applicability of the method to unstructured grids provides an effective means for solving complex geological systems credit authorship contribution statement stephan de hoop methodology validation writing review editing elodie jones software investigation writing original draft denis voskov conceptualization methodology supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like to thank the entire darts group for their assistance and help we would also like to acknowledge financial support from the sfb1313 research group at university of stuttgart and particularly like to thank beatrix becker and prof rainer helmig for their valuable time and help appendix 
284,a coupled description of flow and thermal reactive transport is spanning a wide range of scales in space and time which often introduces a significant complexity for the modelling of such processes subsurface reservoir heterogeneity with complex multi scale features increases the modelling complexity even further traditional multiscale techniques are usually focused on the accuracy of the pressure solution and often ignore the transport improving the transport solution can however be quite significant for the performance of the simulation especially in complex applications related to thermal compositional flow the use of an adaptive mesh refinement enables the grid to adapt dynamically during the simulation which facilitates the efficient use of computational resources this is especially important in applications with thermal flow and transport where the region requires high resolution calculations as often localized in space in this work the aim is to develop an adaptive mesh refinement framework for geothermal reservoir simulation the approach uses a multi level connection list and can be applied to fully unstructured grids the adaptivity of the grid in the developed framework is based on a hierarchical connectivity list first the fine scale model is constructed which accurately approximates all reservoir heterogeneity next a global flow based upscaling is applied where an unstructured partitioning of the original grid is created once the full hierarchy of levels is constructed the simulation is started at the coarsest grid grid space refinement criteria is based on the local changes and can be adjusted for specific models and governing physics the multi level connectivity lists are redefined at each timestep and used as an input for the next the developed adaptive mesh refinement framework was implemented in delft advanced research terra simulator which uses the operator based linearization technique the performance of the proposed approach is illustrated for several challenging geothermal applications of practical interest keywords geothermal simulation adaptive mesh refinement unstructured grid darts 1 introduction production development of prospective reservoirs includes the use of various technologies that provide information at many different scales these scales range from core plugs being a few centimeters in size to well logs detecting properties a few meters around the well and to seismic imaging covering a significant volume with limited resolution few meters vertically and 10 s of meters horizontally however time and capital limitations result in sparse direct sampling of reservoir rock and fluid properties this is why the construction of reservoir models through integration of these data using geostatistical reservoir description algorithms has become a crucial step in resource development branets et al 2009 these algorithms conventionally result in fine scale descriptions of reservoir properties porosity permeability on grids of tens of millions of cells christie 1996 an issue of considerable importance is the risk and uncertainty assessment of reservoir performance the uncertainty can be gauged by simulating an ensemble of different geological realizations chen et al 2015 this may require to run thousands of simulations to cover a wide range of parameter variation it is however not computationally feasible or desirable to perform these simulations on the high fidelity fine grid model significantly upscaled models i e the mapping of rock and fluid properties to a coarser resolution are therefore required where these models should ideally be even coarser than typical reservoir simulators which can handle on the order of 10 5 10 6 simulation cells durlofsky 2005 in the presence of more complex physics excessive upscaling may however result in non satisfactory results which necessitate the use of advanced algorithms and solvers to allow for higher resolution grids to be employed cusini et al 2016 traditional multiscale techniques jenny et al 2003 wang et al 2014 developed to solve the elliptic or parabolic pressure equation in sequentially coupled simulations mainly focus on the pressure solution and often ignore the transport however in complex applications related to chemical and compositional eor enhanced oil recovery reservoir storage and geothermal industry the number of conserved chemical species can be large which makes any improvement in transport solution quite significant for the performance and robustness of the simulation a technique called adaptive mesh refinement amr provides an effective means for adapting the resolution of a model to solution requirements this method is well developed in many areas of computational physics e g fluid dynamics and solid mechanics but is however not widely used for practical reservoir simulation karimi fard and durlofsky 2014 in today s literature several researchers have developed and proposed amr procedures to capture the local nature of transport processes bahrainian and dezfuli 2014 have developed a novel unstructured grid generation algorithm which considers the effect of geological features and well locations in the grid resolution this strategy involves the definition and construction of the initial grid based on the geological model geometry adaptation of geological features and grid resolution control trangenstein 2002 used the combination of high resolution discretization methods with dynamically adaptive mesh refinement for a two component single phase model for miscible flooding pau et al 2012 proposed an amr algorithm for compressible two phase flow in porous media the method is implemented within a block structured adaptive mesh refinement framework which allows the grids to dynamically adapt to flow features and enables efficient parallelization of the algorithm the coarse scale permeability was obtained by averaging the fine scale permeability similar techniques have been developed for compositional simulation sammon et al 2003 thermal problems christensen et al 2004 improved enhanced oil recovery processes van batenburg et al 2011 discrete fracture networks berrone et al 2019 and many more applications in this work the aim was to develop a dynamic amr scheme using an unstructured multi level gridding framework for geothermal simulation in complex reservoirs the focus lied particularly on thermal reactive flow and transport formulation which are required for a wide range of subsurface applications relevant to the energy transition including geothermal notice that heterogeneity plays a very important role in geothermal applications shetty et al 2018 babaei and nick 2019 the geothermal doublet lifetime and heat recovery rate usually vary a lot with both reservoir parameters and operational management where uncertainties due to heterogeneity are dominating willems and m nick 2019 heterogeneity in flow path and shale facies play an important role in water heat recharge which directly affects doublet performance at low net to gross ratio crooijmans et al 2016 besides complex heat extraction process and corresponding chemical interactions can also amplify the effect of heterogeneity cui et al 2016 kala and voskov 2020 as a starting point of our framework a fine scale geological model has to be constructed accurately approximating all reservoir heterogeneity in reservoir simulation this model is often represented by an array of volumes depths and a connectivity list lim et al 1995 describing each control volume next a global flow based upscaling was applied and an unstructured partitioning of the original grid was constructed as suggested in karimi fard and durlofsky 2014 this partitioning provides coarser levels of the original model which is also described by an array of volumes depths and a connectivity list a coarser connectivity list includes connections between control volumes at the given level as well as interconnections between the levels once the full hierarchy of levels is constructed the simulation is started at the coarsest grid grid space refinement criterion is developed for particular applications the multi level connection list is reconstructed at each time step and used for the simulation the proposed approach was implemented in delft advanced research terra simulator darts kala and voskov 2020 wang et al 2020 2 methodology 2 1 governing equations general purpose reservoir simulation is based on the solution of governing equations which describe mass and energy transfer of various species in the subsurface the flow of mass and energy in a system with n p phases and n c components are described in this section for this general purpose thermal compositional model n c component mass conservation equations and a single energy conservation equation need to be solved khait and voskov 2018b when chemical reactions occur in the system an additional term describing n k kinetic reactions is added to the mass conservation equation kala and voskov 2020 these governing relations are described as 1 t ϕ p 1 n p x c p ρ p s p div p 1 n p x c p ρ p u p p 1 n p x c p ρ p q p k n k v c k r k c 1 n c k 1 n k 2 t ϕ p 1 n p ρ p s p u p 1 ϕ u r div p 1 n p h p ρ p u p div κ t p 1 n p h p ρ p q p 0 where t is the time v c k is the stoichiometric coefficient associated with kinetic reaction k r k is the rate of kinetic reaction k the right hand side of the mass conservation eq 1 is the kinetic term which describes reactions it is set to zero when no chemical processes are involved in the system the rest of the terms in the system can be described as functions of spatial coordinate ξ and or physical state ω ϕ ξ ω porosity x c p ω the mole fraction of component c in phase p s p ω phase saturation ρ p ω phase molar density u p ξ ω phase velocity q p ξ ω u source of phase p u p ξ phase internal energy u r ξ rock internal energy h p ξ phase enthalpy κ ξ ω thermal conduction an exception is the phase source term which is also dependent on u well control variables the rock internal energy and thermal conduction are assumed to be spatially homogeneous for simplification of the problem meaning that they are characterized as functions of the spatial coordinate ξ only phase flow velocity u p is assumed to follow darcy s law expressed as 3 u p k k r p μ p p p γ p d p 1 n p where k ξ permeability tensor k r p ω relative permeability of phase p μ p ω phase viscosity p p ω pressure in phase p γ p ω gravity vector d ξ depth backward oriented the nonlinear unknowns in this system of equations are the pressure p the overall compositions z c of each component and the enthalpy h 2 2 modeling approach in order to solve the governing eqs 1 and 2 we apply a finite volume discretization on a general unstructured mesh and perform a backward euler approximation in time to both equations where the phase velocities u p are substituted by the darcy relation 3 4 v ϕ p 1 n p x c p ρ p s p n 1 ϕ p 1 n p x c p ρ p s p n δ t l p 1 n p x c p l ρ p l γ p l δ ψ l δ t p 1 n p ρ p x c p q p v δ t k n r v c k r k c 1 n c 5 v ϕ p 1 n p ρ p s p u p 1 ϕ u r n 1 ϕ p 1 n p ρ p s p u p 1 ϕ u r n δ t l p 1 n p h p l ρ p l γ p l δ ψ l γ c l δ t l δ t p 1 n p h p ρ p q p 0 here v is the control volume for which the system is being solved q p q p v is a source of phase p n is the previous time step whereas n 1 is the time step we want to solve for capillarity and gravity are neglected in these equations and a two point flux approximation tpfa with an upstream weighting is applied δ ψ l the phase potential therefore simply becomes the difference in pressure between blocks connected via interface l while δ t l is the temperature difference between these blocks γ p l γ l k r p l μ p l is a phase transmissibility where γ l is the constant geometrical part of the transmissibility involving permeability and geometry of the control volume finally γ c l γ l κ is the thermal conductive transmissibility khait and voskov 2018b this system of equations is solved for each mesh element in time where the unknowns are the composition of the n c components and the pressure for the mass conservation eq 4 and the pressure and enthalpy for the energy eq 5 in general purpose reservoir simulation the solving process requires the linearization of strongly nonlinear governing equations in conventional reservoir simulators a newton raphson based method is typically used for the linearization which solves on each nonlinear iteration a linear system of equations in the following form 6 j ω n ω n 1 ω n r ω n where r is the residual and j is the jacobian which is the derivative of the residual with respect to primary nonlinear unknowns defined at a nonlinear iteration n in this work we use a recently developed approach called operator based linearization obl the main idea of obl is to transform the discretized mass and energy conservation eqs 4 and 5 to an operator form where space dependent ξ and state dependent ω properties of governing equations are separated this provides the opportunity to approximate the representation of the exact physics of a problem through the discretization of the state dependent properties the underlying methodology of obl is explained in more details in voskov 2017 and khait and voskov 2018a b 2 3 connectivity list the proposed amr technique uses the finite volume method fvm for discretization the implementation of the finite volume discretization method to the mass conservation eq 1 requires the evaluation of the flow between two adjacent control volumes in terms of the cell pressures using a two point flux approximation tpfa the flow rate is defined as 7 q i j γ p i j p i p j where q i j flow rate at interface of cells i and j γ p i j phase transmissibility at interface of cells i and j p i pressure of cell i p j pressure of cell j similarly the heat flux between two adjacent control volumes is expressed in terms of thermal transmissibility γ c and is also using a tpfa defined as 8 q i j h γ c i j t i t j where γ c i j is the thermal transmissibility at interface i j t i and t j are the temperatures of cell i and j respectively and q i j h is the heat flux at interface i j to evaluate the flux between two adjacent control volumes a so called connectivity list is constructed i e for each interface between two neighbouring control volumes the indices of these cells are listed together with the transmissibility lim et al 1995 the result is a list with all connection pairs present in the grid a few important points to be noted are each connection consists of only two elements the connection pairs are not repetitive no flow boundaries imply the absence of connections and are hence not listed in the connectivity list fig 1 shows a simple example of a 2d cartesian structured grid with corresponding cell indexing table 1 shows its connectivity list the list is expressed as two arrays cell i and cell j where each column represent a connection pair each pair has an associated interface transmissibility stored in the connectivity list 3 multi level grid generation the adaptivity of the grid in the developed amr scheme is based on a hierarchical representation of connectivity list the simulation grid is composed of several predefined levels representing the same geological properties at different resolutions we start with a fine scale model static geological model which accurately represents all reservoir heterogeneity this grid is defined as level 0 and represents our finest level the modeling grid is defined by a list of control volumes depths reservoir properties all spatially distributed properties required to solve the discretized relations 4 and 5 for each mesh element and a list of connectivity with corresponding transmissibility between neighbouring cells next level 1 is defined where control volumes are constructed by aggregating fine grid cells upscaling is applied to redefine volume depth and reservoir properties at a coarser level a connectivity list with corresponding transmissibility is constructed for this level and inter level connections are defined in addition similarly more levels of coarsening can be constructed a control volume in grid level n always consists of cells from grid level n 1 resulting in a hierarchical relationship karimi fard and durlofsky 2014 the simulation grid is then obtained by combining control volumes from grids of different levels a schematic representation of this procedure is illustrated in fig 2 3 1 cell aggregation a mesh consists of a set of finite control volumes each having vertices with allocated coordinates to conduct cell aggregation the centroid is first computed for each mesh element within the grid fig 3 shows an example 2d unstructured grid to illustrate how cell aggregation is conducted as can be seen in this particular example each cell has 3 vertices and a centroid represented in red with coordinates x c and y c defined as x 1 x 2 x 3 3 y 1 y 2 y 3 3 where x i and y i are the coordinates of the vertices each mesh element has an assigned index number cell aggregation is then carried out by dividing the grid in the x and y direction and in the z direction for 3d models into equidistant intervals δ x and δ y using a predefined scaling factor each interval has coordinates i i δ x in the x direction and j j δ y in the y direction centroids of cells whose coordinates are within a given x y area are aggregated to form one coarse cell to check whether a fine cell f is within a given plane which will form coarse cell f the following algorithm is implemented for the coordinates x c f and y c f of the centroid of fine cell f 9 if i x c f i δ x and j y c f j δ y cell f cell f fig 3 shows the range partitioning illustrated by the white lines for a 2d unstructured grid the x and y range were divided in 5 and 3 equidistant intervals respectively the yellow highlighted 2d plane has range i i δ x in the x direction and j j δ y in the y direction for this given example all cell centroids whose coordinates fall within this plane are aggregated to form one coarse cell for example cells 41 46 68 77 84 92 106 111 and 118 form coarse cell 0 for the given 2d unstructured grid example in fig 3 the so called level 1 i e the next level of coarsening is shown in fig 4 the numbers represent the assigned indices of the newly constructed coarse cells if one wants to construct an additional level the same procedure can be followed with a larger x and y range partitioning where grid cells of level 1 are aggregated to form level 2 for further steps into the generation of the levels a list fines in coarse is constructed where the corresponding indices of the aggregated fine cells are listed for each coarse cell table 2 tabulates this list for the example above figs 3 to 4 this type of list is generated for each coarse level level 0 in the hierarchical grid these lists are stored for the construction of the cell properties e g volume porosity of the coarse levels where the cell data from the fine level is needed during upscaling note that cell aggregation can also be conducted while taking care of highlighting geological features e g fractures and different facies in the model for example cell aggregation can be conducted by grouping domains with the same facies together into one coarse cell or in fractured reservoirs by aggregating cells by isobar contours similar to karimi fard and durlofsky 2014 after cell aggregation is conducted the connectivity list is then constructed describing all connections within each level and the inter level connections to illustrate the methodology we use the simple structured grid from fig 1 where cell aggregation was performed to form one coarse level in the proposed amr scheme the connectivity list of each level is determined systematically each mesh element consists of a set of vertices x e g a triangular mesh element comprises 3 vertices and a cartesian grid comprises 4 vertices these vertices are numbered uniquely the vertices x comprised in a cell i are stored in a list this is done for each mesh element in level 0 to determine whether two control volumes i and j are adjacent we take the intersection of both sets of vertices that is 10 x x i x x j each geometry has a different criterion for 2d shaped mesh elements the interface is a line for 3d shaped cells the interface is a plane hence the criterion is that the intersection length should equal 2 for 2d shapes and 3 or more for 3d shapes this methodology is applied to the finest level of refinement level 0 the result is a connectivity list representing all the unique connection pairs within level 0 the interface area is subsequently computed and stored for transmissibility computation in further steps for each connection for the construction of the coarse level connectivity list we first store for each cell i connection pairs interfaces which consist of cell i describing its faces table 3 illustrates this methodology for the fine grid in fig 1 a similar list is constructed for coarser levels level 0 which is constructed by aggregating the faces f of the fine grid cells i listed in table 3 in this example contained in each coarse cell i inner fine interfaces are unaccounted for as they are not contained in the coarse interface for the example above fig 5 this results in the list shown in table 4 next the common faces between each coarse cell are determined this is implemented by evaluating the intersection between the set of faces f belonging to coarse cell i and the set of faces f forming coarse cell j this is expressed as 11 f f i f f j if a given coarse cell i has one or multiple common faces f with another coarse cell j these two cells form neighbouring blocks for transmissibility computation in further steps the area of the connecting interface is stored which is here expressed as the sum of the intersecting fine grid faces for inter level connections a similar method is implemented for each coarse cell i in level n the intersection of its set of faces f with the set of faces f of a given fine cell i is determined this operation is conducted for every fine cell i in level n 1 except for the fine cells comprised in the evaluated coarse cell i i i this is expressed mathematically as follows 12 f f i f f i where i i i similarly if a given coarse cell i has a common face with a fine cell i the two cells are connected this procedure is applied between all levels n and n 1 the result is a list of connections within level 0 a list of connections for each level n and an inter level connectivity list which describe the full hierarchical grid 3 2 transmissibility and upscaling in this work the amr method is implemented for unstructured grids of any geometry the definition of the transmissibility for unstructured grids is expressed as 13 γ p 12 γ 12 λ with γ 12 α 1 α 2 α 1 α 2 and α i a k i d i n d i where γ p 12 transmissibility between cells 1 and 2 γ 12 constant geometrical part of the transmissibility λ mobility of a given phase p a interface area k i permeability of cell i d i distance between centroid of cell i to interface area a n unit vector normal to the interface d i unit vector along the line joining centroid of cell i to the center of interface a here the directional permeability of each cell is expressed as the magnitude of the cell s k x k y k z coordinates multiplied by the unit vector d i to solve the mass conservation eq 1 the flow rate must be computed for the interface of every neighbouring cells it is therefore necessary to compute the transmissibility for each dual connection listed in the connectivity list the result is a list consisting of all connections with their corresponding transmissibility this methodology is applied at the finest level of refinement level 0 for thermal problems another type of transmissibility γ c l must be computed to approximate thermal conductive flux in the energy eq 2 since thermal rock conduction is not as heterogeneous as permeability the thermal transmissibility is defined as the geometric coefficient that is the area of the interface l divided by the sum of the distances d 1 and d 2 from centroids to interface l multiplied by the average conduction κ 12 14 γ c 12 κ 12 a d 1 d 2 as mentioned earlier level 0 is represented by an array of volumes depths and reservoir properties which are derived from the static geological model once the hierarchical grid is constructed all cell properties must be redefined for the coarser levels level 0 this is done by upscaling the properties of the corresponding fine grid cells the volume is upscaled by simply summing the volumes of the aggregated fine grid cells ν i 15 v i i i ν i depth upscaling is done by taking the average of the fine scale depths the porosity thermal conductivity and rock heat capacity are upscaled using a volumetric averaging for example the sum of the porosity ϕ i multiplied by the corresponding cell volume ν i of each fine cell i is taken over the total volume of the coarse cell v i 16 ϕ i 1 v i i i ν i ϕ i in this study for the upscaling of permeability we use the flow based upscaling technique developed by karimi fard et al 2006 gong et al 2008 karimi fard and durlofsky 2012 this technique uses the pressure solution when the system has reached steady state to compute the flow across each interface the transmissibility can then be derived by rearranging the flow eq 7 these approaches can be applied to unstructured coarse grids with generally shaped control volumes karimi fard and durlofsky 2014 the coarsening technique defines the coarse transmissibility γ p i j between two adjacent control volumes i and j this is expressed as 17 γ p i j q i j p i p j the coarse grid average pressures p i and p j and the coarse grid flow rate q i j are computed using a fine grid pressure solution these quantities are given by 18 p i 1 v i i i ν i p i p j 1 v j j j ν j p j q i j i i j j q i j i i j j γ p i j p i p j where p i and p j define the fine scale pressures in the corresponding coarse blocks in the flow rate expression q i j i j indicates the interface between fine cells i and j and γ p i j denotes the transmissibility for this interface this i j interface comprises a portion of the interface between coarse blocks i and j for inter level connections a similar approach is used for a given fine cell i and coarse cell j with interface i j eq 18 is used with p i p i the pressure of fine cell i and p j the pressure of coarse cell j this procedure is conducted for each inter level connection found within the hierarchical grid for thermal problems a similar method can be implemented but is however not computationally efficient as temperature takes significantly longer to reach a steady state we therefore use eq 14 to compute the upscaled thermal transmissibility where the area is expressed as the sum of the fine scale faces which compose interface i j and the distances d i and d j represent the distances between the cell centroid and the centroid of the coarse interface 3 2 1 global indexing once all needed parameters at every hierarchical level are evaluated which include cell properties and a connectivity list with associated transmissibility for each level and between levels it is necessary to combine the levels in order to form a global hierarchical set of grids to combine the levels it is however necessary to assign a unique indexing to each and every mesh element contained in the multi level grid this is where global indexing plays a role for convenience indexing is ordered starting from the coarsest level an example of global indexing is shown in fig 6 the procedure used to assign global indexing is to simply add the number of cells of the previous level s to the current level e g for level 1 from fig 6 numbering starts at the total number of cells of level 2 n 2 for level 0 numbering starts at n 2 n 1 this procedure is applied to the bookkeeping lists such as fines in coarse and to the connectivity list of the corresponding levels after the global indexing is applied to the connectivity lists the connectivity list at each level and the inter level connection lists are combined into one list this is conducted by concatenating these lists to form one list describing all existing connections within the hierarchical grid regarding the array of cell properties global indexing is applied by simply concatenating the arrays together in the right order i e from the coarsest level to the finest level this way indexing is done in the same order as the global indexing the result is a global array of volumes depths and relevant reservoir properties describing each mesh element within the hierarchical grid having constructed the hierarchical grid and assigned it global indexing the pre processing stage is complete and the simulation with dynamic adaptivity can be performed 4 dynamic adaptivity framework to determine whether grid adaptivity is necessary we define refinement and coarsening criteria which are dependent on the application used in this study we adopted an approach where the difference in solution variable is analysed between neighbouring blocks therefore the difference in the solution variable of interest x is computed between each pair of cells active in the simulation grid if this difference is higher than a given threshold both neighbouring blocks are refined for the coarsening of a set of fine cells belonging to a given coarse cell the difference between all the corresponding fine cells and their neighbouring cells is computed if each and every one of these connections have a difference in solution variable below a given threshold the fine cells are coarsened to the next consecutive level for cells marked for refinement the corresponding fine cells from the level below are added to the array of active blocks which is used for implementation of the next time step while the indices of the coarse cells in question are suppressed similarly the cells marked for coarsening are suppressed from the active cells and the corresponding coarse blocks are added fig 7 shows an example of a two level hierarchical grid the current time step simulation grid is represented on the bottom left after a check for adaptivity was conducted cells 1 and 2 were marked for refinement hence as explained above the cell indices 1 and 2 are suppressed from the array of active blocks and their corresponding fine cell indices are added 6 7 10 and 11 for coarse cell 1 and 12 13 16 and 17 for coarse cell 2 the scheme at the bottom right of the figure shows the simulation grid which will be used for the next time step as can be seen cell adaptivity results in an unstructured indexing once the simulation grid is redefined and the array of active cells is updated the connectivity list and corresponding transmissibility must be redefined this is done by copying the list of connections for the whole hierarchical grid where only the connections and corresponding transmissibility involving the active cells are kept while connections involving non active cells and their corresponding transmissibility are suppressed similarly the same holds for the array of volume depth and relevant properties only the cell properties of the active blocks are stored for computation of the next time step solution x n 1 the solution of the previous time step x n is required see eqs 4 and 5 however x n doesn t have the same grid configuration as the next time step t n 1 it is therefore necessary to convert the grid of solution x n to the same configuration as the simulation grid at t n 1 to do so we use simple mapping techniques a prolongation operator is firstly used to redefine the solution variable x at each cell of the finest level of refinement level 0 a so called constant prolongation is implemented i e all sub domain values x i are set to the coarse value solution variable x i 19 x i x i i i subsequently restriction to the new simulation grid is conducted on the prolongated solution i e for cells already at the finest level the solution stays the same when several control volumes are grouped into a single coarser control volume the coarse value x i is set to the volume weighted average of all sub domain values x i karimi fard and durlofsky 2014 20 x i 1 v i i i ν i x i a schematic representation of this procedure for the 2 level hierarchical grid and for the new simulation grid of fig 7 t n 1 is shown in fig 8 the model however necessitates sequential numbering for mesh generation it can be seen in fig 7 that indexing is non consecutive when grid adaptivity is applied this is where local indexing comes in play that is the active blocks indices are re numbered in a sequential order to prevent undefined indices in the mesh the global indexing is stored in a so called global to local array for conversion back to the global indices for adaptivity check in the next time step the described procedure which redefines the grid configuration for the next simulation is repeated at each time t it is also important to note that all previously computed operators in the obl method are re used after each successive timestep this is possible since the parameter space for each state dependent operator in the obl method is decoupled from any spatial property or discretization this provides a significant speeds up of the computation especially when simulation property is expensive to evaluate in the synthetic examples used to illustrate the performance of the amr framework the first time step simulation is started at the coarsest level for improved accuracy the cells containing the wells are kept at the finest level of refinement level 0 5 applications for geothermal reservoirs geothermal technology has recently received substantial attention as an alternative source of energy however geothermal production systems have a relatively low return on investment where uncertainties related to lack of detailed information about subsurface formations can significantly affect the quantification of the economic planning and feasibility of geothermal projects willems 2017 it is therefore important to reduce the uncertainty and produce a high accuracy solution while keeping the computational costs low geothermal systems therefore represent a good candidate for implementation of our amr framework since it keeps the accuracy of simulation process close to the fine scale while the performance is close to coarse scale models simulation of geothermal reservoirs implicates the solution of both mass 1 and energy 2 conservation equations where pressure and enthalpy are the solution variables we are mostly interested in the accurate prediction of the temperature displacement front and resulting thermal breakthrough time dynamic adaptivity will be illustrated for 2 synthetic geothermal examples a homogeneous reservoir with unstructured meshing a heterogeneous fluvial system from shetty et al 2018 with low net to gross ratio in darts the enthalpy is used as nonlinear unknown instead of the temperature the adaptivity criteria are therefore applied to the enthalpy solution where the difference in enthalpy between two adjacent control volumes is analysed this is done for each pair of connection within the simulation grid here we applied the following adaptivity criteria 21 if δ h i j 70 k j mark cells i and j for refinement if δ h l 40 k j l i mark cells i i for coarsening this adaptivity criteria is a simple heuristic and serves a practical purpose in this work the proposed amr method would greatly benefit from a more sophisticated criteria for example a criteria based on a posteriori error estimates similar to vohralík and wheeler 2013 vohralík and yousef 2018 the geothermal examples are illustrated by showing the fine scale solution at different time steps versus the amr solution and the coarse scale solution each synthetic example was analyzed quantitatively by conducting an error analysis where the error of both amr and coarse solution are computed relative to the fine scale solution both the l2 norm and l infinity norm were calculated for each time step throughout the simulation moreover to define the performance of the amr method in terms of computational resources the percentage of grid cells utilized in the simulation using the amr grid relative to the total number of cells in the fine scale model was plotted for each example 5 1 case 1 homogeneous model the first model is a simple 2d homogeneous reservoir constant permeability with unstructured triangular mesh we consider a single injector i and a single producer p configuration a two level hierarchical grid is used with 1420 cells in level 0 and 75 cells in level 1 fig 9 illustrates both levels along with the permeability field constant permeability of 2000 md and the well locations the simulation parameters for this model are specified in tables 6 and 7 in the appendix the level 1 is illustrated in fig 9 where each color represents a coarse cell as can be seen cell aggregation was conducted by dividing the x and y axes into 5 and 15 equidistant intervals the cells at the well locations are kept fine at all times the simulation was conducted for a period of 5500 days the temperature solution at three different times is shown in fig 10 figure a represents the temperature solution at fine scale figure b the solution on the amr grid figure c shows the coarse scale solution and figure d shows the node distribution for the amr simulation run the solution on the amr grid demonstrates a particularly good match with the fine scale solution the node distribution shows high concentration along the front and at the well locations and low concentration behind and ahead of the front where no significant changes are observed this considerably lowers the computational time as compared to running the fine scale model the coarse scale solution differs notably from the amr and fine scale solution with a faster cold front propagation at the coarse grid which is more pronounced in comparison at late times t d 0 3 and 1 the relative error of the amr solution is significantly lower than the coarse solution in both the l2 and l infinity norm fig 11 moreover the number of cells is considerably reduced see fig 12 ranging from 8 to 60 the trend shows an overall increase as the front propagates and a decrease when the cold front has reached the producing well which results in coarsening at locations where no more thermal variations are detected this considerably improves the performance of simulation since the amr approach is much more favourable in terms of efficient use of computational resources see table 5 in section 6 5 2 case 2 sugar cube shale model shales are often neglected in conventional reservoir simulation as the convective flow is never acquired in shales due to low permeability for geothermal applications they represent an important source of heat for thermal recharge of the cold water front modelling of the shales however significantly increases computational time since shales often occupy a significant amount of computational grid here we test an application of our amr approach to a sugar cube model where cubes represents shale bodies and space between them fluvial channels we use a simple 2d setup with in total a 5 by 6 shale block configuration shale blocks have a permeability of 10 2 md while the sand bodies have a permeability of 10 3 md the injector and producer are placed at opposite corners of the reservoir as shown in the fig 13 level 0 consists of 4588 fine cells level 1 is constructed differently from the conventional amr approach with the sand channels kept at fine level and only the shale blocks are coarsened by a ratio of 100 10 10 the coarse grid level 1 contains 1618 cells the simulation parameters for this model are specified in tables 6 and 7 in the appendix fig 14 depicts the temperature solution at three different times throughout the simulation t d 0 1 0 3 and 0 7 similarly to the previous example the solution is shown for a the fine grid b the amr grid and c the coarse grid as can be seen on the amr figure b the grid refines as soon as the cold front arrives at proximity to a shale body the cold front is accurately represented on the amr grid and there are no differences compared to the fine grid on the coarse grid however the cold front propagates further than for the fine and amr model which is clearly visible at the late time recording t d 0 7 when the cold front passes part of the shales blocks and these have cooled down coarsening occurs as observed at t d 0 7 fig 15 depicts the error distribution through time of both the amr and coarse model relative to the fine model as can be seen in fig 15 the error of the coarse model is significantly larger than for the amr model where the error is close to zero the high frequency changes in the error especially observed in the l norm seem to correlate with refinement and coarsening of the mesh in between timesteps similar to what was observed in berrone 2010 the percentage of cells used in the amr grid relative to the number of cells used in the fine grid is shown in fig 16 as can be seen the percentage of cells ranges from roughly 35 to around 90 halfway through the simulation when the cold front reaches the producing well and then lowers to 65 when shale blocks proximal to the injector wells have cooled down to injection temperature and hence coarsening occurs as observed the computational time and effort was considerably reduced throughout the simulation and the amr solution outcome shows a very accurate representation of the fine scale model see table 5 in section 6 5 3 case 3 fluvial heterogeneous model our amr framework was tested for a heterogeneous reservoir with a low net to gross ratio n g 35 the permeability field ranges from 5 to 3400 md with a significant amount of shale regions present the hierarchical grid for this example is a structured grid and it comprises two levels the finest grid level 0 consists of 2400 grid cells with 40 cells in the x direction and 60 cells in the y direction level 1 was reduced to 150 mesh elements where aggregation was done using 4x4 fine mesh elements resulting in 10 grid cells in the x direction and 15 grid cells in the y direction the permeability field along with the hierarchical grid for this example is shown in fig 17 the location of the injector i and producer p are depicted in yellow on the permeability distribution the simulation parameters for this model are specified in tables 6 and 7 in the appendix the simulation was conducted until cold water breakthrough reached the producing well fig 18 illustrates the temperature solution at different times throughout the simulation for each time shown figure a represents the fine scale solution figure b is the amr solution and figure c is the coarse scale solution the grid is kept at its finest level at well locations the amr mesh exhibits a significant improvement in temperature solution compared to the solution on the coarse grid refinement is mainly focused at the front and slightly beyond the front while areas where insignificant changes occur remain coarse important details such as fingering effects at the cold water front which are neglected on the coarse grid are clearly visible in both fine and amr solutions which results in a more accurate representation of this physical phenomenon the relative error throughout the simulation run was recorded where the fine model is taken as reference solution for comparison between the coarse and amr model fig 19 shows the l2 norm and the l infinity norm error in time as can be seen the marked improvement is also recorded in the error analysis where the error between the coarse and fine model is notably larger than the error between the amr and fine model the l2 norm remains relatively constant for the amr solution whereas it increases slightly in time for the coarse solution the number of grid cells used in the simulation ranges from 8 to 70 throughout the simulation see fig 20 this represents a significant improvement in computational effort and time while still capturing important features see table 5 in section 6 5 4 case 4 reactive transport carbonate reservoirs host a major part of the world s hydrocarbon reserves but besides hydrocarbon reserves the ongoing energy transition has resulted in an increase interest in geothermal systems where many are hosted by carbonate rocks these reservoirs can have heavily fractured and karstified intervals resulting in unforeseen hazards during drilling furthermore naturally fractured carbonate reservoirs contain a large uncertainty in flow response due to the poor ability to predict the spatial distribution of discontinuity networks at reservoir scale another important process related to dissolution is well acidization used to increase the production this process involves the dissolution of reservoir rock to stimulate flow towards the wells these chemical reactions are localized and form important features for accurate representation of the flow response furthermore reaction rates which occur during dissolution are high resulting in a sharp front in the flow response moreover during dissolution formation and development of an unstable dissolution front with multiple wormholes can occur and its modeling is quite sensitive to the resolution shaik et al 2018 in near well acidization processes the regime which forms a single dominating wormhole is the most preferable it is therefore important to accurately predict this unstable dissolution while keeping the computational time reasonable amr is therefore a good solution to model these reservoirs and chemical processes to solution requirements in the flow example analyzed in this study dissolution involves the following simple reaction where carbonate is dissolved 22 caco 3 s ca 2 co 3 2 the kinetic reaction rate for this reaction is 23 r k a k k 1 q k s p s s where a is the mineral surface area k k is the kinetic reaction constant q is the ion activity product k s p is the equilibrium product and s s is the solid saturation permeability is updated using a power law relationship defined as follows 24 k k 0 ϕ ϕ 0 n where k 0 and ϕ 0 are the initial permeability and porosity and n is the power law exponent the proposed model simulates the phenomenon of unstable wormhole formation triggered by small perturbations in permeability on one side of the reservoir an injector well is placed which is perforated throughout the whole thickness on the other side the producer well is placed also spanning the entire thickness of the reservoir the model described in this example has dimensions of 100 by 100 meters a constant permeability of 1 md is used with 5 of random noise the left illustration in fig 21 shows the well locations along with the permeability of the reservoir the hierarchical grid consists of two levels where level 0 is an unstructured grid containing 2194 triangular cells cell aggregation was conducted to construct level 1 where the x and y axes were divided in 10 equidistant intervals resulting in a grid with only 100 cells level 0 and 1 are shown in fig 21 the simulation parameters for this model are specified in tables 8 and 9 in the appendix the amr simulation was started at the coarse level while keeping the well cells at the finest level throughout the entire simulation run for this application the adaptivity criterion is based on the solid composition x c a c o 3 the adaptivity criteria used in this example are defined as follows 25 if δ x c a c o 3 i j 0 02 mark cells i and j for refinement if δ x c a c o 3 l 0 01 l i mark cells i i for coarsening where δ x c a c o 3 is the difference in composition of the calcium carbonate c a c o 3 component the simulation was recorded at three different times 0 13 0 25 and at the final time expressed in dimensionless time fig 22 depicts the solid composition the composition of c a c o 3 in time where figure a is the fine scale solution figure b is the amr solution figure c is the coarse scale solution and figure d represents the node distribution of the amr grid as can be seen the amr solution is considerably more accurate than the coarse scale solution the far propagating wormhole at t d 1 which is present in both the fine scale solution and the amr solution is not well represented on the coarse scale solution where two extensive wormholes are present the amr solution however shows a very good representation of the fine scale solution throughout time the most extensive wormhole exhibits slight differences in thickness and some minor variations are observed at the other smaller wormholes the node distribution follows the front which is in this example quite dispersed resulting in refinement spanning a wide area especially at the last time step however considerable computational resources are saved at the early stage of the simulation to quantify the differences between fine scale amr and coarse scale solutions an error analysis was conducted here again both the l2 norm and the l infinity norm were computed for the amr and coarse model relative to the fine scale solution the graphs in fig 23 depict the outcome as can be seen the amr error is once more significantly less than the coarse model for both norms for the l2 norm the coarse fine relative error is three times greater than the amr fine error at the final time step the l infinity norm of the coarse fine error starts low at the first time step where no extensive propagation is observed and where the model is close to the initial conditions but then rapidly increases to 0 8 and remains more or less constant throughout the l infinity norm of the amr fine error seems to increase in time this is due to the propagation of initially small errors in the solution note however that the relatively big error for both the amr and coarse scale model are not representative for this example and are related to another type of instability in the solution similarly to the previous example we have analyzed the total number of cells used in the amr model relative to the total number of cells contained in the fine scale grid the graph in fig 24 shows this quantity expressed in percentage as can be seen the number of cells used during the amr simulation is overall less than the number of cells present in the fine scale model initially the number of cells starts at 20 which represents the use of the coarsest level with both left and right boundaries kept at the finest level it then increases fairly steeply to around 95 due to the high injection velocity which corresponds to the dominating wormhole regime around the end of the simulation the number of cells starts to decrease which indicates coarsening at some locations see table 5 in the next section for the actual computational time although almost 100 of the cells is used at two thirds of the simulation which is computationally expensive considerable resources are saved in the beginning moreover this problem is sensitive to the resolution which requires refinement at many locations in order to accurately capture the wormhole propagation 6 discussion in the results section all computational speed up was indicated in terms of of active cells w r t fine scale in this section the actual cpu times are highlighted for all cases in order to have a fair comparison the same nonlinear solver newton s based update with a fixed number of iterations linear solver direct one and time stepping strategy is used for the fine coarse and amr runs all these results are shown in table 5 even though the overhead of the amr method is substantial this can easily be explained by the non vectorized python implementation of the amr procedure vs highly optimized python and c implementation of the conventional simulation used in the coarse and fine simulation since the scope of this work is a proof of concept of the proposed amr procedure prototyped outside of the simulation loop our amr code has not been optimized yet this can be solved by either an application of numba just in time compiler for python or rewriting the procedure in c an expected speed up in our experience is around two orders of magnitude when compared to the original python implementation thereby reducing the overhead to around 1 5 of the runtime of the amr method and making it a viable strategy for geoscience applications the framework presented in this paper is developed in the darts platform which can be used for a more general set of applications related to the energy transition however it is important to note the major differences in various energy applications for example in two types of applications shown in this study geothermal and chemical dissolution cases the coarser representation is still capable of accurately capturing important features of the geothermal dynamic behaviour the coarse scale simulation in chemical dissolution however completely fails to represent the same dynamic behaviour dissolution pattern and effective characterization of the process e g effective rock dissolution it is therefore evident as is also pointed out in the literature that problems contained localized sharp gradients can greatly benefit from the amr technique 7 conclusions this study aimed at developing an adaptive mesh refinement amr technique in delft advanced research terra simulator darts for general purpose reservoir simulation the developed amr framework consists of a multi level hierarchical grid where levels are constructed through a mesh partitioning of the fine scale model the static geological model which is represented by an array of properties e g volume and porosity the framework consists of the construction of the coarse levels through cell aggregation of the next consecutive fine level at the pre processing stage the method used to aggregate fine cells includes the grouping of subdomains whose centroids are found within a predefined 3d domain in this study domains are grouped by the partitioning of the x y and z axes into equidistant intervals however this strategy can easily be changed and improved the aggregation of the subdomains to form a coarser level is stored as an array of indices for the next stages which consists of the indices of the fine cells comprised in its coarse control volume for each coarse cell next in order to solve the relevant governing equations the flow must be computed at each interface present in the mesh we therefore generate a list called a connectivity list describing all neighbouring cells within each level and between levels the fine scale transmissibility is then computed using the permeability field hereafter a flow based upscaling is applied in order to acquire the transmissibility of coarser levels and the inter level transmissibility each control volume has defined parameters that are relevant for solving the system volume porosity depth etc once the hierarchy of levels is complete the simulation can be started adaptivity check is performed at every time step using criteria specific to the application once the regions for coarsening and refinement are defined the solution is prolongated to the finest meshing level and subsequently restricted from fine to the adaptive simulation grid a new connection list and grid properties are constructed for the new coarsened schema once it is completed the simulation runs for the next time step using the constructed simulation model the accuracy of the method was demonstrated for geothermal applications two models were tested including a homogeneous model with unstructured gridding a synthetic sugar cube like model with high permeability channels surrounded by shale blocks and a heterogeneous fluvial system model with a low net to gross ratio high levels of solution accuracy relative to the reference fine scale results are observed for both cases an error analysis was conducted to record the differences between the amr and the coarse solution relative to the reference fine scale solution the error resulting from the amr model is significantly lower than for the coarse model for all tested problems the overall percentage of grid cells used in the amr model relative to the fine scale model is considerably decreased for most problems to conclude the developed amr method shows high levels of accuracy for both homogeneous and heterogeneous models and can be used for geothermal applications as well as for other applications implemented in darts the number of cells in the amr simulation relative to the total number of cells of the finest level is considerably reduced which is very favourable in terms of efficient use of computational resources the framework is applicable to two and three dimensional models and for unstructured as well as structured meshes the applicability of the method to unstructured grids provides an effective means for solving complex geological systems credit authorship contribution statement stephan de hoop methodology validation writing review editing elodie jones software investigation writing original draft denis voskov conceptualization methodology supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like to thank the entire darts group for their assistance and help we would also like to acknowledge financial support from the sfb1313 research group at university of stuttgart and particularly like to thank beatrix becker and prof rainer helmig for their valuable time and help appendix 
