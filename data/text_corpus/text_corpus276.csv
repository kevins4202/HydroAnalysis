index,text
1380,surfactant enhanced aquifer remediation sear is an appropriate method for dnapl contaminated aquifer remediation however due to the high cost of the sear method finding the optimal remediation scenario is usually essential embedding numerical simulation models of dnapl remediation within the optimization routines are computationally expensive and in this situation using surrogate models instead of numerical models is a proper alternative ensemble methods are also utilized to enhance the accuracy of surrogate models and in this study the stacking ensemble method was applied and compared with conventional methods first six machine learning methods were used as surrogate models and various feature scaling techniques were employed and their impact on the models performance was evaluated also bagging and boosting homogeneous ensemble methods were used to improve the base models accuracy a total of six stand alone surrogate models and 12 homogeneous ensemble models were used as the base input models of the stacking ensemble model due to the large size of the stacking model bayesian hyper parameter optimization method was used to find its optimal hyper parameters the results showed that the bayesian hyper parameter optimization method had better performance than common methods such as random search and grid search the artificial neural network model whose input data was scaled by the power transformer method had the best performance with a cross validation rmse of 0 065 the boosting method increased the base models accuracy more than other homogeneous methods and the best boosting model had a test rmse of 0 039 the stacking ensemble method significantly increased the base models accuracy and performed better than other ensemble methods the best ensemble surrogate model constructed with stacking had a cross validation rmse of 0 016 finally a differential evolution optimization model was used by substituting the stacking ensemble model with the numerical model and the optimal remediation strategy was obtained at a total cost of 72 706 keywords dnapl surfactant enhanced aquifer remediation ensemble surrogate model stacking bayesian hyper parameter optimization feature scaling 1 introduction the appearance of dnapls one of the most hazardous environmental pollutants in groundwater resources has become widespread due to abundant usage human errors improper disposal and accidents like spillage kueper and mcwhorter 1991 akyol and turkkan 2018 due to their accumulation in the subsurface low mobility low solubility and relatively high density and interfacial tension dnapls may remain in the aquifers for a long time causing a toxic contaminant qin et al 2007 akyol et al 2013 mohammed et al 2019 epa has classified some dnapls like tetrachloroethylene pce as likely to be a carcinogen to humans epa 2011 karaoglu et al 2019 by penetrating dnapls to the subsurface due to the ability of the porous medium to hold a portion of dnapls the amount of leaked liquids continuously decreases and dnapls stay as residual danpl however if the amount of dnapls exceeds the soil retention capacity it can continue to move to the saturation zone and bedrock or an impermeable layer the extra dnapls move downward and accumulate on the impermeable layer in the form of pools with higher saturation and act as a long term continuous source of toxic contamination qin et al 2007 suthersan et al 2016 akyol 2018 karaoglu et al 2019 traditional remediation techniques such as the pump and treat method cannot be efficiently used for dnapls remediation because of their low solubility and mobility high density and high interfacial tension of dnapl with water qin et al 2007 mohammed et al 2019 the surfactant enhanced aquifer remediation sear is widely used to remove dnapl pollutants from contaminated groundwater pennell et al 1993 shiau et al 2002 adding surfactant to the contaminated water increases the mobility and solubility of dnapls delshad et al 1996 thus the efficiency of dnapl remediation significantly increases than the conventional pump and treat method nevertheless the sear approach is costly due to the high costs of applied materials especially the surfactant and cosolvent therefore finding an optimal remediation strategy by minimizing the costs and maximizing the performance is of great importance luo et al 2013 jiang et al 2015 mathematical models and optimization tools can provide an optimal strategy in these situations guan and aral 1999 zhang et al 2005 schaerlaekens et al 2006 he et al 2009 however embedding the simulation models in the optimization routines may cause the approach to become computationally too expensive and even practically impossible luo et al 2013 chu and lu 2015a ouyang et al 2017a one alternative approach to solve this dilemma is replacing the sear simulation model with a fast to run machine learning ml model that emulates the original simulation model with acceptable accuracy this approach is also called surrogate modeling lu et al 2013 used the kriging technique instead of the real simulation model in the dnapl remediation optimization to decrease computational times luo et al 2013 applied radial basis function artificial neural network rbfann as a surrogate model in the same problem and reported that the surrogate model s errors were less than 5 relative to the real simulation model luo and lu 2014a compared three ml models performance including kriging rbfann and polynomial regression pr and showed two latter have better performance and kriging slightly outperforms the rbfann model in another work luo and lu 2014b used kriging and rbfann methods for sobol sensitivity analysis of the sear method they concluded that both models have enough accuracy to replace the sear simulation model based on the residual errors in terms of mean square error mse and the coefficient of determination r2 luo and lu 2014c used mixed integer non linear programming minlp to find the optimal dnapls remediation strategy an rbfann model was used instead of the sear numerical model and then minlp was run to find the optimal strategy the problem was also solved by a genetic algorithm ga hou et al 2015 applied three ml models of support vector regression svr kriging and rbfann to optimize dnapl contaminated aquifer remediation although the accuracy of a stand alone surrogate model might be generally satisfactory its performance in some parts of the feasible space of the input variables may not be stable and produces relatively large errors so this can direct the search algorithm towards the wrong values researches showed that combining the outputs of different surrogate models to construct an ensemble surrogate es model can provide higher accuracy and robustness zerpa et al 2005 goel et al 2007 acar and rais rohani 2009 viana et al 2009 acar 2010 zhou et al 2013 according to the cutting edge literature review most studies have used a weighted average to combine outputs of different surrogate models and create an es model jiang et al 2015 examined four surrogate models of rbfann svr kriging and kernel extreme learning machines kelm and then selected kelm and kriging as the best ml models to construct an es model they developed an optimization model to find the optimal weights of each ml model by minimizing the rmse of the es model results indicated that the es model outperforms the stand alone ml models chu and lu 2015b estimated the weight coefficients of an es model combining polynomial regression model rbfann and kriging according to the predicted residual sum of squares press and found that obtained es model improve the performance of surrogate modeling hou et al 2017 used set pair analysis spa to construct an es model the surrogate models used for this purpose were rbfann svr and kriging models in which the kriging model had the best performance two ensemble methods were developed in their work the first by combining rbfann svr and kriging models using the spa method and the second by combining several kriging models with different training sample sets the results showed that es models especially the one constructed with four kriging models had better estimation accuracy than the stand alone surrogate models ouyang et al 2017b applied multi gene genetic programming mggp kriging and svr as surrogate models they used the optimal weighted average surrogate method to construct an es model where the optimal weights were obtained by minimizing the mean square error mse of the ensemble model the results indicated that the es model based on kriging and mggp had the best performance among other es models and stand alone surrogate models ouyang et al 2017c developed a set of es models by a weighted combination of five surrogate models polynomial response surface rbfann kriging svr and gaussian process regression gpr similar to their previous work optimal weights were obtained by minimizing the mean square error and results indicated that the es model constructed by combining all five surrogate models had the best performance among the other models hou and lu 2018 used four kriging models as surrogate models trained with different training datasets also they used the spa method to construct an es model and embedded it as a constraint condition into the optimization model hou et al 2019 studied the application of an es model within minlp for optimization of the dnapl contaminated aquifer remediation process the kelm kriging and svr models were used as base learners to create four ensemble models they followed two procedures to build es models using all three surrogate models and using the best surrogate model by developing four base learners with four different training sample sets es models were constructed using the weighted average of different surrogate models outputs two methods also calculated the weights 1 based on the rmse ratio of the surrogate models and 2 solving an optimization problem to find optimal weights they concluded that es models performed better than stand alone surrogate models and reported that the most accurate es model had the r2 and maximum relative error equal to 0 9837 and 13 14 respectively many studies related to the application of es models in sear optimization have used only one es method various research studies have also been focused on weighted average to combine surrogate models however there are some new efficient ensemble methods that have not been investigated in the dnapls remediation context this study aims at comparing the performance of different es methods including previously used simple average voting spa weighted average based on press press wa weighted average based on rmse ratio rmse wa and optimized weighted average based on minimizing rmse of ensemble model owa and new approaches of bagging boosting and stacking in order to introduce the superior es model as far as the authors know to date these new ensemble methods have not been used as an es model in optimizing dnapl contaminated aquifer remediation the weighted average es methods such as voting spa press wa and rmse wa methods have the lowest computational burden however they are simple es methods and cannot combine base models nonlinearly or train them with the required characteristics the bagging method combines several base models and reduces the variance of the final es model by introducing randomness into its construction procedure géron 2019 boosting methods can construct a sequence of base models so that each model corrects the weaknesses of the former model hastie et al 2009 the stacking method can combine base models in a non linear or black box way using different ml models thus having the ability to construct more complex and accurate es models wolpert 1992 zhou 2012 géron 2019 besides the effects of various pre processing methods and hyper parameter tunning strategies on the performance of surrogate models are also investigated additionally a wide range of conventional surrogate models including artificial neural network ann kernel ridge regression krr k nearest neighborhood knn support vector regression svr gaussian process regression gpr and decision tree dt have also been examined in this research work it is noticeable that models such as krr knn and dt have not been used in similar studies some advantages and disadvantages of surrogate models used in this study are presented in table 1 in brief this paper as an extension of previous ones intends to perform a comprehensive study on the performance of various stand alone surrogate models and es methods for function approximation and applied in optimizing dnapl contaminated aquifer remediation the novel aspects of this paper include the following 1 three new ensemble methods including bagging boosting and stacking were used to improve the performance and accuracy of surrogate modeling which have not been applied in the sear method optimization studies yet 2 several scalers have been applied in the pre processing stage and their effect on the surrogate model performance has been investigated 3 the bayesian hyper parameter optimization method has been used to find optimal hyper parameters of the stand alone surrogate and ensemble models and its performance has been compared with the conventional methods including random search with grid search techniques 2 methodology 2 1 sear simulation model remediation of dnapls by the pump and treat method due to the characteristics of dnapls such as high interfacial tension of dnapl with water low solubility and mobility and high density is not efficient akyol and turkkan 2018 mohammed et al 2019 the surfactant enhanced aquifer remediation sear is a remediation technique which extensively utilized in the dnapl contaminated aquifer remediation pennell et al 1993 shiau et al 2002 akyol 2018 the pump and treat is not an efficient technique for remediation of dnapls due to dnapls characteristics such as low solubility and mobility and high interfacial tension of dnapl with water mohammed et al 2019 in the sear method by adding surfactants and cosolvents the interfacial tension between dnapl and water decreases and the solubility of dnapl increases thus the efficiency of dnapl remediation significantly improves than the conventional pump and treat method delshad et al 1996 akyol et al 2013 karaoglu et al 2019 mohammed et al 2019 delshad et al 1996 developed a 3d multi component multi phase compositional simulation model for the analysis of the sear method for napls contaminants in this simulation model each component s mass balance equation is solved by the finite difference method delshad et al 1996 for more details about the mathematical form of the equations and numerical schemes of solving the equations readers can refer to delshad et al 1996 qin et al 2007 lu et al 2013 ouyang et al 2017c the numerical simulation model developed by delshad et al 1996 was then expanded at the university of texas called the university of texas chemical compositional simulator utchem utchem is known as a general model of sear process simulation and has been used in various studies for the remediation of napls zhang et al 2003 schaerlaekens et al 2005 ouyang et al 2017c chemical physical and biological properties as well as porous media heterogeneity non equilibrium sorption decay geochemical reactions and aquifer remediation by the sear method are considered in utchem delshad et al 1996 qin et al 2007 in this study surfactant enhanced aquifer remediation was performed using utchem 2 2 surrogate models 2 2 1 ann rbf neural networks have been used in most previous studies related to aquifer remediation optimization using surrogate models luo and lu 2014a jiang et al 2015 hou et al 2017 luo et al 2018 and mlp neural network has been less examined in this study in addition to the rbf neural network mlp neural network was also employed multilayer perceptron mlp neural networks can learn a non linear function f r m r o by training on a sample set as a supervised learning algorithm where m and o are the numbers of input and output dimensions respectively schalkoff 1997 kriesel 2007 mlp neural networks may have more than one hidden layer and have different activation functions mlp uses the squared error loss function for training its weights and biases which is written as 1 loss y y w 1 2 y y 2 2 2 w 2 2 where w represents the weights w 2 2 represents ridge regularization term and is a non negative hyper parameter which controls the penalty s magnitude in order to find the best ann structure various factors including the number of hidden layers number of neurons in hidden layers pre processing methods the type of activation functions identity logistic sigmoid hyperbolic tangent and rectified linear unit generalization hyper parameter values learning rate values and solvers for finding optimal weights have been tested 2 2 2 gpr gpr applies the gaussian process gp for regression purposes the gp is a set of random variables any finite number of which have a joint gaussian distribution rasmussen 2004 the gp is defined by a mean function representing the mean at any point in the input space and a covariance function that sets the covariance between points 2 f x gp m x k x x gpr for defining the covariance of a prior distribution over the target functions uses the kernel also it uses the observed data training data to define a likelihood function posterior distribution over target functions is obtained using bayes theorem and posterior mean is used for prediction rasmussen 2004 pedregosa et al 2012 predictive equations for gpr is expressed as follows 3 f x y x n f cov f 4 f e f x y x k x x k x x σ n 2 i 1 y 5 cov f k x x k x x k x x σ n 2 i 1 k x x where x and y are respectively input and output of training dataset f is the distribution of the target values k is the covariance fuction which applies kernel functions and σ is called noise variance in order to include the inherent noise into the distribution to achieve the best gpr several factors such as different kernels σ term noise level and the number of optimizers restart for finding optimal kernel s hyper parameters were considered the kernels used in this study are linear polynomial radial basis function rbf rational quadratic exp sine squared matérn sigmoid laplacian and chi squared functions they can be expressed as follows rasmussen and williams 2005 zhang et al 2007 rupp 2015 6 linear k x y x y 7 polynomi a l k x y x y l c 0 d 8 rbf k x y exp 1 2 l 2 x y 2 9 rational quadratic k x y 1 x y 2 2 α l 2 α 10 exp sine squared k x y exp 2 l 2 sin 2 π x y p 11 mat é rn k x y 2 1 υ γ υ 2 υ 1 2 υ x y l υ k υ 2 υ x y l 12 sigmoid k x y tanh x y l c 0 13 laplacian k x y exp x y l 14 ch i 2 k x y exp 1 l i x i y i 2 x i y i where l 0 is the length scale parameter such a wide range of different kernels in aquifer remediation optimization studies have not been utilized to obtain the best gpr as a surrogate model 2 2 3 krr kernel ridge regression incorporates ridge regression and kernel trick to learn non linear functions ridge regression shrinks the regression coefficients by adding a regularization term to linear regression s cost function the objective function of ridge regression for training the model is as follows hastie et al 2009 15 min w xw y 2 λ w 2 here xw y 2 is the sum of squared error and λ w 2 is l2 regularization term with the complexity parameter λ 0 that controls the amount of coefficients shrinkage in order to give the capability of learning non linear functions to the model x is replaced by φ x and the coefficients are extracted by 16 w φ x φ x φ x λi 1 y after obtaining the weights for new test data x the target value y can be calculated by 17 y ϕ x φ x φ x φ x λi 1 y kernel k can be considered as k x 1 x 2 ϕ x 1 ϕ x 2 and φ x as a matrix where each row is ϕ x i by replacing φ x φ x by matrix k calculated as k ij k x i x j and ϕ x φ x by i 1 n ϕ x ϕ x i i 1 n k x x i all dot products express in terms of kernels given α k λi 1 y the target value of new test data can be obtained as 18 y i 1 n α i k x x i the best krr model is determined after checking different kernels complexity parameter values and kernel s hyper parameter values the kernels used for krr are the same as those of the gpr model in this study kernel s hyper parameters of the krr model were estimated by hyper parameters optimization methods while in gpr they were selected based on the gradient ascent on the marginal likelihood function 2 2 4 svr support vector regression svr is a supervised learning method and a variant of support vector machine svm which can be applied for classification and regression unlike the linear regression model whose training is based on minimizing the sum of squared errors svr minimizes simultaneously the regression model s weight values and sum of deviations from ε tube the error is also regulated by setting an absolute error less than or equal to maximum error margin ε in constraints with the x and y vectors as input and outputs of training samples an optimization problem called primal problem is solved by svr smola and schölkopf 2004 clarke et al 2005 19 min w b ζ ζ 1 2 w w c i 1 n ζ i ζ i 20 subject to y i w ϕ x i b ε ζ i w ϕ x i b y i ε ζ i ζ ζ 0 i 1 n by applying the lagrangian principle the dual form optimization problem is obtained as follows smola and schölkopf 2004 pan et al 2010 zhou et al 2013 jiang et al 2015 21 min α α 1 2 α α q α α ε e α α y α α 22 s u b j e c t t o e t α α 0 0 α i α i c i 1 n in which w is the weighting vector ζ and ζ are called slack variables to relieve hard constraint of the optimization problem c is a non negative parameter which indicates the trade off between the approximation and generalization of the trained model q ij k x i x j ϕ x i t ϕ x j is the kernel function and α and α are lagrange multipliers by solving the above problem by quadratic programming the regression function can be found as 23 i 1 n α i α i k x i x ρ in the context of dnapl contaminated aquifer remediation svr has been used as a surrogate model but only with one type of kernel this study investigated the performance of svr in terms of various types of kernels to achieve the best svr model nine different kernel types different regularization parameters different epsilon values and different kernel hyper parameter values have been tested the kernels used for svr are the same as those of the gpr and krr models 2 2 5 knn k nearest neighbors knn is a nonparametric method for unsupervised and supervised ml problems altman 1992 brownlee 2016 here supervised knn based learning is used for regression the estimation of the output variable in knn is done by a predetermined number k of its closest data in training samples in which every neighbor has a weight based on different weight functions various distance metrics can be used but minkowski distance that is a generalization of euclidean distance is more common to use kramer 2013 kuhn and johnson 2013 24 i 1 n x ai x bi 1 q where q 0 if q 1 minkowski distance is called manhattan distance and if q 2 it is equal to euclidean distance despite its simplicity and proper performance the knn model still has not been used as a surrogate model in similar studies for dnapl remediation in order to find the best knn model the different number of neighbors and associated radius different weight functions uniform inverse distance and gaussian kernel different algorithms for calculating the nearest neighbors ball tree kd tree and brute force and their parameters different distance metrics like manhattan and euclidean have been examined 2 2 6 dt decision tree dt learner is a group of ml algorithms used in statistical classification and regression decision trees belong to the group of supervised learning algorithms and most of them are based on a quantitative minimization called entropy dt is trained by several training samples to learn simple decision rules and split the training set in each node breiman et al 1984 hastie et al 2009 shalev shwartz and ben david 2014 considering the training samples of input vector x i r n i 1 l and output vector y r l space recursively is partitioned by a dt so that training samples with similar values are grouped together if s denotes samples in node m and each candidate split θ j t m defines by its feature j and threshold t m then the samples can be partitioned into s left and s right branches as below 25 s left θ x y x j t m s right θ s s left θ in order to find the best split in each node a split candidate that minimizes impurity must be selected hastie et al 2009 26 g s θ n left n m h s left θ n right n m h s right θ 27 θ argmin θ g s θ here the number of samples in node m is n m and h is an impurity function in node m for regression problems that the output variable is continuous at node m with s observations mean squared error is minimized as a residual metric to find an appropriate split point hastie et al 2009 28 h x m 1 n m i n m y i y m 2 y m 1 n m i n m y i other measures like mean absolute error and friedman mse may also be used in this study the best possible dt was obtained by evaluating different criteria including different maximum tree depth different number of samples required to split the criterion of split quality different number of samples required to be at a leaf node and different maximum leaf nodes 2 3 ensemble methods ensemble methods train several stand alone surrogate models called base learners and combine their outputs for solving the same problem ensemble methods that use one type of base learning algorithm are called homogeneous ensembles whereas those that employ multiple learning algorithms are called heterogeneous ensembles zhou 2012 the ensemble methods for combining surrogate models are diverse in this study several methods for creating es models used separately in previous studies were investigated together these are voting spa hou et al 2017 hou and lu 2018 press wa chu and lu 2015b rmse wa hou et al 2019 owa jiang et al 2015 ouyang et al 2017c hou et al 2019 were investigated in addition to the mentioned ensemble models bagging boosting and stacking es methods have also been examined for the first time for the dnapl contaminated aquifer remediation problem 2 3 1 set pair analysis spa given o x 1 x 2 x n as the set of contaminant removal rates obtained from the simulator model for the test samples and p k x 1 k x 2 k x n k as the set of contaminant removal rates obtained from the kth surrogate model for the test samples a set pair s o p k and its connection degree is obtained then the set pair based on the absolute error value between surrogate model and simulation model outputs are divided into four categories identity has an error of less than 0 3 mild discrepancy has an error of between 0 3 and 0 6 severe discrepancy has an error of between 0 6 and 1 and contradictory has an error of more than 1 using these categories the connection degree between the sear simulation model and kth surrogate model is calculated by wang et al 2009 hou and lu 2018 29 μ k i k n d 1 k n i 1 d 2 k n i 2 c n j here i k is the number of identity terms in set pair c is the number of contradistinction terms d 1 k is mild discrepancy terms and d 2 k is severe discrepancy terms the uncertainty component coefficients of discrepancy degrees i 1 and i 2 and the uncertainty component coefficients of contradictory j are 0 5 0 5 and 1 respectively given that the range of μ is 1 1 it must first be scaled to 0 1 range 30 u k μ k 0 5 0 5 eventually the set pair weight of the kth surrogate model w k is calculated as follows 31 w k u k k 1 4 u k 2 3 2 weighted average based on press press wa in this ensemble method one weight is chosen for each surrogate model calculated according to the press predicted residual sum of squares weighted average surrogate then the ensemble model is constructed by the weighted average of the surrogate models w i the weight coefficient of the ith surrogate model is calculated by 32 w i e i 0 05 e 1 i 1 n e i 0 05 e 1 33 e 1 n i 1 n e i where e i is the press error of the ith surrogate model and n is the number of surrogate models goel et al 2007 chu and lu 2015b 2 3 3 weighted average based on rmse ratio in this method weights are assigned to the surrogate models according to the inverse of the root mean square error ratio rmse 2 3 4 weighted average based on minimizing rmse in order to find the optimal weights an optimization problem is solved by minimizing the rmse of the es model as follows jiang et al 2015 hou et al 2019 34 min w rmse y es 1 m k 1 m i 1 n w i y i y actual 2 35 subject to i 1 n w i 1 where y i the output predicted by the ith surrogate model y actual the output of the simulation model and m is the number of test samples 2 3 5 bagging bagging bootstrap aggregation is an ensemble method that constructs several instances of an ml algorithm on random subsets of the initial set of training samples and then combines their predictions to produce a final prediction the final prediction is made by averaging over the individual surrogate models breiman 1996 géron 2019 generally bagging outperforms any of the base models breiman 1996 the bagging es model has less chance of overfitting and has a lower variance than individual predictors by introducing randomness into its construction procedure géron 2019 when random subsets are drawn without replacement it is called pasting breiman 1999 but if they are drawing with replacement then the algorithm is known as bagging breiman 1996 also if base surrogate models are created on subsets of both samples and features it is called random patches louppe and geurts 2012 the structure of the bagging method has been shown in fig 1 in order to find the best bagging es different number of base learners and different subsets of training samples were investigated 2 3 6 boosting boosting methods construct a sequence of surrogate models in a way that each model corrects the defects of the previous model among the boosting methods the adaboost method is the most popular freund and schapire 1997 hastie et al 2009 géron 2019 an adaboost estimator starts by training a regressor on the initial training dataset and for every consecutive iteration trains new regressor models on the training dataset but the weights of samples are modified based on the error of the current regressor prediction all samples are initially given a weight which is the same at the beginning and is equal to w i 1 n in the later iterations each sample s weight is modified based on the error value and weights of training samples that the predecessor estimator improperly predicted are boosted the structure of the boosting method has been shown in fig 2 in order to find the best boosting es model different numbers of base learners different learning rates and different error functions have been investigated it is noticeable that the loss function used for updating the weights after each boosting iteration and the learning rate determine each regressor s contribution 2 3 7 stacking stacking also called stacked generalization wolpert 1992 is an ensemble method in which a meta learner is trained to combine several base surrogate models the base surrogate models are called the first level model or base model and the learning model that combines them is called the second level model or meta model the core principle of stacking is to train base models using the training set and then the meta model is trained on the outputs of base models as input features the base models are often created using different learning algorithms and therefore stacking es models are often heterogeneous however it is also possible to create a homogeneous stacking es zhou 2012 when the same data used for training the base models is also used to create the new data set to train the meta level model overfitting will likely occur therefore it is recommended to use k fold cross validation to prevent overfitting and in each fold the out of fold oof part of the train set is predicted when using k fold cross validation the training set d is split into k part d 1 d k given t learning methods and by defining d j and d j d d j as training and test sets for the jth fold and h t j as a base learner that obtained by training the tth learning method on d j for each x i in d j the output of the learner h t j on x i can be denoted by z it finally after the cross validation process is completed the new dataset from the t individual base learners can be obtained as 36 d z i 1 z i 2 z it y i i 1 m then the meta level learner h will be trained on d generally after constructing a meta model h the base learners are reconstructed by training on the entire training set wolpert 1992 zhou 2012 géron 2019 in order to determine a relevant stacking es model different learning algorithms have been used as a meta model and its optimal hyper parameters have been determined by bo 2 4 feature scaling feature scaling fs is the most critical transformation that needs to apply to data almost all ml algorithms are sensitive to the scaling of the data for instance several components in the objective function of learning algorithms such as the ridge and lasso regularizers or the rbf kernel in svm expect input values like a standard normal distribution and neural networks often expect input values in the range 0 1 müller and guido 2016 géron 2019 there are several scaling techniques each of which can significantly influence the performance of an ml model 1 standard scaler this scaler standardizes each feature by subtracting the mean and dividing by variance thus the resulting feature has zero mean and unit variance and bringing all features to the same magnitude han et al 2011 albon 2018 standard scaler maps x i of feature a to x i by computing 37 x i x i a σ a here x i is a standardized form of x i a is the mean and σ a is the variance of the feature 2 robust scaler this scaler is similar to the standard scaler but uses statistics that are robust to outliers so instead of mean and variance median and interquartile range iqr have been used the iqr is the range between the 1st quartile and the 3rd quartile the difference between 75th and 25th percentiles müller and guido 2016 3 minmax scaler minmax scaler performs a linear transformation on each feature and shifts the dataset to a given range supposing that min a and max a are minimum and maximum values of feature a and new max a and new min a are minimum and maximum values of the desired range the transformation is obtained by han et al 2011 géron 2019 38 x i x i min a max a min a new max a new min a new min a 4 maxabs scaler maxabs scaler is similar to the minmax scaler but scales a feature in a way that the data lies within the range 1 to 1 for this purpose the dataset is divided into the maximum absolute value of the dataset 5 quantile transformer quantile transformer is a non linear monotonic transformer that put the features into the desired distribution based on g 1 f x formula where f is the cumulative distribution function of the feature and g 1 is the quantile function of the desired output distribution g gilchrist 2000 the output distribution is usually the normal or uniform distribution 6 power transformer power transforms are non linear monotonic parametric transformations that are applied to map data from existing distribution to as close to a gaussian like distribution as possible box cox and yeo johnson power transforms were used in this study the yeo johnson transform is given by yeo and johnson 2000 weisberg 2001 39 x i λ x i 1 λ 1 λ if λ 0 x i 0 ln x i 1 if λ 0 x i 0 x i 1 2 λ 1 2 λ if λ 2 x i 0 ln x i 1 if λ 2 x i 0 and the box cox transform is given by box and cox 1964 osborne 2010 40 x i λ x i λ 1 λ i f λ 0 l n x i i f λ 0 in both methods lambda is the transformation parameter and is estimated through maximum likelihood in this study all the fs methods mentioned are used to obtain the best possible surrogate models and their influence on the performance of surrogate models has been examined 2 5 bayesian hyper parameter optimization ml models used as surrogate models have many hyper parameters that control different aspects of their behavior and performance therefore finding optimal hyper parameters for surrogate models is essential goodfellow et al 2016 grid search and random search are two commonly accepted techniques for tuning hyper parameters bergstra and bengio 2012 hutter et al 2019 grid search the most used technique for hyper parameter optimization is a brute force technique for tuning hyper parameters using cross validation in grid search a set of multiple hyper parameters possible values for a surrogate model is specified then the surrogate model for each combination of hyper parameter values in the possible set is trained and each model is evaluated using cross validation to select the best model albon 2018 grid search is an exhaustive search and its weakness is the high computational cost which increases exponentially with the increasing number of hyper parameters goodfellow et al 2016 the grid search technique is suitable for situations where the number of the combination is not relatively high but if the hyper parameter search domain is too large the random search technique is often preferred grid search evaluates all possible combinations of hyper parameters but in contrast random search evaluates a given number of random combinations a random combination of hyper parameters is obtained in each iteration by choosing a random value for each hyper parameter from the distribution specified for it bergstra and bengio 2012 albon 2018 géron 2019 random search performance is better than grid search when some hyper parameters do not significantly affect the performance measure bergstra and bengio 2012 goodfellow et al 2016 hutter et al 2019 however when the surrogate model has many hyper parameters the number of models that need to be trained and evaluated will be huge and the process of finding an optimal hyper parameter can be computationally expensive wu et al 2019 grid search and random search are comparatively inefficient because they are entirely uninformed in each iteration from the past evaluations bayesian hyper parameter optimization bo is a derivative free global optimization method based on bayes theorem that considers past evaluations results and consciously selects a set of hyper parameters for evaluation at the next stage brochu et al 2010 wu et al 2019 to obtain the optimal hyper parameter mathematically the following optimization problem should be solved brochu et al 2010 shahriari et al 2016 41 x arg min x x f x here f x is the objective function x is the optimal set of hyper parameters and x can be any value in the x domain in bo the objective function takes in hyper parameters and outputs a score such as the rmse of cross validation that should be minimized usually the objective function is a black box function non convex noisy high dimensional and computationally expensive to evaluate brochu et al 2010 shahriari et al 2016 wu et al 2019 bo is an iterative method and operates by developing a probabilistic surrogate model of the objective function and applying the acquisition function to select the most promising hyper parameters for the next iteration in each iteration the probabilistic surrogate model is re trained by all objective function observations generated so far then by maximizing the acquisition function the efficiency of different candidate hyper parameters set is determined hutter et al 2019 the acquisition function is defined in a way that can automatically trade off exploration and exploitation brochu et al 2010 hutter et al 2019 evaluation of the black box objective function than the acquisition function which is cheap to compute and thus can be completely optimized is computationally more expensive shahriari et al 2016 there are several probabilistic models and acquisition functions for use in bo in this study the gaussian process model and the expected improvement have been used as the probabilistic model and the acquisition function respectively mockus et al 1978 defined the improvement function as 42 i x max 0 f x f x here f x is the probabilistic surrogate model gaussian process f x is the value of the best point observed so far and x is its location i x is non negative and becomes positive when the prediction in x is higher than the best value observed so far otherwise it becomes zero selected hyper parameters for the next iteration is obtained by maximizing the expected improvement brochu et al 2010 wu et al 2019 43 x arg max x e max 0 f t 1 x f x f x gp posterior predictive has a normal distribution with the mean μ x and the variance σ x at x as a result distribution of improvement function i x is normal with the mean μ x f x and a variance σ x the probability density function of i is computed as wu et al 2019 44 f i 1 2 π σ x exp μ x f x i 2 2 σ 2 x i 0 thereupon the expected improvement can be calculated as brochu et al 2010 45 e i i 0 i i 1 2 π σ x exp μ x f x i 2 2 σ 2 x d i σ x μ x f x σ x φ μ x f x σ x ϕ μ x f x σ x here φ and ϕ are cdf and pdf of the standard normal distribution respectively eq 45 can be calculated analytically mockus et al 1978 jones et al 1998 brochu et al 2010 as follows 46 e i μ x f x φ z σ x ϕ z i f σ x 0 0 i f σ x 0 z μ x f x σ x the bo procedure is shown in the following algorithm shahriari et al 2016 wu et al 2019 unlabelled table bayesian hyper parameter optimization algorithm for t 1 t 1 construct a gaussian model for the objective f given observations xi yi f xi for i 1 t 2 optimize acquisition function u based on the posterior distribution for selecting the next hyper parameter set xt 1 argmax u x 3 apply selected hyper parameter set to the objective function and then update the gaussian process model incorporating the new result repeat loop until max iteration or time is reached the expected improvement can automatically balance the trade off between exploiting and exploring eq 46 has two terms the first term is exploitation and the second one is exploration term lizotte 2008 suggests an ξ 0 parameter to express expected improvement in a generalized form that controls the trade off between exploiting and exploring 47 e i μ x f x ξ φ z σ x ϕ z if σ x 0 0 if σ x 0 z μ x f x ξ σ x parameter ξ determines the importance of exploration and exploitation so that higher ξ leads to more exploration and the importance of points by high μ x decreases relative to points in regions by high prediction uncertainty which has large σ x lizotte 2008 suggested ξ 0 01 and conclude that it works well in most cases 3 case study 3 1 site overview the shallow surficial aquifer in site 88 the location of the dry cleaning building at the marine corps base mcb camp lejeune jacksonville north carolina was selected as the case study aquifer the location of the aquifer and site 88 is shown in fig 3 there are no surface water bodies near the site and the nearest surface water is the new river located about 3000 ft 920 m west of the site duke engineering and services 1999 two aquifer systems exist in site 88 a low permeability clay aquitard separates the shallow surficial aquifer and the castle hayne aquifer the shallow surficial aquifer has a depth of approximately 20 ft 6 1 m below the ground surface bgs the castle hayne aquifer is further divided into the upper castle hayne 7 5 23 m the middle castle hayne 23 38 m and the lower castle hayne 38 m duke engineering and services 1999 ch2 hill 2010 duke engineering services co has investigated the contaminated zone at site 88 in cooperation with baker environmental during 1997 and the first half of 1998 duke engineering and services 1999 cone penetration tests and soil borings have been used to collect detailed lithologic data and soil samples a relatively uniform depositional sequence of sediments has been recognized in borings across the site the shallow surficial aquifer consists of fine to very fine grained sands and silt and this combination is the predominant type of sediments up to approximately 18 ft 5 5 m bgs the shallow aquifer is restricted beneath by a clay aquitard at a depth of approximately 20 ft 6 1 m bgs core samples have indicated that sediments at depths of 18 to 20 ft 5 5 to 6 1 m bgs become significantly finer e g clayey silt sediment becomes finer with increasing depth from sandy silt to clayey silt the location of a geological cross section and the geological characteristics of that cross section are also presented in fig 3 two samples have been collected at depths of 17 2 and 19 1 ft 5 2 and 5 8 m which had very similar mineralogy both had greater than 80 quartz with some feldspar and pyrite clay minerals had comprised 7 and 9 of the samples respectively with kaolinite illite chlorite and smectite duke engineering and services 1999 investigations have reported that the clay layer is approximately 14 16 ft 4 3 4 9 m thick beneath building 25 baker environmental 1998 duke engineering and services 1999 observation wells had also been installed to conduct pumping test the average values of 1 4 ft day 5 10 4 cm s for the hydraulic conductivity and 0 01 for the specific yield have been obtained by conducting the pumping test fig 3 also illustrates the potentiometric surface of the surficial aquifer obtained by the shallow monitoring wells 25 ft 7 6 m the potentiometric surface map shows a highly variable horizontal hydraulic gradient varying from 0 004 to 0 03 m m ch2 hill 2010 however as seen in fig 3 the hydraulic gradient is relatively low about 0 015 m m in the immediate of the contaminated area the water table fluctuates annually from about 7 9 ft 2 1 2 7 m bgs or about 16 18 ft 4 9 5 5 m above mean sea level amsl duke engineering and services 1999 the saturated zone of the shallow aquifer can be roughly divided into three permeability zones the upper zone 18 ft bgs 5 5 m bgs the middle zone 18 19 ft bgs 5 5 5 8 m bgs and the lower zone 19 20 ft bgs 5 8 6 1 m bgs the upper zone is generally composed of fine to very fine sand and silty sand therefore the upper zone is the most permeable zone of the shallow aquifer and its hydraulic conductivity is estimated to be about 1 4 ft day 5 10 4 cm s results from multilevel sampler mls samples confirm that the hydraulic conductivity of the basal clayey sandy silt zone is lower than the upper zone and is estimated to be approximately 0 28 ft day 1 10 4 cm s the lower zone has been composed predominantly of clayey silt with a hydraulic conductivity of approximately 5 10 5 cm s 0 14 ft day hydraulic conductivity of the upper and middle zone has been estimated based on the analysis of tracer test data from mlss and for bottom zone has been estimated based on grain size analyses on 72 soil samples from the bottom 3 ft of the aquifer battelle de s 2001 using site data gained from field investigations as input parameters a geosystem model of the site has been constructed by the university of texas austin applying utchem these input parameters have been updated and calibrated against the field data obtained from the results of the conservative interwell tracer test citt and partitioning interwell tracer test pitt duke engineering and services 1998a duke engineering and services 1998b 1999 the citt was performed using one non partitioning tracer bromide ion br the tracer curves were analyzed using the temporal moments method based upon the results from the citt the geosystem model has been then calibrated to reflect more closely the actual test domain duke engineering and services 1999 delshad et al 2000 battelle de s 2002 the effective permeability contrast at the different mls depths has been represented by the ratio of the first moments for the non partitioning tracer response curves the results indicated that the effective permeability of the basal silt layer is about four times lower than that of the overlying fine sands and permeability may be even lower near the basal contact of the shallow aquifer at the aquitard the simulations with the calibrated numerical model indicated a satisfactory match to the citt and pitt field results after completing the sear process the numerical model was further examined and calibrated to match field history delshad et al 2000 the simulated dnapl ipa and surfactant concentrations were adequately matched by field history in extraction wells delshad et al 2000 fig 4 shows the comparison of measured and history match of surfactant and ipa concentrations in an extraction well the study aquifer has been simulated using a three dimensional 25 25 16 mesh consisting of 10 000 grid blocks the horizontal extent of the model is 141 ft 23 m long by 80 ft 24 4 m wide with a 13 ft 4 m saturated thickness the dimensions of cells vary from 3 2 0 5 ft3 0 91 0 61 0 15 m3 in the center of the lowest layer to 24 12 2 ft3 7 31 3 66 0 61 m3 in the margin of the surface layer the pressures at two west east boundaries have been kept constant to establish a local hydraulic gradient of 0 015 representing observed field static water level conditions in the simulated region the other boundaries have been assumed as the no flux boundary the aquifer porosity in the model has a mean of 0 28 and a standard deviation of 0 05 the permeability in vertical is 10 of the permeability in horizontal duke engineering and services 1999 battelle de s 2002 regarding the thickness of the clay aquitard at the base of the shallow aquifer the aquitard provides adequate protection against further downward migration of dnapl contamination to the underlying castle hayne aquifer duke engineering and services 1999 the aquitard grid blocks have been treated as a no flux boundary and were assigned a low porosity of 0 01 and a low permeability of 5 6 10 4 ft day 2 10 7 cm s the distribution of the initial concentration volume fraction of the pollutant is depicted in fig 5 this contamination has been caused by a spill of pce and its penetration into the ground for 300 day also the permeability values of different layers are shown in fig 5b the university of texas austin performed the surfactant selection experiments and the surfactant formulation was designed specifically for site 88 battelle de s 2002 solubility data for site 88 dnapl obtained from laboratory experiments have been used to calibrate the phase behavior model parameters ooi 1998 delshad et al 2000 in addition to modeling surfactant phase behavior the physical parameters including microemulsion viscosity and microemulsion dnapl interfacial tension were also calibrated against experimental measurements delshad et al 2000 a total of 155 surfactant formulations have been screened by examining the phase behavior and measuring phase properties such as microemulsion viscosity weerasooriya et al 1999 delshad et al 2000 battelle de s 2002 also soil column experiments to ensure surfactant compatibility with the aquifer soils have been conducted moreover the required concentration of calcium to mitigate ion exchange has been calibrated by these experiments the values of the other physical and chemical parameters used in the sear simulation model are presented in table 2 due to the characteristics of the pce contaminated area a regular 7 spot for wells pattern was used regular 7 spot well pattern includes an extraction well in the center of the contaminated area and six injection wells around it this pattern has been obtained from evaluating various line drive and k spot patterns in previous studies on this site shams et al 2021 simulation indicated that the highest dnapl saturations are located within the low permeability sediment layer i e clayey silt just above a clay aquitard at a depth of approximately 18 20 ft 5 5 6 1 m bgs first water flush was applied for ten days and then 4 alfoterra 145 4po sulfate as a surfactant plus 16 ipa isopropyl alcohol as a cosolvent was injected hydraulic equilibrium was also achieved by setting the pumping and injection rate equal 3 2 surrogate models of the numerical simulation model in the present research work duration time for the remediation and the rate of injection wells were considered as input variables of the surrogate models the injection rate of wells was in the range of 0 200 ft3 day 0 5 66 m3 day and the remediation duration was in the range of 10 250 days for each set of randomly generated input variables the sear simulation model was run and the obtained pce removal rate was considered as the output variable therefore for surrogate model construction there were seven input variables and one output variable in general the more training samples the more accurate the surrogate model will be on the other hand the more training samples there are the greater the computational burden of generating training samples training surrogate models and predicting some surrogate models luo et al 2019 therefore selecting the appropriate number of training samples is a dilemma and a trade off must be made between generating more training samples and the resulting computational load some studies have considered the minimum number of training samples to be ten times the number of input variables 10n jin et al 2001 loeppky et al 2009 østergård et al 2018 however in other similar studies different ratios from 8n to 23n have been used luo and lu 2014a 2014b chu and lu 2015b jiang et al 2015 hou et al 2016 luo et al 2019 in this study according to the results of the previous study the use of homogeneous es models and the limitation due to the high computational load of the numerical simulation model the highest ratio of generating training samples used in previous studies 23n was utilized the number of 220 random samples were generated in the feasible space by space filling based optimal lhs sflhs samples were divided into training and test data sets including 75 and 25 of data respectively before using the samples to construct surrogate models the dataset was transformed by applying different fs methods because each surrogate model s performance is different when using various fs methods all six fs methods were used and evaluated for all ml models six ml algorithms ann krr knn svr gpr and dt were trained and hyper parameters were obtained using hyper parameter optimization methods including grid search random search and bayesian hyper parameter optimization method in other words to construct a surrogate model e g the ann model six fs methods and three hyper parameter optimization methods have been used which include 18 different cases root mean square error rmse r2 and maximum error me criteria were used to evaluate the performance of surrogate models in cross validation and parameter optimization according to these metrics the best primary surrogate models were chosen all surrogate models have been coded in python using the scikit learn module pedregosa et al 2012 which is a comprehensive ml module in python 3 3 ensemble of surrogate models homogeneous ensembles were first built using voting spa press wa rmse wa owa methods by combining six base surrogate models then they have been compared with the bagging and boosting ensemble models to construct homogeneous models except for the bagging and boosting ensemble models four random subsets of samples were selected each containing 60 of the initial dataset four instances of each base surrogate model were trained using random subsets and then using each of the ensemble methods an es model has been created by combining four base models bayesian hyper parameter optimization method was used to construct the best bagging and boosting ensemble models therefore the number of base models and the percentage of samples drawn from the training set to construct the bagging ensemble and the number of base models the learning rate and the appropriate error function to construct the boosting ensemble were obtained using bayesian hyper parameter optimization finally seven homogeneous ensemble models were created for each of the six base ml models and then the two best performing ensemble models of each base model were selected the selected models with the corresponding base models were used to build the final heterogeneous es models using 18 obtained models heterogeneous es models were created by applying voting spa press wa rmse wa owa and stacking methods the stacking method s performance was compared with other ensemble methods and then the most accurate model was replaced in the optimization model instead of the numerical simulation model 3 4 optimization model the cost of the remediation process was considered as the objective function while the desired level of contamination removal was set as the constraint the decision variables were defined as the remediation duration and injection rate of the wells the lower and upper bound of the decision variables and the desired removal rate were also the model constraints the most accurate es model has been replaced with the sear simulation model in the optimization model the mathematical form of the optimization problem can be expressed as follows 48a min q t f q t c 1 n m c 2 t 10 i 1 n q i in c 3 t j 1 n q j in subject to 48b t min t t max 48c q min in q i in q max in 48d i 1 n q i in j 1 m q j ex 48e r q t r 0 in eq 48a that denotes the objective function f q t is the total cost of the remediation process c 1 is the coefficient of the installation cost of injection and extraction wells in this study c 1 3000 n and m are the numbers of injection and extraction wells respectively in this study n 6 m 1 c 2 is the coefficient of injected chemicals cost in this study c 2 240 m 3 q i in is the injection rate of the ith injection well ft 3 day c 3 is the operating cost coefficient in this study c 3 0 25 m 3 t is the remediation duration and 10 is the water flush duration in eq 48b that denotes the remediation duration constraint t min and t max are the maximum and minimum allowable remediation duration in this study t min 10 t max 250 day respectively in eq 48c that denotes injection rates constraint q min in and q max in are minimum and maximum allowable injection rate of ith injection well in this study q min in 0 q max in 200 ft 3 day eq 48d denote extraction rate constraint so that the sum of the injection and extraction rates should be equal in eq 48e that denotes the remediation efficiency constraint r q t is the pce removal rate predicted by the es model and r 0 is the minimum allowable rate of pce removal in this study r 0 95 evaluations showed that increasing the duration of water flush by more than 10 days had no significant effect on pce removal water flush has been used to remove a fraction of contaminants that could be removed from the environment due to the establishment of a hydraulic gradient on the other hand the main cost of the sear process arises from the cost of expensive chemicals used and the installation cost of injection and extraction wells and their equipment therefore the duration of water flush has no significant effect on the cost of remediation as a result adding water flush duration to decision variables not only does not have a significant effect on the cost of the optimal remediation scenario but also increases the computational burden of training and finding optimal surrogate models as well as the optimization model 4 results 4 1 training and analyzing the performance of base ml models six ml models including ann krr gpr svr knn and dt were used as surrogate models the input data which included 220 random samples were scaled using six fs methods to train and test the models because ml models are often very sensitive to input scaling hyper parameters of ml models were tuned and optimized by three methods gr rs and bo the best models were selected based on cross validation rmse and r2 criteria finally 18 instances of each ml algorithm were generated using fs methods and hyper parameter optimization methods table 3 shows the cross validation rmse values of different ml models whose input data has been scaled by different fs methods the hyper parameters of ml models were tuned using the bayesian optimization method and 5 fold cross validation was used in each row the better rmse values the greener they are and the worse rmse values the redder they are the results showed that kernel based ml algorithms especially krr and gpr methods perform best using the minmax fs method and other ml algorithms are more accurate when the power transformer method is used dt models performance was not significantly different when using different fs methods because these algorithms rely on rules and fundamentally the performance of this method does not depend on the scaling of its input data proper tuning of hyper parameters is essential to improve the accuracy of ml models in this study the bo method was evaluated and has been compared with gs and rs methods fig 6 shows the cross validation rmse of the best ml models obtained by gs rs and bo methods as the results show tuning hyper parameters by the bo method has led to more accurate models bo has also reduced computational time because bo chooses the next iteration hyper parameters in an informed manner and uses prior information up to that stage it should be noted that the more warm up samples there are the better the bo method explores the search space the number of warm up runs in this method is equal to 5 of the total number of iterations in the gs method the rs method s total iterations are equal to 50 of the gs method overall bo and gs hyper parameters optimization methods had the lowest and highest execution times respectively the ann model had the best performance among the base ml models followed by the krr and gp models the dt model and then the knn had the worst performance among the base ml models table 4 presents the structural properties and some important hyper parameters values used to construct the best base model of any type in the ann model the network s structural characteristics and the activation function play the most important role in improving the performance in krr gp and svr kernel based models the kernel used has the most role in improving the final model s performance the chi square kernel was the best choice for constructing krr and svr models and although hybrid kernels were used in the gpr model kernels that included the chi square kernel performed the best in the knn model the weight function and the number of neighbors are the most important factors in improving the final model s performance the dt model did not have the appropriate accuracy in estimating the input output relationship of the simulation model 4 2 evaluation and comparison of homogeneous es models using various techniques homogeneous ensemble models were constructed and compared in addition to common methods bagging and boosting methods were also used to build homogeneous ensemble models fig 7 shows the test rmse and r2 of the best base surrogate model and their homogeneous ensemble models constructed by bagging boosting and owa methods among the common homogeneous ensemble methods owa had the best performance and was therefore used for comparison with the bagging and boosting methods boosting and bagging homogeneous ensemble methods performed better than other homogeneous ensemble methods especially in high bias models e g the test rmse of the best single dt model was 0 145 and the boosting ensemble method was able to reduce test rmse by 50 but the test rmse of the single ann model was only about 5 5 better than its boosting homogeneous ensemble model finally due to the boosting and bagging homogeneous ensemble models proper performance compared to other methods the models constructed by them along with the best single base models have been used to build the final heterogeneous ensemble models 4 3 evaluating and comparing the performance of the stacking ensemble model to construct the heterogeneous ensemble model by the stacking method 18 ml models including six single ml models and 12 boosting and bagging homogeneous ensemble models have been used as base models different ml models were examined to find the best meta model and the best structure and hyper parameters were obtained using different optimizing hyper parameters methods including gs rs and bo table 5 presents the rmse r2 and maximum error values for the various stacking models obtained by different hyper parameter optimization methods in the first row of table 5 the best base model s performance criteria are presented to compare the performance with stacking ensemble models the best stacking ensemble model is obtained using the bo hyper parameter optimization method the noteworthy point is that the ensemble model is large making the common hyper parameters optimization methods very time consuming in this situation bo method in this respect has a much better performance than common gs and rs methods because in each iteration by informative selection based on past evaluations unnecessary iterations are avoided and with much fewer iterations finds models with better accuracy so that the time to find the optimal hyper parameters using the bo method was less than half compared to other methods the stacking ensemble model obtained using the bo method improved the cross validation rmse of the best single model by 75 while the model obtained using the rs method improved the cross validation rmse by 45 the stacking ensemble method was compared with other heterogeneous ensemble methods fig 8 presents the values of test rmse r2 and the maximum error of ensemble models with different ensemble methods the best homogeneous ensemble model is presented along with heterogeneous ensemble models to compare their performance better the stacking method has the best results in terms of all three performance criteria compared to the other ensemble methods the voting method has the worst rmse and maximum error values although the owa method has a good rmse its maximum error is not suitable compared to other methods it is worth mentioning that rmse r2 and maximum error of cross validation are provided for stacking heterogeneous ensemble and boosting homogeneous ensemble because cross validation can only be used to evaluate these methods using cross validation is an effective method to prevent over fitting and one of the reasons for the simultaneous improvement of all performance criteria in the stacking method can be this feature eventually the ensemble model obtained using the stacking method was selected for use in the optimization model instead of the numerical simulation model because of the appropriate performance 4 4 optimal remediation scenario a differential evolution de optimization model was developed in the python environment using the scipy framework to find the optimal remediation scenario the mentioned model is a non linear constrained model whose objective function is to minimize the remediation cost by satisfying an essential constraint of the dnapl removal rate of more than 95 the injection rates of each well and the remediation duration are the decision variables of this model to estimate the dnapl removal rate in each iteration of the optimization the stacking ensemble model was used in lieu of the numerical simulation model table 6 presents the specifications of the optimal remediation scenario and the corresponding removal rate calculated by the computationally expensive sear simulation model the results showed that the total remediation duration in the optimal scenario is 18 7 days and the surfactant injection time is 8 7 days the constraint of the optimization is a removal rate of more than 95 which the sear simulation model also confirms the satisfaction of this constraint with the obtained scenario the maximum injection rate is for well no 6 and is injected at the maximum possible injection rate in that well well no 6 is located upstream of the contaminated area and has the most significant impact on contaminated area remediation each run of the utchem simulation model takes an average of 2 5 h significantly reduced using the surrogate model the execution time of the optimization model with replacing the ensemble surrogate model was about 120 min and more than 20 000 iterations were done to achieve the optimal scenario therefore with the same computational burden i e the same function evaluation the required time for optimization model using the numerical simulation model will be about 5 7 years when the surrogate model was used the main time cost is spent for the training and validation of the surrogate model and generating input samples 220 samples have been generated only once and as a result the numerical simulation model has been run only 220 times the same input samples were used to train all surrogate models training each surrogate model and finding their optimal hyper parameters depending on the complexity and process of model training took a maximum of few hours as a result the time required to train six different type surrogate models was about 12 h the time required to train es models except for the stacking model was about 12 h the most time of training and finding the optimal hyper parameters was related to the stacking model which took about 4 h therefore the total time required to generate 220 input samples 550 h training of surrogate models 12h and training of es models 16 h was 578 h this time is about 99 less than the time required to run a similar simulation optimization model using a numerical simulation model 5 discussion the present study presented significant results in three areas the effect of different feature scaling and hyper parameters optimization methods 2 evaluation of homogeneous ensemble models 3 presenting and evaluating the optimized stacking model 1 ml models are often sensitive to data scaling and fs methods greatly influence the accuracy of ml models the results showed that different fs methods must be tested when using different machine learning methods the kernel based ml models performed best when using the min max method and other ml models performed best when the power transformer method was used it should be noted that when using different kernels in kernel based methods the appropriate fs method may be changed so when using different kernels it is necessary to evaluate different fs methods for each kernel to train the most accurate surrogate model the optimal hyper parameters of the surrogate model must be found in this study the bo method was used and compared with conventional gs and rs methods the bo method in all cases led to training of a more accurate surrogate model the bo method consciously chooses the next candidate hyper parameters based on prior information thus bo also reduces the computational time 2 among the homogeneous es methods the boosting method had the best performance especially in high bias models the boosting method creates a base model in each iteration so that the new model corrects the weakness of the previous model unlike other homogeneous es methods such as weighted average methods that combine a few previously trained base models the boosting method constructs the required base models iteratively and eventually combines them this is the reason why boosting generated better results than other homogeneous es methods 3 the optimized stacking es method had a high ability to construct an accurate es model this model unlike other es methods utilized in other similar studies has the ability to combine base models in a non linear or black box manner the resulting stacking model had a large size so finding its optimal hyper parameters by conventional methods like gs and rs was computationally expensive and inefficient therefore the bo method was used to find its optimal hyperparameters the use of the bo method significantly reduced the computational load compared to conventional methods and led to the construction of a more accurate es model 6 conclusion in this study different surrogate models have been constructed and compared to select the best for emulating the numerical model in the problem of finding the optimal scenario of dnapl contaminated aquifer remediation using the sear method first six ml algorithms were applied and evaluated using the generated samples different fs methods were evaluated and compared to evaluate their impact on the performance of surrogate models also gs rs and bo methods were used and compared to tune and optimize ml models hyper parameters then using different homogeneous ensemble methods homogeneous ensemble models were constructed bagging and boosting homogeneous ensemble methods were used in this study and were compared with other methods subsequently the stacking ensemble model was constructed using the best single models and homogeneous ensemble models and the performance of the stacking method was compared with the common methods used in this field the stacking ensemble model outperformed others and was substituted as the best surrogate model in the optimization model instead of the numerical simulation model which had a very high computational load finally the optimal scenario of contaminated aquifer remediation was obtained the most important results can be summarized as below 1 the use of appropriate fs methods in the pre processing stage is very effective on ml models performance kernel based methods whose input data was scaled with the minmax method had the best performance and other models with the power transform scaler method had the best performance 2 bayesian optimization hyper parameters tuning method compared to other hyper parameter tuning methods has led to the construction of ml models with better performance 3 performance of homogeneous ensemble methods was compared and bagging and boosting methods performed better than other methods the efficiency of these methods is especially impressive when the base model has a high bias 4 the stacking ensemble method was compared with common ensemble methods results showed that the stacking performed better e g the stacking method reduced the test maximum error by 44 compared to the best heterogeneous es method owa the stacking method is more resistant to over fitting due to using cross validation and out of fold prediction 5 compared to gs and rs methods the bayesian optimization method led to finding better hyper parameters for the ensemble model that significantly increased the ensemble model s accuracy also the computational load and its execution time were much less because bayesian optimization chooses the next iteration hyper parameters in an informed manner funding this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors authorship statement all persons who meet authorship criteria are listed as authors and all authors certify that they have participated sufficiently in the work to take public responsibility for the content including participation in the concept design analysis writing or revision of the manuscript furthermore each author certifies that this material or similar material has not been and will not be submitted to or published in any other publication before its appearance in the journal of contaminant hydrology declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper the authors declare the following financial interests personal relationships which may be considered as potential competing interests appendix a supplementary data supplementary material image 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jconhyd 2021 103914 
1380,surfactant enhanced aquifer remediation sear is an appropriate method for dnapl contaminated aquifer remediation however due to the high cost of the sear method finding the optimal remediation scenario is usually essential embedding numerical simulation models of dnapl remediation within the optimization routines are computationally expensive and in this situation using surrogate models instead of numerical models is a proper alternative ensemble methods are also utilized to enhance the accuracy of surrogate models and in this study the stacking ensemble method was applied and compared with conventional methods first six machine learning methods were used as surrogate models and various feature scaling techniques were employed and their impact on the models performance was evaluated also bagging and boosting homogeneous ensemble methods were used to improve the base models accuracy a total of six stand alone surrogate models and 12 homogeneous ensemble models were used as the base input models of the stacking ensemble model due to the large size of the stacking model bayesian hyper parameter optimization method was used to find its optimal hyper parameters the results showed that the bayesian hyper parameter optimization method had better performance than common methods such as random search and grid search the artificial neural network model whose input data was scaled by the power transformer method had the best performance with a cross validation rmse of 0 065 the boosting method increased the base models accuracy more than other homogeneous methods and the best boosting model had a test rmse of 0 039 the stacking ensemble method significantly increased the base models accuracy and performed better than other ensemble methods the best ensemble surrogate model constructed with stacking had a cross validation rmse of 0 016 finally a differential evolution optimization model was used by substituting the stacking ensemble model with the numerical model and the optimal remediation strategy was obtained at a total cost of 72 706 keywords dnapl surfactant enhanced aquifer remediation ensemble surrogate model stacking bayesian hyper parameter optimization feature scaling 1 introduction the appearance of dnapls one of the most hazardous environmental pollutants in groundwater resources has become widespread due to abundant usage human errors improper disposal and accidents like spillage kueper and mcwhorter 1991 akyol and turkkan 2018 due to their accumulation in the subsurface low mobility low solubility and relatively high density and interfacial tension dnapls may remain in the aquifers for a long time causing a toxic contaminant qin et al 2007 akyol et al 2013 mohammed et al 2019 epa has classified some dnapls like tetrachloroethylene pce as likely to be a carcinogen to humans epa 2011 karaoglu et al 2019 by penetrating dnapls to the subsurface due to the ability of the porous medium to hold a portion of dnapls the amount of leaked liquids continuously decreases and dnapls stay as residual danpl however if the amount of dnapls exceeds the soil retention capacity it can continue to move to the saturation zone and bedrock or an impermeable layer the extra dnapls move downward and accumulate on the impermeable layer in the form of pools with higher saturation and act as a long term continuous source of toxic contamination qin et al 2007 suthersan et al 2016 akyol 2018 karaoglu et al 2019 traditional remediation techniques such as the pump and treat method cannot be efficiently used for dnapls remediation because of their low solubility and mobility high density and high interfacial tension of dnapl with water qin et al 2007 mohammed et al 2019 the surfactant enhanced aquifer remediation sear is widely used to remove dnapl pollutants from contaminated groundwater pennell et al 1993 shiau et al 2002 adding surfactant to the contaminated water increases the mobility and solubility of dnapls delshad et al 1996 thus the efficiency of dnapl remediation significantly increases than the conventional pump and treat method nevertheless the sear approach is costly due to the high costs of applied materials especially the surfactant and cosolvent therefore finding an optimal remediation strategy by minimizing the costs and maximizing the performance is of great importance luo et al 2013 jiang et al 2015 mathematical models and optimization tools can provide an optimal strategy in these situations guan and aral 1999 zhang et al 2005 schaerlaekens et al 2006 he et al 2009 however embedding the simulation models in the optimization routines may cause the approach to become computationally too expensive and even practically impossible luo et al 2013 chu and lu 2015a ouyang et al 2017a one alternative approach to solve this dilemma is replacing the sear simulation model with a fast to run machine learning ml model that emulates the original simulation model with acceptable accuracy this approach is also called surrogate modeling lu et al 2013 used the kriging technique instead of the real simulation model in the dnapl remediation optimization to decrease computational times luo et al 2013 applied radial basis function artificial neural network rbfann as a surrogate model in the same problem and reported that the surrogate model s errors were less than 5 relative to the real simulation model luo and lu 2014a compared three ml models performance including kriging rbfann and polynomial regression pr and showed two latter have better performance and kriging slightly outperforms the rbfann model in another work luo and lu 2014b used kriging and rbfann methods for sobol sensitivity analysis of the sear method they concluded that both models have enough accuracy to replace the sear simulation model based on the residual errors in terms of mean square error mse and the coefficient of determination r2 luo and lu 2014c used mixed integer non linear programming minlp to find the optimal dnapls remediation strategy an rbfann model was used instead of the sear numerical model and then minlp was run to find the optimal strategy the problem was also solved by a genetic algorithm ga hou et al 2015 applied three ml models of support vector regression svr kriging and rbfann to optimize dnapl contaminated aquifer remediation although the accuracy of a stand alone surrogate model might be generally satisfactory its performance in some parts of the feasible space of the input variables may not be stable and produces relatively large errors so this can direct the search algorithm towards the wrong values researches showed that combining the outputs of different surrogate models to construct an ensemble surrogate es model can provide higher accuracy and robustness zerpa et al 2005 goel et al 2007 acar and rais rohani 2009 viana et al 2009 acar 2010 zhou et al 2013 according to the cutting edge literature review most studies have used a weighted average to combine outputs of different surrogate models and create an es model jiang et al 2015 examined four surrogate models of rbfann svr kriging and kernel extreme learning machines kelm and then selected kelm and kriging as the best ml models to construct an es model they developed an optimization model to find the optimal weights of each ml model by minimizing the rmse of the es model results indicated that the es model outperforms the stand alone ml models chu and lu 2015b estimated the weight coefficients of an es model combining polynomial regression model rbfann and kriging according to the predicted residual sum of squares press and found that obtained es model improve the performance of surrogate modeling hou et al 2017 used set pair analysis spa to construct an es model the surrogate models used for this purpose were rbfann svr and kriging models in which the kriging model had the best performance two ensemble methods were developed in their work the first by combining rbfann svr and kriging models using the spa method and the second by combining several kriging models with different training sample sets the results showed that es models especially the one constructed with four kriging models had better estimation accuracy than the stand alone surrogate models ouyang et al 2017b applied multi gene genetic programming mggp kriging and svr as surrogate models they used the optimal weighted average surrogate method to construct an es model where the optimal weights were obtained by minimizing the mean square error mse of the ensemble model the results indicated that the es model based on kriging and mggp had the best performance among other es models and stand alone surrogate models ouyang et al 2017c developed a set of es models by a weighted combination of five surrogate models polynomial response surface rbfann kriging svr and gaussian process regression gpr similar to their previous work optimal weights were obtained by minimizing the mean square error and results indicated that the es model constructed by combining all five surrogate models had the best performance among the other models hou and lu 2018 used four kriging models as surrogate models trained with different training datasets also they used the spa method to construct an es model and embedded it as a constraint condition into the optimization model hou et al 2019 studied the application of an es model within minlp for optimization of the dnapl contaminated aquifer remediation process the kelm kriging and svr models were used as base learners to create four ensemble models they followed two procedures to build es models using all three surrogate models and using the best surrogate model by developing four base learners with four different training sample sets es models were constructed using the weighted average of different surrogate models outputs two methods also calculated the weights 1 based on the rmse ratio of the surrogate models and 2 solving an optimization problem to find optimal weights they concluded that es models performed better than stand alone surrogate models and reported that the most accurate es model had the r2 and maximum relative error equal to 0 9837 and 13 14 respectively many studies related to the application of es models in sear optimization have used only one es method various research studies have also been focused on weighted average to combine surrogate models however there are some new efficient ensemble methods that have not been investigated in the dnapls remediation context this study aims at comparing the performance of different es methods including previously used simple average voting spa weighted average based on press press wa weighted average based on rmse ratio rmse wa and optimized weighted average based on minimizing rmse of ensemble model owa and new approaches of bagging boosting and stacking in order to introduce the superior es model as far as the authors know to date these new ensemble methods have not been used as an es model in optimizing dnapl contaminated aquifer remediation the weighted average es methods such as voting spa press wa and rmse wa methods have the lowest computational burden however they are simple es methods and cannot combine base models nonlinearly or train them with the required characteristics the bagging method combines several base models and reduces the variance of the final es model by introducing randomness into its construction procedure géron 2019 boosting methods can construct a sequence of base models so that each model corrects the weaknesses of the former model hastie et al 2009 the stacking method can combine base models in a non linear or black box way using different ml models thus having the ability to construct more complex and accurate es models wolpert 1992 zhou 2012 géron 2019 besides the effects of various pre processing methods and hyper parameter tunning strategies on the performance of surrogate models are also investigated additionally a wide range of conventional surrogate models including artificial neural network ann kernel ridge regression krr k nearest neighborhood knn support vector regression svr gaussian process regression gpr and decision tree dt have also been examined in this research work it is noticeable that models such as krr knn and dt have not been used in similar studies some advantages and disadvantages of surrogate models used in this study are presented in table 1 in brief this paper as an extension of previous ones intends to perform a comprehensive study on the performance of various stand alone surrogate models and es methods for function approximation and applied in optimizing dnapl contaminated aquifer remediation the novel aspects of this paper include the following 1 three new ensemble methods including bagging boosting and stacking were used to improve the performance and accuracy of surrogate modeling which have not been applied in the sear method optimization studies yet 2 several scalers have been applied in the pre processing stage and their effect on the surrogate model performance has been investigated 3 the bayesian hyper parameter optimization method has been used to find optimal hyper parameters of the stand alone surrogate and ensemble models and its performance has been compared with the conventional methods including random search with grid search techniques 2 methodology 2 1 sear simulation model remediation of dnapls by the pump and treat method due to the characteristics of dnapls such as high interfacial tension of dnapl with water low solubility and mobility and high density is not efficient akyol and turkkan 2018 mohammed et al 2019 the surfactant enhanced aquifer remediation sear is a remediation technique which extensively utilized in the dnapl contaminated aquifer remediation pennell et al 1993 shiau et al 2002 akyol 2018 the pump and treat is not an efficient technique for remediation of dnapls due to dnapls characteristics such as low solubility and mobility and high interfacial tension of dnapl with water mohammed et al 2019 in the sear method by adding surfactants and cosolvents the interfacial tension between dnapl and water decreases and the solubility of dnapl increases thus the efficiency of dnapl remediation significantly improves than the conventional pump and treat method delshad et al 1996 akyol et al 2013 karaoglu et al 2019 mohammed et al 2019 delshad et al 1996 developed a 3d multi component multi phase compositional simulation model for the analysis of the sear method for napls contaminants in this simulation model each component s mass balance equation is solved by the finite difference method delshad et al 1996 for more details about the mathematical form of the equations and numerical schemes of solving the equations readers can refer to delshad et al 1996 qin et al 2007 lu et al 2013 ouyang et al 2017c the numerical simulation model developed by delshad et al 1996 was then expanded at the university of texas called the university of texas chemical compositional simulator utchem utchem is known as a general model of sear process simulation and has been used in various studies for the remediation of napls zhang et al 2003 schaerlaekens et al 2005 ouyang et al 2017c chemical physical and biological properties as well as porous media heterogeneity non equilibrium sorption decay geochemical reactions and aquifer remediation by the sear method are considered in utchem delshad et al 1996 qin et al 2007 in this study surfactant enhanced aquifer remediation was performed using utchem 2 2 surrogate models 2 2 1 ann rbf neural networks have been used in most previous studies related to aquifer remediation optimization using surrogate models luo and lu 2014a jiang et al 2015 hou et al 2017 luo et al 2018 and mlp neural network has been less examined in this study in addition to the rbf neural network mlp neural network was also employed multilayer perceptron mlp neural networks can learn a non linear function f r m r o by training on a sample set as a supervised learning algorithm where m and o are the numbers of input and output dimensions respectively schalkoff 1997 kriesel 2007 mlp neural networks may have more than one hidden layer and have different activation functions mlp uses the squared error loss function for training its weights and biases which is written as 1 loss y y w 1 2 y y 2 2 2 w 2 2 where w represents the weights w 2 2 represents ridge regularization term and is a non negative hyper parameter which controls the penalty s magnitude in order to find the best ann structure various factors including the number of hidden layers number of neurons in hidden layers pre processing methods the type of activation functions identity logistic sigmoid hyperbolic tangent and rectified linear unit generalization hyper parameter values learning rate values and solvers for finding optimal weights have been tested 2 2 2 gpr gpr applies the gaussian process gp for regression purposes the gp is a set of random variables any finite number of which have a joint gaussian distribution rasmussen 2004 the gp is defined by a mean function representing the mean at any point in the input space and a covariance function that sets the covariance between points 2 f x gp m x k x x gpr for defining the covariance of a prior distribution over the target functions uses the kernel also it uses the observed data training data to define a likelihood function posterior distribution over target functions is obtained using bayes theorem and posterior mean is used for prediction rasmussen 2004 pedregosa et al 2012 predictive equations for gpr is expressed as follows 3 f x y x n f cov f 4 f e f x y x k x x k x x σ n 2 i 1 y 5 cov f k x x k x x k x x σ n 2 i 1 k x x where x and y are respectively input and output of training dataset f is the distribution of the target values k is the covariance fuction which applies kernel functions and σ is called noise variance in order to include the inherent noise into the distribution to achieve the best gpr several factors such as different kernels σ term noise level and the number of optimizers restart for finding optimal kernel s hyper parameters were considered the kernels used in this study are linear polynomial radial basis function rbf rational quadratic exp sine squared matérn sigmoid laplacian and chi squared functions they can be expressed as follows rasmussen and williams 2005 zhang et al 2007 rupp 2015 6 linear k x y x y 7 polynomi a l k x y x y l c 0 d 8 rbf k x y exp 1 2 l 2 x y 2 9 rational quadratic k x y 1 x y 2 2 α l 2 α 10 exp sine squared k x y exp 2 l 2 sin 2 π x y p 11 mat é rn k x y 2 1 υ γ υ 2 υ 1 2 υ x y l υ k υ 2 υ x y l 12 sigmoid k x y tanh x y l c 0 13 laplacian k x y exp x y l 14 ch i 2 k x y exp 1 l i x i y i 2 x i y i where l 0 is the length scale parameter such a wide range of different kernels in aquifer remediation optimization studies have not been utilized to obtain the best gpr as a surrogate model 2 2 3 krr kernel ridge regression incorporates ridge regression and kernel trick to learn non linear functions ridge regression shrinks the regression coefficients by adding a regularization term to linear regression s cost function the objective function of ridge regression for training the model is as follows hastie et al 2009 15 min w xw y 2 λ w 2 here xw y 2 is the sum of squared error and λ w 2 is l2 regularization term with the complexity parameter λ 0 that controls the amount of coefficients shrinkage in order to give the capability of learning non linear functions to the model x is replaced by φ x and the coefficients are extracted by 16 w φ x φ x φ x λi 1 y after obtaining the weights for new test data x the target value y can be calculated by 17 y ϕ x φ x φ x φ x λi 1 y kernel k can be considered as k x 1 x 2 ϕ x 1 ϕ x 2 and φ x as a matrix where each row is ϕ x i by replacing φ x φ x by matrix k calculated as k ij k x i x j and ϕ x φ x by i 1 n ϕ x ϕ x i i 1 n k x x i all dot products express in terms of kernels given α k λi 1 y the target value of new test data can be obtained as 18 y i 1 n α i k x x i the best krr model is determined after checking different kernels complexity parameter values and kernel s hyper parameter values the kernels used for krr are the same as those of the gpr model in this study kernel s hyper parameters of the krr model were estimated by hyper parameters optimization methods while in gpr they were selected based on the gradient ascent on the marginal likelihood function 2 2 4 svr support vector regression svr is a supervised learning method and a variant of support vector machine svm which can be applied for classification and regression unlike the linear regression model whose training is based on minimizing the sum of squared errors svr minimizes simultaneously the regression model s weight values and sum of deviations from ε tube the error is also regulated by setting an absolute error less than or equal to maximum error margin ε in constraints with the x and y vectors as input and outputs of training samples an optimization problem called primal problem is solved by svr smola and schölkopf 2004 clarke et al 2005 19 min w b ζ ζ 1 2 w w c i 1 n ζ i ζ i 20 subject to y i w ϕ x i b ε ζ i w ϕ x i b y i ε ζ i ζ ζ 0 i 1 n by applying the lagrangian principle the dual form optimization problem is obtained as follows smola and schölkopf 2004 pan et al 2010 zhou et al 2013 jiang et al 2015 21 min α α 1 2 α α q α α ε e α α y α α 22 s u b j e c t t o e t α α 0 0 α i α i c i 1 n in which w is the weighting vector ζ and ζ are called slack variables to relieve hard constraint of the optimization problem c is a non negative parameter which indicates the trade off between the approximation and generalization of the trained model q ij k x i x j ϕ x i t ϕ x j is the kernel function and α and α are lagrange multipliers by solving the above problem by quadratic programming the regression function can be found as 23 i 1 n α i α i k x i x ρ in the context of dnapl contaminated aquifer remediation svr has been used as a surrogate model but only with one type of kernel this study investigated the performance of svr in terms of various types of kernels to achieve the best svr model nine different kernel types different regularization parameters different epsilon values and different kernel hyper parameter values have been tested the kernels used for svr are the same as those of the gpr and krr models 2 2 5 knn k nearest neighbors knn is a nonparametric method for unsupervised and supervised ml problems altman 1992 brownlee 2016 here supervised knn based learning is used for regression the estimation of the output variable in knn is done by a predetermined number k of its closest data in training samples in which every neighbor has a weight based on different weight functions various distance metrics can be used but minkowski distance that is a generalization of euclidean distance is more common to use kramer 2013 kuhn and johnson 2013 24 i 1 n x ai x bi 1 q where q 0 if q 1 minkowski distance is called manhattan distance and if q 2 it is equal to euclidean distance despite its simplicity and proper performance the knn model still has not been used as a surrogate model in similar studies for dnapl remediation in order to find the best knn model the different number of neighbors and associated radius different weight functions uniform inverse distance and gaussian kernel different algorithms for calculating the nearest neighbors ball tree kd tree and brute force and their parameters different distance metrics like manhattan and euclidean have been examined 2 2 6 dt decision tree dt learner is a group of ml algorithms used in statistical classification and regression decision trees belong to the group of supervised learning algorithms and most of them are based on a quantitative minimization called entropy dt is trained by several training samples to learn simple decision rules and split the training set in each node breiman et al 1984 hastie et al 2009 shalev shwartz and ben david 2014 considering the training samples of input vector x i r n i 1 l and output vector y r l space recursively is partitioned by a dt so that training samples with similar values are grouped together if s denotes samples in node m and each candidate split θ j t m defines by its feature j and threshold t m then the samples can be partitioned into s left and s right branches as below 25 s left θ x y x j t m s right θ s s left θ in order to find the best split in each node a split candidate that minimizes impurity must be selected hastie et al 2009 26 g s θ n left n m h s left θ n right n m h s right θ 27 θ argmin θ g s θ here the number of samples in node m is n m and h is an impurity function in node m for regression problems that the output variable is continuous at node m with s observations mean squared error is minimized as a residual metric to find an appropriate split point hastie et al 2009 28 h x m 1 n m i n m y i y m 2 y m 1 n m i n m y i other measures like mean absolute error and friedman mse may also be used in this study the best possible dt was obtained by evaluating different criteria including different maximum tree depth different number of samples required to split the criterion of split quality different number of samples required to be at a leaf node and different maximum leaf nodes 2 3 ensemble methods ensemble methods train several stand alone surrogate models called base learners and combine their outputs for solving the same problem ensemble methods that use one type of base learning algorithm are called homogeneous ensembles whereas those that employ multiple learning algorithms are called heterogeneous ensembles zhou 2012 the ensemble methods for combining surrogate models are diverse in this study several methods for creating es models used separately in previous studies were investigated together these are voting spa hou et al 2017 hou and lu 2018 press wa chu and lu 2015b rmse wa hou et al 2019 owa jiang et al 2015 ouyang et al 2017c hou et al 2019 were investigated in addition to the mentioned ensemble models bagging boosting and stacking es methods have also been examined for the first time for the dnapl contaminated aquifer remediation problem 2 3 1 set pair analysis spa given o x 1 x 2 x n as the set of contaminant removal rates obtained from the simulator model for the test samples and p k x 1 k x 2 k x n k as the set of contaminant removal rates obtained from the kth surrogate model for the test samples a set pair s o p k and its connection degree is obtained then the set pair based on the absolute error value between surrogate model and simulation model outputs are divided into four categories identity has an error of less than 0 3 mild discrepancy has an error of between 0 3 and 0 6 severe discrepancy has an error of between 0 6 and 1 and contradictory has an error of more than 1 using these categories the connection degree between the sear simulation model and kth surrogate model is calculated by wang et al 2009 hou and lu 2018 29 μ k i k n d 1 k n i 1 d 2 k n i 2 c n j here i k is the number of identity terms in set pair c is the number of contradistinction terms d 1 k is mild discrepancy terms and d 2 k is severe discrepancy terms the uncertainty component coefficients of discrepancy degrees i 1 and i 2 and the uncertainty component coefficients of contradictory j are 0 5 0 5 and 1 respectively given that the range of μ is 1 1 it must first be scaled to 0 1 range 30 u k μ k 0 5 0 5 eventually the set pair weight of the kth surrogate model w k is calculated as follows 31 w k u k k 1 4 u k 2 3 2 weighted average based on press press wa in this ensemble method one weight is chosen for each surrogate model calculated according to the press predicted residual sum of squares weighted average surrogate then the ensemble model is constructed by the weighted average of the surrogate models w i the weight coefficient of the ith surrogate model is calculated by 32 w i e i 0 05 e 1 i 1 n e i 0 05 e 1 33 e 1 n i 1 n e i where e i is the press error of the ith surrogate model and n is the number of surrogate models goel et al 2007 chu and lu 2015b 2 3 3 weighted average based on rmse ratio in this method weights are assigned to the surrogate models according to the inverse of the root mean square error ratio rmse 2 3 4 weighted average based on minimizing rmse in order to find the optimal weights an optimization problem is solved by minimizing the rmse of the es model as follows jiang et al 2015 hou et al 2019 34 min w rmse y es 1 m k 1 m i 1 n w i y i y actual 2 35 subject to i 1 n w i 1 where y i the output predicted by the ith surrogate model y actual the output of the simulation model and m is the number of test samples 2 3 5 bagging bagging bootstrap aggregation is an ensemble method that constructs several instances of an ml algorithm on random subsets of the initial set of training samples and then combines their predictions to produce a final prediction the final prediction is made by averaging over the individual surrogate models breiman 1996 géron 2019 generally bagging outperforms any of the base models breiman 1996 the bagging es model has less chance of overfitting and has a lower variance than individual predictors by introducing randomness into its construction procedure géron 2019 when random subsets are drawn without replacement it is called pasting breiman 1999 but if they are drawing with replacement then the algorithm is known as bagging breiman 1996 also if base surrogate models are created on subsets of both samples and features it is called random patches louppe and geurts 2012 the structure of the bagging method has been shown in fig 1 in order to find the best bagging es different number of base learners and different subsets of training samples were investigated 2 3 6 boosting boosting methods construct a sequence of surrogate models in a way that each model corrects the defects of the previous model among the boosting methods the adaboost method is the most popular freund and schapire 1997 hastie et al 2009 géron 2019 an adaboost estimator starts by training a regressor on the initial training dataset and for every consecutive iteration trains new regressor models on the training dataset but the weights of samples are modified based on the error of the current regressor prediction all samples are initially given a weight which is the same at the beginning and is equal to w i 1 n in the later iterations each sample s weight is modified based on the error value and weights of training samples that the predecessor estimator improperly predicted are boosted the structure of the boosting method has been shown in fig 2 in order to find the best boosting es model different numbers of base learners different learning rates and different error functions have been investigated it is noticeable that the loss function used for updating the weights after each boosting iteration and the learning rate determine each regressor s contribution 2 3 7 stacking stacking also called stacked generalization wolpert 1992 is an ensemble method in which a meta learner is trained to combine several base surrogate models the base surrogate models are called the first level model or base model and the learning model that combines them is called the second level model or meta model the core principle of stacking is to train base models using the training set and then the meta model is trained on the outputs of base models as input features the base models are often created using different learning algorithms and therefore stacking es models are often heterogeneous however it is also possible to create a homogeneous stacking es zhou 2012 when the same data used for training the base models is also used to create the new data set to train the meta level model overfitting will likely occur therefore it is recommended to use k fold cross validation to prevent overfitting and in each fold the out of fold oof part of the train set is predicted when using k fold cross validation the training set d is split into k part d 1 d k given t learning methods and by defining d j and d j d d j as training and test sets for the jth fold and h t j as a base learner that obtained by training the tth learning method on d j for each x i in d j the output of the learner h t j on x i can be denoted by z it finally after the cross validation process is completed the new dataset from the t individual base learners can be obtained as 36 d z i 1 z i 2 z it y i i 1 m then the meta level learner h will be trained on d generally after constructing a meta model h the base learners are reconstructed by training on the entire training set wolpert 1992 zhou 2012 géron 2019 in order to determine a relevant stacking es model different learning algorithms have been used as a meta model and its optimal hyper parameters have been determined by bo 2 4 feature scaling feature scaling fs is the most critical transformation that needs to apply to data almost all ml algorithms are sensitive to the scaling of the data for instance several components in the objective function of learning algorithms such as the ridge and lasso regularizers or the rbf kernel in svm expect input values like a standard normal distribution and neural networks often expect input values in the range 0 1 müller and guido 2016 géron 2019 there are several scaling techniques each of which can significantly influence the performance of an ml model 1 standard scaler this scaler standardizes each feature by subtracting the mean and dividing by variance thus the resulting feature has zero mean and unit variance and bringing all features to the same magnitude han et al 2011 albon 2018 standard scaler maps x i of feature a to x i by computing 37 x i x i a σ a here x i is a standardized form of x i a is the mean and σ a is the variance of the feature 2 robust scaler this scaler is similar to the standard scaler but uses statistics that are robust to outliers so instead of mean and variance median and interquartile range iqr have been used the iqr is the range between the 1st quartile and the 3rd quartile the difference between 75th and 25th percentiles müller and guido 2016 3 minmax scaler minmax scaler performs a linear transformation on each feature and shifts the dataset to a given range supposing that min a and max a are minimum and maximum values of feature a and new max a and new min a are minimum and maximum values of the desired range the transformation is obtained by han et al 2011 géron 2019 38 x i x i min a max a min a new max a new min a new min a 4 maxabs scaler maxabs scaler is similar to the minmax scaler but scales a feature in a way that the data lies within the range 1 to 1 for this purpose the dataset is divided into the maximum absolute value of the dataset 5 quantile transformer quantile transformer is a non linear monotonic transformer that put the features into the desired distribution based on g 1 f x formula where f is the cumulative distribution function of the feature and g 1 is the quantile function of the desired output distribution g gilchrist 2000 the output distribution is usually the normal or uniform distribution 6 power transformer power transforms are non linear monotonic parametric transformations that are applied to map data from existing distribution to as close to a gaussian like distribution as possible box cox and yeo johnson power transforms were used in this study the yeo johnson transform is given by yeo and johnson 2000 weisberg 2001 39 x i λ x i 1 λ 1 λ if λ 0 x i 0 ln x i 1 if λ 0 x i 0 x i 1 2 λ 1 2 λ if λ 2 x i 0 ln x i 1 if λ 2 x i 0 and the box cox transform is given by box and cox 1964 osborne 2010 40 x i λ x i λ 1 λ i f λ 0 l n x i i f λ 0 in both methods lambda is the transformation parameter and is estimated through maximum likelihood in this study all the fs methods mentioned are used to obtain the best possible surrogate models and their influence on the performance of surrogate models has been examined 2 5 bayesian hyper parameter optimization ml models used as surrogate models have many hyper parameters that control different aspects of their behavior and performance therefore finding optimal hyper parameters for surrogate models is essential goodfellow et al 2016 grid search and random search are two commonly accepted techniques for tuning hyper parameters bergstra and bengio 2012 hutter et al 2019 grid search the most used technique for hyper parameter optimization is a brute force technique for tuning hyper parameters using cross validation in grid search a set of multiple hyper parameters possible values for a surrogate model is specified then the surrogate model for each combination of hyper parameter values in the possible set is trained and each model is evaluated using cross validation to select the best model albon 2018 grid search is an exhaustive search and its weakness is the high computational cost which increases exponentially with the increasing number of hyper parameters goodfellow et al 2016 the grid search technique is suitable for situations where the number of the combination is not relatively high but if the hyper parameter search domain is too large the random search technique is often preferred grid search evaluates all possible combinations of hyper parameters but in contrast random search evaluates a given number of random combinations a random combination of hyper parameters is obtained in each iteration by choosing a random value for each hyper parameter from the distribution specified for it bergstra and bengio 2012 albon 2018 géron 2019 random search performance is better than grid search when some hyper parameters do not significantly affect the performance measure bergstra and bengio 2012 goodfellow et al 2016 hutter et al 2019 however when the surrogate model has many hyper parameters the number of models that need to be trained and evaluated will be huge and the process of finding an optimal hyper parameter can be computationally expensive wu et al 2019 grid search and random search are comparatively inefficient because they are entirely uninformed in each iteration from the past evaluations bayesian hyper parameter optimization bo is a derivative free global optimization method based on bayes theorem that considers past evaluations results and consciously selects a set of hyper parameters for evaluation at the next stage brochu et al 2010 wu et al 2019 to obtain the optimal hyper parameter mathematically the following optimization problem should be solved brochu et al 2010 shahriari et al 2016 41 x arg min x x f x here f x is the objective function x is the optimal set of hyper parameters and x can be any value in the x domain in bo the objective function takes in hyper parameters and outputs a score such as the rmse of cross validation that should be minimized usually the objective function is a black box function non convex noisy high dimensional and computationally expensive to evaluate brochu et al 2010 shahriari et al 2016 wu et al 2019 bo is an iterative method and operates by developing a probabilistic surrogate model of the objective function and applying the acquisition function to select the most promising hyper parameters for the next iteration in each iteration the probabilistic surrogate model is re trained by all objective function observations generated so far then by maximizing the acquisition function the efficiency of different candidate hyper parameters set is determined hutter et al 2019 the acquisition function is defined in a way that can automatically trade off exploration and exploitation brochu et al 2010 hutter et al 2019 evaluation of the black box objective function than the acquisition function which is cheap to compute and thus can be completely optimized is computationally more expensive shahriari et al 2016 there are several probabilistic models and acquisition functions for use in bo in this study the gaussian process model and the expected improvement have been used as the probabilistic model and the acquisition function respectively mockus et al 1978 defined the improvement function as 42 i x max 0 f x f x here f x is the probabilistic surrogate model gaussian process f x is the value of the best point observed so far and x is its location i x is non negative and becomes positive when the prediction in x is higher than the best value observed so far otherwise it becomes zero selected hyper parameters for the next iteration is obtained by maximizing the expected improvement brochu et al 2010 wu et al 2019 43 x arg max x e max 0 f t 1 x f x f x gp posterior predictive has a normal distribution with the mean μ x and the variance σ x at x as a result distribution of improvement function i x is normal with the mean μ x f x and a variance σ x the probability density function of i is computed as wu et al 2019 44 f i 1 2 π σ x exp μ x f x i 2 2 σ 2 x i 0 thereupon the expected improvement can be calculated as brochu et al 2010 45 e i i 0 i i 1 2 π σ x exp μ x f x i 2 2 σ 2 x d i σ x μ x f x σ x φ μ x f x σ x ϕ μ x f x σ x here φ and ϕ are cdf and pdf of the standard normal distribution respectively eq 45 can be calculated analytically mockus et al 1978 jones et al 1998 brochu et al 2010 as follows 46 e i μ x f x φ z σ x ϕ z i f σ x 0 0 i f σ x 0 z μ x f x σ x the bo procedure is shown in the following algorithm shahriari et al 2016 wu et al 2019 unlabelled table bayesian hyper parameter optimization algorithm for t 1 t 1 construct a gaussian model for the objective f given observations xi yi f xi for i 1 t 2 optimize acquisition function u based on the posterior distribution for selecting the next hyper parameter set xt 1 argmax u x 3 apply selected hyper parameter set to the objective function and then update the gaussian process model incorporating the new result repeat loop until max iteration or time is reached the expected improvement can automatically balance the trade off between exploiting and exploring eq 46 has two terms the first term is exploitation and the second one is exploration term lizotte 2008 suggests an ξ 0 parameter to express expected improvement in a generalized form that controls the trade off between exploiting and exploring 47 e i μ x f x ξ φ z σ x ϕ z if σ x 0 0 if σ x 0 z μ x f x ξ σ x parameter ξ determines the importance of exploration and exploitation so that higher ξ leads to more exploration and the importance of points by high μ x decreases relative to points in regions by high prediction uncertainty which has large σ x lizotte 2008 suggested ξ 0 01 and conclude that it works well in most cases 3 case study 3 1 site overview the shallow surficial aquifer in site 88 the location of the dry cleaning building at the marine corps base mcb camp lejeune jacksonville north carolina was selected as the case study aquifer the location of the aquifer and site 88 is shown in fig 3 there are no surface water bodies near the site and the nearest surface water is the new river located about 3000 ft 920 m west of the site duke engineering and services 1999 two aquifer systems exist in site 88 a low permeability clay aquitard separates the shallow surficial aquifer and the castle hayne aquifer the shallow surficial aquifer has a depth of approximately 20 ft 6 1 m below the ground surface bgs the castle hayne aquifer is further divided into the upper castle hayne 7 5 23 m the middle castle hayne 23 38 m and the lower castle hayne 38 m duke engineering and services 1999 ch2 hill 2010 duke engineering services co has investigated the contaminated zone at site 88 in cooperation with baker environmental during 1997 and the first half of 1998 duke engineering and services 1999 cone penetration tests and soil borings have been used to collect detailed lithologic data and soil samples a relatively uniform depositional sequence of sediments has been recognized in borings across the site the shallow surficial aquifer consists of fine to very fine grained sands and silt and this combination is the predominant type of sediments up to approximately 18 ft 5 5 m bgs the shallow aquifer is restricted beneath by a clay aquitard at a depth of approximately 20 ft 6 1 m bgs core samples have indicated that sediments at depths of 18 to 20 ft 5 5 to 6 1 m bgs become significantly finer e g clayey silt sediment becomes finer with increasing depth from sandy silt to clayey silt the location of a geological cross section and the geological characteristics of that cross section are also presented in fig 3 two samples have been collected at depths of 17 2 and 19 1 ft 5 2 and 5 8 m which had very similar mineralogy both had greater than 80 quartz with some feldspar and pyrite clay minerals had comprised 7 and 9 of the samples respectively with kaolinite illite chlorite and smectite duke engineering and services 1999 investigations have reported that the clay layer is approximately 14 16 ft 4 3 4 9 m thick beneath building 25 baker environmental 1998 duke engineering and services 1999 observation wells had also been installed to conduct pumping test the average values of 1 4 ft day 5 10 4 cm s for the hydraulic conductivity and 0 01 for the specific yield have been obtained by conducting the pumping test fig 3 also illustrates the potentiometric surface of the surficial aquifer obtained by the shallow monitoring wells 25 ft 7 6 m the potentiometric surface map shows a highly variable horizontal hydraulic gradient varying from 0 004 to 0 03 m m ch2 hill 2010 however as seen in fig 3 the hydraulic gradient is relatively low about 0 015 m m in the immediate of the contaminated area the water table fluctuates annually from about 7 9 ft 2 1 2 7 m bgs or about 16 18 ft 4 9 5 5 m above mean sea level amsl duke engineering and services 1999 the saturated zone of the shallow aquifer can be roughly divided into three permeability zones the upper zone 18 ft bgs 5 5 m bgs the middle zone 18 19 ft bgs 5 5 5 8 m bgs and the lower zone 19 20 ft bgs 5 8 6 1 m bgs the upper zone is generally composed of fine to very fine sand and silty sand therefore the upper zone is the most permeable zone of the shallow aquifer and its hydraulic conductivity is estimated to be about 1 4 ft day 5 10 4 cm s results from multilevel sampler mls samples confirm that the hydraulic conductivity of the basal clayey sandy silt zone is lower than the upper zone and is estimated to be approximately 0 28 ft day 1 10 4 cm s the lower zone has been composed predominantly of clayey silt with a hydraulic conductivity of approximately 5 10 5 cm s 0 14 ft day hydraulic conductivity of the upper and middle zone has been estimated based on the analysis of tracer test data from mlss and for bottom zone has been estimated based on grain size analyses on 72 soil samples from the bottom 3 ft of the aquifer battelle de s 2001 using site data gained from field investigations as input parameters a geosystem model of the site has been constructed by the university of texas austin applying utchem these input parameters have been updated and calibrated against the field data obtained from the results of the conservative interwell tracer test citt and partitioning interwell tracer test pitt duke engineering and services 1998a duke engineering and services 1998b 1999 the citt was performed using one non partitioning tracer bromide ion br the tracer curves were analyzed using the temporal moments method based upon the results from the citt the geosystem model has been then calibrated to reflect more closely the actual test domain duke engineering and services 1999 delshad et al 2000 battelle de s 2002 the effective permeability contrast at the different mls depths has been represented by the ratio of the first moments for the non partitioning tracer response curves the results indicated that the effective permeability of the basal silt layer is about four times lower than that of the overlying fine sands and permeability may be even lower near the basal contact of the shallow aquifer at the aquitard the simulations with the calibrated numerical model indicated a satisfactory match to the citt and pitt field results after completing the sear process the numerical model was further examined and calibrated to match field history delshad et al 2000 the simulated dnapl ipa and surfactant concentrations were adequately matched by field history in extraction wells delshad et al 2000 fig 4 shows the comparison of measured and history match of surfactant and ipa concentrations in an extraction well the study aquifer has been simulated using a three dimensional 25 25 16 mesh consisting of 10 000 grid blocks the horizontal extent of the model is 141 ft 23 m long by 80 ft 24 4 m wide with a 13 ft 4 m saturated thickness the dimensions of cells vary from 3 2 0 5 ft3 0 91 0 61 0 15 m3 in the center of the lowest layer to 24 12 2 ft3 7 31 3 66 0 61 m3 in the margin of the surface layer the pressures at two west east boundaries have been kept constant to establish a local hydraulic gradient of 0 015 representing observed field static water level conditions in the simulated region the other boundaries have been assumed as the no flux boundary the aquifer porosity in the model has a mean of 0 28 and a standard deviation of 0 05 the permeability in vertical is 10 of the permeability in horizontal duke engineering and services 1999 battelle de s 2002 regarding the thickness of the clay aquitard at the base of the shallow aquifer the aquitard provides adequate protection against further downward migration of dnapl contamination to the underlying castle hayne aquifer duke engineering and services 1999 the aquitard grid blocks have been treated as a no flux boundary and were assigned a low porosity of 0 01 and a low permeability of 5 6 10 4 ft day 2 10 7 cm s the distribution of the initial concentration volume fraction of the pollutant is depicted in fig 5 this contamination has been caused by a spill of pce and its penetration into the ground for 300 day also the permeability values of different layers are shown in fig 5b the university of texas austin performed the surfactant selection experiments and the surfactant formulation was designed specifically for site 88 battelle de s 2002 solubility data for site 88 dnapl obtained from laboratory experiments have been used to calibrate the phase behavior model parameters ooi 1998 delshad et al 2000 in addition to modeling surfactant phase behavior the physical parameters including microemulsion viscosity and microemulsion dnapl interfacial tension were also calibrated against experimental measurements delshad et al 2000 a total of 155 surfactant formulations have been screened by examining the phase behavior and measuring phase properties such as microemulsion viscosity weerasooriya et al 1999 delshad et al 2000 battelle de s 2002 also soil column experiments to ensure surfactant compatibility with the aquifer soils have been conducted moreover the required concentration of calcium to mitigate ion exchange has been calibrated by these experiments the values of the other physical and chemical parameters used in the sear simulation model are presented in table 2 due to the characteristics of the pce contaminated area a regular 7 spot for wells pattern was used regular 7 spot well pattern includes an extraction well in the center of the contaminated area and six injection wells around it this pattern has been obtained from evaluating various line drive and k spot patterns in previous studies on this site shams et al 2021 simulation indicated that the highest dnapl saturations are located within the low permeability sediment layer i e clayey silt just above a clay aquitard at a depth of approximately 18 20 ft 5 5 6 1 m bgs first water flush was applied for ten days and then 4 alfoterra 145 4po sulfate as a surfactant plus 16 ipa isopropyl alcohol as a cosolvent was injected hydraulic equilibrium was also achieved by setting the pumping and injection rate equal 3 2 surrogate models of the numerical simulation model in the present research work duration time for the remediation and the rate of injection wells were considered as input variables of the surrogate models the injection rate of wells was in the range of 0 200 ft3 day 0 5 66 m3 day and the remediation duration was in the range of 10 250 days for each set of randomly generated input variables the sear simulation model was run and the obtained pce removal rate was considered as the output variable therefore for surrogate model construction there were seven input variables and one output variable in general the more training samples the more accurate the surrogate model will be on the other hand the more training samples there are the greater the computational burden of generating training samples training surrogate models and predicting some surrogate models luo et al 2019 therefore selecting the appropriate number of training samples is a dilemma and a trade off must be made between generating more training samples and the resulting computational load some studies have considered the minimum number of training samples to be ten times the number of input variables 10n jin et al 2001 loeppky et al 2009 østergård et al 2018 however in other similar studies different ratios from 8n to 23n have been used luo and lu 2014a 2014b chu and lu 2015b jiang et al 2015 hou et al 2016 luo et al 2019 in this study according to the results of the previous study the use of homogeneous es models and the limitation due to the high computational load of the numerical simulation model the highest ratio of generating training samples used in previous studies 23n was utilized the number of 220 random samples were generated in the feasible space by space filling based optimal lhs sflhs samples were divided into training and test data sets including 75 and 25 of data respectively before using the samples to construct surrogate models the dataset was transformed by applying different fs methods because each surrogate model s performance is different when using various fs methods all six fs methods were used and evaluated for all ml models six ml algorithms ann krr knn svr gpr and dt were trained and hyper parameters were obtained using hyper parameter optimization methods including grid search random search and bayesian hyper parameter optimization method in other words to construct a surrogate model e g the ann model six fs methods and three hyper parameter optimization methods have been used which include 18 different cases root mean square error rmse r2 and maximum error me criteria were used to evaluate the performance of surrogate models in cross validation and parameter optimization according to these metrics the best primary surrogate models were chosen all surrogate models have been coded in python using the scikit learn module pedregosa et al 2012 which is a comprehensive ml module in python 3 3 ensemble of surrogate models homogeneous ensembles were first built using voting spa press wa rmse wa owa methods by combining six base surrogate models then they have been compared with the bagging and boosting ensemble models to construct homogeneous models except for the bagging and boosting ensemble models four random subsets of samples were selected each containing 60 of the initial dataset four instances of each base surrogate model were trained using random subsets and then using each of the ensemble methods an es model has been created by combining four base models bayesian hyper parameter optimization method was used to construct the best bagging and boosting ensemble models therefore the number of base models and the percentage of samples drawn from the training set to construct the bagging ensemble and the number of base models the learning rate and the appropriate error function to construct the boosting ensemble were obtained using bayesian hyper parameter optimization finally seven homogeneous ensemble models were created for each of the six base ml models and then the two best performing ensemble models of each base model were selected the selected models with the corresponding base models were used to build the final heterogeneous es models using 18 obtained models heterogeneous es models were created by applying voting spa press wa rmse wa owa and stacking methods the stacking method s performance was compared with other ensemble methods and then the most accurate model was replaced in the optimization model instead of the numerical simulation model 3 4 optimization model the cost of the remediation process was considered as the objective function while the desired level of contamination removal was set as the constraint the decision variables were defined as the remediation duration and injection rate of the wells the lower and upper bound of the decision variables and the desired removal rate were also the model constraints the most accurate es model has been replaced with the sear simulation model in the optimization model the mathematical form of the optimization problem can be expressed as follows 48a min q t f q t c 1 n m c 2 t 10 i 1 n q i in c 3 t j 1 n q j in subject to 48b t min t t max 48c q min in q i in q max in 48d i 1 n q i in j 1 m q j ex 48e r q t r 0 in eq 48a that denotes the objective function f q t is the total cost of the remediation process c 1 is the coefficient of the installation cost of injection and extraction wells in this study c 1 3000 n and m are the numbers of injection and extraction wells respectively in this study n 6 m 1 c 2 is the coefficient of injected chemicals cost in this study c 2 240 m 3 q i in is the injection rate of the ith injection well ft 3 day c 3 is the operating cost coefficient in this study c 3 0 25 m 3 t is the remediation duration and 10 is the water flush duration in eq 48b that denotes the remediation duration constraint t min and t max are the maximum and minimum allowable remediation duration in this study t min 10 t max 250 day respectively in eq 48c that denotes injection rates constraint q min in and q max in are minimum and maximum allowable injection rate of ith injection well in this study q min in 0 q max in 200 ft 3 day eq 48d denote extraction rate constraint so that the sum of the injection and extraction rates should be equal in eq 48e that denotes the remediation efficiency constraint r q t is the pce removal rate predicted by the es model and r 0 is the minimum allowable rate of pce removal in this study r 0 95 evaluations showed that increasing the duration of water flush by more than 10 days had no significant effect on pce removal water flush has been used to remove a fraction of contaminants that could be removed from the environment due to the establishment of a hydraulic gradient on the other hand the main cost of the sear process arises from the cost of expensive chemicals used and the installation cost of injection and extraction wells and their equipment therefore the duration of water flush has no significant effect on the cost of remediation as a result adding water flush duration to decision variables not only does not have a significant effect on the cost of the optimal remediation scenario but also increases the computational burden of training and finding optimal surrogate models as well as the optimization model 4 results 4 1 training and analyzing the performance of base ml models six ml models including ann krr gpr svr knn and dt were used as surrogate models the input data which included 220 random samples were scaled using six fs methods to train and test the models because ml models are often very sensitive to input scaling hyper parameters of ml models were tuned and optimized by three methods gr rs and bo the best models were selected based on cross validation rmse and r2 criteria finally 18 instances of each ml algorithm were generated using fs methods and hyper parameter optimization methods table 3 shows the cross validation rmse values of different ml models whose input data has been scaled by different fs methods the hyper parameters of ml models were tuned using the bayesian optimization method and 5 fold cross validation was used in each row the better rmse values the greener they are and the worse rmse values the redder they are the results showed that kernel based ml algorithms especially krr and gpr methods perform best using the minmax fs method and other ml algorithms are more accurate when the power transformer method is used dt models performance was not significantly different when using different fs methods because these algorithms rely on rules and fundamentally the performance of this method does not depend on the scaling of its input data proper tuning of hyper parameters is essential to improve the accuracy of ml models in this study the bo method was evaluated and has been compared with gs and rs methods fig 6 shows the cross validation rmse of the best ml models obtained by gs rs and bo methods as the results show tuning hyper parameters by the bo method has led to more accurate models bo has also reduced computational time because bo chooses the next iteration hyper parameters in an informed manner and uses prior information up to that stage it should be noted that the more warm up samples there are the better the bo method explores the search space the number of warm up runs in this method is equal to 5 of the total number of iterations in the gs method the rs method s total iterations are equal to 50 of the gs method overall bo and gs hyper parameters optimization methods had the lowest and highest execution times respectively the ann model had the best performance among the base ml models followed by the krr and gp models the dt model and then the knn had the worst performance among the base ml models table 4 presents the structural properties and some important hyper parameters values used to construct the best base model of any type in the ann model the network s structural characteristics and the activation function play the most important role in improving the performance in krr gp and svr kernel based models the kernel used has the most role in improving the final model s performance the chi square kernel was the best choice for constructing krr and svr models and although hybrid kernels were used in the gpr model kernels that included the chi square kernel performed the best in the knn model the weight function and the number of neighbors are the most important factors in improving the final model s performance the dt model did not have the appropriate accuracy in estimating the input output relationship of the simulation model 4 2 evaluation and comparison of homogeneous es models using various techniques homogeneous ensemble models were constructed and compared in addition to common methods bagging and boosting methods were also used to build homogeneous ensemble models fig 7 shows the test rmse and r2 of the best base surrogate model and their homogeneous ensemble models constructed by bagging boosting and owa methods among the common homogeneous ensemble methods owa had the best performance and was therefore used for comparison with the bagging and boosting methods boosting and bagging homogeneous ensemble methods performed better than other homogeneous ensemble methods especially in high bias models e g the test rmse of the best single dt model was 0 145 and the boosting ensemble method was able to reduce test rmse by 50 but the test rmse of the single ann model was only about 5 5 better than its boosting homogeneous ensemble model finally due to the boosting and bagging homogeneous ensemble models proper performance compared to other methods the models constructed by them along with the best single base models have been used to build the final heterogeneous ensemble models 4 3 evaluating and comparing the performance of the stacking ensemble model to construct the heterogeneous ensemble model by the stacking method 18 ml models including six single ml models and 12 boosting and bagging homogeneous ensemble models have been used as base models different ml models were examined to find the best meta model and the best structure and hyper parameters were obtained using different optimizing hyper parameters methods including gs rs and bo table 5 presents the rmse r2 and maximum error values for the various stacking models obtained by different hyper parameter optimization methods in the first row of table 5 the best base model s performance criteria are presented to compare the performance with stacking ensemble models the best stacking ensemble model is obtained using the bo hyper parameter optimization method the noteworthy point is that the ensemble model is large making the common hyper parameters optimization methods very time consuming in this situation bo method in this respect has a much better performance than common gs and rs methods because in each iteration by informative selection based on past evaluations unnecessary iterations are avoided and with much fewer iterations finds models with better accuracy so that the time to find the optimal hyper parameters using the bo method was less than half compared to other methods the stacking ensemble model obtained using the bo method improved the cross validation rmse of the best single model by 75 while the model obtained using the rs method improved the cross validation rmse by 45 the stacking ensemble method was compared with other heterogeneous ensemble methods fig 8 presents the values of test rmse r2 and the maximum error of ensemble models with different ensemble methods the best homogeneous ensemble model is presented along with heterogeneous ensemble models to compare their performance better the stacking method has the best results in terms of all three performance criteria compared to the other ensemble methods the voting method has the worst rmse and maximum error values although the owa method has a good rmse its maximum error is not suitable compared to other methods it is worth mentioning that rmse r2 and maximum error of cross validation are provided for stacking heterogeneous ensemble and boosting homogeneous ensemble because cross validation can only be used to evaluate these methods using cross validation is an effective method to prevent over fitting and one of the reasons for the simultaneous improvement of all performance criteria in the stacking method can be this feature eventually the ensemble model obtained using the stacking method was selected for use in the optimization model instead of the numerical simulation model because of the appropriate performance 4 4 optimal remediation scenario a differential evolution de optimization model was developed in the python environment using the scipy framework to find the optimal remediation scenario the mentioned model is a non linear constrained model whose objective function is to minimize the remediation cost by satisfying an essential constraint of the dnapl removal rate of more than 95 the injection rates of each well and the remediation duration are the decision variables of this model to estimate the dnapl removal rate in each iteration of the optimization the stacking ensemble model was used in lieu of the numerical simulation model table 6 presents the specifications of the optimal remediation scenario and the corresponding removal rate calculated by the computationally expensive sear simulation model the results showed that the total remediation duration in the optimal scenario is 18 7 days and the surfactant injection time is 8 7 days the constraint of the optimization is a removal rate of more than 95 which the sear simulation model also confirms the satisfaction of this constraint with the obtained scenario the maximum injection rate is for well no 6 and is injected at the maximum possible injection rate in that well well no 6 is located upstream of the contaminated area and has the most significant impact on contaminated area remediation each run of the utchem simulation model takes an average of 2 5 h significantly reduced using the surrogate model the execution time of the optimization model with replacing the ensemble surrogate model was about 120 min and more than 20 000 iterations were done to achieve the optimal scenario therefore with the same computational burden i e the same function evaluation the required time for optimization model using the numerical simulation model will be about 5 7 years when the surrogate model was used the main time cost is spent for the training and validation of the surrogate model and generating input samples 220 samples have been generated only once and as a result the numerical simulation model has been run only 220 times the same input samples were used to train all surrogate models training each surrogate model and finding their optimal hyper parameters depending on the complexity and process of model training took a maximum of few hours as a result the time required to train six different type surrogate models was about 12 h the time required to train es models except for the stacking model was about 12 h the most time of training and finding the optimal hyper parameters was related to the stacking model which took about 4 h therefore the total time required to generate 220 input samples 550 h training of surrogate models 12h and training of es models 16 h was 578 h this time is about 99 less than the time required to run a similar simulation optimization model using a numerical simulation model 5 discussion the present study presented significant results in three areas the effect of different feature scaling and hyper parameters optimization methods 2 evaluation of homogeneous ensemble models 3 presenting and evaluating the optimized stacking model 1 ml models are often sensitive to data scaling and fs methods greatly influence the accuracy of ml models the results showed that different fs methods must be tested when using different machine learning methods the kernel based ml models performed best when using the min max method and other ml models performed best when the power transformer method was used it should be noted that when using different kernels in kernel based methods the appropriate fs method may be changed so when using different kernels it is necessary to evaluate different fs methods for each kernel to train the most accurate surrogate model the optimal hyper parameters of the surrogate model must be found in this study the bo method was used and compared with conventional gs and rs methods the bo method in all cases led to training of a more accurate surrogate model the bo method consciously chooses the next candidate hyper parameters based on prior information thus bo also reduces the computational time 2 among the homogeneous es methods the boosting method had the best performance especially in high bias models the boosting method creates a base model in each iteration so that the new model corrects the weakness of the previous model unlike other homogeneous es methods such as weighted average methods that combine a few previously trained base models the boosting method constructs the required base models iteratively and eventually combines them this is the reason why boosting generated better results than other homogeneous es methods 3 the optimized stacking es method had a high ability to construct an accurate es model this model unlike other es methods utilized in other similar studies has the ability to combine base models in a non linear or black box manner the resulting stacking model had a large size so finding its optimal hyper parameters by conventional methods like gs and rs was computationally expensive and inefficient therefore the bo method was used to find its optimal hyperparameters the use of the bo method significantly reduced the computational load compared to conventional methods and led to the construction of a more accurate es model 6 conclusion in this study different surrogate models have been constructed and compared to select the best for emulating the numerical model in the problem of finding the optimal scenario of dnapl contaminated aquifer remediation using the sear method first six ml algorithms were applied and evaluated using the generated samples different fs methods were evaluated and compared to evaluate their impact on the performance of surrogate models also gs rs and bo methods were used and compared to tune and optimize ml models hyper parameters then using different homogeneous ensemble methods homogeneous ensemble models were constructed bagging and boosting homogeneous ensemble methods were used in this study and were compared with other methods subsequently the stacking ensemble model was constructed using the best single models and homogeneous ensemble models and the performance of the stacking method was compared with the common methods used in this field the stacking ensemble model outperformed others and was substituted as the best surrogate model in the optimization model instead of the numerical simulation model which had a very high computational load finally the optimal scenario of contaminated aquifer remediation was obtained the most important results can be summarized as below 1 the use of appropriate fs methods in the pre processing stage is very effective on ml models performance kernel based methods whose input data was scaled with the minmax method had the best performance and other models with the power transform scaler method had the best performance 2 bayesian optimization hyper parameters tuning method compared to other hyper parameter tuning methods has led to the construction of ml models with better performance 3 performance of homogeneous ensemble methods was compared and bagging and boosting methods performed better than other methods the efficiency of these methods is especially impressive when the base model has a high bias 4 the stacking ensemble method was compared with common ensemble methods results showed that the stacking performed better e g the stacking method reduced the test maximum error by 44 compared to the best heterogeneous es method owa the stacking method is more resistant to over fitting due to using cross validation and out of fold prediction 5 compared to gs and rs methods the bayesian optimization method led to finding better hyper parameters for the ensemble model that significantly increased the ensemble model s accuracy also the computational load and its execution time were much less because bayesian optimization chooses the next iteration hyper parameters in an informed manner funding this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors authorship statement all persons who meet authorship criteria are listed as authors and all authors certify that they have participated sufficiently in the work to take public responsibility for the content including participation in the concept design analysis writing or revision of the manuscript furthermore each author certifies that this material or similar material has not been and will not be submitted to or published in any other publication before its appearance in the journal of contaminant hydrology declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper the authors declare the following financial interests personal relationships which may be considered as potential competing interests appendix a supplementary data supplementary material image 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jconhyd 2021 103914 
1381,although migration of subsurface volatile organic compounds vocs from contaminant sources in unsaturated soil widely exists the related analytical models are quite limited a two dimensional analytical solution is hence developed to simulate vapor diffusion from the subsurface contaminant source in the layered unsaturated zone the contaminant source is simplified as a point source leaking at a constant rate the influences of several important factors including thickness of stagnant air layer depth of groundwater table source characteristics and soil layering characteristics on vapor migration in subsurface soil are comprehensively investigated by the present model soil type does not affect the normalized vapor concentration profile for homogeneous soil which is not valid for layered soil the width and effective diffusivity of the upward diffusion pathway and downward diffusion pathway are favorable indexes to evaluate the intensity of subsurface vapor horizontal diffusion the single layer capillary fringe assumption overestimates the vapor plume the assumption can give acceptable result for coarse soil while it is recommended to divide the soil into several layers based on the water filled porosity profile for fine soil graphical abstract unlabelled image keywords two dimensional model analytical solution layered soil point source vapor migration 1 introduction the leakages of underground storage tanks sanitary sewer lines as well as gas pipes have caused volatile organic compounds vocs contamination in soil and groundwater clement et al 2000 picone 2012 us epa 2012a vocs vapor will volatilize from these contaminated sites people around the contaminated sites may be exposed to the vocs vapor and their health will be threatened since most vocs are toxic for example the indoor air at a public office building in morgantown west virginia was contaminated by vocs vapors which led to eye irritation headache and nausea of the local residents kullman and hill 1990 thus the migration of vocs vapor in subsurface soil has caught extensive attention in engineering practices the subsurface contaminant sources are of various types most studies assume that the vapor originates from either the dissolved vocs in groundwater or the non aqueous phase liquid napl source on top of the groundwater table us epa 2012a ma et al 2014 yao et al 2017 feng et al 2020 however it is also possible that the vapor comes from a point source located in the unsaturated zone above the groundwater table for example the leakage of the sewer system containing contaminants results in soil contamination around the sewer line pennell et al 2013 dafny 2017 roghani et al 2018 additionally the injection of soil fumigant also leads to the existence of vocs e g 1 3 dichloropropene source in the unsaturated zone yates 2009 yates et al 2016 the movement of voc vapors from the subsurface contaminant sources may make people living in the nearby building threatened by vapor intrusion the vapor intrusion problems have caught lots of attention and numerous models have been developed in the last decade verginelli and yao 2021 to consider various factors such as soil layering yao et al 2015a 2017 feng et al 2020 bekele et al 2018 biodegradation verginelli and baciocchi 2011 2014 yao et al 2015b 2016 verginelli et al 2016 bekele et al 2018 moisture variation of soil shen et al 2013 bekele et al 2018 however the aforementioned vapor intrusion models in the last decade cannot consider point contaminant source located in the unsaturated zone in order to simulate vapor migration from point contaminant source in the unsaturated zone great efforts have been made by numerous scholars to develop appropriate numerical models falta et al 1989 bohy et al 2006 wang et al 2013 or analytical models shan and stephens 1995 troldborg et al 2009 yates 2009 compared with numerical methods analytical methods have the advantage of requiring much less computational cost and providing basic insight into the migration mechanisms liu et al 1998 yao et al 2015a carr and march 2018 therefore this study will focus on analytical models shan and stephens 1995 first proposed a one dimensional analytical solution to simulate the vertical migration of vocs but the horizontal migration of vocs in subsurface soil cannot be considered in order to simulate the potential multi dimensional migration of vocs in unsaturated zone yates 2009 and troldborg et al 2009 proposed two dimensional and three dimensional analytical models to simulate vocs migration from a point source in homogeneous unsaturated zone respectively but the two models assume the unsaturated zone is infinite with depth thus cannot consider the influence of groundwater table on the vapor migration additionally the ground generally consists of several soil layers and the soil properties may vary substantially with depth the soil layering characteristics may significantly influence the migration process to the best of our knowledge no existing analytical solution is available for vapor migration in the layered soil after releasing from a point source in unsaturated zone the objective of this study is to develop a two dimensional analytical solution for vocs vapor migration from the point source in layered unsaturated zone the detailed procedures of analytical solution derivation are presented in section 2 the analytical solution is verified against the numerical model in section 3 first the influences of several important factors on vapor migration are then investigated and further discussed in sections 4 and 5 respectively the analytical solution is expected to be an applicable tool to evaluate the vocs vapor plume in the subsurface which is helpful for risk assessment and remediation of soil contamination 2 unified two dimensional analytical solution for vapor diffusion in layered soil from point source 2 1 governing equation and boundary conditions the subsurface contaminant source in unsaturated soil is simplified as a point source in the two dimensional plane fig 1 the vocs vapor leaks from the point source continuously and diffuses in the layered soil which can be described by the following equation 1 d i 2 c i x z x 2 d i 2 c i x z z 2 q s δ x x s δ z z s i 1 2 n where q s mol m s is the leaking rate at the source x s m and z s m are the coordinates of the point source whose position is determined by the δ function d i m2 s is the effective diffusivity of the i th layer soil and can be calculated by millington quirk equation millington and quirk 1961 2 d i d a φ g i 10 3 φ t i 2 where φ g i is the gas filled porosity of the i th layer φ t i is the total porosity of the i th layer d a m2 s is the diffusivity of vapor in the atmosphere in practice the soil layer immediately above the groundwater table will suck the water up from the groundwater so that the capillary fringe exists near the groundwater table which has relatively low effective diffusivity us epa 2012a the johnson and ettinger model considers the capillary fringe as a distinct layer divided from the soil layer and characterized by a single moisture content and certain thickness us epa 2004 the same assumption is adopted here to consider the capillary fringe the subscript i in eq 1 and eq 2 is replaced by cf for the capillary fringe in fig 1 the ground along the horizontal direction is infinite thus horizontal infinite boundary should have been adopted here however this type of boundary condition bc significantly enhances the complexity of derivation therefore a horizontal finite calculation area the grey rectangle in fig 1c is introduced in order to reduce the influence of boundary condition an analysis area the red rectangle in fig 1c where the vapor concentration profile is assumed to be the same as that under the horizontal infinite boundary condition is divided from the calculation area the rationality of the assumption will be proved in the model verification part as shown in fig 1c the boundary conditions of the analytical model can be expressed as 3a top boundary λ d 1 c 1 x 0 z μ c 1 x 0 0 3b bottom boundary c n x l z 0 3c left boundary c i 0 z x 0 3d right boundary c i l x z x 0 where l x m is the width of the calculation area l m is the depth of groundwater table the bottom boundary is the groundwater table the point source is located at the horizontal center of the calculation area as well as the analysis area and the horizontal coordinate of the point source x s is half of l x as for the top boundary condition the vapor migration above the ground surface is complex and depends on the environmental factors e g surface cover wind in this paper two ground surface scenarios are considered namely open ground and stagnant air layer the scenario of open ground fig 1a widely exists and represents that the vapor can be quickly diluted in the atmosphere after reaching the ground surface the dirichlet boundary condition λ 0 and μ 1 in eq 3a is adopted for this scenario as for the condition that the vapor migration from the ground surface is impeded by the surface plants the scenario of stagnant air layer fig 1b is more suitable which has been considered in numerous researches jury et al 1983 1990 thomson et al 1997 nopper jaunky and wortham 2000 simunek and van genuchten 2007 the thickness of stagnant air layer d sal m is the key factor and the robin boundary condition λ 1 and μ d a d sal in eq 3a is introduced for this scenario the continuities of vapor concentration and flux between adjacent layers should be satisfied and can be expressed as 4a c i x h i c i 1 x h i 4b d i c i x h i z d i 1 c i 1 x h i z 2 2 analytical solutions for layered soil and homogeneous soil in order to obtain the analytical solution for layered soil the vapor concentration c i mol m3 can be divided into two independent variables 5 c i x z u i x z v i x z the governing equation can be rewritten as 6 d i 2 u i x z x 2 2 u i x z z 2 0 7 d i 2 v i x z x 2 2 v i x z z 2 q s δ x x s δ z z s the corresponding boundary conditions of the i th layer can be expressed as 8a u i x h i 1 p i 1 x 0 x l x 8b u i x h i p i x 0 x l x 8c u i 0 z x 0 h i 1 z h i 8d u i l x z x 0 h i 1 z h i 9a v i x h i 1 0 0 x l x 9b v i x h i 0 0 x l x 9c v i 0 z x 0 h i 1 z h i 9d v i l x z x 0 h i 1 z h i where p i x mol m3 is the vapor concentration profile at the interface between two adjacent layers in terms of u i the general solution to the governing equation eq 6 can be derived and simplified according to the boundary conditions from eq 8a to eq 8d 10 u i a i 0 z b i 0 m 1 a i m e λ m z b i m e λ m z cos λ m x where a i 0 b i 0 a i m and b i m are the undetermined coefficients of the general solution at the i th layer soil the eigenvalue λ m can be formulated as 11 λ m m π l x as for v i the general solution to eq 7 can be derived according to the boundary conditions from eq 9a to eq 9d 12 v i k 1 f i k sin β i k z h i 1 m 1 k 1 g i m k cos λ m x sin β i k z h i 1 the eigenvalue β i k for the i th layer soil can be formulated as 13 β i k k π l i where l i m is the thickness of the i th layer the coefficients f i k and g i m k in eq 12 can be formulated by submitting eq 12 back into the governing equation eq 7 14 f i k 2 q s β i k 2 d i l x l i sin β i k z s h i 1 15 g i m k 4 q s λ m 2 β i k 2 d i l x l i cos λ m x s sin β i k z s h i 1 by submitting eq 10 and eq 12 into eq 5 the general solution to vapor concentration profile can be expressed as 16 c i x z a i 0 z b i 0 m 1 a i m e λ m z z b i m e λ m z cos λ m x k 1 f i k sin β i k z h i 1 m 1 k 1 g i m k cos λ m x sin β i k z h i 1 then by submitting eq 16 into the concentration continuity condition eq 4a the relationship of undetermined coefficients between two adjacent layers can be built 17a a i 0 h i b i 0 a i 1 0 h i b i 1 0 17b a i m e λ m h i b i m e λ m h i a i 1 m e λ m h i b i 1 m e λ m h i similar relationship can be built by submitting eq 16 into the flux continuity condition eq 4b 18a a i 0 d i 1 k d i k 1 f i k β i k a i 1 0 d i 1 d i 1 k 1 f i 1 k β i 1 k 18b d i a i m e λ m h i b i m e λ m h i 1 k d i k 1 g i m k β i k d i 1 a i 1 m e λ m h i b i 1 m e λ m h i d i 1 k 1 g i 1 m k β i 1 k based on the boundary conditions eq 3a and eq 3b and relationships of undetermined coefficients between two adjacent layers from eq 17a to eq 18b all the undetermined coefficients can be formulated in a matrix for the condition that the eigenvalue λ m is 0 m 0 the equation can be built as 19 m 0 a 1 0 b 1 0 a 2 0 b 2 0 b i 0 a i 1 0 b i 1 0 a i 2 0 a n 1 0 b n 1 0 a n 0 b n 0 t a 0 0 d 2 k 1 f 2 k β 2 k d 1 k 1 1 k f 1 k β 1 k 0 d i 1 k 1 f i 1 k β i 1 k d i k 1 1 k f i k β i k 0 d i 2 k 1 f i 2 k β i 2 k d i 1 k 1 1 k f i 1 k β i 1 k 0 d n k 1 f n k β n k d n 1 k 1 1 k f n 1 k β n 1 k k 1 1 k f n k β n k where the matrix m 0 can be expressed as 20 m 0 t 1 0 t 2 0 0 0 0 0 0 0 0 0 0 0 h 1 1 h 1 1 0 0 0 0 0 0 0 0 d 1 0 d 2 0 0 0 0 0 0 0 0 0 0 0 0 0 h i 1 h i 1 0 0 0 0 0 0 0 0 d i 0 d i 1 0 0 0 0 0 0 0 0 0 0 0 h i 1 1 0 0 0 0 0 0 0 0 0 0 d i 1 0 0 0 0 0 0 0 0 0 0 0 0 0 h n 1 1 h n 1 1 0 0 0 0 0 0 0 0 d n 1 0 d n 0 0 0 0 0 0 0 0 0 0 0 1 0 similarly for the condition that the eigenvalue λ m is larger than 0 m 0 the equation can be built as 21 m m a 1 m b 1 m a 2 m b 2 m b i m a i 1 m b i 1 m a i 2 m a n 1 m b n 1 m a n m b n m t a m 0 d 2 k 1 g 2 m k β 2 k d 1 k 1 1 k g 1 m k β 1 k 0 d i 1 k 1 g i 1 m k β i 1 k d i k 1 1 k g i m k β i k 0 d i 2 k 1 g i 2 m k β i 2 k d i 1 k 1 1 k g i 1 m k β i 1 k 0 d n k 1 g n m k β n k d n 1 k 1 1 k g n 1 m k β n 1 k d n k 1 1 k g n m k β n k where the matrix m m can be expressed as 22 m m t 1 m t 2 m 0 0 0 0 0 0 e λ m h 1 e λ m h 1 0 0 0 0 0 0 d 1 e λ m h 1 d 1 e λ m h 1 0 0 0 0 0 0 0 0 e λ m h i e λ m h i e λ m h i e λ m h i 0 0 0 0 d i e λ m h i d i e λ m h i d i 1 e λ m h i d i 1 e λ m h i 0 0 0 0 0 0 e λ m h i 1 e λ m h i 1 0 0 0 0 0 0 d i 1 e λ m h i 1 d i 1 e λ m h i 1 0 0 0 0 0 0 0 0 e λ m h n 1 e λ m h n 1 0 0 0 0 0 0 d n e λ m h n 1 d n e λ m h n 1 0 0 0 0 0 0 d n e λ m l d n e λ m l the coefficients t 1 0 t 2 0 t a 0 t 1 m t 2 m and t a m in the above equations depend on the top boundary condition the expressions of these coefficients are all listed in table 1 the flowchart of derivation procedures is shown in fig 2 for the homogeneous soil scenario the unified analytical solution can be simplified as a particular case the derivation procedures are similar to those for layered soil and the general solution to vapor concentration i 1 is 23 c i x z a 0 z b 0 m 1 a m e λ m z z b m e λ m z cos λ m x k 1 f k sin β k z m 1 k 1 g m k cos λ m x sin β k z where a 0 b 0 a m b m f k and g m k are the coefficients that can be formulated as follows 24 a 0 k 1 f k β k 1 k 25 b 0 k 1 f k β k λ d 1 1 k 1 μ 26 a m k 1 g m k β k λ m e λ m l λ d 1 μ λ m λ d 1 1 k λ m e λ m l λ m λ d 1 μ λ m e λ m l μ λ m λ d 1 27 b m k 1 g m k β k λ m e λ m l λ d 1 μ λ m λ d 1 1 k λ m e λ m l λ m λ d 1 μ λ m e λ m l μ λ m λ d 1 28 f k 2 q s β k 2 d 1 l x l sin β k z s 29 g m k 4 q s λ m 2 β k 2 d 1 l x l cos λ m x s sin β k z s the eigenvalue β k can be formulated as 30 β k k π l in order to discuss the vapor migration in the analysis area more conveniently g i x fs z is introduced here 31 g i x fs z c i x fs x s z where x fs m is the horizontal distance from point source 3 model verification due to the lack of related experimental study a two dimensional numerical model built by the comsol software is adopted for model verification in the concerned model l x 60 m the source depth z s is 4 m and the leaking rate q s is 10 5 mol m s the depth of groundwater table is 8 m tce is taken as the representative contaminant and the tce vapor diffusivity in the atmosphere is 6 90 10 6 m2 s the thickness of the stagnant air layer is taken as 0 4 m following thomson et al 1997 for the homogeneous soil cases sand is adopted and the effect of capillary fringe is not considered for the layered soil cases sand loam and silt are adopted from top to bottom and the layer thicknesses are 3 3 and 2 m respectively a 1 63 m thick capillary fringe is divided from the bottom of silt layer the porosity characteristics and effective diffusivities are listed in table 2 in the comsol simulation a two dimensional model with a width of 120 m and a height of 8 m is built a relatively fine mesh is built by comsol automatically the number of mesh points is 1243 and the number of triangle elements is 2279 the maximum and minimum element sizes are 3 7 m and 0 0125 m respectively the vapor concentration profiles calculated by the analytical solution and the numerical model for homogeneous soil cases and layered soil cases are shown in fig s1 and fig 3 respectively the vapor concentration isopleths calculated by the analytical solution dashed lines match the results simulated by the numerical model g comsol color contours excellently well it is assumed that the vapor concentration profile in the analysis area with horizontal finite boundary can be equivalent to that with horizontal infinite boundary in order to prove this a two dimensional numerical model considering zero concentration condition at the horizontal infinite boundary is built using comsol to simulate the real scenario the parameters and soil types are the same as those in the layered soil cases in fig 3 and the capillary fringe is considered the result is compared with that calculated by the present analytical solution with a 60 m wide calculation area see fig s2 it is clear that the results calculated by the two methods are almost the same when the horizontal distance from point source is less than 20 m therefore taking the width of analysis area as 40 m is reasonable given l x 60 m the rationality of the assumption is proved 4 migration characteristics of vocs vapor leaking from the point source in the following sections the concerned contaminant is tce vapor whose diffusivity in the atmosphere is 6 90 10 6 m2 s without special mentioning the depth of groundwater table l is 6 m while the source depth z s is 3 m and the leaking rate q s equals 10 5 mol m s the open ground scenario is mainly concerned from sections 4 1 to 4 3 homogeneous silt cases are adopted in which the effective diffusivity of tce vapor is 6 60 10 7 m2 s the capillary fringe is not considered for better understanding the migration mechanism of vapor in section 4 4 sand and silt are adopted as high diffusivity soil and low diffusivity soil respectively to investigate the effect of capillary fringe and soil layering the default parameter values reported by us epa 2012b for soils as well as their capillary fringe are adopted see table 2 4 1 influence of thickness of stagnant air layer in fact the vapor concentration around the point source attracts more attention because the vapor concentration attenuates exponentially in horizontal direction lowell and eklund 2004 feng et al 2020 before the parametric study an appropriate width of calculation area should be determined to ensure that the vapor concentration in the analysis area is not affected by the horizontal finite boundary condition most states in the usa adopt 100 ft about 30 m lateral distance from the source as the exclusion criteria for vapor intrusion problem eklund et al 2018 therefore a 60 m wide analysis area d 60 m is adopted in the later parts here the relative difference of vapor concentration at the right boundary of the analysis area δ a n is introduced to determine the required minimum width of calculation area 32 δ a n g analytical g numerical g numerical 100 where g analytical mol m3 is the vapor concentration calculated by the analytical solution g numerical mol m3 is that calculated by the numerical model for the infinite boundary scenario as shown in fig 4 the vapor concentration at the right boundary of the analysis area decreases with increasing l x and has already been stable when l x reaches 200 m fig 4 also shows that δ a n is below 1 for the scenarios of open ground and stagnant air layer d sal 4 m if l x is larger than 77 m a calculation area with a width of 85 m is enough to satisfy a relative difference of 0 1 for the two scenarios herein a conservative value of 200 m is adopted since a larger l x value only slightly affects the computation cost as aforementioned the scenario of open ground is the special case for the scenario of stagnant air layer d sal 0 m the vapor concentration for the scenario of open ground is adopted as the reference to evaluate the influence of thickness of stagnant air layer the relative difference δ s o can be formulated as 33 δ s o g sal g og g og 100 where g sal and g og mol m3 are the vapor concentrations for the scenarios of stagnant air layer and open ground respectively as shown in fig 5 the relative difference increases with increasing d sal indicating that the existence of stagnant air layer increases the vapor concentration in soil the phenomenon is more obvious at the farther location from the point source moreover the stagnant air layer affects the vapor concentration in the shallow area more significantly than that in the deep area for example δ s o at a horizontal distance of 30 m from the source x fs exceeds 100 at the shallow area z 1 m in fig 5 a while δ s o at the same x fs is only 68 at the deep area z 5 m in fig 5 b the increase of vapor concentration with increasing d sal reflects the enhancement of subsurface horizontal diffusion of vapor δ s o varies slightly when the thickness of stagnant air layer is smaller than 0 4 m in fig 5 which can explain why the vapor concentration profiles in fig 3 and fig s1 are quite similar for the two scenarios since the open ground scenario is more classical and studied by most researches the following parts focus on this scenario fig 6 shows the vertical profile of normalized vapor concentration profiles at different x fs values where the vapor concentration is normalized by the vapor concentration at the groundwater table the profile pattern is significantly affected by the point source when x fs is less than 5 m and the influence is negligible when x fs is larger than 10 m thus the point source can only affect the pattern of vertical vapor concentration profile in the nearby area the difference of the vertical concentration profiles when x fs 5 m is so significant that this area nearby the source is regarded as the point source impact area 4 2 influence of depth of groundwater table fig 7 shows the influence of depth of groundwater table on the vapor concentration at different positions for x fs 0 m the vapor concentration at different depths decreases with increasing depth of groundwater table and then keeps constant see fig 7a the decreasing trend is more significant in the deep area e g z 5 m in fig 7a for the positions further laterally away from the point source the vapor concentration at different depths increases first and then decreases with increasing depth of groundwater table see fig 7b the peak of the vapor concentration occurs when the depth of groundwater table is close to x fs fig 8 shows the horizontal distribution of normalized vapor concentration for different depths of groundwater table the normalized vapor concentration attenuates exponentially from the point source along the horizontal direction after x fs is larger than 10 m the normalized vapor concentration decreases by one order of magnitude if the lateral distance increases by 9 m for l 6 m while 17 m for l 12 m therefore a shallower groundwater table enhances horizontal attenuation of vapor concentration however the normalized vapor concentration profiles near the source are very close which means that the effect of depth of groundwater table on horizontal attenuation in this area e g point source impact area is negligible 4 3 influence of source characteristics in engineering practices the vapor source may be located at any depth in the unsaturated zone and the thickness of soil over the source may also be changed thus the effect of source depth i e the distance between the ground surface and point source on vapor diffusion is necessary to be studied fig 9 a shows the influence of soil thickness over the point source on the horizontal distribution of normalized vapor concentration herein the distance between the point source and the groundwater table keeps constant 3 m when the soil thickness over the point source increases the depths of point source and groundwater table both increase this scenario corresponds to the condition that the soil at the ground surface is removed or added as shown in fig 9 a the horizontal attenuation of normalized vapor concentration for thinner soil is faster which drops by one order of magnitude with horizontal distance increasing by 8 m for l 6 m while 17 m for l 12 m when x fs is large enough it is interesting that the horizontal attenuation rate is similar to that in fig 8 which implies that the horizontal attenuation rate mainly depends on the depth of groundwater table and the influence of source depth is negligible for the scenario that the depth of groundwater table is a constant e g l 6 m the influence of source depth on the horizontal distribution of normalized vapor concentration is shown in fig 9 b the normalized vapor concentration decreases more significantly for shallow source especially in the point source impact area however the horizontal attenuation rates are similar for different source depths when x fs is large enough the normalized vapor concentration drops by one order of magnitude with the horizontal distance increasing by 9 m this phenomenon indicates that the influence of source depth on the horizontal attenuation rate outside the point source impact area is negligible fig 10 compares the calculated vapor concentration in subsurface soil for different source leaking rates q s 10 4 10 5 10 6 mol m s it can be found that the calculated vapor concentration ratios are equal to the source leaking rate ratios everywhere in the subsurface soil thus the vapor concentration in soil is linearly correlated with the source leaking rate the source leaking rate does not affect the vapor concentration distribution pattern when the diffusion process is considered only in engineering practices the entire distribution of vapor concentration and the source leaking rate can be back analyzed based on the detected concentration information at limited locations 4 4 influence of characteristics of layered soil apart from the aforementioned parameters the effective diffusivity of soil also has significant effect on vapor migration the influence of effective diffusivity of the homogeneous soil is shown in fig 11 a two types of soil are considered namely high diffusivity soil sand and low diffusivity soil silt the left part of fig 11 a shows that vapor concentration in low effective diffusivity soil is higher than that in high effective diffusivity soil the reason is that the low effective diffusivity soil can better impede the vapor migration from the source so that a greater amount of vapor is accumulated near the source the right part shows that the vapor concentration profiles normalized by source concentration g s are the same for the two soils thus the effective diffusivity of homogeneous soil does not affect the normalized vapor concentration profiles if the capillary fringe is not considered fig 11 b shows the vapor concentration profiles for the two types of soil considering the capillary fringe compared to that without considering the capillary fringe the vapor concentration attenuates more rapidly along the horizontal direction especially for the low diffusivity soil which contains a thicker capillary fringe meanwhile the normalized vapor concentration profiles for different soils are different when considering the capillary fringe it should be noted that taking the capillary fringe as a single distinct layer on the groundwater table is an approximate assumption for simplification the variation of water filled porosity φ w with depth may make the assumption unreasonable in fact the influence of φ w variation also can be considered by the proposed analytical solution by dividing the soil into several layers with different φ w φ w can be calculated by van genuchten model vg model herein the same two types of soil sand and silt are considered and the van genuchten model parameters are listed in table s1 figs s3 and s4 show comparison of the calculated vapor concentration between analytical solution and comsol different layer numbers num 10 20 and 30 are adopted to calculate the vapor concentration by the proposed analytical solution it can be found that num 20 is enough to obtain vapor concentration profile well matching the numerical results thus num 20 is adopted to calculate the vapor concentration profile in fig 11 c although the soil layers are divided evenly for simplification here the layer number and layer thickness can be optimized further for the area where φ w changes sharply e g bottom of sand layer in fig s3 the layer number can be increased and the layer thickness can be reduced for the area where φ w changes slightly e g top of sand layer in fig s3 the layer number can be decreased and the layer thickness can be increased for sand with high diffusivity φ w changes sharply in the bottom part but keeps almost constant in the other part see fig s3 φ w at the depth between 0 and 4 m is about 0 05 close to the value of sand in table 2 0 054 which explains why the vapor concentration in fig 11 c for high diffusivity soil is similar to that in fig 11 a and b for silt with low diffusivity φ w varies greatly over the entire depth see fig s4 the value is overall much larger than the value of silt in table 2 0 167 which explains why the vapor concentration in fig 11 c for low diffusivity soil is significantly different from that in fig 11 a and b therefore single layer capillary fringe assumption can give acceptable result for coarse soil like sand while it is recommended to divide the soil into several layers based on φ w calculated by vg model for fine soil like silt this also reflects that soil layering can significantly influence the vapor concentration profile moreover for both high diffusivity soil and low diffusivity soil the vapor concentration away from the source in fig 11 c is smaller than the respective one in fig 11 a and b namely single layer capillary fringe assumption or no consideration of capillary fringe overestimates the vapor plume fig 12 shows the normalized vapor concentration profile for four soil arrangements which refer to the typical scenarios reported by us epa 2012a the point source depth is 2 m the higher moisture content soil used in us epa 2012a is replaced by the low diffusivity soil silt here while the lower moisture content soil is the high diffusivity soil sand the groundwater table depth is 8 m case a only consists of high diffusivity soil case b includes alternately distributed low diffusivity soil and high diffusivity soil in case c the top low diffusivity soil layer in case b is replaced by high diffusivity soil in case d a 1 m thick low diffusivity soil layer is underlain by a 7 m thick high diffusivity soil layer in the four cases the bottom layers are all the high diffusivity soil from which the 0 17 m thick capillary fringe is divided as a distinct layer the horizontal attenuation of normalized vapor concentration is slower if there exists a low diffusivity soil layer above the source fig 12b and d the low diffusivity soil under the source mainly affects the scope of vapor plume but has less influence on the horizontal attenuation of vapor concentration the normalized vapor concentration drops by one order of magnitude with the horizontal distance increasing by about 11 m for case a fig 12a and case c fig 12c while 12 m for case b fig 12b and case d fig 12d 5 discussion 5 1 influence of contaminant source type feng et al 2020 recently proposed a 2 d analytical solution simulating vapor migration in layered soil from a semi infinite contaminant source at the groundwater the difference between the solution in this study and that in feng et al 2020 mainly lies in two aspects firstly the present solution considers a point source in the unsaturated zone e g leakage of the sewer system while that in feng et al 2020 considers semi infinite contaminant source at the groundwater e g groundwater contaminated by lnapls secondly the different source types lead to different governing equations boundary conditions and solution derivation procedures due to the evident difference the two analytical solutions are difficult to be equated in order to compare the two solutions the simulated case is the same as that for the open ground scenario adopted in section 3 the vertical vapor concentration profile at x fs 0 calculated by comsol can be linearized piecewise into 10 layers artificially fig s5 which is regarded as the boundary condition at the vertical profile close to the source for the solution in feng et al 2020 fig 13 shows comparison of the vapor concentration profiles calculated by the two solutions the vapor concentration calculated by the solution in feng et al 2020 is overall smaller the relative difference of calculated concentration by the two solutions ranges from 15 to 10 for most areas fig s6 the difference is caused by the non monotonic vapor concentration profile fig s5 used in the semi infinite source model proposed by feng et al 2020 as the boundary condition the directions of vapor flux on both sides of the point source are contrary which is in conflict with the required vapor flux continuity condition in the solution proposed by feng et al 2020 therefore the vapor concentration calculated by the solution in feng et al 2020 is not accurate for the point source scenario however for the groundwater source scenario the analytical solution proposed by feng et al 2020 is suitable because it is monotonic vapor concentration profile close to the source 5 2 effects of upward diffusion pathway and downward diffusion pathway the influences of several important parameters on vapor migration have been introduced in the preceding part in this section the influences of these parameters are further discussed by introducing the upward diffusion pathway and downward diffusion pathway the upward diffusion pathway is the soil above the point source while the downward diffusion pathway is that under the source the distance between the point source and the groundwater table is defined as the width of the downward diffusion pathway w ddp the increase of w ddp provides more space for vapor migrating downward which can enhance the horizontal diffusion of vapor and decrease the horizontal attenuation rate as shown in fig 8 similarly the distance between the point source and ground surface is defined as the width of the upward diffusion pathway w udp the increase of w udp enhances the impeding effect on vapor diffusion outward the ground which leads to the accumulation of vapor around the point source and the decrease of horizontal attenuation rate see fig 9a the influence of source depth given a certain depth of groundwater table is related to both w ddp and w udp with the increase of source depth w udp increases while w ddp decreases the effects of decreasing w ddp and increasing w udp on horizontal diffusion of vapor are contrary as aforementioned which leads to the similar horizontal attenuation rate of vapor concentration outside the point source impact area as shown in fig 9 b this can explain why the horizontal attenuation rate outside the point source impact area is affected by the depth of groundwater table only as shown in fig 8 the change in the depth of groundwater table does not influence the horizontal attenuation rate in the point source impact area indicating that the difference of horizontal attenuation rate in the point source impact area in fig 9 b is due to the influence of w udp while the influence of w ddp is negligible the effective diffusivity of soil within the upward diffusion pathway or downward diffusion pathway also affects the vapor diffusion the decrease of effective diffusivity of the soil layer in the upward diffusion pathway has an impeding effect on the upward diffusion enhancing the horizontal vapor diffusion and leading to the smaller horizontal attenuation rate fig 12b and d the decrease of effective diffusivity of the soil layer in the downward diffusion pathway impedes the downward diffusion so that the upward diffusion of vapor is enhanced which weakens horizontal diffusion in soil the influence of the capillary fringe can also confirm this with higher water filled porosity the capillary fringe has much lower effective diffusivity which weakens the horizontal diffusion as shown in fig 11 b and c in a word the increase of w udp or decrease of the effective diffusivity in the upward diffusion pathway will enhance the vapor horizontal diffusion while the decrease of w ddp or decrease of the effective diffusivity in the downward diffusion pathway can weaken the horizontal diffusion this is a qualitative conclusion for the simple cases considered in this study for the cases with more complex soil layer arrangements the proposed analytical solution can still be used to predict the vocs vapor plume and horizontal attenuation rate 5 3 application for determining contaminated region the steady state vocs vapor plume in unsaturated zone can be determined using the analytical solution which can be regarded as the most dangerous scenario here the contaminated region is where the vapor concentration is larger than the soil gas screening value which can be estimated according to the calculated vapor plume in this study the soil gas screening value of missouri reported by eklund et al 2018 is adopted which is 0 0042 mol m3 for tce the vapor contaminated regions of the four cases in fig 12 are calculated for example see fig 14 since the contaminated regions exceed the analysis area in fig 12 an 80 m wide analysis area is adopted in fig 14 and the corresponding width of calculation area is 400 m the edge of contaminated region is related to the soil layer arrangement and the areas of contaminated regions for the four cases are 447 01 501 15 440 73 and 511 27 m2 respectively in accordance with fig 12 stronger horizontal diffusion leads to a larger contaminated region the estimated vapor contaminated region is helpful for site investigation risk assessment and contamination remediation in soil 6 summary and conclusions an analytical solution is developed for simulating vocs vapor diffusion from the subsurface point source which is applicable for both homogenous soil as well as layered soil cases the solution is first verified against the results of the numerical model built by comsol the influences of thickness of stagnant air layer depth of groundwater table source characteristics and soil layering characteristics on the vapor diffusion in soil are investigated some major conclusions are drawn as follows 1 the existence of stagnant air layer enhances the subsurface horizontal diffusion of vapor the stagnant air layer affects the vapor concentration in shallow soil above the point source more significantly than that in the deep soil under the source if the thickness of stagnant air layer is smaller than 0 4 m its influence is weak 2 the increase of w udp or decrease of the effective diffusivity in the upward diffusion pathway enhances horizontal diffusion of vapor while the decrease of w ddp or decrease of the effective diffusivity in the downward diffusion pathway can weaken the horizontal diffusion the horizontal concentration attenuation rate is affected by the source depth mainly near the source but depend on the depth of groundwater table away from the source 3 the change in the effective diffusivity of soil as well as source leaking rate leads to the variation of vapor concentration in homogeneous soil but does not affect the normalized vapor concentration profile in engineering practices the entire distribution of vapor concentration and the source leaking rate can be back analyzed based on the detected concentration information at limited locations 4 the existence of capillary fringe weakens the horizontal diffusion of vapor single layer capillary fringe assumption overestimates the vapor plume the assumption can give acceptable result for coarse soil like sand while it is recommended to divide the soil into several layers based on φ w calculated by vg model for fine soil like silt 5 the analytical solution can be an applicable tool to preliminarily determine the subsurface contaminated region with a given screening value the estimated vapor contaminated region is helpful for site investigation risk assessment and remediation for subsurface vocs vapor contamination declaration of competing interest we declare that we do not have any commercial or associative interest that represents a conflict of interest in connection with the work submitted acknowledgements much of the work described in this paper was supported by the national key research and development program of china under grant no 2020yfc1808104 the national natural science foundation of china under grant nos 41725012 41931289 42077250 and the fundamental research funds for the central universities the authors would like to greatly acknowledge all these financial supports and express their most sincere gratitude appendix a supplementary data supplementary material image 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jconhyd 2021 103916 
1381,although migration of subsurface volatile organic compounds vocs from contaminant sources in unsaturated soil widely exists the related analytical models are quite limited a two dimensional analytical solution is hence developed to simulate vapor diffusion from the subsurface contaminant source in the layered unsaturated zone the contaminant source is simplified as a point source leaking at a constant rate the influences of several important factors including thickness of stagnant air layer depth of groundwater table source characteristics and soil layering characteristics on vapor migration in subsurface soil are comprehensively investigated by the present model soil type does not affect the normalized vapor concentration profile for homogeneous soil which is not valid for layered soil the width and effective diffusivity of the upward diffusion pathway and downward diffusion pathway are favorable indexes to evaluate the intensity of subsurface vapor horizontal diffusion the single layer capillary fringe assumption overestimates the vapor plume the assumption can give acceptable result for coarse soil while it is recommended to divide the soil into several layers based on the water filled porosity profile for fine soil graphical abstract unlabelled image keywords two dimensional model analytical solution layered soil point source vapor migration 1 introduction the leakages of underground storage tanks sanitary sewer lines as well as gas pipes have caused volatile organic compounds vocs contamination in soil and groundwater clement et al 2000 picone 2012 us epa 2012a vocs vapor will volatilize from these contaminated sites people around the contaminated sites may be exposed to the vocs vapor and their health will be threatened since most vocs are toxic for example the indoor air at a public office building in morgantown west virginia was contaminated by vocs vapors which led to eye irritation headache and nausea of the local residents kullman and hill 1990 thus the migration of vocs vapor in subsurface soil has caught extensive attention in engineering practices the subsurface contaminant sources are of various types most studies assume that the vapor originates from either the dissolved vocs in groundwater or the non aqueous phase liquid napl source on top of the groundwater table us epa 2012a ma et al 2014 yao et al 2017 feng et al 2020 however it is also possible that the vapor comes from a point source located in the unsaturated zone above the groundwater table for example the leakage of the sewer system containing contaminants results in soil contamination around the sewer line pennell et al 2013 dafny 2017 roghani et al 2018 additionally the injection of soil fumigant also leads to the existence of vocs e g 1 3 dichloropropene source in the unsaturated zone yates 2009 yates et al 2016 the movement of voc vapors from the subsurface contaminant sources may make people living in the nearby building threatened by vapor intrusion the vapor intrusion problems have caught lots of attention and numerous models have been developed in the last decade verginelli and yao 2021 to consider various factors such as soil layering yao et al 2015a 2017 feng et al 2020 bekele et al 2018 biodegradation verginelli and baciocchi 2011 2014 yao et al 2015b 2016 verginelli et al 2016 bekele et al 2018 moisture variation of soil shen et al 2013 bekele et al 2018 however the aforementioned vapor intrusion models in the last decade cannot consider point contaminant source located in the unsaturated zone in order to simulate vapor migration from point contaminant source in the unsaturated zone great efforts have been made by numerous scholars to develop appropriate numerical models falta et al 1989 bohy et al 2006 wang et al 2013 or analytical models shan and stephens 1995 troldborg et al 2009 yates 2009 compared with numerical methods analytical methods have the advantage of requiring much less computational cost and providing basic insight into the migration mechanisms liu et al 1998 yao et al 2015a carr and march 2018 therefore this study will focus on analytical models shan and stephens 1995 first proposed a one dimensional analytical solution to simulate the vertical migration of vocs but the horizontal migration of vocs in subsurface soil cannot be considered in order to simulate the potential multi dimensional migration of vocs in unsaturated zone yates 2009 and troldborg et al 2009 proposed two dimensional and three dimensional analytical models to simulate vocs migration from a point source in homogeneous unsaturated zone respectively but the two models assume the unsaturated zone is infinite with depth thus cannot consider the influence of groundwater table on the vapor migration additionally the ground generally consists of several soil layers and the soil properties may vary substantially with depth the soil layering characteristics may significantly influence the migration process to the best of our knowledge no existing analytical solution is available for vapor migration in the layered soil after releasing from a point source in unsaturated zone the objective of this study is to develop a two dimensional analytical solution for vocs vapor migration from the point source in layered unsaturated zone the detailed procedures of analytical solution derivation are presented in section 2 the analytical solution is verified against the numerical model in section 3 first the influences of several important factors on vapor migration are then investigated and further discussed in sections 4 and 5 respectively the analytical solution is expected to be an applicable tool to evaluate the vocs vapor plume in the subsurface which is helpful for risk assessment and remediation of soil contamination 2 unified two dimensional analytical solution for vapor diffusion in layered soil from point source 2 1 governing equation and boundary conditions the subsurface contaminant source in unsaturated soil is simplified as a point source in the two dimensional plane fig 1 the vocs vapor leaks from the point source continuously and diffuses in the layered soil which can be described by the following equation 1 d i 2 c i x z x 2 d i 2 c i x z z 2 q s δ x x s δ z z s i 1 2 n where q s mol m s is the leaking rate at the source x s m and z s m are the coordinates of the point source whose position is determined by the δ function d i m2 s is the effective diffusivity of the i th layer soil and can be calculated by millington quirk equation millington and quirk 1961 2 d i d a φ g i 10 3 φ t i 2 where φ g i is the gas filled porosity of the i th layer φ t i is the total porosity of the i th layer d a m2 s is the diffusivity of vapor in the atmosphere in practice the soil layer immediately above the groundwater table will suck the water up from the groundwater so that the capillary fringe exists near the groundwater table which has relatively low effective diffusivity us epa 2012a the johnson and ettinger model considers the capillary fringe as a distinct layer divided from the soil layer and characterized by a single moisture content and certain thickness us epa 2004 the same assumption is adopted here to consider the capillary fringe the subscript i in eq 1 and eq 2 is replaced by cf for the capillary fringe in fig 1 the ground along the horizontal direction is infinite thus horizontal infinite boundary should have been adopted here however this type of boundary condition bc significantly enhances the complexity of derivation therefore a horizontal finite calculation area the grey rectangle in fig 1c is introduced in order to reduce the influence of boundary condition an analysis area the red rectangle in fig 1c where the vapor concentration profile is assumed to be the same as that under the horizontal infinite boundary condition is divided from the calculation area the rationality of the assumption will be proved in the model verification part as shown in fig 1c the boundary conditions of the analytical model can be expressed as 3a top boundary λ d 1 c 1 x 0 z μ c 1 x 0 0 3b bottom boundary c n x l z 0 3c left boundary c i 0 z x 0 3d right boundary c i l x z x 0 where l x m is the width of the calculation area l m is the depth of groundwater table the bottom boundary is the groundwater table the point source is located at the horizontal center of the calculation area as well as the analysis area and the horizontal coordinate of the point source x s is half of l x as for the top boundary condition the vapor migration above the ground surface is complex and depends on the environmental factors e g surface cover wind in this paper two ground surface scenarios are considered namely open ground and stagnant air layer the scenario of open ground fig 1a widely exists and represents that the vapor can be quickly diluted in the atmosphere after reaching the ground surface the dirichlet boundary condition λ 0 and μ 1 in eq 3a is adopted for this scenario as for the condition that the vapor migration from the ground surface is impeded by the surface plants the scenario of stagnant air layer fig 1b is more suitable which has been considered in numerous researches jury et al 1983 1990 thomson et al 1997 nopper jaunky and wortham 2000 simunek and van genuchten 2007 the thickness of stagnant air layer d sal m is the key factor and the robin boundary condition λ 1 and μ d a d sal in eq 3a is introduced for this scenario the continuities of vapor concentration and flux between adjacent layers should be satisfied and can be expressed as 4a c i x h i c i 1 x h i 4b d i c i x h i z d i 1 c i 1 x h i z 2 2 analytical solutions for layered soil and homogeneous soil in order to obtain the analytical solution for layered soil the vapor concentration c i mol m3 can be divided into two independent variables 5 c i x z u i x z v i x z the governing equation can be rewritten as 6 d i 2 u i x z x 2 2 u i x z z 2 0 7 d i 2 v i x z x 2 2 v i x z z 2 q s δ x x s δ z z s the corresponding boundary conditions of the i th layer can be expressed as 8a u i x h i 1 p i 1 x 0 x l x 8b u i x h i p i x 0 x l x 8c u i 0 z x 0 h i 1 z h i 8d u i l x z x 0 h i 1 z h i 9a v i x h i 1 0 0 x l x 9b v i x h i 0 0 x l x 9c v i 0 z x 0 h i 1 z h i 9d v i l x z x 0 h i 1 z h i where p i x mol m3 is the vapor concentration profile at the interface between two adjacent layers in terms of u i the general solution to the governing equation eq 6 can be derived and simplified according to the boundary conditions from eq 8a to eq 8d 10 u i a i 0 z b i 0 m 1 a i m e λ m z b i m e λ m z cos λ m x where a i 0 b i 0 a i m and b i m are the undetermined coefficients of the general solution at the i th layer soil the eigenvalue λ m can be formulated as 11 λ m m π l x as for v i the general solution to eq 7 can be derived according to the boundary conditions from eq 9a to eq 9d 12 v i k 1 f i k sin β i k z h i 1 m 1 k 1 g i m k cos λ m x sin β i k z h i 1 the eigenvalue β i k for the i th layer soil can be formulated as 13 β i k k π l i where l i m is the thickness of the i th layer the coefficients f i k and g i m k in eq 12 can be formulated by submitting eq 12 back into the governing equation eq 7 14 f i k 2 q s β i k 2 d i l x l i sin β i k z s h i 1 15 g i m k 4 q s λ m 2 β i k 2 d i l x l i cos λ m x s sin β i k z s h i 1 by submitting eq 10 and eq 12 into eq 5 the general solution to vapor concentration profile can be expressed as 16 c i x z a i 0 z b i 0 m 1 a i m e λ m z z b i m e λ m z cos λ m x k 1 f i k sin β i k z h i 1 m 1 k 1 g i m k cos λ m x sin β i k z h i 1 then by submitting eq 16 into the concentration continuity condition eq 4a the relationship of undetermined coefficients between two adjacent layers can be built 17a a i 0 h i b i 0 a i 1 0 h i b i 1 0 17b a i m e λ m h i b i m e λ m h i a i 1 m e λ m h i b i 1 m e λ m h i similar relationship can be built by submitting eq 16 into the flux continuity condition eq 4b 18a a i 0 d i 1 k d i k 1 f i k β i k a i 1 0 d i 1 d i 1 k 1 f i 1 k β i 1 k 18b d i a i m e λ m h i b i m e λ m h i 1 k d i k 1 g i m k β i k d i 1 a i 1 m e λ m h i b i 1 m e λ m h i d i 1 k 1 g i 1 m k β i 1 k based on the boundary conditions eq 3a and eq 3b and relationships of undetermined coefficients between two adjacent layers from eq 17a to eq 18b all the undetermined coefficients can be formulated in a matrix for the condition that the eigenvalue λ m is 0 m 0 the equation can be built as 19 m 0 a 1 0 b 1 0 a 2 0 b 2 0 b i 0 a i 1 0 b i 1 0 a i 2 0 a n 1 0 b n 1 0 a n 0 b n 0 t a 0 0 d 2 k 1 f 2 k β 2 k d 1 k 1 1 k f 1 k β 1 k 0 d i 1 k 1 f i 1 k β i 1 k d i k 1 1 k f i k β i k 0 d i 2 k 1 f i 2 k β i 2 k d i 1 k 1 1 k f i 1 k β i 1 k 0 d n k 1 f n k β n k d n 1 k 1 1 k f n 1 k β n 1 k k 1 1 k f n k β n k where the matrix m 0 can be expressed as 20 m 0 t 1 0 t 2 0 0 0 0 0 0 0 0 0 0 0 h 1 1 h 1 1 0 0 0 0 0 0 0 0 d 1 0 d 2 0 0 0 0 0 0 0 0 0 0 0 0 0 h i 1 h i 1 0 0 0 0 0 0 0 0 d i 0 d i 1 0 0 0 0 0 0 0 0 0 0 0 h i 1 1 0 0 0 0 0 0 0 0 0 0 d i 1 0 0 0 0 0 0 0 0 0 0 0 0 0 h n 1 1 h n 1 1 0 0 0 0 0 0 0 0 d n 1 0 d n 0 0 0 0 0 0 0 0 0 0 0 1 0 similarly for the condition that the eigenvalue λ m is larger than 0 m 0 the equation can be built as 21 m m a 1 m b 1 m a 2 m b 2 m b i m a i 1 m b i 1 m a i 2 m a n 1 m b n 1 m a n m b n m t a m 0 d 2 k 1 g 2 m k β 2 k d 1 k 1 1 k g 1 m k β 1 k 0 d i 1 k 1 g i 1 m k β i 1 k d i k 1 1 k g i m k β i k 0 d i 2 k 1 g i 2 m k β i 2 k d i 1 k 1 1 k g i 1 m k β i 1 k 0 d n k 1 g n m k β n k d n 1 k 1 1 k g n 1 m k β n 1 k d n k 1 1 k g n m k β n k where the matrix m m can be expressed as 22 m m t 1 m t 2 m 0 0 0 0 0 0 e λ m h 1 e λ m h 1 0 0 0 0 0 0 d 1 e λ m h 1 d 1 e λ m h 1 0 0 0 0 0 0 0 0 e λ m h i e λ m h i e λ m h i e λ m h i 0 0 0 0 d i e λ m h i d i e λ m h i d i 1 e λ m h i d i 1 e λ m h i 0 0 0 0 0 0 e λ m h i 1 e λ m h i 1 0 0 0 0 0 0 d i 1 e λ m h i 1 d i 1 e λ m h i 1 0 0 0 0 0 0 0 0 e λ m h n 1 e λ m h n 1 0 0 0 0 0 0 d n e λ m h n 1 d n e λ m h n 1 0 0 0 0 0 0 d n e λ m l d n e λ m l the coefficients t 1 0 t 2 0 t a 0 t 1 m t 2 m and t a m in the above equations depend on the top boundary condition the expressions of these coefficients are all listed in table 1 the flowchart of derivation procedures is shown in fig 2 for the homogeneous soil scenario the unified analytical solution can be simplified as a particular case the derivation procedures are similar to those for layered soil and the general solution to vapor concentration i 1 is 23 c i x z a 0 z b 0 m 1 a m e λ m z z b m e λ m z cos λ m x k 1 f k sin β k z m 1 k 1 g m k cos λ m x sin β k z where a 0 b 0 a m b m f k and g m k are the coefficients that can be formulated as follows 24 a 0 k 1 f k β k 1 k 25 b 0 k 1 f k β k λ d 1 1 k 1 μ 26 a m k 1 g m k β k λ m e λ m l λ d 1 μ λ m λ d 1 1 k λ m e λ m l λ m λ d 1 μ λ m e λ m l μ λ m λ d 1 27 b m k 1 g m k β k λ m e λ m l λ d 1 μ λ m λ d 1 1 k λ m e λ m l λ m λ d 1 μ λ m e λ m l μ λ m λ d 1 28 f k 2 q s β k 2 d 1 l x l sin β k z s 29 g m k 4 q s λ m 2 β k 2 d 1 l x l cos λ m x s sin β k z s the eigenvalue β k can be formulated as 30 β k k π l in order to discuss the vapor migration in the analysis area more conveniently g i x fs z is introduced here 31 g i x fs z c i x fs x s z where x fs m is the horizontal distance from point source 3 model verification due to the lack of related experimental study a two dimensional numerical model built by the comsol software is adopted for model verification in the concerned model l x 60 m the source depth z s is 4 m and the leaking rate q s is 10 5 mol m s the depth of groundwater table is 8 m tce is taken as the representative contaminant and the tce vapor diffusivity in the atmosphere is 6 90 10 6 m2 s the thickness of the stagnant air layer is taken as 0 4 m following thomson et al 1997 for the homogeneous soil cases sand is adopted and the effect of capillary fringe is not considered for the layered soil cases sand loam and silt are adopted from top to bottom and the layer thicknesses are 3 3 and 2 m respectively a 1 63 m thick capillary fringe is divided from the bottom of silt layer the porosity characteristics and effective diffusivities are listed in table 2 in the comsol simulation a two dimensional model with a width of 120 m and a height of 8 m is built a relatively fine mesh is built by comsol automatically the number of mesh points is 1243 and the number of triangle elements is 2279 the maximum and minimum element sizes are 3 7 m and 0 0125 m respectively the vapor concentration profiles calculated by the analytical solution and the numerical model for homogeneous soil cases and layered soil cases are shown in fig s1 and fig 3 respectively the vapor concentration isopleths calculated by the analytical solution dashed lines match the results simulated by the numerical model g comsol color contours excellently well it is assumed that the vapor concentration profile in the analysis area with horizontal finite boundary can be equivalent to that with horizontal infinite boundary in order to prove this a two dimensional numerical model considering zero concentration condition at the horizontal infinite boundary is built using comsol to simulate the real scenario the parameters and soil types are the same as those in the layered soil cases in fig 3 and the capillary fringe is considered the result is compared with that calculated by the present analytical solution with a 60 m wide calculation area see fig s2 it is clear that the results calculated by the two methods are almost the same when the horizontal distance from point source is less than 20 m therefore taking the width of analysis area as 40 m is reasonable given l x 60 m the rationality of the assumption is proved 4 migration characteristics of vocs vapor leaking from the point source in the following sections the concerned contaminant is tce vapor whose diffusivity in the atmosphere is 6 90 10 6 m2 s without special mentioning the depth of groundwater table l is 6 m while the source depth z s is 3 m and the leaking rate q s equals 10 5 mol m s the open ground scenario is mainly concerned from sections 4 1 to 4 3 homogeneous silt cases are adopted in which the effective diffusivity of tce vapor is 6 60 10 7 m2 s the capillary fringe is not considered for better understanding the migration mechanism of vapor in section 4 4 sand and silt are adopted as high diffusivity soil and low diffusivity soil respectively to investigate the effect of capillary fringe and soil layering the default parameter values reported by us epa 2012b for soils as well as their capillary fringe are adopted see table 2 4 1 influence of thickness of stagnant air layer in fact the vapor concentration around the point source attracts more attention because the vapor concentration attenuates exponentially in horizontal direction lowell and eklund 2004 feng et al 2020 before the parametric study an appropriate width of calculation area should be determined to ensure that the vapor concentration in the analysis area is not affected by the horizontal finite boundary condition most states in the usa adopt 100 ft about 30 m lateral distance from the source as the exclusion criteria for vapor intrusion problem eklund et al 2018 therefore a 60 m wide analysis area d 60 m is adopted in the later parts here the relative difference of vapor concentration at the right boundary of the analysis area δ a n is introduced to determine the required minimum width of calculation area 32 δ a n g analytical g numerical g numerical 100 where g analytical mol m3 is the vapor concentration calculated by the analytical solution g numerical mol m3 is that calculated by the numerical model for the infinite boundary scenario as shown in fig 4 the vapor concentration at the right boundary of the analysis area decreases with increasing l x and has already been stable when l x reaches 200 m fig 4 also shows that δ a n is below 1 for the scenarios of open ground and stagnant air layer d sal 4 m if l x is larger than 77 m a calculation area with a width of 85 m is enough to satisfy a relative difference of 0 1 for the two scenarios herein a conservative value of 200 m is adopted since a larger l x value only slightly affects the computation cost as aforementioned the scenario of open ground is the special case for the scenario of stagnant air layer d sal 0 m the vapor concentration for the scenario of open ground is adopted as the reference to evaluate the influence of thickness of stagnant air layer the relative difference δ s o can be formulated as 33 δ s o g sal g og g og 100 where g sal and g og mol m3 are the vapor concentrations for the scenarios of stagnant air layer and open ground respectively as shown in fig 5 the relative difference increases with increasing d sal indicating that the existence of stagnant air layer increases the vapor concentration in soil the phenomenon is more obvious at the farther location from the point source moreover the stagnant air layer affects the vapor concentration in the shallow area more significantly than that in the deep area for example δ s o at a horizontal distance of 30 m from the source x fs exceeds 100 at the shallow area z 1 m in fig 5 a while δ s o at the same x fs is only 68 at the deep area z 5 m in fig 5 b the increase of vapor concentration with increasing d sal reflects the enhancement of subsurface horizontal diffusion of vapor δ s o varies slightly when the thickness of stagnant air layer is smaller than 0 4 m in fig 5 which can explain why the vapor concentration profiles in fig 3 and fig s1 are quite similar for the two scenarios since the open ground scenario is more classical and studied by most researches the following parts focus on this scenario fig 6 shows the vertical profile of normalized vapor concentration profiles at different x fs values where the vapor concentration is normalized by the vapor concentration at the groundwater table the profile pattern is significantly affected by the point source when x fs is less than 5 m and the influence is negligible when x fs is larger than 10 m thus the point source can only affect the pattern of vertical vapor concentration profile in the nearby area the difference of the vertical concentration profiles when x fs 5 m is so significant that this area nearby the source is regarded as the point source impact area 4 2 influence of depth of groundwater table fig 7 shows the influence of depth of groundwater table on the vapor concentration at different positions for x fs 0 m the vapor concentration at different depths decreases with increasing depth of groundwater table and then keeps constant see fig 7a the decreasing trend is more significant in the deep area e g z 5 m in fig 7a for the positions further laterally away from the point source the vapor concentration at different depths increases first and then decreases with increasing depth of groundwater table see fig 7b the peak of the vapor concentration occurs when the depth of groundwater table is close to x fs fig 8 shows the horizontal distribution of normalized vapor concentration for different depths of groundwater table the normalized vapor concentration attenuates exponentially from the point source along the horizontal direction after x fs is larger than 10 m the normalized vapor concentration decreases by one order of magnitude if the lateral distance increases by 9 m for l 6 m while 17 m for l 12 m therefore a shallower groundwater table enhances horizontal attenuation of vapor concentration however the normalized vapor concentration profiles near the source are very close which means that the effect of depth of groundwater table on horizontal attenuation in this area e g point source impact area is negligible 4 3 influence of source characteristics in engineering practices the vapor source may be located at any depth in the unsaturated zone and the thickness of soil over the source may also be changed thus the effect of source depth i e the distance between the ground surface and point source on vapor diffusion is necessary to be studied fig 9 a shows the influence of soil thickness over the point source on the horizontal distribution of normalized vapor concentration herein the distance between the point source and the groundwater table keeps constant 3 m when the soil thickness over the point source increases the depths of point source and groundwater table both increase this scenario corresponds to the condition that the soil at the ground surface is removed or added as shown in fig 9 a the horizontal attenuation of normalized vapor concentration for thinner soil is faster which drops by one order of magnitude with horizontal distance increasing by 8 m for l 6 m while 17 m for l 12 m when x fs is large enough it is interesting that the horizontal attenuation rate is similar to that in fig 8 which implies that the horizontal attenuation rate mainly depends on the depth of groundwater table and the influence of source depth is negligible for the scenario that the depth of groundwater table is a constant e g l 6 m the influence of source depth on the horizontal distribution of normalized vapor concentration is shown in fig 9 b the normalized vapor concentration decreases more significantly for shallow source especially in the point source impact area however the horizontal attenuation rates are similar for different source depths when x fs is large enough the normalized vapor concentration drops by one order of magnitude with the horizontal distance increasing by 9 m this phenomenon indicates that the influence of source depth on the horizontal attenuation rate outside the point source impact area is negligible fig 10 compares the calculated vapor concentration in subsurface soil for different source leaking rates q s 10 4 10 5 10 6 mol m s it can be found that the calculated vapor concentration ratios are equal to the source leaking rate ratios everywhere in the subsurface soil thus the vapor concentration in soil is linearly correlated with the source leaking rate the source leaking rate does not affect the vapor concentration distribution pattern when the diffusion process is considered only in engineering practices the entire distribution of vapor concentration and the source leaking rate can be back analyzed based on the detected concentration information at limited locations 4 4 influence of characteristics of layered soil apart from the aforementioned parameters the effective diffusivity of soil also has significant effect on vapor migration the influence of effective diffusivity of the homogeneous soil is shown in fig 11 a two types of soil are considered namely high diffusivity soil sand and low diffusivity soil silt the left part of fig 11 a shows that vapor concentration in low effective diffusivity soil is higher than that in high effective diffusivity soil the reason is that the low effective diffusivity soil can better impede the vapor migration from the source so that a greater amount of vapor is accumulated near the source the right part shows that the vapor concentration profiles normalized by source concentration g s are the same for the two soils thus the effective diffusivity of homogeneous soil does not affect the normalized vapor concentration profiles if the capillary fringe is not considered fig 11 b shows the vapor concentration profiles for the two types of soil considering the capillary fringe compared to that without considering the capillary fringe the vapor concentration attenuates more rapidly along the horizontal direction especially for the low diffusivity soil which contains a thicker capillary fringe meanwhile the normalized vapor concentration profiles for different soils are different when considering the capillary fringe it should be noted that taking the capillary fringe as a single distinct layer on the groundwater table is an approximate assumption for simplification the variation of water filled porosity φ w with depth may make the assumption unreasonable in fact the influence of φ w variation also can be considered by the proposed analytical solution by dividing the soil into several layers with different φ w φ w can be calculated by van genuchten model vg model herein the same two types of soil sand and silt are considered and the van genuchten model parameters are listed in table s1 figs s3 and s4 show comparison of the calculated vapor concentration between analytical solution and comsol different layer numbers num 10 20 and 30 are adopted to calculate the vapor concentration by the proposed analytical solution it can be found that num 20 is enough to obtain vapor concentration profile well matching the numerical results thus num 20 is adopted to calculate the vapor concentration profile in fig 11 c although the soil layers are divided evenly for simplification here the layer number and layer thickness can be optimized further for the area where φ w changes sharply e g bottom of sand layer in fig s3 the layer number can be increased and the layer thickness can be reduced for the area where φ w changes slightly e g top of sand layer in fig s3 the layer number can be decreased and the layer thickness can be increased for sand with high diffusivity φ w changes sharply in the bottom part but keeps almost constant in the other part see fig s3 φ w at the depth between 0 and 4 m is about 0 05 close to the value of sand in table 2 0 054 which explains why the vapor concentration in fig 11 c for high diffusivity soil is similar to that in fig 11 a and b for silt with low diffusivity φ w varies greatly over the entire depth see fig s4 the value is overall much larger than the value of silt in table 2 0 167 which explains why the vapor concentration in fig 11 c for low diffusivity soil is significantly different from that in fig 11 a and b therefore single layer capillary fringe assumption can give acceptable result for coarse soil like sand while it is recommended to divide the soil into several layers based on φ w calculated by vg model for fine soil like silt this also reflects that soil layering can significantly influence the vapor concentration profile moreover for both high diffusivity soil and low diffusivity soil the vapor concentration away from the source in fig 11 c is smaller than the respective one in fig 11 a and b namely single layer capillary fringe assumption or no consideration of capillary fringe overestimates the vapor plume fig 12 shows the normalized vapor concentration profile for four soil arrangements which refer to the typical scenarios reported by us epa 2012a the point source depth is 2 m the higher moisture content soil used in us epa 2012a is replaced by the low diffusivity soil silt here while the lower moisture content soil is the high diffusivity soil sand the groundwater table depth is 8 m case a only consists of high diffusivity soil case b includes alternately distributed low diffusivity soil and high diffusivity soil in case c the top low diffusivity soil layer in case b is replaced by high diffusivity soil in case d a 1 m thick low diffusivity soil layer is underlain by a 7 m thick high diffusivity soil layer in the four cases the bottom layers are all the high diffusivity soil from which the 0 17 m thick capillary fringe is divided as a distinct layer the horizontal attenuation of normalized vapor concentration is slower if there exists a low diffusivity soil layer above the source fig 12b and d the low diffusivity soil under the source mainly affects the scope of vapor plume but has less influence on the horizontal attenuation of vapor concentration the normalized vapor concentration drops by one order of magnitude with the horizontal distance increasing by about 11 m for case a fig 12a and case c fig 12c while 12 m for case b fig 12b and case d fig 12d 5 discussion 5 1 influence of contaminant source type feng et al 2020 recently proposed a 2 d analytical solution simulating vapor migration in layered soil from a semi infinite contaminant source at the groundwater the difference between the solution in this study and that in feng et al 2020 mainly lies in two aspects firstly the present solution considers a point source in the unsaturated zone e g leakage of the sewer system while that in feng et al 2020 considers semi infinite contaminant source at the groundwater e g groundwater contaminated by lnapls secondly the different source types lead to different governing equations boundary conditions and solution derivation procedures due to the evident difference the two analytical solutions are difficult to be equated in order to compare the two solutions the simulated case is the same as that for the open ground scenario adopted in section 3 the vertical vapor concentration profile at x fs 0 calculated by comsol can be linearized piecewise into 10 layers artificially fig s5 which is regarded as the boundary condition at the vertical profile close to the source for the solution in feng et al 2020 fig 13 shows comparison of the vapor concentration profiles calculated by the two solutions the vapor concentration calculated by the solution in feng et al 2020 is overall smaller the relative difference of calculated concentration by the two solutions ranges from 15 to 10 for most areas fig s6 the difference is caused by the non monotonic vapor concentration profile fig s5 used in the semi infinite source model proposed by feng et al 2020 as the boundary condition the directions of vapor flux on both sides of the point source are contrary which is in conflict with the required vapor flux continuity condition in the solution proposed by feng et al 2020 therefore the vapor concentration calculated by the solution in feng et al 2020 is not accurate for the point source scenario however for the groundwater source scenario the analytical solution proposed by feng et al 2020 is suitable because it is monotonic vapor concentration profile close to the source 5 2 effects of upward diffusion pathway and downward diffusion pathway the influences of several important parameters on vapor migration have been introduced in the preceding part in this section the influences of these parameters are further discussed by introducing the upward diffusion pathway and downward diffusion pathway the upward diffusion pathway is the soil above the point source while the downward diffusion pathway is that under the source the distance between the point source and the groundwater table is defined as the width of the downward diffusion pathway w ddp the increase of w ddp provides more space for vapor migrating downward which can enhance the horizontal diffusion of vapor and decrease the horizontal attenuation rate as shown in fig 8 similarly the distance between the point source and ground surface is defined as the width of the upward diffusion pathway w udp the increase of w udp enhances the impeding effect on vapor diffusion outward the ground which leads to the accumulation of vapor around the point source and the decrease of horizontal attenuation rate see fig 9a the influence of source depth given a certain depth of groundwater table is related to both w ddp and w udp with the increase of source depth w udp increases while w ddp decreases the effects of decreasing w ddp and increasing w udp on horizontal diffusion of vapor are contrary as aforementioned which leads to the similar horizontal attenuation rate of vapor concentration outside the point source impact area as shown in fig 9 b this can explain why the horizontal attenuation rate outside the point source impact area is affected by the depth of groundwater table only as shown in fig 8 the change in the depth of groundwater table does not influence the horizontal attenuation rate in the point source impact area indicating that the difference of horizontal attenuation rate in the point source impact area in fig 9 b is due to the influence of w udp while the influence of w ddp is negligible the effective diffusivity of soil within the upward diffusion pathway or downward diffusion pathway also affects the vapor diffusion the decrease of effective diffusivity of the soil layer in the upward diffusion pathway has an impeding effect on the upward diffusion enhancing the horizontal vapor diffusion and leading to the smaller horizontal attenuation rate fig 12b and d the decrease of effective diffusivity of the soil layer in the downward diffusion pathway impedes the downward diffusion so that the upward diffusion of vapor is enhanced which weakens horizontal diffusion in soil the influence of the capillary fringe can also confirm this with higher water filled porosity the capillary fringe has much lower effective diffusivity which weakens the horizontal diffusion as shown in fig 11 b and c in a word the increase of w udp or decrease of the effective diffusivity in the upward diffusion pathway will enhance the vapor horizontal diffusion while the decrease of w ddp or decrease of the effective diffusivity in the downward diffusion pathway can weaken the horizontal diffusion this is a qualitative conclusion for the simple cases considered in this study for the cases with more complex soil layer arrangements the proposed analytical solution can still be used to predict the vocs vapor plume and horizontal attenuation rate 5 3 application for determining contaminated region the steady state vocs vapor plume in unsaturated zone can be determined using the analytical solution which can be regarded as the most dangerous scenario here the contaminated region is where the vapor concentration is larger than the soil gas screening value which can be estimated according to the calculated vapor plume in this study the soil gas screening value of missouri reported by eklund et al 2018 is adopted which is 0 0042 mol m3 for tce the vapor contaminated regions of the four cases in fig 12 are calculated for example see fig 14 since the contaminated regions exceed the analysis area in fig 12 an 80 m wide analysis area is adopted in fig 14 and the corresponding width of calculation area is 400 m the edge of contaminated region is related to the soil layer arrangement and the areas of contaminated regions for the four cases are 447 01 501 15 440 73 and 511 27 m2 respectively in accordance with fig 12 stronger horizontal diffusion leads to a larger contaminated region the estimated vapor contaminated region is helpful for site investigation risk assessment and contamination remediation in soil 6 summary and conclusions an analytical solution is developed for simulating vocs vapor diffusion from the subsurface point source which is applicable for both homogenous soil as well as layered soil cases the solution is first verified against the results of the numerical model built by comsol the influences of thickness of stagnant air layer depth of groundwater table source characteristics and soil layering characteristics on the vapor diffusion in soil are investigated some major conclusions are drawn as follows 1 the existence of stagnant air layer enhances the subsurface horizontal diffusion of vapor the stagnant air layer affects the vapor concentration in shallow soil above the point source more significantly than that in the deep soil under the source if the thickness of stagnant air layer is smaller than 0 4 m its influence is weak 2 the increase of w udp or decrease of the effective diffusivity in the upward diffusion pathway enhances horizontal diffusion of vapor while the decrease of w ddp or decrease of the effective diffusivity in the downward diffusion pathway can weaken the horizontal diffusion the horizontal concentration attenuation rate is affected by the source depth mainly near the source but depend on the depth of groundwater table away from the source 3 the change in the effective diffusivity of soil as well as source leaking rate leads to the variation of vapor concentration in homogeneous soil but does not affect the normalized vapor concentration profile in engineering practices the entire distribution of vapor concentration and the source leaking rate can be back analyzed based on the detected concentration information at limited locations 4 the existence of capillary fringe weakens the horizontal diffusion of vapor single layer capillary fringe assumption overestimates the vapor plume the assumption can give acceptable result for coarse soil like sand while it is recommended to divide the soil into several layers based on φ w calculated by vg model for fine soil like silt 5 the analytical solution can be an applicable tool to preliminarily determine the subsurface contaminated region with a given screening value the estimated vapor contaminated region is helpful for site investigation risk assessment and remediation for subsurface vocs vapor contamination declaration of competing interest we declare that we do not have any commercial or associative interest that represents a conflict of interest in connection with the work submitted acknowledgements much of the work described in this paper was supported by the national key research and development program of china under grant no 2020yfc1808104 the national natural science foundation of china under grant nos 41725012 41931289 42077250 and the fundamental research funds for the central universities the authors would like to greatly acknowledge all these financial supports and express their most sincere gratitude appendix a supplementary data supplementary material image 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jconhyd 2021 103916 
1382,the transport of cationic amine modified latex aml and anionic carboxylate modified latex cml microspheres through a discrete sandstone fracture with mineralogical heterogeneity and roughness was studied two microsphere sizes 200 nm and 1000 nm two ionic strengths 5 mm and 10 mm and two specific discharges 0 35 mm s 1 and 0 70 mm s 1 were tested to observe the impact on transport under favorable and unfavorable conditions the difference in retention between aml net favorable and cml net unfavorable microsphere attachment was 25 for the 200 nm microspheres and 13 for the 1000 nm microspheres less than 50 of the aml microspheres were retained in the fracture postulated to be due to the effects of mineralogical heterogeneity and fracture surface roughness the effect of an increase in ionic strength in increasing retention was significant for unfavorable attachment but was not significant for favorable attachment conditions the effect of specific discharge was minor for all but the 200 nm cml microspheres at 10 mm ionic strength when flushing the fracture first with cationic microspheres then with anionic microspheres the recovery of anionic microspheres resembled favorable attachment presumably due to interaction with cationic microspheres that remained attached to the sandstone surface colloid breakthrough curves could be fit well with a two site attachment model with reversible and irreversible sites keywords colloids sandstone fracture unfavorable attachment favorable attachment 1 introduction colloid transport in the subsurface is a well studied subject in the literature because of the impact different types of colloids can have on the environment and human health from the movement of pathogenic biocolloids bacteria and viruses to colloid mediated radionuclide transport in the subsurface to application of remediation agents e g nano particles and bio augmentation agents improving the understanding of colloid transport is important interest in recent subsurface remediation techniques such as the use of nano scale zero valent iron nzvi has also increased the focus on studying colloid transport kocur et al 2013 kocur et al 2014 johnson et al 2013 numerous studies related to colloid transport in porous media have been conducted and recently there has been increased interest in transport through fractures which can be preferential pathways for contaminant migration fractures create preferential pathways for colloids because they can have high velocity flows diffusion of colloids into the matrix will be low and the attachment of colloids to fracture walls may be limited especially in unfavorable attachment conditions zvikelsky and weisbrod 2006 lab studies mccarthy et al 2002 albarran et al 2013 alaskar et al 2015 stoll et al 2016 field studies becker et al 2003 masciopinto and chrysikopoulos 2008 zhang et al 2015 and modeling abdel salam and chrysikopoulos 1994 zheng et al 2009 james and chrysikopoulos 2011 james et al 2018 and literature reviews zhang et al 2012 of colloid transport through fractures have been increasing in recent years but it is still a subject that requires further investigation the surface charges of the collector fractured rock or porous media and the colloid are key factors in colloid transport unfavorable attachment conditions exist when the colloid and collector have like charges and electric double layer interactions create an energy barrier to attachment favorable attachment conditions exist when the colloid and collector have opposite charges creating an electrostatic attractive force in the subsurface environment unfavorable attachment conditions are more common because most colloids and collectors of environmental interest bacteria viruses clays sandstone etc have a negative surface charge at typical ph values of groundwater being more common in the environment and being more difficult to predict than favorable attachment has resulted in most colloid transport studies being focused on unfavorable conditions although less common in nature favorable attachment conditions can develop in low ph or high ionic strength environments the existence of positively charged minerals such as titanium oxide iron oxide magnesium oxide and magnesium calcite which have points of zero charge pzc at near neutral or higher ph elimelech et al 1995 can also lead to favorable attachment conditions there have been favorable attachment focused studies in porous media elimelech 1991 elimelech 1994 but in fractures the subject has not been well explored while unfavorable attachment conditions associated with negatively charged colloids and negatively charged mineral surfaces are common colloid retention under unfavorable conditions in both porous media and fractured media has been widely observed tufenkji and elimelech 2004a johnson and tong 2006 mondal and sleep 2012 this retention has been attributed to various mechanisms colloid retention under unfavorable conditions has often been attributed to retention in the secondary minimum of the derjaguin landau verwey and overbeek dlvo energy profile colloids in the secondary minimum zone are subject to hydrodynamic drag from fluid flow and may continue to translate along solid surfaces in the direction of fluid flow unless or until they end up in stagnant flow zones redman et al 2004 kuznar and elimelech 2007 johnson et al 2007 while the role of stagnant flow zones has been investigated for porous media this has not been studied for colloid transport in fractures rasmuson et al 2020 showed in lab studies that colloid retention in fractured till was lower by orders of magnitude than colloid retention in sand accompanying simulations of colloid transport in idealized porous media and smooth parallel plate fractures indicated that the difference in retention could be explained by the impact of flow impingement on forward flow stagnation zones in porous media a phenomenon not present in the smooth parallel plate simulations in natural rough fractures the relative importance of stagnant flow zones in fractures would depend on the fracture roughness the role of recirculation zones briggs et al 2017 and the extent of contact points between fracture surfaces that could produce stagnant zones as well as colloid retention due to wedging johnson et al 2007 previous studies of colloid transport in fractures under unfavorable conditions for example mondal and sleep 2012 mondal and sleep 2013 rasmuson et al 2020 would suggest that these impacts are much less important in fractures compared to porous media geochemical heterogeneity of collector surfaces where nanoscale positively charged zones exist on surfaces that are overall negatively charged has been shown to lead to colloid attachment to these zones pazmino et al 2014a 2014b nanoscale surface roughness has also been theoretically shown to reduce repulsion as roughness decreases the radius of curvature of the collector surface reducing edl and van der waals interactions as they decrease with decreasing radius of curvature israelachvili 2011 bendersky and davis 2011 roughness can also promote attachment by increasing potential points of colloid collector contact jin et al 2015 rasmuson et al 2017 however the impact of roughness in colloid retention and attachment is complex studies have shown that nanoscale roughness would be expected to decrease retention in the secondary minimum and enhance attachment in the primary minimum but with weaker primary minimum attachment bradford and torkzaban 2015 torkzaban and bradford 2016 bradford et al 2017 in contrast microscale roughness was expected to increase colloid retention through decreases in lever arms related to hydrodynamic torque and increases in lever arms related to adhesive torque bradford and torkzaban 2015 torkzaban and bradford 2016 increased spatial variability in mineralogy and roughness and the associated spatial variability in collector efficiencies or attachment coefficients can increase the spreading of a colloid plume and enhance transport katzourakis and chrysikopoulos 2017 while roughness can enhance colloid attachment under unfavorable conditions it may also decrease attachment under favorable conditions as roughness can decrease repulsion under unfavorable conditions it can also decrease attraction under favorable conditions rasmuson et al 2017 rasmuson et al 2017 studied microsphere attachment to glass slides of varying roughness with an impinging jet system for colloids in the range of 0 25 to 2 μm they found two orders of magnitude difference in collector efficiencies between favorable and unfavorable conditions on slides with less than 1 nm roughness collector efficiencies for 20 nm and 4 4 μm colloids were similar for both favorable and unfavorable conditions for the 1 nm roughness with roughness of 38 nm and 546 nm collector efficiencies for favorable conditions decreased relative to the 1 nm roughness efficiencies this was attributed to dlvo effects reduced radius of surface curvature and increased hydrodynamic slip length and near surface velocities over rougher surfaces for unfavorable conditions rasmuson et al 2017 found that collector efficiencies were increased due to the lower repulsive forces associated with smaller radius of surface curvature and due to greater adhesion associated with contact with multiple asperities with the decrease in attachment under favorable conditions and increase in attachment under unfavorable conditions retention of colloids was similar for both conditions for roughnesses of 38 nm and 546 nm no significant difference was found between results with 38 nm roughness and 546 nm roughness colloid transport across a glass slide may have similarities to transport along fracture surfaces although with much greater roughness perhaps more mineralogical heterogeneity with some contact points between fracture surfaces consequently it is hypothesized that colloid transport in fractures under favorable and unfavorable conditions would have more similar retention than would typically occur in porous media with lower retention than expected in porous media particularly for favorable condtions in addition to retention and attachment the fate of colloids in the subsurface can be impacted by changes in porewater geochemistry which may lead to detachment and reentrainment retention in the secondary minimum is reversible and a reduction in ionic strength which reduces the depth of the secondary minimum has been postulated as the reason for re entrainment of colloids in porous media franchi and o melia 2003 hahn et al 2004 redman et al 2004 and fractured dolomite mondal and sleep 2012 with reductions in ionic strength pazmino et al 2014b showed through modeling that steric repulsion coupled with discrete zones of chemical heterogeneity could lead to conditions under which perturbations in velocity or ionic strength could cause release from primary attachment with weaker primary minimum attachment in the presence of nanoscale roughness torkzaban and bradford 2016 postulated from model predictions that reductions in ionic strength could lead to colloid detachment through hydrodynamic shear or diffusion shen et al 2018 showed that primary attachment to nanoscale convex asperities was reversible with reductions in ionic strength while primary attachment in nanoscale convex asperities was irreversible due to an increase in primary minimum depth with a decrease in ionic strength rock fracture surfaces are likely to exhibit both nanoscale and microscale roughness and chemical heterogeneity and transport of colloids in rock fractures under favorable and unfavorable conditions would be expected to be a complex function of the interacting effects of these features as sandstone aquifers are common sources for drinking water the transport of colloids in sandstone fractures is of particular interest in this study transport experiments were conducted in a lab scale sandstone single fracture using amine and carboxylate modified latex microspheres to investigate colloid transport for both favorable and unfavorable attachment conditions experimental variables included solution ionic strength specific discharge and colloid size 2 materials and methods 2 1 sandstone fracture a block of sandstone originating from a quarry in northern china was acquired from a building materials supplier in toronto ontario and was cut to 280 210 70 mm in size a single fracture along the longer directions was artificially created in lab and the fractured sandstone block was used in the transport experiments the top and bottom fracture wall surfaces were analyzed using atos ii system a 3d optical scanning system from gom mbh germany which allowed the fracture aperture field to be quantified the arithmetic and geometric mean apertures were determined from the atos ii data and an equivalent hydraulic aperture was found by performing hydraulic tests with the fracture set up the hydraulic tests involved measuring the head loss from piezometers at the inlet and outlet along with the flow rate to calculate the hydraulic aperture of the fracture using the cubic law waste rock that was generated from cutting the rock block was used to determine the mineral composition matrix porosity and surface charge of the sandstone to determine the mineral and elemental composition of the sandstone x ray diffraction xrd and sem eds analyses were used the porosity was determined from images that were generated using scanning electron microscope backscatter electron sem bse technique on epoxy impregnated polished rock pieces the surface charge of the sandstone was found by using the surpass electrokinetic analyzer anton paar gmbh to find the streaming potential of a crushed sandstone sample 2 2 colloids latex microspheres were used as surrogates for natural colloids two microsphere sizes 200 nm and 1000 nm as well as two surface modifications amine modified and carboxylate modified were employed the 200 nm and 1000 nm carboxylate modified latex cml microspheres from molecular probes invitrogen canada inc are negatively charged at neutral solution ph and were used to simulate an unfavorable attachment condition when paired with the negatively charged sandstone surface the amine modified latex aml microspheres 200 nm from molecular probes invitrogen canada inc and 1000 nm from sigma aldrich canada are positively charged and were used to simulate favorable attachment conditions when paired with the negatively charged sandstone surface the concentration of microspheres in the tracer solution was 6 37 108 and 9 09 106 particles ml 1 for the 200 nm and 1000 nm microspheres respectively 2 3 solution preparation three different solutions were prepared for use in the transport tests a tracer solution containing the microspheres and bromide a flush solution with identical ionic strength and ph to the tracer solution without bromide or microspheres and a low ionic strength flush solution all the solutions were prepared with autoclaved de aired milli q water the cml microsphere tracer and flush solutions were buffered with 1 mm sodium bicarbonate nahco3 at a ph of 8 1 0 2 while the aml microsphere tracer and corresponding flush solutions were buffered with 5 mm bis tris c8h19no5 at a ph of 5 9 0 2 and ionic strength of 4 mm sodium chloride nacl was used to adjust the ionic strength of the solutions to either 5 mm or 10 mm a non reactive tracer 1 mm sodium bromide was added to the aml and cml microsphere tracer solutions and was replaced with 1 mm nacl in the flush solution to keep the ionic strength constant the low ionic strength flush solution for both aml and cml microsphere transport tests contained 1 mm nahco3 without any additional salts added 2 4 colloid properties the colloid particles were analyzed to determine their hydrodynamic diameter and their zeta potential the zeta potential and hydrodynamic diameter of the microspheres were measured using horiba sz 100 nano particle analyzer the hydrodynamic size was reported as the mean diameter for both the cml and aml microspheres a confirmation of the microsphere size was performed by viewing and sizing the microspheres under a scanning electron microscope sem the size and zeta potential were measured under all solution conditions used in the transport experiments the zeta potential of the 200 nm microspheres in solution was also determined before entering and after exiting the fracture to ensure that the microspheres remained positively charged throughout the transport experiment 2 5 experimental set up the fractured sandstone rock block was held in a stainless steel frame and sealed with rubber gaskets to prevent water leakage an inlet chamber was placed at the upstream end of the fracture and 5 outlet ports each with 1 mm diameter near the downstream end of the fracture were connected with tubes to form an outlet manifold there were 5 outlet piezometers and an inlet piezometer for monitoring the hydraulic head difference across the fracture a syringe pump was used to inject the tracer and flush solutions and a fraction collector was used at the combined outlet for collecting samples a detailed description of a similar experimental set up that was used to study colloid transport through fractured dolomite rock can be found in mondal and sleep 2012 the fracture saturation was maintained between experiments by pumping buffered 1 mm nahco3 de aired milli q water with a continuously operating peristaltic pump 2 6 transport experiments transport experiments were run at specific discharges of 0 35 mm s 1 and 0 70 mm s 1 and ionic strengths of 5 mm and 10 mm tracer solution was injected with a syringe pump for approximately 10 pore volume pv calculated based on the equivalent hydraulic aperture followed by a 10 pv injection of flush solution and finally a 5 pv injection of low ionic strength flush solution at an increased flow rate 3 times flow rate used in experiment samples were analyzed using ion chromatography to determine bromide concentration and fluorescent spectrophotometry to determine microsphere concentration and develop breakthrough curves the fracture was flushed with solution of equal ionic strength and ph to the experimental conditions for 24 to 48 h before the transport test was started the favorable attachment tests required 48 h of flushing to allow for the ph of the water exiting the fracture to stabilize at ph 5 9 0 1 the tracer and flush solutions were prepared on the same day as the experiment for unfavorable experiments the ph of water exiting the fracture was 8 1 0 2 for 5 mm ionic strength and 7 7 0 1 for 10 mm ionic strength the time between subsequent experiments was 5 days for the favorable attachment transport tests and 3 4 days for the unfavorable attachment transport tests the unfavorable attachment tests were all completed before beginning the favorable attachment tests to prevent any interference from positively charged microspheres attached to the fracture surface 2 7 transport parameter estimation the bromide and microsphere btcs were fitted using hydrus 1d šimůnek et al 2013 a dual porosity physical non equilibrium model was used to fit the bromide btc the microsphere btcs were simulated using a two site kinetic chemical non equilibrium model the two site kinetic model was fit using two attachment coefficients katt1 and katt2 and two detachment coefficients kdet1 and kdet2 which were determined by parameter estimation in addition to the longitudinal dispersivity for colloid transport under different conditions mobile and immobile porosities within the fracture were also estimated 3 results and discussion 3 1 colloid and collector properties the hydraulic aperture of the fracture was calculated as 311 17 μm based on the cubic law from the atos ii scanning of the fracture surfaces the arithmetic and geometric mean of the aperture were calculated as 210 and 218 μm respectively the xrd analysis revealed five primary mineral phases in the sandstone sample anorthoclase 48 albite 24 quartz 17 kaolinite 9 and magnesium calcite 2 an sem eds point analysis on three sites of interest indicated the presence of a small percentage 1 of iron hematite and titanium rutile that may represent impurities these impurities as well as magnesium calcite may have a positive zeta potential at the ph of the tracer tests because of their high point of zero charge 7 6 9 1 for synthetic rutile and 5 1 6 4 for hematite kosmulski 2009 the overall zeta potential of the sandstone was measured to be between 17 5 mv and 19 5 mv for all solution conditions tested indicating that conditions were overall favorable for aml microspheres and overall unfavorable for cml microspheres the porosity of the sandstone from sem bse analysis was 10 8 2 5 although the presence of micro fractures and fissures on the fracture surface caused by the fracturing process would result in a higher porosity and potential areas for microsphere attachment the zeta potential for the cml microspheres varied between 33 and 74 mv and the zeta potential of the aml microspheres varied between 19 and 51 mv for both aml and cml microspheres the magnitude of the zeta potential was higher for the larger microspheres 1000 nm and decreased with increasing ionic strength the zeta potential of the 200 nm aml microspheres exiting the fracture was between 17 and 22 mv which confirmed that conditions remained favorable throughout the tests the mean diameter of the 200 nm cml microspheres was close to the supplier specified size but the mean diameter of the 200 nm aml microspheres was slightly larger closer to 300 nm than the specified size sem images were used to confirm the supplier specified sizes for all four microspheres scaled sem images not shown of each microsphere showed that the microspheres were close to the supplier specified size and the distribution appeared to be uniform 3 2 bromide transport the mass recovery mr of bromide was between 93 and 97 for all transport tests see fig 1 for 0 35 mm s 1 specific discharge results the peak normalized concentration c co was close to 1 0 with some btc tailing in the time after the pulse the tailing is evidence of diffusive transport into the sandstone matrix and immobile and low aperture areas during the pulse which results in back diffusion into the mobile water during the flush solution injection bromide had a longer mean residence time mrt and a later time of first arrival than the microspheres under all conditions that were tested the faster travel of microspheres in a fracture compared to a conservative solute has been noted by other studies bales et al 1989 champ and schroeter 1988 reimus et al 1994 becker et al 1999 and is due to greater matrix diffusion of bromide and the size exclusion of microspheres from smaller aperture areas of the fracture specific discharge and ionic strength had no identifiable impact on the transport of bromide the bromide btcs were fitted with a dual porosity model in hydrus 1d the hydrus 1d fitted immobile water content of the fracture was determined to be approximately 25 which is due to both the matrix porosity 10 8 by sem bse analysis and stagnant water zones in the fracture 3 3 favorable versus unfavorable attachment transport fig 2 shows a comparison between favorable and unfavorable attachment btcs for both microsphere sizes with 5 mm is and 0 35 mm s 1 specific discharge the corresponding mass recoveries peak normalized concentrations c c0 and residence times for aml and cml microspheres are given in table 1 the plateau concentrations for the 200 nm cml microspheres fig 2 are similar to those observed by mondal and sleep 2013 for 200 nm cml microspheres in a dolomite fracture indicating similar unfavorable condition retention in sandstone and dolomite for similar ionic strength and solution velocity the results are also similar to the results of the experiments on cml microsphere 0 1 1 0 and 4 2 μm transport in fractured till reported in rasmuson et al 2020 rasmuson et al 2020 also found that retention of the cml microspheres in the fractured till was orders of magnitude greater than retention in sand as discussed in the introduction fig 2 shows significant breakthrough of both 200 nm and 1000 nm aml colloids under favorable conditions in contrast a study in a natural chalk fracture by tang and weisbrod 2009 showed that pbco3 colloids were immobile under favorable attachment conditions similar to investigations in porous media the hydraulic aperture of the chalk fracture 139 μm used by tang and weisbrod 2009 was smaller than the sandstone fracture 311 μm used in this investigation which could have led to more colloid collector interaction and decreased mobility in addition to this the pbco3 colloids were larger and denser than the latex microspheres and could have been subject to higher rates of deposition through sedimentation a number of studies in porous media have also found that colloids were significantly less mobile under favorable attachment conditions compared to unfaavorable conditions kretzschmar and sticher 1998 li and johnson 2005 for example tufenkji and elimelech 2004a showed the attachment efficiency increased from 0 019 under unfavorable conditions to 0 68 under favorable conditions for colloid transport in porous media li and johnson 2005 showed that there was a close to a 4 order of magnitude difference in peak normalized concentration between favorable and unfavorable attachment btcs sem analysis of the fracture surfaces in the current study showed significant heterogeneity in mineralogy therefore it is very likely that the fracture surfaces contained zones of positive charge which would produce unfavorable conditions for attachment of the aml microspheres as well as favorable conditions for attachment of the cml microspheres there is also a possibility that the bis tris in the aml buffer could have produced steric repulsion effects thereby reducing aml colloid retention in addition to geochemical heterogeneity and possible steric effects the substantial fracture roughness would affect the interaction energy for transport of both aml and cml microspheres rasmuson et al 2017 2019 showed with impinging jet tests and modeling that with nanoscale roughness decreased attachment compared to smooth surfaces under favorable conditions could be expected due to the effect of nanoscale roughness in increasing the hydrodynamic slip length and reducing the interaction energy increased attachment with roughness and mineralogical variability under net unfavorable conditions would be expected due to reduced repulsion forces and increased points of contact with roughness asperities as well as possibly greater surface charge heterogeneity rasmuson et al 2017 3 4 ionic strength effects the effect of ionic strength on colloid transport under unfavorable attachment conditions has been well established by previous investigations mccarthy et al 2002 mondal and sleep 2012 mondal and sleep 2013 with the trend being an increase in ionic strength leading to an increase in retention through decrease in the primary energy barrier associated with lower electric double layer thickness possibly an increase in the depth of the secondary minimum and decreases in the size of zones of influence of colloids the transport results of the 200 nm and 1000 nm cml microspheres in this investigation fig 3 both follow this trend for unfavorable attachment the mass recovery decreased from 83 to 65 for the 200 nm microspheres and decreased from 77 to 69 for the 1000 nm microspheres when the ionic strength was increased from 5 mm to 10 mm these results are consistent with an increase in ionic strength decreasing the electrical double layer interaction energy thereby decreasing the height of the primary minimum energy barrier and increasing the depth of the secondary minimum effects that might be more significant when combined with the roughness of the fracture surfaces an additional impact of increased ionic strength arises in the presence of surface charge heterogeneity with surface charge heterogeneity conditions for colloid attachment can become favorable when a colloid encounters a surface region where the net surface charge in its zone of influence is attractive i e the opposite of the colloid surface charge increasing ionic strength decreases the radius of the zone of influence of colloids thereby increasing at the lower size end the range of regions of opposite surface charge that can attract the colloids the overall effect of this process can be an increase in colloid retention pazmino et al 2014a ron et al 2019 the potential for release of colloids was investigated by injecting a low ionic strength flush solution 1 mm after the initial flush at three times the original flow rate a small peak in concentration was observed for the 200 nm btc fig 4 the 1000 nm microspheres did not have the same peak in concentration at the low ionic strength and the number of 200 nm microspheres released was not high enough to account for the decrease in recovery at the higher ionic strength condition 10 mm the implication of this is that a significant fraction of the microspheres may have been irreversibly attached either on zones of net positive surface charge associated with surface heterogeneity or jumps from secondary minimum attachment to primary attachment or due to both of these mechanisms kuznar and elimelech 2007 observed an incomplete release of particles with a low ionic strength flush and cml microspheres trapped in the secondary minimum translating along the surface of glass beads and becoming held in regions of stagnant flow at the rear of the collector they also speculated that microspheres initially trapped in the secondary minimum could become attached in the primary minimum due to protruding polyelectrolytes from the cml surface and local asperities on the glass beads being able to reach through the primary energy barrier and form contact kuznar and elimelech 2007 it is possible that a similar scenario was occurring in the sandstone fracture the initial 10 mm ionic strength causing more retention in the secondary minimum and microspheres translating over the rough fracture surfaces moving to the primary minimum and becoming irreversibly attached possibly in stagnant small aperture regions microspheres may be able to make the jump from the secondary minimum to the primary minimum because of the shorter separation distance of the secondary minimum from the fracture surface at higher ionic strength the roughness of the sandstone surface and protruding polyelectrolytes from the cml microsphere surface given the heterogeneity of the fracture surface mineralogy and the observed retention of cml microspheres at 10 mm is it is likely that a fraction of the microspheres were deposited on regions that had a net positive charge over the zones of influence of the microspheres reducing the ionic strength would increase the size of the colloidal zone of influence possibly leading to lower retention of microspheres in the smallest regions of net positive charge if the net charge in the increased zone of influence became negative pazmino et al 2014a rasmuson et al 2019 due to the expansion of the zone of influence to include more negatively charged surface area the small amount of 200 nm and 1000 nm microspheres released during the low is flush indicates that if there was a zone of influence effect only a small proportion of microspheres attached to positively charged regions were impacted by the change for both the 200 nm and 1000 nm cml microspheres the effect of increasing the ionic strength of the tracer solution for favorable conditions differed from the effect under unfavorable conditions the mass recovery and btc did not vary significantly for either particle size when the ionic strength was increased under favorable attachment conditions results not shown this result is consistent with the attachment occurring in the primary minimum as no secondary minimum or energy barrier exists under favorable conditions an investigation in porous media by elimelech 1991 showed that the deposition rate of colloids under favorable attachment conditions increased when the ionic strength was reduced in contrast to the trends observed for unfavorable attachment the primary impact that increasing ionic strength has under favorable attachment conditions is to decrease the zeta potential of the microspheres and reduce the electrostatic attraction between the colloid and collector an increased recovery at higher ionic strength was not observed in this investigation but it is possible that the increase in ionic strength was not large enough to produce this effect 3 5 particle size effects for favorable conditions the peak btc concentration was slightly greater for the 1000 nm aml colloids compared to the 200 nm aml colloids this is consistent with the expected trend in the single collector contact efficiency which decreases as particle size increases from 200 nm to 1 μm tufenkji and elimelech 2004b ron et al 2019 ron and johnson 2020 this trend is due to a greater rate of decrease of diffusion related colloid collector contact compared to the rates of increase of contact due to interception and gravity effects for unfavorable conditions at 10 mm is and 0 35 mm s 1 specific discharge the peak concentrations table 1 fig 3 were higher for the 1000 nm cml microspheres than for the 200 nm cml microspheres this is consistent with the study of cml microsphere transport in dolomite fractures at 3 mm is mondal and sleep 2012 in which the peak c c0 values were higher for 500 nm microspheres than for 200 nm microspheres the same trend of increasing c c0 i e lower retention with increasing microsphere diameter between 100 and 1000 nm in a fractured till 3 7 mm ionic strength was observed by rasmuson et al 2020 in impinging jet experiments and simulations ron et al 2019 2020 found decreasing collector efficiency with increasing colloid size in the range from 100 nm to 1000 nm was also ron et al 2019 2020 indicating an expectation of higher peak concentrations for the 1000 nm cml microspheres compared to the 200 nm cml microspheres while the initial breakthrough concentrations for the 200 and 1000 nm cml microspheres were similar at 5 mm ionic strength the effluent concentration for the 200 nm cml microspheres continued to slowly increase with pore volume and at a higher rate of concentration increase than observed for the 1000 nm cml colloids such that the final 200 nm cml microsphere c c0 value was 0 86 compared to 0 78 for the 1000 nm cml microspheres the reason for this deviation from the expected trend of increasing peak concentration and mass recovery with increasing colloid size over the range from 200 to 1000 nm under unfavorable conditions is unclear given the increasing effluent concentration in time for the 200 nm microspheres and the higher number of 200 nm microspheres injected compared to the 1000 nm microspheres there was perhaps an increasing degree of saturation of net positive charge regions of a size consistent with the zone of influence of the 200 nm microspheres at 5 mm is in the period following the colloid addition at both 5 mm and 10 mm ionic strength fig 3 there was more tailing of concentrations of the 200 nm cml microspheres compared to the 1000 nm cml microspheres this larger tailing of smaller microspheres consistent with the results of rasmuson et al 2020 may have been due to continued translation of the 200 nm microspheres along the fracture surface in the secondary minimum zone or shearing of microspheres from localized zones of attachment given the range of aperture sizes and water velocities there may have also been more diffusion of the 200 nm cml microspheres into and then out of low aperture low water velocity fracture regions compared to the 1000 nm microspheres resulting in greater tailing for the 200 nm microspheres the difference between favorable and unfavorable conditions based on the peak concentrations in the breakthrough curves fig 2a and mass recoveries is largest for the 200 nm microspheres at 5 mm is at 10 mm is the differences between favorable and unfavorable conditions are similar for the 200 and 1000 nm microspheres and are smaller than the differences for both microsphere sizes at 5 mm is these results indicate that differences between favorable and unfavorable conditions were affected more by changes in is from 5 mm to 10 mm than by a change in colloid size from 200 nm to 1000 nm studies have suggested that an optimal particle size can be determined for transport through a fractured system with 500 nm commonly being determined to have the highest mobility cumbie and mckay 1999 mccarthy et al 2002 the optimal particle size could not be determined experimentally for this study because only two particle sizes were tested although the optimal particle size could not be determined one clear impact of particle size that was observed from the transport tests is that the larger microspheres were not as strongly affected by changing the test conditions changing the specific discharge ionic strength and switching from anionic to cationic microspheres did not result in any major changes in the btc for the larger microspheres 1000 nm the larger microspheres also had a shorter mean residence time table 1 for both favorable and unfavorable attachment both the shorter mean residence time and more consistent btc can be explained by the size exclusion effect lower diffusion coefficient and reduced brownian motion of the larger microspheres the larger microspheres move through the fracture faster because they are excluded from slow moving low aperture regions and diffuse into slower flow paths less frequently than the smaller microspheres james and chrysikopoulos 2003 and james et al 2018 have also noted that the effective velocity of larger colloids will be higher because the finite size of a colloid will exclude it from the slowest moving portion of the parabolic velocity profile nearest the wall the faster moving larger colloids have less opportunity to attach in the primary or secondary minimum when they are further from the fracture walls and are therefore less affected by changing test conditions under favorable attachment conditions recovery of the 1000 nm microspheres was higher than the recovery of the 200 nm microspheres for all ionic strength and specific discharge conditions despite the larger microspheres having a more positive zeta potential higher electrostatic attractive forces not resulting in more retention is further evidence that the size of the colloid and the tendency to stay in the bulk flow region is particularly important to the mobility of colloids under favorable attachment conditions 3 6 specific discharge effects studies have shown that retention generally decreases with increasing specific discharge albarran et al 2013 mondal and sleep 2013 although this is not true in all cases as noted by rodrigues et al 2013 the decrease in retention is attributed to higher fluid drag at higher velocities which impacts the balance of mobilizing and arresting torques given this it would be expected that the impact of an increase in specific discharge would be greater for larger colloids tong and johnson 2006 in the current study a doubling of specific discharge at 5 mm is increased mass recovery by 3 3 for 200 nm aml colloids and by 3 7 for 1000 nm colloids there was less than a 1 increase in mass recovery for the 200 nm and 1000 nm cml colloids with a doubling of specific discharge at 5 mm is at 10 mm is there was less than 1 change in mass recovery of 1000 nm cml colloids while there was an increase in mass recovery of 9 4 for the 200 nm cml colloids at higher is the secondary minimum would be deeper and the zone of influence of the microspheres would be lower under these conditions the higher water velocity could result in more cml microspheres in the secondary minimum reaching the fracture outlet there would also be reduced time for the cml microspheres to interact with the localized zones of net positive charge 3 7 generation of favorable attachment sites positively charged attachment sites can have a major impact on transport of negatively charged colloids even if the zeta potential results suggest the overall conditions are still unfavorable this phenomenon was noted by elimelech et al 2000 when they discovered that colloid deposition kinetics were controlled by the degree of patch wise chemical heterogeneity and not by the measured zeta potential of the porous media liang et al 2021 observed that deposition of silver nanoparticles was reduced by the presence of attached soil colloids and that the nanoparticles were more likely to be released with reductions in ionic strength due to soil colloids creating convex locations on soil grains in the current study performing favorable attachment tracer tests resulted in cationic aml microspheres being attached to the sandstone surface which created positively charged impurities on the overall anionic sandstone surface and potentially also changed the surface morphology after the favorable attachment experiments were completed an unfavorable attachment tracer test was run to analyze the impact of attached aml microspheres on cml microsphere transport the 200 nm btcs in fig 5 show that the transport results of cml microspheres closely resemble favorable attachment in the presence of attached cationic microspheres a similar effect was found for the larger 1000 nm microspheres result not shown the increased tailing and spike in concentration after the ionic strength was reduced both indicated that the overall conditions were still unfavorable despite the significant increase in retention the attached cationic microspheres did not have an observable impact on favorable attachment transport tests the assumption that the overall zeta potential of the collector can determine the transport behavior of colloids through fractured media may lead to poor predictions in the presence of surface charge heterogeneities caused by mineral heterogeneity and roughness or due to attachment of other colloids that change local zeta potentials and surface morphology by running multiple tracer tests with 200 nm cml microspheres the favorable attachment sites were eventually filled and blocked for further attachment after the favorable attachment sites were blocked the mass recovery increased back to what it was for previous unfavorable transport tests it should be noted that all the cml microsphere tracer tests were performed prior to the aml microsphere tests to avoid this interference 3 8 estimated transport parameters modeling with hydrus 1d was used to identify the changes in parameters for different transport conditions the btcs were fit using a two site kinetic non equilibrium model which incorporated attachment and detachment figs 2 and 3 present the observed and model fitted microsphere btcs under favorable and unfavorable attachment conditions with the fitted parameters listed in table 2 a good fit was found for all microsphere btcs r2 98 5 by assigning site 1 with a very low detachment coefficient kdet1 of 1 0 10 7 to 1 0 10 10 s 1 consistent with mostly irreversible attachment and site 2 with a higher detachment coefficient kdet2 of 1 0 10 3 to 3 0 10 3 s 1 consistent with a significant proportion of reversible attachment sites having reversible and irreversible attachments site can simulate with the existence of primary and secondary minima for unfavorable attachment conditions the difference between fits for favorable and unfavorable attachment was primarily related to the magnitude of the two attachment coefficients for favorable attachment katt1 irreversible attachment site was higher and katt2 reversible attachment site was lower than for unfavorable attachment this is logical given that irreversible attachment should be a more significant retention mechanism under favorable conditions the higher value of kdet1 irreversible attachment site for the 200 nm microspheres under unfavorable conditions provided the fit to the higher tailing measured for this case compared to other cases it is possible that the higher detachment coefficient for site 1 accounts for the slow release of microspheres back into the higher velocity zones from small aperture low velocity regions contributing to tailing an increase in ionic strength under unfavorable attachment conditions resulted in a higher value of katt1 irreversible attachment site this is consistent with higher ionic strength leading to more microspheres that were initially attached in the secondary minimum jumping into the primary minimum and becoming irreversibly attached including the effect of blocking did not significantly improve the fit of the btcs a similar two site kinetic was used by mondal and sleep 2012 to simulate colloid transport under unfavorable attachment conditions in dolomite rock fractures 4 conclusions the comparison between favorable and unfavorable attachment in this study revealed a small difference in mobility between microspheres under favorable and unfavorable attachment conditions in fractured media the mobility of cationic colloids favorable conditions for attachment appeared to be dependent on the opportunity for interaction with the fracture surfaces which would be influenced by factors such as colloid size fracture aperture surface roughness and specific discharge the results of this study indicate that it should not be assumed in all scenarios that colloids in fractures will be close to immobile if attachment conditions are net favorable increasing ionic strength significantly decreased the recovery of microspheres under unfavorable attachment assumed to be due to the effect on the depth of the secondary minimum but did not affect the mass recovery under favorable attachment conditions the small release of microspheres with a low ionic strength flush under unfavorable attachment conditions indicated that many of the retained microspheres were irreversibly attached this could be attributed to the effects of the mineralogical heterogeneity of the fracture surfaces creating local domains for favorable attachment and the effects of fracture surface roughness in lowering energy barriers to deposition in primary minima deposition and recovery of microspheres under all but the highest ionic strength with 200 nm microspheres were not very sensitive to specific discharge prior transport and attachment of cationic microspheres can increase the attachment of anionic microspheres in subsequent transport of anionic microspheres declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was funded by the natural sciences and engineering research council of canada discovery grant rgpin 2017 06504 held by b sleep additional data can be found in the masc thesis of s spanik https tspace library utoronto ca bitstream 1807 79153 3 spanik sean p 201711 mas thesis pdf 
1382,the transport of cationic amine modified latex aml and anionic carboxylate modified latex cml microspheres through a discrete sandstone fracture with mineralogical heterogeneity and roughness was studied two microsphere sizes 200 nm and 1000 nm two ionic strengths 5 mm and 10 mm and two specific discharges 0 35 mm s 1 and 0 70 mm s 1 were tested to observe the impact on transport under favorable and unfavorable conditions the difference in retention between aml net favorable and cml net unfavorable microsphere attachment was 25 for the 200 nm microspheres and 13 for the 1000 nm microspheres less than 50 of the aml microspheres were retained in the fracture postulated to be due to the effects of mineralogical heterogeneity and fracture surface roughness the effect of an increase in ionic strength in increasing retention was significant for unfavorable attachment but was not significant for favorable attachment conditions the effect of specific discharge was minor for all but the 200 nm cml microspheres at 10 mm ionic strength when flushing the fracture first with cationic microspheres then with anionic microspheres the recovery of anionic microspheres resembled favorable attachment presumably due to interaction with cationic microspheres that remained attached to the sandstone surface colloid breakthrough curves could be fit well with a two site attachment model with reversible and irreversible sites keywords colloids sandstone fracture unfavorable attachment favorable attachment 1 introduction colloid transport in the subsurface is a well studied subject in the literature because of the impact different types of colloids can have on the environment and human health from the movement of pathogenic biocolloids bacteria and viruses to colloid mediated radionuclide transport in the subsurface to application of remediation agents e g nano particles and bio augmentation agents improving the understanding of colloid transport is important interest in recent subsurface remediation techniques such as the use of nano scale zero valent iron nzvi has also increased the focus on studying colloid transport kocur et al 2013 kocur et al 2014 johnson et al 2013 numerous studies related to colloid transport in porous media have been conducted and recently there has been increased interest in transport through fractures which can be preferential pathways for contaminant migration fractures create preferential pathways for colloids because they can have high velocity flows diffusion of colloids into the matrix will be low and the attachment of colloids to fracture walls may be limited especially in unfavorable attachment conditions zvikelsky and weisbrod 2006 lab studies mccarthy et al 2002 albarran et al 2013 alaskar et al 2015 stoll et al 2016 field studies becker et al 2003 masciopinto and chrysikopoulos 2008 zhang et al 2015 and modeling abdel salam and chrysikopoulos 1994 zheng et al 2009 james and chrysikopoulos 2011 james et al 2018 and literature reviews zhang et al 2012 of colloid transport through fractures have been increasing in recent years but it is still a subject that requires further investigation the surface charges of the collector fractured rock or porous media and the colloid are key factors in colloid transport unfavorable attachment conditions exist when the colloid and collector have like charges and electric double layer interactions create an energy barrier to attachment favorable attachment conditions exist when the colloid and collector have opposite charges creating an electrostatic attractive force in the subsurface environment unfavorable attachment conditions are more common because most colloids and collectors of environmental interest bacteria viruses clays sandstone etc have a negative surface charge at typical ph values of groundwater being more common in the environment and being more difficult to predict than favorable attachment has resulted in most colloid transport studies being focused on unfavorable conditions although less common in nature favorable attachment conditions can develop in low ph or high ionic strength environments the existence of positively charged minerals such as titanium oxide iron oxide magnesium oxide and magnesium calcite which have points of zero charge pzc at near neutral or higher ph elimelech et al 1995 can also lead to favorable attachment conditions there have been favorable attachment focused studies in porous media elimelech 1991 elimelech 1994 but in fractures the subject has not been well explored while unfavorable attachment conditions associated with negatively charged colloids and negatively charged mineral surfaces are common colloid retention under unfavorable conditions in both porous media and fractured media has been widely observed tufenkji and elimelech 2004a johnson and tong 2006 mondal and sleep 2012 this retention has been attributed to various mechanisms colloid retention under unfavorable conditions has often been attributed to retention in the secondary minimum of the derjaguin landau verwey and overbeek dlvo energy profile colloids in the secondary minimum zone are subject to hydrodynamic drag from fluid flow and may continue to translate along solid surfaces in the direction of fluid flow unless or until they end up in stagnant flow zones redman et al 2004 kuznar and elimelech 2007 johnson et al 2007 while the role of stagnant flow zones has been investigated for porous media this has not been studied for colloid transport in fractures rasmuson et al 2020 showed in lab studies that colloid retention in fractured till was lower by orders of magnitude than colloid retention in sand accompanying simulations of colloid transport in idealized porous media and smooth parallel plate fractures indicated that the difference in retention could be explained by the impact of flow impingement on forward flow stagnation zones in porous media a phenomenon not present in the smooth parallel plate simulations in natural rough fractures the relative importance of stagnant flow zones in fractures would depend on the fracture roughness the role of recirculation zones briggs et al 2017 and the extent of contact points between fracture surfaces that could produce stagnant zones as well as colloid retention due to wedging johnson et al 2007 previous studies of colloid transport in fractures under unfavorable conditions for example mondal and sleep 2012 mondal and sleep 2013 rasmuson et al 2020 would suggest that these impacts are much less important in fractures compared to porous media geochemical heterogeneity of collector surfaces where nanoscale positively charged zones exist on surfaces that are overall negatively charged has been shown to lead to colloid attachment to these zones pazmino et al 2014a 2014b nanoscale surface roughness has also been theoretically shown to reduce repulsion as roughness decreases the radius of curvature of the collector surface reducing edl and van der waals interactions as they decrease with decreasing radius of curvature israelachvili 2011 bendersky and davis 2011 roughness can also promote attachment by increasing potential points of colloid collector contact jin et al 2015 rasmuson et al 2017 however the impact of roughness in colloid retention and attachment is complex studies have shown that nanoscale roughness would be expected to decrease retention in the secondary minimum and enhance attachment in the primary minimum but with weaker primary minimum attachment bradford and torkzaban 2015 torkzaban and bradford 2016 bradford et al 2017 in contrast microscale roughness was expected to increase colloid retention through decreases in lever arms related to hydrodynamic torque and increases in lever arms related to adhesive torque bradford and torkzaban 2015 torkzaban and bradford 2016 increased spatial variability in mineralogy and roughness and the associated spatial variability in collector efficiencies or attachment coefficients can increase the spreading of a colloid plume and enhance transport katzourakis and chrysikopoulos 2017 while roughness can enhance colloid attachment under unfavorable conditions it may also decrease attachment under favorable conditions as roughness can decrease repulsion under unfavorable conditions it can also decrease attraction under favorable conditions rasmuson et al 2017 rasmuson et al 2017 studied microsphere attachment to glass slides of varying roughness with an impinging jet system for colloids in the range of 0 25 to 2 μm they found two orders of magnitude difference in collector efficiencies between favorable and unfavorable conditions on slides with less than 1 nm roughness collector efficiencies for 20 nm and 4 4 μm colloids were similar for both favorable and unfavorable conditions for the 1 nm roughness with roughness of 38 nm and 546 nm collector efficiencies for favorable conditions decreased relative to the 1 nm roughness efficiencies this was attributed to dlvo effects reduced radius of surface curvature and increased hydrodynamic slip length and near surface velocities over rougher surfaces for unfavorable conditions rasmuson et al 2017 found that collector efficiencies were increased due to the lower repulsive forces associated with smaller radius of surface curvature and due to greater adhesion associated with contact with multiple asperities with the decrease in attachment under favorable conditions and increase in attachment under unfavorable conditions retention of colloids was similar for both conditions for roughnesses of 38 nm and 546 nm no significant difference was found between results with 38 nm roughness and 546 nm roughness colloid transport across a glass slide may have similarities to transport along fracture surfaces although with much greater roughness perhaps more mineralogical heterogeneity with some contact points between fracture surfaces consequently it is hypothesized that colloid transport in fractures under favorable and unfavorable conditions would have more similar retention than would typically occur in porous media with lower retention than expected in porous media particularly for favorable condtions in addition to retention and attachment the fate of colloids in the subsurface can be impacted by changes in porewater geochemistry which may lead to detachment and reentrainment retention in the secondary minimum is reversible and a reduction in ionic strength which reduces the depth of the secondary minimum has been postulated as the reason for re entrainment of colloids in porous media franchi and o melia 2003 hahn et al 2004 redman et al 2004 and fractured dolomite mondal and sleep 2012 with reductions in ionic strength pazmino et al 2014b showed through modeling that steric repulsion coupled with discrete zones of chemical heterogeneity could lead to conditions under which perturbations in velocity or ionic strength could cause release from primary attachment with weaker primary minimum attachment in the presence of nanoscale roughness torkzaban and bradford 2016 postulated from model predictions that reductions in ionic strength could lead to colloid detachment through hydrodynamic shear or diffusion shen et al 2018 showed that primary attachment to nanoscale convex asperities was reversible with reductions in ionic strength while primary attachment in nanoscale convex asperities was irreversible due to an increase in primary minimum depth with a decrease in ionic strength rock fracture surfaces are likely to exhibit both nanoscale and microscale roughness and chemical heterogeneity and transport of colloids in rock fractures under favorable and unfavorable conditions would be expected to be a complex function of the interacting effects of these features as sandstone aquifers are common sources for drinking water the transport of colloids in sandstone fractures is of particular interest in this study transport experiments were conducted in a lab scale sandstone single fracture using amine and carboxylate modified latex microspheres to investigate colloid transport for both favorable and unfavorable attachment conditions experimental variables included solution ionic strength specific discharge and colloid size 2 materials and methods 2 1 sandstone fracture a block of sandstone originating from a quarry in northern china was acquired from a building materials supplier in toronto ontario and was cut to 280 210 70 mm in size a single fracture along the longer directions was artificially created in lab and the fractured sandstone block was used in the transport experiments the top and bottom fracture wall surfaces were analyzed using atos ii system a 3d optical scanning system from gom mbh germany which allowed the fracture aperture field to be quantified the arithmetic and geometric mean apertures were determined from the atos ii data and an equivalent hydraulic aperture was found by performing hydraulic tests with the fracture set up the hydraulic tests involved measuring the head loss from piezometers at the inlet and outlet along with the flow rate to calculate the hydraulic aperture of the fracture using the cubic law waste rock that was generated from cutting the rock block was used to determine the mineral composition matrix porosity and surface charge of the sandstone to determine the mineral and elemental composition of the sandstone x ray diffraction xrd and sem eds analyses were used the porosity was determined from images that were generated using scanning electron microscope backscatter electron sem bse technique on epoxy impregnated polished rock pieces the surface charge of the sandstone was found by using the surpass electrokinetic analyzer anton paar gmbh to find the streaming potential of a crushed sandstone sample 2 2 colloids latex microspheres were used as surrogates for natural colloids two microsphere sizes 200 nm and 1000 nm as well as two surface modifications amine modified and carboxylate modified were employed the 200 nm and 1000 nm carboxylate modified latex cml microspheres from molecular probes invitrogen canada inc are negatively charged at neutral solution ph and were used to simulate an unfavorable attachment condition when paired with the negatively charged sandstone surface the amine modified latex aml microspheres 200 nm from molecular probes invitrogen canada inc and 1000 nm from sigma aldrich canada are positively charged and were used to simulate favorable attachment conditions when paired with the negatively charged sandstone surface the concentration of microspheres in the tracer solution was 6 37 108 and 9 09 106 particles ml 1 for the 200 nm and 1000 nm microspheres respectively 2 3 solution preparation three different solutions were prepared for use in the transport tests a tracer solution containing the microspheres and bromide a flush solution with identical ionic strength and ph to the tracer solution without bromide or microspheres and a low ionic strength flush solution all the solutions were prepared with autoclaved de aired milli q water the cml microsphere tracer and flush solutions were buffered with 1 mm sodium bicarbonate nahco3 at a ph of 8 1 0 2 while the aml microsphere tracer and corresponding flush solutions were buffered with 5 mm bis tris c8h19no5 at a ph of 5 9 0 2 and ionic strength of 4 mm sodium chloride nacl was used to adjust the ionic strength of the solutions to either 5 mm or 10 mm a non reactive tracer 1 mm sodium bromide was added to the aml and cml microsphere tracer solutions and was replaced with 1 mm nacl in the flush solution to keep the ionic strength constant the low ionic strength flush solution for both aml and cml microsphere transport tests contained 1 mm nahco3 without any additional salts added 2 4 colloid properties the colloid particles were analyzed to determine their hydrodynamic diameter and their zeta potential the zeta potential and hydrodynamic diameter of the microspheres were measured using horiba sz 100 nano particle analyzer the hydrodynamic size was reported as the mean diameter for both the cml and aml microspheres a confirmation of the microsphere size was performed by viewing and sizing the microspheres under a scanning electron microscope sem the size and zeta potential were measured under all solution conditions used in the transport experiments the zeta potential of the 200 nm microspheres in solution was also determined before entering and after exiting the fracture to ensure that the microspheres remained positively charged throughout the transport experiment 2 5 experimental set up the fractured sandstone rock block was held in a stainless steel frame and sealed with rubber gaskets to prevent water leakage an inlet chamber was placed at the upstream end of the fracture and 5 outlet ports each with 1 mm diameter near the downstream end of the fracture were connected with tubes to form an outlet manifold there were 5 outlet piezometers and an inlet piezometer for monitoring the hydraulic head difference across the fracture a syringe pump was used to inject the tracer and flush solutions and a fraction collector was used at the combined outlet for collecting samples a detailed description of a similar experimental set up that was used to study colloid transport through fractured dolomite rock can be found in mondal and sleep 2012 the fracture saturation was maintained between experiments by pumping buffered 1 mm nahco3 de aired milli q water with a continuously operating peristaltic pump 2 6 transport experiments transport experiments were run at specific discharges of 0 35 mm s 1 and 0 70 mm s 1 and ionic strengths of 5 mm and 10 mm tracer solution was injected with a syringe pump for approximately 10 pore volume pv calculated based on the equivalent hydraulic aperture followed by a 10 pv injection of flush solution and finally a 5 pv injection of low ionic strength flush solution at an increased flow rate 3 times flow rate used in experiment samples were analyzed using ion chromatography to determine bromide concentration and fluorescent spectrophotometry to determine microsphere concentration and develop breakthrough curves the fracture was flushed with solution of equal ionic strength and ph to the experimental conditions for 24 to 48 h before the transport test was started the favorable attachment tests required 48 h of flushing to allow for the ph of the water exiting the fracture to stabilize at ph 5 9 0 1 the tracer and flush solutions were prepared on the same day as the experiment for unfavorable experiments the ph of water exiting the fracture was 8 1 0 2 for 5 mm ionic strength and 7 7 0 1 for 10 mm ionic strength the time between subsequent experiments was 5 days for the favorable attachment transport tests and 3 4 days for the unfavorable attachment transport tests the unfavorable attachment tests were all completed before beginning the favorable attachment tests to prevent any interference from positively charged microspheres attached to the fracture surface 2 7 transport parameter estimation the bromide and microsphere btcs were fitted using hydrus 1d šimůnek et al 2013 a dual porosity physical non equilibrium model was used to fit the bromide btc the microsphere btcs were simulated using a two site kinetic chemical non equilibrium model the two site kinetic model was fit using two attachment coefficients katt1 and katt2 and two detachment coefficients kdet1 and kdet2 which were determined by parameter estimation in addition to the longitudinal dispersivity for colloid transport under different conditions mobile and immobile porosities within the fracture were also estimated 3 results and discussion 3 1 colloid and collector properties the hydraulic aperture of the fracture was calculated as 311 17 μm based on the cubic law from the atos ii scanning of the fracture surfaces the arithmetic and geometric mean of the aperture were calculated as 210 and 218 μm respectively the xrd analysis revealed five primary mineral phases in the sandstone sample anorthoclase 48 albite 24 quartz 17 kaolinite 9 and magnesium calcite 2 an sem eds point analysis on three sites of interest indicated the presence of a small percentage 1 of iron hematite and titanium rutile that may represent impurities these impurities as well as magnesium calcite may have a positive zeta potential at the ph of the tracer tests because of their high point of zero charge 7 6 9 1 for synthetic rutile and 5 1 6 4 for hematite kosmulski 2009 the overall zeta potential of the sandstone was measured to be between 17 5 mv and 19 5 mv for all solution conditions tested indicating that conditions were overall favorable for aml microspheres and overall unfavorable for cml microspheres the porosity of the sandstone from sem bse analysis was 10 8 2 5 although the presence of micro fractures and fissures on the fracture surface caused by the fracturing process would result in a higher porosity and potential areas for microsphere attachment the zeta potential for the cml microspheres varied between 33 and 74 mv and the zeta potential of the aml microspheres varied between 19 and 51 mv for both aml and cml microspheres the magnitude of the zeta potential was higher for the larger microspheres 1000 nm and decreased with increasing ionic strength the zeta potential of the 200 nm aml microspheres exiting the fracture was between 17 and 22 mv which confirmed that conditions remained favorable throughout the tests the mean diameter of the 200 nm cml microspheres was close to the supplier specified size but the mean diameter of the 200 nm aml microspheres was slightly larger closer to 300 nm than the specified size sem images were used to confirm the supplier specified sizes for all four microspheres scaled sem images not shown of each microsphere showed that the microspheres were close to the supplier specified size and the distribution appeared to be uniform 3 2 bromide transport the mass recovery mr of bromide was between 93 and 97 for all transport tests see fig 1 for 0 35 mm s 1 specific discharge results the peak normalized concentration c co was close to 1 0 with some btc tailing in the time after the pulse the tailing is evidence of diffusive transport into the sandstone matrix and immobile and low aperture areas during the pulse which results in back diffusion into the mobile water during the flush solution injection bromide had a longer mean residence time mrt and a later time of first arrival than the microspheres under all conditions that were tested the faster travel of microspheres in a fracture compared to a conservative solute has been noted by other studies bales et al 1989 champ and schroeter 1988 reimus et al 1994 becker et al 1999 and is due to greater matrix diffusion of bromide and the size exclusion of microspheres from smaller aperture areas of the fracture specific discharge and ionic strength had no identifiable impact on the transport of bromide the bromide btcs were fitted with a dual porosity model in hydrus 1d the hydrus 1d fitted immobile water content of the fracture was determined to be approximately 25 which is due to both the matrix porosity 10 8 by sem bse analysis and stagnant water zones in the fracture 3 3 favorable versus unfavorable attachment transport fig 2 shows a comparison between favorable and unfavorable attachment btcs for both microsphere sizes with 5 mm is and 0 35 mm s 1 specific discharge the corresponding mass recoveries peak normalized concentrations c c0 and residence times for aml and cml microspheres are given in table 1 the plateau concentrations for the 200 nm cml microspheres fig 2 are similar to those observed by mondal and sleep 2013 for 200 nm cml microspheres in a dolomite fracture indicating similar unfavorable condition retention in sandstone and dolomite for similar ionic strength and solution velocity the results are also similar to the results of the experiments on cml microsphere 0 1 1 0 and 4 2 μm transport in fractured till reported in rasmuson et al 2020 rasmuson et al 2020 also found that retention of the cml microspheres in the fractured till was orders of magnitude greater than retention in sand as discussed in the introduction fig 2 shows significant breakthrough of both 200 nm and 1000 nm aml colloids under favorable conditions in contrast a study in a natural chalk fracture by tang and weisbrod 2009 showed that pbco3 colloids were immobile under favorable attachment conditions similar to investigations in porous media the hydraulic aperture of the chalk fracture 139 μm used by tang and weisbrod 2009 was smaller than the sandstone fracture 311 μm used in this investigation which could have led to more colloid collector interaction and decreased mobility in addition to this the pbco3 colloids were larger and denser than the latex microspheres and could have been subject to higher rates of deposition through sedimentation a number of studies in porous media have also found that colloids were significantly less mobile under favorable attachment conditions compared to unfaavorable conditions kretzschmar and sticher 1998 li and johnson 2005 for example tufenkji and elimelech 2004a showed the attachment efficiency increased from 0 019 under unfavorable conditions to 0 68 under favorable conditions for colloid transport in porous media li and johnson 2005 showed that there was a close to a 4 order of magnitude difference in peak normalized concentration between favorable and unfavorable attachment btcs sem analysis of the fracture surfaces in the current study showed significant heterogeneity in mineralogy therefore it is very likely that the fracture surfaces contained zones of positive charge which would produce unfavorable conditions for attachment of the aml microspheres as well as favorable conditions for attachment of the cml microspheres there is also a possibility that the bis tris in the aml buffer could have produced steric repulsion effects thereby reducing aml colloid retention in addition to geochemical heterogeneity and possible steric effects the substantial fracture roughness would affect the interaction energy for transport of both aml and cml microspheres rasmuson et al 2017 2019 showed with impinging jet tests and modeling that with nanoscale roughness decreased attachment compared to smooth surfaces under favorable conditions could be expected due to the effect of nanoscale roughness in increasing the hydrodynamic slip length and reducing the interaction energy increased attachment with roughness and mineralogical variability under net unfavorable conditions would be expected due to reduced repulsion forces and increased points of contact with roughness asperities as well as possibly greater surface charge heterogeneity rasmuson et al 2017 3 4 ionic strength effects the effect of ionic strength on colloid transport under unfavorable attachment conditions has been well established by previous investigations mccarthy et al 2002 mondal and sleep 2012 mondal and sleep 2013 with the trend being an increase in ionic strength leading to an increase in retention through decrease in the primary energy barrier associated with lower electric double layer thickness possibly an increase in the depth of the secondary minimum and decreases in the size of zones of influence of colloids the transport results of the 200 nm and 1000 nm cml microspheres in this investigation fig 3 both follow this trend for unfavorable attachment the mass recovery decreased from 83 to 65 for the 200 nm microspheres and decreased from 77 to 69 for the 1000 nm microspheres when the ionic strength was increased from 5 mm to 10 mm these results are consistent with an increase in ionic strength decreasing the electrical double layer interaction energy thereby decreasing the height of the primary minimum energy barrier and increasing the depth of the secondary minimum effects that might be more significant when combined with the roughness of the fracture surfaces an additional impact of increased ionic strength arises in the presence of surface charge heterogeneity with surface charge heterogeneity conditions for colloid attachment can become favorable when a colloid encounters a surface region where the net surface charge in its zone of influence is attractive i e the opposite of the colloid surface charge increasing ionic strength decreases the radius of the zone of influence of colloids thereby increasing at the lower size end the range of regions of opposite surface charge that can attract the colloids the overall effect of this process can be an increase in colloid retention pazmino et al 2014a ron et al 2019 the potential for release of colloids was investigated by injecting a low ionic strength flush solution 1 mm after the initial flush at three times the original flow rate a small peak in concentration was observed for the 200 nm btc fig 4 the 1000 nm microspheres did not have the same peak in concentration at the low ionic strength and the number of 200 nm microspheres released was not high enough to account for the decrease in recovery at the higher ionic strength condition 10 mm the implication of this is that a significant fraction of the microspheres may have been irreversibly attached either on zones of net positive surface charge associated with surface heterogeneity or jumps from secondary minimum attachment to primary attachment or due to both of these mechanisms kuznar and elimelech 2007 observed an incomplete release of particles with a low ionic strength flush and cml microspheres trapped in the secondary minimum translating along the surface of glass beads and becoming held in regions of stagnant flow at the rear of the collector they also speculated that microspheres initially trapped in the secondary minimum could become attached in the primary minimum due to protruding polyelectrolytes from the cml surface and local asperities on the glass beads being able to reach through the primary energy barrier and form contact kuznar and elimelech 2007 it is possible that a similar scenario was occurring in the sandstone fracture the initial 10 mm ionic strength causing more retention in the secondary minimum and microspheres translating over the rough fracture surfaces moving to the primary minimum and becoming irreversibly attached possibly in stagnant small aperture regions microspheres may be able to make the jump from the secondary minimum to the primary minimum because of the shorter separation distance of the secondary minimum from the fracture surface at higher ionic strength the roughness of the sandstone surface and protruding polyelectrolytes from the cml microsphere surface given the heterogeneity of the fracture surface mineralogy and the observed retention of cml microspheres at 10 mm is it is likely that a fraction of the microspheres were deposited on regions that had a net positive charge over the zones of influence of the microspheres reducing the ionic strength would increase the size of the colloidal zone of influence possibly leading to lower retention of microspheres in the smallest regions of net positive charge if the net charge in the increased zone of influence became negative pazmino et al 2014a rasmuson et al 2019 due to the expansion of the zone of influence to include more negatively charged surface area the small amount of 200 nm and 1000 nm microspheres released during the low is flush indicates that if there was a zone of influence effect only a small proportion of microspheres attached to positively charged regions were impacted by the change for both the 200 nm and 1000 nm cml microspheres the effect of increasing the ionic strength of the tracer solution for favorable conditions differed from the effect under unfavorable conditions the mass recovery and btc did not vary significantly for either particle size when the ionic strength was increased under favorable attachment conditions results not shown this result is consistent with the attachment occurring in the primary minimum as no secondary minimum or energy barrier exists under favorable conditions an investigation in porous media by elimelech 1991 showed that the deposition rate of colloids under favorable attachment conditions increased when the ionic strength was reduced in contrast to the trends observed for unfavorable attachment the primary impact that increasing ionic strength has under favorable attachment conditions is to decrease the zeta potential of the microspheres and reduce the electrostatic attraction between the colloid and collector an increased recovery at higher ionic strength was not observed in this investigation but it is possible that the increase in ionic strength was not large enough to produce this effect 3 5 particle size effects for favorable conditions the peak btc concentration was slightly greater for the 1000 nm aml colloids compared to the 200 nm aml colloids this is consistent with the expected trend in the single collector contact efficiency which decreases as particle size increases from 200 nm to 1 μm tufenkji and elimelech 2004b ron et al 2019 ron and johnson 2020 this trend is due to a greater rate of decrease of diffusion related colloid collector contact compared to the rates of increase of contact due to interception and gravity effects for unfavorable conditions at 10 mm is and 0 35 mm s 1 specific discharge the peak concentrations table 1 fig 3 were higher for the 1000 nm cml microspheres than for the 200 nm cml microspheres this is consistent with the study of cml microsphere transport in dolomite fractures at 3 mm is mondal and sleep 2012 in which the peak c c0 values were higher for 500 nm microspheres than for 200 nm microspheres the same trend of increasing c c0 i e lower retention with increasing microsphere diameter between 100 and 1000 nm in a fractured till 3 7 mm ionic strength was observed by rasmuson et al 2020 in impinging jet experiments and simulations ron et al 2019 2020 found decreasing collector efficiency with increasing colloid size in the range from 100 nm to 1000 nm was also ron et al 2019 2020 indicating an expectation of higher peak concentrations for the 1000 nm cml microspheres compared to the 200 nm cml microspheres while the initial breakthrough concentrations for the 200 and 1000 nm cml microspheres were similar at 5 mm ionic strength the effluent concentration for the 200 nm cml microspheres continued to slowly increase with pore volume and at a higher rate of concentration increase than observed for the 1000 nm cml colloids such that the final 200 nm cml microsphere c c0 value was 0 86 compared to 0 78 for the 1000 nm cml microspheres the reason for this deviation from the expected trend of increasing peak concentration and mass recovery with increasing colloid size over the range from 200 to 1000 nm under unfavorable conditions is unclear given the increasing effluent concentration in time for the 200 nm microspheres and the higher number of 200 nm microspheres injected compared to the 1000 nm microspheres there was perhaps an increasing degree of saturation of net positive charge regions of a size consistent with the zone of influence of the 200 nm microspheres at 5 mm is in the period following the colloid addition at both 5 mm and 10 mm ionic strength fig 3 there was more tailing of concentrations of the 200 nm cml microspheres compared to the 1000 nm cml microspheres this larger tailing of smaller microspheres consistent with the results of rasmuson et al 2020 may have been due to continued translation of the 200 nm microspheres along the fracture surface in the secondary minimum zone or shearing of microspheres from localized zones of attachment given the range of aperture sizes and water velocities there may have also been more diffusion of the 200 nm cml microspheres into and then out of low aperture low water velocity fracture regions compared to the 1000 nm microspheres resulting in greater tailing for the 200 nm microspheres the difference between favorable and unfavorable conditions based on the peak concentrations in the breakthrough curves fig 2a and mass recoveries is largest for the 200 nm microspheres at 5 mm is at 10 mm is the differences between favorable and unfavorable conditions are similar for the 200 and 1000 nm microspheres and are smaller than the differences for both microsphere sizes at 5 mm is these results indicate that differences between favorable and unfavorable conditions were affected more by changes in is from 5 mm to 10 mm than by a change in colloid size from 200 nm to 1000 nm studies have suggested that an optimal particle size can be determined for transport through a fractured system with 500 nm commonly being determined to have the highest mobility cumbie and mckay 1999 mccarthy et al 2002 the optimal particle size could not be determined experimentally for this study because only two particle sizes were tested although the optimal particle size could not be determined one clear impact of particle size that was observed from the transport tests is that the larger microspheres were not as strongly affected by changing the test conditions changing the specific discharge ionic strength and switching from anionic to cationic microspheres did not result in any major changes in the btc for the larger microspheres 1000 nm the larger microspheres also had a shorter mean residence time table 1 for both favorable and unfavorable attachment both the shorter mean residence time and more consistent btc can be explained by the size exclusion effect lower diffusion coefficient and reduced brownian motion of the larger microspheres the larger microspheres move through the fracture faster because they are excluded from slow moving low aperture regions and diffuse into slower flow paths less frequently than the smaller microspheres james and chrysikopoulos 2003 and james et al 2018 have also noted that the effective velocity of larger colloids will be higher because the finite size of a colloid will exclude it from the slowest moving portion of the parabolic velocity profile nearest the wall the faster moving larger colloids have less opportunity to attach in the primary or secondary minimum when they are further from the fracture walls and are therefore less affected by changing test conditions under favorable attachment conditions recovery of the 1000 nm microspheres was higher than the recovery of the 200 nm microspheres for all ionic strength and specific discharge conditions despite the larger microspheres having a more positive zeta potential higher electrostatic attractive forces not resulting in more retention is further evidence that the size of the colloid and the tendency to stay in the bulk flow region is particularly important to the mobility of colloids under favorable attachment conditions 3 6 specific discharge effects studies have shown that retention generally decreases with increasing specific discharge albarran et al 2013 mondal and sleep 2013 although this is not true in all cases as noted by rodrigues et al 2013 the decrease in retention is attributed to higher fluid drag at higher velocities which impacts the balance of mobilizing and arresting torques given this it would be expected that the impact of an increase in specific discharge would be greater for larger colloids tong and johnson 2006 in the current study a doubling of specific discharge at 5 mm is increased mass recovery by 3 3 for 200 nm aml colloids and by 3 7 for 1000 nm colloids there was less than a 1 increase in mass recovery for the 200 nm and 1000 nm cml colloids with a doubling of specific discharge at 5 mm is at 10 mm is there was less than 1 change in mass recovery of 1000 nm cml colloids while there was an increase in mass recovery of 9 4 for the 200 nm cml colloids at higher is the secondary minimum would be deeper and the zone of influence of the microspheres would be lower under these conditions the higher water velocity could result in more cml microspheres in the secondary minimum reaching the fracture outlet there would also be reduced time for the cml microspheres to interact with the localized zones of net positive charge 3 7 generation of favorable attachment sites positively charged attachment sites can have a major impact on transport of negatively charged colloids even if the zeta potential results suggest the overall conditions are still unfavorable this phenomenon was noted by elimelech et al 2000 when they discovered that colloid deposition kinetics were controlled by the degree of patch wise chemical heterogeneity and not by the measured zeta potential of the porous media liang et al 2021 observed that deposition of silver nanoparticles was reduced by the presence of attached soil colloids and that the nanoparticles were more likely to be released with reductions in ionic strength due to soil colloids creating convex locations on soil grains in the current study performing favorable attachment tracer tests resulted in cationic aml microspheres being attached to the sandstone surface which created positively charged impurities on the overall anionic sandstone surface and potentially also changed the surface morphology after the favorable attachment experiments were completed an unfavorable attachment tracer test was run to analyze the impact of attached aml microspheres on cml microsphere transport the 200 nm btcs in fig 5 show that the transport results of cml microspheres closely resemble favorable attachment in the presence of attached cationic microspheres a similar effect was found for the larger 1000 nm microspheres result not shown the increased tailing and spike in concentration after the ionic strength was reduced both indicated that the overall conditions were still unfavorable despite the significant increase in retention the attached cationic microspheres did not have an observable impact on favorable attachment transport tests the assumption that the overall zeta potential of the collector can determine the transport behavior of colloids through fractured media may lead to poor predictions in the presence of surface charge heterogeneities caused by mineral heterogeneity and roughness or due to attachment of other colloids that change local zeta potentials and surface morphology by running multiple tracer tests with 200 nm cml microspheres the favorable attachment sites were eventually filled and blocked for further attachment after the favorable attachment sites were blocked the mass recovery increased back to what it was for previous unfavorable transport tests it should be noted that all the cml microsphere tracer tests were performed prior to the aml microsphere tests to avoid this interference 3 8 estimated transport parameters modeling with hydrus 1d was used to identify the changes in parameters for different transport conditions the btcs were fit using a two site kinetic non equilibrium model which incorporated attachment and detachment figs 2 and 3 present the observed and model fitted microsphere btcs under favorable and unfavorable attachment conditions with the fitted parameters listed in table 2 a good fit was found for all microsphere btcs r2 98 5 by assigning site 1 with a very low detachment coefficient kdet1 of 1 0 10 7 to 1 0 10 10 s 1 consistent with mostly irreversible attachment and site 2 with a higher detachment coefficient kdet2 of 1 0 10 3 to 3 0 10 3 s 1 consistent with a significant proportion of reversible attachment sites having reversible and irreversible attachments site can simulate with the existence of primary and secondary minima for unfavorable attachment conditions the difference between fits for favorable and unfavorable attachment was primarily related to the magnitude of the two attachment coefficients for favorable attachment katt1 irreversible attachment site was higher and katt2 reversible attachment site was lower than for unfavorable attachment this is logical given that irreversible attachment should be a more significant retention mechanism under favorable conditions the higher value of kdet1 irreversible attachment site for the 200 nm microspheres under unfavorable conditions provided the fit to the higher tailing measured for this case compared to other cases it is possible that the higher detachment coefficient for site 1 accounts for the slow release of microspheres back into the higher velocity zones from small aperture low velocity regions contributing to tailing an increase in ionic strength under unfavorable attachment conditions resulted in a higher value of katt1 irreversible attachment site this is consistent with higher ionic strength leading to more microspheres that were initially attached in the secondary minimum jumping into the primary minimum and becoming irreversibly attached including the effect of blocking did not significantly improve the fit of the btcs a similar two site kinetic was used by mondal and sleep 2012 to simulate colloid transport under unfavorable attachment conditions in dolomite rock fractures 4 conclusions the comparison between favorable and unfavorable attachment in this study revealed a small difference in mobility between microspheres under favorable and unfavorable attachment conditions in fractured media the mobility of cationic colloids favorable conditions for attachment appeared to be dependent on the opportunity for interaction with the fracture surfaces which would be influenced by factors such as colloid size fracture aperture surface roughness and specific discharge the results of this study indicate that it should not be assumed in all scenarios that colloids in fractures will be close to immobile if attachment conditions are net favorable increasing ionic strength significantly decreased the recovery of microspheres under unfavorable attachment assumed to be due to the effect on the depth of the secondary minimum but did not affect the mass recovery under favorable attachment conditions the small release of microspheres with a low ionic strength flush under unfavorable attachment conditions indicated that many of the retained microspheres were irreversibly attached this could be attributed to the effects of the mineralogical heterogeneity of the fracture surfaces creating local domains for favorable attachment and the effects of fracture surface roughness in lowering energy barriers to deposition in primary minima deposition and recovery of microspheres under all but the highest ionic strength with 200 nm microspheres were not very sensitive to specific discharge prior transport and attachment of cationic microspheres can increase the attachment of anionic microspheres in subsequent transport of anionic microspheres declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was funded by the natural sciences and engineering research council of canada discovery grant rgpin 2017 06504 held by b sleep additional data can be found in the masc thesis of s spanik https tspace library utoronto ca bitstream 1807 79153 3 spanik sean p 201711 mas thesis pdf 
1383,transport and transformation processes of nitrogen in the soil are an essential part of understanding the relationship between agricultural input and nitrate no3 concentrations in groundwater the presented study describes these transformation processes around no3 degradation at a water catchment in the lower rhine embayment germany despite intensive agriculture extracted groundwater at a depth of 21 to 22 m shows unexpectedly very low no3 levels below 3 mg l no3 for all wells the local water supplier therefore carried out investigations in this area and generated soil data from 22 representative areas 142 soil samples from 82 drilling meters from the surface to a max depth of 5 5 m and groundwater analyses from 17 groundwater monitoring wells from 3 to 5 m below ground surface soil types are predominantly luvisol and gleysol the substrate in the topsoil is mainly clayey silt underneath there are mostly medium grained sands with partial silt intercalations which appear as a separate layer based on this dataset the percolating water residence times and the no3 leaching potential were calculated in this study together with the nitrogen surplus and with the help of reactive transport modelling the denitrification potential in the vadose zone was simulated the comparison of simulation results with laboratory measured data shows a high correlation substantial no3 reduction in the vadose zone was observed dependent on soil type reduction capacity and water residence time up to 25 of the no3 was reduced here the applied modelling is considered an improvement in no3 degradation potential assessment because it considers many relevant variables such as precipitation soil parameters grain size field capacity available water capacity coarse fragments and nitrogen input therefore a transfer to other sites with comparable hydro geo logical conditions is possible also due to relatively easily determinable input data this assessment of nitrogen degradation in the vadose zone will be a useful tool for no3 levels forecast in groundwater keywords nitrate groundwater agriculture degradation hydrogeochemistry abbreviations a exchange frequency ard depth of root horizon apl soil water available apot potential leaching of nitrogen in soil arh depth of root horizon leaching dmax max denitrification rate dsoil denitrification loss in the soil fc field capacity fcrd field capacity in the effective root zone gwr groundwater recharge k michaelis constant ng nitrogen discharge after the denitrification nmin autumn available mineralized nmint depth of nmin sample nminn measured excess nitrogen nsu summer precipitation nwi winter precipitation n t no3 level after the residence time pv percolation velocity pw no3 concentration in percolating water t residence time tsoil residence time of the percolating water ufc usable field capacity ufcrd soil water available to plants crop rooting depth 1 introduction most of the nitrogen n input to soils and groundwater is caused by agricultural fertilization the respective n demand for crop growth is based on land use to develop suitable and sustainable countermeasures to excess n migration from soils to groundwater it is essential to characterize and forecast its fate from fertilization to groundwater entry including passage through the vadose zone prediction of n input and discharge via the unsaturated soil zone into groundwater was investigated in many studies e g aulakh et al 1992 stenger et al 2002 baran et al 2007 with the majority of current research examining the rooted soil zone and its potential for nitrate no3 reduction to better predict how much no3 reaches the groundwater zone in a given time span the main denitrification processes chemo organotrophic and chemo lithothrophic denitrification need to be taken into account in these processes no3 is converted under mostly anaerobic conditions by reducing agents organic carbon or sulphide s with the help of denitrificants microorganisms nitrate is converted into nitrite nitric oxide nitrous oxide and molecular nitrogen the major process is the reduction of no3 to n2 gas in a metabolic oxidation of organic matter canfield et al 2010 which is a main pathway for reactive n removal this study investigates the unsaturated zone below the root horizon in the following referred to as deep vadose zone in an area affected by intensive agricultural use the water retention time combined with the no3 input into this zone and subsequent reduction allow a quantitative conclusion on the degradation efficiency of the deep vadose zone numerous studies have investigated the leaching of no3 into the deep vadose zone below the root zone e g seong and rubin 1999 onsoy et al 2005 fraters et al 2006 botros et al 2012 turkeltaub et al 2016 baram et al 2017 turkeltaub et al 2018 most assume no denitrification or a negligible amount in this area schulte kellinghaus 1988 chen et al 2018 to the authors best knowledge no approach combining n input water retention time and discharge has been presented so far to predict n fate in the deep vadose zone knoll et al 2020 estimate no3 reduction across the unsaturated zone and the groundwater body they combine different aspects of the flow paths of nitrate n through the vadose zone and groundwater but for simplification they assumed no3 for the entire n load in the percolating water and no reduction between the root zone and the groundwater individual ex situ experiments to determine the denitrification potential provide information on individual situations the difficulty is the upscaling to real conditions we consider a unified simple model which approximates the discharge of n in different soil zones more useful to predict n flow because it can determine no3 input into the aquifer relatively quickly and allows for an estimation of the denitrification potential of the deep vadose zone in a given area the complex situation in the soil zone due to different hydrogeochemical conditions and processes makes a general model difficult a mass balance study by onsoy et al 2005 using 1200 soil samples showed the heterogeneity and complexity in the deep vadose zone denitrification depends on various factors such as oxygen concentration carbon concentration ph temperature percolating water residence time n input and types of microbes rivett et al 2008 in addition studies such as he et al 2018 show that climate change is likely to have an impact on no3 leaching from the vadose zone in the future nitrogen can accumulate in the soil over a longer time span during dry periods and then be increasingly washed out in the form of no3 studies suggest that there may be a large increase in no3 concentrations in groundwater in some regions of the earth in the future due to lower groundwater recharge no3 concentrations increase as a result of missing dilution ducharne et al 2007 ortmeyer et al 2021 due to this fact it is even more important to understand relevant hydrogeochemical processes in the unsaturated zone furthermore no3 storage in the vadose zone can be much higher than often expected ascott et al 2016 estimate a potential high impact in areas with a thick vadose zone and extensive historical agriculture the percolating water has a longer travel time that leads to a delay that will have an impact on groundwater quality in this study an agricultural area in the lower rhine embayment western germany is investigated a local water supplier carried out investigations in this area and collected data consisting of soil and groundwater analyses which were used as the database despite high n inputs groundwater shows low no3 concentrations the hypothesis is that the discrepancy between the mass of n input and the mass of n discharge into the aquifer is caused by no3 reduction in the unsaturated zone this deep vadose zone below the root zone is therefore the presumed decisive factor for the reduced no3 input up to 25 the aim of this work is to develop and test a model which allows an estimation of the denitrification potential below the root zone with easily available variables such as precipitation soil parameters and n input this is achieved by modelling using michaelis menten kinetics this approach combines the kinetics of 1st and 0th order depending on no3 concentration in the percolating water and is coupled to the water residence time bowman and focht 1974 the model helps estimate the entry of no3 via percolating water into the aquifer over a larger area the predicted no3 reduction allows conclusions to be drawn about the expectable concentration in groundwater effective groundwater pollution from agriculture can thus be better predicted in addition the deep vadose zone is illuminated regarding its capacity for no3 reduction 2 materials and methods 2 1 site description the area is located in the lower rhine embayment in the western part of the german federal state north rhine westphalia about 10 km from the city of mönchengladbach geologically the embayment is an intraplate rift structure within the european variscan mountains active since paleogene times and filled with a thick succession of oligocene to quaternary sediments representing a multi aquifer formation in the study area highly permeable fluvial quaternary sediments terraces of the river rhine contain substantial groundwater resources used for drinking water production the about 30 m thick sands and fine gravels partly overlie the paleogene as one aquifer while in other parts two aquifers developed separated by an interglacial aquitard grabert 1998 in the studied water catchment groundwater is extracted from the bottom of the quaternary sediments in the forested northern part depth to groundwater table is between 1 and 3 m below ground surface while in the southern agricultural areas it is between 3 and 5 m below ground surface the average water table depth for soil drilling sites is 3 43 m pedologically using the classification after iuss working group wrb 2015 most of the investigated agricultural land in the southern part of the study area can be assigned to the type luvisol in the northern part there are mainly semi terrestrial soils with groundwater influenced soil types gleysol and stagnosol in the northeastern forested area lowland moor soils are common along the stream the semi terrestrial soils can partially migrate bioavailable organic matter into the unsaturated zone mehranfar 2003 the northern part is therefore predestined for chemo organotrophic denitrification most of the arable land can be classified as gleyic luvisol fig 1 in terms of groundwater no3 concentrations a decreasing trend from south to north can be observed concentrations above the drinking water limit are regularly measured in the southernmost part not completely shown in fig 1 the german drinking water ordinance stipulates a maximum value for no3 of 50 mg l trinkwv 2001 in the following all presented nitrate concentrations in mg l are expressed as no3 not as no3 n the production wells are in the northern part where no3 levels are very low fig 1 or groundwater is virtually no3 free the annual average no3 concentration of 17 groundwater monitoring wells in the last decade is between about 13 and 23 mg l fig 2 the average sampling depth of the groundwater samples is 5 5 m the maximum is at 10 m furthermore the monitoring stations in the water catchment have shown almost constant levels of cl average 41 mg l with a max 23 deviation so4 2 average 113 mg l with a max 20 deviation and hco3 average 379 mg l with a max 6 deviation concentrations during the last decade 2 2 vadose zone sampling in the investigation area soil samples were obtained from deep drillings as ram core samples from 22 locations these are divided into 19 arable areas 1 grassland area no 24 and 2 forest areas no 25 and 26 fig 1 the soil between 1 m below ground level and the groundwater surface and partly beyond the latter was explored the deep vadose zone was recorded which extends from underneath the root zone to the groundwater the soil samples are taken layer by layer in depth sections of 50 cm thickness each the root horizon was fixed defined by the soil mapping guide ad hoc ag boden 2005 at 1 1 m because of the root penetration depth in the given soil types for the determination of no3 photometric determination was carried out by a continuous flow method continuous flow analysis flow injection analysis sfa with dialyzer and cd cu reduction column this is a standard procedure according to din 38406 e5 vdlufa method volume i a 6 1 4 1 calibration is carried out with a series of no3 standard 25 0 g of dried sample is mixed with 100 ml of an extraction solution of 0 0125 m calcium chloride solution the suspension is shaken with an overhead shaker for 30 min at approx 30 rpm the analyses were carried out by a certified analytical laboratory wasserlabor niederrhein gmbh concentrations of no3 so4 2 and nh4 were quantified additionally the soil was analyzed soil substrate humus content color carbonate content moisture hydromorphic characteristics according to ad hoc ag boden 2005 the soil types described here have been transferred to the wrb classification wrb 2015 this is important for determining the soil type and the soil conditions a total of 142 soil samples were taken 2 3 modelling framework to estimate the denitrification potential in the vadose zone the denuz denitrification in the unsaturated zone german abbreviation model wendland 2010 is used as a basis for an approach that considers the upper soil type and environmental parameters therefore different initial data such as soil parameters precipitation and empirical data are necessary the required values are calculated or determined by values based on empirical studies the equation is 1 n t dn t dt d max n t k n t 0 where n t is the no3 level after the residence time kg n ha a t is the residence time a dmax is the max denitrification rate kg n ha a k is the michaelis constant kg n ha a it has been shown that the soil type reflects the most important influencing factors like soil water content availability of organic carbon temperature and ph nlkwn 2010 in the denuz model soil types are divided into classes of denitrification rates maximum values dmax for denitrification of the individual soil types were determined the calculation details are provided in the next subsection to model time dependent denitrification the denuz model uses the michaelis menten kinetics following the example of bowman and focht 1974 this describes the relationship between enzymatic reaction rate and substrate concentration the constant k influences the range in which the no3 conversion is controlled by the no3 concentration the time is represented by the residence time of the percolating water t the dissolved n in the percolating water moves gravimetrically via precipitation towards groundwater the residual n in the soil is used as the initial value autumn available mineralized nitrogen nmin this value is measured around mid november as this is usually the time when percolating water begins to form in substantial volumes it is assumed that the n is mobile in the form of no3 in the topsoil fig 3 gives an overview of the calculation approach field capacities and groundwater recharge are calculated and used to estimate the percolating water residence time the soil type determines the maximum denitrification rate and the constant k the autumn nmin value provides the initial concentration of n that is potentially washed out into the deep vadose zone using the michaelis menten equation denitrification is calculated as a function of residence time soil type and excess n fig 3 2 4 percolating water residence time the method after renger 2002 is used to determine the no3 shifting depth and duration of stay firstly the amount of percolating water is calculated the percolating water flows vertically into the groundwater without any intermediate flow eqs 2 4 calculating groundwater recharge show that the influencing factors are winter nwi and summer precipitation nsu evapotranspiration according to haude 1954 ehaude and the amount of soil water available to plants apl arable land 2 v 0 92 n wi 0 61 n su 153 log a pl 0 12 e haude 109 grassland 3 v 0 9 n wi 0 52 n su 286 log a pl 0 10 e haude 330 coniferous forest 4 v 0 71 n wi 0 67 n su 166 log a pl 0 19 e haude 127 a nearby weather station 5 km away provides the amount of precipitation and potential evapotranspiration after haude the groundwater recharge is attributed to corrections due to cultivation since the equation for arable land is based on winter wheat as a standardized crop the usable field capacity of the crop rooting depth ufcrd is calculated by multiplying the individual usable field capacities of the soil types by the effective root zone thickness 5 ufc rd mm ufc mm dm crop rooting depth dm the crop rooting depth rd is fixed at a depth of 11 dm for gleysol and stagnosol the ufcrd is a measure for soil water available to plants the field capacity is determined analogously a mean storage density of 1 55 1 8 g m3 is defined by the soil mapping guide ad hoc ag boden 2005 for further classification the storage density is assigned to dry bulk densities pt based on the dry bulk densities combined with the individual soil types the usable field capacity can be determined the proportion of organic matter and the proportion of coarse soil must be considered in the calculation for this purpose the humus content is converted into organic matter content table 1 the coarse soil content is to be considered because larger skeletal components in the soil have a negative effect on the field capacity the average coarse soil content is 18 sd 22 29 it differs for each test field from 0 to 68 due to high variability the residence time is calculated from the deep infiltration quantity and the field capacity under the root zone the field capacity is used because there is no influence by plant roots in this zone 6 t soil fc d pv where tsoil is the residence time of the percolating water a pv is the percolation velocity mm a fc is the field capacity mm dm d is the layer thickness of the zone dm an average value is calculated for the field capacity of the zone for each area this represents the averaged field capacity of the individual soil layers per field profile it is multiplied by the layer thickness of the deep vadose zone and divided by the amount of percolating water eq 6 2 5 nitrogen input and leaching it has been shown that an accurate estimation is achieved with the help of n demand determination patterns it is an alternative representation of surplus no3 at the end of the growing season carey et al 2017 determining the real n surplus is difficult due to various factors the heterogeneity of the different sites and their different management allow wide ranges of n surpluses the measured autumn nmin content in the soil provides information on the amount of n in the soil this value represents the available soil n which can potentially leach out with the percolating water the autumn nmin value is influenced by various factors the most important of which are crop type harvesting technique and the mineralization potential of the soil sullivan and cogger 2003 it is measured shortly before the formation of percolating water due to high autumn precipitation the relationship between n balance nmin and no3 concentration in the leachate is described in bechtel 2008 other studies use the nmin to estimate the soil net n on a larger scale risch et al 2019 several authors were able to estimate no3 discharge by using autumn nmin scheffer 1999 bechtel 2008 in this study the average autumn nmin is derived from the calculation of 21 arable land areas these are from the cultivation years 2016 to 2018 with average nmin values of 85 kg n ha a potential no3 discharge the amount of n leached into the deep vadose zone depends on the exchange frequency ef the higher the frequency ef the more no3 can potentially be washed out to determine the potentially leachable excess n eqs 7 to 9 are used nlwkn 2010 the leachable excess n consists of the individual n species no3 no2 nh4 nh3 and nh4 eq 7 describes the percentage of soil water that is exchanged within one year the simplified case of uniform downward displacement of water is assumed values above 100 represent a complete exchange of the soil water the exchange frequency ef is 1 for values below 100 the respective value corresponds to the percentage washout a the calculation is based on the published recommendations of the lower saxony state office for water management coastal and nature conservation nlwkn 2010 these are calculated according to 7 ef gwr fc rd 100 where ef is the exchange frequency of soil water gwr is the groundwater recharge mm a fcrd is the field capacity in the rooted soil mm a is ef 100 if ef 100 and ef 1 if ef 100 if the sample depth does not correspond to the depth of the root horizon the exchange frequency must be adjusted in percentage to the depth 8 based on the sample depth of the nmin data the leaching of the measured nmin value of the soil is calculated as a percentage 8 a rh a rd where arh is the depth of root horizon leaching m a is the exchange percentage rd is the root horizon thickness m the potential leaching is then calculated by multiplying the depth of leaching of the root horizon by the depth of nmin sampling multiplied by the measured nmin 9 a pot a rd nmin t nmin n where apot is the potential soil n leaching kg n ha ard is the depth of root horizon leaching m nmint is the depth of nmin sample m nminn is the measured excess n kg n ha it is assumed that the nh4 n content is of minor importance nlwkn 2010 to predict the no3 concentration in the percolating water the autumn nmin value is converted according to 10 pw a pot 443 gwr where pw is the no3 concentration in the percolating water mg l apot is the potential leaching of n from the soil kg n ha gwr is the groundwater recharge mm a 443 is the conversion factor 4 43 of no3 with the factor 100 2 6 estimating denitrification potential the degradation capacity of the zone below the root zone is based on the classification of the maximum potentials of the soil according to nlkwn 2010 for the calculation the deep vadose zone under the soil type gleyic luvisol is set to a maximum denitrification capacity dmax of 10 kg n ha a the zones below gleysol and stagnosol were set to a dmax of 30 kg n ha a at these sites new organic matter can be introduced into the vadose zone leading to anaerobic no3 reducing conditions therefore the potential for denitrification is substantially increased evaluation of the denitrification potential of individual soil types is taken from müller and raissi 2002 the classifications are each at the lower limit of the classes such that the potential is estimated rather conservatively 2 7 model parameterization the denuz model approach assigns a pair of dmax and k values to each denitrification level according to köhne and wendland 1992 the constant k is set to values between 18 7 kg n ha a good denitrification conditions and 2 5 kg n ha a poor denitrification conditions in this study the constant k is set to 2 5 kg n ha a at a dmax of 10 kg n ha a and to 4 kg n ha a at a dmax of 30 kg n ha a köhne and wendland 1992 the percentage of denitrification loss is given by 11 d soil n t n 0 100 where dsoil is the denitrification loss in the soil n t is the n content in the soil after the percolation time t kg n ha a n0 is the n content in the soil kg n ha a that is equivalent to nmin n t is solved iteratively according to eq 1 thus the reduction over time is simulated the n discharge from the vadose zone is calculated by 12 n g n 0 n t where ng is the n discharge after the denitrification kg n ha a n t is the n content in the soil after the percolation time t kg n ha a n0 is the n discharge in the soil kg n ha a 3 results 3 1 processes in the vadose zone hydromorphic soil characteristics may give an indication of reduced conditions these occur in almost all profiles as bleached or marked green grey to blue grey colors fig 4a and b show the decrease of no3 concentrations in soil solutions towards depth with a constantly low so4 2 concentration in the leachate fig 4a shows a profile of land section 6 cf fig 1 on which summer wheat and turnips were cultivated from 2016 to 2018 on land section 27 fig 4b cf fig 1 a crop sequence of wheat arable grass maize was cultivated in the same period the soil type in land section 6 can be assigned as gleyic luvisol and in land section 27 as gleysol 3 2 nitrogen input for the n discharge the average of all profiles from autumn nmin is considered the rooting depth is 1 1 m a calculated average groundwater recharge of 202 mm a is used the average field capacity for all areas is 365 mm table 2 annually an average of 57 kg n ha no3 and nh4 combined is washed into the deep vadose zone that begins in a depth of 70 bis 110 cm below the surface 3 3 relative denitrification the input value is 57 kg n ha a the different denitrification rates in the zones are shown in fig 5 while the vadose zone at the gleysol stagnosol sites has denitrified 90 of the n after a percolating water residence time of almost two years the zone at the gleyic luvisol sites requires more than five years for the same percentage the average retention time of the percolating water under the gleyic luvisol soil is 2 years that of the stagnosol gley 1 8 years table 1 this corresponds to an average reduction of 34 of the input n in the deep vadose zone at the gleyic luvisol sites while for the other sites average reduction is 63 consequently at the gleyic luvisol sites an average of 37 7 kg n ha a remains as residual n which can potentially be introduced into groundwater with the percolating water at the stagnosol sites 23 4 kg n ha a remain this corresponds to a residual amount of 41 to 66 of the originally calculated input n the potential of denitrification below the root zone has been calculated individually for each arable site to make a general comparison the average of the last half meter before groundwater is used this average no3 concentration is 85 mg l the calculated value for an average excess after denitrification is 35 kg n ha a which corresponds to a no3 concentration of 79 mg l the deviation of the no3 concentrations of the calculated values to the measured ones is therefore only 6 mg l or about 7 in fig 6 the concentration curve of no3 is shown as an average development of all arable sites with depth at a depth of 4 m the average measured concentration is 90 mg l 11 profiles go down to a depth of 4 m which still allows for a good comparison overall measured concentrations decrease from 121 mg l to 90 mg l this corresponds to a decrease of 31 mg l over 3 m thickness or converted to the n input 14 kg n ha a using a percolating water quantity of 202 mm a 4 discussion estimations of no3 reduction in the unsaturated zone and the groundwater body often do not consider the deep vadose zone below the root zone or estimate them to be negligible fraters et al 2006 chen et al 2018 often transport modelling is done in the vadose zone rock and kupfersberger 2019 even combine a 1d simwaser stotrasim model with a 2d vadose zone feflow model groundwater in addition calculations of no3 storage and travel time in the vadose zone are equally common turkeltaub et al 2020 predict regional scale groundwater recharge and no3 storage and likewise turkeltaub et al 2018 calculate the travel time of no3 through the vadose zone however modelling of denitrification is very rare our results indicate that this zone appears to be of underestimated importance in this regard and should therefore be included in nutrient cycle considerations the soil types are crucial as they can bring microbially usable organic matter with the percolating water into deeper vadose zones since measured laboratory data are available from the soil profiles an estimation can be made of how realistic the presented model is and where it is possibly limited or has optimization potential the amount of n in the soil can change significantly due to natural processes the most relevant of which are organotrophic and lithotrophic denitrification the calculated input using the measured autumn nmin values results in an input value which is taken as an average value for all fields the calculated amount of percolating water and the average field capacity in the root zone as well as the residual n quantity are included in the calculation the equation according to renger 2002 for the calculation of the quantity of percolating water for the field sites has a high multiple pearson correlation coefficient r of 0 84 eqs 2 and 3 eqs 2 4 show that the depth infiltration increases with increasing potential precipitation but decreases with increasing plant available soil water and potential evapotranspiration consequently a decrease in depth infiltration can be expected in the future as potential evapotranspiration will increase which will be triggered by rising temperatures in the course of climate change ortmeyer et al 2021 high positive correlation coefficients r show that the relationship between deep infiltration and the used climate and soil characteristics are relatively narrow the standard deviation for the calculated deep infiltration values is 20 30 mm a the pedological recording of the depth profiles provides only a punctual insight into the subsoil of the areas at each drilling point there may be strong deviations from layer thickness to the structure of the subsoil the calculated residence time for 60 of the investigated areas is 1 to 2 years for 2 sites it is between 3 and 3 5 years which is due to the higher groundwater table depths of 4 and 5 m akbariyeh et al 2018 similarly point out that a consideration of the groundwater level plays an important role if the groundwater level rises it is easier to transport a large amount of n in the form of no3 into the groundwater similarly a study by juntakut et al 2019 indicates aquifers with relatively lower no3 concentrations in areas with thicker vadose zones nevertheless ascott et al 2017 demonstrate long travel times in the vadose zone in areas with thick vadose zones and extensive historical agriculture so its no3 leaching may take a long time to occur and measures such as a change in agricultural practices may have a delayed effect in any case mass balances show that the excess n corresponds very well to the annual n accumulation in the soil profiles baram et al 2017 the michaelis menten kinetics models the degradation of n under idealized conditions the denitrification rate k which describes the range from which the no3 reduction is limited by the no3 concentration itself appears to play a minor role in the soil type gleyic luvisol in the case of very high n surpluses with low degradation rates k becomes a small factor the calculated leaching potential was determined using calculated average values an input of 57 kg n ha a results in a concentration of 125 mg l no3 in the percolating water eq 10 the coarse soil fraction and the humus fraction in the sediment were taken into account for the percolating water retention time the sediment plays a decisive role in the residence time of the vadose zone and thus also in the time in which denitrification can take place a coarse soil texture is prone to high no3 leaching turkeltaub et al 2016 derby et al 2009 also indicate that high no3 concentrations occur in sandy soil even when very conservative n rates are used intensive irrigation increases the rate of no3 leaching to groundwater juntakut et al 2019 a study by akbariyeh et al 2018 shows sediment types with higher saturated hydraulic conductivity and lower residual water content have lower water holding capacity which increased both water infiltration rates and no3 leaching rates in the present study measured laboratory value in the first 0 5 m below the root horizon is 121 mg l on average for all profiles so there is a deviation of only 3 between modelled and measured data sites 24 25 and 26 were not considered because they are not cultivated arable land fig 7 shows a summarized comparison of the calculated and the measured results the input of n as well as the calculated values of the usable field capacity the field capacity the amount of percolating water the resulting no3 concentration and its reduction below the root zone are altogether consistent calibration of the model with measured laboratory data can therefore be considered successful the unit mg l was chosen to understand the potential input of no3 into the aquifer by the percolating water the no3 measured by the nmin is dissolved and carried into the deep vadose zone under the root zone the comparison fig 7 shows a good agreement of the results with the real measured values 5 conclusion the no3 problem in groundwater and drinking water has aggravated over the last decades ward et al 2018 due to intensive agriculture high n doses reach the soil which are subject to transport and transformation processes in the vadose zone to quantify the n input into the soil zone the autumn nmin is used which provided the information necessary for calculating no3 shift and thus the residence time in the unsaturated zone denitrification potential in the unsaturated zone is determined according to the denuz model wendland 2010 the usable field capacities and the resulting groundwater recharge were calculated soil types were determined and assigned a maximum denitrification rate these data were combined with the n input and the degradation kinetics of the michaelis menten equation this resulted in the degradation potential below the root zone of the individual studied sites average calculated values were in good agreement with the averaged measured laboratory results we conclude that our initial hypothesis substantial no3 degradation in the deep vadose zone leading to unexpectedly low groundwater conditions despite massive surface n input can be verified the developed modelling approach is a promising tool to assess n degradation in the vadose zone it can be applied for an improved forecast of no3 levels in groundwater in affected areas using relatively easily determinable input data it investigates a section of the n flux that is mostly neglected focusing on the previously often underestimated denitrification potential in the deep vadose zone between the root zone and the groundwater table this zone should be increasingly taken into account in future studies on the nitrate problem still one of the most pressing groundwater quality concerns on a global scale declaration of competing interest authors declare that there is no conflict of interest acknowledgments we would like to thank new niederrheinwasser gmbh esp roland schindler and katharina greven for providing extensive hydrogeochemical and soil data and for discussions appendix a supplementary data supplementary material image 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jconhyd 2021 103843 
1383,transport and transformation processes of nitrogen in the soil are an essential part of understanding the relationship between agricultural input and nitrate no3 concentrations in groundwater the presented study describes these transformation processes around no3 degradation at a water catchment in the lower rhine embayment germany despite intensive agriculture extracted groundwater at a depth of 21 to 22 m shows unexpectedly very low no3 levels below 3 mg l no3 for all wells the local water supplier therefore carried out investigations in this area and generated soil data from 22 representative areas 142 soil samples from 82 drilling meters from the surface to a max depth of 5 5 m and groundwater analyses from 17 groundwater monitoring wells from 3 to 5 m below ground surface soil types are predominantly luvisol and gleysol the substrate in the topsoil is mainly clayey silt underneath there are mostly medium grained sands with partial silt intercalations which appear as a separate layer based on this dataset the percolating water residence times and the no3 leaching potential were calculated in this study together with the nitrogen surplus and with the help of reactive transport modelling the denitrification potential in the vadose zone was simulated the comparison of simulation results with laboratory measured data shows a high correlation substantial no3 reduction in the vadose zone was observed dependent on soil type reduction capacity and water residence time up to 25 of the no3 was reduced here the applied modelling is considered an improvement in no3 degradation potential assessment because it considers many relevant variables such as precipitation soil parameters grain size field capacity available water capacity coarse fragments and nitrogen input therefore a transfer to other sites with comparable hydro geo logical conditions is possible also due to relatively easily determinable input data this assessment of nitrogen degradation in the vadose zone will be a useful tool for no3 levels forecast in groundwater keywords nitrate groundwater agriculture degradation hydrogeochemistry abbreviations a exchange frequency ard depth of root horizon apl soil water available apot potential leaching of nitrogen in soil arh depth of root horizon leaching dmax max denitrification rate dsoil denitrification loss in the soil fc field capacity fcrd field capacity in the effective root zone gwr groundwater recharge k michaelis constant ng nitrogen discharge after the denitrification nmin autumn available mineralized nmint depth of nmin sample nminn measured excess nitrogen nsu summer precipitation nwi winter precipitation n t no3 level after the residence time pv percolation velocity pw no3 concentration in percolating water t residence time tsoil residence time of the percolating water ufc usable field capacity ufcrd soil water available to plants crop rooting depth 1 introduction most of the nitrogen n input to soils and groundwater is caused by agricultural fertilization the respective n demand for crop growth is based on land use to develop suitable and sustainable countermeasures to excess n migration from soils to groundwater it is essential to characterize and forecast its fate from fertilization to groundwater entry including passage through the vadose zone prediction of n input and discharge via the unsaturated soil zone into groundwater was investigated in many studies e g aulakh et al 1992 stenger et al 2002 baran et al 2007 with the majority of current research examining the rooted soil zone and its potential for nitrate no3 reduction to better predict how much no3 reaches the groundwater zone in a given time span the main denitrification processes chemo organotrophic and chemo lithothrophic denitrification need to be taken into account in these processes no3 is converted under mostly anaerobic conditions by reducing agents organic carbon or sulphide s with the help of denitrificants microorganisms nitrate is converted into nitrite nitric oxide nitrous oxide and molecular nitrogen the major process is the reduction of no3 to n2 gas in a metabolic oxidation of organic matter canfield et al 2010 which is a main pathway for reactive n removal this study investigates the unsaturated zone below the root horizon in the following referred to as deep vadose zone in an area affected by intensive agricultural use the water retention time combined with the no3 input into this zone and subsequent reduction allow a quantitative conclusion on the degradation efficiency of the deep vadose zone numerous studies have investigated the leaching of no3 into the deep vadose zone below the root zone e g seong and rubin 1999 onsoy et al 2005 fraters et al 2006 botros et al 2012 turkeltaub et al 2016 baram et al 2017 turkeltaub et al 2018 most assume no denitrification or a negligible amount in this area schulte kellinghaus 1988 chen et al 2018 to the authors best knowledge no approach combining n input water retention time and discharge has been presented so far to predict n fate in the deep vadose zone knoll et al 2020 estimate no3 reduction across the unsaturated zone and the groundwater body they combine different aspects of the flow paths of nitrate n through the vadose zone and groundwater but for simplification they assumed no3 for the entire n load in the percolating water and no reduction between the root zone and the groundwater individual ex situ experiments to determine the denitrification potential provide information on individual situations the difficulty is the upscaling to real conditions we consider a unified simple model which approximates the discharge of n in different soil zones more useful to predict n flow because it can determine no3 input into the aquifer relatively quickly and allows for an estimation of the denitrification potential of the deep vadose zone in a given area the complex situation in the soil zone due to different hydrogeochemical conditions and processes makes a general model difficult a mass balance study by onsoy et al 2005 using 1200 soil samples showed the heterogeneity and complexity in the deep vadose zone denitrification depends on various factors such as oxygen concentration carbon concentration ph temperature percolating water residence time n input and types of microbes rivett et al 2008 in addition studies such as he et al 2018 show that climate change is likely to have an impact on no3 leaching from the vadose zone in the future nitrogen can accumulate in the soil over a longer time span during dry periods and then be increasingly washed out in the form of no3 studies suggest that there may be a large increase in no3 concentrations in groundwater in some regions of the earth in the future due to lower groundwater recharge no3 concentrations increase as a result of missing dilution ducharne et al 2007 ortmeyer et al 2021 due to this fact it is even more important to understand relevant hydrogeochemical processes in the unsaturated zone furthermore no3 storage in the vadose zone can be much higher than often expected ascott et al 2016 estimate a potential high impact in areas with a thick vadose zone and extensive historical agriculture the percolating water has a longer travel time that leads to a delay that will have an impact on groundwater quality in this study an agricultural area in the lower rhine embayment western germany is investigated a local water supplier carried out investigations in this area and collected data consisting of soil and groundwater analyses which were used as the database despite high n inputs groundwater shows low no3 concentrations the hypothesis is that the discrepancy between the mass of n input and the mass of n discharge into the aquifer is caused by no3 reduction in the unsaturated zone this deep vadose zone below the root zone is therefore the presumed decisive factor for the reduced no3 input up to 25 the aim of this work is to develop and test a model which allows an estimation of the denitrification potential below the root zone with easily available variables such as precipitation soil parameters and n input this is achieved by modelling using michaelis menten kinetics this approach combines the kinetics of 1st and 0th order depending on no3 concentration in the percolating water and is coupled to the water residence time bowman and focht 1974 the model helps estimate the entry of no3 via percolating water into the aquifer over a larger area the predicted no3 reduction allows conclusions to be drawn about the expectable concentration in groundwater effective groundwater pollution from agriculture can thus be better predicted in addition the deep vadose zone is illuminated regarding its capacity for no3 reduction 2 materials and methods 2 1 site description the area is located in the lower rhine embayment in the western part of the german federal state north rhine westphalia about 10 km from the city of mönchengladbach geologically the embayment is an intraplate rift structure within the european variscan mountains active since paleogene times and filled with a thick succession of oligocene to quaternary sediments representing a multi aquifer formation in the study area highly permeable fluvial quaternary sediments terraces of the river rhine contain substantial groundwater resources used for drinking water production the about 30 m thick sands and fine gravels partly overlie the paleogene as one aquifer while in other parts two aquifers developed separated by an interglacial aquitard grabert 1998 in the studied water catchment groundwater is extracted from the bottom of the quaternary sediments in the forested northern part depth to groundwater table is between 1 and 3 m below ground surface while in the southern agricultural areas it is between 3 and 5 m below ground surface the average water table depth for soil drilling sites is 3 43 m pedologically using the classification after iuss working group wrb 2015 most of the investigated agricultural land in the southern part of the study area can be assigned to the type luvisol in the northern part there are mainly semi terrestrial soils with groundwater influenced soil types gleysol and stagnosol in the northeastern forested area lowland moor soils are common along the stream the semi terrestrial soils can partially migrate bioavailable organic matter into the unsaturated zone mehranfar 2003 the northern part is therefore predestined for chemo organotrophic denitrification most of the arable land can be classified as gleyic luvisol fig 1 in terms of groundwater no3 concentrations a decreasing trend from south to north can be observed concentrations above the drinking water limit are regularly measured in the southernmost part not completely shown in fig 1 the german drinking water ordinance stipulates a maximum value for no3 of 50 mg l trinkwv 2001 in the following all presented nitrate concentrations in mg l are expressed as no3 not as no3 n the production wells are in the northern part where no3 levels are very low fig 1 or groundwater is virtually no3 free the annual average no3 concentration of 17 groundwater monitoring wells in the last decade is between about 13 and 23 mg l fig 2 the average sampling depth of the groundwater samples is 5 5 m the maximum is at 10 m furthermore the monitoring stations in the water catchment have shown almost constant levels of cl average 41 mg l with a max 23 deviation so4 2 average 113 mg l with a max 20 deviation and hco3 average 379 mg l with a max 6 deviation concentrations during the last decade 2 2 vadose zone sampling in the investigation area soil samples were obtained from deep drillings as ram core samples from 22 locations these are divided into 19 arable areas 1 grassland area no 24 and 2 forest areas no 25 and 26 fig 1 the soil between 1 m below ground level and the groundwater surface and partly beyond the latter was explored the deep vadose zone was recorded which extends from underneath the root zone to the groundwater the soil samples are taken layer by layer in depth sections of 50 cm thickness each the root horizon was fixed defined by the soil mapping guide ad hoc ag boden 2005 at 1 1 m because of the root penetration depth in the given soil types for the determination of no3 photometric determination was carried out by a continuous flow method continuous flow analysis flow injection analysis sfa with dialyzer and cd cu reduction column this is a standard procedure according to din 38406 e5 vdlufa method volume i a 6 1 4 1 calibration is carried out with a series of no3 standard 25 0 g of dried sample is mixed with 100 ml of an extraction solution of 0 0125 m calcium chloride solution the suspension is shaken with an overhead shaker for 30 min at approx 30 rpm the analyses were carried out by a certified analytical laboratory wasserlabor niederrhein gmbh concentrations of no3 so4 2 and nh4 were quantified additionally the soil was analyzed soil substrate humus content color carbonate content moisture hydromorphic characteristics according to ad hoc ag boden 2005 the soil types described here have been transferred to the wrb classification wrb 2015 this is important for determining the soil type and the soil conditions a total of 142 soil samples were taken 2 3 modelling framework to estimate the denitrification potential in the vadose zone the denuz denitrification in the unsaturated zone german abbreviation model wendland 2010 is used as a basis for an approach that considers the upper soil type and environmental parameters therefore different initial data such as soil parameters precipitation and empirical data are necessary the required values are calculated or determined by values based on empirical studies the equation is 1 n t dn t dt d max n t k n t 0 where n t is the no3 level after the residence time kg n ha a t is the residence time a dmax is the max denitrification rate kg n ha a k is the michaelis constant kg n ha a it has been shown that the soil type reflects the most important influencing factors like soil water content availability of organic carbon temperature and ph nlkwn 2010 in the denuz model soil types are divided into classes of denitrification rates maximum values dmax for denitrification of the individual soil types were determined the calculation details are provided in the next subsection to model time dependent denitrification the denuz model uses the michaelis menten kinetics following the example of bowman and focht 1974 this describes the relationship between enzymatic reaction rate and substrate concentration the constant k influences the range in which the no3 conversion is controlled by the no3 concentration the time is represented by the residence time of the percolating water t the dissolved n in the percolating water moves gravimetrically via precipitation towards groundwater the residual n in the soil is used as the initial value autumn available mineralized nitrogen nmin this value is measured around mid november as this is usually the time when percolating water begins to form in substantial volumes it is assumed that the n is mobile in the form of no3 in the topsoil fig 3 gives an overview of the calculation approach field capacities and groundwater recharge are calculated and used to estimate the percolating water residence time the soil type determines the maximum denitrification rate and the constant k the autumn nmin value provides the initial concentration of n that is potentially washed out into the deep vadose zone using the michaelis menten equation denitrification is calculated as a function of residence time soil type and excess n fig 3 2 4 percolating water residence time the method after renger 2002 is used to determine the no3 shifting depth and duration of stay firstly the amount of percolating water is calculated the percolating water flows vertically into the groundwater without any intermediate flow eqs 2 4 calculating groundwater recharge show that the influencing factors are winter nwi and summer precipitation nsu evapotranspiration according to haude 1954 ehaude and the amount of soil water available to plants apl arable land 2 v 0 92 n wi 0 61 n su 153 log a pl 0 12 e haude 109 grassland 3 v 0 9 n wi 0 52 n su 286 log a pl 0 10 e haude 330 coniferous forest 4 v 0 71 n wi 0 67 n su 166 log a pl 0 19 e haude 127 a nearby weather station 5 km away provides the amount of precipitation and potential evapotranspiration after haude the groundwater recharge is attributed to corrections due to cultivation since the equation for arable land is based on winter wheat as a standardized crop the usable field capacity of the crop rooting depth ufcrd is calculated by multiplying the individual usable field capacities of the soil types by the effective root zone thickness 5 ufc rd mm ufc mm dm crop rooting depth dm the crop rooting depth rd is fixed at a depth of 11 dm for gleysol and stagnosol the ufcrd is a measure for soil water available to plants the field capacity is determined analogously a mean storage density of 1 55 1 8 g m3 is defined by the soil mapping guide ad hoc ag boden 2005 for further classification the storage density is assigned to dry bulk densities pt based on the dry bulk densities combined with the individual soil types the usable field capacity can be determined the proportion of organic matter and the proportion of coarse soil must be considered in the calculation for this purpose the humus content is converted into organic matter content table 1 the coarse soil content is to be considered because larger skeletal components in the soil have a negative effect on the field capacity the average coarse soil content is 18 sd 22 29 it differs for each test field from 0 to 68 due to high variability the residence time is calculated from the deep infiltration quantity and the field capacity under the root zone the field capacity is used because there is no influence by plant roots in this zone 6 t soil fc d pv where tsoil is the residence time of the percolating water a pv is the percolation velocity mm a fc is the field capacity mm dm d is the layer thickness of the zone dm an average value is calculated for the field capacity of the zone for each area this represents the averaged field capacity of the individual soil layers per field profile it is multiplied by the layer thickness of the deep vadose zone and divided by the amount of percolating water eq 6 2 5 nitrogen input and leaching it has been shown that an accurate estimation is achieved with the help of n demand determination patterns it is an alternative representation of surplus no3 at the end of the growing season carey et al 2017 determining the real n surplus is difficult due to various factors the heterogeneity of the different sites and their different management allow wide ranges of n surpluses the measured autumn nmin content in the soil provides information on the amount of n in the soil this value represents the available soil n which can potentially leach out with the percolating water the autumn nmin value is influenced by various factors the most important of which are crop type harvesting technique and the mineralization potential of the soil sullivan and cogger 2003 it is measured shortly before the formation of percolating water due to high autumn precipitation the relationship between n balance nmin and no3 concentration in the leachate is described in bechtel 2008 other studies use the nmin to estimate the soil net n on a larger scale risch et al 2019 several authors were able to estimate no3 discharge by using autumn nmin scheffer 1999 bechtel 2008 in this study the average autumn nmin is derived from the calculation of 21 arable land areas these are from the cultivation years 2016 to 2018 with average nmin values of 85 kg n ha a potential no3 discharge the amount of n leached into the deep vadose zone depends on the exchange frequency ef the higher the frequency ef the more no3 can potentially be washed out to determine the potentially leachable excess n eqs 7 to 9 are used nlwkn 2010 the leachable excess n consists of the individual n species no3 no2 nh4 nh3 and nh4 eq 7 describes the percentage of soil water that is exchanged within one year the simplified case of uniform downward displacement of water is assumed values above 100 represent a complete exchange of the soil water the exchange frequency ef is 1 for values below 100 the respective value corresponds to the percentage washout a the calculation is based on the published recommendations of the lower saxony state office for water management coastal and nature conservation nlwkn 2010 these are calculated according to 7 ef gwr fc rd 100 where ef is the exchange frequency of soil water gwr is the groundwater recharge mm a fcrd is the field capacity in the rooted soil mm a is ef 100 if ef 100 and ef 1 if ef 100 if the sample depth does not correspond to the depth of the root horizon the exchange frequency must be adjusted in percentage to the depth 8 based on the sample depth of the nmin data the leaching of the measured nmin value of the soil is calculated as a percentage 8 a rh a rd where arh is the depth of root horizon leaching m a is the exchange percentage rd is the root horizon thickness m the potential leaching is then calculated by multiplying the depth of leaching of the root horizon by the depth of nmin sampling multiplied by the measured nmin 9 a pot a rd nmin t nmin n where apot is the potential soil n leaching kg n ha ard is the depth of root horizon leaching m nmint is the depth of nmin sample m nminn is the measured excess n kg n ha it is assumed that the nh4 n content is of minor importance nlwkn 2010 to predict the no3 concentration in the percolating water the autumn nmin value is converted according to 10 pw a pot 443 gwr where pw is the no3 concentration in the percolating water mg l apot is the potential leaching of n from the soil kg n ha gwr is the groundwater recharge mm a 443 is the conversion factor 4 43 of no3 with the factor 100 2 6 estimating denitrification potential the degradation capacity of the zone below the root zone is based on the classification of the maximum potentials of the soil according to nlkwn 2010 for the calculation the deep vadose zone under the soil type gleyic luvisol is set to a maximum denitrification capacity dmax of 10 kg n ha a the zones below gleysol and stagnosol were set to a dmax of 30 kg n ha a at these sites new organic matter can be introduced into the vadose zone leading to anaerobic no3 reducing conditions therefore the potential for denitrification is substantially increased evaluation of the denitrification potential of individual soil types is taken from müller and raissi 2002 the classifications are each at the lower limit of the classes such that the potential is estimated rather conservatively 2 7 model parameterization the denuz model approach assigns a pair of dmax and k values to each denitrification level according to köhne and wendland 1992 the constant k is set to values between 18 7 kg n ha a good denitrification conditions and 2 5 kg n ha a poor denitrification conditions in this study the constant k is set to 2 5 kg n ha a at a dmax of 10 kg n ha a and to 4 kg n ha a at a dmax of 30 kg n ha a köhne and wendland 1992 the percentage of denitrification loss is given by 11 d soil n t n 0 100 where dsoil is the denitrification loss in the soil n t is the n content in the soil after the percolation time t kg n ha a n0 is the n content in the soil kg n ha a that is equivalent to nmin n t is solved iteratively according to eq 1 thus the reduction over time is simulated the n discharge from the vadose zone is calculated by 12 n g n 0 n t where ng is the n discharge after the denitrification kg n ha a n t is the n content in the soil after the percolation time t kg n ha a n0 is the n discharge in the soil kg n ha a 3 results 3 1 processes in the vadose zone hydromorphic soil characteristics may give an indication of reduced conditions these occur in almost all profiles as bleached or marked green grey to blue grey colors fig 4a and b show the decrease of no3 concentrations in soil solutions towards depth with a constantly low so4 2 concentration in the leachate fig 4a shows a profile of land section 6 cf fig 1 on which summer wheat and turnips were cultivated from 2016 to 2018 on land section 27 fig 4b cf fig 1 a crop sequence of wheat arable grass maize was cultivated in the same period the soil type in land section 6 can be assigned as gleyic luvisol and in land section 27 as gleysol 3 2 nitrogen input for the n discharge the average of all profiles from autumn nmin is considered the rooting depth is 1 1 m a calculated average groundwater recharge of 202 mm a is used the average field capacity for all areas is 365 mm table 2 annually an average of 57 kg n ha no3 and nh4 combined is washed into the deep vadose zone that begins in a depth of 70 bis 110 cm below the surface 3 3 relative denitrification the input value is 57 kg n ha a the different denitrification rates in the zones are shown in fig 5 while the vadose zone at the gleysol stagnosol sites has denitrified 90 of the n after a percolating water residence time of almost two years the zone at the gleyic luvisol sites requires more than five years for the same percentage the average retention time of the percolating water under the gleyic luvisol soil is 2 years that of the stagnosol gley 1 8 years table 1 this corresponds to an average reduction of 34 of the input n in the deep vadose zone at the gleyic luvisol sites while for the other sites average reduction is 63 consequently at the gleyic luvisol sites an average of 37 7 kg n ha a remains as residual n which can potentially be introduced into groundwater with the percolating water at the stagnosol sites 23 4 kg n ha a remain this corresponds to a residual amount of 41 to 66 of the originally calculated input n the potential of denitrification below the root zone has been calculated individually for each arable site to make a general comparison the average of the last half meter before groundwater is used this average no3 concentration is 85 mg l the calculated value for an average excess after denitrification is 35 kg n ha a which corresponds to a no3 concentration of 79 mg l the deviation of the no3 concentrations of the calculated values to the measured ones is therefore only 6 mg l or about 7 in fig 6 the concentration curve of no3 is shown as an average development of all arable sites with depth at a depth of 4 m the average measured concentration is 90 mg l 11 profiles go down to a depth of 4 m which still allows for a good comparison overall measured concentrations decrease from 121 mg l to 90 mg l this corresponds to a decrease of 31 mg l over 3 m thickness or converted to the n input 14 kg n ha a using a percolating water quantity of 202 mm a 4 discussion estimations of no3 reduction in the unsaturated zone and the groundwater body often do not consider the deep vadose zone below the root zone or estimate them to be negligible fraters et al 2006 chen et al 2018 often transport modelling is done in the vadose zone rock and kupfersberger 2019 even combine a 1d simwaser stotrasim model with a 2d vadose zone feflow model groundwater in addition calculations of no3 storage and travel time in the vadose zone are equally common turkeltaub et al 2020 predict regional scale groundwater recharge and no3 storage and likewise turkeltaub et al 2018 calculate the travel time of no3 through the vadose zone however modelling of denitrification is very rare our results indicate that this zone appears to be of underestimated importance in this regard and should therefore be included in nutrient cycle considerations the soil types are crucial as they can bring microbially usable organic matter with the percolating water into deeper vadose zones since measured laboratory data are available from the soil profiles an estimation can be made of how realistic the presented model is and where it is possibly limited or has optimization potential the amount of n in the soil can change significantly due to natural processes the most relevant of which are organotrophic and lithotrophic denitrification the calculated input using the measured autumn nmin values results in an input value which is taken as an average value for all fields the calculated amount of percolating water and the average field capacity in the root zone as well as the residual n quantity are included in the calculation the equation according to renger 2002 for the calculation of the quantity of percolating water for the field sites has a high multiple pearson correlation coefficient r of 0 84 eqs 2 and 3 eqs 2 4 show that the depth infiltration increases with increasing potential precipitation but decreases with increasing plant available soil water and potential evapotranspiration consequently a decrease in depth infiltration can be expected in the future as potential evapotranspiration will increase which will be triggered by rising temperatures in the course of climate change ortmeyer et al 2021 high positive correlation coefficients r show that the relationship between deep infiltration and the used climate and soil characteristics are relatively narrow the standard deviation for the calculated deep infiltration values is 20 30 mm a the pedological recording of the depth profiles provides only a punctual insight into the subsoil of the areas at each drilling point there may be strong deviations from layer thickness to the structure of the subsoil the calculated residence time for 60 of the investigated areas is 1 to 2 years for 2 sites it is between 3 and 3 5 years which is due to the higher groundwater table depths of 4 and 5 m akbariyeh et al 2018 similarly point out that a consideration of the groundwater level plays an important role if the groundwater level rises it is easier to transport a large amount of n in the form of no3 into the groundwater similarly a study by juntakut et al 2019 indicates aquifers with relatively lower no3 concentrations in areas with thicker vadose zones nevertheless ascott et al 2017 demonstrate long travel times in the vadose zone in areas with thick vadose zones and extensive historical agriculture so its no3 leaching may take a long time to occur and measures such as a change in agricultural practices may have a delayed effect in any case mass balances show that the excess n corresponds very well to the annual n accumulation in the soil profiles baram et al 2017 the michaelis menten kinetics models the degradation of n under idealized conditions the denitrification rate k which describes the range from which the no3 reduction is limited by the no3 concentration itself appears to play a minor role in the soil type gleyic luvisol in the case of very high n surpluses with low degradation rates k becomes a small factor the calculated leaching potential was determined using calculated average values an input of 57 kg n ha a results in a concentration of 125 mg l no3 in the percolating water eq 10 the coarse soil fraction and the humus fraction in the sediment were taken into account for the percolating water retention time the sediment plays a decisive role in the residence time of the vadose zone and thus also in the time in which denitrification can take place a coarse soil texture is prone to high no3 leaching turkeltaub et al 2016 derby et al 2009 also indicate that high no3 concentrations occur in sandy soil even when very conservative n rates are used intensive irrigation increases the rate of no3 leaching to groundwater juntakut et al 2019 a study by akbariyeh et al 2018 shows sediment types with higher saturated hydraulic conductivity and lower residual water content have lower water holding capacity which increased both water infiltration rates and no3 leaching rates in the present study measured laboratory value in the first 0 5 m below the root horizon is 121 mg l on average for all profiles so there is a deviation of only 3 between modelled and measured data sites 24 25 and 26 were not considered because they are not cultivated arable land fig 7 shows a summarized comparison of the calculated and the measured results the input of n as well as the calculated values of the usable field capacity the field capacity the amount of percolating water the resulting no3 concentration and its reduction below the root zone are altogether consistent calibration of the model with measured laboratory data can therefore be considered successful the unit mg l was chosen to understand the potential input of no3 into the aquifer by the percolating water the no3 measured by the nmin is dissolved and carried into the deep vadose zone under the root zone the comparison fig 7 shows a good agreement of the results with the real measured values 5 conclusion the no3 problem in groundwater and drinking water has aggravated over the last decades ward et al 2018 due to intensive agriculture high n doses reach the soil which are subject to transport and transformation processes in the vadose zone to quantify the n input into the soil zone the autumn nmin is used which provided the information necessary for calculating no3 shift and thus the residence time in the unsaturated zone denitrification potential in the unsaturated zone is determined according to the denuz model wendland 2010 the usable field capacities and the resulting groundwater recharge were calculated soil types were determined and assigned a maximum denitrification rate these data were combined with the n input and the degradation kinetics of the michaelis menten equation this resulted in the degradation potential below the root zone of the individual studied sites average calculated values were in good agreement with the averaged measured laboratory results we conclude that our initial hypothesis substantial no3 degradation in the deep vadose zone leading to unexpectedly low groundwater conditions despite massive surface n input can be verified the developed modelling approach is a promising tool to assess n degradation in the vadose zone it can be applied for an improved forecast of no3 levels in groundwater in affected areas using relatively easily determinable input data it investigates a section of the n flux that is mostly neglected focusing on the previously often underestimated denitrification potential in the deep vadose zone between the root zone and the groundwater table this zone should be increasingly taken into account in future studies on the nitrate problem still one of the most pressing groundwater quality concerns on a global scale declaration of competing interest authors declare that there is no conflict of interest acknowledgments we would like to thank new niederrheinwasser gmbh esp roland schindler and katharina greven for providing extensive hydrogeochemical and soil data and for discussions appendix a supplementary data supplementary material image 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jconhyd 2021 103843 
1384,among the different factors that influence the liquid solid adsorption technique equilibrium time is one of the most relevant and requires a large number of experiments over a long period of time for its determination this work evaluates the southwell plot as a further tool that can contribute to determining the equilibrium time in adsorption processes it can also optimize the operating conditions in a batch system for the removal of phosphate in adsorbents produced from domestic sewage sludge and clam shell residue sewage sludge and clam shell residues were ground sieved and sintered at 700 c for 1 h the material was characterized by thermal analyses tg dtg chemical analysis edx x ray diffraction xrd fourier transform infrared spectroscopy ftir and adsorption studies the kinetic studies were investigated by varying the initial concentration of the phosphate solution and mass of the adsorbent the equilibrium time was determined by applying the southwell plot method to the kinetic data and the results showed some fluctuations as a function of the adsorbent mass at 0 30 g of the adsorbent in 30 ml of the phosphate solution regardless of the initial phosphate concentration the equilibrium time determined by the southwell plot was 4 h the maximum phosphate adsorption capacity in this condition determined by the langmuir equation was 49 45 mg g 1 keywords southwell plot method sewage sludge phosphate adsorption 1 introduction adsorption as a pollutant removal technique in water and wastewater treatment is a process widely investigated due to its versatility high efficiency and low cost singh et al 2018 the adsorption process is especially relevant in this environmental area as it can be used to remove various contaminants found in effluents such as metals dyes medications pesticides pathogenic microorganisms among others burakov et al 2018 martucci et al 2012 ferronato et al 2015 kanjilal et al 2014 although easy to apply the adsorption process requires the analysis of many variables at the same time such as the amount of adsorbent dosage initial solute concentration ph of the solution agitation speed equilibrium time among others in order to determine the parameters of the operation ruthven 1984 these variables are initially tested on a small scale in laboratories with the execution of many preliminary experiments which uses a large amount of inputs energy and financial resources it is known that the adsorption capacity defines the performance of an adsorbent however it is the kinetic performance that defines the equilibrium time and the production scale of industrial adsorbents qiu et al 2009 largittea and pasquier 2016 schwaab et al 2017 weber and smith 1987 ho 2006 in 1932 r v southwell proposed a semi empirical method that provides for the determination of critical theoretical buckling loads in perfect columns based on the application of deflection theories of elastic materials and empirical data in real systems singer 1989 buckling is considered to be an elastic instability and occurs when the part undergoes transversal flexion due to axial compression the buckling capacity is determined by the lateral displacement of a column when an axial force is applied on it during the performance of non destructive tests this method is applied to determine the strength and stiffness of a real structure where the load used in the column remains within the elastic limit mandal and calladine 2002 ariaratnam 1961 the southwell plot method can be described by eq 1 which establishes a linear relationship between the lateral deflection h the applied load n critical load pcr and the initial mid height lateral deformation lateral a bernat maso et al 2014 1 h n h p cr a p cr eq 1 can be applied to the data of the equilibrium trajectory of a column subjected to applied load n and the critical buckling load is determined by the inverse of the slope of the line adjusted between the points h n and h the simplicity of the method makes its application to experimental data in parametric studies attractive despite the fact that the model does not consider the imperfection effects that may appear in real systems during experiments such as the heterogeneity of the material and the disturbances generated during the study of buckling singer 1989 some studies in the literature demonstrate the efficiency of the southwell method to determine column buckling loads in different systems such as cold formed steel portal frames blum and rasmussen 2018 pultruded fiber reinforced polymer beams nguyen et al 2014 reinforced concrete beams kalkan 2010 and laminated glass columns liu et al 2017 the equilibrium trajectories of the column buckling phenomenon are very similar to the curves obtained by kinetic studies in an adsorption process therefore just as the critical buckling load on the columns is estimated our hypothesis is that by analogy the stabilization in the adsorption process defined by the amount adsorbed qe at equilibrium time t can also be estimated more accurately the proposal for applying the southwell plot method in an adsorption process consists in the construction of a plot time t adsorbed quantity qe vs time t and from there the inverse of the slope of the adjusted line between these points will indicate the value of qe that determines the asymptotic point that is the value at which it stabilizes the value of qe found for the entire set of experimental data will be compared with the value at each time t the equilibrium time will be defined by analyzing the errors found at each point thus the objective of this work is to evaluate the application of the southwell plot method to determine the equilibrium time in the removal of phosphate in adsorbents produced from domestic sewage sludge and shell residue which is a system of great interest from an environmental point of view domestic sewage sludge in addition to having high carbon content also presents mineral compounds containing si al fe ca and others which allow different types of active sites that favor phosphate removal from aqueous medium smith et al 2009 xu et al 2015 the use of calcium oxide containing residues such as shell residues in the preparation of the adsorbent to improve phosphate removal is a more sustainable strategy kong et al 2018 souza et al 2018 2 material and methods 2 1 materials the sewage sludge ms01 was provided by a sewage treatment plant located in feira de santana state of bahia brazil ete jacuípe ii waste clam shells rcm were used as a source of calcium and were collected from artisanal fishing areas located in maragogipe state of bahia brazil the residues were obtained from the bivalve clam shells anomalocardia brasiliana and tagelus plebeius the solutions were prepared using monobasic sodium phosphate nah2po4 h2o 98 synth brazil and stored in dark flasks to prevent any photodecomposition 2 2 preparations the method used to produce the adsorbent was adapted from souza et al 2018 the dry sewage sludge ms01 and waste clam shells rcm 10 w w clam were ground and sieved 80 mesh and sintered in a muffle at 700 c for 1 h the adsorbent prepared by this method was named mad10 2 3 characterization the chemical composition of the adsorbent mad10 was determined on a shimadzu edx720 energy dispersive x ray fluorescence spectrometer operating with a rhodium source using a semi quantitative method thermal analysis of materials was made using a ta model sdt q600 from 25 to 1000 c with a heating rate of 10 c min 1 and an air flow of 50 ml min 1 flow of synthetic air the x ray diffraction patterns of the samples were collected on a shimadzu xrd 6000 operating with cukα radiation at a voltage of 40 kv current of 30 ma and graphite monochromator in the region of 1 4 50 2θ at scan rate of 2 min 1 fourier transform infrared spectroscopy was used to identify the functional groups present on the surface of the adsorbent the spectra were obtained using a spectrometer perkin elmer spectrum uatr two massachusetts usa with spectrum between 4000 and 400 nm 1 2 cm 1 resolution 20 scans with samples prepared in kbr pellets 2 4 kinetic studies the kinetic studies were investigated by varying the initial concentration of the phosphate solution and mass of the adsorbent the adsorption kinetics studies were performed in duplicate and the suspensions were separated by vacuum filtration on 0 45 μm cellulose acetate membrane the determination of the phosphate concentration was performed using a tu 1880 double beam uv vis spectrophotometer at a wavelength λ of 880 nm according to the molybdenum blue method eaton et al 2005 the effect of the initial phosphate solution concentration on the adsorption kinetics was evaluated using three different concentrations 30 mgp l 1 50 mgp l 1 and 100 mgp l 1 and the mass of adsorbent varied between 0 1 and 0 3 g in 30 ml of the phosphate solution the kinetic studies were conducted with ph values around 7 0 7 5 in order to work in conditions close to ph values in natural aquatic systems the adsorption kinetics was studied using the pseudo first order schwaab et al 2017 ho and mckay 1998 pseudo second order schwaab et al 2017 ho 2006 and the elovich kinetic models schwaab et al 2017 chien and clayton 1980 table 1 where qt and qe represent the capacity of phosphorous mg g 1 adsorbed at time t min and equilibrium respectively k1 is the rate constant of the pseudo first order min 1 and k2 is the pseudo second order rate constant g mg 1 min 1 for comparative effect the models were considered the best fit according to the analysis of the correlation coefficient r2 2 5 adsorption studies the adsorption studies were performed using 0 30 g of adsorbent in 30 ml of the phosphate solution and concentrations varying between 30 and 400 mg l 1 the equilibrium time was determined by the southwell plot method the determination of the phosphate concentration was performed using a tu 1880 double beam uv vis spectrophotometer at a wavelength λ of 880 nm according to the molybdenum blue method eaton et al 2005 to evaluate maximum amount of phosphate adsorbed the equilibrium data were adjusted to the langmuir hamdaouia and naffrechoux 2007 and freundlich foo and hameed 2010 models table 2 where ce is the equilibrium concentration of phosphate solution mg l 1 qe is the adsorbed capacity at the equilibrium mg g 1 qm mg g 1 and kl mg g 1 are the langmuir constants n and kf mg g 1 are the freundlich constants for comparative effect the models were considered the best fit according to the analysis of the correlation coefficient r2 3 results and discussion 3 1 characterization of the precursors ms01 and rcm and adsorbent mad10 the mad10 adsorbent was produced by combining domestic sewage sludge ms01 and clam shell residues rcm and the curves of thermogravimetric analysis tg and dtg are presented in fig 1a and b respectively the sewage sludge used in this work showed a total weight loss of 73 that must consist of water and volatile solids including the organic matter characteristic of domestic sewage sludge the decomposition profile of the ms01 shows the main characteristic steps for combustion of domestic sludge i t 200 c due to the loss of adsorbed water ii 200 c t 500 c corresponding to the biodegradable material and iii t 500 c attributed to more stable and non degradable inorganic material font et al 2001 the mass loss of mad10 adsorbent occurs in temperature ranges slightly altered in relation to its precursors but with two important highlights i the presence of calcium carbonate caco3 contributed to the total loss reduced to about 61 resulting in a greater stability for the adsorbent ii in the rcm curve a loss of 43 in mass is observed due to the decomposition of caco3 in the region of 700 800 c checa et al 2005 but in the adsorbent this process occurs at lower temperatures between 650 and 750 c the mad10 sample was calcined at a temperature of 700 c based on these tg results and the produced adsorbent kept the name mad10 the chemical composition of precursors sewage sludge ms01 and waste clam shells rcm and adsorbent mad10 were determined as oxides by edx and are presented in table 3 the compound with the highest concentration in mad10 adsorbent is calcium oxide cao 29 82 due to the addition of rcm material rich in calcium then silicon oxide sio2 with 18 68 due to the presence of common minerals sand silt and clay in the sewage sludge iron oxide fe2o3 and aluminum oxide al2o3 also form the basis of the chemical composition of mad10 the presence of calcium iron and aluminum ions favors the adsorption of phosphate ions in aqueous solutions chen et al 2013 the xrd patterns of the sample are shown in fig 2 where the observed phases are those formed during the sintering step of the adsorbent preparation the sample mad10 has heterogeneity in terms of its crystalline structure due to the variety of inorganic oxides present in its composition as shown by the edx results the phases identified by x ray diffraction patterns as a function of oxide concentration and detection limit of the equipment were α quartz sio2 calcite caco3 and hematite α fe2o3 souza et al 2018 3 2 kinetics studies the kinetics of phosphate adsorption on the mas10 adsorbent was investigated considering the dosage adsorbent and initial phosphate concentration the results are shown in fig 3 regardless of the initial solute concentration the increase in adsorption rate with increasing solute concentration is expected due to the higher amount of solute in the aqueous medium mezenner and bensmaili 2009 this behavior confirms the efficiency of the mad10 material synthesized from waste with adsorbent potential in phosphate removal at the beginning of the process the adsorption rate is high and gradually decreases over time due to the saturation of available sites the kinetics of phosphate adsorption on the mad10 adsorbent was investigated and the data were fit to the pseudo first order pseudo second and elovich order model details can be found in the supplementary material a and the results are presented in table 4 the values of the correlation coefficients r2 presented in table 4 indicate that the experimental data are adjustable to the three studied kinetic models r2 0 95 however some differences appear with the increase in the adsorbent dosage the kinetic data obtained for a mass of 0 10 g are adjustable to practically all kinetic models through the values of r2 that are very close except for concentration of 100 mg l 1 with 0 90 r2 0 93 in this case the pseudo first order model seems the most suitable to describe the kinetics of the system with r2 0 9282 this result demonstrates that the effects of the diffusion and chemical reaction processes are inseparable during solute adsorption qiu et al 2009 the mass transport effect should affect the rate values for concentrations of 30 mg l 1 and 50 mg l 1 but not enough to minimize the effect of the chemical affinity between the solute and mad10 adsorbent adsorption sites as can be observed by adjusting the three kinetic models with r2 0 96 the same behavior is observed for a mass of 0 15 g with the increase in the mass of the adsorbent to 0 25 g and 0 30 g regardless of the initial concentration the kinetic data are better adjusted to the pseudo second order and elovich models this indicates that the possible chemical reactions on the surface of the adsorbent contribute more significantly to rates than the diffusion process 3 3 southwell plot method the southwell plot method was applied to the kinetic data of phosphate adsorption in the mad10 adsorbent to define the equilibrium time the experimental data time t and adsorption capacity at time qe were used to construct the graphic t qe vs t and the inverse of the line slope adjusted between these points indicated the value of qe adsorption capacity determined by the asymptotic point that is the value at which it stabilizes the adsorption capacity qe found for the entire set of experimental data was compared with the value of the amount adsorbed qt at each time t by the values of the errors the error was calculated by the simple expression of relative error considering the value of qe in the maximum time as the accepted value and the qt in each time eq 2 2 error q t q e q e x 100 details can be found in the supplementary material b and the results are shown in table 5 the analysis of the results was performed through the values of simple errors within a limit of 10 a considerable value as a standard error for the method ko 1987 as can be seen in table 5 the greatest fluctuations were observed for the lowest values of the adsorbent dosage the system with 0 10 g of mad10 proved to be quite unstable and only reached equilibrium after 540 min for 0 15 g and 0 25 g initial concentration of 100 mg l 1 the fluctuations start to decrease and after 420 min stability in the adsorbed quantity is observed the best result was obtained with the mass 0 30 g of the adsorbent in which the equilibrium time is reached in 240 min observation was carried out at 24 h to confirm the stabilization predicted with the southwell plot method the values observed experimentally remained within the range of variation of the errors considering the predicted value as average the application of the southwell plot to the kinetic data seems to be a promising alternative to define the equilibrium time with greater precision than the simple observation of the plateaus in the graphs of qe vs t see fig 3 for validation of the southwell plot method we suggest performing tests on adsorbents with a more uniform structure so that the influence of the adsorbent dosage can be evaluated a very heterogeneous material was used for mad10 as can be seen in the results of drx and edx and this composition may have influenced the oscillations observed in lower dosages of the adsorbent 3 4 adsorption studies equilibrium data were obtained for a time of 4 h as defined by the southwell plot method shown in fig 4 the langmuir and freundlich models were applied to obtain the adsorption isotherm parameters see table 6 the r2 correlation values were used to determine which model best fit the equilibrium data and the results showed that the two models fit r2 0 95 satisfactorily however the freundlich model fits the adsorption data even better r2 0 9916 than the langmuir model this suggests that the phosphorus chemisorption on the surface of mad10 is prevalent as already reported in other studies souza et al 2018 ajmal et al 2018 the value of n 1 n 1 61 reinforces this the maximum amount adsorbed by the langmuir equation was 49 45 mg g 1 and this adsorption capacity is comparable to other studies as can be seen in table 7 despite the different experimental conditions it is possible to observe that the adsorption capacity of mad10 in 4 h is greater than others in the same period or even in longer equilibrium times the analysis of the kinetics and balance data performed by the appropriate mathematical models indicate phosphorus chemisorption on the surface of mad10 the infrared technique was performed on the samples before and after adsorption to investigate this assumption see fig 5 fig 5 presents a comparison between the dry domestic sewage sludge ms01 the adsorbent prepared by combining the sewage sludge and the clam shell residues before mad10 and after mad10 p phosphate adsorption the bands observed in sample ms01 correspond to the characteristic bands of fulvic and humic acids present in the sewage sludge stevenson 1994 the range from 3600 to 2500 cm 1 corresponds to the oh and ch stretches present in the aliphatic hydrocarbon structures ros et al 2006 and the range from 1500 to 1800 cm 1 corresponds to the carbonyl group c c c o coo typical of carbonates ketones and carboxylic acids chen et al 2012 these bands appear altered in the mad10 sample due to the presence of calcium carbonate caco3 from the rcm the band at 1646 cm 1 almost disappears and the band in the coo antisymetric stretching group moves to 1437 cm 1 after phosphate adsorption an increase in the intensity of this band by 1437 cm 1 is observed indicating the presence of phosphate as well as changes in the bands in the range from 1000 to 1250 cm 1 characteristic of the stretching vibration of po and absorption of p o kong et al 2018 these modifications confirm the adsorption of phosphate on mad 10 4 conclusions the mad10 adsorbent was synthesized by grinding and sintering the combination of waste materials sewage sludge and clam shells the adsorption studies showed that this material was efficient in removing phosphate in solutions with a neutral ph around 7 5 close to that of a contaminated effluent the equilibrium time was determined by applying the southwell plot method to the kinetic data and the results showed some fluctuations as a function of the adsorbent mass at 0 30 g of the adsorbent regardless of the initial phosphate concentration the equilibrium time determined by the southwell plot was 4 h the maximum phosphate adsorption capacity in this condition determined by the langmuir equation was 49 45 mg g 1 the kinetic data are better adjusted to the pseudo second order and elovich models indicating that possible chemical reactions on the surface of the adsorbent contribute to the rates more significantly than the diffusion process declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements maíra luane s de almeida would like to thank capes coordenação de aperfeiçoamento de pessoal de ensino superior ministério da educação brazil for the scholarship laboratório de catálise e materiais departamento de química geral e inorgânica instituto de química universidade federal da bahia ufba for the analyses this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors appendix a supplementary data supplementary material 1 image 1 supplementary material 2 image 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jconhyd 2021 103841 
1384,among the different factors that influence the liquid solid adsorption technique equilibrium time is one of the most relevant and requires a large number of experiments over a long period of time for its determination this work evaluates the southwell plot as a further tool that can contribute to determining the equilibrium time in adsorption processes it can also optimize the operating conditions in a batch system for the removal of phosphate in adsorbents produced from domestic sewage sludge and clam shell residue sewage sludge and clam shell residues were ground sieved and sintered at 700 c for 1 h the material was characterized by thermal analyses tg dtg chemical analysis edx x ray diffraction xrd fourier transform infrared spectroscopy ftir and adsorption studies the kinetic studies were investigated by varying the initial concentration of the phosphate solution and mass of the adsorbent the equilibrium time was determined by applying the southwell plot method to the kinetic data and the results showed some fluctuations as a function of the adsorbent mass at 0 30 g of the adsorbent in 30 ml of the phosphate solution regardless of the initial phosphate concentration the equilibrium time determined by the southwell plot was 4 h the maximum phosphate adsorption capacity in this condition determined by the langmuir equation was 49 45 mg g 1 keywords southwell plot method sewage sludge phosphate adsorption 1 introduction adsorption as a pollutant removal technique in water and wastewater treatment is a process widely investigated due to its versatility high efficiency and low cost singh et al 2018 the adsorption process is especially relevant in this environmental area as it can be used to remove various contaminants found in effluents such as metals dyes medications pesticides pathogenic microorganisms among others burakov et al 2018 martucci et al 2012 ferronato et al 2015 kanjilal et al 2014 although easy to apply the adsorption process requires the analysis of many variables at the same time such as the amount of adsorbent dosage initial solute concentration ph of the solution agitation speed equilibrium time among others in order to determine the parameters of the operation ruthven 1984 these variables are initially tested on a small scale in laboratories with the execution of many preliminary experiments which uses a large amount of inputs energy and financial resources it is known that the adsorption capacity defines the performance of an adsorbent however it is the kinetic performance that defines the equilibrium time and the production scale of industrial adsorbents qiu et al 2009 largittea and pasquier 2016 schwaab et al 2017 weber and smith 1987 ho 2006 in 1932 r v southwell proposed a semi empirical method that provides for the determination of critical theoretical buckling loads in perfect columns based on the application of deflection theories of elastic materials and empirical data in real systems singer 1989 buckling is considered to be an elastic instability and occurs when the part undergoes transversal flexion due to axial compression the buckling capacity is determined by the lateral displacement of a column when an axial force is applied on it during the performance of non destructive tests this method is applied to determine the strength and stiffness of a real structure where the load used in the column remains within the elastic limit mandal and calladine 2002 ariaratnam 1961 the southwell plot method can be described by eq 1 which establishes a linear relationship between the lateral deflection h the applied load n critical load pcr and the initial mid height lateral deformation lateral a bernat maso et al 2014 1 h n h p cr a p cr eq 1 can be applied to the data of the equilibrium trajectory of a column subjected to applied load n and the critical buckling load is determined by the inverse of the slope of the line adjusted between the points h n and h the simplicity of the method makes its application to experimental data in parametric studies attractive despite the fact that the model does not consider the imperfection effects that may appear in real systems during experiments such as the heterogeneity of the material and the disturbances generated during the study of buckling singer 1989 some studies in the literature demonstrate the efficiency of the southwell method to determine column buckling loads in different systems such as cold formed steel portal frames blum and rasmussen 2018 pultruded fiber reinforced polymer beams nguyen et al 2014 reinforced concrete beams kalkan 2010 and laminated glass columns liu et al 2017 the equilibrium trajectories of the column buckling phenomenon are very similar to the curves obtained by kinetic studies in an adsorption process therefore just as the critical buckling load on the columns is estimated our hypothesis is that by analogy the stabilization in the adsorption process defined by the amount adsorbed qe at equilibrium time t can also be estimated more accurately the proposal for applying the southwell plot method in an adsorption process consists in the construction of a plot time t adsorbed quantity qe vs time t and from there the inverse of the slope of the adjusted line between these points will indicate the value of qe that determines the asymptotic point that is the value at which it stabilizes the value of qe found for the entire set of experimental data will be compared with the value at each time t the equilibrium time will be defined by analyzing the errors found at each point thus the objective of this work is to evaluate the application of the southwell plot method to determine the equilibrium time in the removal of phosphate in adsorbents produced from domestic sewage sludge and shell residue which is a system of great interest from an environmental point of view domestic sewage sludge in addition to having high carbon content also presents mineral compounds containing si al fe ca and others which allow different types of active sites that favor phosphate removal from aqueous medium smith et al 2009 xu et al 2015 the use of calcium oxide containing residues such as shell residues in the preparation of the adsorbent to improve phosphate removal is a more sustainable strategy kong et al 2018 souza et al 2018 2 material and methods 2 1 materials the sewage sludge ms01 was provided by a sewage treatment plant located in feira de santana state of bahia brazil ete jacuípe ii waste clam shells rcm were used as a source of calcium and were collected from artisanal fishing areas located in maragogipe state of bahia brazil the residues were obtained from the bivalve clam shells anomalocardia brasiliana and tagelus plebeius the solutions were prepared using monobasic sodium phosphate nah2po4 h2o 98 synth brazil and stored in dark flasks to prevent any photodecomposition 2 2 preparations the method used to produce the adsorbent was adapted from souza et al 2018 the dry sewage sludge ms01 and waste clam shells rcm 10 w w clam were ground and sieved 80 mesh and sintered in a muffle at 700 c for 1 h the adsorbent prepared by this method was named mad10 2 3 characterization the chemical composition of the adsorbent mad10 was determined on a shimadzu edx720 energy dispersive x ray fluorescence spectrometer operating with a rhodium source using a semi quantitative method thermal analysis of materials was made using a ta model sdt q600 from 25 to 1000 c with a heating rate of 10 c min 1 and an air flow of 50 ml min 1 flow of synthetic air the x ray diffraction patterns of the samples were collected on a shimadzu xrd 6000 operating with cukα radiation at a voltage of 40 kv current of 30 ma and graphite monochromator in the region of 1 4 50 2θ at scan rate of 2 min 1 fourier transform infrared spectroscopy was used to identify the functional groups present on the surface of the adsorbent the spectra were obtained using a spectrometer perkin elmer spectrum uatr two massachusetts usa with spectrum between 4000 and 400 nm 1 2 cm 1 resolution 20 scans with samples prepared in kbr pellets 2 4 kinetic studies the kinetic studies were investigated by varying the initial concentration of the phosphate solution and mass of the adsorbent the adsorption kinetics studies were performed in duplicate and the suspensions were separated by vacuum filtration on 0 45 μm cellulose acetate membrane the determination of the phosphate concentration was performed using a tu 1880 double beam uv vis spectrophotometer at a wavelength λ of 880 nm according to the molybdenum blue method eaton et al 2005 the effect of the initial phosphate solution concentration on the adsorption kinetics was evaluated using three different concentrations 30 mgp l 1 50 mgp l 1 and 100 mgp l 1 and the mass of adsorbent varied between 0 1 and 0 3 g in 30 ml of the phosphate solution the kinetic studies were conducted with ph values around 7 0 7 5 in order to work in conditions close to ph values in natural aquatic systems the adsorption kinetics was studied using the pseudo first order schwaab et al 2017 ho and mckay 1998 pseudo second order schwaab et al 2017 ho 2006 and the elovich kinetic models schwaab et al 2017 chien and clayton 1980 table 1 where qt and qe represent the capacity of phosphorous mg g 1 adsorbed at time t min and equilibrium respectively k1 is the rate constant of the pseudo first order min 1 and k2 is the pseudo second order rate constant g mg 1 min 1 for comparative effect the models were considered the best fit according to the analysis of the correlation coefficient r2 2 5 adsorption studies the adsorption studies were performed using 0 30 g of adsorbent in 30 ml of the phosphate solution and concentrations varying between 30 and 400 mg l 1 the equilibrium time was determined by the southwell plot method the determination of the phosphate concentration was performed using a tu 1880 double beam uv vis spectrophotometer at a wavelength λ of 880 nm according to the molybdenum blue method eaton et al 2005 to evaluate maximum amount of phosphate adsorbed the equilibrium data were adjusted to the langmuir hamdaouia and naffrechoux 2007 and freundlich foo and hameed 2010 models table 2 where ce is the equilibrium concentration of phosphate solution mg l 1 qe is the adsorbed capacity at the equilibrium mg g 1 qm mg g 1 and kl mg g 1 are the langmuir constants n and kf mg g 1 are the freundlich constants for comparative effect the models were considered the best fit according to the analysis of the correlation coefficient r2 3 results and discussion 3 1 characterization of the precursors ms01 and rcm and adsorbent mad10 the mad10 adsorbent was produced by combining domestic sewage sludge ms01 and clam shell residues rcm and the curves of thermogravimetric analysis tg and dtg are presented in fig 1a and b respectively the sewage sludge used in this work showed a total weight loss of 73 that must consist of water and volatile solids including the organic matter characteristic of domestic sewage sludge the decomposition profile of the ms01 shows the main characteristic steps for combustion of domestic sludge i t 200 c due to the loss of adsorbed water ii 200 c t 500 c corresponding to the biodegradable material and iii t 500 c attributed to more stable and non degradable inorganic material font et al 2001 the mass loss of mad10 adsorbent occurs in temperature ranges slightly altered in relation to its precursors but with two important highlights i the presence of calcium carbonate caco3 contributed to the total loss reduced to about 61 resulting in a greater stability for the adsorbent ii in the rcm curve a loss of 43 in mass is observed due to the decomposition of caco3 in the region of 700 800 c checa et al 2005 but in the adsorbent this process occurs at lower temperatures between 650 and 750 c the mad10 sample was calcined at a temperature of 700 c based on these tg results and the produced adsorbent kept the name mad10 the chemical composition of precursors sewage sludge ms01 and waste clam shells rcm and adsorbent mad10 were determined as oxides by edx and are presented in table 3 the compound with the highest concentration in mad10 adsorbent is calcium oxide cao 29 82 due to the addition of rcm material rich in calcium then silicon oxide sio2 with 18 68 due to the presence of common minerals sand silt and clay in the sewage sludge iron oxide fe2o3 and aluminum oxide al2o3 also form the basis of the chemical composition of mad10 the presence of calcium iron and aluminum ions favors the adsorption of phosphate ions in aqueous solutions chen et al 2013 the xrd patterns of the sample are shown in fig 2 where the observed phases are those formed during the sintering step of the adsorbent preparation the sample mad10 has heterogeneity in terms of its crystalline structure due to the variety of inorganic oxides present in its composition as shown by the edx results the phases identified by x ray diffraction patterns as a function of oxide concentration and detection limit of the equipment were α quartz sio2 calcite caco3 and hematite α fe2o3 souza et al 2018 3 2 kinetics studies the kinetics of phosphate adsorption on the mas10 adsorbent was investigated considering the dosage adsorbent and initial phosphate concentration the results are shown in fig 3 regardless of the initial solute concentration the increase in adsorption rate with increasing solute concentration is expected due to the higher amount of solute in the aqueous medium mezenner and bensmaili 2009 this behavior confirms the efficiency of the mad10 material synthesized from waste with adsorbent potential in phosphate removal at the beginning of the process the adsorption rate is high and gradually decreases over time due to the saturation of available sites the kinetics of phosphate adsorption on the mad10 adsorbent was investigated and the data were fit to the pseudo first order pseudo second and elovich order model details can be found in the supplementary material a and the results are presented in table 4 the values of the correlation coefficients r2 presented in table 4 indicate that the experimental data are adjustable to the three studied kinetic models r2 0 95 however some differences appear with the increase in the adsorbent dosage the kinetic data obtained for a mass of 0 10 g are adjustable to practically all kinetic models through the values of r2 that are very close except for concentration of 100 mg l 1 with 0 90 r2 0 93 in this case the pseudo first order model seems the most suitable to describe the kinetics of the system with r2 0 9282 this result demonstrates that the effects of the diffusion and chemical reaction processes are inseparable during solute adsorption qiu et al 2009 the mass transport effect should affect the rate values for concentrations of 30 mg l 1 and 50 mg l 1 but not enough to minimize the effect of the chemical affinity between the solute and mad10 adsorbent adsorption sites as can be observed by adjusting the three kinetic models with r2 0 96 the same behavior is observed for a mass of 0 15 g with the increase in the mass of the adsorbent to 0 25 g and 0 30 g regardless of the initial concentration the kinetic data are better adjusted to the pseudo second order and elovich models this indicates that the possible chemical reactions on the surface of the adsorbent contribute more significantly to rates than the diffusion process 3 3 southwell plot method the southwell plot method was applied to the kinetic data of phosphate adsorption in the mad10 adsorbent to define the equilibrium time the experimental data time t and adsorption capacity at time qe were used to construct the graphic t qe vs t and the inverse of the line slope adjusted between these points indicated the value of qe adsorption capacity determined by the asymptotic point that is the value at which it stabilizes the adsorption capacity qe found for the entire set of experimental data was compared with the value of the amount adsorbed qt at each time t by the values of the errors the error was calculated by the simple expression of relative error considering the value of qe in the maximum time as the accepted value and the qt in each time eq 2 2 error q t q e q e x 100 details can be found in the supplementary material b and the results are shown in table 5 the analysis of the results was performed through the values of simple errors within a limit of 10 a considerable value as a standard error for the method ko 1987 as can be seen in table 5 the greatest fluctuations were observed for the lowest values of the adsorbent dosage the system with 0 10 g of mad10 proved to be quite unstable and only reached equilibrium after 540 min for 0 15 g and 0 25 g initial concentration of 100 mg l 1 the fluctuations start to decrease and after 420 min stability in the adsorbed quantity is observed the best result was obtained with the mass 0 30 g of the adsorbent in which the equilibrium time is reached in 240 min observation was carried out at 24 h to confirm the stabilization predicted with the southwell plot method the values observed experimentally remained within the range of variation of the errors considering the predicted value as average the application of the southwell plot to the kinetic data seems to be a promising alternative to define the equilibrium time with greater precision than the simple observation of the plateaus in the graphs of qe vs t see fig 3 for validation of the southwell plot method we suggest performing tests on adsorbents with a more uniform structure so that the influence of the adsorbent dosage can be evaluated a very heterogeneous material was used for mad10 as can be seen in the results of drx and edx and this composition may have influenced the oscillations observed in lower dosages of the adsorbent 3 4 adsorption studies equilibrium data were obtained for a time of 4 h as defined by the southwell plot method shown in fig 4 the langmuir and freundlich models were applied to obtain the adsorption isotherm parameters see table 6 the r2 correlation values were used to determine which model best fit the equilibrium data and the results showed that the two models fit r2 0 95 satisfactorily however the freundlich model fits the adsorption data even better r2 0 9916 than the langmuir model this suggests that the phosphorus chemisorption on the surface of mad10 is prevalent as already reported in other studies souza et al 2018 ajmal et al 2018 the value of n 1 n 1 61 reinforces this the maximum amount adsorbed by the langmuir equation was 49 45 mg g 1 and this adsorption capacity is comparable to other studies as can be seen in table 7 despite the different experimental conditions it is possible to observe that the adsorption capacity of mad10 in 4 h is greater than others in the same period or even in longer equilibrium times the analysis of the kinetics and balance data performed by the appropriate mathematical models indicate phosphorus chemisorption on the surface of mad10 the infrared technique was performed on the samples before and after adsorption to investigate this assumption see fig 5 fig 5 presents a comparison between the dry domestic sewage sludge ms01 the adsorbent prepared by combining the sewage sludge and the clam shell residues before mad10 and after mad10 p phosphate adsorption the bands observed in sample ms01 correspond to the characteristic bands of fulvic and humic acids present in the sewage sludge stevenson 1994 the range from 3600 to 2500 cm 1 corresponds to the oh and ch stretches present in the aliphatic hydrocarbon structures ros et al 2006 and the range from 1500 to 1800 cm 1 corresponds to the carbonyl group c c c o coo typical of carbonates ketones and carboxylic acids chen et al 2012 these bands appear altered in the mad10 sample due to the presence of calcium carbonate caco3 from the rcm the band at 1646 cm 1 almost disappears and the band in the coo antisymetric stretching group moves to 1437 cm 1 after phosphate adsorption an increase in the intensity of this band by 1437 cm 1 is observed indicating the presence of phosphate as well as changes in the bands in the range from 1000 to 1250 cm 1 characteristic of the stretching vibration of po and absorption of p o kong et al 2018 these modifications confirm the adsorption of phosphate on mad 10 4 conclusions the mad10 adsorbent was synthesized by grinding and sintering the combination of waste materials sewage sludge and clam shells the adsorption studies showed that this material was efficient in removing phosphate in solutions with a neutral ph around 7 5 close to that of a contaminated effluent the equilibrium time was determined by applying the southwell plot method to the kinetic data and the results showed some fluctuations as a function of the adsorbent mass at 0 30 g of the adsorbent regardless of the initial phosphate concentration the equilibrium time determined by the southwell plot was 4 h the maximum phosphate adsorption capacity in this condition determined by the langmuir equation was 49 45 mg g 1 the kinetic data are better adjusted to the pseudo second order and elovich models indicating that possible chemical reactions on the surface of the adsorbent contribute to the rates more significantly than the diffusion process declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements maíra luane s de almeida would like to thank capes coordenação de aperfeiçoamento de pessoal de ensino superior ministério da educação brazil for the scholarship laboratório de catálise e materiais departamento de química geral e inorgânica instituto de química universidade federal da bahia ufba for the analyses this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors appendix a supplementary data supplementary material 1 image 1 supplementary material 2 image 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jconhyd 2021 103841 
