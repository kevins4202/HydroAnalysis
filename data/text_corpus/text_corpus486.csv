index,text
2430,this work evaluates the effects of climate change on the surface water resources river flow of the sanjabi basin iran by comparing data mining lumped and distributed models namely artificial neural networks ann the identification of unit hydrographs and component flows from rainfall evaporation and streamflow ihacres model and the soil and water assessment tool swat climate projections in terms of monthly temperature and rainfall made by 17 atmosphere ocean general circulation models aogcms by the 5th assessment report ar5 of the intergovernmental panel on climate change ipcc under emission scenarios of representative concentration pathways rcps rcp2 6 rcp4 5 and rcp8 5 during the baseline period 1971 2000 and future periods 2040 2069 and 2070 2099 are applied in the sanjabi basin the predictive skill of the aogcms is evaluated with performance criteria the evaluation results indicate the cnrm cm5 model features the best performance in terms of rainfall average temperature and minimum temperature projections and the gfdl cm3 provides the most accurate maximum temperature projections four downscaling methods change factor delta climgen lars wg and genetic programming gp are compared based on the r 2 rmse mae and nse the predictive skill of the lars wg method was the highest ann ihacres and swat are implemented to project future runoff following calibration and testing the ihacres model exhibits the best performance the ihacres model is applied to project future runoff under climate change scenarios the results indicate a reduction in runoff under all emission scenarios in the two future periods with the rcp8 5 scenario featuring the largest reductions in runoff in 2040 2069 and 2070 2099 and being equal to 42 0 and 44 3 respectively keywords climate change river flow delta model climgen model lars wg model gp model ann model ihacres model swat model data availability data will be made available on request 1 introduction increasing burning of fossil fuels for energy generation on the one hand and deforestation and environmental degradation on the other hand have led to an increase in greenhouse gases and rising surface air temperature the alteration of the earth atmosphere radiation balance has modified climatic patterns which is suspected of altering the intensity duration and frequency of precipitation and poses challenges to the utilization of water resources and agriculture in many parts of the world vanuytrecht et al 2012 the ipcc 2001 reported that the average global air surface temperature has risen since 1861 through present time by 0 6 0 2 c temperate increases are predicted to continue unless greenhouse gases concentrations are reduced ipcc 2021 various studies have simulated the effects of climate change on the hydrologic cycle and compared the performance of models applied for this purpose a brief review of studies reporting hydrologic simulations under climate change conditions is presented next afinowicz et al 2005 implemented the swat to evaluate the influence of woody plants on the water budgets in the semi arid karstic basin of the guadalupe river in texas united states li et al 2007 applied the swat in a basin of west africa and showed that changing areas covered with forest grassland and savanna to agricultural lands or urban areas changed the natural hydrological conditions in the basin abbaspour et al 2007 implemented the swat to simulate processes affecting water quantity sediment transport and nutrient loads in the thur river basin switzerland mishra et al 2007 used the swat to assess surface runoff and sediment transport from the 17 km2 banha watershed located in northeast india on a daily and monthly scale rostamian et al 2008 applied the swat to assess runoff and sediment transport in the beheshtabad 3860 km2 watershed in the northern karun basin in central iran ustoorikar and deo 2008 applied genetic programming gp in filling of missing information about wave height in the gulf of mexico aytek and kisi 2008 implemented gp for the explicit formulation of the daily suspended sediment discharge relationship in the tongue river in the state of montana united states vale and holman 2009 provided an improved strategy for the future hydrologic management of the bosherston lakes in west wales england and demonstrated the applicability of the swat in simulating karstic systems guven 2009 implemented linear genetic programming lgp and two versions of neural networks nns in predicting time series of daily flow rates at a station on the schuylkill river at berne pennsylvania usa jeong et al 2010 presented the development and testing of a sub hourly rainfall runoff model in swat sommerlot et al 2013 compared three watershed scale models namely the swat the high impact targeting hit model and the revised universal soil loss equation rusle2 model with the p factor and r factor performance criteria their results demonstrated the swat was the most accurate among the compared models while hit was the least accurate nikolaidis et al 2013 modified the swat to simulate the hydrologic and chemical response of karstic systems they evaluated the impacts of land use management and climate change in a mediterranean watershed in crete greece lin et al 2015 conducted an investigation of runoff responses using the swat on jinjiang a coastal basin of southeastern china zuo et al 2016 assessed the impacts of land use and climate changes on water and sediment yields in the huangfuchuan river basin hfcrb by means of the swat ashofteh et al 2016 developed comparative strategies for managing water demand under climate change conditions in the aidoghmoush basin in east azerbaijan iran they implemented the ihacres model for simulating runoff sarzaeim et al 2017 proposed data mining algorithms for runoff projection under climate change conditions moghadam et al 2019 assessed the impact of climate change uncertainties on the khorramabad river basin s runoff in lorestan province iran applying the ihacres model for simulating streamflow forecasting and modeling temperature precipitation and runoff to determine the impact of climate change on water resources is necessary for planning the future management of water resources the type of models used affects the accuracy of the modeling results previous comparison works have not evaluated the predictive skill of several hydrological models applied under climatic conditions nor the uncertainties stemming from temperature and rainfall downscaling it is however necessary to consider these uncertainties to choose the best model for a specific area application therefore this paper introduces four methods namely the change factor lars wg climgen and gp for downscaling the temperature and rainfall obtained from the fifth report of ipcc have been compared also this study presents a novel comparison of ann ihacres and swat which are data mining lumped and distributed models respectively with the purpose of assessing their accuracy in projecting runoff under climate change conditions 2 methodology the following sections describe i the study area ii several greenhouse gases scenarios entertained in this work this stage involves assessing the performance of 17 aogcms and choosing the best one iii the selection of the method for downscaling rainfall and temperature data from alternative aogcm models this step applies the change factor climgen lars wg and gp methods and the best model is chosen based on performance criteria and iv the application and comparison of the ann ihacres and swat models for runoff projection in the periods 2040 2069 and 2070 2099 2 1 the study area the sanjabi basin with an areal extent of 1230 km2 lies within kermanshah province iran as depicted in fig 1 kermanshah province is surrounded by kurdistan province to the north lorestan and ilam provinces to the south hamedan province to the east and iraq to the west the sanjabi region is one of the sub basins of the karkheh river basin the main river in the sanjabi basin is the merck river the highest and lowest elevations in the sanjabi basin equal 2800 and1307 m respectively this study used monthly runoff data gathered at the doab merek and qarasu hydrometric stations and rainfall surface air temperature and other daily meteorological data collected at selected meteorological and synoptic stations within the study area table 1 outlines the specifications of the stations the precipitation maximum temperature and minimum temperature were used in the period 1971 2000 for the swat wind speed relative humidity and daily insolation hours are required the swat simulated these variables at stations where data were not available for stations during the baseline period 1971 2000 2 2 creating future climate scenarios atmospheric ocean general circulation model aogcms simulate the earth s climate system lane et al 1999 mitchell 2003 wilby and harris 2006 the 17 models applied in the ipcc ar5 climate change simulation were considered in this work namely bcc csm1 1 bcc csm1 1 m bnu esm canesm2 ccsm4 cesm1 cam5 cnrm cm5 gfdl cm3 gfdl esm2g giss e2 h giss e2 r miroc5 miroc esm miroc esm chem mpi esm lr mpi esm mr and noresm1 m 2 3 non climatic scenarios emission scenarios a non climatic scenario describes the socio economic status and greenhouse gas emissions on earth ipcc tgcia 1999 there have been three such scenarios published namely the is92 far sar 1992 sres tar ar4 1998 2007 and the representative concentration pathways rcps ar5 2013 this study considers rcp2 6 rcp4 5 and rcp8 5 the rcp2 6 scenario predicts a maximum radiative forcing of 3 w m2 by 2050 followed by a decreasing trend according to scenario rcp4 5 radiative forcing will increase until 2070 before stabilizing under the rcp8 5 scenario radiative forcing will increase until the end of the 21st century when it will reach 8 5 w m2 in terms of co2 concentration the rcp2 6 and rcp8 5 forecast the smallest and largest concentrations by 2100 respectively van vuuren et al 2011 2 4 downscaling the process of generating regional local climate change scenarios from aogcm to increase the resolution of the models in terms of temporal and spatial is called downscaling wilby and harris 2006 this work considers three downscaling methods change factor delta the climgen model the lars wg model and gp which are described next a change factor method delta this work calculates the values of the difference between the simulated average temperature and relative humidity in the month i corresponding to the future period and the simulated average precipitation corresponding to baseline period equation 1 and the values of the ratios of the simulated average precipitation and wind speed solar radiation on the earth s surface in the month i corresponding to the future period over the simulated average precipitation and wind speed solar radiation corresponding to the baseline period equation 2 in each cell of the computational network 1 δ x i x aogcm f u t i x aogcm b a s e i 2 δ y i y aogcm f u t i y aogcm b a s e i in which δ x i the climate change scenario for the difference in average temperature and relative humidity in month i and δ y i the climate change scenario for the ratios of precipitation solar radiation relative humidity and wind speed in a month i x aogcm f u t i the average temperature and relative humidity in a month i simulated by the aogcms corresponding to the future period x aogcm b a s e i the average temperature and relative humidity in month i simulated by the aogcms corresponding to the baseline period y aogcm f u t i the average precipitation wind speed solar radiation in a month i simulated by the aogcms corresponding to the future period y aogcm b a s e i the average precipitation wind speed solar radiation in a month i simulated by the aogcms corresponding to the baseline period the calculated differences equation 1 are added to the observed monthly averages of the relevant variable surface air temperature and relative humidity equation 3 and the ratio values equation 2 are multiplied by the observed values precipitation wind speed or radiation equation 4 wilby and harris 2006 3 x i x obs i δ x i 4 y i y obs i δ y i in which x obs i and y obs i the time series of observed temperature and precipitation in the baseline period respectively x i and y i the time series projected by the climate scenarios for temperature relative humidity and precipitation wind speed solar radiation corresponding to month i in future periods b the climgen model the climgen model is a weather generator that was developed by campbell 1990 climgen generates precipitation daily maximum and minimum temperature solar radiation air humidity and wind speed the climgen model stöckle et al 1999 is a modified version of the wgen model richardson and wright 1984 the climgen algorithm starts with precipitation simulation the generation of precipitation is based on two assumptions 1 the rainfall status on day t is related to the rain status on day t and 2 the amount of rainfall on rainy days is described by a suitable probability distribution function the first assumption describes a type of model called a markov chain the result of markov modeling of rainfall status occurrence or non occurrence in the transition probability matrix is summarized in equation 5 5 p p t 1 t p dd p dw p wd p ww p dd 1 p dw p wd 1 p ww in which p t 1 t the probability of transition from one state on day t to another state on day t and indexes d and w determines whether a particular day is dry or wet the rainfall status on day t is determined by applying computer algorithms based on the congruential method mccuen 2002 and generating a random number u t in the range 0 1 the generated random number is compared with one of the transition probabilities p dw or p ww depending on whether the day t is dry or wet if u t p dw or u t p ww then day t is set to be dry otherwise it is set to wet concerning assumption 2 it has been determined that in many parts of the world the weibull distribution has a good fit on the daily rainfall values the weibull cumulative distribution function is defined by equation 6 6 f r r 1 e x p r β α in which f r r the cumulative probability of precipitation equal or less than r and α and β parameters of the distribution function that are calculated for monthly precipitation this distribution is sampled to each precipitation event using the inverse method according to equation 7 7 r β l n 1 1 f r 1 α unlike rainfall which is simulated independently modeling of other weather variables such as maximum temperature t x minimum temperature t n and solar radiation r s are affected by rainfall conditions on the desired day the climgen model uses the richardson 1981 method to generate minimum and maximum temperature data the latter method assumes weakly stationary second order stationarity data and the variables t x and t n are expressed as a first order multivariate auto regressive model according to equation 8 8 z t j a z t 1 j b ε t j in which z t j the 3 1 matrices for day t whose elements are the residuals of t x for j 1 and t n for j 2 ε t the 3 1 matrix of independent random components normally distributed with mean zero and with variance σ 2 a and b 3 3 matrices whose elements are defined as equation coefficients after calculating z t the daily values of t x and t n are estimated from equation 9 9 x t j z t j s t j x t j in which x t j the daily values of t x for j 1 and t n for j 2 s t j and x t j standard deviation and mean of variable j for day t respectively the values x t j and s t j vary depending on the wet or dry conditions of a given day the total solar radiation simulation is also produced using the maximum and minimum temperatures and is performed using equation 10 proposed by bristow and campbell 1984 10 r s t j c 1 e x p d δ t r 0 in which c and d experimental coefficients δ t the range of the temperature changes difference between the maximum temperature and the minimum temperature in degrees celsius and r 0 sunlight at the top of atmosphere in megajoules per square meter per day it should be noted that all the required parameters in the climgen model are determined on a monthly basis and spline functions are used for daily interpolation of the monthly parameters c the lars wg model this model is a random weather data generator that is used to generate minimum and maximum temperatures precipitation and radiation daily under climate change conditions this model was developed by racsko et al 1991 and was revised by semenov and barrow 1997 the lars wg model serves two purposes 1 to provide a means of simulating synthetic weather time series with statistical characteristics corresponding to the observed statistics at a site 2 to provide a long term average of weather time series for stations with data statistics or where it is not possible to monitor some needed variables lars wg utilizes semi empirical distributions for the lengths of wet and dry day series daily precipitation and daily solar radiation according to equation 11 which defines a histogram with ten intervals a i 1 a i where a i 1 a i and h i denotes the number of events from the observed data in the i th interval 11 emp a 0 a i h i i 1 10 random values from the semi empirical distributions are chosen first by selecting one of the intervals and then selecting a value within that interval from the uniform distribution assuming the data adheres to a uniform distribution in the desired interval such a distribution is flexible and can be approximated with a wide variety of shapes by adjusting the intervals a i 1 a i the intervals a i 1 a i are chosen based on the expected properties of the weather variables the intervals a i 1 a i are equally spaced for solar radiation between the minimum and maximum values of the observed data for the month the interval size gradually increases as i increases for the lengths of dry and wet series as well as for precipitation based on the wet or dry state of the day the daily mean and the daily standard deviation correspond to stochastic processes in the lars wg generator maximum and minimum temperatures are defined by the same algorithms as in climgen however the interpolation of monthly parameters is based on a finite fourier series of order 3 d gp gp developed by koza 1992 is a leading evolutionary algorithm gp unlike the ga operates on a tree structure of formulas instead of a series of binary digits and tree structures are created from a set of functions mathematical operators used in formulas and terminals problem variables and constant numbers gp uses the objective function to compare different generated solutions of the problem being solved in a step by step process of correcting the data structure and finally calculating the appropriate solution ferreira 2002 gp first defines the existing blocks which include the input and target variables and their connecting function the connecting functions between input and output variables allow gp to automatically select the appropriate variables of the model and delete the unrelated variables which reduce the dimensions of the input variables the appropriate structure of the model and its coefficients are determined next selecting the model s appropriate inputs is a key choice in gp this becomes more significant when using secondary input data because providing unrelated input data reduces model accuracy and creates more complex models chen 2003 the gp step by step process is as follows 1 an initial population is generated randomly chromosome formation 2 input the initial population chromosomes and evaluate each individual gene of the population using fitness functions identifying the most influential individuals in a population 3 selection of effective genes for mutation mating and reproduction of new individuals with modified traits offspring 4 apply a repetitive development process to the offspring in each population the fourth step is repeated a specified number times or until the best solution is obtained according to a user defined termination criterion liong et al 2002 this work applied gp with the discipulus software which is a product of register machine learning technologies inc the values of the parameters chosen to be used with the discipulus software are listed in table 2 franco 2000 2 5 rainfall runoff simulation this work applied ann the ihacres model and the swat to perform monthly rainfall runoff simulations a ann model artificial neural networks are dynamic systems that by processing experimental data transfer the knowledge or patterns hidden in the data to the network structure and learn general rules based on numerical data or samples artificial neural networks extract patterns or regression functions hidden in large data sets and use them to predict values for a new set of information each artificial neural network is made up of processing elements or artificial neurons which can be organized in different ways to form the network structure the neuron as the smallest data processing unit in an artificial neural network forms the building block of an ann cells are made up of a combination of several neurons which depending on the type of cell have specific tasks in a network the connections between cells belonging to different layers determine the structure of the ann the neural network consists of several layers input layers hidden layers and output layers layers are responsible for receiving data processing data and generating output quantities thiery et al 2008 the steps of a neural network model for forecasting or estimating include recognizing input and output variables normalizing input and output values to 0 1 range selecting the appropriate geometry for the neural network training with identifying data testing the network with data independent of the training set to continue training the network and its parameters as needed anagu et al 2009 this work implements an ann to simulate runoff with the matlab software the observed data was input to the ann training it with the data for the period 1971 1990 and the trained ann was tested with data for the period 1991 2000 and then applied for prediction of runoff b the ihacres model this is a lumped conceptual model for rainfall runoff simulation proposed by jakeman and hornberger 1993 to predict runoff from rainfall the model consists of two modules namely a non linear loss module and a linear unit hydrograph module rainfall r k and temperature t k in each time step k are converted into effective rainfall by the non linear module the effective rainfall is in turn converted to surface runoff in each time step by the linear unit hydrograph module calculation of effective rainfall in time step k u k 12 u k s k r k where s k the experimental basin moisture coefficient which is a function of evapotranspiration in the basin and is expressed by equation 13 13 s k c r k 1 1 τ w t k s k 1 s 0 0 τ w t k controls the value of the s k index in equation 13 when no rainfall occurs according to equation 14 r denotes the reference temperature parameter c is determined in such a way that the effective rainfall volume and the observed runoff are equal in the calibration period 14 τ w t k τ w e 0 062 f r t k τ w t k 1 where τ w and f denote the basin drying time constant and a temperature adjustment coefficient respectively runoff is predicted in time step k x k with equation 15 15 x k a q x k 1 b q u k 1 a s x k 1 b s u k 1 in which q and s denote respectively the separation of the basin hydrograph into a fast hydrograph q and slow hydrograph s the ihacres model involves three parameters namely τ w f and c from the non linear module equations 12 13 and 14 and four parameters namely a q a s b q and b s from the linear unit hydrograph module equation 15 that must be calibrated based on observed data c swat the swat is a continuous conceptual and distributed model that uses the water balance equation to simulate hydrological processes in a basin according to equation 16 16 δ s w i 1 t r day q surf e a w seep q gw in which δ s w changes of water stored in the soil r day rainfall q surf surface runoff e a actual evapotranspiration w seep water infiltrated into the unsaturated soil zone and q gw groundwater flow which joins the river all the variables are in millimeters and on a daily time scale mengistu 2009 swat provides two surface runoff computation methods the soil conservation service scs modified method and the green and ampt infiltration method green and ampt 1911 the scs method calculates the runoff depth based on the curve number water infiltration into the soil and initial soil moisture the calculation of the runoff depth is as follows 17 q surface r day i a 2 r day i a s in which q surface the accumulated runoff mm r day the rainfall depth for the day mm i a initial abstraction mm and s the potential maximum moisture retention after runoff begins mm to remove the necessity for independent estimation of initial abstraction a linear function between i a and s was introduced by the scs i a λs where λ is an initial abstraction ratio λ ranges between 0 and 0 3 the variable s varies with antecedent soil moisture and other variables and it is calculated as follows 18 s 25 4 1000 cn 10 in which cn curve number of water penetration into the soil mengistu 2009 a watershed is divided into a number of sub watersheds based upon drainage areas of the tributaries and each sub watershed is further divided into a number of hydrologic response units hru based on land use and land cover soil and slope characteristics soil water surface runoff sediment and chemical elements are calculated first for each hru and then for each sub watershed and finally for the entire watershed hosseini 2014 particle swarm optimization pso and the sufi2 algorithm are applied in this work to calibrate the swat parameters pso and the sufi2 algorithm are described next i particle swarm optimization pso the basis of pso is the simulation of the movement of the members or particles of a group of animals such as birds or fishes kennedy and eberhart 1995 pso like other evolutionary computational algorithms generates a population of potential solutions to a problem for exploring the search space pso assigns to each member of the population of solutions an adaptive speed relocation and a memory thus the particles remember the best position they can find in the search space therefore each particle moves in two directions i towards the best situation they have ever occupied ii towards the best situation that the best member in their neighborhood has ever occupied assume that the search space for problem is d dimensional so that the ith particle of the population can be represented by d dimensional vector x i x i 1 x ii x id t and a velocity vector v i v i 1 v ii v id t the best position occupied by each particle is denoted by pbest and the best position occupied by anyone in the population is denoted by gbest the population moves according to equations 19 and 20 19 v i d n 1 wv i d n c 1 r 1 n pbest i d n x i d n c 2 r 2 n gbest d n x i d n 20 xx i d n 1 x i d n v i d n 1 in which d 1 2 d i 1 2 n n population size w inertia weight constant c 1 and c 2 two constants and positive coefficients which are called cognitive and social parameters respectively r 1 and r 2 random numbers in the range 01 with uniform distribution n 1 2 n index for the algorithmic iterations the particles maximum velocity is denoted by w max the value of w max is central to the optimization search because high values of w max may cause high particle dispersion thus preventing some particles of finding suitable solutions on the other hand low values of w max may prevent proper search of the search space the parameter w in equation 19 is used to control the effect of previous speeds on current speeds this parameter is important for balancing the global search also known as exploration when w takes relatively high values and the local search known as exploitation when w takes relatively low values experimental results have shown that it is best to first assign a relatively large value to w to improve the overall exploration of the search space and gradually reduce its value to improve the solution extraction shi and eberhart 1998 shi and eberhart 1999 equation 21 is used assign the value of w 21 w w max w max w min n iter max in which w max the first inertia weight w min the final inertia weight iter max maximum number of iterations and n the iteration number experimental studies indicate that the value of w should be less than 1 and according to its best value is between 0 4 and 0 9 the coefficients c 1 and c 2 do not have a significant effect on convergence although appropriate values may result in increased convergence speed and improved local solution a study of c 1 and c 2 was conducted by kennedy 1998 from which it was recommended that c 1 c 2 2 but other experimental results indicate that that c 1 c 2 0 5 may produce better results more research suggests that selecting c 1 larger than c 2 with c 1 c 2 4 may lead to better results carlisle and dozier 2001 ii the sufi2 algorithm from swat cup inverse modeling im algorithms have become a common for model calibration the sufi2 algorithm is of the im variety it receives the values of the observed data and the allowable ranges of the swat parameters such as the soil curve number snowmelt temperature and others and it estimates the optimal parameter values abbaspour et al 2007 the objective function of the sufi2 algorithm must be defined in the first step several studies have shown that the sufi2 results vary with the choice of different objective functions sao et al 2020 the second step defines the upper and lower limits of each parameter it is assumed that the parameters are evenly distributed across their ranges equation 22 represents the range of the j th parameter b j 22 b j b j a b s m i n b j b j a b s m a x j 1 m in which b j a b s m i n and b j a b s m a x denote the lower and the upper bounds of the range of the j th parameter b j the range in equation 22 must be selected as large as possible while being physically meaningful dillaha and beasley 1983 the third step involves the parameters sensitivities sensitivity evaluation is performed by changing one parameter in each step while keeping all other parameters constant to assess the effect of changes in each parameter on the objective function the fourth step evaluates the range of uncertainty of each parameter by means of latin hypercube lh sampling 23 b j b min b j b max j 1 m in which b min b m i n and b max b m a x denote respectively the lower and the upper bounds of the initial uncertainty ranges of the j th parameter b j b j in general the above ranges are smaller than the absolute ranges they are subjective and are dependent upon experience the fifth step implements sampling with lh in each simulation step lh sampling divides the probability distribution of a random variable into n intervals each of the intervals has the same probability and is equal to 1 n these intervals are ranked randomly a value of each variable is then randomly extracted from each of these intervals dillaha and beasley 1983 the sixth step calculates objective function selected in the first step in each simulation step the seventh step calculates the elements j i j of the sensitivity matrix for the objective function using equation 24 24 j i j δ g i δ b j i 1 c 2 n j 1 m in which c 2 n the number of rows in the matrix which is equal to the number of simulation steps j index for the matrix columns whose number is equal to the number of parameters g objective function b j desired parameter δ g i objective function change δ b j δ b j desired parameter change the eighth step calculates the 95 confidence interval 95ppu for all parameters by means of the 2 5th xl and 97 5th xu percentiles the average distance between the upper and lower 95 ppu limits d x is obtained from equation 25 abbaspour 2007 25 d x 1 m i 1 m x u x l i in which m the number of observed values the optimal state is when 100 of the observed values are in the 95ppu confidence range and the value of d x is close to zero but due to measurement errors and model uncertainty a suitable value might not be obtained therefore based on experience a suitable way for estimating d x is the calculation of the rfactor according to equation 26 abbaspour et al 2007 26 rfactor d x σ x in which σ x the standard deviation of the measured variable x the ninth step adjusts the ranges of the parameters because their uncertainty is initially large a new range for each parameter is obtained from equations 27 and 28 abbaspour et al 2007 27 b j min b j l o w e r m a x b j l o w e r b j min 2 b j max b j u p p e r 2 28 b j l o w e r b j u p p e r m a x b j l o w e r b j min 2 b j max b j u p p e r 2 where b j m i n and b j m a x denote the updated values of the minimum and maximum j th parameter respectively the best p solutions are used to calculate b j l o w e r b j l o w e r and b j u p p e r b j u p p e r this work employs the nash sutcliffe efficiency nse index equation 28 as the objective function for optimizing the swat parameters mccuen et al 2006 28 nse 1 i 1 m q m i q s i 2 i 1 m q m i q m 2 in which q m the average observed flow in cubic meters per second q m i and q s i the observed and simulated discharge values respectively during the simulation period and i the index for the data values the value of nse shows the degree of agreement between the observed and the simulated flows and its ranges from to 1 with 1 indicates perfect goodness of fit it is noteworthy that in the baseline period 1971 19000 the performances of downscaling methods and hydrological models were compared based on the correlation coefficient r2 the root mean square rmse the maximum absolute error mae and the nse equations 29 31 chicco et al 2021 correlation coefficient r2 29 r 2 1 n i q m i q m q s i q s σ m σ s 2 where σ m and σ s denote the standard deviation of the measured and simulated variables the r2 varies between 1 and 1 it measures the degree of linear statistical association beween the measured and simulated variables it is sometimes expressed as a percentage see results root mean square error rmse 30 rmse 1 m i q m i q s i 2 it takes non negative values mean absolute error mae 31 mae 1 m i q m i q s i mae is most useful if outliers are present in the data it takes non negative values 3 results and discussion 3 1 performance assessment of the aogcms and choosing the best model the best performance among 17 aogcms was selected with the r 2 rmse mae and nse criteria calculated for the baseline period 1971 2000 tables 3 6 detail the results for rainfall average temperature maximum temperature and minimum temperature an accurate model is best characterized by a high correlation coefficient r2 and a low mean absolute error the cnrm cm5 model performed better than other models with in terms of rainfall average temperature and minimum temperature the gfdl cm3 model performed better than other models for maximum temperature 3 2 calculation of climate change scenarios in future periods the climate change scenarios for temperature and rainfall in the basin were calculated for the future period after choosing the best models for projecting climatic variables under the rcp2 6 rcp4 5 and rcp8 5 scenarios the time series of temperature and rainfall projections were downscaled to the basin scale the downscaling methods that were implemented were change factor lars wg climgen and gp the results of the calibration for the downscaling models are listed in table 7 it is noteworthy that the lars wg and climgen methods predicted well the minimum and maximum temperatures but not the average temperature therefore the methods were compared based on rainfall maximum temperature and minimum temperature table 7 indicates the change factor and climgen methods do not perform well in predicting rainfall and minimum temperature in 1971 2000 while lars wg and gp predict rainfall maximum temperature and minimum temperature better than other methods in general the lars wg model performed better than the other three methods the lars wg method was chosen to estimate rainfall maximum and minimum temperature in the future periods 2040 2069 and 2070 2099 due to its superior performance the corresponding results are presented in fig 2 fig 2 a c shows that the rainfall corresponding to the rcp2 6 and rcp4 5 scenarios in the two future periods would decrease in all months except march and november it is also shown that the rainfall in the period 2070 2099 would be less than that for the period 2040 2069 except for february october and november also rainfall in the period 2070 2099 would be higher than it was in the period 2040 2069 except for october and december according to the rcp8 5 scenario precipitation will be reduced in all months except april and november with regard to future periods more rain would fall in january and april than it would in the near future but less rain would fall in all other months in 2070 2099 than in 2040 2069 fig 2 d f illustrates that according to scenarios rcp2 6 rcp4 5 and rcp8 5 the maximum temperature for 2040 2069 and 2070 2099 is projected to increase by comparing future periods it appears that temperatures will increase more in 2070 2099 than in 2040 2069 the comparison of seasonal projections under the three emissions scenarios establishes that winter and summer would have the highest and lowest increases in temperature respectively compared to the observed values in the future two periods it is seen in fig 2 g i that under the three emissions scenarios the minimum temperature would increase in the future two periods compared to the baseline period the comparison of future periods establishes that under the rcp2 6 scenario the minimum temperature would increase in 2070 2099 from late spring to late summer but in spring and early summer the minimum temperature would be higher in 2040 2069 than in 2070 2099 under the rcp4 5 and rcp8 5 emission scenarios the minimum temperature in the period 2070 2099 would be lower than in 2040 2069 in all months 3 3 calibration and testing results for the rainfall runoff models a the ihacres model the ihacres model was calibrated and tested with the average monthly rainfall and temperature data observed temperature precipitation and runoff for 1971 2000 were input information to ihacres ihacres was calibrated for the first 20 years of the period 1971 1990 and tested for the second 10 years of the period 1991 2000 table 8 outlines the parameters calibrated in the model while table 9 summarizes the results related to the calibration and testing of ihacres for the calibration and testing periods fig 3 a b show the time series of river flow according to table 9 and fig 3 a b the r 2 in the calibration and verification periods was high the error rates were low and the nse is close to one this provides ample evidence that the ihacres has good predictive skill for river flow simulation in the study area b the ann model ann was run in matlab with two hidden layers each of which has two neurons the transmission function is tansig and the training was performed with the levenberg marquardt algorithm the results of ann training and testing are listed in table 9 and the river flow time series for the calibration and testing periods are depicted in fig 3 c d the ann results in table 9 and fig 3 c d due to the calculated low nse it is clear that this model did not perform accurately in estimating the peak flow compared to the ihacres also according to the r 2 and the magnitude of the errors it is determined that the accuracy of this model is lower than the ihacres s c the swat raster maps including digital elevation map dem soil and land use of ravansar sanjabi basin were extracted fig 4 information about the soils and land uses in the sanjabi basin is found in section of supplemental information appendixes a and b three dems soil and land uses were combined followed by performing slope classification into five categories to delineate the hrus within the study basin the adopted slope classification includes five categories 0 10 66 10 66 21 33 21 33 31 99 31 99 42 66 and 42 66 53 33 degrees thirty three sub basins and 260 hrus were formed in swat in the sanjabi basin the daily rainfall and the maximum and minimum daily temperatures of stations near the basin in the 30 year period 1971 2000 were input to swat the monthly data for 1971 1972 and 1973 were input as warm up and the swat2012 model was implemented in arcgis10 2 software with a monthly time step the initial values of some properties of the basin such as soil chemical composition were not available therefore the swat can calculate these values in the warm up period the map of sub basins and rivers is displayed in fig 5 swat calculates runoff components sub surface flow groundwater storage and other variables and exports these as text files swat store this text file in a folder called txtintout which is interfaced with the swat cup software swat cup calibrates swat parameters with the sufi2 algorithm and pso using flow data for the period 1974 1991 swat cup tests swat parameters for the period 1992 2000 tables 10 and 11 list the calibration parameters and the optimal values of these parameters derived from sufi2 and pso respectively the results of swat model calibration and testing with sufi2 and pso are listed in tables 9 and the river flow time series for the calibration and testing periods are displayed in fig 3 the calibration results for swat indicate better accuracy for the sufi2 algorithm than for pso however the low nse obtained with sufi2 and pso in the testing period establishes the low performance of the swat model in the simulation of peak discharges in the period 1992 2000 comparing three hydrological models ihacres ann and swat reveals that the performance of ihacres was the best in this application and therefore the future runoff under the three scenarios rcp2 6 rcp4 5 and rcp8 5 and the two future periods 2040 2069 and 2070 2099 are estimated with this model 3 4 simulation of future river flow the ihacres model projected the monthly time series of river flow relying on future temperature and precipitation projected values that were estimated with the lars wg method fig 6 shows a comparison of the average long term monthly river flow in the 2040 2069 and 2070 2099 under the three emission scenarios rcp2 6 rcp4 5 and rcp8 5 with baseline and observed values also the average long term values of observed baseline and simulated runoff and future periods under different scenarios are listed in table 12 the results are shown in fig 6 and table 12 indicate that the simulated runoff values in the two future periods under the three emission scenarios in all months would decrease compared to the simulated baseline values and the reduction from early winter until mid summer is much larger than in the autumn this trend is the same when comparing future runoff with observed values however only in january and june under the rcp2 6 2040 2069 and january june and december under rcp4 5 2070 2099 would the future runoff be slightly larger than the observed runoff the comparison of the emission scenarios reveals that the rcp8 5 scenario would produce a larger runoff reduction in the future than the other two scenarios and the rcp4 5 would produce a smaller reduction in the runoff and from the comparison of future periods it transpires that under the rcp2 6 and rcp8 5 scenarios there would be less runoff in 2070 2099 than in 2040 2069 but this trend is the opposite for rcp4 5 the annual changes in simulation runoff calculated with the ihacres model relative to the observed values are listed in table 13 it is seen in table 13 that the annual runoff reduction under the rcp2 6 emissions scenario for 2040 2069 and 2070 2099 would be 23 5 and 36 0 respectively the runoff simulated under the rcp4 5 emissions scenario for 2040 2069 and 2070 2099 would be reduced with respect to the observed values by 32 9 and 25 6 respectively the rcp8 5 shows the largest reduction of runoff which is 42 0 and 44 3 for 2040 2069 and 2070 2099 respectively 4 discussion there is a variety of hydrology models available so choosing the most suitable one when undertaking a study is not trivial it is therefore necessary to conduct a comparative analysis of basin models to evaluate their capabilities and limitations in the study area studies comparing hydrological models provide managers and planners with strategic results to help them manage water resources it is noteworthy that comparing the performance of several rainfall runoff models in basins with different climates is an effective way determining which model is the most appropriate several hydrological models have been widely applied in recent years the ihacres model predicts flow better than distributed models in many cases littlewood and jakeman 1994 ye et al 1997 it is also easy to implement with respect to calibration and testing moreover the input data are readily available and the calculations are simple boorman and sefton 1997 evaluated the differences between the results of two conceptual hydrologic models and their results showed that the two models had the same capability in simulating historical flows despite the complexity of distributed models some researchers recommend their use because they provide more accurate predictions due to their more realistic representation of hydrologic processes and basin conditions ghavidelfar et al 2011 te linde et al 2008 analysts must weigh in the tradeoffs between model complexity and ease of implementation in choosing a hydrologic model the impact of climate change on temperature and precipitation parameters and their influence on the hydrological cycle forecasting and modeling future temperature and precipitation accurately are factors that must be considered prior to engaging in future planning and choosing a hydrologic model there have been several studies comparing the performance of models considering their accuracy for forecasting changes in weather conditions and the role that they play in simulating and predicting parameters that affect the hydrological cycle adamowski and prasher 2012 evaluated svm and ann models to simulate rainfall in a mountain basin his results showed the superiority of svm tezel and buyukyildiz 2016 evaluated the efficiency of ann and svm in estimating monthly evaporation the latter authors reported that both models exhibited similar performance in predicting evaporation based on temperature relative humidity wind speed and precipitation the comparison of ann ihacres and swat presented in this study is novel this study was performed to determine if the use of data mining and conceptual models where input data are readily available and calculations are relatively simple yet they lead to better predictions than those of distributed models whose data inputs require spatial and temporal detail this work compared the change factor delta climgen lars wg and gp downscaling methods and their accuracy was measured by the r 2 rmse mae and nse coefficients overall the results showed that the ihacres model performed better than the other two models and this result was consistent with other studies that have indicated that flow simulation by the ihacres model may be better than those produced by distributed models littlewood and jakeman 1994 ye et al 1997 5 conclusions this work compared the runoff projections obtained with the data mining model ann the lumped model ihacres and the distributed model swat for the sanjabi basin in kermanshah province iran under climate change conditions for this purpose 17 models from the ipcc 5th assessment report climate change scenarios were adopted in this work namely bcc csm1 1 bcc csm1 1 m bnu esm canesm2 ccsm4 cesm1 cam5 cnrm cm5 gfdl cm3 gfdl esm2g giss e2 h giss e2 r miroc5 miroc esm miroc esm chem mpi esm lr mpi esm mr and noresm1 m corresponding to 2040 2069 and 2070 2099 and under three emission scenarios i e rcp2 6 rcp4 5 and rcp8 5 the performance criteria r 2 rmse mae and nse evaluated projections of rainfall average temperature and minimum temperature and it was concluded that thecnrm cm5 performed better than other models the gfdl cm3 performed better than other models with respect to maximum temperature the change factor delta climgen lars wg and gp methods were implemented for downscaling climate variables and their results were evaluated with performance criteria the lars wg method performed best in the ravansar sanjabi basin the lars wg method was therefore implemented to project future rainfall and temperature ann ihacres and swat were implemented to simulate runoff these models were calibrated and their predictive skill was evaluated among the hydrological models the ihacres model had the best performance with r 2 rmse mae by 71 6 4 93 m3 s 2 72 m3 s and 0 71 in the testing period 1971 1990 and 72 87 3 89 m3 s 2 44 m3 s and 0 72 in the calibration period 1992 1000 therefore ihacres was applied to project future runoff the comparison of the projected long term average monthly runoff under the rcp2 6 rcp4 5 and rcp8 5 scenarios for 2040 2069 and 2070 2099 with baseline values reveals that the runoff would decline under all scenarios and in the two future periods furthermore the annual runoff projected under the rcp2 6 rcp4 5 and rcp8 5 emissions scenarios for 2040 2069 would decline by 23 5 36 0 and 32 9 respectively in comparison to the observed runoff and under the same scenarios for 2070 2099 the projected runoff would be reduced respectively by 25 6 42 0 and 44 3 compared to observed runoff credit authorship contribution statement seyedeh hadis moghadam methodology software validation parisa sadat ashofteh supervision writing review editing conceptualization investigation validation hugo a loáiciga writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
2430,this work evaluates the effects of climate change on the surface water resources river flow of the sanjabi basin iran by comparing data mining lumped and distributed models namely artificial neural networks ann the identification of unit hydrographs and component flows from rainfall evaporation and streamflow ihacres model and the soil and water assessment tool swat climate projections in terms of monthly temperature and rainfall made by 17 atmosphere ocean general circulation models aogcms by the 5th assessment report ar5 of the intergovernmental panel on climate change ipcc under emission scenarios of representative concentration pathways rcps rcp2 6 rcp4 5 and rcp8 5 during the baseline period 1971 2000 and future periods 2040 2069 and 2070 2099 are applied in the sanjabi basin the predictive skill of the aogcms is evaluated with performance criteria the evaluation results indicate the cnrm cm5 model features the best performance in terms of rainfall average temperature and minimum temperature projections and the gfdl cm3 provides the most accurate maximum temperature projections four downscaling methods change factor delta climgen lars wg and genetic programming gp are compared based on the r 2 rmse mae and nse the predictive skill of the lars wg method was the highest ann ihacres and swat are implemented to project future runoff following calibration and testing the ihacres model exhibits the best performance the ihacres model is applied to project future runoff under climate change scenarios the results indicate a reduction in runoff under all emission scenarios in the two future periods with the rcp8 5 scenario featuring the largest reductions in runoff in 2040 2069 and 2070 2099 and being equal to 42 0 and 44 3 respectively keywords climate change river flow delta model climgen model lars wg model gp model ann model ihacres model swat model data availability data will be made available on request 1 introduction increasing burning of fossil fuels for energy generation on the one hand and deforestation and environmental degradation on the other hand have led to an increase in greenhouse gases and rising surface air temperature the alteration of the earth atmosphere radiation balance has modified climatic patterns which is suspected of altering the intensity duration and frequency of precipitation and poses challenges to the utilization of water resources and agriculture in many parts of the world vanuytrecht et al 2012 the ipcc 2001 reported that the average global air surface temperature has risen since 1861 through present time by 0 6 0 2 c temperate increases are predicted to continue unless greenhouse gases concentrations are reduced ipcc 2021 various studies have simulated the effects of climate change on the hydrologic cycle and compared the performance of models applied for this purpose a brief review of studies reporting hydrologic simulations under climate change conditions is presented next afinowicz et al 2005 implemented the swat to evaluate the influence of woody plants on the water budgets in the semi arid karstic basin of the guadalupe river in texas united states li et al 2007 applied the swat in a basin of west africa and showed that changing areas covered with forest grassland and savanna to agricultural lands or urban areas changed the natural hydrological conditions in the basin abbaspour et al 2007 implemented the swat to simulate processes affecting water quantity sediment transport and nutrient loads in the thur river basin switzerland mishra et al 2007 used the swat to assess surface runoff and sediment transport from the 17 km2 banha watershed located in northeast india on a daily and monthly scale rostamian et al 2008 applied the swat to assess runoff and sediment transport in the beheshtabad 3860 km2 watershed in the northern karun basin in central iran ustoorikar and deo 2008 applied genetic programming gp in filling of missing information about wave height in the gulf of mexico aytek and kisi 2008 implemented gp for the explicit formulation of the daily suspended sediment discharge relationship in the tongue river in the state of montana united states vale and holman 2009 provided an improved strategy for the future hydrologic management of the bosherston lakes in west wales england and demonstrated the applicability of the swat in simulating karstic systems guven 2009 implemented linear genetic programming lgp and two versions of neural networks nns in predicting time series of daily flow rates at a station on the schuylkill river at berne pennsylvania usa jeong et al 2010 presented the development and testing of a sub hourly rainfall runoff model in swat sommerlot et al 2013 compared three watershed scale models namely the swat the high impact targeting hit model and the revised universal soil loss equation rusle2 model with the p factor and r factor performance criteria their results demonstrated the swat was the most accurate among the compared models while hit was the least accurate nikolaidis et al 2013 modified the swat to simulate the hydrologic and chemical response of karstic systems they evaluated the impacts of land use management and climate change in a mediterranean watershed in crete greece lin et al 2015 conducted an investigation of runoff responses using the swat on jinjiang a coastal basin of southeastern china zuo et al 2016 assessed the impacts of land use and climate changes on water and sediment yields in the huangfuchuan river basin hfcrb by means of the swat ashofteh et al 2016 developed comparative strategies for managing water demand under climate change conditions in the aidoghmoush basin in east azerbaijan iran they implemented the ihacres model for simulating runoff sarzaeim et al 2017 proposed data mining algorithms for runoff projection under climate change conditions moghadam et al 2019 assessed the impact of climate change uncertainties on the khorramabad river basin s runoff in lorestan province iran applying the ihacres model for simulating streamflow forecasting and modeling temperature precipitation and runoff to determine the impact of climate change on water resources is necessary for planning the future management of water resources the type of models used affects the accuracy of the modeling results previous comparison works have not evaluated the predictive skill of several hydrological models applied under climatic conditions nor the uncertainties stemming from temperature and rainfall downscaling it is however necessary to consider these uncertainties to choose the best model for a specific area application therefore this paper introduces four methods namely the change factor lars wg climgen and gp for downscaling the temperature and rainfall obtained from the fifth report of ipcc have been compared also this study presents a novel comparison of ann ihacres and swat which are data mining lumped and distributed models respectively with the purpose of assessing their accuracy in projecting runoff under climate change conditions 2 methodology the following sections describe i the study area ii several greenhouse gases scenarios entertained in this work this stage involves assessing the performance of 17 aogcms and choosing the best one iii the selection of the method for downscaling rainfall and temperature data from alternative aogcm models this step applies the change factor climgen lars wg and gp methods and the best model is chosen based on performance criteria and iv the application and comparison of the ann ihacres and swat models for runoff projection in the periods 2040 2069 and 2070 2099 2 1 the study area the sanjabi basin with an areal extent of 1230 km2 lies within kermanshah province iran as depicted in fig 1 kermanshah province is surrounded by kurdistan province to the north lorestan and ilam provinces to the south hamedan province to the east and iraq to the west the sanjabi region is one of the sub basins of the karkheh river basin the main river in the sanjabi basin is the merck river the highest and lowest elevations in the sanjabi basin equal 2800 and1307 m respectively this study used monthly runoff data gathered at the doab merek and qarasu hydrometric stations and rainfall surface air temperature and other daily meteorological data collected at selected meteorological and synoptic stations within the study area table 1 outlines the specifications of the stations the precipitation maximum temperature and minimum temperature were used in the period 1971 2000 for the swat wind speed relative humidity and daily insolation hours are required the swat simulated these variables at stations where data were not available for stations during the baseline period 1971 2000 2 2 creating future climate scenarios atmospheric ocean general circulation model aogcms simulate the earth s climate system lane et al 1999 mitchell 2003 wilby and harris 2006 the 17 models applied in the ipcc ar5 climate change simulation were considered in this work namely bcc csm1 1 bcc csm1 1 m bnu esm canesm2 ccsm4 cesm1 cam5 cnrm cm5 gfdl cm3 gfdl esm2g giss e2 h giss e2 r miroc5 miroc esm miroc esm chem mpi esm lr mpi esm mr and noresm1 m 2 3 non climatic scenarios emission scenarios a non climatic scenario describes the socio economic status and greenhouse gas emissions on earth ipcc tgcia 1999 there have been three such scenarios published namely the is92 far sar 1992 sres tar ar4 1998 2007 and the representative concentration pathways rcps ar5 2013 this study considers rcp2 6 rcp4 5 and rcp8 5 the rcp2 6 scenario predicts a maximum radiative forcing of 3 w m2 by 2050 followed by a decreasing trend according to scenario rcp4 5 radiative forcing will increase until 2070 before stabilizing under the rcp8 5 scenario radiative forcing will increase until the end of the 21st century when it will reach 8 5 w m2 in terms of co2 concentration the rcp2 6 and rcp8 5 forecast the smallest and largest concentrations by 2100 respectively van vuuren et al 2011 2 4 downscaling the process of generating regional local climate change scenarios from aogcm to increase the resolution of the models in terms of temporal and spatial is called downscaling wilby and harris 2006 this work considers three downscaling methods change factor delta the climgen model the lars wg model and gp which are described next a change factor method delta this work calculates the values of the difference between the simulated average temperature and relative humidity in the month i corresponding to the future period and the simulated average precipitation corresponding to baseline period equation 1 and the values of the ratios of the simulated average precipitation and wind speed solar radiation on the earth s surface in the month i corresponding to the future period over the simulated average precipitation and wind speed solar radiation corresponding to the baseline period equation 2 in each cell of the computational network 1 δ x i x aogcm f u t i x aogcm b a s e i 2 δ y i y aogcm f u t i y aogcm b a s e i in which δ x i the climate change scenario for the difference in average temperature and relative humidity in month i and δ y i the climate change scenario for the ratios of precipitation solar radiation relative humidity and wind speed in a month i x aogcm f u t i the average temperature and relative humidity in a month i simulated by the aogcms corresponding to the future period x aogcm b a s e i the average temperature and relative humidity in month i simulated by the aogcms corresponding to the baseline period y aogcm f u t i the average precipitation wind speed solar radiation in a month i simulated by the aogcms corresponding to the future period y aogcm b a s e i the average precipitation wind speed solar radiation in a month i simulated by the aogcms corresponding to the baseline period the calculated differences equation 1 are added to the observed monthly averages of the relevant variable surface air temperature and relative humidity equation 3 and the ratio values equation 2 are multiplied by the observed values precipitation wind speed or radiation equation 4 wilby and harris 2006 3 x i x obs i δ x i 4 y i y obs i δ y i in which x obs i and y obs i the time series of observed temperature and precipitation in the baseline period respectively x i and y i the time series projected by the climate scenarios for temperature relative humidity and precipitation wind speed solar radiation corresponding to month i in future periods b the climgen model the climgen model is a weather generator that was developed by campbell 1990 climgen generates precipitation daily maximum and minimum temperature solar radiation air humidity and wind speed the climgen model stöckle et al 1999 is a modified version of the wgen model richardson and wright 1984 the climgen algorithm starts with precipitation simulation the generation of precipitation is based on two assumptions 1 the rainfall status on day t is related to the rain status on day t and 2 the amount of rainfall on rainy days is described by a suitable probability distribution function the first assumption describes a type of model called a markov chain the result of markov modeling of rainfall status occurrence or non occurrence in the transition probability matrix is summarized in equation 5 5 p p t 1 t p dd p dw p wd p ww p dd 1 p dw p wd 1 p ww in which p t 1 t the probability of transition from one state on day t to another state on day t and indexes d and w determines whether a particular day is dry or wet the rainfall status on day t is determined by applying computer algorithms based on the congruential method mccuen 2002 and generating a random number u t in the range 0 1 the generated random number is compared with one of the transition probabilities p dw or p ww depending on whether the day t is dry or wet if u t p dw or u t p ww then day t is set to be dry otherwise it is set to wet concerning assumption 2 it has been determined that in many parts of the world the weibull distribution has a good fit on the daily rainfall values the weibull cumulative distribution function is defined by equation 6 6 f r r 1 e x p r β α in which f r r the cumulative probability of precipitation equal or less than r and α and β parameters of the distribution function that are calculated for monthly precipitation this distribution is sampled to each precipitation event using the inverse method according to equation 7 7 r β l n 1 1 f r 1 α unlike rainfall which is simulated independently modeling of other weather variables such as maximum temperature t x minimum temperature t n and solar radiation r s are affected by rainfall conditions on the desired day the climgen model uses the richardson 1981 method to generate minimum and maximum temperature data the latter method assumes weakly stationary second order stationarity data and the variables t x and t n are expressed as a first order multivariate auto regressive model according to equation 8 8 z t j a z t 1 j b ε t j in which z t j the 3 1 matrices for day t whose elements are the residuals of t x for j 1 and t n for j 2 ε t the 3 1 matrix of independent random components normally distributed with mean zero and with variance σ 2 a and b 3 3 matrices whose elements are defined as equation coefficients after calculating z t the daily values of t x and t n are estimated from equation 9 9 x t j z t j s t j x t j in which x t j the daily values of t x for j 1 and t n for j 2 s t j and x t j standard deviation and mean of variable j for day t respectively the values x t j and s t j vary depending on the wet or dry conditions of a given day the total solar radiation simulation is also produced using the maximum and minimum temperatures and is performed using equation 10 proposed by bristow and campbell 1984 10 r s t j c 1 e x p d δ t r 0 in which c and d experimental coefficients δ t the range of the temperature changes difference between the maximum temperature and the minimum temperature in degrees celsius and r 0 sunlight at the top of atmosphere in megajoules per square meter per day it should be noted that all the required parameters in the climgen model are determined on a monthly basis and spline functions are used for daily interpolation of the monthly parameters c the lars wg model this model is a random weather data generator that is used to generate minimum and maximum temperatures precipitation and radiation daily under climate change conditions this model was developed by racsko et al 1991 and was revised by semenov and barrow 1997 the lars wg model serves two purposes 1 to provide a means of simulating synthetic weather time series with statistical characteristics corresponding to the observed statistics at a site 2 to provide a long term average of weather time series for stations with data statistics or where it is not possible to monitor some needed variables lars wg utilizes semi empirical distributions for the lengths of wet and dry day series daily precipitation and daily solar radiation according to equation 11 which defines a histogram with ten intervals a i 1 a i where a i 1 a i and h i denotes the number of events from the observed data in the i th interval 11 emp a 0 a i h i i 1 10 random values from the semi empirical distributions are chosen first by selecting one of the intervals and then selecting a value within that interval from the uniform distribution assuming the data adheres to a uniform distribution in the desired interval such a distribution is flexible and can be approximated with a wide variety of shapes by adjusting the intervals a i 1 a i the intervals a i 1 a i are chosen based on the expected properties of the weather variables the intervals a i 1 a i are equally spaced for solar radiation between the minimum and maximum values of the observed data for the month the interval size gradually increases as i increases for the lengths of dry and wet series as well as for precipitation based on the wet or dry state of the day the daily mean and the daily standard deviation correspond to stochastic processes in the lars wg generator maximum and minimum temperatures are defined by the same algorithms as in climgen however the interpolation of monthly parameters is based on a finite fourier series of order 3 d gp gp developed by koza 1992 is a leading evolutionary algorithm gp unlike the ga operates on a tree structure of formulas instead of a series of binary digits and tree structures are created from a set of functions mathematical operators used in formulas and terminals problem variables and constant numbers gp uses the objective function to compare different generated solutions of the problem being solved in a step by step process of correcting the data structure and finally calculating the appropriate solution ferreira 2002 gp first defines the existing blocks which include the input and target variables and their connecting function the connecting functions between input and output variables allow gp to automatically select the appropriate variables of the model and delete the unrelated variables which reduce the dimensions of the input variables the appropriate structure of the model and its coefficients are determined next selecting the model s appropriate inputs is a key choice in gp this becomes more significant when using secondary input data because providing unrelated input data reduces model accuracy and creates more complex models chen 2003 the gp step by step process is as follows 1 an initial population is generated randomly chromosome formation 2 input the initial population chromosomes and evaluate each individual gene of the population using fitness functions identifying the most influential individuals in a population 3 selection of effective genes for mutation mating and reproduction of new individuals with modified traits offspring 4 apply a repetitive development process to the offspring in each population the fourth step is repeated a specified number times or until the best solution is obtained according to a user defined termination criterion liong et al 2002 this work applied gp with the discipulus software which is a product of register machine learning technologies inc the values of the parameters chosen to be used with the discipulus software are listed in table 2 franco 2000 2 5 rainfall runoff simulation this work applied ann the ihacres model and the swat to perform monthly rainfall runoff simulations a ann model artificial neural networks are dynamic systems that by processing experimental data transfer the knowledge or patterns hidden in the data to the network structure and learn general rules based on numerical data or samples artificial neural networks extract patterns or regression functions hidden in large data sets and use them to predict values for a new set of information each artificial neural network is made up of processing elements or artificial neurons which can be organized in different ways to form the network structure the neuron as the smallest data processing unit in an artificial neural network forms the building block of an ann cells are made up of a combination of several neurons which depending on the type of cell have specific tasks in a network the connections between cells belonging to different layers determine the structure of the ann the neural network consists of several layers input layers hidden layers and output layers layers are responsible for receiving data processing data and generating output quantities thiery et al 2008 the steps of a neural network model for forecasting or estimating include recognizing input and output variables normalizing input and output values to 0 1 range selecting the appropriate geometry for the neural network training with identifying data testing the network with data independent of the training set to continue training the network and its parameters as needed anagu et al 2009 this work implements an ann to simulate runoff with the matlab software the observed data was input to the ann training it with the data for the period 1971 1990 and the trained ann was tested with data for the period 1991 2000 and then applied for prediction of runoff b the ihacres model this is a lumped conceptual model for rainfall runoff simulation proposed by jakeman and hornberger 1993 to predict runoff from rainfall the model consists of two modules namely a non linear loss module and a linear unit hydrograph module rainfall r k and temperature t k in each time step k are converted into effective rainfall by the non linear module the effective rainfall is in turn converted to surface runoff in each time step by the linear unit hydrograph module calculation of effective rainfall in time step k u k 12 u k s k r k where s k the experimental basin moisture coefficient which is a function of evapotranspiration in the basin and is expressed by equation 13 13 s k c r k 1 1 τ w t k s k 1 s 0 0 τ w t k controls the value of the s k index in equation 13 when no rainfall occurs according to equation 14 r denotes the reference temperature parameter c is determined in such a way that the effective rainfall volume and the observed runoff are equal in the calibration period 14 τ w t k τ w e 0 062 f r t k τ w t k 1 where τ w and f denote the basin drying time constant and a temperature adjustment coefficient respectively runoff is predicted in time step k x k with equation 15 15 x k a q x k 1 b q u k 1 a s x k 1 b s u k 1 in which q and s denote respectively the separation of the basin hydrograph into a fast hydrograph q and slow hydrograph s the ihacres model involves three parameters namely τ w f and c from the non linear module equations 12 13 and 14 and four parameters namely a q a s b q and b s from the linear unit hydrograph module equation 15 that must be calibrated based on observed data c swat the swat is a continuous conceptual and distributed model that uses the water balance equation to simulate hydrological processes in a basin according to equation 16 16 δ s w i 1 t r day q surf e a w seep q gw in which δ s w changes of water stored in the soil r day rainfall q surf surface runoff e a actual evapotranspiration w seep water infiltrated into the unsaturated soil zone and q gw groundwater flow which joins the river all the variables are in millimeters and on a daily time scale mengistu 2009 swat provides two surface runoff computation methods the soil conservation service scs modified method and the green and ampt infiltration method green and ampt 1911 the scs method calculates the runoff depth based on the curve number water infiltration into the soil and initial soil moisture the calculation of the runoff depth is as follows 17 q surface r day i a 2 r day i a s in which q surface the accumulated runoff mm r day the rainfall depth for the day mm i a initial abstraction mm and s the potential maximum moisture retention after runoff begins mm to remove the necessity for independent estimation of initial abstraction a linear function between i a and s was introduced by the scs i a λs where λ is an initial abstraction ratio λ ranges between 0 and 0 3 the variable s varies with antecedent soil moisture and other variables and it is calculated as follows 18 s 25 4 1000 cn 10 in which cn curve number of water penetration into the soil mengistu 2009 a watershed is divided into a number of sub watersheds based upon drainage areas of the tributaries and each sub watershed is further divided into a number of hydrologic response units hru based on land use and land cover soil and slope characteristics soil water surface runoff sediment and chemical elements are calculated first for each hru and then for each sub watershed and finally for the entire watershed hosseini 2014 particle swarm optimization pso and the sufi2 algorithm are applied in this work to calibrate the swat parameters pso and the sufi2 algorithm are described next i particle swarm optimization pso the basis of pso is the simulation of the movement of the members or particles of a group of animals such as birds or fishes kennedy and eberhart 1995 pso like other evolutionary computational algorithms generates a population of potential solutions to a problem for exploring the search space pso assigns to each member of the population of solutions an adaptive speed relocation and a memory thus the particles remember the best position they can find in the search space therefore each particle moves in two directions i towards the best situation they have ever occupied ii towards the best situation that the best member in their neighborhood has ever occupied assume that the search space for problem is d dimensional so that the ith particle of the population can be represented by d dimensional vector x i x i 1 x ii x id t and a velocity vector v i v i 1 v ii v id t the best position occupied by each particle is denoted by pbest and the best position occupied by anyone in the population is denoted by gbest the population moves according to equations 19 and 20 19 v i d n 1 wv i d n c 1 r 1 n pbest i d n x i d n c 2 r 2 n gbest d n x i d n 20 xx i d n 1 x i d n v i d n 1 in which d 1 2 d i 1 2 n n population size w inertia weight constant c 1 and c 2 two constants and positive coefficients which are called cognitive and social parameters respectively r 1 and r 2 random numbers in the range 01 with uniform distribution n 1 2 n index for the algorithmic iterations the particles maximum velocity is denoted by w max the value of w max is central to the optimization search because high values of w max may cause high particle dispersion thus preventing some particles of finding suitable solutions on the other hand low values of w max may prevent proper search of the search space the parameter w in equation 19 is used to control the effect of previous speeds on current speeds this parameter is important for balancing the global search also known as exploration when w takes relatively high values and the local search known as exploitation when w takes relatively low values experimental results have shown that it is best to first assign a relatively large value to w to improve the overall exploration of the search space and gradually reduce its value to improve the solution extraction shi and eberhart 1998 shi and eberhart 1999 equation 21 is used assign the value of w 21 w w max w max w min n iter max in which w max the first inertia weight w min the final inertia weight iter max maximum number of iterations and n the iteration number experimental studies indicate that the value of w should be less than 1 and according to its best value is between 0 4 and 0 9 the coefficients c 1 and c 2 do not have a significant effect on convergence although appropriate values may result in increased convergence speed and improved local solution a study of c 1 and c 2 was conducted by kennedy 1998 from which it was recommended that c 1 c 2 2 but other experimental results indicate that that c 1 c 2 0 5 may produce better results more research suggests that selecting c 1 larger than c 2 with c 1 c 2 4 may lead to better results carlisle and dozier 2001 ii the sufi2 algorithm from swat cup inverse modeling im algorithms have become a common for model calibration the sufi2 algorithm is of the im variety it receives the values of the observed data and the allowable ranges of the swat parameters such as the soil curve number snowmelt temperature and others and it estimates the optimal parameter values abbaspour et al 2007 the objective function of the sufi2 algorithm must be defined in the first step several studies have shown that the sufi2 results vary with the choice of different objective functions sao et al 2020 the second step defines the upper and lower limits of each parameter it is assumed that the parameters are evenly distributed across their ranges equation 22 represents the range of the j th parameter b j 22 b j b j a b s m i n b j b j a b s m a x j 1 m in which b j a b s m i n and b j a b s m a x denote the lower and the upper bounds of the range of the j th parameter b j the range in equation 22 must be selected as large as possible while being physically meaningful dillaha and beasley 1983 the third step involves the parameters sensitivities sensitivity evaluation is performed by changing one parameter in each step while keeping all other parameters constant to assess the effect of changes in each parameter on the objective function the fourth step evaluates the range of uncertainty of each parameter by means of latin hypercube lh sampling 23 b j b min b j b max j 1 m in which b min b m i n and b max b m a x denote respectively the lower and the upper bounds of the initial uncertainty ranges of the j th parameter b j b j in general the above ranges are smaller than the absolute ranges they are subjective and are dependent upon experience the fifth step implements sampling with lh in each simulation step lh sampling divides the probability distribution of a random variable into n intervals each of the intervals has the same probability and is equal to 1 n these intervals are ranked randomly a value of each variable is then randomly extracted from each of these intervals dillaha and beasley 1983 the sixth step calculates objective function selected in the first step in each simulation step the seventh step calculates the elements j i j of the sensitivity matrix for the objective function using equation 24 24 j i j δ g i δ b j i 1 c 2 n j 1 m in which c 2 n the number of rows in the matrix which is equal to the number of simulation steps j index for the matrix columns whose number is equal to the number of parameters g objective function b j desired parameter δ g i objective function change δ b j δ b j desired parameter change the eighth step calculates the 95 confidence interval 95ppu for all parameters by means of the 2 5th xl and 97 5th xu percentiles the average distance between the upper and lower 95 ppu limits d x is obtained from equation 25 abbaspour 2007 25 d x 1 m i 1 m x u x l i in which m the number of observed values the optimal state is when 100 of the observed values are in the 95ppu confidence range and the value of d x is close to zero but due to measurement errors and model uncertainty a suitable value might not be obtained therefore based on experience a suitable way for estimating d x is the calculation of the rfactor according to equation 26 abbaspour et al 2007 26 rfactor d x σ x in which σ x the standard deviation of the measured variable x the ninth step adjusts the ranges of the parameters because their uncertainty is initially large a new range for each parameter is obtained from equations 27 and 28 abbaspour et al 2007 27 b j min b j l o w e r m a x b j l o w e r b j min 2 b j max b j u p p e r 2 28 b j l o w e r b j u p p e r m a x b j l o w e r b j min 2 b j max b j u p p e r 2 where b j m i n and b j m a x denote the updated values of the minimum and maximum j th parameter respectively the best p solutions are used to calculate b j l o w e r b j l o w e r and b j u p p e r b j u p p e r this work employs the nash sutcliffe efficiency nse index equation 28 as the objective function for optimizing the swat parameters mccuen et al 2006 28 nse 1 i 1 m q m i q s i 2 i 1 m q m i q m 2 in which q m the average observed flow in cubic meters per second q m i and q s i the observed and simulated discharge values respectively during the simulation period and i the index for the data values the value of nse shows the degree of agreement between the observed and the simulated flows and its ranges from to 1 with 1 indicates perfect goodness of fit it is noteworthy that in the baseline period 1971 19000 the performances of downscaling methods and hydrological models were compared based on the correlation coefficient r2 the root mean square rmse the maximum absolute error mae and the nse equations 29 31 chicco et al 2021 correlation coefficient r2 29 r 2 1 n i q m i q m q s i q s σ m σ s 2 where σ m and σ s denote the standard deviation of the measured and simulated variables the r2 varies between 1 and 1 it measures the degree of linear statistical association beween the measured and simulated variables it is sometimes expressed as a percentage see results root mean square error rmse 30 rmse 1 m i q m i q s i 2 it takes non negative values mean absolute error mae 31 mae 1 m i q m i q s i mae is most useful if outliers are present in the data it takes non negative values 3 results and discussion 3 1 performance assessment of the aogcms and choosing the best model the best performance among 17 aogcms was selected with the r 2 rmse mae and nse criteria calculated for the baseline period 1971 2000 tables 3 6 detail the results for rainfall average temperature maximum temperature and minimum temperature an accurate model is best characterized by a high correlation coefficient r2 and a low mean absolute error the cnrm cm5 model performed better than other models with in terms of rainfall average temperature and minimum temperature the gfdl cm3 model performed better than other models for maximum temperature 3 2 calculation of climate change scenarios in future periods the climate change scenarios for temperature and rainfall in the basin were calculated for the future period after choosing the best models for projecting climatic variables under the rcp2 6 rcp4 5 and rcp8 5 scenarios the time series of temperature and rainfall projections were downscaled to the basin scale the downscaling methods that were implemented were change factor lars wg climgen and gp the results of the calibration for the downscaling models are listed in table 7 it is noteworthy that the lars wg and climgen methods predicted well the minimum and maximum temperatures but not the average temperature therefore the methods were compared based on rainfall maximum temperature and minimum temperature table 7 indicates the change factor and climgen methods do not perform well in predicting rainfall and minimum temperature in 1971 2000 while lars wg and gp predict rainfall maximum temperature and minimum temperature better than other methods in general the lars wg model performed better than the other three methods the lars wg method was chosen to estimate rainfall maximum and minimum temperature in the future periods 2040 2069 and 2070 2099 due to its superior performance the corresponding results are presented in fig 2 fig 2 a c shows that the rainfall corresponding to the rcp2 6 and rcp4 5 scenarios in the two future periods would decrease in all months except march and november it is also shown that the rainfall in the period 2070 2099 would be less than that for the period 2040 2069 except for february october and november also rainfall in the period 2070 2099 would be higher than it was in the period 2040 2069 except for october and december according to the rcp8 5 scenario precipitation will be reduced in all months except april and november with regard to future periods more rain would fall in january and april than it would in the near future but less rain would fall in all other months in 2070 2099 than in 2040 2069 fig 2 d f illustrates that according to scenarios rcp2 6 rcp4 5 and rcp8 5 the maximum temperature for 2040 2069 and 2070 2099 is projected to increase by comparing future periods it appears that temperatures will increase more in 2070 2099 than in 2040 2069 the comparison of seasonal projections under the three emissions scenarios establishes that winter and summer would have the highest and lowest increases in temperature respectively compared to the observed values in the future two periods it is seen in fig 2 g i that under the three emissions scenarios the minimum temperature would increase in the future two periods compared to the baseline period the comparison of future periods establishes that under the rcp2 6 scenario the minimum temperature would increase in 2070 2099 from late spring to late summer but in spring and early summer the minimum temperature would be higher in 2040 2069 than in 2070 2099 under the rcp4 5 and rcp8 5 emission scenarios the minimum temperature in the period 2070 2099 would be lower than in 2040 2069 in all months 3 3 calibration and testing results for the rainfall runoff models a the ihacres model the ihacres model was calibrated and tested with the average monthly rainfall and temperature data observed temperature precipitation and runoff for 1971 2000 were input information to ihacres ihacres was calibrated for the first 20 years of the period 1971 1990 and tested for the second 10 years of the period 1991 2000 table 8 outlines the parameters calibrated in the model while table 9 summarizes the results related to the calibration and testing of ihacres for the calibration and testing periods fig 3 a b show the time series of river flow according to table 9 and fig 3 a b the r 2 in the calibration and verification periods was high the error rates were low and the nse is close to one this provides ample evidence that the ihacres has good predictive skill for river flow simulation in the study area b the ann model ann was run in matlab with two hidden layers each of which has two neurons the transmission function is tansig and the training was performed with the levenberg marquardt algorithm the results of ann training and testing are listed in table 9 and the river flow time series for the calibration and testing periods are depicted in fig 3 c d the ann results in table 9 and fig 3 c d due to the calculated low nse it is clear that this model did not perform accurately in estimating the peak flow compared to the ihacres also according to the r 2 and the magnitude of the errors it is determined that the accuracy of this model is lower than the ihacres s c the swat raster maps including digital elevation map dem soil and land use of ravansar sanjabi basin were extracted fig 4 information about the soils and land uses in the sanjabi basin is found in section of supplemental information appendixes a and b three dems soil and land uses were combined followed by performing slope classification into five categories to delineate the hrus within the study basin the adopted slope classification includes five categories 0 10 66 10 66 21 33 21 33 31 99 31 99 42 66 and 42 66 53 33 degrees thirty three sub basins and 260 hrus were formed in swat in the sanjabi basin the daily rainfall and the maximum and minimum daily temperatures of stations near the basin in the 30 year period 1971 2000 were input to swat the monthly data for 1971 1972 and 1973 were input as warm up and the swat2012 model was implemented in arcgis10 2 software with a monthly time step the initial values of some properties of the basin such as soil chemical composition were not available therefore the swat can calculate these values in the warm up period the map of sub basins and rivers is displayed in fig 5 swat calculates runoff components sub surface flow groundwater storage and other variables and exports these as text files swat store this text file in a folder called txtintout which is interfaced with the swat cup software swat cup calibrates swat parameters with the sufi2 algorithm and pso using flow data for the period 1974 1991 swat cup tests swat parameters for the period 1992 2000 tables 10 and 11 list the calibration parameters and the optimal values of these parameters derived from sufi2 and pso respectively the results of swat model calibration and testing with sufi2 and pso are listed in tables 9 and the river flow time series for the calibration and testing periods are displayed in fig 3 the calibration results for swat indicate better accuracy for the sufi2 algorithm than for pso however the low nse obtained with sufi2 and pso in the testing period establishes the low performance of the swat model in the simulation of peak discharges in the period 1992 2000 comparing three hydrological models ihacres ann and swat reveals that the performance of ihacres was the best in this application and therefore the future runoff under the three scenarios rcp2 6 rcp4 5 and rcp8 5 and the two future periods 2040 2069 and 2070 2099 are estimated with this model 3 4 simulation of future river flow the ihacres model projected the monthly time series of river flow relying on future temperature and precipitation projected values that were estimated with the lars wg method fig 6 shows a comparison of the average long term monthly river flow in the 2040 2069 and 2070 2099 under the three emission scenarios rcp2 6 rcp4 5 and rcp8 5 with baseline and observed values also the average long term values of observed baseline and simulated runoff and future periods under different scenarios are listed in table 12 the results are shown in fig 6 and table 12 indicate that the simulated runoff values in the two future periods under the three emission scenarios in all months would decrease compared to the simulated baseline values and the reduction from early winter until mid summer is much larger than in the autumn this trend is the same when comparing future runoff with observed values however only in january and june under the rcp2 6 2040 2069 and january june and december under rcp4 5 2070 2099 would the future runoff be slightly larger than the observed runoff the comparison of the emission scenarios reveals that the rcp8 5 scenario would produce a larger runoff reduction in the future than the other two scenarios and the rcp4 5 would produce a smaller reduction in the runoff and from the comparison of future periods it transpires that under the rcp2 6 and rcp8 5 scenarios there would be less runoff in 2070 2099 than in 2040 2069 but this trend is the opposite for rcp4 5 the annual changes in simulation runoff calculated with the ihacres model relative to the observed values are listed in table 13 it is seen in table 13 that the annual runoff reduction under the rcp2 6 emissions scenario for 2040 2069 and 2070 2099 would be 23 5 and 36 0 respectively the runoff simulated under the rcp4 5 emissions scenario for 2040 2069 and 2070 2099 would be reduced with respect to the observed values by 32 9 and 25 6 respectively the rcp8 5 shows the largest reduction of runoff which is 42 0 and 44 3 for 2040 2069 and 2070 2099 respectively 4 discussion there is a variety of hydrology models available so choosing the most suitable one when undertaking a study is not trivial it is therefore necessary to conduct a comparative analysis of basin models to evaluate their capabilities and limitations in the study area studies comparing hydrological models provide managers and planners with strategic results to help them manage water resources it is noteworthy that comparing the performance of several rainfall runoff models in basins with different climates is an effective way determining which model is the most appropriate several hydrological models have been widely applied in recent years the ihacres model predicts flow better than distributed models in many cases littlewood and jakeman 1994 ye et al 1997 it is also easy to implement with respect to calibration and testing moreover the input data are readily available and the calculations are simple boorman and sefton 1997 evaluated the differences between the results of two conceptual hydrologic models and their results showed that the two models had the same capability in simulating historical flows despite the complexity of distributed models some researchers recommend their use because they provide more accurate predictions due to their more realistic representation of hydrologic processes and basin conditions ghavidelfar et al 2011 te linde et al 2008 analysts must weigh in the tradeoffs between model complexity and ease of implementation in choosing a hydrologic model the impact of climate change on temperature and precipitation parameters and their influence on the hydrological cycle forecasting and modeling future temperature and precipitation accurately are factors that must be considered prior to engaging in future planning and choosing a hydrologic model there have been several studies comparing the performance of models considering their accuracy for forecasting changes in weather conditions and the role that they play in simulating and predicting parameters that affect the hydrological cycle adamowski and prasher 2012 evaluated svm and ann models to simulate rainfall in a mountain basin his results showed the superiority of svm tezel and buyukyildiz 2016 evaluated the efficiency of ann and svm in estimating monthly evaporation the latter authors reported that both models exhibited similar performance in predicting evaporation based on temperature relative humidity wind speed and precipitation the comparison of ann ihacres and swat presented in this study is novel this study was performed to determine if the use of data mining and conceptual models where input data are readily available and calculations are relatively simple yet they lead to better predictions than those of distributed models whose data inputs require spatial and temporal detail this work compared the change factor delta climgen lars wg and gp downscaling methods and their accuracy was measured by the r 2 rmse mae and nse coefficients overall the results showed that the ihacres model performed better than the other two models and this result was consistent with other studies that have indicated that flow simulation by the ihacres model may be better than those produced by distributed models littlewood and jakeman 1994 ye et al 1997 5 conclusions this work compared the runoff projections obtained with the data mining model ann the lumped model ihacres and the distributed model swat for the sanjabi basin in kermanshah province iran under climate change conditions for this purpose 17 models from the ipcc 5th assessment report climate change scenarios were adopted in this work namely bcc csm1 1 bcc csm1 1 m bnu esm canesm2 ccsm4 cesm1 cam5 cnrm cm5 gfdl cm3 gfdl esm2g giss e2 h giss e2 r miroc5 miroc esm miroc esm chem mpi esm lr mpi esm mr and noresm1 m corresponding to 2040 2069 and 2070 2099 and under three emission scenarios i e rcp2 6 rcp4 5 and rcp8 5 the performance criteria r 2 rmse mae and nse evaluated projections of rainfall average temperature and minimum temperature and it was concluded that thecnrm cm5 performed better than other models the gfdl cm3 performed better than other models with respect to maximum temperature the change factor delta climgen lars wg and gp methods were implemented for downscaling climate variables and their results were evaluated with performance criteria the lars wg method performed best in the ravansar sanjabi basin the lars wg method was therefore implemented to project future rainfall and temperature ann ihacres and swat were implemented to simulate runoff these models were calibrated and their predictive skill was evaluated among the hydrological models the ihacres model had the best performance with r 2 rmse mae by 71 6 4 93 m3 s 2 72 m3 s and 0 71 in the testing period 1971 1990 and 72 87 3 89 m3 s 2 44 m3 s and 0 72 in the calibration period 1992 1000 therefore ihacres was applied to project future runoff the comparison of the projected long term average monthly runoff under the rcp2 6 rcp4 5 and rcp8 5 scenarios for 2040 2069 and 2070 2099 with baseline values reveals that the runoff would decline under all scenarios and in the two future periods furthermore the annual runoff projected under the rcp2 6 rcp4 5 and rcp8 5 emissions scenarios for 2040 2069 would decline by 23 5 36 0 and 32 9 respectively in comparison to the observed runoff and under the same scenarios for 2070 2099 the projected runoff would be reduced respectively by 25 6 42 0 and 44 3 compared to observed runoff credit authorship contribution statement seyedeh hadis moghadam methodology software validation parisa sadat ashofteh supervision writing review editing conceptualization investigation validation hugo a loáiciga writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
2431,the role of dissolved organic nitrogen don in the eutrophication of surface water and the potential risk to drinking water safety as a precursor of disinfection by products is gradually emphasized therefore accurate estimates of watershed don load fluxes and identifying thresholds of streamflow that cause abrupt changes in riverine don concentration are important here we adopted the load estimator model loadest and nonparametric change point analysis ncpa to comprehensively investigate the variation of don load from 2011 to 2016 in different seasons in the fengyu river watershed southwest china results indicated that the wet season is the hot moment of don loss and the average cumulative load reaches 14 1 t accounting for 72 5 of the annual load meanwhile baseflow is the main pathway of don export accounting for about 74 4 of the stream export load furthermore the abrupt threshold of streamflow on don concentration is 2 03 m3 s to 2 15 m3 s when the stream flow is below 2 15 m3 s the concentration of river don increases rapidly and poses a great threat to human drinking water safety this study will provide a scientific basis for regulating don at the watershed scale and water quality protection in drinking water source areas keywords dissolved organic nitrogen baseflow segmentation load estimator baseflow load segmentation abrupt change thresholds data availability the authors do not have permission to share data 1 introduction excess nitrogen n in the surface water is a worldwide concern as it degrades water quality and decreases available water resources in aquatic ecosystems huang et al 2014 liu et al 2022a 2022b pellerin et al 2014 the nitrogen in rivers mainly consists of inorganic and organic portions nie et al 2018 dissolved inorganic nitrogen din ie no3 n and nh4 n have been emphasized for their impact on eutrophication in streams and targeted as the n forms that be controlled in the watershed with excess n loads li et al 2018a 2018b wang et al 2020 however recent studies indicated that dissolved organic nitrogen don stimulates phytoplankton growth and produces nearly 10 times more chlorophyll a than din eom et al 2017 the effect of don facilitating river eutrophication has been underestimated in published studies zheng et al 2021 in many streams the don concentrations and their transported load can exceed those of din mackay et al 2020 meanwhile the don could react with chlorine as a common disinfectant to generate a number of nitrogenous disinfection byproducts n dbps during the drinking water processing process gu et al 2011 which are toxic and dangerous and increase the risk of cancer liu et al 2022a 2022b whereas most water environment management and restoration measures primarily target din the hazardous effects of don concentration on drinking water safety were ignored mallick et al 2022 hence understanding the temporal dynamics characteristic of don concentrations and loads in streams is significant to controlling water eutrophication and protecting drinking water safety given that many studies investigate the factors influencing don concentrations and loads in the aquatic environment for instance mackay et al 2020 found that the bio utilization of don by natural river phytoplankton will reduce the concentration of don in the stream li et al 2018 found that fertilization and land use pattern both affects don concentrations studies revealed that cropland and vegetation is the main source of river don nie et al 2018 wang et al 2020 however these studies lack investigation of the factors directly affecting don concentration in rivers ie streamflow and different runoff component streamflow was found to be positively related to don concentration guo et al 2019 which means that increased streamflow accelerates don loss a significant increase in don loss flux in rivers whereas when the streamflow increases to a certain level the dilution effect on pollutants will dominate zhang et al 2015 moreover zhang et al 2019 and bond et al 2011 reviewed the occurrence and control of n dbps and concluded that high concentrations of don in rivers promote n dbps formation therefore identifying the abrupt points that streamflow dilution don concentrations have great importance for reducing the generation of n dbps groundwater as one of the primary don pools in a watershed wang et al 2021 is an important source of recharge for baseflow xin et al 2019 therefore baseflow as a stable runoff component of rivers exports large amounts of the dissolved pollutants in the groundwater aquifer to the river mcmahon and nathan 2021 numerous studies have confirmed the significant contribution of baseflow to water quality deterioration schilling et al 2018 however the study on the baseflow don load was limited since no direct approach to measuring baseflow pollutant loads in a watershed zhu et al 2019 in particular the proportion of baseflow to the river varies significantly from the wet to dry season therefore an integrated method for evaluating don load transported by baseflow during different seasons at the watershed scale is needed n fertilizers have been intensively applied to enhance crop yields especially in china which is one of the largest consumer countries of n fertilizers in the world li et al 2018a 2018b previous studies indicated that agricultural production is the main source of river don van kessel et al 2009 therefore revealing the pattern of don export in a typical agricultural watershed is helpful to formulate management measures to alleviate water quality degradation caused by excessive don the fengyu river watershed is a typical agricultural headwater watershed of the erhai lake to supply dali city with domestic water li et al 2019 thus our objectives of this study are to 1 investigate the pattern of don export at the watershed scale with seasonal variations 2 quantify don transport pathways from streamflow and baseflow 3 identify thresholds of streamflow that cause abrupt changes in don concentration 2 materials and methods 2 1 study area fengyu river watershed 99 85 100 02 e 25 88 26 09 n elevation 2082 3615 m is a typical plateau watershed located in yunnan province southwest china fig 1 a it covers a total area of 219 km2 and the mean annual precipitation is 745 mm and the mean annual temperature is 13 9 c annual streamflow reaches 100 million m3 which is an important water source for erhai lake and dali city the wet season in the study area is from may to october and the dry season is from november to april another year and more than 85 of the annual precipitation occurs in the wet season forest 29 6 and grassland 45 9 areas are mainly located in the uplands with their water yield flowing down through agricultural areas 21 before entering the main streams of fengyu river watershed li et al 2018a 2018b 2 2 data collection the streamflow of the watershed was monitored at the outlet from 2011 to 2016 specifically the current meter method was used from january 1 2011 to may 31 2012 river discharge measurement criterion gb50179 93 from june 1 2012 to october 31 2016 the automatic bubble water level meter waterlog h 3553 u s was installed at the watershed outlet to automatically and continuously monitor the water levels the water level flow relationship curve was established based on the measurements for the different magnitudes of streamflow and the automatic monitoring water level data was fed into the water level flow relationship curve for calculating daily streamflow at the outlet of the watershed water samples were taken daily from january 1 2011 to march 9 2014 and periodically from march 10 2014 to october 31 2016 at the watershed outlet a total of 1416 water samples were taken to detect dissolved total nitrogen dtn ammonium nitrogen nh4 n and nitrate nitrogen no3 n the metrics were determined by alkaline potassium persulfate digestion uv spectrophotometric method gb11894 89 nessler s reagent colorimetric method gb7480 87 and spectrophotometric method with phenol sulfonic respectively gb7479 87 the concentration of don was calculated as the concentration of dtn minus the sum of nh4 n and no3 n nie et al 2018 2 3 baseflow separation and don load estimation the streamflow and its load are influenced by hydrological conditions caused by storm events so they can be regarded as high frequency signals in contrast the baseflow and its load are relatively stable due to the storage effect of groundwater aquifers thus they can be regarded as low frequency signals he et al 2019 he and lu 2016 therefore the time series of river streamflow and nutrient load can be regarded as the superposition of high and low frequency signals in this study the eckhardt automated filter method eckhardt 2005 was used for baseflow and baseflow load separation based on measured flow and estimated load at the outlet station the eck filter is a two parameter recursive digital filter and the filter equation for baseflow and their load is given as eqs 1 and 2 the specific calculation process of the parameters is as follows 1 b k 1 bfi max α b k 1 1 α bfi max y k 1 α bfi max 2 l i 1 bli max β l i 1 1 β bli max l i 1 β bli max where bk yk mm d is the baseflow and streamflow at the time step k day respectively α dimensionless is the filter parameter recession constant and bfimax dimensionless is the maximum baseflow index meanwhile li li kg d is the streamflow and baseflow don load at the time step i day β dimensionless is the load recession constant blimax dimensionless is the maximum baseflow load index the parameters α β bfimax and blimax are obtained as follows firstly calculating the runoff duration after peak streamflow according to equation 3 3 n 0 83 a 0 2 where n is the number of days of streamflow after the peak and a is the catchment area of the basin n was used as 4 83 in this study which means screening the data for more than 5 consecutive days of flow recession and then exponential fitting the receding process eckhardt 2008 finally excluding the α values of r2 0 9 the average value of the remaining values was taken as the receding constant α this study selected 28 recession process curves that satisfy the conditions and calculated the receding constants for each curve fig s1 thus the recession constant α is 0 934 in this study as the bfimax index is normally given because of the stream and its aquifers zhu et al 2019 the bfimax is suggested as 0 80 0 50 and 0 25 for perennial streams with porous aquifers ephemeral streams with porous aquifers and perennial streams with hard rock aquifers respectively eckhardt 2005 according to the geological conditions of the fengyu river watershed the bfimax was assumed to be 0 8 in this study in addition the β was obtained by screening and fitting the streamflow load recession process this study uses the loadest model to estimate the don load of streamflow park and engel 2014 at the outlet station the daily don load regression model was set up based on daily flow data n 2132 and discrete don concentration data n 1416 the akaike information criterion aic and the schwarz posterior probability criterion sppc were used for the selection of the regression equations the regression models shown in equation 4 were obtained as the best model for don load estimation in streamflow table s1 and the load bias in percent bp and nash sutcliffe efficiency coefficient nse were used to justify the model performance 4 l n q load a 0 a 1 l n q f a 2 s i n 2 π d t i m e a 3 c o s 2 π d t i m e a 4 d t i m e where a0 6 are coefficients qf is streamflow and baseflow dtime is decimal time lnqload and dtime use the method of centralized transformation to solve multicollinearity problems the formula is as follows 5 l o a d c o n s t i t u e n t l o a d k g d 6 l n q l o a d l n q c e n t e r o f l n q l o a d 7 d t i m e d e c i m a l t i m e c e n t e r o f d e c i m a l t i m e based on the estimated don load of streamflow this study selected 16 load recession process curves to determine β as 0 905 fig s2 meanwhile the estimated blimax is 0 68 fig s3 by adopting the backward filtering approach collischonn and fan 2013 miller et al 2016 in particular the don load when the proportion of baseflow in streamflow is greater than 95 was assumed as the measurement of don loads in baseflow based on the schilling method schilling and zhang 2004 the coefficient of determination r2 and nash sutcliffe coefficient nse were used to evaluate the accuracy of the baseflow load separation model 2 4 abrupt analysis the nonparametric change point analysis ncpa is effective in determining the abrupt changepoints of dependent variables cao et al 2017 wu and lu 2021 in this study the variation of streamflow caused by the changing hydrological conditions results in significant changes in don concentration thus the ncpa was used to explore the abrupt threshold of the streamflow that is the most sensitive to don concentration as the availability of observed data is limited the bootstrap method is usually used to estimate the frequency distribution of each change point wu and lu 2019 since the bias corrected and accelerated bca bootstrap correcting for bias and skewness in the distribution of bootstrap estimates improves the accuracy of the frequency distribution of change point estimation this study built 1000 new random samples extracted from the original flow don data set by the bca bootstrap method and quantified the flow abrupt threshold the specific analysis steps are as follows firstly the flow variables x1 x2 xn and the corresponding don variables y1 y2 yn are arranged in ascending order then the change point i divides the don variables into two groups y1 yi and yi 1 yn is assumed 1 i n secondly the deviance value of each group was calculated using equation 8 and the deviance reduction δi is calculated based on equation 9 8 d k 1 m y k μ 2 9 δ i d d i d i where d is the deviance of the entire data y1 yn m is the sample size and μ is the average of the m observations yk i 1 2 n d i is the deviance of the data y1 yi and d i is the deviance of the data yi 1 yn finally the i value that presented the maximum δi is considered to a change point of flow 2 5 statistical analyses the loadest model https water usgs gov software loadest was used to estimate the daily don load of streamflow which was used to investigate the temporal variation of don load in the watershed meanwhile the eckhardt digital filter method was used for baseflow and baseflow don load separation and the method was automatically calculated in the bfi 3 0 tool hydrooffice https www hydrooffice org furthermore correlation analysis and abrupt analysis were used to reveal the relationship between streamflow baseflow and don the pearson correlation analysis and nonparametric change point analysis ncpa were conducted in r v 4 1 1 3 results 3 1 estimation of don loads from streamflow and baseflow due to the limitation of sampling conditions the water quality monitoring had been changed from daily sampling to periodic sampling since june 2014 meanwhile the simulated residuals are independent of each other and normally distributed scr 0 39 ppcc 1 thus this study selected the asymptotic maximum likelihood estimation amle method for streamflow daily don load estimated the calibrated coefficients and the performance of selected regression equations in the loadest model were shown in table 1 previous studies show that the acceptable level of model simulation accuracy is related to the data scale daily monthly yearly pellerin et al 2014 moriasi et al 2015 indicated that daily scale watershed n estimated results are satisfactory when the nse greater than 0 25 and bp 25 in this study the bp is 20 5 and the nse is 0 39 means the estimated results are satisfactory and the errors are within the acceptable ranges the comparisons between model estimated streamflow don load results and measured data were shown in fig 2 and fig s2 for baseflow don load separation the values of r2 and nse are greater than 0 85 fig 3 indicating that the eck filter method can be used for separating baseflow don load from streamflow load 3 2 temporal variations for don load from streamflow and baseflow baseflow is an essential source of watershed don and their correlation reaches a significant level p 0 01 fig 4 a although the percentage of baseflow to streamflow fluctuates less during different seasons their don load drops rapidly from 83 9 to 65 1 from dry to wet season fig 4a during the wet season the average don load of streamflow reached 76 6 kg day peaking at 133 5 kg day in august during the dry season the don load of streamflow was just 31 2 kg day fig 4b while the averaged baseflow don load was 50 67 kg day in the wet season and 29 7 kg day in the dry season fig 4c correspondingly the monthly don load from streamflow and baseflow reached 2 35 t month and 1 56 t month during the wet season and 0 94 t month and 0 89 t month during the dry season baseflow are the don mainly sources during the dry season fig 5 a furthermore the watershed don load increases annually and the percentage of baseflow don load maintain stability at annual scale is about 74 4 fig 5b 3 3 abrupt thresholds of the streamflow on don the ncpa results indicated the tendency of don concentration to abruptly change in response to the increasing streamflow based on the cumulative distribution of the change points the abrupt change threshold ranging from 2 03 m3 s to 2 15 m3 s with a 95 confidence interval was obtained fig 6 a combining the results of the correlation analysis between streamflow and don load we discovered don concentration will increase rapidly as streamflow increases however if the streamflow in the watershed exceeds 2 15 m3 s the increasing trend of doc concentration significantly declines fig 6b furthermore the response of don load to streamflow variation is different in different seasons fig 7 don load was best fitted to streamflow during the wet period average r2 greater than 0 9 and streamflow is mostly above the abrupt point in this season on the contrary the flow rate in the dry period is mostly lower than 2 15 m3 s and the don concentration is significantly higher than that in the abundant period 4 discussions 4 1 the characteristics of don loss and its implication for watershed water quality management long term inter annual don load variations in the fengyu river watershed are investigated from 2011 to 2016 the results indicated that the don load in the study area showed inter annual variations due to different hydrological conditions for different years and the highest annual load reached 23 1 t in 2013 furthermore the wet season is the hot moment of don loss in the watershed accounting for 72 5 of the annual load fig 4 previous studies have found that when vegetation forestland and grassland and cropland cover are higher than 70 and 5 both land use types will be the primary sources of don in this watershed lusk et al 2018 wang et al 2020 the percentages of vegetation and cropland in the fengyu river watershed were 75 5 and 21 0 respectively li et al 2018a 2018b thus don from the decomposition of organic matter from litter in cropland and vegetation will be transported to streams via surface and subsurface runoff li et al 2018a 2018b wang et al 2020 willett et al 2004 meanwhile increased precipitation during the wet season facilitates the runoff formed and accelerates the dissolution of don promoting don transport from the landscape to streams in 2020 chemical fertilizers has banned in the erhai lake basin and the increased organic fertilizer will certainly enhance the risk of don loss improved management of cropland in the wet season is essential to mitigate don export from the watershed previously more attention has been paid to the manage the streamflow environment by intercepting pollutants transported from surface runoff or soil erosion however the water quality improvement may be still limited even though the nps pollutants transported by surface runoff are well managed rivett et al 2011 the reason could be a large number of nutrients are delivered to the river by the baseflow gao et al 2018 mcmahon and nathan 2021 this study confirmed that don exported by baseflow average accounts for 74 4 of streamflow don load even reaching 83 9 during the dry season fig 4 baseflow comes from groundwater and constitutes the main component of the stream during the decline of streamflow zhu et al 2019 hence reducing the leaching of domestic and agricultural wastewater into the groundwater is key to protecting the water environment notably although the don load transported by baseflow during the wet season only accounts for 65 1 of streamflow don load the don loads were more than twice those of the dry season fig 5 the reasons are closely related to the don loss caused by agricultural activities during the wet season in the study area adame et al 2016 the previous study found that approximately 3 of the applied n fertilizers could be lost via don leaching in the paddy field nie et al 2018 there are 25 9 km2 of paddy fields in the fengyu watershed accounting for 11 8 of the total watershed area therefore don leaching from paddy soil to the groundwater aquifer might be one of the important pathways for don transport in addition the high contribution from baseflow to don load also indicated that the groundwater aquifer receives don leaching from the soil in contrast farming shifts from paddy fields to dryland crops during the dry season with a significant reduction in the leaching of cropland meanwhile the decrease in temperature and precipitation is not conducive to vegetation decomposition resulting in generally low river don loads during the dry season berman and bronk 2003 xin et al 2019 hence baseflow in the study area exports most of the don load during the wet season when there are much more don sources from the perspective of watershed water quality management reducing infiltration of field runoff is key to managing don loads in watersheds thus the construction of ditches to connect fields and ponds to form a field ditch pond irrigation system may be beneficial to reducing the infiltration of field runoff hua et al 2019a 2019b ditch drainage reduces the time of water storage in the field during the wet season reducing the leaching of cropland yang et al 2019 meanwhile by absorbing and reusing wastewater from the cropland the system also effectively reduces don concentration liu et al 2022a 2022b yan et al 2021 the aquatic ecosystems and drinking water safety are sensitive to the changing don concentration excessive don concentrations may induce the outbreak of water bloom zhang et al 2015 and release quantities of toxic disinfection byproducts during the processes of potable water production consequently identifying the abrupt change point of the don concentration contributes to assessing the risk of river eutrophication and managing the human production process of drinking water here this study revealed that the streamflow is greater than 2 15 m3 s the don concentration in the river decreases significantly but its export load to the receiving water body erhai lake increases rapidly fig 6 and fig 7 therefore if the don concentration in the river is a concern management practices should be implemented when streamflow is below 2 15 m3 s while if the don load in the receiving lake is a concern management practices should be implemented to regulate the land use pattern to reduce the don load of the watershed 4 2 processes affecting modeling of don loads and improvement although the model error for don load estimation in this study was within the acceptable range table 1 the accuracy of the simulation results was lower than in other studies loadest model pellerin et al 2014 it is possible to hypothesize that these conditions are related to the following two main aspects firstly this study used a mixture of daily continuous 2011 1 1 2014 3 9 and periodic water quality monitoring data 2014 3 10 2016 10 31 in the process of establishing the regression model sampling data with different monitoring frequencies could increase the estimated error sharifi et al 2017 several reports have shown that the more frequent sampling data did not necessarily lead to more accurate and precise annual pollutant load estimates park and engel 2014 the most effective sampling strategy depends on the duration of the monitoring and the variations of streamflow this study is a six years long term investigation thus taking the fixed period semimonthly sampling 2014 2016 may result in not only the least biased but also the most precise loads robertson and roerish 1999 secondly the existing methods are not able to directly measure the amount of don in the stream nie et al 2018 since don consists of complex compounds including urea dissolved free amino acids proteins nucleic acids and amino sugars yang et al 2019 thus the don data in this study were calculated using the differential method since don was influenced by the accuracy of other metrics measurements dtn nh4 n no3 n which greatly increases the model uncertainty park and engel 2014 sharifi et al 2017 loadest used streamflow and periodicity as predictors to estimate don load using the regression model the accuracy of the model is highly dependent on measured data for flow and don concentration thus the regression model may not fully represent the dynamics of don loads machine learning has been proposed more conducive to establishing stable and highly accurate load estimates wherry et al 2021 therefore different models including regressions and machine learning could be combined for estimating the in stream don load in future studies 5 conclusions don accelerates eutrophication in streams and threatens drinking water safety in agricultural watersheds river however don load fluxes and export characteristics remain unclear here we monitored the long term variation of don in the fengyu river watershed and the watershed don load was found to be increasing annually in particular the highest total annual don load reached 23 1 t in 2013 and the don loss was concentrated in the wet season the average load reaches 76 6 kg day and the cumulative load reaches 14 1 t accounting for 72 5 of the annual load meanwhile the baseflow export don average accounts for 74 4 of streamflow don load especially reaching 83 9 during the dry season becoming the main export pathway of don in the watershed furthermore the abrupt threshold of streamflow on don concentration is 2 03 m3 s to 2 15 m3 s our study could contribute to understanding the characteristics of don export in agricultural watersheds and establishing measures for the protection of drinking water safety credit authorship contribution statement qiyu xu conceptualization methodology formal analysis software writing original draft writing review editing limei zhai supervision validation funding acquisition xinru liu conceptualization methodology xinzhong du conceptualization supervision writing review editing resources funding acquisition hongbin liu validation supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was funded by the national natural science foundation of china grant nos 42107076 u20a20114 fundamental research funds for central non profit scientific institution no 1610132022008 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 129054 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
2431,the role of dissolved organic nitrogen don in the eutrophication of surface water and the potential risk to drinking water safety as a precursor of disinfection by products is gradually emphasized therefore accurate estimates of watershed don load fluxes and identifying thresholds of streamflow that cause abrupt changes in riverine don concentration are important here we adopted the load estimator model loadest and nonparametric change point analysis ncpa to comprehensively investigate the variation of don load from 2011 to 2016 in different seasons in the fengyu river watershed southwest china results indicated that the wet season is the hot moment of don loss and the average cumulative load reaches 14 1 t accounting for 72 5 of the annual load meanwhile baseflow is the main pathway of don export accounting for about 74 4 of the stream export load furthermore the abrupt threshold of streamflow on don concentration is 2 03 m3 s to 2 15 m3 s when the stream flow is below 2 15 m3 s the concentration of river don increases rapidly and poses a great threat to human drinking water safety this study will provide a scientific basis for regulating don at the watershed scale and water quality protection in drinking water source areas keywords dissolved organic nitrogen baseflow segmentation load estimator baseflow load segmentation abrupt change thresholds data availability the authors do not have permission to share data 1 introduction excess nitrogen n in the surface water is a worldwide concern as it degrades water quality and decreases available water resources in aquatic ecosystems huang et al 2014 liu et al 2022a 2022b pellerin et al 2014 the nitrogen in rivers mainly consists of inorganic and organic portions nie et al 2018 dissolved inorganic nitrogen din ie no3 n and nh4 n have been emphasized for their impact on eutrophication in streams and targeted as the n forms that be controlled in the watershed with excess n loads li et al 2018a 2018b wang et al 2020 however recent studies indicated that dissolved organic nitrogen don stimulates phytoplankton growth and produces nearly 10 times more chlorophyll a than din eom et al 2017 the effect of don facilitating river eutrophication has been underestimated in published studies zheng et al 2021 in many streams the don concentrations and their transported load can exceed those of din mackay et al 2020 meanwhile the don could react with chlorine as a common disinfectant to generate a number of nitrogenous disinfection byproducts n dbps during the drinking water processing process gu et al 2011 which are toxic and dangerous and increase the risk of cancer liu et al 2022a 2022b whereas most water environment management and restoration measures primarily target din the hazardous effects of don concentration on drinking water safety were ignored mallick et al 2022 hence understanding the temporal dynamics characteristic of don concentrations and loads in streams is significant to controlling water eutrophication and protecting drinking water safety given that many studies investigate the factors influencing don concentrations and loads in the aquatic environment for instance mackay et al 2020 found that the bio utilization of don by natural river phytoplankton will reduce the concentration of don in the stream li et al 2018 found that fertilization and land use pattern both affects don concentrations studies revealed that cropland and vegetation is the main source of river don nie et al 2018 wang et al 2020 however these studies lack investigation of the factors directly affecting don concentration in rivers ie streamflow and different runoff component streamflow was found to be positively related to don concentration guo et al 2019 which means that increased streamflow accelerates don loss a significant increase in don loss flux in rivers whereas when the streamflow increases to a certain level the dilution effect on pollutants will dominate zhang et al 2015 moreover zhang et al 2019 and bond et al 2011 reviewed the occurrence and control of n dbps and concluded that high concentrations of don in rivers promote n dbps formation therefore identifying the abrupt points that streamflow dilution don concentrations have great importance for reducing the generation of n dbps groundwater as one of the primary don pools in a watershed wang et al 2021 is an important source of recharge for baseflow xin et al 2019 therefore baseflow as a stable runoff component of rivers exports large amounts of the dissolved pollutants in the groundwater aquifer to the river mcmahon and nathan 2021 numerous studies have confirmed the significant contribution of baseflow to water quality deterioration schilling et al 2018 however the study on the baseflow don load was limited since no direct approach to measuring baseflow pollutant loads in a watershed zhu et al 2019 in particular the proportion of baseflow to the river varies significantly from the wet to dry season therefore an integrated method for evaluating don load transported by baseflow during different seasons at the watershed scale is needed n fertilizers have been intensively applied to enhance crop yields especially in china which is one of the largest consumer countries of n fertilizers in the world li et al 2018a 2018b previous studies indicated that agricultural production is the main source of river don van kessel et al 2009 therefore revealing the pattern of don export in a typical agricultural watershed is helpful to formulate management measures to alleviate water quality degradation caused by excessive don the fengyu river watershed is a typical agricultural headwater watershed of the erhai lake to supply dali city with domestic water li et al 2019 thus our objectives of this study are to 1 investigate the pattern of don export at the watershed scale with seasonal variations 2 quantify don transport pathways from streamflow and baseflow 3 identify thresholds of streamflow that cause abrupt changes in don concentration 2 materials and methods 2 1 study area fengyu river watershed 99 85 100 02 e 25 88 26 09 n elevation 2082 3615 m is a typical plateau watershed located in yunnan province southwest china fig 1 a it covers a total area of 219 km2 and the mean annual precipitation is 745 mm and the mean annual temperature is 13 9 c annual streamflow reaches 100 million m3 which is an important water source for erhai lake and dali city the wet season in the study area is from may to october and the dry season is from november to april another year and more than 85 of the annual precipitation occurs in the wet season forest 29 6 and grassland 45 9 areas are mainly located in the uplands with their water yield flowing down through agricultural areas 21 before entering the main streams of fengyu river watershed li et al 2018a 2018b 2 2 data collection the streamflow of the watershed was monitored at the outlet from 2011 to 2016 specifically the current meter method was used from january 1 2011 to may 31 2012 river discharge measurement criterion gb50179 93 from june 1 2012 to october 31 2016 the automatic bubble water level meter waterlog h 3553 u s was installed at the watershed outlet to automatically and continuously monitor the water levels the water level flow relationship curve was established based on the measurements for the different magnitudes of streamflow and the automatic monitoring water level data was fed into the water level flow relationship curve for calculating daily streamflow at the outlet of the watershed water samples were taken daily from january 1 2011 to march 9 2014 and periodically from march 10 2014 to october 31 2016 at the watershed outlet a total of 1416 water samples were taken to detect dissolved total nitrogen dtn ammonium nitrogen nh4 n and nitrate nitrogen no3 n the metrics were determined by alkaline potassium persulfate digestion uv spectrophotometric method gb11894 89 nessler s reagent colorimetric method gb7480 87 and spectrophotometric method with phenol sulfonic respectively gb7479 87 the concentration of don was calculated as the concentration of dtn minus the sum of nh4 n and no3 n nie et al 2018 2 3 baseflow separation and don load estimation the streamflow and its load are influenced by hydrological conditions caused by storm events so they can be regarded as high frequency signals in contrast the baseflow and its load are relatively stable due to the storage effect of groundwater aquifers thus they can be regarded as low frequency signals he et al 2019 he and lu 2016 therefore the time series of river streamflow and nutrient load can be regarded as the superposition of high and low frequency signals in this study the eckhardt automated filter method eckhardt 2005 was used for baseflow and baseflow load separation based on measured flow and estimated load at the outlet station the eck filter is a two parameter recursive digital filter and the filter equation for baseflow and their load is given as eqs 1 and 2 the specific calculation process of the parameters is as follows 1 b k 1 bfi max α b k 1 1 α bfi max y k 1 α bfi max 2 l i 1 bli max β l i 1 1 β bli max l i 1 β bli max where bk yk mm d is the baseflow and streamflow at the time step k day respectively α dimensionless is the filter parameter recession constant and bfimax dimensionless is the maximum baseflow index meanwhile li li kg d is the streamflow and baseflow don load at the time step i day β dimensionless is the load recession constant blimax dimensionless is the maximum baseflow load index the parameters α β bfimax and blimax are obtained as follows firstly calculating the runoff duration after peak streamflow according to equation 3 3 n 0 83 a 0 2 where n is the number of days of streamflow after the peak and a is the catchment area of the basin n was used as 4 83 in this study which means screening the data for more than 5 consecutive days of flow recession and then exponential fitting the receding process eckhardt 2008 finally excluding the α values of r2 0 9 the average value of the remaining values was taken as the receding constant α this study selected 28 recession process curves that satisfy the conditions and calculated the receding constants for each curve fig s1 thus the recession constant α is 0 934 in this study as the bfimax index is normally given because of the stream and its aquifers zhu et al 2019 the bfimax is suggested as 0 80 0 50 and 0 25 for perennial streams with porous aquifers ephemeral streams with porous aquifers and perennial streams with hard rock aquifers respectively eckhardt 2005 according to the geological conditions of the fengyu river watershed the bfimax was assumed to be 0 8 in this study in addition the β was obtained by screening and fitting the streamflow load recession process this study uses the loadest model to estimate the don load of streamflow park and engel 2014 at the outlet station the daily don load regression model was set up based on daily flow data n 2132 and discrete don concentration data n 1416 the akaike information criterion aic and the schwarz posterior probability criterion sppc were used for the selection of the regression equations the regression models shown in equation 4 were obtained as the best model for don load estimation in streamflow table s1 and the load bias in percent bp and nash sutcliffe efficiency coefficient nse were used to justify the model performance 4 l n q load a 0 a 1 l n q f a 2 s i n 2 π d t i m e a 3 c o s 2 π d t i m e a 4 d t i m e where a0 6 are coefficients qf is streamflow and baseflow dtime is decimal time lnqload and dtime use the method of centralized transformation to solve multicollinearity problems the formula is as follows 5 l o a d c o n s t i t u e n t l o a d k g d 6 l n q l o a d l n q c e n t e r o f l n q l o a d 7 d t i m e d e c i m a l t i m e c e n t e r o f d e c i m a l t i m e based on the estimated don load of streamflow this study selected 16 load recession process curves to determine β as 0 905 fig s2 meanwhile the estimated blimax is 0 68 fig s3 by adopting the backward filtering approach collischonn and fan 2013 miller et al 2016 in particular the don load when the proportion of baseflow in streamflow is greater than 95 was assumed as the measurement of don loads in baseflow based on the schilling method schilling and zhang 2004 the coefficient of determination r2 and nash sutcliffe coefficient nse were used to evaluate the accuracy of the baseflow load separation model 2 4 abrupt analysis the nonparametric change point analysis ncpa is effective in determining the abrupt changepoints of dependent variables cao et al 2017 wu and lu 2021 in this study the variation of streamflow caused by the changing hydrological conditions results in significant changes in don concentration thus the ncpa was used to explore the abrupt threshold of the streamflow that is the most sensitive to don concentration as the availability of observed data is limited the bootstrap method is usually used to estimate the frequency distribution of each change point wu and lu 2019 since the bias corrected and accelerated bca bootstrap correcting for bias and skewness in the distribution of bootstrap estimates improves the accuracy of the frequency distribution of change point estimation this study built 1000 new random samples extracted from the original flow don data set by the bca bootstrap method and quantified the flow abrupt threshold the specific analysis steps are as follows firstly the flow variables x1 x2 xn and the corresponding don variables y1 y2 yn are arranged in ascending order then the change point i divides the don variables into two groups y1 yi and yi 1 yn is assumed 1 i n secondly the deviance value of each group was calculated using equation 8 and the deviance reduction δi is calculated based on equation 9 8 d k 1 m y k μ 2 9 δ i d d i d i where d is the deviance of the entire data y1 yn m is the sample size and μ is the average of the m observations yk i 1 2 n d i is the deviance of the data y1 yi and d i is the deviance of the data yi 1 yn finally the i value that presented the maximum δi is considered to a change point of flow 2 5 statistical analyses the loadest model https water usgs gov software loadest was used to estimate the daily don load of streamflow which was used to investigate the temporal variation of don load in the watershed meanwhile the eckhardt digital filter method was used for baseflow and baseflow don load separation and the method was automatically calculated in the bfi 3 0 tool hydrooffice https www hydrooffice org furthermore correlation analysis and abrupt analysis were used to reveal the relationship between streamflow baseflow and don the pearson correlation analysis and nonparametric change point analysis ncpa were conducted in r v 4 1 1 3 results 3 1 estimation of don loads from streamflow and baseflow due to the limitation of sampling conditions the water quality monitoring had been changed from daily sampling to periodic sampling since june 2014 meanwhile the simulated residuals are independent of each other and normally distributed scr 0 39 ppcc 1 thus this study selected the asymptotic maximum likelihood estimation amle method for streamflow daily don load estimated the calibrated coefficients and the performance of selected regression equations in the loadest model were shown in table 1 previous studies show that the acceptable level of model simulation accuracy is related to the data scale daily monthly yearly pellerin et al 2014 moriasi et al 2015 indicated that daily scale watershed n estimated results are satisfactory when the nse greater than 0 25 and bp 25 in this study the bp is 20 5 and the nse is 0 39 means the estimated results are satisfactory and the errors are within the acceptable ranges the comparisons between model estimated streamflow don load results and measured data were shown in fig 2 and fig s2 for baseflow don load separation the values of r2 and nse are greater than 0 85 fig 3 indicating that the eck filter method can be used for separating baseflow don load from streamflow load 3 2 temporal variations for don load from streamflow and baseflow baseflow is an essential source of watershed don and their correlation reaches a significant level p 0 01 fig 4 a although the percentage of baseflow to streamflow fluctuates less during different seasons their don load drops rapidly from 83 9 to 65 1 from dry to wet season fig 4a during the wet season the average don load of streamflow reached 76 6 kg day peaking at 133 5 kg day in august during the dry season the don load of streamflow was just 31 2 kg day fig 4b while the averaged baseflow don load was 50 67 kg day in the wet season and 29 7 kg day in the dry season fig 4c correspondingly the monthly don load from streamflow and baseflow reached 2 35 t month and 1 56 t month during the wet season and 0 94 t month and 0 89 t month during the dry season baseflow are the don mainly sources during the dry season fig 5 a furthermore the watershed don load increases annually and the percentage of baseflow don load maintain stability at annual scale is about 74 4 fig 5b 3 3 abrupt thresholds of the streamflow on don the ncpa results indicated the tendency of don concentration to abruptly change in response to the increasing streamflow based on the cumulative distribution of the change points the abrupt change threshold ranging from 2 03 m3 s to 2 15 m3 s with a 95 confidence interval was obtained fig 6 a combining the results of the correlation analysis between streamflow and don load we discovered don concentration will increase rapidly as streamflow increases however if the streamflow in the watershed exceeds 2 15 m3 s the increasing trend of doc concentration significantly declines fig 6b furthermore the response of don load to streamflow variation is different in different seasons fig 7 don load was best fitted to streamflow during the wet period average r2 greater than 0 9 and streamflow is mostly above the abrupt point in this season on the contrary the flow rate in the dry period is mostly lower than 2 15 m3 s and the don concentration is significantly higher than that in the abundant period 4 discussions 4 1 the characteristics of don loss and its implication for watershed water quality management long term inter annual don load variations in the fengyu river watershed are investigated from 2011 to 2016 the results indicated that the don load in the study area showed inter annual variations due to different hydrological conditions for different years and the highest annual load reached 23 1 t in 2013 furthermore the wet season is the hot moment of don loss in the watershed accounting for 72 5 of the annual load fig 4 previous studies have found that when vegetation forestland and grassland and cropland cover are higher than 70 and 5 both land use types will be the primary sources of don in this watershed lusk et al 2018 wang et al 2020 the percentages of vegetation and cropland in the fengyu river watershed were 75 5 and 21 0 respectively li et al 2018a 2018b thus don from the decomposition of organic matter from litter in cropland and vegetation will be transported to streams via surface and subsurface runoff li et al 2018a 2018b wang et al 2020 willett et al 2004 meanwhile increased precipitation during the wet season facilitates the runoff formed and accelerates the dissolution of don promoting don transport from the landscape to streams in 2020 chemical fertilizers has banned in the erhai lake basin and the increased organic fertilizer will certainly enhance the risk of don loss improved management of cropland in the wet season is essential to mitigate don export from the watershed previously more attention has been paid to the manage the streamflow environment by intercepting pollutants transported from surface runoff or soil erosion however the water quality improvement may be still limited even though the nps pollutants transported by surface runoff are well managed rivett et al 2011 the reason could be a large number of nutrients are delivered to the river by the baseflow gao et al 2018 mcmahon and nathan 2021 this study confirmed that don exported by baseflow average accounts for 74 4 of streamflow don load even reaching 83 9 during the dry season fig 4 baseflow comes from groundwater and constitutes the main component of the stream during the decline of streamflow zhu et al 2019 hence reducing the leaching of domestic and agricultural wastewater into the groundwater is key to protecting the water environment notably although the don load transported by baseflow during the wet season only accounts for 65 1 of streamflow don load the don loads were more than twice those of the dry season fig 5 the reasons are closely related to the don loss caused by agricultural activities during the wet season in the study area adame et al 2016 the previous study found that approximately 3 of the applied n fertilizers could be lost via don leaching in the paddy field nie et al 2018 there are 25 9 km2 of paddy fields in the fengyu watershed accounting for 11 8 of the total watershed area therefore don leaching from paddy soil to the groundwater aquifer might be one of the important pathways for don transport in addition the high contribution from baseflow to don load also indicated that the groundwater aquifer receives don leaching from the soil in contrast farming shifts from paddy fields to dryland crops during the dry season with a significant reduction in the leaching of cropland meanwhile the decrease in temperature and precipitation is not conducive to vegetation decomposition resulting in generally low river don loads during the dry season berman and bronk 2003 xin et al 2019 hence baseflow in the study area exports most of the don load during the wet season when there are much more don sources from the perspective of watershed water quality management reducing infiltration of field runoff is key to managing don loads in watersheds thus the construction of ditches to connect fields and ponds to form a field ditch pond irrigation system may be beneficial to reducing the infiltration of field runoff hua et al 2019a 2019b ditch drainage reduces the time of water storage in the field during the wet season reducing the leaching of cropland yang et al 2019 meanwhile by absorbing and reusing wastewater from the cropland the system also effectively reduces don concentration liu et al 2022a 2022b yan et al 2021 the aquatic ecosystems and drinking water safety are sensitive to the changing don concentration excessive don concentrations may induce the outbreak of water bloom zhang et al 2015 and release quantities of toxic disinfection byproducts during the processes of potable water production consequently identifying the abrupt change point of the don concentration contributes to assessing the risk of river eutrophication and managing the human production process of drinking water here this study revealed that the streamflow is greater than 2 15 m3 s the don concentration in the river decreases significantly but its export load to the receiving water body erhai lake increases rapidly fig 6 and fig 7 therefore if the don concentration in the river is a concern management practices should be implemented when streamflow is below 2 15 m3 s while if the don load in the receiving lake is a concern management practices should be implemented to regulate the land use pattern to reduce the don load of the watershed 4 2 processes affecting modeling of don loads and improvement although the model error for don load estimation in this study was within the acceptable range table 1 the accuracy of the simulation results was lower than in other studies loadest model pellerin et al 2014 it is possible to hypothesize that these conditions are related to the following two main aspects firstly this study used a mixture of daily continuous 2011 1 1 2014 3 9 and periodic water quality monitoring data 2014 3 10 2016 10 31 in the process of establishing the regression model sampling data with different monitoring frequencies could increase the estimated error sharifi et al 2017 several reports have shown that the more frequent sampling data did not necessarily lead to more accurate and precise annual pollutant load estimates park and engel 2014 the most effective sampling strategy depends on the duration of the monitoring and the variations of streamflow this study is a six years long term investigation thus taking the fixed period semimonthly sampling 2014 2016 may result in not only the least biased but also the most precise loads robertson and roerish 1999 secondly the existing methods are not able to directly measure the amount of don in the stream nie et al 2018 since don consists of complex compounds including urea dissolved free amino acids proteins nucleic acids and amino sugars yang et al 2019 thus the don data in this study were calculated using the differential method since don was influenced by the accuracy of other metrics measurements dtn nh4 n no3 n which greatly increases the model uncertainty park and engel 2014 sharifi et al 2017 loadest used streamflow and periodicity as predictors to estimate don load using the regression model the accuracy of the model is highly dependent on measured data for flow and don concentration thus the regression model may not fully represent the dynamics of don loads machine learning has been proposed more conducive to establishing stable and highly accurate load estimates wherry et al 2021 therefore different models including regressions and machine learning could be combined for estimating the in stream don load in future studies 5 conclusions don accelerates eutrophication in streams and threatens drinking water safety in agricultural watersheds river however don load fluxes and export characteristics remain unclear here we monitored the long term variation of don in the fengyu river watershed and the watershed don load was found to be increasing annually in particular the highest total annual don load reached 23 1 t in 2013 and the don loss was concentrated in the wet season the average load reaches 76 6 kg day and the cumulative load reaches 14 1 t accounting for 72 5 of the annual load meanwhile the baseflow export don average accounts for 74 4 of streamflow don load especially reaching 83 9 during the dry season becoming the main export pathway of don in the watershed furthermore the abrupt threshold of streamflow on don concentration is 2 03 m3 s to 2 15 m3 s our study could contribute to understanding the characteristics of don export in agricultural watersheds and establishing measures for the protection of drinking water safety credit authorship contribution statement qiyu xu conceptualization methodology formal analysis software writing original draft writing review editing limei zhai supervision validation funding acquisition xinru liu conceptualization methodology xinzhong du conceptualization supervision writing review editing resources funding acquisition hongbin liu validation supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was funded by the national natural science foundation of china grant nos 42107076 u20a20114 fundamental research funds for central non profit scientific institution no 1610132022008 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 129054 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
2432,this study investigates the climatological spatial scales csss of meteorological droughts in china and the linkages to climate variability the global precipitation climatology centre monthly gridded precipitation with a spatial resolution of 25 km 25 km for 1961 2010 is used the standardized precipitation index at different timescales 1 3 6 9 12 24 36 and 48 month is applied to characterize meteorological droughts the csss of meteorological droughts are calculated using a method considering spatial correlation and anisotropy the relationships between the csss and main monsoons and climate teleconnections in five selected regions are quantified using dynamic spatial panel models the five regions are south china sc the yangtze river valley yrv north china nc northeast china ne and the tibetan plateau tib the following results are obtained 1 the csss of multi timescale meteorological droughts in china are determined the means of the csss at various timescales in china are 68 9 73 8 71 8 69 7 68 8 65 9 62 9 and 60 8 104 km2 respectively the csss in eastern china generally show a significant decrease with increasing timescale 2 significant quantification relationships r2 greater than 0 85 suggest that there is not a simple linear relationship between climate anomalies and the csss but rather a complex spatiotemporal interaction including exogenous and endogenous interaction effects the csss not only strongly depend on their neighbors on the same timescale but also significantly depend on the csss of neighbors on adjacent timescales three monsoon indices indian monsoon east asian monsoon and western north pacific monsoon have significant impacts on the multi timescale css variations in the five regions especially for ne yrv sc and tib while the el niño southern oscillation and pacific decadal oscillation affect mainly sc and yrv these findings could broaden our understanding of the spatiotemporal relationships of droughts and be useful for drought risk management keywords standardized precipitation index multi timescale climatological spatial scales meteorological drought climate variability china data availability we have shared the link to the data in section availability statement 1 introduction drought is one of the most serious and extensive natural disasters it has had severe impacts on the environment society and the economy in many regions of the world the continually intensifying global warming raises concerns about more frequent and severe droughts in the future seneviratne ciais 2017 hoegh guldberg et al 2018 su et al 2018 therefore it is important for enriching the understanding of drought spatial and temporal scales are key elements in studying the complexity of various drought processes and phenomena multi scale interactions between the atmosphere ocean and land often suggest different physical meanings and present complex multi wave structures mishra singh 2011 zhang zhou 2015 kiem et al 2016 mardian 2022 the complex interactions lead to a variety of drought characteristics at different temporal and spatial scales our level of understanding of drought processes is highly dependent on their various temporal and spatial scales van loon 2015 zhao et al 2018 hao et al 2018 nowadays the multi timescale characteristics of drought have been studied widely drought indices at different timescales are often used to quantify drought variation for instance considering the significant seasonal characteristics of droughts many studies focus on a 3 month or seasonal timescale e g hao et al 2018 li et al 2015 liu et al 2019 xu et al 2015 zhai et al 2017 zhan et al 2020 a single scale cannot accurately describe drought propagation attributes because the interaction among different types of droughts in drought propagation involves multi scale processes wu et al 2021 to better understand drought and its impacts more multi scale analysis of drought has been undertaken liu et al 2017 veettil and mishra 2020 zhang et al 2018 zhou et al 2019 the temporal and spatial scales of drought are generally related to the typical scales of meteorological and hydrological phenomena van loon 2015 which indicates the significant scale dependence of drought similar to the multi timescale features of drought the drought also can occur on a wide range of spatial scales from the movement of soil water on a hillslope to global atmospheric circulation mardian 2022 we note that many studies pay more attention to the spatial distribution of drought in areas that often present a wide range of sizes zhou et al 2020 du et al 2020 emphasized the importance of quantifying the climatological spatial scales or spatial extent csss of extreme precipitation events hence it is reasonable to infer that drought should be characterized by multi spatial scales leelaruban and padmanabhan 2017 investigated the variation of spatiotemporal characteristics of droughts under multi spatial scales e g national regional state climatic division and county scales and verified the spatial scale dependence of drought characteristics fluixá sanmartín et al 2018 searched for the optimal drought index and timescale combination to detect different regional scale drought characteristics considering differences in drought monitoring at the site scale and regional scale spinoni et al 2019 quantified meteorological drought events for a country and a region using the 0 75 standard deviation sd as a drought threshold rather than 1 0 sd zhou et al 2020 investigated the spatial extent of meteorological droughts at multiple timescales 1 3 6 and 12 month over the poyang lake basin and indicated that spatial extent is potentially associated with timescale in addition many studies on the spatiotemporal dynamics of drought focus on different drought patch scales i e the contiguous spatial extent of drought ranging from thousands of square kilometers to hundreds of thousands of square kilometers e g lloyd hughes 2012 ren et al 2012 xu et al 2015 zhai et al 2017 guo et al 2018 liu et al 2019 zhou et al 2019 diaz et al 2020 moreover different spatial patches are used even in the same study area for example on a global scale sheffield et al 2009 herrera estrada et al 2017 zhan et al 2020 additionally some studies the issue on the spatial scales of extreme precipitation drought events touma et al 2018 identified regional and seasonal variations in the climatological length scales of extreme daily precipitation over the united states du et al 2020 quantified the spatial scales of heavy meiyu precipitation events zhou et al 2020 proposed a framework for measuring the spatial scales of drought considering spatial anisotropy these studies emphasize the role of spatial scale analysis in drought research and broaden our understanding of drought characteristics however it is unclear whether drought can occur across all spatial scales more importantly what the relationship is between the spatial scale and timescale and what the possible causes are which might be a major reason why drought characteristics are not easily revealed lloyd hughes 2012 hao et al 2018 zhou et al 2019 thus the object of this study is to investigate multi spatial scale characteristics of meteorological droughts and the possible linkage with climate variability the study is organized as follows section 2 gives the study area introduces the observations gridded precipitation datasets and five main climate teleconnections and examines the performance of the gridded precipitation section 3 mainly describes the methods for studying multi scale characteristics of meteorological drought these include the standardized precipitation index the spatial scale identification method and the dynamic spatial panel models the results of the multi spatial scale features of meteorological drought and its possible factors are provided in section 4 sections 5 and 6 contain a discussion and conclusion respectively 2 study area and data two precipitation datasets are used in this study one is the global precipitation climatology centre monthly land surface precipitation dataset with a spatial resolution of 0 25 0 25 i e 25 km 25 km for 1891 2016 hereafter gpcc it is available from the deutscher wetterdienst https www dwd de en ourservices gpcc gpcc html the other is the monthly observational precipitation from 839 meteorological stations in china for 1961 2010 50 years it is obtained from the national meteorological information centre of the china meteorological administration https data cma cn the observations are used to evaluate the performance of the gpcc the results show that most station grid pairs 93 68 have excellent consistency r 0 90 and the minimum correlation is greater than 0 5 p 0 05 fig 1 which indicates that the gpcc is highly accurate although the accuracy of the gpcc precipitation dataset in other areas especially outside china has not been tested it can be speculated that it still has good performance adnan et al 2016 schneider et al 2018 spinoni et al 2019 iqbal et al 2022 thus the gpcc monthly precipitation for 1961 2010 is used in this study we use the dataset to investigate the csss of meteorological droughts at different accumulation scales at any location in china to obtain the spatial scales of meteorological droughts along national boundaries the larger area within 4 n 70 n latitude and 60 e 150 e longitude is taken into account fig 1 there are more than 100 000 grid points with a spatial resolution of 0 25 0 25 within the selected region and about 15 000 grid points in china monsoons and teleconnections are two climatic phenomena that affect precipitation variability in china ding and wang 2016 in this study three important monsoon indices the indian monsoon index imi east asian monsoon index eami and western north pacific monsoon index wnpmi and two major climate teleconnections the el niño southern oscillation enso and pacific decadal oscillation pdo are selected to interpret the multi spatial scale variation of meteorological droughts the monthly eami can be calculated using the index proposed by zhao et al 2015 based on ncep reanalysis data https downloads psl noaa gov the daily imi and wnpmi are available from the monsoon monitoring page maintained by the university of hawaii http apdrc soest hawaii edu projects monsoon realtime monidx html the daily series are aggregated into monthly series the monthly niño3 4 index characterizing enso and pdo index are obtained from the national oceanic and atmospheric administration national centers for environmental information https www ncdc noaa gov climate monitoring wx patterns five regions of interest south china sc the yangtze river valley yrv north china nc northeast china ne and the tibetan plateau tib are selected because they have significant differences in climatic conditions zhang zhou 2015 chang et al 2019 fig 1 the relationships between the multi spatial scales of meteorological droughts and the selected five climate anomalies are analyzed in the above five regions 3 methodology 3 1 standardized precipitation index the standardized precipitation index spi proposed by mckee et al 1993 is applied to characterize meteorological drought because of its advantages of multiple timescales and comparability across different climate zones hayes et al 2011 the spi at various timescales 1 3 6 9 12 24 36 and 48 month is used meteorological drought levels and the corresponding probabilities are given table 1 the detailed procedures of the spi are introduced by zhou and liu 2016 3 2 a method for determining the climatological spatial scales of meteorological droughts the climatological spatial scales or spatial extent csss of meteorological droughts are obtained based on the multi directional climatological spatial length cl because of the spatial anisotropy of droughts the cl is defined as the average length scale of meteorological droughts in a given direction and time the semivariogram is often used to test whether there is spatial autocorrelation in the sample data we use the semivariogram to calculate the cl of meteorological droughts the semivariogram quantifies the squared difference between the values of two points as a function of the distance between those points by calculating the semivariogram for multiple pairs of points in a given direction and time we can find the distance at which points with drought values are no longer substantially correlated in that direction and time we use this distance to quantify the length scale of drought for that given direction and time a method considering spatial anisotropy proposed by zhou et al 2020 to measure the spatial scales of meteorological droughts is used in the study using an example of a grid point 114 125 e 29 875 n a brief description is given fig 2 a dry wet map over the selected region can be obtained using the spi at a given timescale e g a t month timescale then a drought threshold spi t 1 is used to judge whether a grid point is under drought or not after drought identification every grid point in the region will be binarized that is a grid point under drought has the value of 1 and a non drought grid point has the value of 0 fig 2a and e the number of drought months at a given point can be counted e g fig 2a a scanning window for a grid point with drought is constructed fig 2e certain parameters are applied to shape the scanning window including search direction θ unit search radius sr unit km distance step h unit km bandwidth bandw unit km and angle tolerance α unit generally when the parameters sr and h are specified there is a total of g g sr h polygons based on the scanning window the number of 1 1 and 1 0 pairs within each polygon gray polygon can be counted fig 2b and c for a selected grid point with drought the relationship between semivariance values and distances is analyzed the mean semivariance value γ k φ of the k th polygon k g can be calculated using expressions 1 and 2 fig 2c and d 1 γ a b 1 2 z x a z x b 2 2 γ k φ γ 1 1 φ y c 1 1 θ k φ γ 1 0 φ y c 1 0 θ k φ φ y c 1 1 θ k φ φ y c 1 0 θ k φ where γ a b represents the semivariance of the a b point pair z xa and z xb represent the variable value 0 or 1 of location xa and location xb after binarization respectively xa xb represents the distance from point b to point a according to the theoretical semivariogram the semivariance value of a 1 1 point pair is γ 1 1 0 and γ 1 0 0 5 for a 1 0 point pair φ y c 1 1 θ k φ and φ y c 1 0 θ k φ represent the number of 1 1 point pairs and 1 0 point pairs within the k th polygon along the direction of θ during the selected periods i e the set φ respectively the set y represents the whole study period from 1961 to 2010 the climatological spatial length clθ along a search direction θ can be calculated using an exponential model fig 2d aalto et al 2016 berndt et al 2014 touma et al 2018 the range r0 is considered the cl namely clθ r0 because there is little spatial correlation when the distance is larger than the range r0 further the csss can be obtained based on the multi directional cl more details can be found in zhou et al 2020 we specify some parameters i e h 25 km bandw 56 95 km α 22 5 and m 8 of the method which is related to the used grid precipitation dataset i e gpcc given the regular grids 25 km 25 km of the dataset used the distance step is set as h 25 km to reduce sampling error as much as possible we consider 8 search directions counterclockwise at 45 intervals i e m 8 the angle tolerance and bandwidth are set at α 22 5 and bandw 56 95 km fig 2a and 2e which is designed to eliminate the interference between point pairs within two adjacent polygons reduce the effects caused by increasing polygon size and ensure sufficient point pairs the different bandwidths bandw from 15 53 km to 201 93 km are investigated in our previous study zhou et al 2020 the search radius sr 500 km is specified moreover a dynamic search radius is also adopted with the maximum constraint of 1000 km however we investigate the sensitivity of the search radius sr in section 5 because the previous tests were conducted in a relatively small area 3 3 dynamic spatial panel models drought and other extreme phenomena are usually characterized by spatial interaction effects spatially the drought condition at a location is generally affected by the surrounding drought conditions and vice versa meanwhile the possible dependence between the spatial scale and timescale of meteorological droughts is considered therefore we select a dynamic spatial panel model to investigate the impacts of climate anomalies on multi scale characteristics of meteorological droughts a dynamic general spatial nesting model is a linear regression model extended to include spatial lag in the dependent variable the temporal lag of the dependent variable the explanatory variables the error term or some combination thereof lee and yu 2010 its expression is as follows 3 y t τ y t 1 ρ w y t η w y t 1 x t β w x t θ μ ξ t u t a n d u t λ w u t ε t where y t represents a vector n 1 of the log of the csss of meteorological droughts at timescale t n is the number of grids in each region as shown in fig 1 x t is a matrix n k of exogenous explanatory variables associated with the coefficient vector β k 1 w is a spatial weight matrix n n describing the neighbors of a grid wy t represents the endogenous spatial lag wx t is the exogenous spatial lag and wu t is the spatial lag among the error terms y t 1 represents the one period temporal lag of the dependent variable and wy t 1 is the temporal and spatial simultaneous lag of the dependent variables in this study the one period lag is considered the one timescale lag because the drought is characterized by multiple timescales it is helpful to analyze the relationship between the spatial scale and timescale of meteorological droughts furthermore the scalers τ ρ η and λ and the vector θ k 1 of the parameters are used to depict the strength of these spatial lags furthermore μ is a vector n 1 of spatial fixed effects ξt is a time fixed effect and ε t is a vector n 1 of the error terms if the scalar λ is equal to 0 the nesting model is regarded as a dynamic spatial dubin model sdm if the scalar λ is equal to 0 and the vector θ is the vector of 0 the model is simplified to a dynamic spatial lag autoregressive model sar these two simplified dynamic spatial models will be used to investigate the relationships between the climate anomalies and multi scale characteristics of meteorological droughts in this study the simple procedures are as follows 1 the spatial panel data which include the spatial grids in one region the csss of meteorological droughts at multiple timescales and the exogenous explanatory variables are collected t is the timescale t 1 3 6 9 12 24 36 and 48 month the exogenous explanatory variables are imi eami wnpmi enso and pdo respectively thus k 5 the correlation coefficient between the drought series of a grid and a climate anomaly is regarded as the intensity of the impact of the climate anomaly on the grid 2 a spatial weight matrix is constructed a simple rule that grids that share a boundary will be neighbors is used in this study the value of 0 in the matrix means that a grid does not have neighbors 3 whether spatial fixed effects time period fixed effects or two way fixed effects in the spatial panel models need to be included can be tested using likelihood ratio lr tests if the null hypothesis that the spatial fixed effects time period fixed effects are jointly insignificant can be rejected p 0 05 this means that the spatial fixed effects time period fixed effects should be included in the spatial models if the two above hypotheses can be rejected this suggests that two way fixed effects should be considered 4 the dynamic sdm is first applied to investigate the relationship between the climate anomalies and the csss of meteorological droughts then the wald test and lr test are used to judge whether the dynamic sdm can be simplified to dynamic sar the null hypothesis is that the dynamic sdm can be simplified to dynamic sar further details can be found in elhorst 2014 4 results 4 1 climatological spatial lengths of meteorological droughts in 8 directions for meteorological droughts at different timescales the frequency distributions of cls in china for each search direction are given in the left panel of fig 3 the cls of most grid points 60 for all timescales are 500 km and the proportion generally becomes larger as the timescales increase fig 3 for example in terms of 0 it is 66 71 for spi 1 59 80 for spi 3 62 30 for spi 6 63 47 for spi 9 64 72 for spi 12 63 56 for spi 24 65 89 for spi 36 and 66 67 for spi 48 the results indicate that the search radius of 500 km should be reasonable section 5 in addition the mean cl sd in the same direction shows a decreasing increasing change with increasing timescale fig 3 generally the mean cl at 0 decreases from about 500 km for the 3 month scale to about 440 km for the 48 month scale while the sd increases from 196 km to 230 km there are relatively large differences in the mean cl in different directions for the same timescale for example about 530 km at 45 is significantly greater than 450 km at 270 for the 3 month scale the cl of meteorological droughts presents a significant directional dependence the sds of cls in the 8 directions for each timescale are shown in the right panel of fig 3 for spi 1 the sds of the 8 direction cls of 30 33 of the total grid points are larger than 200 km and up to 86 00 for those larger than 100 km this phenomenon is also observed at other timescales 28 09 200 km for spi 3 32 94 for spi 6 37 92 for spi 9 42 25 for spi 12 48 28 for spi 24 50 49 for spi 36 and 55 58 for spi 48 moreover the proportion of sds larger than 100 km increases from 85 81 to 96 03 along with increasing timescale these findings suggest that the spatial anisotropy of meteorological droughts is confirmed and its degree increases along with timescale 4 2 statistics of the climatological spatial scales of meteorological droughts the frequency distributions of the csss of multi scale meteorological droughts in china are characterized by positive skewing fig 4 a the mean csss are larger than the median for all timescales fig 4b it should be noted that almost all statistics of csss show decreasing trends as timescale increases for example the medians of the csss of meteorological droughts at the 1 month to 48 month timescales are 64 6 70 0 68 6 65 8 65 0 62 4 57 8 and 55 8 104 km2 respectively this phenomenon is in line with the change trends of cls in the 8 directions fig 3 the csss of multi scale meteorological droughts in china show a large scope ranging from 20 104 km2 to 130 104 km2 from the 5th to 95th percentile while the csss of about 50 of grid points from the 25th to 75th percentile are mainly between 40 104 km2 and 90 104 km2 additionally the high csss the 75th percentile become smaller with increasing timescales which indicates that the spatial aggregation patterns of high csss might weaken and change although positive skewed distributions are also shown at the regional scale there are significant differences in the statistics of the csss fig 4 and table 2 the medians or means of the frequency distributions of the csss in different regions generally move toward smaller values with increasing timescales except for tib table 2 the medians of the csss at 1 month to 48 month scales in ne decrease from 76 10 104 km2 to 45 80 104 km2 while that in tib shows a trend of first increasing and then decreasing a similar phenomenon is present in the 5th percentile of the csss moreover the 5th percentile in tib ranges from 15 52 104 km2 to 19 05 104 km2 which is significantly less than that in the other four regions e g from 25 36 104 km2 to 63 03 104 km2 in yrv the 95th percentile of the csss in tib from 133 26 104 km2 to 153 09 104 km2 is in contrast higher than that in the other four regions e g from 95 48 104 km2 to 125 81 104 km2 in nc it should be noted that there are significant shifts in the median of the csss in yrv and sc fig 4e and 4f they appear at the 6 month timescale in sc 63 98 104 km2 and at the 24 month timescale in yrv 78 48 104 km2 table 2 these phenomena suggest that the csss of meteorological droughts depend on the timescale and they vary in different climate zones 4 3 spatial patterns of the climatological spatial scales of meteorological droughts fig 5 shows the spatial distributions of the csss for multi timescale meteorological droughts including short term scales i e 1 month and 3 month mid term scales i e 6 month 9 month and 12 month and long term scales i e 24 month 36 month and 48 month for meteorological droughts at short term scales fig 5b and 5c spatially there are three regions with high csss they are in western mainly on the tibetan plateau north northeastern and southeastern china which is consistent with the statistics in table 2 and fig 4 topographically these high csss are distributed mainly at altitudes above 4 000 m and below 500 m fig 5a moreover the csss of meteorological droughts at short term scales have significant p 0 01 increasing changes with longitude 0 73 104 km2 deg for the 1 month timescale 0 36 104 km2 deg for the 3 month timescale although there are insignificant increasing changes with latitude there are several places with consistently high low csss for example high values around 25 n and 33 n and low values around 28 n in comparison to the short term scales the distribution of csss for the mid term scales has greater spatial fragmentation especially in southeastern china fig 5d e and f the csss in sc yrv and nc decrease with increasing timescale while the high values are still distributed mainly on the tibetan plateau in north northeastern china and in a small area in southeastern china therefore the meridional and zonal trends of the csss in china indicate significantly increased trends p 0 05 the slope 0 98 1 30 104 km2 deg in latitude is significantly larger than that in longitude 0 19 0 27 104 km2 deg the csss of meteorological droughts at the long term scales show spatial patterns with the greatest spatial fragmentation compared to the short and mid term scales fig 5g 5 h and 5i compared with those in the other four regions the csss at long term scales in ne decrease significantly table 2 the csss for long term scales show decreasing trends 0 24 0 12 104 km2 deg with longitude which suggests that the spatial aggregation characteristics weaken with increasing timescales particularly in eastern china although the low csss show a more widespread spatial distribution for the long term scales there is a new region with high values the north central part of china meanwhile the correlations between climate anomalies and multi timescale meteorological droughts are analyzed figures s1 s5 only those that pass the significance test p 0 05 are shown the results indicate that the impacts of the three monsoon indices and two major climate anomalies on meteorological drought have significantly regional characteristics and the regional impacts depend on the timescale nc yrv and sc are affected relatively singly by climate anomalies in comparison to ne and tib for instance in terms of yrv the imi shows a significant negative correlation with spi 1 and a significant positive correlation with mid term and long term meteorological droughts the wnpmi has a negative correlation with short term and mid term meteorological drought the eami presents a positive correlation with meteorological drought while intensity decreases with increasing timescales the niño3 4 is the opposite of the imi the pdo is significantly correlated only with long term meteorological droughts and the positive negative spatial pattern is complex for tib the imi shows a relatively consistent significant positive correlation for almost all timescales the impacts of the eami on droughts in the region are compound and the intensity and scope generally decrease along with timescale however the pdo is negatively correlated with the long term droughts in this region 4 4 effect of climate variability on the climatological spatial scales of meteorological droughts we perform lr tests for each region to judge whether spatial fixed effects time period fixed effects or two way fixed effects need to be included table s1 the results suggest that two way fixed effects should be included in the spatial models subsequently the wald test and lr test are carried out to investigate whether the dynamic sdm can be simplified to the dynamic sar table s2 according to the results of the tests the likely dynamic spatial models are specified dynamic sars for ne nc and yrv and dynamic sdms for sc and tib furthermore in table 3 we give the estimated results of the dynamic spatial panel models for the five regions for yrv significant endogenous interaction effects of the csss are revealed using the dynamic sar the csss of meteorological droughts depend strongly on their values in the previous timescale the coefficient τ is 0 1102 with high significance p 0 01 the coefficient η of the csss in the neighboring grids in the previous timescale i e simultaneous timescale and spatial lags is 0 0134 with high significance p 0 01 moreover the css of a grid is significantly affected by that of its neighbors on the same timescale ρ 0 8506 p 0 01 except for the imi its coefficient is positive but insignificant the other four climate anomalies have significant contributions to the csss of meteorological droughts the wnpmi eami and niño3 4 show significant negative effects on the csss with coefficients of 0 1686 p 0 01 0 3036 p 0 01 and 0 0373 p 0 10 while the pdo has a significant positive effect 0 0603 p 0 05 for tib significant endogenous interaction effects of the csss and exogenous interaction effects of the independent variables i e imi and eami are revealed using the dynamic sdm table 3 the coefficients τ η and ρ are positive with high significance p 0 01 which suggests the important roles of the previous timescale and neighboring grids in the csss due to the complex spatial patterns of the effects of climate anomalies figure s1 s5 only the imi has a significant impact on the csss in tib 0 6548 p 0 01 it should be noted that there are two significant spatial lag terms of independent variables namely w imi and w eami the coefficient of w imi is 0 8491 p 0 01 this phenomenon suggests that the positive impact of the imi on the csss in neighboring grids could produce a significant negative impact on that in the current grid the coefficient of w eami is 0 5769 p 0 10 which suggests that the positive impact of the eami on the csss in neighboring grids could play an active role in that in the current grid similarly the significant roles of the spatial and timescale lag terms in the csss are observed in the other three regions the imi 0 0841 p 0 10 eami 0 1136 p 0 05 and pdo 0 0365 p 0 05 play important roles in the csss in ne while only the wnpmi 0 2533 p 0 05 does so in nc all five climate anomalies and their spatial lag terms have significant effects on the csss in sc while the impacts of the climate anomalies and their spatial lag terms are opposite for instance the eami 0 9175 p 0 05 and niño3 4 1 4215 p 0 01 show a positive role in the csss in sc while w eami 1 3454 p 0 01 and w nino 3 4 1 0502 p 0 01 are negative these results suggest that there is not a simple linear correspondence between climate anomalies and the csss of meteorological droughts but rather a complex temporal and spatial interaction 5 discussion 5 1 uncertainty of measuring climatological spatial scales of meteorological droughts a css identification method is used in this study the parameter settings of the method and the absence of data might affect the csss 5 1 1 sensitivity testing of the scanning window search radius since the study is conducted in different climate zones there is a need to discuss the sensitivity of the scanning window search radius taking spi 3 as an example the statistics of multi directional cls under different radii are shown in fig 6 when the search radius is set at 500 km the cls of approximately 61 4 of the total grid points are smaller than that search radius fig 6a while this is true for only approximately 29 1 at 300 km fig 6b and up to 83 4 at 700 km fig 6c under the search radius of 700 km the spatial anisotropy e g sd 200 km of more grid points becomes weaker and the probability of high csss becomes larger fig 6d and 6e this phenomenon suggests that the increment e g δ 200 km of the radius has a significant amplifying effect on the cl particularly for a search radius larger than 500 km considering the lower variability and the need for most grid points the search radius of 500 km is specified for each scanning window the drought threshold spi 1 is used in the study but we find that the different drought thresholds can influence the spatial scales of droughts figure s6 and s7 in the future we would study the impact of the threshold on the spatial scale of drought 5 1 2 impact of the default processing on climatological spatial lengths due to the failed statistical distribution fitting i e absence of spi values and the failed semivariation function fitting i e absence of precipitation data over the ocean there are some null values cls in some of the 8 directions we attempt to fill in the null values by calculating the average cl using a 3 3 window in its corresponding direction similarly taking spi 3 as an example on average the number of null values accounts for 8 67 of total grid points 15127 fig 4b which are distributed mostly along coastal areas of those the largest ratio at 180 is 12 27 while the smallest ratio at 90 is 3 65 a total of 9154 grid points has effective cls in all 8 directions the functional relationship y a xb between the mean cls i e x and csss i e y of all 9154 grid points is regarded as the reference fig 7 a the remaining 5973 grid points with null values are filled using the window averaging method two processing schemes are calculated and compared one is to use the fitted functional relationship i e y a xb fitted css and the other is to fill by calculating the area of the polygon consisting of the 8 directions and their cls i e filled css the results are given in fig 7 the mean cls and csss show an excellent power function relationship y 3 06x 1 96 n 9154 r2 0 9892 p 0 05 fig 7a the two processing schemes have good consistency slope 0 99 n 5973 r2 0 9842 p 0 05 fig 7b moreover the relative errors are distributed mainly between 10 and 10 fig 7c the mean error is close to 0 0 06 the spi at other timescales shows similar findings not shown these results suggest that the window averaging scheme could be used for addressing the grid points with null values 5 2 the climatological spatial scales of meteorological droughts and the linkage with climate anomalies 5 2 1 rationality of the climatological spatial scales of meteorological droughts the csss of multi timescale meteorological droughts in china are quantified for instance we find that meteorological droughts at the 6 month scale have spatial scales with a mean radius and extent of about 480 km sd 200 km and 71 8 104 km2 drought events of similar spatial extent are frequently observed e g lloyd hughes 2012 zhai et al 2017 spinoni et al 2019 from the perspective of the spatiotemporal continuity of drought events the area affected by a drought event can reach up to 106 km2 xu et al 2015 diaz et al 2020 schumacher et al 2022 we note studies on the spatial scales of extreme precipitation events touma et al 2018 reported that the cls of daily extreme precipitation events ranged from 200 to 400 km over the eastern us du et al 2020 found that heavy meiyu precipitation events had regular spatial scales with an average length width and extent of about 1 400 km 500 km and 40 104 km2 respectively these studies are based on spatial isotropy and spatial morphology the results reported above confirm the importance of considering spatial anisotropy and also provide a comparative reference for the spatial scales of meteorological droughts meanwhile we find that the csss of meteorological droughts in china present a decreasing trend with increasing timescales based on the statistics of spatial areas of drought events rouault and richard 2005 found that the average drought area spi 1 on the 6 month scale was a bit larger than that on the 24 month scale which was consistent with the results shown in fig 4a and b zhai et al 2017 indicated that the main drought affected areas showed a significant decreasing trend along with duration these studies provide some indirect support for the reasonability of our results 5 2 2 linkage with climate anomalies the statistics and spatial distributions of the csss in the five regions ne nc yrv sc and tib vary with timescale which might be controlled by multiple large scale climate variability shelton 2009 sun et al 2020 the three monsoon indices imi eami and wnpmi have significant impacts on the multi timescale csss in the five regions table 3 and figure s1 s5 especially ne yrv and sc belonging to the east asian monsoon region which agrees with the widely reported influence of these indices on precipitation variation over eastern china sun and wang 2015 zhang zhou 2015 ding wang 2016 however only the imi and its spatial lag term w imi among these three indices have a significant impact on tib p 0 01 which is consistent with the results reported by chang et al 2019 who found that the imi mainly affects precipitation variability in tib on the intra annual 0 5 1 year and inter annual 2 10 year timescales the wnmp and eami are developed mainly for describing precipitation and air temperature variations over east asia while tib is usually not directly affected by east asian monsoon systems due to its high elevation above 4 000 m fig 1 however it should be noted that the spatial lag term of the eami w eami indicates a significant effect on the csss of droughts in tib p 0 10 this phenomenon might be related to the 200 hpa upper atmosphere zonal wind considered in the eami while the wnmp considers just the 850 hpa lower atmosphere zonal wind wang et al 2001 zhao et al 2015 the positive impact 0 5769 p 0 10 of the eami on the csss of the neighbors in tib could play an active role in tib through the transmission of monsoon circulation anomalies in the mid upper troposphere 500 200 hpa zhao et al 2015 sun wang 2018 he et al 2019 enso has a significant negative impact on the spatiotemporal variation of the csss in yrv and a positive impact in sc the pdo is the opposite additionally it also negatively contributes to ne table 3 this phenomenon might be related to the phase relationship between these two teleconnections ma shao 2006 shelton 2009 zhang et al 2017 enso affects regional precipitation by modulating the intensity and extent of the western pacific subtropical high and the impacts of the pdo are attributed mainly to alterations in the intensity of the east asian monsoon and westerly circulation zhou et al 2008 li et al 2014 sun et al 2020 zhang et al 2022 the results of the spatial models confirm the spatial interaction effects and timescale dependence of the csss of meteorological droughts however a limited number of timescales from 1 month to 48 month scales are used in this study which may not well reveal the impacts of multi scale weather and climate systems these climate anomalies may have significant effects on precipitation anomalies just at their prominent timescales further investigation should be carried out from the perspective of the dominant timescales 6 conclusions this study investigates the csss of multi timescale meteorological droughts in china and their quantification relationship with three important monsoon indices imi eami and wnmp and two major teleconnections enso and pdo in five regions ne nc yrv sc and tib the main conclusions are as follows 1 the csss of multi timescale meteorological droughts i e 1 3 6 9 12 24 36 and 48 month timescales are determined the frequencies of the csss of the various timescale meteorological droughts in china present positive skewed distributions the means of the csss of meteorological droughts at various timescales in china are 68 9 73 8 71 8 69 7 68 8 65 9 62 9 and 60 8 104 km2 respectively which are generally larger than their median values 64 6 70 0 68 6 65 8 65 0 62 4 57 8 and 55 8 104 km2 additionally the large differences in the cls in eight directions confirm the spatial anisotropy of meteorological droughts the sds of the eight direction cls increase along with timescale the proportion generally increases from about 28 to 56 for sds larger than 200 km and from about 85 to 96 for sds larger than 100 km 2 except for tib the csss of meteorological droughts in the other four regions of interest ne ec yrv and sc generally show a significant decrease with increasing timescales statistical distribution characteristics similar to those at the national scale are observed in these five regions meanwhile it is found that the csss in tib have a larger fluctuation range from about 18 104 km2 to 145 104 km2 from the 5th to 95th percentile than that in the other regions for instance 40 104 km2 to 128 104 km2 for yrv these results are consistent with the distinct changes in the spatial patterns of the csss of meteorological droughts at short mid and long term scales although the high csss show significant regional characteristics the spatial fragmentation of high csss increases along with timescale 3 the quantification relationships between the csss of meteorological droughts and five important climate anomalies are revealed in the five regions using dynamic spatial panel models the spatial interaction effects and timescale dependence of the csss of meteorological droughts are confirmed there is not a simple linear correspondence between climate anomalies and the csss of meteorological droughts but rather a complex temporal and spatial interaction for all regions the csss of meteorological droughts not only strongly depend on their neighbors on the same timescale but also significantly depend on the csss of the neighbors on the previous timescale moreover the results indicate that ne is affected mainly by the imi eami and pdo except for the imi the other four climate anomalies have significant impacts on yrv while only the wnpmi negatively affects nc tib is significantly affected by the indian monsoon imi and w imi and the teleconnection mechanism of the east asian monsoon w eami all five climate anomalies contribute to the css variation of meteorological droughts in sc the ccss of multi timescale meteorological droughts and their relationship with climate variability could help understand the spatiotemporal dynamics of droughts and provide implications for improving the management of drought risk 7 data availability statement the gpcc is available from https www dwd de en ourservices gpcc gpcc html the observational precipitation can be obtained from https data cma cn ncep reanalysis data is available from https downloads psl noaa gov the imi and wnpmi are available from https apdrc soest hawaii edu projects monsoon realtime monidx html the niño3 4 index and pdo index are obtained from https www ncdc noaa gov climate monitoring wx patterns credit authorship contribution statement han zhou conceptualization funding acquisition data curation investigation formal analysis methodology visualization writing original draft writing review editing wen zhou data curation investigation formal analysis methodology visualization writing original draft supervision writing review editing yuanbo liu conceptualization supervision project administration formal analysis writing review editing jiejun huang formal analysis validation writing review editing yanbin yuan funding acquisition formal analysis writing review editing yongwei liu formal analysis writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the national natural science foundation of china nos 42001018 52079101 and 42171415 and the fundamental research funds for the central universities wut 2021iva111 and 2021ivb016 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 129056 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
2432,this study investigates the climatological spatial scales csss of meteorological droughts in china and the linkages to climate variability the global precipitation climatology centre monthly gridded precipitation with a spatial resolution of 25 km 25 km for 1961 2010 is used the standardized precipitation index at different timescales 1 3 6 9 12 24 36 and 48 month is applied to characterize meteorological droughts the csss of meteorological droughts are calculated using a method considering spatial correlation and anisotropy the relationships between the csss and main monsoons and climate teleconnections in five selected regions are quantified using dynamic spatial panel models the five regions are south china sc the yangtze river valley yrv north china nc northeast china ne and the tibetan plateau tib the following results are obtained 1 the csss of multi timescale meteorological droughts in china are determined the means of the csss at various timescales in china are 68 9 73 8 71 8 69 7 68 8 65 9 62 9 and 60 8 104 km2 respectively the csss in eastern china generally show a significant decrease with increasing timescale 2 significant quantification relationships r2 greater than 0 85 suggest that there is not a simple linear relationship between climate anomalies and the csss but rather a complex spatiotemporal interaction including exogenous and endogenous interaction effects the csss not only strongly depend on their neighbors on the same timescale but also significantly depend on the csss of neighbors on adjacent timescales three monsoon indices indian monsoon east asian monsoon and western north pacific monsoon have significant impacts on the multi timescale css variations in the five regions especially for ne yrv sc and tib while the el niño southern oscillation and pacific decadal oscillation affect mainly sc and yrv these findings could broaden our understanding of the spatiotemporal relationships of droughts and be useful for drought risk management keywords standardized precipitation index multi timescale climatological spatial scales meteorological drought climate variability china data availability we have shared the link to the data in section availability statement 1 introduction drought is one of the most serious and extensive natural disasters it has had severe impacts on the environment society and the economy in many regions of the world the continually intensifying global warming raises concerns about more frequent and severe droughts in the future seneviratne ciais 2017 hoegh guldberg et al 2018 su et al 2018 therefore it is important for enriching the understanding of drought spatial and temporal scales are key elements in studying the complexity of various drought processes and phenomena multi scale interactions between the atmosphere ocean and land often suggest different physical meanings and present complex multi wave structures mishra singh 2011 zhang zhou 2015 kiem et al 2016 mardian 2022 the complex interactions lead to a variety of drought characteristics at different temporal and spatial scales our level of understanding of drought processes is highly dependent on their various temporal and spatial scales van loon 2015 zhao et al 2018 hao et al 2018 nowadays the multi timescale characteristics of drought have been studied widely drought indices at different timescales are often used to quantify drought variation for instance considering the significant seasonal characteristics of droughts many studies focus on a 3 month or seasonal timescale e g hao et al 2018 li et al 2015 liu et al 2019 xu et al 2015 zhai et al 2017 zhan et al 2020 a single scale cannot accurately describe drought propagation attributes because the interaction among different types of droughts in drought propagation involves multi scale processes wu et al 2021 to better understand drought and its impacts more multi scale analysis of drought has been undertaken liu et al 2017 veettil and mishra 2020 zhang et al 2018 zhou et al 2019 the temporal and spatial scales of drought are generally related to the typical scales of meteorological and hydrological phenomena van loon 2015 which indicates the significant scale dependence of drought similar to the multi timescale features of drought the drought also can occur on a wide range of spatial scales from the movement of soil water on a hillslope to global atmospheric circulation mardian 2022 we note that many studies pay more attention to the spatial distribution of drought in areas that often present a wide range of sizes zhou et al 2020 du et al 2020 emphasized the importance of quantifying the climatological spatial scales or spatial extent csss of extreme precipitation events hence it is reasonable to infer that drought should be characterized by multi spatial scales leelaruban and padmanabhan 2017 investigated the variation of spatiotemporal characteristics of droughts under multi spatial scales e g national regional state climatic division and county scales and verified the spatial scale dependence of drought characteristics fluixá sanmartín et al 2018 searched for the optimal drought index and timescale combination to detect different regional scale drought characteristics considering differences in drought monitoring at the site scale and regional scale spinoni et al 2019 quantified meteorological drought events for a country and a region using the 0 75 standard deviation sd as a drought threshold rather than 1 0 sd zhou et al 2020 investigated the spatial extent of meteorological droughts at multiple timescales 1 3 6 and 12 month over the poyang lake basin and indicated that spatial extent is potentially associated with timescale in addition many studies on the spatiotemporal dynamics of drought focus on different drought patch scales i e the contiguous spatial extent of drought ranging from thousands of square kilometers to hundreds of thousands of square kilometers e g lloyd hughes 2012 ren et al 2012 xu et al 2015 zhai et al 2017 guo et al 2018 liu et al 2019 zhou et al 2019 diaz et al 2020 moreover different spatial patches are used even in the same study area for example on a global scale sheffield et al 2009 herrera estrada et al 2017 zhan et al 2020 additionally some studies the issue on the spatial scales of extreme precipitation drought events touma et al 2018 identified regional and seasonal variations in the climatological length scales of extreme daily precipitation over the united states du et al 2020 quantified the spatial scales of heavy meiyu precipitation events zhou et al 2020 proposed a framework for measuring the spatial scales of drought considering spatial anisotropy these studies emphasize the role of spatial scale analysis in drought research and broaden our understanding of drought characteristics however it is unclear whether drought can occur across all spatial scales more importantly what the relationship is between the spatial scale and timescale and what the possible causes are which might be a major reason why drought characteristics are not easily revealed lloyd hughes 2012 hao et al 2018 zhou et al 2019 thus the object of this study is to investigate multi spatial scale characteristics of meteorological droughts and the possible linkage with climate variability the study is organized as follows section 2 gives the study area introduces the observations gridded precipitation datasets and five main climate teleconnections and examines the performance of the gridded precipitation section 3 mainly describes the methods for studying multi scale characteristics of meteorological drought these include the standardized precipitation index the spatial scale identification method and the dynamic spatial panel models the results of the multi spatial scale features of meteorological drought and its possible factors are provided in section 4 sections 5 and 6 contain a discussion and conclusion respectively 2 study area and data two precipitation datasets are used in this study one is the global precipitation climatology centre monthly land surface precipitation dataset with a spatial resolution of 0 25 0 25 i e 25 km 25 km for 1891 2016 hereafter gpcc it is available from the deutscher wetterdienst https www dwd de en ourservices gpcc gpcc html the other is the monthly observational precipitation from 839 meteorological stations in china for 1961 2010 50 years it is obtained from the national meteorological information centre of the china meteorological administration https data cma cn the observations are used to evaluate the performance of the gpcc the results show that most station grid pairs 93 68 have excellent consistency r 0 90 and the minimum correlation is greater than 0 5 p 0 05 fig 1 which indicates that the gpcc is highly accurate although the accuracy of the gpcc precipitation dataset in other areas especially outside china has not been tested it can be speculated that it still has good performance adnan et al 2016 schneider et al 2018 spinoni et al 2019 iqbal et al 2022 thus the gpcc monthly precipitation for 1961 2010 is used in this study we use the dataset to investigate the csss of meteorological droughts at different accumulation scales at any location in china to obtain the spatial scales of meteorological droughts along national boundaries the larger area within 4 n 70 n latitude and 60 e 150 e longitude is taken into account fig 1 there are more than 100 000 grid points with a spatial resolution of 0 25 0 25 within the selected region and about 15 000 grid points in china monsoons and teleconnections are two climatic phenomena that affect precipitation variability in china ding and wang 2016 in this study three important monsoon indices the indian monsoon index imi east asian monsoon index eami and western north pacific monsoon index wnpmi and two major climate teleconnections the el niño southern oscillation enso and pacific decadal oscillation pdo are selected to interpret the multi spatial scale variation of meteorological droughts the monthly eami can be calculated using the index proposed by zhao et al 2015 based on ncep reanalysis data https downloads psl noaa gov the daily imi and wnpmi are available from the monsoon monitoring page maintained by the university of hawaii http apdrc soest hawaii edu projects monsoon realtime monidx html the daily series are aggregated into monthly series the monthly niño3 4 index characterizing enso and pdo index are obtained from the national oceanic and atmospheric administration national centers for environmental information https www ncdc noaa gov climate monitoring wx patterns five regions of interest south china sc the yangtze river valley yrv north china nc northeast china ne and the tibetan plateau tib are selected because they have significant differences in climatic conditions zhang zhou 2015 chang et al 2019 fig 1 the relationships between the multi spatial scales of meteorological droughts and the selected five climate anomalies are analyzed in the above five regions 3 methodology 3 1 standardized precipitation index the standardized precipitation index spi proposed by mckee et al 1993 is applied to characterize meteorological drought because of its advantages of multiple timescales and comparability across different climate zones hayes et al 2011 the spi at various timescales 1 3 6 9 12 24 36 and 48 month is used meteorological drought levels and the corresponding probabilities are given table 1 the detailed procedures of the spi are introduced by zhou and liu 2016 3 2 a method for determining the climatological spatial scales of meteorological droughts the climatological spatial scales or spatial extent csss of meteorological droughts are obtained based on the multi directional climatological spatial length cl because of the spatial anisotropy of droughts the cl is defined as the average length scale of meteorological droughts in a given direction and time the semivariogram is often used to test whether there is spatial autocorrelation in the sample data we use the semivariogram to calculate the cl of meteorological droughts the semivariogram quantifies the squared difference between the values of two points as a function of the distance between those points by calculating the semivariogram for multiple pairs of points in a given direction and time we can find the distance at which points with drought values are no longer substantially correlated in that direction and time we use this distance to quantify the length scale of drought for that given direction and time a method considering spatial anisotropy proposed by zhou et al 2020 to measure the spatial scales of meteorological droughts is used in the study using an example of a grid point 114 125 e 29 875 n a brief description is given fig 2 a dry wet map over the selected region can be obtained using the spi at a given timescale e g a t month timescale then a drought threshold spi t 1 is used to judge whether a grid point is under drought or not after drought identification every grid point in the region will be binarized that is a grid point under drought has the value of 1 and a non drought grid point has the value of 0 fig 2a and e the number of drought months at a given point can be counted e g fig 2a a scanning window for a grid point with drought is constructed fig 2e certain parameters are applied to shape the scanning window including search direction θ unit search radius sr unit km distance step h unit km bandwidth bandw unit km and angle tolerance α unit generally when the parameters sr and h are specified there is a total of g g sr h polygons based on the scanning window the number of 1 1 and 1 0 pairs within each polygon gray polygon can be counted fig 2b and c for a selected grid point with drought the relationship between semivariance values and distances is analyzed the mean semivariance value γ k φ of the k th polygon k g can be calculated using expressions 1 and 2 fig 2c and d 1 γ a b 1 2 z x a z x b 2 2 γ k φ γ 1 1 φ y c 1 1 θ k φ γ 1 0 φ y c 1 0 θ k φ φ y c 1 1 θ k φ φ y c 1 0 θ k φ where γ a b represents the semivariance of the a b point pair z xa and z xb represent the variable value 0 or 1 of location xa and location xb after binarization respectively xa xb represents the distance from point b to point a according to the theoretical semivariogram the semivariance value of a 1 1 point pair is γ 1 1 0 and γ 1 0 0 5 for a 1 0 point pair φ y c 1 1 θ k φ and φ y c 1 0 θ k φ represent the number of 1 1 point pairs and 1 0 point pairs within the k th polygon along the direction of θ during the selected periods i e the set φ respectively the set y represents the whole study period from 1961 to 2010 the climatological spatial length clθ along a search direction θ can be calculated using an exponential model fig 2d aalto et al 2016 berndt et al 2014 touma et al 2018 the range r0 is considered the cl namely clθ r0 because there is little spatial correlation when the distance is larger than the range r0 further the csss can be obtained based on the multi directional cl more details can be found in zhou et al 2020 we specify some parameters i e h 25 km bandw 56 95 km α 22 5 and m 8 of the method which is related to the used grid precipitation dataset i e gpcc given the regular grids 25 km 25 km of the dataset used the distance step is set as h 25 km to reduce sampling error as much as possible we consider 8 search directions counterclockwise at 45 intervals i e m 8 the angle tolerance and bandwidth are set at α 22 5 and bandw 56 95 km fig 2a and 2e which is designed to eliminate the interference between point pairs within two adjacent polygons reduce the effects caused by increasing polygon size and ensure sufficient point pairs the different bandwidths bandw from 15 53 km to 201 93 km are investigated in our previous study zhou et al 2020 the search radius sr 500 km is specified moreover a dynamic search radius is also adopted with the maximum constraint of 1000 km however we investigate the sensitivity of the search radius sr in section 5 because the previous tests were conducted in a relatively small area 3 3 dynamic spatial panel models drought and other extreme phenomena are usually characterized by spatial interaction effects spatially the drought condition at a location is generally affected by the surrounding drought conditions and vice versa meanwhile the possible dependence between the spatial scale and timescale of meteorological droughts is considered therefore we select a dynamic spatial panel model to investigate the impacts of climate anomalies on multi scale characteristics of meteorological droughts a dynamic general spatial nesting model is a linear regression model extended to include spatial lag in the dependent variable the temporal lag of the dependent variable the explanatory variables the error term or some combination thereof lee and yu 2010 its expression is as follows 3 y t τ y t 1 ρ w y t η w y t 1 x t β w x t θ μ ξ t u t a n d u t λ w u t ε t where y t represents a vector n 1 of the log of the csss of meteorological droughts at timescale t n is the number of grids in each region as shown in fig 1 x t is a matrix n k of exogenous explanatory variables associated with the coefficient vector β k 1 w is a spatial weight matrix n n describing the neighbors of a grid wy t represents the endogenous spatial lag wx t is the exogenous spatial lag and wu t is the spatial lag among the error terms y t 1 represents the one period temporal lag of the dependent variable and wy t 1 is the temporal and spatial simultaneous lag of the dependent variables in this study the one period lag is considered the one timescale lag because the drought is characterized by multiple timescales it is helpful to analyze the relationship between the spatial scale and timescale of meteorological droughts furthermore the scalers τ ρ η and λ and the vector θ k 1 of the parameters are used to depict the strength of these spatial lags furthermore μ is a vector n 1 of spatial fixed effects ξt is a time fixed effect and ε t is a vector n 1 of the error terms if the scalar λ is equal to 0 the nesting model is regarded as a dynamic spatial dubin model sdm if the scalar λ is equal to 0 and the vector θ is the vector of 0 the model is simplified to a dynamic spatial lag autoregressive model sar these two simplified dynamic spatial models will be used to investigate the relationships between the climate anomalies and multi scale characteristics of meteorological droughts in this study the simple procedures are as follows 1 the spatial panel data which include the spatial grids in one region the csss of meteorological droughts at multiple timescales and the exogenous explanatory variables are collected t is the timescale t 1 3 6 9 12 24 36 and 48 month the exogenous explanatory variables are imi eami wnpmi enso and pdo respectively thus k 5 the correlation coefficient between the drought series of a grid and a climate anomaly is regarded as the intensity of the impact of the climate anomaly on the grid 2 a spatial weight matrix is constructed a simple rule that grids that share a boundary will be neighbors is used in this study the value of 0 in the matrix means that a grid does not have neighbors 3 whether spatial fixed effects time period fixed effects or two way fixed effects in the spatial panel models need to be included can be tested using likelihood ratio lr tests if the null hypothesis that the spatial fixed effects time period fixed effects are jointly insignificant can be rejected p 0 05 this means that the spatial fixed effects time period fixed effects should be included in the spatial models if the two above hypotheses can be rejected this suggests that two way fixed effects should be considered 4 the dynamic sdm is first applied to investigate the relationship between the climate anomalies and the csss of meteorological droughts then the wald test and lr test are used to judge whether the dynamic sdm can be simplified to dynamic sar the null hypothesis is that the dynamic sdm can be simplified to dynamic sar further details can be found in elhorst 2014 4 results 4 1 climatological spatial lengths of meteorological droughts in 8 directions for meteorological droughts at different timescales the frequency distributions of cls in china for each search direction are given in the left panel of fig 3 the cls of most grid points 60 for all timescales are 500 km and the proportion generally becomes larger as the timescales increase fig 3 for example in terms of 0 it is 66 71 for spi 1 59 80 for spi 3 62 30 for spi 6 63 47 for spi 9 64 72 for spi 12 63 56 for spi 24 65 89 for spi 36 and 66 67 for spi 48 the results indicate that the search radius of 500 km should be reasonable section 5 in addition the mean cl sd in the same direction shows a decreasing increasing change with increasing timescale fig 3 generally the mean cl at 0 decreases from about 500 km for the 3 month scale to about 440 km for the 48 month scale while the sd increases from 196 km to 230 km there are relatively large differences in the mean cl in different directions for the same timescale for example about 530 km at 45 is significantly greater than 450 km at 270 for the 3 month scale the cl of meteorological droughts presents a significant directional dependence the sds of cls in the 8 directions for each timescale are shown in the right panel of fig 3 for spi 1 the sds of the 8 direction cls of 30 33 of the total grid points are larger than 200 km and up to 86 00 for those larger than 100 km this phenomenon is also observed at other timescales 28 09 200 km for spi 3 32 94 for spi 6 37 92 for spi 9 42 25 for spi 12 48 28 for spi 24 50 49 for spi 36 and 55 58 for spi 48 moreover the proportion of sds larger than 100 km increases from 85 81 to 96 03 along with increasing timescale these findings suggest that the spatial anisotropy of meteorological droughts is confirmed and its degree increases along with timescale 4 2 statistics of the climatological spatial scales of meteorological droughts the frequency distributions of the csss of multi scale meteorological droughts in china are characterized by positive skewing fig 4 a the mean csss are larger than the median for all timescales fig 4b it should be noted that almost all statistics of csss show decreasing trends as timescale increases for example the medians of the csss of meteorological droughts at the 1 month to 48 month timescales are 64 6 70 0 68 6 65 8 65 0 62 4 57 8 and 55 8 104 km2 respectively this phenomenon is in line with the change trends of cls in the 8 directions fig 3 the csss of multi scale meteorological droughts in china show a large scope ranging from 20 104 km2 to 130 104 km2 from the 5th to 95th percentile while the csss of about 50 of grid points from the 25th to 75th percentile are mainly between 40 104 km2 and 90 104 km2 additionally the high csss the 75th percentile become smaller with increasing timescales which indicates that the spatial aggregation patterns of high csss might weaken and change although positive skewed distributions are also shown at the regional scale there are significant differences in the statistics of the csss fig 4 and table 2 the medians or means of the frequency distributions of the csss in different regions generally move toward smaller values with increasing timescales except for tib table 2 the medians of the csss at 1 month to 48 month scales in ne decrease from 76 10 104 km2 to 45 80 104 km2 while that in tib shows a trend of first increasing and then decreasing a similar phenomenon is present in the 5th percentile of the csss moreover the 5th percentile in tib ranges from 15 52 104 km2 to 19 05 104 km2 which is significantly less than that in the other four regions e g from 25 36 104 km2 to 63 03 104 km2 in yrv the 95th percentile of the csss in tib from 133 26 104 km2 to 153 09 104 km2 is in contrast higher than that in the other four regions e g from 95 48 104 km2 to 125 81 104 km2 in nc it should be noted that there are significant shifts in the median of the csss in yrv and sc fig 4e and 4f they appear at the 6 month timescale in sc 63 98 104 km2 and at the 24 month timescale in yrv 78 48 104 km2 table 2 these phenomena suggest that the csss of meteorological droughts depend on the timescale and they vary in different climate zones 4 3 spatial patterns of the climatological spatial scales of meteorological droughts fig 5 shows the spatial distributions of the csss for multi timescale meteorological droughts including short term scales i e 1 month and 3 month mid term scales i e 6 month 9 month and 12 month and long term scales i e 24 month 36 month and 48 month for meteorological droughts at short term scales fig 5b and 5c spatially there are three regions with high csss they are in western mainly on the tibetan plateau north northeastern and southeastern china which is consistent with the statistics in table 2 and fig 4 topographically these high csss are distributed mainly at altitudes above 4 000 m and below 500 m fig 5a moreover the csss of meteorological droughts at short term scales have significant p 0 01 increasing changes with longitude 0 73 104 km2 deg for the 1 month timescale 0 36 104 km2 deg for the 3 month timescale although there are insignificant increasing changes with latitude there are several places with consistently high low csss for example high values around 25 n and 33 n and low values around 28 n in comparison to the short term scales the distribution of csss for the mid term scales has greater spatial fragmentation especially in southeastern china fig 5d e and f the csss in sc yrv and nc decrease with increasing timescale while the high values are still distributed mainly on the tibetan plateau in north northeastern china and in a small area in southeastern china therefore the meridional and zonal trends of the csss in china indicate significantly increased trends p 0 05 the slope 0 98 1 30 104 km2 deg in latitude is significantly larger than that in longitude 0 19 0 27 104 km2 deg the csss of meteorological droughts at the long term scales show spatial patterns with the greatest spatial fragmentation compared to the short and mid term scales fig 5g 5 h and 5i compared with those in the other four regions the csss at long term scales in ne decrease significantly table 2 the csss for long term scales show decreasing trends 0 24 0 12 104 km2 deg with longitude which suggests that the spatial aggregation characteristics weaken with increasing timescales particularly in eastern china although the low csss show a more widespread spatial distribution for the long term scales there is a new region with high values the north central part of china meanwhile the correlations between climate anomalies and multi timescale meteorological droughts are analyzed figures s1 s5 only those that pass the significance test p 0 05 are shown the results indicate that the impacts of the three monsoon indices and two major climate anomalies on meteorological drought have significantly regional characteristics and the regional impacts depend on the timescale nc yrv and sc are affected relatively singly by climate anomalies in comparison to ne and tib for instance in terms of yrv the imi shows a significant negative correlation with spi 1 and a significant positive correlation with mid term and long term meteorological droughts the wnpmi has a negative correlation with short term and mid term meteorological drought the eami presents a positive correlation with meteorological drought while intensity decreases with increasing timescales the niño3 4 is the opposite of the imi the pdo is significantly correlated only with long term meteorological droughts and the positive negative spatial pattern is complex for tib the imi shows a relatively consistent significant positive correlation for almost all timescales the impacts of the eami on droughts in the region are compound and the intensity and scope generally decrease along with timescale however the pdo is negatively correlated with the long term droughts in this region 4 4 effect of climate variability on the climatological spatial scales of meteorological droughts we perform lr tests for each region to judge whether spatial fixed effects time period fixed effects or two way fixed effects need to be included table s1 the results suggest that two way fixed effects should be included in the spatial models subsequently the wald test and lr test are carried out to investigate whether the dynamic sdm can be simplified to the dynamic sar table s2 according to the results of the tests the likely dynamic spatial models are specified dynamic sars for ne nc and yrv and dynamic sdms for sc and tib furthermore in table 3 we give the estimated results of the dynamic spatial panel models for the five regions for yrv significant endogenous interaction effects of the csss are revealed using the dynamic sar the csss of meteorological droughts depend strongly on their values in the previous timescale the coefficient τ is 0 1102 with high significance p 0 01 the coefficient η of the csss in the neighboring grids in the previous timescale i e simultaneous timescale and spatial lags is 0 0134 with high significance p 0 01 moreover the css of a grid is significantly affected by that of its neighbors on the same timescale ρ 0 8506 p 0 01 except for the imi its coefficient is positive but insignificant the other four climate anomalies have significant contributions to the csss of meteorological droughts the wnpmi eami and niño3 4 show significant negative effects on the csss with coefficients of 0 1686 p 0 01 0 3036 p 0 01 and 0 0373 p 0 10 while the pdo has a significant positive effect 0 0603 p 0 05 for tib significant endogenous interaction effects of the csss and exogenous interaction effects of the independent variables i e imi and eami are revealed using the dynamic sdm table 3 the coefficients τ η and ρ are positive with high significance p 0 01 which suggests the important roles of the previous timescale and neighboring grids in the csss due to the complex spatial patterns of the effects of climate anomalies figure s1 s5 only the imi has a significant impact on the csss in tib 0 6548 p 0 01 it should be noted that there are two significant spatial lag terms of independent variables namely w imi and w eami the coefficient of w imi is 0 8491 p 0 01 this phenomenon suggests that the positive impact of the imi on the csss in neighboring grids could produce a significant negative impact on that in the current grid the coefficient of w eami is 0 5769 p 0 10 which suggests that the positive impact of the eami on the csss in neighboring grids could play an active role in that in the current grid similarly the significant roles of the spatial and timescale lag terms in the csss are observed in the other three regions the imi 0 0841 p 0 10 eami 0 1136 p 0 05 and pdo 0 0365 p 0 05 play important roles in the csss in ne while only the wnpmi 0 2533 p 0 05 does so in nc all five climate anomalies and their spatial lag terms have significant effects on the csss in sc while the impacts of the climate anomalies and their spatial lag terms are opposite for instance the eami 0 9175 p 0 05 and niño3 4 1 4215 p 0 01 show a positive role in the csss in sc while w eami 1 3454 p 0 01 and w nino 3 4 1 0502 p 0 01 are negative these results suggest that there is not a simple linear correspondence between climate anomalies and the csss of meteorological droughts but rather a complex temporal and spatial interaction 5 discussion 5 1 uncertainty of measuring climatological spatial scales of meteorological droughts a css identification method is used in this study the parameter settings of the method and the absence of data might affect the csss 5 1 1 sensitivity testing of the scanning window search radius since the study is conducted in different climate zones there is a need to discuss the sensitivity of the scanning window search radius taking spi 3 as an example the statistics of multi directional cls under different radii are shown in fig 6 when the search radius is set at 500 km the cls of approximately 61 4 of the total grid points are smaller than that search radius fig 6a while this is true for only approximately 29 1 at 300 km fig 6b and up to 83 4 at 700 km fig 6c under the search radius of 700 km the spatial anisotropy e g sd 200 km of more grid points becomes weaker and the probability of high csss becomes larger fig 6d and 6e this phenomenon suggests that the increment e g δ 200 km of the radius has a significant amplifying effect on the cl particularly for a search radius larger than 500 km considering the lower variability and the need for most grid points the search radius of 500 km is specified for each scanning window the drought threshold spi 1 is used in the study but we find that the different drought thresholds can influence the spatial scales of droughts figure s6 and s7 in the future we would study the impact of the threshold on the spatial scale of drought 5 1 2 impact of the default processing on climatological spatial lengths due to the failed statistical distribution fitting i e absence of spi values and the failed semivariation function fitting i e absence of precipitation data over the ocean there are some null values cls in some of the 8 directions we attempt to fill in the null values by calculating the average cl using a 3 3 window in its corresponding direction similarly taking spi 3 as an example on average the number of null values accounts for 8 67 of total grid points 15127 fig 4b which are distributed mostly along coastal areas of those the largest ratio at 180 is 12 27 while the smallest ratio at 90 is 3 65 a total of 9154 grid points has effective cls in all 8 directions the functional relationship y a xb between the mean cls i e x and csss i e y of all 9154 grid points is regarded as the reference fig 7 a the remaining 5973 grid points with null values are filled using the window averaging method two processing schemes are calculated and compared one is to use the fitted functional relationship i e y a xb fitted css and the other is to fill by calculating the area of the polygon consisting of the 8 directions and their cls i e filled css the results are given in fig 7 the mean cls and csss show an excellent power function relationship y 3 06x 1 96 n 9154 r2 0 9892 p 0 05 fig 7a the two processing schemes have good consistency slope 0 99 n 5973 r2 0 9842 p 0 05 fig 7b moreover the relative errors are distributed mainly between 10 and 10 fig 7c the mean error is close to 0 0 06 the spi at other timescales shows similar findings not shown these results suggest that the window averaging scheme could be used for addressing the grid points with null values 5 2 the climatological spatial scales of meteorological droughts and the linkage with climate anomalies 5 2 1 rationality of the climatological spatial scales of meteorological droughts the csss of multi timescale meteorological droughts in china are quantified for instance we find that meteorological droughts at the 6 month scale have spatial scales with a mean radius and extent of about 480 km sd 200 km and 71 8 104 km2 drought events of similar spatial extent are frequently observed e g lloyd hughes 2012 zhai et al 2017 spinoni et al 2019 from the perspective of the spatiotemporal continuity of drought events the area affected by a drought event can reach up to 106 km2 xu et al 2015 diaz et al 2020 schumacher et al 2022 we note studies on the spatial scales of extreme precipitation events touma et al 2018 reported that the cls of daily extreme precipitation events ranged from 200 to 400 km over the eastern us du et al 2020 found that heavy meiyu precipitation events had regular spatial scales with an average length width and extent of about 1 400 km 500 km and 40 104 km2 respectively these studies are based on spatial isotropy and spatial morphology the results reported above confirm the importance of considering spatial anisotropy and also provide a comparative reference for the spatial scales of meteorological droughts meanwhile we find that the csss of meteorological droughts in china present a decreasing trend with increasing timescales based on the statistics of spatial areas of drought events rouault and richard 2005 found that the average drought area spi 1 on the 6 month scale was a bit larger than that on the 24 month scale which was consistent with the results shown in fig 4a and b zhai et al 2017 indicated that the main drought affected areas showed a significant decreasing trend along with duration these studies provide some indirect support for the reasonability of our results 5 2 2 linkage with climate anomalies the statistics and spatial distributions of the csss in the five regions ne nc yrv sc and tib vary with timescale which might be controlled by multiple large scale climate variability shelton 2009 sun et al 2020 the three monsoon indices imi eami and wnpmi have significant impacts on the multi timescale csss in the five regions table 3 and figure s1 s5 especially ne yrv and sc belonging to the east asian monsoon region which agrees with the widely reported influence of these indices on precipitation variation over eastern china sun and wang 2015 zhang zhou 2015 ding wang 2016 however only the imi and its spatial lag term w imi among these three indices have a significant impact on tib p 0 01 which is consistent with the results reported by chang et al 2019 who found that the imi mainly affects precipitation variability in tib on the intra annual 0 5 1 year and inter annual 2 10 year timescales the wnmp and eami are developed mainly for describing precipitation and air temperature variations over east asia while tib is usually not directly affected by east asian monsoon systems due to its high elevation above 4 000 m fig 1 however it should be noted that the spatial lag term of the eami w eami indicates a significant effect on the csss of droughts in tib p 0 10 this phenomenon might be related to the 200 hpa upper atmosphere zonal wind considered in the eami while the wnmp considers just the 850 hpa lower atmosphere zonal wind wang et al 2001 zhao et al 2015 the positive impact 0 5769 p 0 10 of the eami on the csss of the neighbors in tib could play an active role in tib through the transmission of monsoon circulation anomalies in the mid upper troposphere 500 200 hpa zhao et al 2015 sun wang 2018 he et al 2019 enso has a significant negative impact on the spatiotemporal variation of the csss in yrv and a positive impact in sc the pdo is the opposite additionally it also negatively contributes to ne table 3 this phenomenon might be related to the phase relationship between these two teleconnections ma shao 2006 shelton 2009 zhang et al 2017 enso affects regional precipitation by modulating the intensity and extent of the western pacific subtropical high and the impacts of the pdo are attributed mainly to alterations in the intensity of the east asian monsoon and westerly circulation zhou et al 2008 li et al 2014 sun et al 2020 zhang et al 2022 the results of the spatial models confirm the spatial interaction effects and timescale dependence of the csss of meteorological droughts however a limited number of timescales from 1 month to 48 month scales are used in this study which may not well reveal the impacts of multi scale weather and climate systems these climate anomalies may have significant effects on precipitation anomalies just at their prominent timescales further investigation should be carried out from the perspective of the dominant timescales 6 conclusions this study investigates the csss of multi timescale meteorological droughts in china and their quantification relationship with three important monsoon indices imi eami and wnmp and two major teleconnections enso and pdo in five regions ne nc yrv sc and tib the main conclusions are as follows 1 the csss of multi timescale meteorological droughts i e 1 3 6 9 12 24 36 and 48 month timescales are determined the frequencies of the csss of the various timescale meteorological droughts in china present positive skewed distributions the means of the csss of meteorological droughts at various timescales in china are 68 9 73 8 71 8 69 7 68 8 65 9 62 9 and 60 8 104 km2 respectively which are generally larger than their median values 64 6 70 0 68 6 65 8 65 0 62 4 57 8 and 55 8 104 km2 additionally the large differences in the cls in eight directions confirm the spatial anisotropy of meteorological droughts the sds of the eight direction cls increase along with timescale the proportion generally increases from about 28 to 56 for sds larger than 200 km and from about 85 to 96 for sds larger than 100 km 2 except for tib the csss of meteorological droughts in the other four regions of interest ne ec yrv and sc generally show a significant decrease with increasing timescales statistical distribution characteristics similar to those at the national scale are observed in these five regions meanwhile it is found that the csss in tib have a larger fluctuation range from about 18 104 km2 to 145 104 km2 from the 5th to 95th percentile than that in the other regions for instance 40 104 km2 to 128 104 km2 for yrv these results are consistent with the distinct changes in the spatial patterns of the csss of meteorological droughts at short mid and long term scales although the high csss show significant regional characteristics the spatial fragmentation of high csss increases along with timescale 3 the quantification relationships between the csss of meteorological droughts and five important climate anomalies are revealed in the five regions using dynamic spatial panel models the spatial interaction effects and timescale dependence of the csss of meteorological droughts are confirmed there is not a simple linear correspondence between climate anomalies and the csss of meteorological droughts but rather a complex temporal and spatial interaction for all regions the csss of meteorological droughts not only strongly depend on their neighbors on the same timescale but also significantly depend on the csss of the neighbors on the previous timescale moreover the results indicate that ne is affected mainly by the imi eami and pdo except for the imi the other four climate anomalies have significant impacts on yrv while only the wnpmi negatively affects nc tib is significantly affected by the indian monsoon imi and w imi and the teleconnection mechanism of the east asian monsoon w eami all five climate anomalies contribute to the css variation of meteorological droughts in sc the ccss of multi timescale meteorological droughts and their relationship with climate variability could help understand the spatiotemporal dynamics of droughts and provide implications for improving the management of drought risk 7 data availability statement the gpcc is available from https www dwd de en ourservices gpcc gpcc html the observational precipitation can be obtained from https data cma cn ncep reanalysis data is available from https downloads psl noaa gov the imi and wnpmi are available from https apdrc soest hawaii edu projects monsoon realtime monidx html the niño3 4 index and pdo index are obtained from https www ncdc noaa gov climate monitoring wx patterns credit authorship contribution statement han zhou conceptualization funding acquisition data curation investigation formal analysis methodology visualization writing original draft writing review editing wen zhou data curation investigation formal analysis methodology visualization writing original draft supervision writing review editing yuanbo liu conceptualization supervision project administration formal analysis writing review editing jiejun huang formal analysis validation writing review editing yanbin yuan funding acquisition formal analysis writing review editing yongwei liu formal analysis writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the national natural science foundation of china nos 42001018 52079101 and 42171415 and the fundamental research funds for the central universities wut 2021iva111 and 2021ivb016 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 129056 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
2433,to reduce the adverse influence of sudden water pollution accidents it is essential to estimate the unknown contaminant source information normally including the source location initial release time and total release mass as soon as possible the ensemble kalman filter enkf has been proven to be an effective algorithm for such an inverse problem this paper proposes a new method based on enkf to identify contaminant source information the method we called rc enkf uses the relation coefficient of concentration instead of timely concentration as a state variable in the assimilation process the advantage lies in the release source mass that can be decoupled from the parameter group of unknown contaminant information to improve the assimilation speed and reduce interference with the accuracy of assimilation two categories of cases are employed for validating the applicability and testing the performance compared with the traditional enkf method in detail sensitivity analyses are carried out with different observation errors number of observation sites number of ensemble realizations and model grid size the results demonstrate that with the uncertain error of observation data the rc enkf works nicely and shows superiority to the traditional enkf method reflected in the strong immunity to interference from observation data errors and elevated efficiency with the requirement of fewer observation sites ensemble realizations and model grids it illustrates that the rc enkf is a more efficient and robust method for estimating unknown contaminant source information keywords contaminant source identification ensemble kalman filter relation coefficient assimilation parameter decoupled data availability data will be made available on request 1 introduction water pollution problem has posed a great threat to social development and human health cheng and jia 2010 amiri et al 2019 in recent years global water pollution incidents caused by industrial waste or transportation accidents with risky chemical materials occur frequently quickly and precisely obtaining the information on the contaminant sources such as contaminant source location initial release time and initial release concentration is crucial to accident management control risk assessment and remediation mirghani et al 2009 wang et al 2019 xu et al 2021 in the past three decades the problem of identifying the contaminant source has gained much attention from researchers around the world atmadja and bagtzoglou 2001 michalak and kitanidis 2003 2004 sun et al 2006 xu and gómez hernández 2018 in general whether in surface water or groundwater the method of identifying contaminant sources can be classified into two main categories deterministic optimization approaches and probabilistic approaches chen et al 2018 wang et al 2019 moghaddam et al 2021 the former approaches get parameters of the inverse problem optimized based on minimizing a given objective function such as canonical transformation which transforms the inverse problem into a well posed question to solve skaggs and kabala 1994 direct analysis methods neupauer et al 2000 li et al 2006 least squares regression analysis methods gorelick et al 1983 alapati and kabala 2000 bagtzoglou and atmadja 2003 sun et al 2006 or other intelligent optimization algorithms singh and datta 2007 yeh et al 2007 jha and datta 2013 gzyl et al 2014 zhang and xin 2017 the latter approaches convert the awaiting inversion parameters into random variables by introducing a random process to the inverse problem and constructing a stochastic framework to get variables estimated cupola et al 2015 an et al 2021 these two categories of methods have been constantly improved and applied by researchers since the problem of identifying a contaminant source was raised in the first category datta et al 2011 proposed a source identification methodology that used a classical nonlinear optimization model linked to a flow and transport simulation model ayvaz 2016 developed an integrated simulation optimization approach to solve the areal groundwater pollution source identification problems xia et al 2019 developed a simulation optimization approach by combining a transport simulation model and a genetic optimization algorithm to determine the pollutant source fluxes han et al 2020 proposed a method combining the advection dispersion equation ade of contaminants in groundwater with a genetic algorithm ga to identify groundwater pollution sources lei et al 2022 established a new model which combined space time radial basis collocation method rbcm and differential evolution algorithm dea to identify the source release history the applications of the above methods could not only solve the problem by minimizing the difference between the simulated value and the true value but also provide a reliable reference for the control and efficient remediation of site pollution in the second category yan et al 2019 proposed an innovative framework for groundwater contaminant sources identification which was based on bayesian theory and integrated the relative entropy a 0 1 integer programming optimization model markov chain monte carlo and a kriging surrogate model in addition xing et al 2019 proposed an ensemble surrogate model to improve the accuracy and robustness of results to identify groundwater contaminant sources by using an adaptive metropolis markov chain monte carlo method to assign weights to the three models that separately are kriging radial basis functions and least squares support vector machines ghane et al 2016 solved pollution source identification problems in a river network based on the backward probability method the approach was accurate and computationally efficient and did not need any simplification in river geometry and flow kanao and sato 2022 proposed the adjoint marginal sensitivity method a time backward probabilistic method to estimate a leakage position and its flux by assimilating a limited number of observed concentration data the ensemble kalman filter enkf proposed by evense 1994 an efficient probabilistic approach gets much attention and wide applications in various fields in recent decades such as oceanography meteorology petroleum engineering and hydrology houtekamer and mitchell 2001 li et al 2012 xu et al 2013 xia et al 2021 the advantage of the enkf is that the covariances and cross covariances of parameters and state variables of systems are calculated based on an ensemble of realizations making it less computational cost than other bayesian methods in recent years it has been introduced for identifying parameters of contaminant sources or aquifer parameters due to its excellent performance high efficiency and easy integration in dealing with inverse problems franssen and kinzelbach 2009 zhou et al 2011 xu et al 2016b first explored the application of the enkf for the simultaneous identification of contaminant source parameters i e the source location the release time and the release concentration and later xu et al 2018 developed a restart normal score ensemble kalman filter for the simultaneous identification of a contaminant source and the spatially variable hydraulic conductivity in a synthetic aquifer notice that different from the standard enkf in the forecast step of the restart normal score enkf state variables were forecasted from the step of time zero instead of the last updating time step which is necessary to identify the contaminant source the method has not only been successfully proven in synthetic cases but also in sandbox experiments chen et al 2018 2021 in addition to the application in the identification of groundwater contaminant source information it has also been employed in the identification of surface water contaminant source information wang et al 2019 proposed an effective method for point pollution source identification in rivers with a performance improved ensemble kalman filter where the ensemble kalman filter was coupled with the backward location probability to improve its performance in the identification of the river pollution source the main purpose of this paper is to propose an improved ensemble kalman filter based on relation coefficient assimilation rc enkf for the identification of surface water contaminant sources the instantaneous point source parameters of interest include the release position the initial release time and the initial release concentration or total mass in the previous research on the identification of contaminant source parameters the contaminant concentrations at observation points in time series were assimilated it was easy to be understood and implemented and the assimilation result was acceptable if the error distributions of measuring equipment were known in advance however in reality the error distributions of measuring equipment are unknown or uncertain due to many reasons such as equipment aging observation sites human factors and so on to deal with this deficiency we proposed a new ensemble kalman filter method based on relation coefficient assimilation which can avoid the problem mentioned above at the same time promote the efficiency and accuracy of assimilation in this method the relation coefficient between concentrations predicted and observed at every time step is introduced in the enkf instead of the absolute value of concentrations benefited by this the release location and initial release time which are relatively more important can be first obtained not affected by the total amount of pollution then the release mass is calculated independently based on the predicted release location and initial release time obtained from the previous step the paper proceeds firstly with the algorithm description of the improved enkf we proposed then five numerical cases are designed for testing the applicability of the rc enkf on the problem of identifying contaminant source and making a comprehensive comparison between rc enkf and traditional enkf the paper ends with a summary and conclusions of the results 2 methodology 2 1 surface water solute transport equation aiming at the water pollution problem in wide and shallow rivers the pollutant transport process can be depicted by the following two dimensional advection diffusion equation kong et al 2013 wang et al 2018 1 h c t h u c x h v c y x h d xx c x h d xy c y y h d yx c x h d yy c y s where c kg m3 is the contaminant concentration u v m s are the flow velocity component in the y direction respectively h m is the water depth d m2 s is the 2 d diffusion coefficient tensor in the x and y spatial coordinates the value of which is relating to u v s is the source term due to the complexity of the equation the mathematical model is widely used in this paper the total variation diminishing tvd based anisotropic model is adopted kong et al 2013 2 2 the ensemble kalman filter similar to the standard enkf two types of variables involved in the rc enkf are the system parameters and the state variables respectively the state variables in this paper are not the contaminant concentrations as in previous researches xu et al 2016b chen et al 2018 wang et al 2019 which are instead by the relation coefficients between observed concentrations and simulated concentrations the system parameters include the source location x or x and y the initial release time t and the initial release mass m respectively which we need to estimate here we use φ to represent the vector of the system parameters the implementation of the rc enkf for identifying the three parameters of surface water contaminant sources mentioned above includes the following steps 1 generate the initial ensemble in this step an ensemble of nr realizations of the three source parameters mentioned above is generated the initial values of these parameters are selected stochastically from independent uniform distributions the upper values and lower values of uniform distributions can be determined by the prior information and the true values of parameters do not have to be contained in ranges of distributions so nr vectors of φ are constructed 2 φ i x i y i t i m i t where i is the index of realization and the superscript t represents the transposition of the matrix 2 forecast in this step the last parameters updated at observation time t 1 are used as the new inputs to rerun the transport model from time 0 to observation time t then the state of observation time t is simulated and the state of time 0 t 1 is re simulated usually the state variable simulated refers to contaminant concentration the forecast of concentrations is as follows 3 c t f i ψ c 0 φ t 1 a i where c 0 means the state of initial contaminant distribution we consider it as 0 in the whole domain before the contaminant is released the superscript a and the subscript f are the forecasted and updated values respectively ψ represents the state transfer operator which in this paper is the mass transportation model in the rc enkf we take the pollution concentration as the intermediate state and introduce the relation coefficient r the function of observed concentrations and simulated concentrations as the final evaluation state variable the relation coefficient r is expressed by the following formula sidauruk 1998 jing 2018 4 r j 1 n c j s c s c j ob c ob j 1 n c j s c s 2 j 1 n c j ob c ob 2 where the subscript j is the index of the time series the subscript s and ob are the value of simulated and observed respectively the c s and c ob are the mean value of c s and c ob respectively so the forecast of the evaluation state in this paper can be expressed as 5 r t f i f ψ c 0 φ t 1 a i where f is the transformation operator which transforms c i f to r i f because of the characteristic of contaminant transport when the source location and release time are determined and other conditions remain unchanged the value of concentration at one site is proportional to the release mass in other words once the source location and release time are determined no matter what the release mass m is the relation coefficient r will keep the same value so the r introduced decouples the release mass m from the source parameters set to be identified after this step the vector φ just contains the source location and the source release time shown as follows 6 φ x y t t 3 update before this step we need to apply the normal score transform for parameters since the parameters are selected in a uniform distribution and the enkf works best if the parameters are in a gaussian distribution the normal score transform function needs to be recomputed after each forecast step the system parameters are transformed by the following formula 7 φ t x t y t t t t φ t x t φ t y t φ t t t t where φt represents the normal score transformation at time t which is related to the cumulative distribution functions here the evaluation state r remains untransformed because the previous researches verified that the method was capable enough to identify non gaussian distribution parameters when the state variables remained untransformed xu et al 2016a then parameters are updated here we assume the number of observation sites is nob every realization of the ensemble is updated as the following equation 8 φ t a i φ t f i g t r t r t f i e t where φ t a and φ t f are the updated and the forecasted vectors of normal scored source parameter at time t respectively r t is the auto relation coefficient of observation contaminant concentration during time 0 to time t which is equal to 1 e t is the error vector of r t in the traditional enkf e t is the observation error vector which is drawn from a normal distribution with mean zero and covariance matrix p t xu 2016b but here due to the state variable transformed from concentration to the relation coefficient considering observation errors the value of e t between the observed concentrations and the true concentrations is more difficult to estimate compared with that in the traditional enkf so the value of e t is ignored and set to 0 in this paper gt is the kalman gain matrix which is expressed by 9 g t d φ r f d r f r f r t 1 where r t is the covariance matrix of the error vector e t which is ignored due to the value 0 of e t mentioned above d φ r f is the cross variance between source parameters vector φ and the relation coefficient r f forecasted at observation sites d r f r f is the autocovariance of the relation coefficient r f after that the updated transformed source parameters φ t need to be transformed back to φ t shown in step 2 by using the inverse transformation function φ t 1 which is related to the inverse cumulative distribution function the assimilated vector φ t a containing final updated parameters at time t is as follows 10 φ t a x t a y t a t t a φ t 1 x t a φ t 1 y t a φ t 1 t t a 4 go back to step 2 and repeat the calculation process until all observation data in the time series are assimilated 2 3 method for identifying the source mass until now the source location and the initial release time which have been optimized in the rc enkf are considered as the known information at this stage the source release quantity is the only factor that influences the value of concentration at observation sites according to the relationship the source release quantity is optimized by the following optimization objective function cheng 2010 11 m i n i m i z e r m s e i 1 n c i ob c i s 2 n 1 2 where i is the observation time step c i ob and c i s are the concentration observed and simulated at observation time i n is the amount of time series data used for assimilating at the last step 3 validation the proposed method is validated by a surface water numerical case in a stream segment the length and width of the stream segment selected are 8500 m and 1200 m respectively in this case the observation data and the hydrological parameters of the river are obtained from sun 2020 to simplify the calculation of the mass transport equation the mass is assumed to be released instantaneously at the shore and the analytical solution of this problem can be obtained below sun et al 2020 12 c m 4 π h d x d y exp k t x u x t 2 4 d x t 2 exp y 2 4 d y t exp 2 b y 2 4 d y t where c g l is the concentration m kg is the sudden input mass h m is the river depth b is the width of the stream segment dx and dy m2 s are the dispersion coefficient in the x and y direction respectively ux and uy m s are the flow velocity in the x and y direction respectively k 1 min is the total attenuation coefficient here x and y m represent the distance between the mass source and one point e g observation point in this stream segment in the x and y direction respectively and t min represents the time from the moment of mass input to some moment e g observation time in this case a certain mass of contaminants which are replaced by rhodamine b is released suddenly at a distance of 4500 m from the observation site at this observation site the interval of pollution concentration observation time is 25 min and the total observation duration is 300 min the time of mass input is at 100 min before the first observation time to obtain more observation data to keep the model running the raw observation data from sun 2020 are interpolated with a shorter observation interval which is 1 min fig 1 shows the raw observation data and the data interpolated the known hydrological parameters of this stream segment are displayed in table 1 and the parameters to be identified are the released contaminant mass location and time seen in table 2 here due to the sectional observation the location to be inverted is only the distance in the x coordinate the initial ensemble of 200 parameter quadruplets is generated from uniform distributions not centered at the truth values the details are as follows x 0 6000 m and t 0 300 min although this method is irrelevant to the source release mass we still need to assign a value of source release mass first to keep the model running normally here we set the value to 300 kg in the traditional enkf the assimilation can start from the first observation time step but in the rc enkf proposed in this paper the assimilation has to start from the second observation time step at least due to the calculation of the relation coefficient requiring a series of data in this case we start the assimilation from the 10th observation step according to fig 2 fig 4 the rc enkf has been verified that it can be used for identifying the contaminant source parameters successfully by introducing the relation coefficient and decoupling parameters of the contaminant source starting from the ensembles with wide ranges and uniform distributions of unknown source location and initial release time not necessarily required to follow gaussian distributions we finish this validation with an accurate estimation of the values of these two parameters fig 2 shows the boxplots for the parameters that indicate the information of the contaminant source calculated at different time steps during the assimilation process as shown in the figs 1 2 though the total release mass is inputted not as the exact value when the other source parameters are assimilated the median values red bars in the boxplots of source location x and initial release time t ensembles have been quite close to the reference ones from time step 120 and the convergence rate of the parameter x is almost in phase with that of the parameter t it means that this method has a good performance in dealing with the inverse problem of identifying source information and implies that the rc enkf algorithm is an efficient synchronization optimization method fig 3 shows the mean values and variance values of ensembles of source parameters x and t from time step 10th to 300th the results displayed in fig 3 corresponding with fig 2 the evolution of mean values of parameters x and t tend to be stable and get quite close to the reference ones after time step 100 the variance values of parameters x and t decrease with time and reach a small value around 0 when the time step is close to 150 in addition the fluctuation trends of mean values and variance values of x are consistent with that of t it is clear that the parameters of source location and initial release time have been estimated reasonably and close to the real value after the parameters of source location and initial release time identified the rest parameter total release mass m can be calculated easily by equation 11 we use the mean values at the last time step of ensemble x and t as the estimated values and calculate the concentration sequence at the observation site the peak values of concentrations observed and simulated are used to estimate the general range of m which for this case are 728 6 mg l and 225 8 mg l respectively according to the strong linear relationship between concentration and release mass due to the supposed value of the release mass set to 300 kg the true value of m is around 968 kg then if the value is required to estimate more precisely a wide range around the value estimated roughly is selected to generate an ensemble for objective function optimization in this case an ensemble of 2901 realizations are drawn from 100 kg to 3000 kg with equidistance the optimal realization is the one that minimizes the objective function shown as equation 13 among all realizations the result is displayed in fig 4 the total release mass value 999 kg minimizing rmse is almost the same as the true value 1000 kg results confirmed that the method proposed in this paper is reasonable and effective for identifying the contaminant source information 4 discussion in this part a series of cases are used to make an overall comparison between the proposed rc enkf and traditional enkf methods the cases are designed in a two dimensional square domain of 500 m by 500 m with the depth of 1 m all four boundaries are open boundaries the comparison is mainly focused on the efficiency and accuracy of identifying the parameters of contaminant source i e the source location x y the initial release time t and release mass m in the optimization procedure of rc enkf the first is the assimilation of source location and initial release time and the following one is the optimization of the release mass based on the results from the first step whereas the three source parameters are assimilated at the same time in the traditional enkf in the comparison case the contaminant source located at coordinates 250 and 250 is released 10 kg pollutants at t 100 s instantaneously the true parameters of the contaminant source and the range for generating the initial ensemble realizations are listed in table 4 the flow and transport parameters are listed in table 3 the number of observation sites needs to be increased to ensure data comprehensiveness due to the dimension of source parameters increasing here five observation sites 1 5 are distributed at coordinates 290 260 300 255 330 270 300 270 280 260 respectively shown as in fig 5 the observation data of sites 1 3 contain 30 random error which of sites 4 and 5 just contain 10 random error the observation data of five sites with a random error are shown in fig 6 the observation concentration sequence starts from t 130 s and ends at t 300 s which corresponds to the calculation time step from 1 to 171 the interval of observation time is 1 s then five cases case1 2 3 4 5 in contrasts composed of 12 scenarios are designed to make an overall comparison between the rc enkf and traditional enkf on the algorithm efficiency and accuracy these 12 scenarios are determined by different observation errors number of observation sites number of ensemble realizations and grid size of terrain which are listed in table 5 specifically case 1 involves scenario s1 s2 s4 and s8 for testing the stability of two methods in the circumstance of several observation data with quite large error case 2 contains scenario s3 s4 s5 and s8 which is used to verify the sensitivity of these two methods on observation data error case 3 contains scenario s3 s5 s6 and s7 which is used to compare the dependence of two methods on the number of observation sites case 4 contains scenario s3 s5 s9 and s10 which is used to compare the sensitivity of the number of ensemble realizations case 5 contains scenario s3 s5 s11 and s12 which is used to compare sensitivity on the grid size here the grid size represents the uniform spacing of open boundaries and unstructured triangle meshes which are generated based on it these five cases case1 case5 are designed to analyse the comparison between rc enkf and traditional enkf on the influence and sensitivity of different observation errors number of observation sites number of ensemble realizations and number of model grids respectively the accuracy and efficiency of the two methods are compared in these cases the initial ranges of source parameters for the scenarios s1 s12 are listed in table 2 in all scenarios for rc enkf we still assign a mass value that is 50 kg here at the step of assimilating source location x y and release time t the assimilation of the scenarios for rc enkf method starts from the 11th observation time step while the scenarios for the traditional method start from the first time step because of the difference in dealing with source release mass m between rc enkf and enkf just the assimilated results of the source location x y and initial release time t are used to make a comparison of the calculation efficiency of the two methods and when we compare the accuracy of the final optimized results of all scenarios the results of source location x y initial release time t and release mass m are all used typically the uncertainty of observation error at every observation step and randomness of generating initial ensemble realizations of source parameters may influence the assimilation results of source parameters in order to avoid these uncertain factors affecting our judgment on the final results of the two methods every scenario is replicated independently 100 times and all results we used to compare are the mean results of 100 experiments 4 1 influence of abnormal measured data one advantage of the enkf method is synchronization between update of observation data and assimilation but this advantage can also be disturbed and destroyed by occasionally abnormal observed data in case 1 which involves scenario s1 s2 s4 and s8 the results of rc enkf and traditional enkf both influenced by bad data at several time steps are compared for scenario s4 and s8 the observation data includes 20 random error same for scenario s1 and s2 except that the error fixedly increases to 100 bad data at the time step 60 80 and 100 due to the decoupled parameter m in rc enkf we can only compare the final results of m between the two methods and the comparison of evolution process at different steps is only suitable for parameter x y and t the assimilated and statistical results of source parameter have been displayed in fig 7 fig 10 the algorithm efficiency of the four scenarios in case 1 is compared by the evolution in time step of mean values and standard deviation values of source location x y and initial release time t fig 7 shows the mean and standard deviation of ensemble realizations of source parameters x y t at different time step from 20 to 160 with an interval of 10 in case 1 for scenario s1 s2 s4 s8 in general no matter what the parameter x y or t is especially the source location x y the final mean values at the time step 160 assimilated by rc enkf are considerably closer to reference values than that by traditional enkf besides the standard deviations of ensemble x y and t in rc enkf get more and more close to zero which means the ensembles gather at corresponding mean values whereas the standard deviations of ensemble x y and t in traditional enkf don t have the characteristic of this trend for scenario s1 and s2 influenced by the bad data at time step 60 80 and 100 the time evolutions of mean values and standard deviations of ensemble x and y appear obvious fluctuation at the three time steps what s more the range of fluctuation in s1 is much smaller than that in s2 for mean values and standard deviation values of x and y at time step 60 80 and 100 respectively though observation data with huge error at time step 100 in scenario s1 the time evolutions of mean values of ensemble x y and t keep stable and the time evolutions of standard deviations still tend to be close to that in scenario s4 it indicates that the rc enkf is more stable and the parameters assimilated by it are not susceptible to interference when there are major errors in observation data fig 8 shows cumulative frequency of assimilation results of source parameters x y t m at different error ranges 0 10 10 20 20 50 and larger than 50 for scenarios s1 s2 s4 s8 obviously the final results optimized by rc enkf are more reliable than that optimized by the traditional enkf no matter what the parameter assimilated is for source location x and y the error distributions in scenario s1 and s4 are almost the same for initial release time t more than 50 of results from s4 are distributed in the error range 0 20 which is better than the results from s1 but due to the huge observation errors at some time steps the errors of total release mass m optimized in s1 are mainly distributed at the range larger than 50 it also demonstrates that the rc enkf is not susceptible to interference from observation data with abnormal errors for assimilating parameters x y and t but after the assimilation the abnormal errors have influence on optimization of total mass m fig 9 and fig 10 are results from the 10th experiments of all which show boxplots and time evolution of the means and variance values of source parameters x y t updated at different time step for scenario s1 s2 s4 s8 the red bar of boxplots in fig 9 represents the median of ensemble realizations the trends of means and variance values of parameters x y t are in strong accordance with the trends in fig 7 4 2 sensitivity of observation data error in case 2 covering scenario s3 s4 s5 s8 the performance and sensitivity of rc enkf and traditional enkf on different observation errors is presented and discussed the assimilated and statistical results of source parameter have been displayed in fig 11 fig 14 the algorithm efficiency of the four scenarios in case 2 is compared by the evolution in time step of mean values and standard deviation values of source location x y and initial release time t fig 11 shows the mean and standard deviation of ensemble realizations of source parameters x y t at different time step from 20 to 160 with an interval of 10 in case 2 for scenario s3 s4 s5 s8 obviously the trend of assimilated mean values of source location x y and initial release time t in scenario s3 and s4 for rc enkf has gradually reached equilibrium near the time step 100 and the final assimilated mean values of parameters x y t in scenario s3 and s4 for rc enkf are closer to the true value than that in scenario s5 and s8 for traditional enkf the evolution in time step of standard deviation values of parameters x y t for these four scenarios is as the same as that of mean values the ensemble realizations of parameters x y t get centralized as the increase of time step and reach equilibrium nearly at time step 100 in scenario s3 and s4 for rc enkf it indicates that the algorithm of rc enkf is much more efficient than traditional enkf and the assimilated results of source parameters have higher precision in fig 11 it is worth noting that the standard deviations of realizations of source parameters x y and t in s5 and s8 show an increasing trend in the assimilation process this is because the random observation error in each time step and covariance matrix of observation errors are ignored and the parameter group of the contaminant source to be identified includes the total release mass different from the rc enkf also the total release mass has a more direct and greater influence on the concentration process at observation sites when the parameter of total release mass fluctuates slightly the distribution of realizations of source location and release time will be greatly affected the accuracy of algorithm of the four scenarios in this case is compared by the error range distribution of source parameters fig 12 shows cumulative frequency of assimilation results of source parameters x y t m at different error ranges 0 10 10 20 20 50 and larger than 50 for scenarios s3 s4 s5 s8 the results of all source parameters from scenario s3 and s4 for rc enkf are much better than that from scenario s5 and s8 for traditional enkf especially the obvious comparison between scenario s3 for rc enkf with 30 observation error and scenario s5 for enkf with 20 observation error besides the results of source location x y are all very close to the true from 100 experiments for both scenario s3 and s4 though with 30 and 20 random observation error respectively the error distribution of parameter t for scenario s3 is slightly worse than that for scenario s4 while the error distribution of parameter m for scenario s3 is little better than that for scenario s4 this is because the optimization of the total release mass is based on the mean of source location and release time ensemble that are assimilated at the last time step in some circumstances the combination may be composed of the two parameters both with large posterior errors and the error of observation data has a contingency these may lead to excessive distortion in the optimization of the source release mass as a whole it indicates that the algorithm of rc enkf is far more accurate than traditional enkf and the optimized results of source parameters from rc enkf are not easily influenced by the instable observation error and what is worth noting is that the optimization of parameter m is only related to the observation error and final assimilated results of source location and initial release time in algorithm of rc enkf which makes the rc enkf algorithm more stable when updating in assimilation step fig 13 and fig 14 are results from the 10th experiments of all which show boxplots and time evolution of the means and variance values of source parameters x y t updated at different time steps for scenario s3 s4 s5 s8 4 3 influence of the observation sites number in case 3 involving scenario s3 s5 s6 s7 the influence of observation sites number on assimilated results from the rc enkf and traditional enkf is discussed to reflect the difference between these two methods as precisely as possible the random errors of observation sites 4 and 5 are reduced to 10 in scenario s6 and s7 and the observation data of observation sites 1 2 and 3 are still with random 30 error in scenario s3 and s5 fig 15 shows the mean and standard deviation of ensemble realizations of source parameters x y t at different time steps from 20 to 160 with an interval of 10 in case 3 for scenario s3 s5 s6 s7 from the left three pictures except for the mean of parameter t at time step 160 almost same for all scenarios the means of x and y at last time step in scenario s3 are both closer to the true values than that in scenario s5 s6 and s7 though observation data used in scenario s3 are only from observation sites 1 2 and 3 moreover this figure shows that the ensembles of source parameters x y t in scenario s3 assimilated by rc enkf converge at time step 100 faster than that in other scenarios by traditional enkf method fig 16 shows cumulative frequency of assimilation results of source parameters x y t m at different error ranges 0 10 10 20 20 50 and larger than 50 for scenarios s3 s5 s6 s7 apparently the statistical results of all source parameters in scenario s3 for rc enkf are far better than that in other three scenarios by traditional enkf especially the error control of parameters x y and m for the three scenarios of traditional enkf the statistical errors of parameter x y and t assimilated in scenario s7 are just a little better than that in scenario s5 and s6 but the result of parameter m assimilated in scenario s7 is the most unsatisfactory it demonstrates that the accuracy and efficiency of rc enkf algorithm is less dependent on the number of observation sites and more applicable compared with traditional enkf fig 17 and fig 18 are results from the 10th experiments of all the trends of boxplots means and variance values of parameters x y t in the four scenarios are almost the same as that in fig 12 in fig 17 the boxplot of s5 and s6 suddenly appears population divergence in the assimilation process due to the random observation error at every time step and the source release mass participating in the assimilation that influences the simulated concentration greatly the phenomenon is not obvious in s7 because there are more observation sites in s7 and more controlling conditions that can weaken the effect of the assimilation process 4 4 sensitivity of ensemble realizations in case 4 the sensitivity of rc enkf and traditional enkf on the number of ensemble realizations is compared by scenario s3 s5 s9 s10 fig 19 shows the mean and standard deviation of ensemble realizations of source parameters x y t at different time steps from 20 to 160 with an interval of 10 for scenario s3 s5 s9 s10 as is clearly shown whether for the time evolution of the mean or standard deviation of source parameters x y t the results of scenario s3 and s9 for rc enkf are much better than that in scenario s5 and s10 except for the similar results which is the mean value of parameter t from the four scenarios moreover the mean values of every source parameters x y t at time step 160 in scenario s3 and s9 are almost equally close to the corresponding true values the cumulative frequency of assimilation results of source parameters x y t m at different error ranges 0 10 10 20 20 50 and larger than 50 for scenario s3 s5 s9 s10 are shown in fig 20 obviously for source parameters x y and m the results of statistical errors in scenario s3 and s9 are much better than that in scenario s5 and s10 but for source parameter t the results of statistical errors in all scenarios are not so satisfactory and most of errors are in the range of 20 50 the distributions of error cumulative frequency of parameter x y in scenario s3 and s9 are almost the same and the distribution of error cumulative frequency of parameter m in scenario s3 is better than in scenario s9 that results from the different distribution of parameter x y and t in scenario s3 and s9 because the calculation of parameter m in rc enkf is only related to the group of parameter x y and t assimilated at the final time step it demonstrates that the algorithm rc enkf can still work properly even though given less ensemble realizations it makes the calculation efficiency greatly improved with accuracy ensured on identifying contaminant source parameters fig 21 and fig 22 are results from the 10th experiments of all the trends of boxplots means and variance values of parameters x y t in the four scenarios are almost the same as that in fig 16 on the contrary whether in scenario s5 with 1000 realizations or in scenario s10 with 500 realizations the ensembles of source parameters x y t assimilated by traditional enkf are difficult to provide good guidance for the identification of the contaminant source 4 5 sensitivity of the grid size in case 5 the influence of model grid size on source parameter optimization of rc enkf and traditional enkf is compared by scenario s3 s5 s11 and s12 typically the concentration process simulated with finer grid size is more closely fit to the observation data own to the diffusion effect arisen by coarse mesh fig 23 shows the mean and standard deviation of ensemble realizations of source parameters x y t at different time steps from 20 to 160 with an interval of 10 in case 5 for scenario s3 s5 s11 s12 for source parameters x and y the mean value assimilated by rc enkf in s3 and s11 is much closer to the true value than that assimilated by enkf in s5 and s12 and the standard deviation values assimilated by rc enkf in s3 and s11 are gradually close to the value 0 as time step increasing moreover the accuracy of mean values of x and y assimilated at time step 160 in scenario s3 is little higher than that in scenario s11 although the grid number considerably reduced with the mesh size increased the results from scenario s3 and s11 are both acceptable for source parameter t though the time step evolution of mean value tends to be stable at time step 100 in s3 and s11 the final result of mean in s3 is the same as that in s5 and what is in s11 is the farthest from the true value the trends of standard deviation value in s3 and s11 are better than that in s5 and s12 the cumulative frequency of the assimilated source parameters x y t m at different error ranges 0 10 10 20 20 50 and larger than 50 for scenario s3 s5 s11 s12 are shown in fig 24 for source parameter x and y the statistical results in scenario s3 and s11 are much better than that in scenario s5 and s12 especially the errors of parameter y in scenario s3 and s11 are almost less than 10 for the parameters t and m the distributions of errors in s3 are the best of all scenarios whereas the most errors of the two parameters in s11 are larger than 50 and when reducing the number of model grids in enkf the errors of parameter t have a high possibility of increasing influence by it the errors of parameter m are more likely to get larger it illustrates that the rc enkf is more effective in identifying the source location which is not susceptible to the model grid size fig 25 and fig 26 are results from the 10th experiments of all the trends of boxplots means and variance values of parameters x y t in the four scenarios are almost in accordance with that in fig 23 in fig 26 the mean and variance of the realizations of parameters x y and t in s12 are better than those in s5 the result is contingent under the same condition the concentration process calculated in s12 is slower than that in s5 and the peak value in s12 is smaller due to the random error of observation data the simulated concentration of the parameter group ensembles in s12 is coincidentally close to the observation data at some time step and the ensemble of total release mass is just right for convergence it may lead to the result in fig 26 overall the results assimilated by the rc enkf are better than traditional enkf in this part the rc enkf method has a larger deviation in the result of the identification of the source release time compared with that of the pollution source location this is because a constant flow field is adopted in the generalized dynamic model and the velocity is set as a small value in order to make the relation coefficient close to 1 i e there is no phase difference between the observed concentration process and the simulated concentration process of the posterior source parameter when there is a deviation between the posterior source position and the reference source position the deviation of the leading quantity or delay quantity of the release time will be enlarged further throughout the five cases above we can conclude that the rc enkf is more applicable and effective for identifying the contaminant source information benefiting from using observation data sequence at every assimilation step by introducing the relation coefficient as observations the accuracy of this method is not easy to be influenced by the error of observation data and is fewer dependent on the number of observation sites ensemble realizations and model grids than that of the traditional enkf in addition the ensemble of rc enkf is more efficient in assimilation but the ensemble of enkf is inefficient due to the increasing dimension of parameters to be assimilated in the traditional enkf the estimation of source release mass is more greatly and easily influenced by the error of observation data however the ensemble assimilated by the enkf is prone to collapse if the realizations of the ensemble are few which is mainly caused by the underestimation of the covariance of the prediction error chen et al 2020 the rc enkf is based on the enkf so that it is probable to encounter the issues of ensemble collapse and remote spurious relation in this paper the contaminant source identification model has undergone generalization such as the constant flow field and constant diffusion coefficients which reduces the degree of nonlinearity of the model so the issues of remote spurious relation and ensemble collapse in the assimilation process occur with low possibility but in the case of a complex environment such as the estuarine area with the reciprocating flow and variable diffusion coefficients with few realizations in the ensemble the nonlinear degree of the model is increased and the issue of long distance spurious relation may appear and result in the divergence of filtering in addition from the perspective of uncertainty quantification uq the rc enkf has substantially under estimated uncertainties in the posterior ensembles caused by the narrow spreads of the ensemble in order to avoid such issues and decision making errors caused by excessive concentration of realizations in the ensemble we can get the ensemble resampled or inflated when the ensemble realizations are concentrated to a certain extent and then comprehensively consider these final assimilating distributions or consider using covariance localization greybush et al 2011 covariance inflation wang and bishop 2003 liang et al 2012 bauser et al 2018 or updated damping hendricks franssen and kinzelbach 2008 at the assimilating step 5 summary and conclusion the study proposed a new method named rc enkf for identifying the unknown contaminant source information of a sudden pollution accident by a simple surface water numerical case this method has proved to be reasonable and effective for the joint identification of contaminant source parameters i e source location source initial release time and total source release mass by comparing with the traditional enkf method the method displayed the following features 1 by introducing the relation coefficient between concentrations observed and simulated at observation sites the source release mass is decoupled from the parameter group of contaminant source it considerably reduces the calculating load of assimilating the parameter group still with high accuracy 2 applied in the five cases containing 12 scenarios the accuracy and efficiency of rc enkf is verified it performs much better than the traditional enkf when there is an uncertain error in observation data sequence and even huge error in several observation data by using the rc enkf method the single measuring error has minor influence on the correlation coefficient evaluation state r especially when the measuring sequence increases with time this avoids the defect of the traditional method that measuring error will produce a large ensemble deviation notice that the measurement error is often not easy to be identified 3 the accuracy of rc enkf is more applicable and less dependent and sensitive on the number of observation sites ensemble realizations and model grids compared with traditional enkf in view of this there is more room for improvement on the calculation efficiency of the rc enkf than that of the traditional enkf throughout the results from scenarios for rc enkf the value of source location assimilated is very close to the reference value with minor fluctuation the rc enkf proposed in this paper performs as a good method for identifying the contaminant source information accurately and quickly in addition to the instantaneous single point sources the rc enkf is also suitable for the continuous single point sources and multi point sources with different release substances ignoring mutual reactions in the future it is still worth continuing research on identifying the continuous multi source information by this method in a complex area credit authorship contribution statement li jing conceptualization methodology software validation formal analysis writing original draft jun kong conceptualization writing review editing supervision project administration funding acquisition jun wang data curation visualization writing original draft teng xu methodology writing review editing supervision mingjie pan validation formal analysis investigation weilun chen investigation resources declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported by the national key r d program of china 2021yfb2600200 the national natural science foundation of china 51979095 and the qing lan project of jiangsu province 2020 
2433,to reduce the adverse influence of sudden water pollution accidents it is essential to estimate the unknown contaminant source information normally including the source location initial release time and total release mass as soon as possible the ensemble kalman filter enkf has been proven to be an effective algorithm for such an inverse problem this paper proposes a new method based on enkf to identify contaminant source information the method we called rc enkf uses the relation coefficient of concentration instead of timely concentration as a state variable in the assimilation process the advantage lies in the release source mass that can be decoupled from the parameter group of unknown contaminant information to improve the assimilation speed and reduce interference with the accuracy of assimilation two categories of cases are employed for validating the applicability and testing the performance compared with the traditional enkf method in detail sensitivity analyses are carried out with different observation errors number of observation sites number of ensemble realizations and model grid size the results demonstrate that with the uncertain error of observation data the rc enkf works nicely and shows superiority to the traditional enkf method reflected in the strong immunity to interference from observation data errors and elevated efficiency with the requirement of fewer observation sites ensemble realizations and model grids it illustrates that the rc enkf is a more efficient and robust method for estimating unknown contaminant source information keywords contaminant source identification ensemble kalman filter relation coefficient assimilation parameter decoupled data availability data will be made available on request 1 introduction water pollution problem has posed a great threat to social development and human health cheng and jia 2010 amiri et al 2019 in recent years global water pollution incidents caused by industrial waste or transportation accidents with risky chemical materials occur frequently quickly and precisely obtaining the information on the contaminant sources such as contaminant source location initial release time and initial release concentration is crucial to accident management control risk assessment and remediation mirghani et al 2009 wang et al 2019 xu et al 2021 in the past three decades the problem of identifying the contaminant source has gained much attention from researchers around the world atmadja and bagtzoglou 2001 michalak and kitanidis 2003 2004 sun et al 2006 xu and gómez hernández 2018 in general whether in surface water or groundwater the method of identifying contaminant sources can be classified into two main categories deterministic optimization approaches and probabilistic approaches chen et al 2018 wang et al 2019 moghaddam et al 2021 the former approaches get parameters of the inverse problem optimized based on minimizing a given objective function such as canonical transformation which transforms the inverse problem into a well posed question to solve skaggs and kabala 1994 direct analysis methods neupauer et al 2000 li et al 2006 least squares regression analysis methods gorelick et al 1983 alapati and kabala 2000 bagtzoglou and atmadja 2003 sun et al 2006 or other intelligent optimization algorithms singh and datta 2007 yeh et al 2007 jha and datta 2013 gzyl et al 2014 zhang and xin 2017 the latter approaches convert the awaiting inversion parameters into random variables by introducing a random process to the inverse problem and constructing a stochastic framework to get variables estimated cupola et al 2015 an et al 2021 these two categories of methods have been constantly improved and applied by researchers since the problem of identifying a contaminant source was raised in the first category datta et al 2011 proposed a source identification methodology that used a classical nonlinear optimization model linked to a flow and transport simulation model ayvaz 2016 developed an integrated simulation optimization approach to solve the areal groundwater pollution source identification problems xia et al 2019 developed a simulation optimization approach by combining a transport simulation model and a genetic optimization algorithm to determine the pollutant source fluxes han et al 2020 proposed a method combining the advection dispersion equation ade of contaminants in groundwater with a genetic algorithm ga to identify groundwater pollution sources lei et al 2022 established a new model which combined space time radial basis collocation method rbcm and differential evolution algorithm dea to identify the source release history the applications of the above methods could not only solve the problem by minimizing the difference between the simulated value and the true value but also provide a reliable reference for the control and efficient remediation of site pollution in the second category yan et al 2019 proposed an innovative framework for groundwater contaminant sources identification which was based on bayesian theory and integrated the relative entropy a 0 1 integer programming optimization model markov chain monte carlo and a kriging surrogate model in addition xing et al 2019 proposed an ensemble surrogate model to improve the accuracy and robustness of results to identify groundwater contaminant sources by using an adaptive metropolis markov chain monte carlo method to assign weights to the three models that separately are kriging radial basis functions and least squares support vector machines ghane et al 2016 solved pollution source identification problems in a river network based on the backward probability method the approach was accurate and computationally efficient and did not need any simplification in river geometry and flow kanao and sato 2022 proposed the adjoint marginal sensitivity method a time backward probabilistic method to estimate a leakage position and its flux by assimilating a limited number of observed concentration data the ensemble kalman filter enkf proposed by evense 1994 an efficient probabilistic approach gets much attention and wide applications in various fields in recent decades such as oceanography meteorology petroleum engineering and hydrology houtekamer and mitchell 2001 li et al 2012 xu et al 2013 xia et al 2021 the advantage of the enkf is that the covariances and cross covariances of parameters and state variables of systems are calculated based on an ensemble of realizations making it less computational cost than other bayesian methods in recent years it has been introduced for identifying parameters of contaminant sources or aquifer parameters due to its excellent performance high efficiency and easy integration in dealing with inverse problems franssen and kinzelbach 2009 zhou et al 2011 xu et al 2016b first explored the application of the enkf for the simultaneous identification of contaminant source parameters i e the source location the release time and the release concentration and later xu et al 2018 developed a restart normal score ensemble kalman filter for the simultaneous identification of a contaminant source and the spatially variable hydraulic conductivity in a synthetic aquifer notice that different from the standard enkf in the forecast step of the restart normal score enkf state variables were forecasted from the step of time zero instead of the last updating time step which is necessary to identify the contaminant source the method has not only been successfully proven in synthetic cases but also in sandbox experiments chen et al 2018 2021 in addition to the application in the identification of groundwater contaminant source information it has also been employed in the identification of surface water contaminant source information wang et al 2019 proposed an effective method for point pollution source identification in rivers with a performance improved ensemble kalman filter where the ensemble kalman filter was coupled with the backward location probability to improve its performance in the identification of the river pollution source the main purpose of this paper is to propose an improved ensemble kalman filter based on relation coefficient assimilation rc enkf for the identification of surface water contaminant sources the instantaneous point source parameters of interest include the release position the initial release time and the initial release concentration or total mass in the previous research on the identification of contaminant source parameters the contaminant concentrations at observation points in time series were assimilated it was easy to be understood and implemented and the assimilation result was acceptable if the error distributions of measuring equipment were known in advance however in reality the error distributions of measuring equipment are unknown or uncertain due to many reasons such as equipment aging observation sites human factors and so on to deal with this deficiency we proposed a new ensemble kalman filter method based on relation coefficient assimilation which can avoid the problem mentioned above at the same time promote the efficiency and accuracy of assimilation in this method the relation coefficient between concentrations predicted and observed at every time step is introduced in the enkf instead of the absolute value of concentrations benefited by this the release location and initial release time which are relatively more important can be first obtained not affected by the total amount of pollution then the release mass is calculated independently based on the predicted release location and initial release time obtained from the previous step the paper proceeds firstly with the algorithm description of the improved enkf we proposed then five numerical cases are designed for testing the applicability of the rc enkf on the problem of identifying contaminant source and making a comprehensive comparison between rc enkf and traditional enkf the paper ends with a summary and conclusions of the results 2 methodology 2 1 surface water solute transport equation aiming at the water pollution problem in wide and shallow rivers the pollutant transport process can be depicted by the following two dimensional advection diffusion equation kong et al 2013 wang et al 2018 1 h c t h u c x h v c y x h d xx c x h d xy c y y h d yx c x h d yy c y s where c kg m3 is the contaminant concentration u v m s are the flow velocity component in the y direction respectively h m is the water depth d m2 s is the 2 d diffusion coefficient tensor in the x and y spatial coordinates the value of which is relating to u v s is the source term due to the complexity of the equation the mathematical model is widely used in this paper the total variation diminishing tvd based anisotropic model is adopted kong et al 2013 2 2 the ensemble kalman filter similar to the standard enkf two types of variables involved in the rc enkf are the system parameters and the state variables respectively the state variables in this paper are not the contaminant concentrations as in previous researches xu et al 2016b chen et al 2018 wang et al 2019 which are instead by the relation coefficients between observed concentrations and simulated concentrations the system parameters include the source location x or x and y the initial release time t and the initial release mass m respectively which we need to estimate here we use φ to represent the vector of the system parameters the implementation of the rc enkf for identifying the three parameters of surface water contaminant sources mentioned above includes the following steps 1 generate the initial ensemble in this step an ensemble of nr realizations of the three source parameters mentioned above is generated the initial values of these parameters are selected stochastically from independent uniform distributions the upper values and lower values of uniform distributions can be determined by the prior information and the true values of parameters do not have to be contained in ranges of distributions so nr vectors of φ are constructed 2 φ i x i y i t i m i t where i is the index of realization and the superscript t represents the transposition of the matrix 2 forecast in this step the last parameters updated at observation time t 1 are used as the new inputs to rerun the transport model from time 0 to observation time t then the state of observation time t is simulated and the state of time 0 t 1 is re simulated usually the state variable simulated refers to contaminant concentration the forecast of concentrations is as follows 3 c t f i ψ c 0 φ t 1 a i where c 0 means the state of initial contaminant distribution we consider it as 0 in the whole domain before the contaminant is released the superscript a and the subscript f are the forecasted and updated values respectively ψ represents the state transfer operator which in this paper is the mass transportation model in the rc enkf we take the pollution concentration as the intermediate state and introduce the relation coefficient r the function of observed concentrations and simulated concentrations as the final evaluation state variable the relation coefficient r is expressed by the following formula sidauruk 1998 jing 2018 4 r j 1 n c j s c s c j ob c ob j 1 n c j s c s 2 j 1 n c j ob c ob 2 where the subscript j is the index of the time series the subscript s and ob are the value of simulated and observed respectively the c s and c ob are the mean value of c s and c ob respectively so the forecast of the evaluation state in this paper can be expressed as 5 r t f i f ψ c 0 φ t 1 a i where f is the transformation operator which transforms c i f to r i f because of the characteristic of contaminant transport when the source location and release time are determined and other conditions remain unchanged the value of concentration at one site is proportional to the release mass in other words once the source location and release time are determined no matter what the release mass m is the relation coefficient r will keep the same value so the r introduced decouples the release mass m from the source parameters set to be identified after this step the vector φ just contains the source location and the source release time shown as follows 6 φ x y t t 3 update before this step we need to apply the normal score transform for parameters since the parameters are selected in a uniform distribution and the enkf works best if the parameters are in a gaussian distribution the normal score transform function needs to be recomputed after each forecast step the system parameters are transformed by the following formula 7 φ t x t y t t t t φ t x t φ t y t φ t t t t where φt represents the normal score transformation at time t which is related to the cumulative distribution functions here the evaluation state r remains untransformed because the previous researches verified that the method was capable enough to identify non gaussian distribution parameters when the state variables remained untransformed xu et al 2016a then parameters are updated here we assume the number of observation sites is nob every realization of the ensemble is updated as the following equation 8 φ t a i φ t f i g t r t r t f i e t where φ t a and φ t f are the updated and the forecasted vectors of normal scored source parameter at time t respectively r t is the auto relation coefficient of observation contaminant concentration during time 0 to time t which is equal to 1 e t is the error vector of r t in the traditional enkf e t is the observation error vector which is drawn from a normal distribution with mean zero and covariance matrix p t xu 2016b but here due to the state variable transformed from concentration to the relation coefficient considering observation errors the value of e t between the observed concentrations and the true concentrations is more difficult to estimate compared with that in the traditional enkf so the value of e t is ignored and set to 0 in this paper gt is the kalman gain matrix which is expressed by 9 g t d φ r f d r f r f r t 1 where r t is the covariance matrix of the error vector e t which is ignored due to the value 0 of e t mentioned above d φ r f is the cross variance between source parameters vector φ and the relation coefficient r f forecasted at observation sites d r f r f is the autocovariance of the relation coefficient r f after that the updated transformed source parameters φ t need to be transformed back to φ t shown in step 2 by using the inverse transformation function φ t 1 which is related to the inverse cumulative distribution function the assimilated vector φ t a containing final updated parameters at time t is as follows 10 φ t a x t a y t a t t a φ t 1 x t a φ t 1 y t a φ t 1 t t a 4 go back to step 2 and repeat the calculation process until all observation data in the time series are assimilated 2 3 method for identifying the source mass until now the source location and the initial release time which have been optimized in the rc enkf are considered as the known information at this stage the source release quantity is the only factor that influences the value of concentration at observation sites according to the relationship the source release quantity is optimized by the following optimization objective function cheng 2010 11 m i n i m i z e r m s e i 1 n c i ob c i s 2 n 1 2 where i is the observation time step c i ob and c i s are the concentration observed and simulated at observation time i n is the amount of time series data used for assimilating at the last step 3 validation the proposed method is validated by a surface water numerical case in a stream segment the length and width of the stream segment selected are 8500 m and 1200 m respectively in this case the observation data and the hydrological parameters of the river are obtained from sun 2020 to simplify the calculation of the mass transport equation the mass is assumed to be released instantaneously at the shore and the analytical solution of this problem can be obtained below sun et al 2020 12 c m 4 π h d x d y exp k t x u x t 2 4 d x t 2 exp y 2 4 d y t exp 2 b y 2 4 d y t where c g l is the concentration m kg is the sudden input mass h m is the river depth b is the width of the stream segment dx and dy m2 s are the dispersion coefficient in the x and y direction respectively ux and uy m s are the flow velocity in the x and y direction respectively k 1 min is the total attenuation coefficient here x and y m represent the distance between the mass source and one point e g observation point in this stream segment in the x and y direction respectively and t min represents the time from the moment of mass input to some moment e g observation time in this case a certain mass of contaminants which are replaced by rhodamine b is released suddenly at a distance of 4500 m from the observation site at this observation site the interval of pollution concentration observation time is 25 min and the total observation duration is 300 min the time of mass input is at 100 min before the first observation time to obtain more observation data to keep the model running the raw observation data from sun 2020 are interpolated with a shorter observation interval which is 1 min fig 1 shows the raw observation data and the data interpolated the known hydrological parameters of this stream segment are displayed in table 1 and the parameters to be identified are the released contaminant mass location and time seen in table 2 here due to the sectional observation the location to be inverted is only the distance in the x coordinate the initial ensemble of 200 parameter quadruplets is generated from uniform distributions not centered at the truth values the details are as follows x 0 6000 m and t 0 300 min although this method is irrelevant to the source release mass we still need to assign a value of source release mass first to keep the model running normally here we set the value to 300 kg in the traditional enkf the assimilation can start from the first observation time step but in the rc enkf proposed in this paper the assimilation has to start from the second observation time step at least due to the calculation of the relation coefficient requiring a series of data in this case we start the assimilation from the 10th observation step according to fig 2 fig 4 the rc enkf has been verified that it can be used for identifying the contaminant source parameters successfully by introducing the relation coefficient and decoupling parameters of the contaminant source starting from the ensembles with wide ranges and uniform distributions of unknown source location and initial release time not necessarily required to follow gaussian distributions we finish this validation with an accurate estimation of the values of these two parameters fig 2 shows the boxplots for the parameters that indicate the information of the contaminant source calculated at different time steps during the assimilation process as shown in the figs 1 2 though the total release mass is inputted not as the exact value when the other source parameters are assimilated the median values red bars in the boxplots of source location x and initial release time t ensembles have been quite close to the reference ones from time step 120 and the convergence rate of the parameter x is almost in phase with that of the parameter t it means that this method has a good performance in dealing with the inverse problem of identifying source information and implies that the rc enkf algorithm is an efficient synchronization optimization method fig 3 shows the mean values and variance values of ensembles of source parameters x and t from time step 10th to 300th the results displayed in fig 3 corresponding with fig 2 the evolution of mean values of parameters x and t tend to be stable and get quite close to the reference ones after time step 100 the variance values of parameters x and t decrease with time and reach a small value around 0 when the time step is close to 150 in addition the fluctuation trends of mean values and variance values of x are consistent with that of t it is clear that the parameters of source location and initial release time have been estimated reasonably and close to the real value after the parameters of source location and initial release time identified the rest parameter total release mass m can be calculated easily by equation 11 we use the mean values at the last time step of ensemble x and t as the estimated values and calculate the concentration sequence at the observation site the peak values of concentrations observed and simulated are used to estimate the general range of m which for this case are 728 6 mg l and 225 8 mg l respectively according to the strong linear relationship between concentration and release mass due to the supposed value of the release mass set to 300 kg the true value of m is around 968 kg then if the value is required to estimate more precisely a wide range around the value estimated roughly is selected to generate an ensemble for objective function optimization in this case an ensemble of 2901 realizations are drawn from 100 kg to 3000 kg with equidistance the optimal realization is the one that minimizes the objective function shown as equation 13 among all realizations the result is displayed in fig 4 the total release mass value 999 kg minimizing rmse is almost the same as the true value 1000 kg results confirmed that the method proposed in this paper is reasonable and effective for identifying the contaminant source information 4 discussion in this part a series of cases are used to make an overall comparison between the proposed rc enkf and traditional enkf methods the cases are designed in a two dimensional square domain of 500 m by 500 m with the depth of 1 m all four boundaries are open boundaries the comparison is mainly focused on the efficiency and accuracy of identifying the parameters of contaminant source i e the source location x y the initial release time t and release mass m in the optimization procedure of rc enkf the first is the assimilation of source location and initial release time and the following one is the optimization of the release mass based on the results from the first step whereas the three source parameters are assimilated at the same time in the traditional enkf in the comparison case the contaminant source located at coordinates 250 and 250 is released 10 kg pollutants at t 100 s instantaneously the true parameters of the contaminant source and the range for generating the initial ensemble realizations are listed in table 4 the flow and transport parameters are listed in table 3 the number of observation sites needs to be increased to ensure data comprehensiveness due to the dimension of source parameters increasing here five observation sites 1 5 are distributed at coordinates 290 260 300 255 330 270 300 270 280 260 respectively shown as in fig 5 the observation data of sites 1 3 contain 30 random error which of sites 4 and 5 just contain 10 random error the observation data of five sites with a random error are shown in fig 6 the observation concentration sequence starts from t 130 s and ends at t 300 s which corresponds to the calculation time step from 1 to 171 the interval of observation time is 1 s then five cases case1 2 3 4 5 in contrasts composed of 12 scenarios are designed to make an overall comparison between the rc enkf and traditional enkf on the algorithm efficiency and accuracy these 12 scenarios are determined by different observation errors number of observation sites number of ensemble realizations and grid size of terrain which are listed in table 5 specifically case 1 involves scenario s1 s2 s4 and s8 for testing the stability of two methods in the circumstance of several observation data with quite large error case 2 contains scenario s3 s4 s5 and s8 which is used to verify the sensitivity of these two methods on observation data error case 3 contains scenario s3 s5 s6 and s7 which is used to compare the dependence of two methods on the number of observation sites case 4 contains scenario s3 s5 s9 and s10 which is used to compare the sensitivity of the number of ensemble realizations case 5 contains scenario s3 s5 s11 and s12 which is used to compare sensitivity on the grid size here the grid size represents the uniform spacing of open boundaries and unstructured triangle meshes which are generated based on it these five cases case1 case5 are designed to analyse the comparison between rc enkf and traditional enkf on the influence and sensitivity of different observation errors number of observation sites number of ensemble realizations and number of model grids respectively the accuracy and efficiency of the two methods are compared in these cases the initial ranges of source parameters for the scenarios s1 s12 are listed in table 2 in all scenarios for rc enkf we still assign a mass value that is 50 kg here at the step of assimilating source location x y and release time t the assimilation of the scenarios for rc enkf method starts from the 11th observation time step while the scenarios for the traditional method start from the first time step because of the difference in dealing with source release mass m between rc enkf and enkf just the assimilated results of the source location x y and initial release time t are used to make a comparison of the calculation efficiency of the two methods and when we compare the accuracy of the final optimized results of all scenarios the results of source location x y initial release time t and release mass m are all used typically the uncertainty of observation error at every observation step and randomness of generating initial ensemble realizations of source parameters may influence the assimilation results of source parameters in order to avoid these uncertain factors affecting our judgment on the final results of the two methods every scenario is replicated independently 100 times and all results we used to compare are the mean results of 100 experiments 4 1 influence of abnormal measured data one advantage of the enkf method is synchronization between update of observation data and assimilation but this advantage can also be disturbed and destroyed by occasionally abnormal observed data in case 1 which involves scenario s1 s2 s4 and s8 the results of rc enkf and traditional enkf both influenced by bad data at several time steps are compared for scenario s4 and s8 the observation data includes 20 random error same for scenario s1 and s2 except that the error fixedly increases to 100 bad data at the time step 60 80 and 100 due to the decoupled parameter m in rc enkf we can only compare the final results of m between the two methods and the comparison of evolution process at different steps is only suitable for parameter x y and t the assimilated and statistical results of source parameter have been displayed in fig 7 fig 10 the algorithm efficiency of the four scenarios in case 1 is compared by the evolution in time step of mean values and standard deviation values of source location x y and initial release time t fig 7 shows the mean and standard deviation of ensemble realizations of source parameters x y t at different time step from 20 to 160 with an interval of 10 in case 1 for scenario s1 s2 s4 s8 in general no matter what the parameter x y or t is especially the source location x y the final mean values at the time step 160 assimilated by rc enkf are considerably closer to reference values than that by traditional enkf besides the standard deviations of ensemble x y and t in rc enkf get more and more close to zero which means the ensembles gather at corresponding mean values whereas the standard deviations of ensemble x y and t in traditional enkf don t have the characteristic of this trend for scenario s1 and s2 influenced by the bad data at time step 60 80 and 100 the time evolutions of mean values and standard deviations of ensemble x and y appear obvious fluctuation at the three time steps what s more the range of fluctuation in s1 is much smaller than that in s2 for mean values and standard deviation values of x and y at time step 60 80 and 100 respectively though observation data with huge error at time step 100 in scenario s1 the time evolutions of mean values of ensemble x y and t keep stable and the time evolutions of standard deviations still tend to be close to that in scenario s4 it indicates that the rc enkf is more stable and the parameters assimilated by it are not susceptible to interference when there are major errors in observation data fig 8 shows cumulative frequency of assimilation results of source parameters x y t m at different error ranges 0 10 10 20 20 50 and larger than 50 for scenarios s1 s2 s4 s8 obviously the final results optimized by rc enkf are more reliable than that optimized by the traditional enkf no matter what the parameter assimilated is for source location x and y the error distributions in scenario s1 and s4 are almost the same for initial release time t more than 50 of results from s4 are distributed in the error range 0 20 which is better than the results from s1 but due to the huge observation errors at some time steps the errors of total release mass m optimized in s1 are mainly distributed at the range larger than 50 it also demonstrates that the rc enkf is not susceptible to interference from observation data with abnormal errors for assimilating parameters x y and t but after the assimilation the abnormal errors have influence on optimization of total mass m fig 9 and fig 10 are results from the 10th experiments of all which show boxplots and time evolution of the means and variance values of source parameters x y t updated at different time step for scenario s1 s2 s4 s8 the red bar of boxplots in fig 9 represents the median of ensemble realizations the trends of means and variance values of parameters x y t are in strong accordance with the trends in fig 7 4 2 sensitivity of observation data error in case 2 covering scenario s3 s4 s5 s8 the performance and sensitivity of rc enkf and traditional enkf on different observation errors is presented and discussed the assimilated and statistical results of source parameter have been displayed in fig 11 fig 14 the algorithm efficiency of the four scenarios in case 2 is compared by the evolution in time step of mean values and standard deviation values of source location x y and initial release time t fig 11 shows the mean and standard deviation of ensemble realizations of source parameters x y t at different time step from 20 to 160 with an interval of 10 in case 2 for scenario s3 s4 s5 s8 obviously the trend of assimilated mean values of source location x y and initial release time t in scenario s3 and s4 for rc enkf has gradually reached equilibrium near the time step 100 and the final assimilated mean values of parameters x y t in scenario s3 and s4 for rc enkf are closer to the true value than that in scenario s5 and s8 for traditional enkf the evolution in time step of standard deviation values of parameters x y t for these four scenarios is as the same as that of mean values the ensemble realizations of parameters x y t get centralized as the increase of time step and reach equilibrium nearly at time step 100 in scenario s3 and s4 for rc enkf it indicates that the algorithm of rc enkf is much more efficient than traditional enkf and the assimilated results of source parameters have higher precision in fig 11 it is worth noting that the standard deviations of realizations of source parameters x y and t in s5 and s8 show an increasing trend in the assimilation process this is because the random observation error in each time step and covariance matrix of observation errors are ignored and the parameter group of the contaminant source to be identified includes the total release mass different from the rc enkf also the total release mass has a more direct and greater influence on the concentration process at observation sites when the parameter of total release mass fluctuates slightly the distribution of realizations of source location and release time will be greatly affected the accuracy of algorithm of the four scenarios in this case is compared by the error range distribution of source parameters fig 12 shows cumulative frequency of assimilation results of source parameters x y t m at different error ranges 0 10 10 20 20 50 and larger than 50 for scenarios s3 s4 s5 s8 the results of all source parameters from scenario s3 and s4 for rc enkf are much better than that from scenario s5 and s8 for traditional enkf especially the obvious comparison between scenario s3 for rc enkf with 30 observation error and scenario s5 for enkf with 20 observation error besides the results of source location x y are all very close to the true from 100 experiments for both scenario s3 and s4 though with 30 and 20 random observation error respectively the error distribution of parameter t for scenario s3 is slightly worse than that for scenario s4 while the error distribution of parameter m for scenario s3 is little better than that for scenario s4 this is because the optimization of the total release mass is based on the mean of source location and release time ensemble that are assimilated at the last time step in some circumstances the combination may be composed of the two parameters both with large posterior errors and the error of observation data has a contingency these may lead to excessive distortion in the optimization of the source release mass as a whole it indicates that the algorithm of rc enkf is far more accurate than traditional enkf and the optimized results of source parameters from rc enkf are not easily influenced by the instable observation error and what is worth noting is that the optimization of parameter m is only related to the observation error and final assimilated results of source location and initial release time in algorithm of rc enkf which makes the rc enkf algorithm more stable when updating in assimilation step fig 13 and fig 14 are results from the 10th experiments of all which show boxplots and time evolution of the means and variance values of source parameters x y t updated at different time steps for scenario s3 s4 s5 s8 4 3 influence of the observation sites number in case 3 involving scenario s3 s5 s6 s7 the influence of observation sites number on assimilated results from the rc enkf and traditional enkf is discussed to reflect the difference between these two methods as precisely as possible the random errors of observation sites 4 and 5 are reduced to 10 in scenario s6 and s7 and the observation data of observation sites 1 2 and 3 are still with random 30 error in scenario s3 and s5 fig 15 shows the mean and standard deviation of ensemble realizations of source parameters x y t at different time steps from 20 to 160 with an interval of 10 in case 3 for scenario s3 s5 s6 s7 from the left three pictures except for the mean of parameter t at time step 160 almost same for all scenarios the means of x and y at last time step in scenario s3 are both closer to the true values than that in scenario s5 s6 and s7 though observation data used in scenario s3 are only from observation sites 1 2 and 3 moreover this figure shows that the ensembles of source parameters x y t in scenario s3 assimilated by rc enkf converge at time step 100 faster than that in other scenarios by traditional enkf method fig 16 shows cumulative frequency of assimilation results of source parameters x y t m at different error ranges 0 10 10 20 20 50 and larger than 50 for scenarios s3 s5 s6 s7 apparently the statistical results of all source parameters in scenario s3 for rc enkf are far better than that in other three scenarios by traditional enkf especially the error control of parameters x y and m for the three scenarios of traditional enkf the statistical errors of parameter x y and t assimilated in scenario s7 are just a little better than that in scenario s5 and s6 but the result of parameter m assimilated in scenario s7 is the most unsatisfactory it demonstrates that the accuracy and efficiency of rc enkf algorithm is less dependent on the number of observation sites and more applicable compared with traditional enkf fig 17 and fig 18 are results from the 10th experiments of all the trends of boxplots means and variance values of parameters x y t in the four scenarios are almost the same as that in fig 12 in fig 17 the boxplot of s5 and s6 suddenly appears population divergence in the assimilation process due to the random observation error at every time step and the source release mass participating in the assimilation that influences the simulated concentration greatly the phenomenon is not obvious in s7 because there are more observation sites in s7 and more controlling conditions that can weaken the effect of the assimilation process 4 4 sensitivity of ensemble realizations in case 4 the sensitivity of rc enkf and traditional enkf on the number of ensemble realizations is compared by scenario s3 s5 s9 s10 fig 19 shows the mean and standard deviation of ensemble realizations of source parameters x y t at different time steps from 20 to 160 with an interval of 10 for scenario s3 s5 s9 s10 as is clearly shown whether for the time evolution of the mean or standard deviation of source parameters x y t the results of scenario s3 and s9 for rc enkf are much better than that in scenario s5 and s10 except for the similar results which is the mean value of parameter t from the four scenarios moreover the mean values of every source parameters x y t at time step 160 in scenario s3 and s9 are almost equally close to the corresponding true values the cumulative frequency of assimilation results of source parameters x y t m at different error ranges 0 10 10 20 20 50 and larger than 50 for scenario s3 s5 s9 s10 are shown in fig 20 obviously for source parameters x y and m the results of statistical errors in scenario s3 and s9 are much better than that in scenario s5 and s10 but for source parameter t the results of statistical errors in all scenarios are not so satisfactory and most of errors are in the range of 20 50 the distributions of error cumulative frequency of parameter x y in scenario s3 and s9 are almost the same and the distribution of error cumulative frequency of parameter m in scenario s3 is better than in scenario s9 that results from the different distribution of parameter x y and t in scenario s3 and s9 because the calculation of parameter m in rc enkf is only related to the group of parameter x y and t assimilated at the final time step it demonstrates that the algorithm rc enkf can still work properly even though given less ensemble realizations it makes the calculation efficiency greatly improved with accuracy ensured on identifying contaminant source parameters fig 21 and fig 22 are results from the 10th experiments of all the trends of boxplots means and variance values of parameters x y t in the four scenarios are almost the same as that in fig 16 on the contrary whether in scenario s5 with 1000 realizations or in scenario s10 with 500 realizations the ensembles of source parameters x y t assimilated by traditional enkf are difficult to provide good guidance for the identification of the contaminant source 4 5 sensitivity of the grid size in case 5 the influence of model grid size on source parameter optimization of rc enkf and traditional enkf is compared by scenario s3 s5 s11 and s12 typically the concentration process simulated with finer grid size is more closely fit to the observation data own to the diffusion effect arisen by coarse mesh fig 23 shows the mean and standard deviation of ensemble realizations of source parameters x y t at different time steps from 20 to 160 with an interval of 10 in case 5 for scenario s3 s5 s11 s12 for source parameters x and y the mean value assimilated by rc enkf in s3 and s11 is much closer to the true value than that assimilated by enkf in s5 and s12 and the standard deviation values assimilated by rc enkf in s3 and s11 are gradually close to the value 0 as time step increasing moreover the accuracy of mean values of x and y assimilated at time step 160 in scenario s3 is little higher than that in scenario s11 although the grid number considerably reduced with the mesh size increased the results from scenario s3 and s11 are both acceptable for source parameter t though the time step evolution of mean value tends to be stable at time step 100 in s3 and s11 the final result of mean in s3 is the same as that in s5 and what is in s11 is the farthest from the true value the trends of standard deviation value in s3 and s11 are better than that in s5 and s12 the cumulative frequency of the assimilated source parameters x y t m at different error ranges 0 10 10 20 20 50 and larger than 50 for scenario s3 s5 s11 s12 are shown in fig 24 for source parameter x and y the statistical results in scenario s3 and s11 are much better than that in scenario s5 and s12 especially the errors of parameter y in scenario s3 and s11 are almost less than 10 for the parameters t and m the distributions of errors in s3 are the best of all scenarios whereas the most errors of the two parameters in s11 are larger than 50 and when reducing the number of model grids in enkf the errors of parameter t have a high possibility of increasing influence by it the errors of parameter m are more likely to get larger it illustrates that the rc enkf is more effective in identifying the source location which is not susceptible to the model grid size fig 25 and fig 26 are results from the 10th experiments of all the trends of boxplots means and variance values of parameters x y t in the four scenarios are almost in accordance with that in fig 23 in fig 26 the mean and variance of the realizations of parameters x y and t in s12 are better than those in s5 the result is contingent under the same condition the concentration process calculated in s12 is slower than that in s5 and the peak value in s12 is smaller due to the random error of observation data the simulated concentration of the parameter group ensembles in s12 is coincidentally close to the observation data at some time step and the ensemble of total release mass is just right for convergence it may lead to the result in fig 26 overall the results assimilated by the rc enkf are better than traditional enkf in this part the rc enkf method has a larger deviation in the result of the identification of the source release time compared with that of the pollution source location this is because a constant flow field is adopted in the generalized dynamic model and the velocity is set as a small value in order to make the relation coefficient close to 1 i e there is no phase difference between the observed concentration process and the simulated concentration process of the posterior source parameter when there is a deviation between the posterior source position and the reference source position the deviation of the leading quantity or delay quantity of the release time will be enlarged further throughout the five cases above we can conclude that the rc enkf is more applicable and effective for identifying the contaminant source information benefiting from using observation data sequence at every assimilation step by introducing the relation coefficient as observations the accuracy of this method is not easy to be influenced by the error of observation data and is fewer dependent on the number of observation sites ensemble realizations and model grids than that of the traditional enkf in addition the ensemble of rc enkf is more efficient in assimilation but the ensemble of enkf is inefficient due to the increasing dimension of parameters to be assimilated in the traditional enkf the estimation of source release mass is more greatly and easily influenced by the error of observation data however the ensemble assimilated by the enkf is prone to collapse if the realizations of the ensemble are few which is mainly caused by the underestimation of the covariance of the prediction error chen et al 2020 the rc enkf is based on the enkf so that it is probable to encounter the issues of ensemble collapse and remote spurious relation in this paper the contaminant source identification model has undergone generalization such as the constant flow field and constant diffusion coefficients which reduces the degree of nonlinearity of the model so the issues of remote spurious relation and ensemble collapse in the assimilation process occur with low possibility but in the case of a complex environment such as the estuarine area with the reciprocating flow and variable diffusion coefficients with few realizations in the ensemble the nonlinear degree of the model is increased and the issue of long distance spurious relation may appear and result in the divergence of filtering in addition from the perspective of uncertainty quantification uq the rc enkf has substantially under estimated uncertainties in the posterior ensembles caused by the narrow spreads of the ensemble in order to avoid such issues and decision making errors caused by excessive concentration of realizations in the ensemble we can get the ensemble resampled or inflated when the ensemble realizations are concentrated to a certain extent and then comprehensively consider these final assimilating distributions or consider using covariance localization greybush et al 2011 covariance inflation wang and bishop 2003 liang et al 2012 bauser et al 2018 or updated damping hendricks franssen and kinzelbach 2008 at the assimilating step 5 summary and conclusion the study proposed a new method named rc enkf for identifying the unknown contaminant source information of a sudden pollution accident by a simple surface water numerical case this method has proved to be reasonable and effective for the joint identification of contaminant source parameters i e source location source initial release time and total source release mass by comparing with the traditional enkf method the method displayed the following features 1 by introducing the relation coefficient between concentrations observed and simulated at observation sites the source release mass is decoupled from the parameter group of contaminant source it considerably reduces the calculating load of assimilating the parameter group still with high accuracy 2 applied in the five cases containing 12 scenarios the accuracy and efficiency of rc enkf is verified it performs much better than the traditional enkf when there is an uncertain error in observation data sequence and even huge error in several observation data by using the rc enkf method the single measuring error has minor influence on the correlation coefficient evaluation state r especially when the measuring sequence increases with time this avoids the defect of the traditional method that measuring error will produce a large ensemble deviation notice that the measurement error is often not easy to be identified 3 the accuracy of rc enkf is more applicable and less dependent and sensitive on the number of observation sites ensemble realizations and model grids compared with traditional enkf in view of this there is more room for improvement on the calculation efficiency of the rc enkf than that of the traditional enkf throughout the results from scenarios for rc enkf the value of source location assimilated is very close to the reference value with minor fluctuation the rc enkf proposed in this paper performs as a good method for identifying the contaminant source information accurately and quickly in addition to the instantaneous single point sources the rc enkf is also suitable for the continuous single point sources and multi point sources with different release substances ignoring mutual reactions in the future it is still worth continuing research on identifying the continuous multi source information by this method in a complex area credit authorship contribution statement li jing conceptualization methodology software validation formal analysis writing original draft jun kong conceptualization writing review editing supervision project administration funding acquisition jun wang data curation visualization writing original draft teng xu methodology writing review editing supervision mingjie pan validation formal analysis investigation weilun chen investigation resources declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported by the national key r d program of china 2021yfb2600200 the national natural science foundation of china 51979095 and the qing lan project of jiangsu province 2020 
2434,over the recent decades particle filter pf based data assimilation has been adopted to improve hydrodynamic simulation due to its wide applicability and effectiveness for nonlinear and non gaussian models the particle weighting is the core procedure to combine the observations and simulations within the pf framework in which the gaussian likelihood function glf is commonly used to update weight values however there are some widely acknowledged challenges within the glf based pf among them the excessive dispersion problem occurs when the standard deviation of the gaussian likelihood is small which may even cause filtering failures unfortunately this issue has not been sufficiently addressed in the hydrology community more attention needs to be paid to the likelihood function since the errors of the observed water stage represented by the standard deviation from hydrometric stations are usually small this study focuses on the form of the likelihood function and parameter tuning within the particle weighting procedure of the pf for data assimilation the limitations of using the common glf were clarified and a novel cauchy likelihood function clf was proposed and tested in both a synthetic and a real world case study using a pf based hydraulic model comparisons showed that the clf based pf can effectively relieve the problem of excessive dispersion within the glf based pf framework resulting in more stable and more accurate results these findings not only provide another option for the selection of likelihood function within the pf framework but also offer reliable evidence for improving hydraulic modeling by incorporating high precision observations using the data assimilation technique keywords hydraulic modeling data assimilation particle filter gaussian likelihood function cauchy likelihood function data availability data will be made available on request 1 introduction climate change has generally accelerated the hydrological cycle and increased the frequency of extreme precipitation events which results in more and more intensive flood events worldwide hosseinzadehtalaei et al 2021 lee and kim 2018 li et al 2022 reggiani and weerts 2008 over the past few decades flood event has become one of the most common and widely distributed natural disasters posing a great threat to human lives and properties ren et al 2010 wang et al 2021 zong and xiqing 2000 this highlights an urgent need to obtain exact knowledge of the hydrodynamic process for the early warning and emergency planning of flood events du et al 2019 fang et al 2012 however the numerical modeling of the hydrodynamic process is extremely complex especially in real world scenarios which often contain various uncertainties thus there are still challenges to implement the accurate simulation of the hydrodynamic process the conventional deterministic hydraulic models costabile and macchione 2012 han et al 2011 which are used to simulate the hydrodynamic processes are not that accurate due to the errors caused by the uncertainties within the forcing data and the model parameters wang et al 2022 the emerging data assimilation based da based models are proven effective to reflect the possible uncertainties by using probabilistic theories cloke and pappenberger 2009 han and coulibaly 2017 krzysztofowicz 1999 and provide a more reasonable description of the dynamic system state mathieu and o neill 2008 the main objective of adopting da based models is to get a posterior estimation of the system s current state by updating the simulations based on the information contained in measured data e g data obtained from hydrometric stations satellite aerial based remote sensing techniques dechant and moradkhani 2014 hostache et al 2018 matgen et al 2010 one of the most common and effective approaches for developing da based hydraulic modeling is to employ the kalman filter kf kalman 1960 liu et al 2016 wu et al 2013 as well as its derivative methods i e extended kalman filter ekf agyeman et al 2021 wu et al 2013 unscented kalman filter ukf sun et al 2020 ensemble kalman filter enkf evensen 2003 lei et al 2019 etc all these kf based methods are derived from the simplified form of sequential bayesian filtering however due to the non gaussian nature of the hydrodynamic system the kf and kf derived methods are limited by their gaussian assumption the emerging particle filter pf relieves the restrictive assumptions within the error distribution related to the state space dynamic models meaning the pf can handle the error propagations of any distribution e g the non gaussian distribution through nonlinear models as a result more and more studies were focused on applying pf to hydraulic modeling which is a typical non gaussian and non linear problem cao et al 2019 giustarini et al 2011 hostache et al 2018 matgen et al 2010 the performances resulting from the pf based hydraulic model are greatly affected by factors like the number of particles rafiee et al 2013 the solution for the particle degeneracy xu et al 2018 and the so called sample impoverishment problem han et al 2021 many researchers have paid attention to these problems and carried out considerable studies gordon et al 1993 liu and chen 1998 moradkhani et al 2005a b zhang et al 2013 for example gordon et al 1993 proposed the resampling algorithm to deal with the degeneracy problem moradkhani et al 2005a b suggested solving the sample impoverishment problem by adding gaussian noise to perturb parameters at each time step however the parameter tuning and type selecting for the likelihood function i e the probability density function of the observation for the particle weighting procedure of pf are often ignored in previous studies specifically the calculation of weights involved in the updating step which associates with the error distribution of the observations greatly affects the performance of the pf thus the likelihood function must be chosen with great care as it needs to represent the uncertainties in the observations shiiba et al 2000 the issue of particle weighting related to the likelihoods has been pointed out many times in the field of visual tracking fontmarty et al 2009 mozhdehi and medeiros 2020 and the comparison results between different parameters within the gaussian likelihood framework have been thoroughly discussed unfortunately this issue has not been paid enough attention to in the hydrology community besides previous studies of hydraulic modeling have not determined an optimal likelihood function for observations matgen et al 2010 as a result with the focus on improving the performances of hydraulic models using the pf based data assimilation this study concentrates on revealing the limitations within the generic gaussian likelihood function glf and exploring a different form of the likelihood function i e the cauchy likelihood function clf in this work the findings were illustrated by a synthetic experiment and a real event based case study in hydraulic modeling the rest of this paper is organized as follows in section 2 the framework of the hydraulic model the basic particle filter methods and procedures are introduced after clarifying the limitations of the glf based pf approach in section 3 section 4 proposes an alternative scheme using the clf based pf approach the synthetic experiment for parameter sensitivity analysis and the case study regarding real flood events are detailedly described in section 5 followed by section 6 for the conclusions 2 hydraulic model and data assimilation 2 1 hydraulic model the classic 1 d unsteady open channel flow model chow et al 1988 is employed within the model the unsteady water flow is governed by the saint venant eqs 1 and 2 1 a t q x q l 2 q t x α q 2 a g a z x q q n 2 a r 4 3 0 where a is the cross section area q is the discharge z is the water stage t is time x refers to the distance along the longitudinal axis of the watercourse ql is the lateral discharge per unit channel length α is the momentum flux correction factor g represents the acceleration of gravity r is the hydraulic radius n is the manning s roughness coefficient it s worth noting that n is a coefficient that reflects the comprehensive resistance to the channel flow related to floodplains terrain vegetation coverage and etc since the main focus of this study is not on the construction of the manning s roughness coefficients thus an overall coefficient for each cross section as the nominal roughness of a channel was simply selected the hyperbolic partial differential equations denoted by eqs 1 and 2 were discretized using the preissmann implicit four point finite difference scheme with appropriate initial and boundary conditions castellarin et al 2009 preissmann 1961 this discretization scheme is widely used in previous studies castellarin et al 2009 shiiba et al 2000 xu et al 2018 for its outstanding characteristics of quick calculation and unconditionally steady convergence with the use of this scheme the cross sections of a channel are linked two by two within the current and the following time steps therefore by adding the two boundary conditions a system of 2 n equations for a channel with n cross sections can be developed then this system which can be linearized by the iterative newton raphson method and the taylor formula is resolved by the double sweep algorithm wang et al 2022 2 2 particle filter based data assimilation the pf is an effective method developed based on the monte carlo mc simulation to directly approximate the probability distribution of the system state vectors within the sequential bayesian estimation framework the earliest mc method for statistical inference was proposed by handschin 1970 and the formal pf approach was developed by gordon et al 1993 kitagawa 1996 which is named after the sequential importance resampling sis technique the mc method here is directly used to approximate the full representation of posterior probability distributions of the system state or observation vectors through a group of weighted particles rather than estimating the covariance matrix within the enkf method thus it is suitable for non gaussian and nonlinear bayesian estimation problems with reliable accuracy and applicability the standardized pf approach has two steps prediction and updating this study presents a brief description of the pf as follows and more detailed information can be found in moradkhani et al 2005a b 2 2 1 sequential bayesian filtering solution using the particle filter the original idea of pf comes from the law of large numbers the core is to represent the probability by frequency i e using a group of random particles with weight to estimate the posterior probability distribution of random variables theoretically when the number of samples n the estimated distribution resulting from pf will tend to be close to the real posterior probability distribution that is to say the posterior probability density function of the system state variables i e p x t y 1 t can be approximated by a series of particles as in eq 3 3 p x t y 1 t i 1 n w t k δ x t x t k where δ is the dirac function which is usually used to represent inevitable events according to statistical theories as the dirac function is easy to be integrated by using which the infinite integral calculation in the sequential bayesian filter is transformed into the accumulation of several particles with different weights n refers to the number of particles x t k is the system state variable of the k th particle at the time t w t k refers to the weight of the k th particle at the t th discrete time step and the sum of all the particles weight satisfies i 1 n w t k 1 2 2 2 procedures for pf based hydraulic modeling as mentioned above eq 3 provides a theoretical basis of pf for data assimilation the main procedures for the pf based hydraulic modeling are listed as follows 1 definition of the particle ensemble a dual state parameter particle ensemble is adopted in this work the water stage observations are directly assimilated into the hydraulic model and the roughness coefficient as well as the discharge are corrected indirectly note that p t k refers to the k th particle at the t th discrete time step and the particle ensemble at the t th discrete time step can be described as eqs 4 7 4 p t k q t k z t k n t k k 1 2 3 n p 5 q t k q t 1 k q t 2 k q t i k q t m k 6 z t k z t 1 k z t 2 k z t i k z t m k 7 n t k n t 1 k n t 2 k n t i k n t m k where q t i k z t i k n t i k represent discharge water lever and roughness coefficient of the k th particle in the i th cross sections at the t th discrete time step respectively np and m are respectively the total numbers of particles and cross sections 2 particle ensemble initialization the original particle ensemble i e p 0 k q 0 k z 0 k n 0 k represents the initial distribution of the routing model state q 0 k z 0 k k 1 n p is generated by adding noise extracted from uniform distributions to the initial value of the model state xu et al 2017 n o k k 1 n p is generated by adding the gaussian noise to the initial roughness coefficient of each cross section the weight of each particle is set as 1 np 3 system states prediction each particle at the t 1 th time step in the posterior ensemble is used as the initial state of the hydraulic modeling for the next time step i e the t th one by providing the forcing data at the t th time step the prior particle ensemble of the system states at the t th time step is calculated which contains the information within the hydraulic model 4 particle weighting it is generally regarded as one of the most critical parts of the updating procedure within the pf its main purpose is to obtain particle weight according to the similarity distances between simulated and observed values through a suitable likelihood function if the water stage observations obtained from hydrometric stations are available at the t th time step the weight of each particle can be updated using the likelihood function shown in eq 8 this function is related to the error of the observations and is usually assumed to be glf cao et al 2019 moradkhani et al 2005a b it is worth noting that this assumption is not suitable for every scenario and more theoretical distribution functions need to be further explored matgen et al 2010 8 w t k j w t 1 k j p y t j x t j where w t k j refers to the local likelihood weight of the k th particle at the j th observation at the t th discrete time step x t j and y t j represents the predicted state variables by dynamic models and observed data at the j th observation node at t th discrete time step respectively after this the global weight w t k of the k th particle at the t th discrete time step is calculated as in eq 9 by multiplying all the local likelihood w t k j at different nodes with observations this equation is employed here because it is commonly used in the hydrology community by other researchers and has been proved effective cao et al 2020 hostache et al 2018 xu et al 2018 then the final weight of the k th particle at the t th discrete time step w t k can be derived by normalizing the global weight using eq 10 9 w t k j 1 n o w t k j 10 w t k w t k i 1 k w t k 5 particle resampling after implementing a couple of updating steps the weight of most particles is often negligible with only a few of them having large weights resulting in wasting computing resources this is the so called particle degeneracy phenomenon the degeneracy problem needs to be reduced by the particle resampling practice gordon et al 1993 in this study the multinomial resampling algorithm is employed at each time step to replicate the particles with high weight and eliminate the particles with low weight after resampling the weight of each particle should be assigned to be 1 np before moving to the next step 6 diversity assurance the resampling step is effective to reduce particle degeneracy however it may lead to the loss of diversity when particles with larger weight values are duplicated too many times adding a gaussian noise to perturb parameters after the resampling step is proven effective to avoid the sample impoverishment problem moradkhani et al 2005a b thus the roughness coefficient of each particle is perturbed eq 11 11 n t k n t k ϛ t k ϛ t k n 0 s 2 v a r t n where ζ t k is the zero mean gaussian noise with a variance of s 2 v a r θ for the k th particle at the t th time step va r t θ is the variance of parameters ensemble at the t th time step before resampling s represents the regulation parameter controlling the amplitude of disturbance 7 system states estimation the expectation of the posterior state variable can be estimated by eq 12 according to the assimilated particle ensemble then turn to step 3 and calculate the values of prior state variables at the time t 1 12 x t j k 1 n w t k x t k j where x t j is the estimated value of the state variable at the j th computational node at the t th time x t k j represents the predicted value of the state variable of the k th particle on the j th computational node 3 limitations of using the glf ever since the pf was proposed it is regarded as a common practice to adopt the glf to update particle weight values gordon et al 1993 moradkhani et al 2005a b on account of the glf possesses the advantages of clear physical meaning smyth et al 2019 consistent with most actual scenarios matgen et al 2010 it has been widely used in previous studies cao et al 2019 han et al 2021 xu et al 2017 however it has some problems that limit its uses and further extensions in other application scenarios among them the filtering failure and dispersion problems are often neglected in hydraulic modeling to make it clear the limitations of the glf are clarified from the two aspects of mathematical analysis and exemplification in this part the form of the glf is as in eq 13 13 w t k j 1 2 π σ t exp x t j y t j 2 2 σ t 2 where σ t is the standard deviation of the observations firstly the smaller the value of σ t is the larger the particle weight value with the same similarity distance of x t j y t j however the value of σ t cannot decrease arbitrarily due to the underflow issue this is regarded as a computational problem fontmarty et al 2009 e g exp 1 2 0 5 2 0 01 2 0 which may then result in filter failure by using glf this problem will be more severe in pf based hydraulic modeling on account of the following reasons 1 the nominal accuracy of the observations from the hydrometric station is high i e the standard deviation is small 2 the joint probability density formula i e eq 9 was often employed to assimilate multiple observations from different hydrometric stations in hydraulic models cao et al 2019 specifically the local likelihood weight obtained from multiple assimilation stations will be multiplied to obtain a smaller global weight this global weight of each particle at the t th discrete time step is then normalized by eq 10 hence a denominator that is approximately equal to 0 may result in a null likelihood and lead to filtering failure even if the null likelihood phenomenon doesn t happen when a small standard deviation is used most particles will have weights close to 0 and only very few particles play a dominant role when the slope of the likelihood function is high a slight change in the simulated value of the state variables of a particle will cause significant change in the weight of particle and this will cause excessive dispersion to clarify the dispersion problem we conducted a hypothetic mathematic analysis in this section and further illustrated this through numerical experiments in the case study section in the hypothetic mathematic analysis assume that there are only two effective particles with the values of 0 53 and 0 45 and the observed value is zero fig 1 illustrates the diagram of the gaussian distribution under various standard deviations and the positions of the assumed particles are also indicated by quadrangular stars table 1 gives the normalized weights of the two particles and the expectations calculated from them by using different values of the standard deviation the results showed that if the likelihood was supposed to follow the gaussian distribution with the value of σ being 1 the values of the weights p x were 0 347 and 0 361 respectively and their respective normalized weights were 0 493 and 0 507 thus the estimated value based on these two particles was 0 033 which is very close to the observed value when the σ is 0 6 the normalized weights changed to 0 450 and 0 502 respectively and the estimated value based on the two samples was 0 013 indicating the fact that by reducing the σ the accuracy of the posterior estimate can be greatly improved however when the σ drops to 0 2 the weights changed to 0 06 and 0 159 respectively and their respective normalized weights were 0 273 and 0 727 the estimated value based on the two particles was 0 182 which is far from the observed value and even greater than the case of σ 1 the high estimated error is mainly attributed to the steep gradient of gaussian distribution around standard deviation this extreme example explains that it is because of the dispersion problem better results cannot be achieved by simply reducing the value of the standard deviation in general the glf based model has the potential to achieve better filtering performance by reducing the value of standard deviation i e adopting observations with high precision in some cases however when the value of the standard deviation in the likelihood function is too small or the simulated particles deviate a lot from the observed ones the excessive dispersion or violation of computational limitation is likely to happen and in that case the glf based model performs poor or even fails 4 alternative of the cauchy likelihood 4 1 the cauchy likelihood this study proposes to use the clf eq 14 to replace the glf in the pf framework and improve the filtering performance the long tailed characteristic of the cauchy distribution is expected to solve the above mentioned problems 14 w t k j 1 π γ t 1 x t j y t j γ t 2 1 where γ t represents a scale parameter that specifies the half width at half maximum and is sometimes called the probable error the gaussian and cauchy distributions both belong to a family of student s t distribution student 1908 and are both symmetric about the central value the main benefit of the cauchy likelihood lies in it having a fat tail distribution meaning much larger probabilities under extreme conditions liu et al 2012 thus the computational limitation and dispersion problems within the glf based pf framework can be relieved 4 2 determination of the scale parameter in clf in the clf there is a scale parameter that needs to be determined in practice the σ in glf has a clear physical meaning which can be determined according to the error of the observation value the scale parameter can be determined according to the standard deviation by assuming that the two distributions share the same peak probability at the observed value x 0 a conversion relationship between σ and γ is established as in eq 15 15 1 π γ 1 0 2 1 1 2 π σ exp 0 1 2 π σ 1 π γ γ 2 π σ 0 8 σ supposing the value of σ is equal to 1 and the corresponding value of γ is 0 8 fig 2 demonstrates the diagram of two distributions with the same peak using corresponding parameters it can be seen that the profiles of the two distributions are very similar the cumulative probability of the particles within the range 3 σ 3 σ is 99 7 for glf and the corresponding value obtained from using the clf is 91 7 this indicates that the cumulative probability value by using clf is similar to that of gaussian cases under the assumption of an equal peak furthermore the particles beyond this range play a negligible role no matter which likelihood function is used this figure also indicates the clf premise leads to larger likelihood values when the simulated x is far from the observed value x 0 specifically in extreme situations a certain weight value can still be obtained by using the clf the aforementioned computational problem will not happen and thus the operation of the filtering algorithm can be ensured that is to say the clf is capable to obtain a better filtering performance when the corresponding σ is small to make it clear a similar analysis as conducted in section 3 is also presented here as mentioned above this study suggests that the performances from the use of glf and clf can be comparable under the condition of the same peak probability as the cumulative probability is also similar in that case fig 3 illustrates the diagram of cauchy distribution under various scale parameters and the positions of the two particles shown in section 3 are also indicated by quadrangular stars the corresponding scale parameters are 0 8 0 48 and 0 16 and table 2 gives the results by using clf when the standard deviation is 1 the corresponding scale parameter is 0 8 and the normalized weight calculated using clf were 0 478 and 0 522 respectively then the estimated value was 0 018 and the estimated value was better than that of the glf the results of the estimated value under the two distributions were similar when the function parameters were 0 6 and 0 48 respectively when the standard deviation reduced to 0 2 the corresponding scale parameter was 0 16 then the estimated value was 0 032 which is significantly better than the estimated value using the glf under the same condition this example illustrates the better performance of clf when the standard deviation and the corresponding scale parameter are small it is worth noting that the dispersion problem also occurs when the scale parameter is small by using the clf but it is much better than using the glf this can be explained that when the slope of the cauchy likelihood function is high the weight of a particle changes significantly with the state value but as more particles are of non negligible weight compared to using the glf the dispersion is thus relieved 5 case study in this section both a synthetic and a real world case are employed to further illustrate the reliability and feasibility of the clf besides the performance of glf and that of clf are compared preliminary experiments showed that there is only a slight improvement in the simulation accuracy for the cases in this study when the number of particles is above 200 hence considering the tradeoff between computational costs and accuracy the number of the particles used in the following experiments was determined as 200 it is acknowledged that with the use of more particles the accuracy of posterior estimation can be improved however the greater the particle number the lower the computational efficiency in previous studies the particle numbers were generally selected in the range of 50 500 in pf based models cao et al 2019 gordon et al 1993 matgen et al 2010 xu et al 2017 magnusson et al 2017 suggested that at least 100 particles are required to stabilize the performance of the pf based model and rafiee et al 2013 proposed that increasing the number of particles only has slight influence on the final outputs the nash sutcliffe efficiency nse and the root mean square error rmse were employed to quantitatively evaluate the performance of the pf based hydraulic model the nse is dimensionless and its perfect value for a simulated process is 1 the unit of rmse is the same as the system variables of the model and its perfect value is 0 their calculation is as in eqs 16 and 17 16 nse 1 k 1 m sim k o b s k 2 k 1 m obs k obs 2 17 rmse 1 m t 1 m sim t o b s t 2 where obs refers to the average value of observations during simulation m refers to the number of measured values within the simulation period obs k is the k th measured value sim k is the expected value of the particle ensemble at the time step corresponding to obs k 5 1 synthetic experiment a synthetic experiment is conducted to investigate the differences resulted from the use of glf and clf the dispersion problem by using the glf was illustrated through the results of various standard deviation cases the effectiveness of the clf is also demonstrated by using various corresponding scale parameters 5 1 1 study area and model configuration the study area is located in the middle part of the yangtze river basin in china to be more specific it is the upstream river reach adjacent to the well known three gorges dam tgd fig 4 shows the location of the study area and the distribution of hydrometric stations the total length of the reach is 604 km and along the reach there are 6 hydrometric stations including the cuntan ct changshou cs qingxichang qxc zhongxian zx wanxian wx and fenjie fj hydrometric stations and 17 tributaries flowing into the reach the average bed slope of the reach is about 0 22 the study reach is conceptualized by the hydraulic model described above in section 2 1 the study reach has been divided into 295 sub reaches with a spatial increment of about 2 km the upstream boundary condition is specified as a discharge hydrograph at the ct station and the water stage at the tgd serves as the downstream boundary condition considering the spatial variability of roughness the study channel was divided into six segments with different values of roughness coefficient and the roughness coefficient of all the sub reaches within the same segment was assumed to be the same the mean and standard deviation of the initial roughness coefficient ensemble of each segment was set as shown in table 3 the standard deviation of the initial distribution is assigned as 10 of the mean values more detailed explanations for the initial value of manning s roughness coefficient topography data and man made structures in the study reach can be found in wu et al 2013 5 1 2 experiment design the synthetic experiment mainly includes the following four steps and more details regarding the synthetic experiment can be found in xu et al 2017 1 generate the true value it is assumed that the model is perfect and the roughness coefficient of the cross section follows the same linear relationship denoted by eq 18 with water depth as used by xu et al 2017 the model was driven by forcing data from a real flood that occurred in the study area from 00 00 on 27 june to 00 00 on 9 july the result of this simulation is regarded as the true value as a result a dataset including the water stage discharge and roughness coefficient of each computational node was generated 18 n 1 0 0014 h 1 0 0114 n 2 0 0013 h 2 0 0075 n 3 0 0014 h 3 0 0457 n 4 0 0012 h 4 0 0225 n 5 0 0020 h 5 0 1340 n 6 0 0017 h 6 0 0600 where n represents the roughness coefficient of the i th sub reach h refers to the water depth of i th sub reach 2 generate the observed value noise with normal distribution was added to the true values of the water stage data to generate observed data different sets of observed data are generated in accordance with the applied standard deviations in various cases fig 5 illustrates an example of observed stage hydrograph with a zero mean and 0 05 m standard deviation 3 pf based data assimilation the 200 particles are created and updated step by step using the algorithm described in the previous sections the boundary conditions used to feed the models were the same as in step 1 and the observed data were sequentially assimilated into the models 4 evaluation of the performance the assimilated results from the previous step were compared with the true values to evaluate the performance of the pf based data assimilation method it is acknowledged that the value of the standard deviation is crucial to the performance of the function in theory the higher is the accuracy of the observed value the smaller is the value of the standard deviation and better filtering performance can be obtained however as mentioned above the standard deviation cannot be arbitrarily reduced due to the computational limitations and the dispersion phenomenon in previous studies han et al 2021 rafiee et al 2013 xu et al 2018 the value of the standard deviation for gaussian distribution was mostly between 0 05 m and 0 1 m considering the high precision of the in situ hydrological gauge and further development of the observation techniques annis et al 2022 pappenberger et al 2006 schmidt 2002 values ranging from 0 01 m to 0 1 m was analyzed in this section 5 1 3 results and discussion fig 6 demonstrates the results when the glf was used the range of the value of the standard deviation is 0 01 m 0 1 m results show that the value of the standard deviation in the range has an obvious effect on the value of the rmses while the effect on the nse values is negligible in this range the nses at all the assimilation stations were greater than 0 96 and all the rmses were less than 0 1 m this illustrated the effective filtering performance of the glf based pf algorithm however the simulated rmses showed a growth trend with a decrease of the standard deviation when it is less than 0 03 m due to the aforementioned excessive dispersion problem in fact we also tested the standard deviation value of 0 008 m results not shown and found the weights of some particles were nan not a number and the filtering totally failed with the accumulated value of the rmses significantly increasing to 13 26 m and the nses decreasing to 9 88 fig 7 demonstrates the results when the clf was used the range of the value of the scale parameter is determined according to that of the standard deviation using eq 15 and it is 0 008 m 0 08 m results showed that the value of the scale parameter in the range also has an obvious effect on the value of the rmses while the effect on the nse values is negligible too the clf based pf algorithm was also very effective with all the nses greater than 0 96 and all the rmses less than 0 11 m the results of the gaussian and cauchy cases were comparable when the scale parameter was greater than 0 024 m the corresponding standard deviation was greater than 0 03 m furthermore the dispersion didn t happen in this case as the rmses kept decreasing with the decrease of the scale parameter this may be due to the fact that the only uncertainty considered in this synthetic experiment is due to the uncertainty of the roughness coefficient and the observed data is closer to the true value when the scale parameter gets smaller in real world cases there are many kinds of uncertainties e g the uncertainties due to inaccuracy of boundary conditions etc 5 2 real world case it is necessary to further compare the filtering performance of the clf based pf with the glf based pf in real world cases the main reason is that the only uncertainty within the above synthetic experiment is supposed to be due to the inaccuracy of the roughness coefficient however in real world cases other uncertainties like that in the boundary conditions are also significant these uncertainties are more likely to lead to instability of filtering or even filtering failure therefore the performance of the two types of likelihood functions under real world scenarios are explored in this sub section 5 2 1 data and settings the same study area and period as in the synthetic experiment are employed to investigate the real world case the observed discharge data at the ct station and the water stage data in front of the tgd from june 27 to july 9 2009 are set as the boundary conditions the in situ measured water stage data from the six hydrometric stations are employed for data assimilation 5 2 2 results and discussion different values of the standard deviation and the corresponding values of the scale parameter were tested to evaluate the two kinds of likelihood functions in practice the standard deviation is usually set based on experience or statistical information the same study area and hydrodynamic event were also studied by xu et al 2017 and the standard deviation of the observed water stage data in that study was assumed to be 0 1 m in fact the error of observed water stage at hydrometric stations should be much lower than 0 1 m which was reported to be possibly as small as 0 02 m annis et al 2022 pappenberger et al 2006 schmidt 2002 as discussed earlier when the standard deviation is too small the filtering may even fail in this real world case when the standard deviation is equal to or smaller than 0 02 m we found that the glf based pf fail with some of the particle weights being nan after t 50 h in contrast the clf based pf still works when the scale parameter is smaller than the corresponding value of 0 016 m fig 8 demonstrates the results when the glf was used and the range of the value of the standard deviation is 0 03 m 0 1 m as mentioned above when the standard deviation is equal to or smaller than 0 02 m the glf based filter failed and the results were not shown in the figure it is worth noting that the failure did not occur in the synthetic case this is because of the fact that in the synthetic experiment the observed data used for assimilation are different in accordance with the applied observation error in various cases which means the observations are more exact i e closer to the true values when the standard error is small in contrast the errors of the observations in this real world case are fixed it can be seen that all the nses in the figure are large and all the rmses are small and this further proves the effectiveness of the filter at the meantime it can be seen that the results at the three stations in the downstream half are not as good as those in the upstream half and this may be attributed to the reason that the observed water stage at the downstream half are incompatible with the downstream boundary condition of water stage the figure also shows that the value of the standard deviation has an obvious effect on the value of the rmses while the effect on the nse values is negligible furthermore the figure shows that due to the dispersion problem the performance of the filter deteriorates when the standard deviation is smaller than 0 06 m and this can cause failure of the filter when the standard deviation is equal to or smaller than 0 02 m fig 9 demonstrates the results when the clf was used and the range of the scale parameter is 0 008 m 0 08 m according to eq 14 the corresponding value of the standard deviation if the glf were used is 0 01 m 0 1 m the figure shows that the performance of the clf based pf also deteriorates when the scale parameter is smaller than 0 032 m corresponding to a standard deviation of 0 04 m but the performance is much better than the glf based pf it is worth noting that the dispersion with the small scale parameter was not obvious in the results of the clf based pf in the synthetic case this is also because of the fact that in the synthetic experiment the observations are more exact when the applied scale parameter is small this phenomenon indicates that the clf based pf has the potential to further improve the model performance if high precision observations are available when the scale parameter is greater than 0 04 m the nses and rmses are comparable to those of the corresponding glf based results when the scale parameter is less than 0 04 m the performance is slightly better if both the pfs are working and most importantly the clf based pf works in the whole range of the scale parameter while the glf based pf fails when the standard deviation is equal to or smaller than 0 02 m this means the clf can effectively relieve the excessive dispersion and filtering failure of the glf to take a closer examination on the results fig 10 illustrates the simulated results using the two types of likelihood functions with the values of σ and γ being respectively 0 03 m and 0 024 m compared with the open loop case both the glf based and clf based pf can improve the simulation results the results at the three stations in the upstream half of the reach ct cs and qxc are similar for the two filters however the results using the glf based filter contain some fluctuations during the peak water stage hours at the three stations zx wx and fj in the downstream half this demonstrates the excessive dispersion caused by the glf based pf and demonstrates that the clf based pf can effectively relieve this problem besides it should be noted that the fluctuations always happen when at peak stages the main reason is that the discrepancy between simulations and observations are large at the peak stages this large discrepancy is more likely to lead to significant dispersion 6 conclusion in this study we illustrated the limitations of excessive dispersion and filtering failure of the glf within the pf data assimilation framework in hydraulic modeling to overcome the limitations a novel particle weighting scheme using the clf was proposed and evaluated the clf based pf data assimilation was tested and compared with the glf based pf through a synthetic experiment and a real world case results confirmed that if the standard deviation is too small or the simulated value deviated too far from the measured value the glf based pf performs poorly because of the excessive dispersion problem and even totally fails the newly proposed clf is an effective approach to overcome the limitations attributed to the fat tail feature of the cauchy distribution and this results in higher stability and better results in the future the clf based pf will be further tested in other scenarios e g in a complex river network in addition its performance in flood forecasting is also planned to be studied credit authorship contribution statement chenhui jiang writing original draft methodology software validation formal analysis visualization writing review editing dejun zhu conceptualization methodology writing review editing supervision project administration funding acquisition haobo li writing review editing formal analysis investigation xingya xu data curation software danxun li resources supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the study is financially supported by the national natural science foundation of china grant number 52179069 and the water conservancy and technology program of hunan province china grant number xskj2021000 08 the authors would like to thank the changjiang water resources commission of the ministry of water resources for providing highly precise water stage data from hydrometric stations 
2434,over the recent decades particle filter pf based data assimilation has been adopted to improve hydrodynamic simulation due to its wide applicability and effectiveness for nonlinear and non gaussian models the particle weighting is the core procedure to combine the observations and simulations within the pf framework in which the gaussian likelihood function glf is commonly used to update weight values however there are some widely acknowledged challenges within the glf based pf among them the excessive dispersion problem occurs when the standard deviation of the gaussian likelihood is small which may even cause filtering failures unfortunately this issue has not been sufficiently addressed in the hydrology community more attention needs to be paid to the likelihood function since the errors of the observed water stage represented by the standard deviation from hydrometric stations are usually small this study focuses on the form of the likelihood function and parameter tuning within the particle weighting procedure of the pf for data assimilation the limitations of using the common glf were clarified and a novel cauchy likelihood function clf was proposed and tested in both a synthetic and a real world case study using a pf based hydraulic model comparisons showed that the clf based pf can effectively relieve the problem of excessive dispersion within the glf based pf framework resulting in more stable and more accurate results these findings not only provide another option for the selection of likelihood function within the pf framework but also offer reliable evidence for improving hydraulic modeling by incorporating high precision observations using the data assimilation technique keywords hydraulic modeling data assimilation particle filter gaussian likelihood function cauchy likelihood function data availability data will be made available on request 1 introduction climate change has generally accelerated the hydrological cycle and increased the frequency of extreme precipitation events which results in more and more intensive flood events worldwide hosseinzadehtalaei et al 2021 lee and kim 2018 li et al 2022 reggiani and weerts 2008 over the past few decades flood event has become one of the most common and widely distributed natural disasters posing a great threat to human lives and properties ren et al 2010 wang et al 2021 zong and xiqing 2000 this highlights an urgent need to obtain exact knowledge of the hydrodynamic process for the early warning and emergency planning of flood events du et al 2019 fang et al 2012 however the numerical modeling of the hydrodynamic process is extremely complex especially in real world scenarios which often contain various uncertainties thus there are still challenges to implement the accurate simulation of the hydrodynamic process the conventional deterministic hydraulic models costabile and macchione 2012 han et al 2011 which are used to simulate the hydrodynamic processes are not that accurate due to the errors caused by the uncertainties within the forcing data and the model parameters wang et al 2022 the emerging data assimilation based da based models are proven effective to reflect the possible uncertainties by using probabilistic theories cloke and pappenberger 2009 han and coulibaly 2017 krzysztofowicz 1999 and provide a more reasonable description of the dynamic system state mathieu and o neill 2008 the main objective of adopting da based models is to get a posterior estimation of the system s current state by updating the simulations based on the information contained in measured data e g data obtained from hydrometric stations satellite aerial based remote sensing techniques dechant and moradkhani 2014 hostache et al 2018 matgen et al 2010 one of the most common and effective approaches for developing da based hydraulic modeling is to employ the kalman filter kf kalman 1960 liu et al 2016 wu et al 2013 as well as its derivative methods i e extended kalman filter ekf agyeman et al 2021 wu et al 2013 unscented kalman filter ukf sun et al 2020 ensemble kalman filter enkf evensen 2003 lei et al 2019 etc all these kf based methods are derived from the simplified form of sequential bayesian filtering however due to the non gaussian nature of the hydrodynamic system the kf and kf derived methods are limited by their gaussian assumption the emerging particle filter pf relieves the restrictive assumptions within the error distribution related to the state space dynamic models meaning the pf can handle the error propagations of any distribution e g the non gaussian distribution through nonlinear models as a result more and more studies were focused on applying pf to hydraulic modeling which is a typical non gaussian and non linear problem cao et al 2019 giustarini et al 2011 hostache et al 2018 matgen et al 2010 the performances resulting from the pf based hydraulic model are greatly affected by factors like the number of particles rafiee et al 2013 the solution for the particle degeneracy xu et al 2018 and the so called sample impoverishment problem han et al 2021 many researchers have paid attention to these problems and carried out considerable studies gordon et al 1993 liu and chen 1998 moradkhani et al 2005a b zhang et al 2013 for example gordon et al 1993 proposed the resampling algorithm to deal with the degeneracy problem moradkhani et al 2005a b suggested solving the sample impoverishment problem by adding gaussian noise to perturb parameters at each time step however the parameter tuning and type selecting for the likelihood function i e the probability density function of the observation for the particle weighting procedure of pf are often ignored in previous studies specifically the calculation of weights involved in the updating step which associates with the error distribution of the observations greatly affects the performance of the pf thus the likelihood function must be chosen with great care as it needs to represent the uncertainties in the observations shiiba et al 2000 the issue of particle weighting related to the likelihoods has been pointed out many times in the field of visual tracking fontmarty et al 2009 mozhdehi and medeiros 2020 and the comparison results between different parameters within the gaussian likelihood framework have been thoroughly discussed unfortunately this issue has not been paid enough attention to in the hydrology community besides previous studies of hydraulic modeling have not determined an optimal likelihood function for observations matgen et al 2010 as a result with the focus on improving the performances of hydraulic models using the pf based data assimilation this study concentrates on revealing the limitations within the generic gaussian likelihood function glf and exploring a different form of the likelihood function i e the cauchy likelihood function clf in this work the findings were illustrated by a synthetic experiment and a real event based case study in hydraulic modeling the rest of this paper is organized as follows in section 2 the framework of the hydraulic model the basic particle filter methods and procedures are introduced after clarifying the limitations of the glf based pf approach in section 3 section 4 proposes an alternative scheme using the clf based pf approach the synthetic experiment for parameter sensitivity analysis and the case study regarding real flood events are detailedly described in section 5 followed by section 6 for the conclusions 2 hydraulic model and data assimilation 2 1 hydraulic model the classic 1 d unsteady open channel flow model chow et al 1988 is employed within the model the unsteady water flow is governed by the saint venant eqs 1 and 2 1 a t q x q l 2 q t x α q 2 a g a z x q q n 2 a r 4 3 0 where a is the cross section area q is the discharge z is the water stage t is time x refers to the distance along the longitudinal axis of the watercourse ql is the lateral discharge per unit channel length α is the momentum flux correction factor g represents the acceleration of gravity r is the hydraulic radius n is the manning s roughness coefficient it s worth noting that n is a coefficient that reflects the comprehensive resistance to the channel flow related to floodplains terrain vegetation coverage and etc since the main focus of this study is not on the construction of the manning s roughness coefficients thus an overall coefficient for each cross section as the nominal roughness of a channel was simply selected the hyperbolic partial differential equations denoted by eqs 1 and 2 were discretized using the preissmann implicit four point finite difference scheme with appropriate initial and boundary conditions castellarin et al 2009 preissmann 1961 this discretization scheme is widely used in previous studies castellarin et al 2009 shiiba et al 2000 xu et al 2018 for its outstanding characteristics of quick calculation and unconditionally steady convergence with the use of this scheme the cross sections of a channel are linked two by two within the current and the following time steps therefore by adding the two boundary conditions a system of 2 n equations for a channel with n cross sections can be developed then this system which can be linearized by the iterative newton raphson method and the taylor formula is resolved by the double sweep algorithm wang et al 2022 2 2 particle filter based data assimilation the pf is an effective method developed based on the monte carlo mc simulation to directly approximate the probability distribution of the system state vectors within the sequential bayesian estimation framework the earliest mc method for statistical inference was proposed by handschin 1970 and the formal pf approach was developed by gordon et al 1993 kitagawa 1996 which is named after the sequential importance resampling sis technique the mc method here is directly used to approximate the full representation of posterior probability distributions of the system state or observation vectors through a group of weighted particles rather than estimating the covariance matrix within the enkf method thus it is suitable for non gaussian and nonlinear bayesian estimation problems with reliable accuracy and applicability the standardized pf approach has two steps prediction and updating this study presents a brief description of the pf as follows and more detailed information can be found in moradkhani et al 2005a b 2 2 1 sequential bayesian filtering solution using the particle filter the original idea of pf comes from the law of large numbers the core is to represent the probability by frequency i e using a group of random particles with weight to estimate the posterior probability distribution of random variables theoretically when the number of samples n the estimated distribution resulting from pf will tend to be close to the real posterior probability distribution that is to say the posterior probability density function of the system state variables i e p x t y 1 t can be approximated by a series of particles as in eq 3 3 p x t y 1 t i 1 n w t k δ x t x t k where δ is the dirac function which is usually used to represent inevitable events according to statistical theories as the dirac function is easy to be integrated by using which the infinite integral calculation in the sequential bayesian filter is transformed into the accumulation of several particles with different weights n refers to the number of particles x t k is the system state variable of the k th particle at the time t w t k refers to the weight of the k th particle at the t th discrete time step and the sum of all the particles weight satisfies i 1 n w t k 1 2 2 2 procedures for pf based hydraulic modeling as mentioned above eq 3 provides a theoretical basis of pf for data assimilation the main procedures for the pf based hydraulic modeling are listed as follows 1 definition of the particle ensemble a dual state parameter particle ensemble is adopted in this work the water stage observations are directly assimilated into the hydraulic model and the roughness coefficient as well as the discharge are corrected indirectly note that p t k refers to the k th particle at the t th discrete time step and the particle ensemble at the t th discrete time step can be described as eqs 4 7 4 p t k q t k z t k n t k k 1 2 3 n p 5 q t k q t 1 k q t 2 k q t i k q t m k 6 z t k z t 1 k z t 2 k z t i k z t m k 7 n t k n t 1 k n t 2 k n t i k n t m k where q t i k z t i k n t i k represent discharge water lever and roughness coefficient of the k th particle in the i th cross sections at the t th discrete time step respectively np and m are respectively the total numbers of particles and cross sections 2 particle ensemble initialization the original particle ensemble i e p 0 k q 0 k z 0 k n 0 k represents the initial distribution of the routing model state q 0 k z 0 k k 1 n p is generated by adding noise extracted from uniform distributions to the initial value of the model state xu et al 2017 n o k k 1 n p is generated by adding the gaussian noise to the initial roughness coefficient of each cross section the weight of each particle is set as 1 np 3 system states prediction each particle at the t 1 th time step in the posterior ensemble is used as the initial state of the hydraulic modeling for the next time step i e the t th one by providing the forcing data at the t th time step the prior particle ensemble of the system states at the t th time step is calculated which contains the information within the hydraulic model 4 particle weighting it is generally regarded as one of the most critical parts of the updating procedure within the pf its main purpose is to obtain particle weight according to the similarity distances between simulated and observed values through a suitable likelihood function if the water stage observations obtained from hydrometric stations are available at the t th time step the weight of each particle can be updated using the likelihood function shown in eq 8 this function is related to the error of the observations and is usually assumed to be glf cao et al 2019 moradkhani et al 2005a b it is worth noting that this assumption is not suitable for every scenario and more theoretical distribution functions need to be further explored matgen et al 2010 8 w t k j w t 1 k j p y t j x t j where w t k j refers to the local likelihood weight of the k th particle at the j th observation at the t th discrete time step x t j and y t j represents the predicted state variables by dynamic models and observed data at the j th observation node at t th discrete time step respectively after this the global weight w t k of the k th particle at the t th discrete time step is calculated as in eq 9 by multiplying all the local likelihood w t k j at different nodes with observations this equation is employed here because it is commonly used in the hydrology community by other researchers and has been proved effective cao et al 2020 hostache et al 2018 xu et al 2018 then the final weight of the k th particle at the t th discrete time step w t k can be derived by normalizing the global weight using eq 10 9 w t k j 1 n o w t k j 10 w t k w t k i 1 k w t k 5 particle resampling after implementing a couple of updating steps the weight of most particles is often negligible with only a few of them having large weights resulting in wasting computing resources this is the so called particle degeneracy phenomenon the degeneracy problem needs to be reduced by the particle resampling practice gordon et al 1993 in this study the multinomial resampling algorithm is employed at each time step to replicate the particles with high weight and eliminate the particles with low weight after resampling the weight of each particle should be assigned to be 1 np before moving to the next step 6 diversity assurance the resampling step is effective to reduce particle degeneracy however it may lead to the loss of diversity when particles with larger weight values are duplicated too many times adding a gaussian noise to perturb parameters after the resampling step is proven effective to avoid the sample impoverishment problem moradkhani et al 2005a b thus the roughness coefficient of each particle is perturbed eq 11 11 n t k n t k ϛ t k ϛ t k n 0 s 2 v a r t n where ζ t k is the zero mean gaussian noise with a variance of s 2 v a r θ for the k th particle at the t th time step va r t θ is the variance of parameters ensemble at the t th time step before resampling s represents the regulation parameter controlling the amplitude of disturbance 7 system states estimation the expectation of the posterior state variable can be estimated by eq 12 according to the assimilated particle ensemble then turn to step 3 and calculate the values of prior state variables at the time t 1 12 x t j k 1 n w t k x t k j where x t j is the estimated value of the state variable at the j th computational node at the t th time x t k j represents the predicted value of the state variable of the k th particle on the j th computational node 3 limitations of using the glf ever since the pf was proposed it is regarded as a common practice to adopt the glf to update particle weight values gordon et al 1993 moradkhani et al 2005a b on account of the glf possesses the advantages of clear physical meaning smyth et al 2019 consistent with most actual scenarios matgen et al 2010 it has been widely used in previous studies cao et al 2019 han et al 2021 xu et al 2017 however it has some problems that limit its uses and further extensions in other application scenarios among them the filtering failure and dispersion problems are often neglected in hydraulic modeling to make it clear the limitations of the glf are clarified from the two aspects of mathematical analysis and exemplification in this part the form of the glf is as in eq 13 13 w t k j 1 2 π σ t exp x t j y t j 2 2 σ t 2 where σ t is the standard deviation of the observations firstly the smaller the value of σ t is the larger the particle weight value with the same similarity distance of x t j y t j however the value of σ t cannot decrease arbitrarily due to the underflow issue this is regarded as a computational problem fontmarty et al 2009 e g exp 1 2 0 5 2 0 01 2 0 which may then result in filter failure by using glf this problem will be more severe in pf based hydraulic modeling on account of the following reasons 1 the nominal accuracy of the observations from the hydrometric station is high i e the standard deviation is small 2 the joint probability density formula i e eq 9 was often employed to assimilate multiple observations from different hydrometric stations in hydraulic models cao et al 2019 specifically the local likelihood weight obtained from multiple assimilation stations will be multiplied to obtain a smaller global weight this global weight of each particle at the t th discrete time step is then normalized by eq 10 hence a denominator that is approximately equal to 0 may result in a null likelihood and lead to filtering failure even if the null likelihood phenomenon doesn t happen when a small standard deviation is used most particles will have weights close to 0 and only very few particles play a dominant role when the slope of the likelihood function is high a slight change in the simulated value of the state variables of a particle will cause significant change in the weight of particle and this will cause excessive dispersion to clarify the dispersion problem we conducted a hypothetic mathematic analysis in this section and further illustrated this through numerical experiments in the case study section in the hypothetic mathematic analysis assume that there are only two effective particles with the values of 0 53 and 0 45 and the observed value is zero fig 1 illustrates the diagram of the gaussian distribution under various standard deviations and the positions of the assumed particles are also indicated by quadrangular stars table 1 gives the normalized weights of the two particles and the expectations calculated from them by using different values of the standard deviation the results showed that if the likelihood was supposed to follow the gaussian distribution with the value of σ being 1 the values of the weights p x were 0 347 and 0 361 respectively and their respective normalized weights were 0 493 and 0 507 thus the estimated value based on these two particles was 0 033 which is very close to the observed value when the σ is 0 6 the normalized weights changed to 0 450 and 0 502 respectively and the estimated value based on the two samples was 0 013 indicating the fact that by reducing the σ the accuracy of the posterior estimate can be greatly improved however when the σ drops to 0 2 the weights changed to 0 06 and 0 159 respectively and their respective normalized weights were 0 273 and 0 727 the estimated value based on the two particles was 0 182 which is far from the observed value and even greater than the case of σ 1 the high estimated error is mainly attributed to the steep gradient of gaussian distribution around standard deviation this extreme example explains that it is because of the dispersion problem better results cannot be achieved by simply reducing the value of the standard deviation in general the glf based model has the potential to achieve better filtering performance by reducing the value of standard deviation i e adopting observations with high precision in some cases however when the value of the standard deviation in the likelihood function is too small or the simulated particles deviate a lot from the observed ones the excessive dispersion or violation of computational limitation is likely to happen and in that case the glf based model performs poor or even fails 4 alternative of the cauchy likelihood 4 1 the cauchy likelihood this study proposes to use the clf eq 14 to replace the glf in the pf framework and improve the filtering performance the long tailed characteristic of the cauchy distribution is expected to solve the above mentioned problems 14 w t k j 1 π γ t 1 x t j y t j γ t 2 1 where γ t represents a scale parameter that specifies the half width at half maximum and is sometimes called the probable error the gaussian and cauchy distributions both belong to a family of student s t distribution student 1908 and are both symmetric about the central value the main benefit of the cauchy likelihood lies in it having a fat tail distribution meaning much larger probabilities under extreme conditions liu et al 2012 thus the computational limitation and dispersion problems within the glf based pf framework can be relieved 4 2 determination of the scale parameter in clf in the clf there is a scale parameter that needs to be determined in practice the σ in glf has a clear physical meaning which can be determined according to the error of the observation value the scale parameter can be determined according to the standard deviation by assuming that the two distributions share the same peak probability at the observed value x 0 a conversion relationship between σ and γ is established as in eq 15 15 1 π γ 1 0 2 1 1 2 π σ exp 0 1 2 π σ 1 π γ γ 2 π σ 0 8 σ supposing the value of σ is equal to 1 and the corresponding value of γ is 0 8 fig 2 demonstrates the diagram of two distributions with the same peak using corresponding parameters it can be seen that the profiles of the two distributions are very similar the cumulative probability of the particles within the range 3 σ 3 σ is 99 7 for glf and the corresponding value obtained from using the clf is 91 7 this indicates that the cumulative probability value by using clf is similar to that of gaussian cases under the assumption of an equal peak furthermore the particles beyond this range play a negligible role no matter which likelihood function is used this figure also indicates the clf premise leads to larger likelihood values when the simulated x is far from the observed value x 0 specifically in extreme situations a certain weight value can still be obtained by using the clf the aforementioned computational problem will not happen and thus the operation of the filtering algorithm can be ensured that is to say the clf is capable to obtain a better filtering performance when the corresponding σ is small to make it clear a similar analysis as conducted in section 3 is also presented here as mentioned above this study suggests that the performances from the use of glf and clf can be comparable under the condition of the same peak probability as the cumulative probability is also similar in that case fig 3 illustrates the diagram of cauchy distribution under various scale parameters and the positions of the two particles shown in section 3 are also indicated by quadrangular stars the corresponding scale parameters are 0 8 0 48 and 0 16 and table 2 gives the results by using clf when the standard deviation is 1 the corresponding scale parameter is 0 8 and the normalized weight calculated using clf were 0 478 and 0 522 respectively then the estimated value was 0 018 and the estimated value was better than that of the glf the results of the estimated value under the two distributions were similar when the function parameters were 0 6 and 0 48 respectively when the standard deviation reduced to 0 2 the corresponding scale parameter was 0 16 then the estimated value was 0 032 which is significantly better than the estimated value using the glf under the same condition this example illustrates the better performance of clf when the standard deviation and the corresponding scale parameter are small it is worth noting that the dispersion problem also occurs when the scale parameter is small by using the clf but it is much better than using the glf this can be explained that when the slope of the cauchy likelihood function is high the weight of a particle changes significantly with the state value but as more particles are of non negligible weight compared to using the glf the dispersion is thus relieved 5 case study in this section both a synthetic and a real world case are employed to further illustrate the reliability and feasibility of the clf besides the performance of glf and that of clf are compared preliminary experiments showed that there is only a slight improvement in the simulation accuracy for the cases in this study when the number of particles is above 200 hence considering the tradeoff between computational costs and accuracy the number of the particles used in the following experiments was determined as 200 it is acknowledged that with the use of more particles the accuracy of posterior estimation can be improved however the greater the particle number the lower the computational efficiency in previous studies the particle numbers were generally selected in the range of 50 500 in pf based models cao et al 2019 gordon et al 1993 matgen et al 2010 xu et al 2017 magnusson et al 2017 suggested that at least 100 particles are required to stabilize the performance of the pf based model and rafiee et al 2013 proposed that increasing the number of particles only has slight influence on the final outputs the nash sutcliffe efficiency nse and the root mean square error rmse were employed to quantitatively evaluate the performance of the pf based hydraulic model the nse is dimensionless and its perfect value for a simulated process is 1 the unit of rmse is the same as the system variables of the model and its perfect value is 0 their calculation is as in eqs 16 and 17 16 nse 1 k 1 m sim k o b s k 2 k 1 m obs k obs 2 17 rmse 1 m t 1 m sim t o b s t 2 where obs refers to the average value of observations during simulation m refers to the number of measured values within the simulation period obs k is the k th measured value sim k is the expected value of the particle ensemble at the time step corresponding to obs k 5 1 synthetic experiment a synthetic experiment is conducted to investigate the differences resulted from the use of glf and clf the dispersion problem by using the glf was illustrated through the results of various standard deviation cases the effectiveness of the clf is also demonstrated by using various corresponding scale parameters 5 1 1 study area and model configuration the study area is located in the middle part of the yangtze river basin in china to be more specific it is the upstream river reach adjacent to the well known three gorges dam tgd fig 4 shows the location of the study area and the distribution of hydrometric stations the total length of the reach is 604 km and along the reach there are 6 hydrometric stations including the cuntan ct changshou cs qingxichang qxc zhongxian zx wanxian wx and fenjie fj hydrometric stations and 17 tributaries flowing into the reach the average bed slope of the reach is about 0 22 the study reach is conceptualized by the hydraulic model described above in section 2 1 the study reach has been divided into 295 sub reaches with a spatial increment of about 2 km the upstream boundary condition is specified as a discharge hydrograph at the ct station and the water stage at the tgd serves as the downstream boundary condition considering the spatial variability of roughness the study channel was divided into six segments with different values of roughness coefficient and the roughness coefficient of all the sub reaches within the same segment was assumed to be the same the mean and standard deviation of the initial roughness coefficient ensemble of each segment was set as shown in table 3 the standard deviation of the initial distribution is assigned as 10 of the mean values more detailed explanations for the initial value of manning s roughness coefficient topography data and man made structures in the study reach can be found in wu et al 2013 5 1 2 experiment design the synthetic experiment mainly includes the following four steps and more details regarding the synthetic experiment can be found in xu et al 2017 1 generate the true value it is assumed that the model is perfect and the roughness coefficient of the cross section follows the same linear relationship denoted by eq 18 with water depth as used by xu et al 2017 the model was driven by forcing data from a real flood that occurred in the study area from 00 00 on 27 june to 00 00 on 9 july the result of this simulation is regarded as the true value as a result a dataset including the water stage discharge and roughness coefficient of each computational node was generated 18 n 1 0 0014 h 1 0 0114 n 2 0 0013 h 2 0 0075 n 3 0 0014 h 3 0 0457 n 4 0 0012 h 4 0 0225 n 5 0 0020 h 5 0 1340 n 6 0 0017 h 6 0 0600 where n represents the roughness coefficient of the i th sub reach h refers to the water depth of i th sub reach 2 generate the observed value noise with normal distribution was added to the true values of the water stage data to generate observed data different sets of observed data are generated in accordance with the applied standard deviations in various cases fig 5 illustrates an example of observed stage hydrograph with a zero mean and 0 05 m standard deviation 3 pf based data assimilation the 200 particles are created and updated step by step using the algorithm described in the previous sections the boundary conditions used to feed the models were the same as in step 1 and the observed data were sequentially assimilated into the models 4 evaluation of the performance the assimilated results from the previous step were compared with the true values to evaluate the performance of the pf based data assimilation method it is acknowledged that the value of the standard deviation is crucial to the performance of the function in theory the higher is the accuracy of the observed value the smaller is the value of the standard deviation and better filtering performance can be obtained however as mentioned above the standard deviation cannot be arbitrarily reduced due to the computational limitations and the dispersion phenomenon in previous studies han et al 2021 rafiee et al 2013 xu et al 2018 the value of the standard deviation for gaussian distribution was mostly between 0 05 m and 0 1 m considering the high precision of the in situ hydrological gauge and further development of the observation techniques annis et al 2022 pappenberger et al 2006 schmidt 2002 values ranging from 0 01 m to 0 1 m was analyzed in this section 5 1 3 results and discussion fig 6 demonstrates the results when the glf was used the range of the value of the standard deviation is 0 01 m 0 1 m results show that the value of the standard deviation in the range has an obvious effect on the value of the rmses while the effect on the nse values is negligible in this range the nses at all the assimilation stations were greater than 0 96 and all the rmses were less than 0 1 m this illustrated the effective filtering performance of the glf based pf algorithm however the simulated rmses showed a growth trend with a decrease of the standard deviation when it is less than 0 03 m due to the aforementioned excessive dispersion problem in fact we also tested the standard deviation value of 0 008 m results not shown and found the weights of some particles were nan not a number and the filtering totally failed with the accumulated value of the rmses significantly increasing to 13 26 m and the nses decreasing to 9 88 fig 7 demonstrates the results when the clf was used the range of the value of the scale parameter is determined according to that of the standard deviation using eq 15 and it is 0 008 m 0 08 m results showed that the value of the scale parameter in the range also has an obvious effect on the value of the rmses while the effect on the nse values is negligible too the clf based pf algorithm was also very effective with all the nses greater than 0 96 and all the rmses less than 0 11 m the results of the gaussian and cauchy cases were comparable when the scale parameter was greater than 0 024 m the corresponding standard deviation was greater than 0 03 m furthermore the dispersion didn t happen in this case as the rmses kept decreasing with the decrease of the scale parameter this may be due to the fact that the only uncertainty considered in this synthetic experiment is due to the uncertainty of the roughness coefficient and the observed data is closer to the true value when the scale parameter gets smaller in real world cases there are many kinds of uncertainties e g the uncertainties due to inaccuracy of boundary conditions etc 5 2 real world case it is necessary to further compare the filtering performance of the clf based pf with the glf based pf in real world cases the main reason is that the only uncertainty within the above synthetic experiment is supposed to be due to the inaccuracy of the roughness coefficient however in real world cases other uncertainties like that in the boundary conditions are also significant these uncertainties are more likely to lead to instability of filtering or even filtering failure therefore the performance of the two types of likelihood functions under real world scenarios are explored in this sub section 5 2 1 data and settings the same study area and period as in the synthetic experiment are employed to investigate the real world case the observed discharge data at the ct station and the water stage data in front of the tgd from june 27 to july 9 2009 are set as the boundary conditions the in situ measured water stage data from the six hydrometric stations are employed for data assimilation 5 2 2 results and discussion different values of the standard deviation and the corresponding values of the scale parameter were tested to evaluate the two kinds of likelihood functions in practice the standard deviation is usually set based on experience or statistical information the same study area and hydrodynamic event were also studied by xu et al 2017 and the standard deviation of the observed water stage data in that study was assumed to be 0 1 m in fact the error of observed water stage at hydrometric stations should be much lower than 0 1 m which was reported to be possibly as small as 0 02 m annis et al 2022 pappenberger et al 2006 schmidt 2002 as discussed earlier when the standard deviation is too small the filtering may even fail in this real world case when the standard deviation is equal to or smaller than 0 02 m we found that the glf based pf fail with some of the particle weights being nan after t 50 h in contrast the clf based pf still works when the scale parameter is smaller than the corresponding value of 0 016 m fig 8 demonstrates the results when the glf was used and the range of the value of the standard deviation is 0 03 m 0 1 m as mentioned above when the standard deviation is equal to or smaller than 0 02 m the glf based filter failed and the results were not shown in the figure it is worth noting that the failure did not occur in the synthetic case this is because of the fact that in the synthetic experiment the observed data used for assimilation are different in accordance with the applied observation error in various cases which means the observations are more exact i e closer to the true values when the standard error is small in contrast the errors of the observations in this real world case are fixed it can be seen that all the nses in the figure are large and all the rmses are small and this further proves the effectiveness of the filter at the meantime it can be seen that the results at the three stations in the downstream half are not as good as those in the upstream half and this may be attributed to the reason that the observed water stage at the downstream half are incompatible with the downstream boundary condition of water stage the figure also shows that the value of the standard deviation has an obvious effect on the value of the rmses while the effect on the nse values is negligible furthermore the figure shows that due to the dispersion problem the performance of the filter deteriorates when the standard deviation is smaller than 0 06 m and this can cause failure of the filter when the standard deviation is equal to or smaller than 0 02 m fig 9 demonstrates the results when the clf was used and the range of the scale parameter is 0 008 m 0 08 m according to eq 14 the corresponding value of the standard deviation if the glf were used is 0 01 m 0 1 m the figure shows that the performance of the clf based pf also deteriorates when the scale parameter is smaller than 0 032 m corresponding to a standard deviation of 0 04 m but the performance is much better than the glf based pf it is worth noting that the dispersion with the small scale parameter was not obvious in the results of the clf based pf in the synthetic case this is also because of the fact that in the synthetic experiment the observations are more exact when the applied scale parameter is small this phenomenon indicates that the clf based pf has the potential to further improve the model performance if high precision observations are available when the scale parameter is greater than 0 04 m the nses and rmses are comparable to those of the corresponding glf based results when the scale parameter is less than 0 04 m the performance is slightly better if both the pfs are working and most importantly the clf based pf works in the whole range of the scale parameter while the glf based pf fails when the standard deviation is equal to or smaller than 0 02 m this means the clf can effectively relieve the excessive dispersion and filtering failure of the glf to take a closer examination on the results fig 10 illustrates the simulated results using the two types of likelihood functions with the values of σ and γ being respectively 0 03 m and 0 024 m compared with the open loop case both the glf based and clf based pf can improve the simulation results the results at the three stations in the upstream half of the reach ct cs and qxc are similar for the two filters however the results using the glf based filter contain some fluctuations during the peak water stage hours at the three stations zx wx and fj in the downstream half this demonstrates the excessive dispersion caused by the glf based pf and demonstrates that the clf based pf can effectively relieve this problem besides it should be noted that the fluctuations always happen when at peak stages the main reason is that the discrepancy between simulations and observations are large at the peak stages this large discrepancy is more likely to lead to significant dispersion 6 conclusion in this study we illustrated the limitations of excessive dispersion and filtering failure of the glf within the pf data assimilation framework in hydraulic modeling to overcome the limitations a novel particle weighting scheme using the clf was proposed and evaluated the clf based pf data assimilation was tested and compared with the glf based pf through a synthetic experiment and a real world case results confirmed that if the standard deviation is too small or the simulated value deviated too far from the measured value the glf based pf performs poorly because of the excessive dispersion problem and even totally fails the newly proposed clf is an effective approach to overcome the limitations attributed to the fat tail feature of the cauchy distribution and this results in higher stability and better results in the future the clf based pf will be further tested in other scenarios e g in a complex river network in addition its performance in flood forecasting is also planned to be studied credit authorship contribution statement chenhui jiang writing original draft methodology software validation formal analysis visualization writing review editing dejun zhu conceptualization methodology writing review editing supervision project administration funding acquisition haobo li writing review editing formal analysis investigation xingya xu data curation software danxun li resources supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the study is financially supported by the national natural science foundation of china grant number 52179069 and the water conservancy and technology program of hunan province china grant number xskj2021000 08 the authors would like to thank the changjiang water resources commission of the ministry of water resources for providing highly precise water stage data from hydrometric stations 
