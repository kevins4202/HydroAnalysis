index,text
26035,common software for land data assimilation is urgently needed to implement a wide variety of assimilation applications however a fast easy to use and multidisciplinary application oriented assimilation platform has not been achieved therefore we developed common software for nonlinear and non gaussian land data assimilation comda comda integrates multiple algorithms including diverse kalman and particle filters models and observation operators e g common land model colm advanced integral equation model aiem and provides general interfaces for additional operators using mixed language programming and parallel computing technologies open multi processing openmp message passing interface mpi and compute unified device architecture cuda comda can assimilate various land surface variables and remote sensing observations high performance computing and synthetic tests and real world tests indicate that comda achieves the standard of common land data assimilation software with parallel computation multiple operators and assimilation algorithms and is compatible with many models comda can be applied for multidisciplinary data assimilation keywords land data assimilation common software multidisciplinary applications parallel computing remote sensing 1 introduction as a new methodology in earth system science land data assimilation incorporates both earth observations and numeric land surface models to further understand and predict land surface processes generally due to the limited awareness of earth science concepts and the inherent errors in measuring methods uncertainties arise that have profound impacts on the accuracy of earth observations crow et al 2012 and modeling merz et al 2009 data assimilation takes advantage of earth observations modeling and their uncertainties and provides a more effective framework for studying land surface processes talagrand 1997 liang et al 2013 li 2014 data assimilation consequently places higher demands on computer development environments for specific applications to date data assimilation has become a widely accepted methodology that has been applied in a variety of research fields including hydrology liu and gupta 2007 the carbon cycle rayner et al 2005 climatology carton and giese 2008 fang and li 2016 and phenology ines et al 2013 its theoretical basis was strengthened by ongoing frontier exploration such as multiple scale assimilation bocquet et al 2011 nonlinear and non gaussian methods apte et al 2007 han and li 2008 van leeuwen 2015 stochastic analysis miller 2007 liu and li 2017 etc correspondingly data assimilation system development has been in full swing typical systems include the global land data assimilation system gldas rodell et al 2004 the european land data assimilation system eldas jacobs et al 2008 the chinese land data assimilation system cldas li et al 2007 the canadian land data assimilation system caldas balsamo et al 2007 and the earth observation land data assimilation system eoldas lewis et al 2012 certain software endeavors have been involved in the general development of platforms for common data assimilation studies for example dart data assimilation research testbed anderson et al 2009 openda open data assimilation library and openmi model interface ridler et al 2014 common software for land data assimilation should have the following qualifications first parallel computation is necessary for rapid research outputs an assimilation system tends to be time consuming especially for high dimensional and exceedingly complex land surface models and must contend with the massive introduction of forcing data state variables or observations additionally a parallel framework favors the performance of ensemble based algorithms grid data and parallel designed models second multiple dynamic models for the wide range of applications and multiple observations for assimilating more earth observations are necessary moreover additional observations introduce corresponding observation operators such as radiative transfer models when assimilating remote sensing data third various algorithms are needed to reduce the potential computer algorithm errors and extend the application of software the ensemble kalman filter enkf and particle filter pf are assimilating algorithms that have widespread use however they are computationally demanding because they require a great amount of ensembles to approximate the model s track additionally enkf is based on the multidimensional gaussian assumption fowler and van leeuwen 2013 which may restrict its ability to adapt to non gaussian methods therefore integrating advanced kf and pf is necessary last sufficient space is necessary for forthcoming extensions that introduce new dynamic models and measurements without the need for substantial programming which is also in demand for the land data assimilation community these qualifications constitute a benchmark for common land data assimilation software developments in particular widely used software such as dart and land information system lis data assimilation kumar et al 2008 consider a large range of land surface models and remote sensing observations for example advanced microwave scanning radiometer earth observing system amsr e and moderate resolution imaging spectroradiometer modis but require restrictions on customized models and algorithms for general users other related works all have shortages as well openda openmi provides an interface standard to bridge the assimilation system and numerical models but is not appropriate for user friendly and fast applications because of the lack of realistic models karssenberg et al 2010 propose a visualized software framework which further contains pf other software including pdaf parallel data assimilation framework nerger and hiller 2013 daspy han et al 2015 terrsysmp pdaf kurtz et al 2016 and the soil and water assessment tool hydrological data assimilation system swat hdas zhang et al 2017 provide limited forecasting models for example community land model or swat and assimilation algorithms are restricted to kalman filters and pf i e enkf and local ensemble transform kalman filter therefore general software for land data assimilation and its corresponding applications are still urgently needed in this paper we introduce common software for nonlinear and non gaussian land data assimilation system comda which we developed to meet the above demands of a general software platform of data assimilation this paper is organized as follows in the next section the system framework of comda is proposed the integrated models methods and other techniques are also described in detail section 3 and section 4 present two case studies based on comda with one that employs the chinese land data assimilation system method of extending comda by introducing a new model is also explained in these sections high performance computing in comda is proposed in section 5 discussions regarding comda are presented in section 6 and conclusions are drawn in section 7 2 system framework 2 1 general design comda see fig 1 is a multisource observation for example remote sensing and in situ data and land surface application oriented software platform that has the ability to run distributed algorithms and work on multiple operating systems this software platform includes multimodels for both forecasting operators and observation operators and various assimilated algorithms and supports fast development and assimilated data analysis across different fields such as ecology hydrology and earth surface system science comda is linux based and mainly developed in the standard c ansi 98 language furthermore some special modules are based on other programming languages such as fortran python and c three types of parallel computation techniques i e openmp open multi processing mpi message passing interface and cuda compute unified device architecture are used to advance the high performance computing of comda this software mainly consists of four components assimilated algorithms modules forecasting and observation operator modules and auxiliaries their detailed unified model language uml class diagrams are illustrated in fig 2 2 2 data assimilation algorithms as the most popular algorithm in data assimilation the kalman filter based assimilation approach is adopted in this software with variants such as the extended kalman filter ekf enkf and unscented kalman filter ukf as well as the pf some advanced approaches are also included for instance the unscented particle filter upf merwe et al 2000 central difference kalman filter cdkf norgaard et al 2000 square root ukf merwe et al 2001 square root cdkf srcdkf merwe 2004 and gauss hermite particle filter ito and xiong 2000 in the framework of comda all the approaches are based on the bayesian filter which is built as a base class therefore the filter based assimilation approaches are inherited from the class of bayesian filter and their uml class diagrams are illustrated in fig 3 each approach is classified into two different base classes kalmanfilter and particlefilter which both inherit the common base class named bayesianfilter all the approaches are designed according to the same standard fig 4 i e one approach should be decomposed into three components the main part of the algorithm the input module and the output module an input module includes the configuring parameters and forcing data transmitting the system states into an assimilation algorithm and introducing observations developers can use the input module to prepare the necessary data set according to the model operators and observation operators and then check their reasonability in the main part of the algorithm not mandatory if the assimilation system is represented by multiple patches or the algorithm is an ensemble method parallel computation should be considered meanwhile developers must consider possible errors in the data set and models and create corresponding exception reports in the output module the analysis field and errors are also provided in the output module 2 3 model operators one of the most important features of comda is that the major land surface models are integrated in this software platform based on these models scientists can rapidly implement studies in specific fields by using this data assimilation system and setting up the related options furthermore this integration platform of the model operators is available for a new model as long as the corresponding input output specifications are available in comda the following land surface models are adopted the common land model colm dai et al 2003 simple biosphere model sib2 sellers et al 1996 the lund potsdam jena dynamic global vegetation model lpj dgvm sitch et al 2003 the noah land surface model lsm ek et al 2003 the simultaneous heat and water shaw model flerchinger 2000 the three layer variable infiltration capacity vic 3l model liang et al 1994 and the geotop model rigon et al 2006 additionally two toy models i e the lorenz 1963 and stochastic lorenz model miller et al 1999 are also incorporated among these models only the stochastic lorenz is stochastically based and thus produces probability distributions of the results the other models employ traditional methods and are deterministic a base class named basemodel is first built to derive various classes of true land surface models in basemodel some necessary functions are defined as interfaces that are inherited by its subclasses such as the parameters configuration input output and running the model the interface of parameters configuration should assert the specific values of parameters or the access addresses of parameter files and the input and output interfaces define the format of data that the models transit these interfaces are composed of a standard procedure to introduce a forecasting operator in comda thus regardless of how the model operators are executed a general process first calls the interfaces of the parameters configuration and the forcing data input to initialize the model operator then the interface for running the model is employed to execute the model and results are obtained from the output interface two different subclasses named onedimensionmodel and threedimensionmodel are inherited from basemodel see fig 5 and each model is classified into these two subclasses the specific forecasting operators should inherit and implement the interfaces defined in basemodel most source codes of the specific model operators are programed using the c language however other models are only loosely coupled into comda meaning that their source codes are kept in the forms of the initial programming languages for example both the noah lsm and shaw models are designed with hybrid programming i e their source codes written in fortran are directly introduced in comda as the core codes of the model operators and the class of the model operator is encapsulated and the interfaces are designed based on c e g parameters configuration input output and model running the same design is also utilized for the model operator of geotop of which initial source code is written in c see the website http www geotop org using initial source codes may result in the loss of computational performance however introducing the new versions of these models should be fast and easy if they are updated this advantage is a trouble saving feature and ensures that no more modifications are necessary if new versions are introduced in comda especially when the structures of these models are extremely complicated 2 4 observation operators the other important feature of comda is that it also integrates some observation operators for remote sensing thus offering support for fast assimilation applications to researchers an observation operator has the ability to map the system state vector from the model space to the observation space and during this process the innovations can be calculated to feedback into the assimilation system moreover a good number of observation operators and model operators can ensure that this software becomes a common assimilation system for most fields engaged in earth observations and modeling similar to model operators a virtual class named radiancetransfermodel forms the basis of observation operators there is only one interface named run to be inherited and implemented in the derived classes meanwhile comda provides five auxiliary classes for geophysical objects to build a specific class soilparameter snowparameter canopyparameter brightness and permittivity fig 6 an observation operator can be initialized by choosing more than one auxiliary parameter class and then executing the model with the run interface for land surface data assimilation the assimilated measurements typically belong to the geophysical quantities of remote sensing such as radiative intensity and microwave brightness temperature therefore the following four remote sensing observation operators are primarily integrated into comda a aiem advanced integral equation model chen et al 2003 which was developed for the scattering of the electromagnetic radiative transfer model on the land surface aiem can simulate the backscattering process of electromagnetic radiation in a wide range of land surface roughnesses the previous version of aiem the integration equation model iem is inappropriate for considering the parameters of the land surface roughness and the fresnel reflection coefficient and aiem has reasonably improved upon these problems the core source code is written in fortran b q h wang and choudhury 1981 which is a semiempirical model that models the electromagnetic radiative transfer on a rough land surface this model builds a relationship of albedos between rough and smooth land surfaces by introducing two roughness parameters q and h instead of a measurable physical quantity the roughness parameters are empirical and cannot be observed c memls microwave emission model of layered snowpacks matzler and wiesmann 1999 which is a passive microwave radiative transfer model for snow cover the advantages of this model are that the multiple scatterings and absorptions are considered and there is no limit to the number of snow layers meanwhile memls is broadband and exhibits fast computing d prosail prospect scattering by arbitrarily inclined leaves baret et al 1992 which is a short wave radiative transfer model to model the reflection of various vegetation canopies it works well in different landscapes such as at the scale of leaves and the canopy the core source code is written in fortran e liou q h liou et al 1998 which considers the land surface processes with freezing thawing soil and crtm common radiative transfer model han et al 2006 which specifies the passive microwave radiation of water and ice etc among these operators aiem and prosail are designed employing hybrid programming with fortran and c meaning that their mechanism functions are implemented in fortran and the interfaces inherited from radiancetransfermodel are written in c both q h and memls are programmed with c 2 5 high performance computing we mainly adopt three general high performance computing hpc techniques to improve the computing efficiency of comda openmp mpi and cuda these three techniques together with the hybrid method openmp mpi constitute the four hpc solutions in comda detailed information about these solutions is provided in section 5 2 6 auxiliary modules to ensure the cooperation of assimilation approaches and the model and observation operators the following modules are also introduced a module of datetime this module provides the datetime synchronization services for the different operators and algorithms and it also supports the functions of datetime format converts recording the running steps of operators and calculating the timespan an instance of datetime is first created when initializing a data assimilation system its start time is defined as the collection time of the forcing data and will be increased according to the timespan when running the model operator b module of landgrid this module is designed for managing the running grids of land surface models the grids are organized in a two dimensional entities array and a masking function is used to improve the computational efficiency and reduce the memory redundancy an instance of landgrid is first created when a model operator is initialized this step also loads some properties of landgrid such as total patches longitude and latitude the assimilation algorithm can determine how to organize the cells in landgrid to fulfill the computing requirement and the results can further be formatted based on landgrid c module of sampling in random distributions this module is used to generate the spatial correlated pseudorandom number set across different hardware platforms there are mainly 35 probability density distributions in this mersenne twister matsumoto and nishimura 1998 based module including multivariate gaussian distribution truncated multivariate gaussian distribution exponential distribution uniform distribution poisson distribution logarithmic distribution etc an instance of random sampling is very helpful for ensemble methods and stochastic models a hands down non gaussian distribution facilitates building a non gaussian assimilation system d open source software package we use third party software packages such as it http itpp sourceforge net lapack http www netlib org lapack and armadillo http arma sourceforge net to assist comda to fulfill the matrix operations filters and parallel computations 3 synthetic experiments and scalability 3 1 tests with lorenz model a complete assimilation test is conducted to employ simple models such as lorenz model and assimilation schemes integrated in comda this test presents a clear routine regarding the operation of an instance in comda in addition simple synthetic models introduce less uncertainties into the assimilation system which produces less interference information and provides an ideal concise instance to compare with other data assimilation software in the following tests the employed lorenz model is used in conjunction with the enkf and pf the lorenz model is composed of ordinary differential equations and was developed by lorenz 1963 to study the atmospheric forced dissipative hydrodynamic flow and it is currently widely applied in meteorology although the lorenz model represents nonlinear dynamic performances of an atmospheric system it indeed cannot fully simulate the real assimilation scenes however tests employing the lorenz model provide easy comparisons between different assimilation systems and consequently form a base model operator for synthetic experiments of data assimilation algorithms regarding the lorenz model as a model operator fig 7 a and b show the assimilation results using enkf and pf as the assimilated algorithms respectively these two tests share the same operational process first the code files should be composed of some necessary configurations such as the running frequencies setting and the definitions of the model s parameters second a lorenz model set with different initial values is run and the corresponding results are recorded then the ensemble and observation field are generated by adding random disturbances third the assimilation system is run based on the lorenz model set and the system state vector is updated when available observations have been generated by the lorenz model the lorenz model and the related assimilation algorithms are all integrated in comda therefore one can easily finish these tests by simply selecting the desired model and algorithms and configuring the related parameters such as the initial value the number of ensemble members and the simulation time the synthetic experiment makes it clear that comda works well based on this entirely modular procedure 3 2 test with stochastic lorenz model this test aims to illustrate how to introduce a new model in comda a compatible software platform must be extended to a wide scope of data assimilation applications by integrating as many operators as possible this test is based on the strength of the previous simple lorenz model based test and can be regarded as an example for further extensions both in methodology and in application scope comda provides a series of interfaces to add a new model and observation operators fig 8 a i e the config interface to initialize the related parameters of the model the run interface to launch the model and the output interface to produce the final analysis state data comda only exchanges data with models through these interfaces regardless of their implementation i e programing languages development environments the following test takes the stochastic lorenz model slm as a new model slm is a stochastic version of the lorenz model miller et al 1999 in which the corresponding system state vector is a stochastic process vector fig 8b shows the detailed routine of enkf assimilation scheme by adding slm into comda the procedure of adding slm in comda is firstly to extend the module of the lorenz model to a stochastic numerical simulation method kloeden and pearson 1997 where the most essential procedure is to fulfill the interfaces mentioned above in this test the config interface requires the lorenz equations parameters the initial value the simulation time etc the model s results are transferred by the output interface and updated by the enkf and the analysis state is input into the run interface as the initial value at the next simulation step fig 9 shows the corresponding assimilation results regarding the extension of the application scope of comda by introducing a new model little work is necessary except to develop the corresponding three predefined interfaces config run and output a new model can be added into comda in the form of a code file or any other component and service such that only the interfaces are required to be implemented this procedure not only ensures the correction of the new model since its content is not mandated to be modified but also supports fast development of a data assimilation system in new research fields 4 real world experiment 4 1 study on assimilating airborne remote sensing data assimilation test using comda is conducted in an irrigation district in the midstream region of the heihe river basin sib2 and enkf are implemented with multiple source measurements to improve the land surface soil moisture prediction in the study area the corresponding forcing data including vapor pressure wind speed air temperature precipitation shortwave longwave downward radiation were collected by an eddy covariance and large aperture scintillometer system in hiwater project li et al 2013 the initial value of soil moisture is measured by a cosmic ray soil moisture observing system and the airborne measurement named polarimetric l band multibeam radiometer flight multiangle observations plmr li et al 2015 are regarded as the available observations soil moisture retrievals from plmr flight data represent the value at the measured depth of 5 cm and spatial resolution of 700 m compared with ground based measurements plmr retrievals has the theoretical accuracy of 0 04 m3m 3 which has the potential to improve the simulation of sib2 in comda the retrievals are assimilated in sib2 for observation times of 2012 06 30 2012 07 10 and 2012 08 02 fig 10 shows the corresponding assimilation results validated by the ground based measurements the accuracies of soil moisture are improved by 13 51 and 29 44 during the entire test period and irrigation period respectively comda facilitates this test by directly providing the sib2 model and enkf and offers the corresponding analysis fields and errors to users however to support further research more common analysis tools such as parameter estimation and uncertainty analysis need to be added in comda in future updates 4 2 chinese land data assimilation system cldas cldas li et al 2007 is the pioneer of operational data assimilation systems in china it aims to establish a large scale and catchment scale data assimilation study based on land surface models and distributed hydrological models cldas can assimilate passive microwave remote sensing data such as the special sensor microwave imager ssm i trmm microwave imager tmi and amsr e cldas ultimately generates high precision products spatial resolution is 0 25 of geophysical elements in the arid region of northwest china and the qinghai tibet plateau the corresponding framework of cldas developed based on comda is shown in fig 11 this framework follows the general flow of land surface data assimilation cldas employs colm or sib2 as the model operator enkf as the assimilated algorithm and the microwave radiative transfer model as the observation operator to introduce the remote sensing data the initial fields usually consist of the geophysical elements of soil snow and vegetation are generated by the auxiliary module of sampling in random distributions and then input into the embedded forecasting models to produce the ensembles of forecasting fields the cldas projects the forecasting fields on the observation spaces by using a radiative transfer model since the vast majority of observations are microwave remote sensing data the various passive microwave radiative transfer models were adopted see the details in the next paragraph and compares the forecasting value with the passive microwave remote sensing observations ultimately cldas obtains the kalman gain that is coordinated with the forecasting fields to produce the analyzed value as the system outputs of the current step and the inputs of the next step cldas produces high resolution data for the arid region of northwest china and the qinghai tibet plateau including soil moisture soil temperature snow frozen soil and evapotranspiration data both aiem and q h are used to specify the features of soil while the former is mainly required by the synthetic experiments and the latter is applied in the real world experiments in view of its higher computational efficiency to assimilate the frozen soil the module of liou q h is called which is a microwave model to describe the radiative transfer processes of freezing and thawing soil for land surfaces covered by snow only in the condition that its thickness is great than 2 cm cldas adopts the snow radiative transfer model memls the hpc techniques adopted in cldas are openmp and mpi a typical data assimilation study of cldas was conducted on the qinghai tibetan plateau li et al 2007 this application employs an improved sib2 as the land surface model and amsr e brightness temperatures of vertical and horizontal polarizations as the observations the improved sib2 is the sib2 model with an extra frozen soil parameterization scheme li and koike 2003 that is adjusted to specify the soil freezing thaw processes in the qinghai tibetan plateau the results indicate that the assimilation data and in situ measurements share the same time varying trends but the residual errors caused by the scale transformations cannot be ignored the development of cldas has provided a considerable number of spatial temporal geophysical data products wang et al 2014 for water cycle studies at a large scale and or the catchment scale and enhances studies on models of land surface processes lei et al 2014 zhang et al 2017 microwave remote sensing jin and li 2009 huang et al 2012 pan et al 2012 che et al 2014 and applications and methodologies for assimilation han and li 2008 huang et al 2008a 2008b 2016 rasmy et al 2012 wang et al 2013 chen et al 2015 the development of clda with comda is a mutual promotion process given that there is a very high degree of landscape diversity and heterogeneity in china a large scale data assimilation system study in china requires the implementation of various land surface models and assimilation of multiple source observations including optical and microwave remotely sensed data ground based observational data this demand requires the characteristics of multiple model operators and observation operators in cldas as well as compatibility with other land surface models in future work meanwhile the non gaussian nonlinear assimilation algorithms integrated in comda will surely contribute to the diversity and reliability of data products generated by cldas and the hpc solutions make it possible to quickly develop a larger scale data assimilation system with higher precision products 5 high performance computing in comda four different hpc solutions are embedded in comda also see table 1 a openmp which is a software library of parallel computing that can fully use the ability of the system architecture with a multicore cpu b mpi which is a distributed hpc method that has advantages of parallel computing and information interaction on multiple nodes based on many computers connected by network c openmp mpi which combines the techniques of openmp and mpi to utilize their advantages and achieve a higher computing performance d cuda which uses the excellent memory processing ability of the gpu graphic processing unit and requires data exchange between the cpu and the gpu meaning that each computing function is executed on the gpu and the results can be accessed through the cpu each of hpc solutions is tested in a series of experiments to validate their computing capabilities the corresponding experiments include an enkf assimilation with colm as the forecasting operator where the solutions of openmp mpi and openmp mpi are tested and colm based on the solution of cuda 5 1 parallel computing test using openmp and mpi a parallel programming solution can be easily built using openmp in a multicore node the parallel computing is implemented at the model grid where the model running units are assigned to each parallel task according to the multithreading mechanism of the cpu meaning that the assimilation system becomes more time saving as the number of parallel tasks increases a related test is based on the colm model the number of model grid units is 7610 and enkf the number of ensemble members is 50 compared with the normal test the assimilation algorithm speeds up 75 using 8 parallel tasks and 93 75 using 24 parallel tasks the parallel data assimilation technique based on mpi shares the same process as the one based on openmp however the parallel tasks are assigned to multiple cores at different nodes solutions that include both spmd single program multiple data and the master slave framework are combined to implement this technique that is the related data are computed on different servers by dynamic deployment according to the bounds of grids the numbers of cpus computing nodes and processes and then the results are summarized to the main node colm and enkf are also adopted in this test with the same configurations the corresponding speed up test is carried out with various numbers of parallel tasks fig 12 there are three different schemes in this test the number of colm grid patches for these schemes is 400 100 and 100 and the corresponding ensemble sizes are 100 50 and 100 respectively we validate the parallel computing performance of mpi based on this test clearly the speed up ratio continues to climb dramatically as the number of tasks increases and then slows when the tasks are more than 12 however a comparison of the red line refers to the test with 400 patches and 100 ensembles with the blue line 100 patches and 100 ensembles or the green line 100 patches and 50 ensembles with the blue line shows that neither the patch number nor the ensemble size has an impact on the speed up ratio meanwhile none of the ratios can achieve the ideal ratio the black dashed line which represents the best performance by using the mpi technique this phenomenon is supposedly caused by the effect of modules that are incapable of parallel computing on the true performance both in colm and in the data assimilation system for the openmp mpi solution we first implement only one mpi on each hardware machine and then deploy the processes of openmp in each mpi its parallel computing performance is similar to that of mpi as shown in fig 13 twin tests to examine the different performances of openmp mpi and openmp mpi are conducted with 4 and 8 parallel tasks respectively the parallel schemes are the same as that of the speed up test i e the numbers of model grid patches are 400 100 and 100 and the ensemble sizes are 100 100 and 50 respectively the results show that openmp is inferior to the other solutions because of its limited application scope such as in the data independent loops therefore most parts of the program cannot be parallelized which results in a lower level of computing performance compared to openmp tests with mpi and openmp mpi present obvious improvements when increasing the parallel tasks however due to the same deficiency of openmp the differences between the solutions of mpi and openmp mpi are not significant 5 2 parallel computing test using cuda the difference between cuda approach and traditional computing is that the model is executed at each grid synchronously based on cuda we test this solution with colm by using remote sensing data with a spatial resolution of 0 25 and a time resolution of 1 h and the computational efficiencies are listed in table 2 compared to the single core processor cuda is 3 times faster however due to the abundance of temporary variables in colm such that the full occupation of the memories of the graphics card the usage rate of the gpu is only 1 8 meanwhile the time intensive operation of transmitting these variables is also time consuming 6 discussion the objective of this study is to develop a data assimilation software platform for a wide range of applications in land surface research therefore some elemental characteristics should be considered integration of common used land surface models and observational models most of them are radiative transfer models for remote sensing data classical and advanced assimilation algorithms available interfaces between different modules and vigorous expansibility for further developments and applications additionally for rapid assimilation applications integrating different land surface models is relatively easy since all the data interfaces of modules are normalized therefore users can be expected to finish their typical assimilation studies by only preparing the built in models and algorithms the related inputs forcing data and parameters and corresponding observations further for more complex studies that introduce new models or algorithms extra work is associated with the redevelopment of data interfaces table 3 presents the difference between comda and the available software for land surface data assimilation this intercomparison is based on the four factors mentioned above namely hpc multiple operators mos wide range of assimilation algorithms wraa and interfaces of embedding models that take future applications into consideration iem additional items such as source code availability and visualization platform are also included apparently comda ranks in the top echelon since it is common application oriented designed using these four factors although this intercomparison cannot prove that comda is better than other data assimilation software since software such as dart pdaf and lis have been available and facilitated the land data assimilation community for a long time and their corresponding updates and supports also made them stronger comda is indeed an ideal and portable option in a variety of fields since these four factors are elemental requirements for the common data assimilation applications additionally forthcoming approaches li et al 2004 bai and li 2011 and measurements http westdc westgis ac cn will enrich the functions and gradually foster the individuality of comda the purpose of developing comda is both for advanced scientific studies and operational applications and theoretically comda can achieve more efficient parallel computing based on low level programming languages and the linux operating system however the implementation scenarios may vary considerably because most of the models are not designed to be parallel computing oriented which has negative impacts on the computing efficiency we mainly adopted colm and three techniques to implement parallel computing tests that require each patch of colm in the grid runs independently according to the model structure colm meets the requirements of mpi and openmp however a more complicated situation occurs when developing colm with cuda cuda must transmit data between the cpu and gpu but the frequent transmission of a large number of temporary variables in colm markedly wastes computing time consequently determining how to resolve this time consuming process is challenging the parallel tests also demonstrate that the ideal computing performance cannot be reached until the explicit parallel computing models and algorithms are introduced therefore the corresponding improved approaches include the redevelopment of models that appeal to more general frameworks of parallel computing meanwhile an optimal scheme is not available for results visualization we plan to solve these problems in our future work developing the comda for more advanced applications is also important such as for assimilating multiple source observations and considering nonlinear and non gaussian land surface processes one method of responding to this demand is to introduce all the available observation models into this system which provides a promising method of projecting the state vector to multiple source observation spaces and then engendering an impact on the model trace the classic nonlinear and non gaussian assimilated algorithms such as enkf ukf pf and many other techniques are integrated into comda since comda is oriented towards land surface processes the seldom used variational approaches are not taken into account a typical nonlinear and non gaussian study can be bolstered by these algorithms together with the nonlinear dynamic models in the synthetic experiments of comda the model errors and observation errors are defined as 1 which is not a good setting for uncertainty analyses because both of these errors vary with the model trace and the observations that are to be assimilated however determining the model errors and observation errors is exceedingly complicated and developments of the corresponding theorem are still being studied fowler and van leeuwen 2013 liu and li 2017 in view of the nonlinear and non gaussian strategies adopted in comda as well as the introduction of multiple source observations the errors derived from the uncertainties of the model and observations may become so considerable that untrustworthy results are deduced therefore error definition interfaces have been reserved for the possible extensions of model errors and observation errors in comda a distinct advantage of comda is the integration of multiple land surface processes models which range from distributed hydrologic models and vegetation models to microwave radiative transfer models thus it further meets the demands for rapid studies in most fields in earth observations and simulations however comda is not eligible for studies of model coupling meanwhile large range applications of comda are required currently the typical applications are only synthetic experiments and cldas we provide a fast development framework for data assimilation studies and leave room for extensions to the community 7 summary this study presents a new general software comda solution for the land data assimilation community the advantages of comda are the integration of multiple forecasting operators including colm sib2 lpj dgvm noah lsm shaw vic 3l geotop lorenz and the stochastic lorenz model and observation operators including aiem q h memls prosail etc and implementation of various parallel computing techniques openmp mpi and cuda furthermore with the adoption of multiple algorithms including enkf ukf pf upf cdkf gauss hermite pf etc and flexible model interfaces for forthcoming new models all of these advantages provide a promising method of rapidly conducting studies in a wide range of multidisciplinary applications including hydrology vegetation and remote sensing the corresponding tests indicate that comda is an ideal software solution for common data assimilation studies as a notable application this software also constitutes the framework of cldas the source code and implementation directions for comda are available on github declarations of competing interest none acknowledgements this work was supported by the strategic priority research program of the chinese academy of sciences grant numbers xda20100104 the national natural science foundation of china grant numbers 41730642 the 13th five year informatization plan of chinese academy of sciences grant numbers xxh13505 06 the national natural science foundation of china grant numbers 41801270 and the foundation for excellent youth scholars of nieer cas appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104638 
26035,common software for land data assimilation is urgently needed to implement a wide variety of assimilation applications however a fast easy to use and multidisciplinary application oriented assimilation platform has not been achieved therefore we developed common software for nonlinear and non gaussian land data assimilation comda comda integrates multiple algorithms including diverse kalman and particle filters models and observation operators e g common land model colm advanced integral equation model aiem and provides general interfaces for additional operators using mixed language programming and parallel computing technologies open multi processing openmp message passing interface mpi and compute unified device architecture cuda comda can assimilate various land surface variables and remote sensing observations high performance computing and synthetic tests and real world tests indicate that comda achieves the standard of common land data assimilation software with parallel computation multiple operators and assimilation algorithms and is compatible with many models comda can be applied for multidisciplinary data assimilation keywords land data assimilation common software multidisciplinary applications parallel computing remote sensing 1 introduction as a new methodology in earth system science land data assimilation incorporates both earth observations and numeric land surface models to further understand and predict land surface processes generally due to the limited awareness of earth science concepts and the inherent errors in measuring methods uncertainties arise that have profound impacts on the accuracy of earth observations crow et al 2012 and modeling merz et al 2009 data assimilation takes advantage of earth observations modeling and their uncertainties and provides a more effective framework for studying land surface processes talagrand 1997 liang et al 2013 li 2014 data assimilation consequently places higher demands on computer development environments for specific applications to date data assimilation has become a widely accepted methodology that has been applied in a variety of research fields including hydrology liu and gupta 2007 the carbon cycle rayner et al 2005 climatology carton and giese 2008 fang and li 2016 and phenology ines et al 2013 its theoretical basis was strengthened by ongoing frontier exploration such as multiple scale assimilation bocquet et al 2011 nonlinear and non gaussian methods apte et al 2007 han and li 2008 van leeuwen 2015 stochastic analysis miller 2007 liu and li 2017 etc correspondingly data assimilation system development has been in full swing typical systems include the global land data assimilation system gldas rodell et al 2004 the european land data assimilation system eldas jacobs et al 2008 the chinese land data assimilation system cldas li et al 2007 the canadian land data assimilation system caldas balsamo et al 2007 and the earth observation land data assimilation system eoldas lewis et al 2012 certain software endeavors have been involved in the general development of platforms for common data assimilation studies for example dart data assimilation research testbed anderson et al 2009 openda open data assimilation library and openmi model interface ridler et al 2014 common software for land data assimilation should have the following qualifications first parallel computation is necessary for rapid research outputs an assimilation system tends to be time consuming especially for high dimensional and exceedingly complex land surface models and must contend with the massive introduction of forcing data state variables or observations additionally a parallel framework favors the performance of ensemble based algorithms grid data and parallel designed models second multiple dynamic models for the wide range of applications and multiple observations for assimilating more earth observations are necessary moreover additional observations introduce corresponding observation operators such as radiative transfer models when assimilating remote sensing data third various algorithms are needed to reduce the potential computer algorithm errors and extend the application of software the ensemble kalman filter enkf and particle filter pf are assimilating algorithms that have widespread use however they are computationally demanding because they require a great amount of ensembles to approximate the model s track additionally enkf is based on the multidimensional gaussian assumption fowler and van leeuwen 2013 which may restrict its ability to adapt to non gaussian methods therefore integrating advanced kf and pf is necessary last sufficient space is necessary for forthcoming extensions that introduce new dynamic models and measurements without the need for substantial programming which is also in demand for the land data assimilation community these qualifications constitute a benchmark for common land data assimilation software developments in particular widely used software such as dart and land information system lis data assimilation kumar et al 2008 consider a large range of land surface models and remote sensing observations for example advanced microwave scanning radiometer earth observing system amsr e and moderate resolution imaging spectroradiometer modis but require restrictions on customized models and algorithms for general users other related works all have shortages as well openda openmi provides an interface standard to bridge the assimilation system and numerical models but is not appropriate for user friendly and fast applications because of the lack of realistic models karssenberg et al 2010 propose a visualized software framework which further contains pf other software including pdaf parallel data assimilation framework nerger and hiller 2013 daspy han et al 2015 terrsysmp pdaf kurtz et al 2016 and the soil and water assessment tool hydrological data assimilation system swat hdas zhang et al 2017 provide limited forecasting models for example community land model or swat and assimilation algorithms are restricted to kalman filters and pf i e enkf and local ensemble transform kalman filter therefore general software for land data assimilation and its corresponding applications are still urgently needed in this paper we introduce common software for nonlinear and non gaussian land data assimilation system comda which we developed to meet the above demands of a general software platform of data assimilation this paper is organized as follows in the next section the system framework of comda is proposed the integrated models methods and other techniques are also described in detail section 3 and section 4 present two case studies based on comda with one that employs the chinese land data assimilation system method of extending comda by introducing a new model is also explained in these sections high performance computing in comda is proposed in section 5 discussions regarding comda are presented in section 6 and conclusions are drawn in section 7 2 system framework 2 1 general design comda see fig 1 is a multisource observation for example remote sensing and in situ data and land surface application oriented software platform that has the ability to run distributed algorithms and work on multiple operating systems this software platform includes multimodels for both forecasting operators and observation operators and various assimilated algorithms and supports fast development and assimilated data analysis across different fields such as ecology hydrology and earth surface system science comda is linux based and mainly developed in the standard c ansi 98 language furthermore some special modules are based on other programming languages such as fortran python and c three types of parallel computation techniques i e openmp open multi processing mpi message passing interface and cuda compute unified device architecture are used to advance the high performance computing of comda this software mainly consists of four components assimilated algorithms modules forecasting and observation operator modules and auxiliaries their detailed unified model language uml class diagrams are illustrated in fig 2 2 2 data assimilation algorithms as the most popular algorithm in data assimilation the kalman filter based assimilation approach is adopted in this software with variants such as the extended kalman filter ekf enkf and unscented kalman filter ukf as well as the pf some advanced approaches are also included for instance the unscented particle filter upf merwe et al 2000 central difference kalman filter cdkf norgaard et al 2000 square root ukf merwe et al 2001 square root cdkf srcdkf merwe 2004 and gauss hermite particle filter ito and xiong 2000 in the framework of comda all the approaches are based on the bayesian filter which is built as a base class therefore the filter based assimilation approaches are inherited from the class of bayesian filter and their uml class diagrams are illustrated in fig 3 each approach is classified into two different base classes kalmanfilter and particlefilter which both inherit the common base class named bayesianfilter all the approaches are designed according to the same standard fig 4 i e one approach should be decomposed into three components the main part of the algorithm the input module and the output module an input module includes the configuring parameters and forcing data transmitting the system states into an assimilation algorithm and introducing observations developers can use the input module to prepare the necessary data set according to the model operators and observation operators and then check their reasonability in the main part of the algorithm not mandatory if the assimilation system is represented by multiple patches or the algorithm is an ensemble method parallel computation should be considered meanwhile developers must consider possible errors in the data set and models and create corresponding exception reports in the output module the analysis field and errors are also provided in the output module 2 3 model operators one of the most important features of comda is that the major land surface models are integrated in this software platform based on these models scientists can rapidly implement studies in specific fields by using this data assimilation system and setting up the related options furthermore this integration platform of the model operators is available for a new model as long as the corresponding input output specifications are available in comda the following land surface models are adopted the common land model colm dai et al 2003 simple biosphere model sib2 sellers et al 1996 the lund potsdam jena dynamic global vegetation model lpj dgvm sitch et al 2003 the noah land surface model lsm ek et al 2003 the simultaneous heat and water shaw model flerchinger 2000 the three layer variable infiltration capacity vic 3l model liang et al 1994 and the geotop model rigon et al 2006 additionally two toy models i e the lorenz 1963 and stochastic lorenz model miller et al 1999 are also incorporated among these models only the stochastic lorenz is stochastically based and thus produces probability distributions of the results the other models employ traditional methods and are deterministic a base class named basemodel is first built to derive various classes of true land surface models in basemodel some necessary functions are defined as interfaces that are inherited by its subclasses such as the parameters configuration input output and running the model the interface of parameters configuration should assert the specific values of parameters or the access addresses of parameter files and the input and output interfaces define the format of data that the models transit these interfaces are composed of a standard procedure to introduce a forecasting operator in comda thus regardless of how the model operators are executed a general process first calls the interfaces of the parameters configuration and the forcing data input to initialize the model operator then the interface for running the model is employed to execute the model and results are obtained from the output interface two different subclasses named onedimensionmodel and threedimensionmodel are inherited from basemodel see fig 5 and each model is classified into these two subclasses the specific forecasting operators should inherit and implement the interfaces defined in basemodel most source codes of the specific model operators are programed using the c language however other models are only loosely coupled into comda meaning that their source codes are kept in the forms of the initial programming languages for example both the noah lsm and shaw models are designed with hybrid programming i e their source codes written in fortran are directly introduced in comda as the core codes of the model operators and the class of the model operator is encapsulated and the interfaces are designed based on c e g parameters configuration input output and model running the same design is also utilized for the model operator of geotop of which initial source code is written in c see the website http www geotop org using initial source codes may result in the loss of computational performance however introducing the new versions of these models should be fast and easy if they are updated this advantage is a trouble saving feature and ensures that no more modifications are necessary if new versions are introduced in comda especially when the structures of these models are extremely complicated 2 4 observation operators the other important feature of comda is that it also integrates some observation operators for remote sensing thus offering support for fast assimilation applications to researchers an observation operator has the ability to map the system state vector from the model space to the observation space and during this process the innovations can be calculated to feedback into the assimilation system moreover a good number of observation operators and model operators can ensure that this software becomes a common assimilation system for most fields engaged in earth observations and modeling similar to model operators a virtual class named radiancetransfermodel forms the basis of observation operators there is only one interface named run to be inherited and implemented in the derived classes meanwhile comda provides five auxiliary classes for geophysical objects to build a specific class soilparameter snowparameter canopyparameter brightness and permittivity fig 6 an observation operator can be initialized by choosing more than one auxiliary parameter class and then executing the model with the run interface for land surface data assimilation the assimilated measurements typically belong to the geophysical quantities of remote sensing such as radiative intensity and microwave brightness temperature therefore the following four remote sensing observation operators are primarily integrated into comda a aiem advanced integral equation model chen et al 2003 which was developed for the scattering of the electromagnetic radiative transfer model on the land surface aiem can simulate the backscattering process of electromagnetic radiation in a wide range of land surface roughnesses the previous version of aiem the integration equation model iem is inappropriate for considering the parameters of the land surface roughness and the fresnel reflection coefficient and aiem has reasonably improved upon these problems the core source code is written in fortran b q h wang and choudhury 1981 which is a semiempirical model that models the electromagnetic radiative transfer on a rough land surface this model builds a relationship of albedos between rough and smooth land surfaces by introducing two roughness parameters q and h instead of a measurable physical quantity the roughness parameters are empirical and cannot be observed c memls microwave emission model of layered snowpacks matzler and wiesmann 1999 which is a passive microwave radiative transfer model for snow cover the advantages of this model are that the multiple scatterings and absorptions are considered and there is no limit to the number of snow layers meanwhile memls is broadband and exhibits fast computing d prosail prospect scattering by arbitrarily inclined leaves baret et al 1992 which is a short wave radiative transfer model to model the reflection of various vegetation canopies it works well in different landscapes such as at the scale of leaves and the canopy the core source code is written in fortran e liou q h liou et al 1998 which considers the land surface processes with freezing thawing soil and crtm common radiative transfer model han et al 2006 which specifies the passive microwave radiation of water and ice etc among these operators aiem and prosail are designed employing hybrid programming with fortran and c meaning that their mechanism functions are implemented in fortran and the interfaces inherited from radiancetransfermodel are written in c both q h and memls are programmed with c 2 5 high performance computing we mainly adopt three general high performance computing hpc techniques to improve the computing efficiency of comda openmp mpi and cuda these three techniques together with the hybrid method openmp mpi constitute the four hpc solutions in comda detailed information about these solutions is provided in section 5 2 6 auxiliary modules to ensure the cooperation of assimilation approaches and the model and observation operators the following modules are also introduced a module of datetime this module provides the datetime synchronization services for the different operators and algorithms and it also supports the functions of datetime format converts recording the running steps of operators and calculating the timespan an instance of datetime is first created when initializing a data assimilation system its start time is defined as the collection time of the forcing data and will be increased according to the timespan when running the model operator b module of landgrid this module is designed for managing the running grids of land surface models the grids are organized in a two dimensional entities array and a masking function is used to improve the computational efficiency and reduce the memory redundancy an instance of landgrid is first created when a model operator is initialized this step also loads some properties of landgrid such as total patches longitude and latitude the assimilation algorithm can determine how to organize the cells in landgrid to fulfill the computing requirement and the results can further be formatted based on landgrid c module of sampling in random distributions this module is used to generate the spatial correlated pseudorandom number set across different hardware platforms there are mainly 35 probability density distributions in this mersenne twister matsumoto and nishimura 1998 based module including multivariate gaussian distribution truncated multivariate gaussian distribution exponential distribution uniform distribution poisson distribution logarithmic distribution etc an instance of random sampling is very helpful for ensemble methods and stochastic models a hands down non gaussian distribution facilitates building a non gaussian assimilation system d open source software package we use third party software packages such as it http itpp sourceforge net lapack http www netlib org lapack and armadillo http arma sourceforge net to assist comda to fulfill the matrix operations filters and parallel computations 3 synthetic experiments and scalability 3 1 tests with lorenz model a complete assimilation test is conducted to employ simple models such as lorenz model and assimilation schemes integrated in comda this test presents a clear routine regarding the operation of an instance in comda in addition simple synthetic models introduce less uncertainties into the assimilation system which produces less interference information and provides an ideal concise instance to compare with other data assimilation software in the following tests the employed lorenz model is used in conjunction with the enkf and pf the lorenz model is composed of ordinary differential equations and was developed by lorenz 1963 to study the atmospheric forced dissipative hydrodynamic flow and it is currently widely applied in meteorology although the lorenz model represents nonlinear dynamic performances of an atmospheric system it indeed cannot fully simulate the real assimilation scenes however tests employing the lorenz model provide easy comparisons between different assimilation systems and consequently form a base model operator for synthetic experiments of data assimilation algorithms regarding the lorenz model as a model operator fig 7 a and b show the assimilation results using enkf and pf as the assimilated algorithms respectively these two tests share the same operational process first the code files should be composed of some necessary configurations such as the running frequencies setting and the definitions of the model s parameters second a lorenz model set with different initial values is run and the corresponding results are recorded then the ensemble and observation field are generated by adding random disturbances third the assimilation system is run based on the lorenz model set and the system state vector is updated when available observations have been generated by the lorenz model the lorenz model and the related assimilation algorithms are all integrated in comda therefore one can easily finish these tests by simply selecting the desired model and algorithms and configuring the related parameters such as the initial value the number of ensemble members and the simulation time the synthetic experiment makes it clear that comda works well based on this entirely modular procedure 3 2 test with stochastic lorenz model this test aims to illustrate how to introduce a new model in comda a compatible software platform must be extended to a wide scope of data assimilation applications by integrating as many operators as possible this test is based on the strength of the previous simple lorenz model based test and can be regarded as an example for further extensions both in methodology and in application scope comda provides a series of interfaces to add a new model and observation operators fig 8 a i e the config interface to initialize the related parameters of the model the run interface to launch the model and the output interface to produce the final analysis state data comda only exchanges data with models through these interfaces regardless of their implementation i e programing languages development environments the following test takes the stochastic lorenz model slm as a new model slm is a stochastic version of the lorenz model miller et al 1999 in which the corresponding system state vector is a stochastic process vector fig 8b shows the detailed routine of enkf assimilation scheme by adding slm into comda the procedure of adding slm in comda is firstly to extend the module of the lorenz model to a stochastic numerical simulation method kloeden and pearson 1997 where the most essential procedure is to fulfill the interfaces mentioned above in this test the config interface requires the lorenz equations parameters the initial value the simulation time etc the model s results are transferred by the output interface and updated by the enkf and the analysis state is input into the run interface as the initial value at the next simulation step fig 9 shows the corresponding assimilation results regarding the extension of the application scope of comda by introducing a new model little work is necessary except to develop the corresponding three predefined interfaces config run and output a new model can be added into comda in the form of a code file or any other component and service such that only the interfaces are required to be implemented this procedure not only ensures the correction of the new model since its content is not mandated to be modified but also supports fast development of a data assimilation system in new research fields 4 real world experiment 4 1 study on assimilating airborne remote sensing data assimilation test using comda is conducted in an irrigation district in the midstream region of the heihe river basin sib2 and enkf are implemented with multiple source measurements to improve the land surface soil moisture prediction in the study area the corresponding forcing data including vapor pressure wind speed air temperature precipitation shortwave longwave downward radiation were collected by an eddy covariance and large aperture scintillometer system in hiwater project li et al 2013 the initial value of soil moisture is measured by a cosmic ray soil moisture observing system and the airborne measurement named polarimetric l band multibeam radiometer flight multiangle observations plmr li et al 2015 are regarded as the available observations soil moisture retrievals from plmr flight data represent the value at the measured depth of 5 cm and spatial resolution of 700 m compared with ground based measurements plmr retrievals has the theoretical accuracy of 0 04 m3m 3 which has the potential to improve the simulation of sib2 in comda the retrievals are assimilated in sib2 for observation times of 2012 06 30 2012 07 10 and 2012 08 02 fig 10 shows the corresponding assimilation results validated by the ground based measurements the accuracies of soil moisture are improved by 13 51 and 29 44 during the entire test period and irrigation period respectively comda facilitates this test by directly providing the sib2 model and enkf and offers the corresponding analysis fields and errors to users however to support further research more common analysis tools such as parameter estimation and uncertainty analysis need to be added in comda in future updates 4 2 chinese land data assimilation system cldas cldas li et al 2007 is the pioneer of operational data assimilation systems in china it aims to establish a large scale and catchment scale data assimilation study based on land surface models and distributed hydrological models cldas can assimilate passive microwave remote sensing data such as the special sensor microwave imager ssm i trmm microwave imager tmi and amsr e cldas ultimately generates high precision products spatial resolution is 0 25 of geophysical elements in the arid region of northwest china and the qinghai tibet plateau the corresponding framework of cldas developed based on comda is shown in fig 11 this framework follows the general flow of land surface data assimilation cldas employs colm or sib2 as the model operator enkf as the assimilated algorithm and the microwave radiative transfer model as the observation operator to introduce the remote sensing data the initial fields usually consist of the geophysical elements of soil snow and vegetation are generated by the auxiliary module of sampling in random distributions and then input into the embedded forecasting models to produce the ensembles of forecasting fields the cldas projects the forecasting fields on the observation spaces by using a radiative transfer model since the vast majority of observations are microwave remote sensing data the various passive microwave radiative transfer models were adopted see the details in the next paragraph and compares the forecasting value with the passive microwave remote sensing observations ultimately cldas obtains the kalman gain that is coordinated with the forecasting fields to produce the analyzed value as the system outputs of the current step and the inputs of the next step cldas produces high resolution data for the arid region of northwest china and the qinghai tibet plateau including soil moisture soil temperature snow frozen soil and evapotranspiration data both aiem and q h are used to specify the features of soil while the former is mainly required by the synthetic experiments and the latter is applied in the real world experiments in view of its higher computational efficiency to assimilate the frozen soil the module of liou q h is called which is a microwave model to describe the radiative transfer processes of freezing and thawing soil for land surfaces covered by snow only in the condition that its thickness is great than 2 cm cldas adopts the snow radiative transfer model memls the hpc techniques adopted in cldas are openmp and mpi a typical data assimilation study of cldas was conducted on the qinghai tibetan plateau li et al 2007 this application employs an improved sib2 as the land surface model and amsr e brightness temperatures of vertical and horizontal polarizations as the observations the improved sib2 is the sib2 model with an extra frozen soil parameterization scheme li and koike 2003 that is adjusted to specify the soil freezing thaw processes in the qinghai tibetan plateau the results indicate that the assimilation data and in situ measurements share the same time varying trends but the residual errors caused by the scale transformations cannot be ignored the development of cldas has provided a considerable number of spatial temporal geophysical data products wang et al 2014 for water cycle studies at a large scale and or the catchment scale and enhances studies on models of land surface processes lei et al 2014 zhang et al 2017 microwave remote sensing jin and li 2009 huang et al 2012 pan et al 2012 che et al 2014 and applications and methodologies for assimilation han and li 2008 huang et al 2008a 2008b 2016 rasmy et al 2012 wang et al 2013 chen et al 2015 the development of clda with comda is a mutual promotion process given that there is a very high degree of landscape diversity and heterogeneity in china a large scale data assimilation system study in china requires the implementation of various land surface models and assimilation of multiple source observations including optical and microwave remotely sensed data ground based observational data this demand requires the characteristics of multiple model operators and observation operators in cldas as well as compatibility with other land surface models in future work meanwhile the non gaussian nonlinear assimilation algorithms integrated in comda will surely contribute to the diversity and reliability of data products generated by cldas and the hpc solutions make it possible to quickly develop a larger scale data assimilation system with higher precision products 5 high performance computing in comda four different hpc solutions are embedded in comda also see table 1 a openmp which is a software library of parallel computing that can fully use the ability of the system architecture with a multicore cpu b mpi which is a distributed hpc method that has advantages of parallel computing and information interaction on multiple nodes based on many computers connected by network c openmp mpi which combines the techniques of openmp and mpi to utilize their advantages and achieve a higher computing performance d cuda which uses the excellent memory processing ability of the gpu graphic processing unit and requires data exchange between the cpu and the gpu meaning that each computing function is executed on the gpu and the results can be accessed through the cpu each of hpc solutions is tested in a series of experiments to validate their computing capabilities the corresponding experiments include an enkf assimilation with colm as the forecasting operator where the solutions of openmp mpi and openmp mpi are tested and colm based on the solution of cuda 5 1 parallel computing test using openmp and mpi a parallel programming solution can be easily built using openmp in a multicore node the parallel computing is implemented at the model grid where the model running units are assigned to each parallel task according to the multithreading mechanism of the cpu meaning that the assimilation system becomes more time saving as the number of parallel tasks increases a related test is based on the colm model the number of model grid units is 7610 and enkf the number of ensemble members is 50 compared with the normal test the assimilation algorithm speeds up 75 using 8 parallel tasks and 93 75 using 24 parallel tasks the parallel data assimilation technique based on mpi shares the same process as the one based on openmp however the parallel tasks are assigned to multiple cores at different nodes solutions that include both spmd single program multiple data and the master slave framework are combined to implement this technique that is the related data are computed on different servers by dynamic deployment according to the bounds of grids the numbers of cpus computing nodes and processes and then the results are summarized to the main node colm and enkf are also adopted in this test with the same configurations the corresponding speed up test is carried out with various numbers of parallel tasks fig 12 there are three different schemes in this test the number of colm grid patches for these schemes is 400 100 and 100 and the corresponding ensemble sizes are 100 50 and 100 respectively we validate the parallel computing performance of mpi based on this test clearly the speed up ratio continues to climb dramatically as the number of tasks increases and then slows when the tasks are more than 12 however a comparison of the red line refers to the test with 400 patches and 100 ensembles with the blue line 100 patches and 100 ensembles or the green line 100 patches and 50 ensembles with the blue line shows that neither the patch number nor the ensemble size has an impact on the speed up ratio meanwhile none of the ratios can achieve the ideal ratio the black dashed line which represents the best performance by using the mpi technique this phenomenon is supposedly caused by the effect of modules that are incapable of parallel computing on the true performance both in colm and in the data assimilation system for the openmp mpi solution we first implement only one mpi on each hardware machine and then deploy the processes of openmp in each mpi its parallel computing performance is similar to that of mpi as shown in fig 13 twin tests to examine the different performances of openmp mpi and openmp mpi are conducted with 4 and 8 parallel tasks respectively the parallel schemes are the same as that of the speed up test i e the numbers of model grid patches are 400 100 and 100 and the ensemble sizes are 100 100 and 50 respectively the results show that openmp is inferior to the other solutions because of its limited application scope such as in the data independent loops therefore most parts of the program cannot be parallelized which results in a lower level of computing performance compared to openmp tests with mpi and openmp mpi present obvious improvements when increasing the parallel tasks however due to the same deficiency of openmp the differences between the solutions of mpi and openmp mpi are not significant 5 2 parallel computing test using cuda the difference between cuda approach and traditional computing is that the model is executed at each grid synchronously based on cuda we test this solution with colm by using remote sensing data with a spatial resolution of 0 25 and a time resolution of 1 h and the computational efficiencies are listed in table 2 compared to the single core processor cuda is 3 times faster however due to the abundance of temporary variables in colm such that the full occupation of the memories of the graphics card the usage rate of the gpu is only 1 8 meanwhile the time intensive operation of transmitting these variables is also time consuming 6 discussion the objective of this study is to develop a data assimilation software platform for a wide range of applications in land surface research therefore some elemental characteristics should be considered integration of common used land surface models and observational models most of them are radiative transfer models for remote sensing data classical and advanced assimilation algorithms available interfaces between different modules and vigorous expansibility for further developments and applications additionally for rapid assimilation applications integrating different land surface models is relatively easy since all the data interfaces of modules are normalized therefore users can be expected to finish their typical assimilation studies by only preparing the built in models and algorithms the related inputs forcing data and parameters and corresponding observations further for more complex studies that introduce new models or algorithms extra work is associated with the redevelopment of data interfaces table 3 presents the difference between comda and the available software for land surface data assimilation this intercomparison is based on the four factors mentioned above namely hpc multiple operators mos wide range of assimilation algorithms wraa and interfaces of embedding models that take future applications into consideration iem additional items such as source code availability and visualization platform are also included apparently comda ranks in the top echelon since it is common application oriented designed using these four factors although this intercomparison cannot prove that comda is better than other data assimilation software since software such as dart pdaf and lis have been available and facilitated the land data assimilation community for a long time and their corresponding updates and supports also made them stronger comda is indeed an ideal and portable option in a variety of fields since these four factors are elemental requirements for the common data assimilation applications additionally forthcoming approaches li et al 2004 bai and li 2011 and measurements http westdc westgis ac cn will enrich the functions and gradually foster the individuality of comda the purpose of developing comda is both for advanced scientific studies and operational applications and theoretically comda can achieve more efficient parallel computing based on low level programming languages and the linux operating system however the implementation scenarios may vary considerably because most of the models are not designed to be parallel computing oriented which has negative impacts on the computing efficiency we mainly adopted colm and three techniques to implement parallel computing tests that require each patch of colm in the grid runs independently according to the model structure colm meets the requirements of mpi and openmp however a more complicated situation occurs when developing colm with cuda cuda must transmit data between the cpu and gpu but the frequent transmission of a large number of temporary variables in colm markedly wastes computing time consequently determining how to resolve this time consuming process is challenging the parallel tests also demonstrate that the ideal computing performance cannot be reached until the explicit parallel computing models and algorithms are introduced therefore the corresponding improved approaches include the redevelopment of models that appeal to more general frameworks of parallel computing meanwhile an optimal scheme is not available for results visualization we plan to solve these problems in our future work developing the comda for more advanced applications is also important such as for assimilating multiple source observations and considering nonlinear and non gaussian land surface processes one method of responding to this demand is to introduce all the available observation models into this system which provides a promising method of projecting the state vector to multiple source observation spaces and then engendering an impact on the model trace the classic nonlinear and non gaussian assimilated algorithms such as enkf ukf pf and many other techniques are integrated into comda since comda is oriented towards land surface processes the seldom used variational approaches are not taken into account a typical nonlinear and non gaussian study can be bolstered by these algorithms together with the nonlinear dynamic models in the synthetic experiments of comda the model errors and observation errors are defined as 1 which is not a good setting for uncertainty analyses because both of these errors vary with the model trace and the observations that are to be assimilated however determining the model errors and observation errors is exceedingly complicated and developments of the corresponding theorem are still being studied fowler and van leeuwen 2013 liu and li 2017 in view of the nonlinear and non gaussian strategies adopted in comda as well as the introduction of multiple source observations the errors derived from the uncertainties of the model and observations may become so considerable that untrustworthy results are deduced therefore error definition interfaces have been reserved for the possible extensions of model errors and observation errors in comda a distinct advantage of comda is the integration of multiple land surface processes models which range from distributed hydrologic models and vegetation models to microwave radiative transfer models thus it further meets the demands for rapid studies in most fields in earth observations and simulations however comda is not eligible for studies of model coupling meanwhile large range applications of comda are required currently the typical applications are only synthetic experiments and cldas we provide a fast development framework for data assimilation studies and leave room for extensions to the community 7 summary this study presents a new general software comda solution for the land data assimilation community the advantages of comda are the integration of multiple forecasting operators including colm sib2 lpj dgvm noah lsm shaw vic 3l geotop lorenz and the stochastic lorenz model and observation operators including aiem q h memls prosail etc and implementation of various parallel computing techniques openmp mpi and cuda furthermore with the adoption of multiple algorithms including enkf ukf pf upf cdkf gauss hermite pf etc and flexible model interfaces for forthcoming new models all of these advantages provide a promising method of rapidly conducting studies in a wide range of multidisciplinary applications including hydrology vegetation and remote sensing the corresponding tests indicate that comda is an ideal software solution for common data assimilation studies as a notable application this software also constitutes the framework of cldas the source code and implementation directions for comda are available on github declarations of competing interest none acknowledgements this work was supported by the strategic priority research program of the chinese academy of sciences grant numbers xda20100104 the national natural science foundation of china grant numbers 41730642 the 13th five year informatization plan of chinese academy of sciences grant numbers xxh13505 06 the national natural science foundation of china grant numbers 41801270 and the foundation for excellent youth scholars of nieer cas appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104638 
26036,a physics based distributed integrated hydrological model in prediction of water balance of a semi arid catchment in india v d loliyana a c p l patel b a mechatronics systems private limited pune 411058 maharashtra india mechatronics systems private limited pune maharashtra 411058 india former research scholar center of excellence on water resources and flood management department of civil engineering sardar vallabhbhai national institute of technology svnit surat 395001 gujarat india b department of civil engineering sardar vallabhbhai national institute of technology svnit surat 395007 gujarat india department of civil engineering sardar vallabhbhai national institute of technology svnit surat gujarat 395007 india professor department of civil engineering sardar vallabhbhai national institute of technology svnit surat 395007 gujarat india c center of excellence on water resources and flood management department of civil engineering sardar vallabhbhai national institute of technology svnit surat 395007 gujarat india center of excellence on water resources and flood management department of civil engineering sardar vallabhbhai national institute of technology svnit surat gujarat 395007 india former research scholar center of excellence on water resources and flood management department of civil engineering sardar vallabhbhai national institute of technology svnit surat 395007 gujarat india corresponding author a physics based distributed coupled integrated hydrological model mike she mike 11 has been calibrated using a multi objective optimization approach for minimization of errors in prediction of stream flows and groundwater levels within a semi arid yerli sub catchment of upper tapi basin in india the performance of the calibrated model has been found to improve significantly in prediction of stream flows and groundwater levels within the sub catchment while incorporating distributed manning s roughness coefficient vegetation soil and geologic parameters in the model also the model has been able to simulate the soil moisture satisfactorily for the areas falling within the plain area of the sub catchment analyses of water balance period 1991 1998 1999 2004 2005 2009 and 2010 2012 revealed that developed integrated hydrological model could simulate better dynamics of hydrological processes in terms of change in land use groundwater draft and irrigation practices to maximize stakeholder sustainability graphical abstract image 1 keywords mike she mike 11 stream flows groundwater levels soil moisture content water balance yerli sub catchment software and data availability 1 mike she mike 11 software https www mikepoweredbydhi com used for simulation of spatially distributed hydrologic variables of the study area 2 arcgis 10 6 software http www esri in used for the preparation of gis database of the study area 3 erdas imagine 10 0 software https www hexagongeospatial com used for the analysis of satellite images spatially such as land use cover classification of the study area 4 archydro groundwater software https www aquaveo com used for the development of bathymetry of hydrogeological units of the study area the academic licenses of aforesaid softwares are available at computational hydraulics laboratory department civil engineering svnit surat india the softwares run on microsoft windows computer with no special hardware requirements data sources for all utilized explanatory variables are provided in table 1 please contact the corresponding author for any further information at premlalpatel1966 gmail com 1 introduction the conjunctive use of surface water and ground water is essentially required for sustainable management of water resources across river basins the unprecedented population growth in india during recent years particularly due to extensive urbanization led to over exploitation of surface and groundwater resources within the river basins the information on spatio temporal distributions of surface water moisture within the root zones and ground water levels would help water resource managers and agronomists for integrated management of water and optimizing the cropping pattern in the river basins lumped hydrological models have the inherent limitations to provide information of stream flows only at the outlet of catchments while physics based distributed integrated hydrological models can provide detailed description of hydrological processes within the catchments refsgaard 1997 modelling of surface unsaturated and saturated flows necessitate rigorous calibration of distributed parameters for prediction of stream flows groundwater levels and water balance within catchments henriksen et al 2003 vázquez et al 2009 foster and allen 2015 in the recent past physics based distributed hydrological models have become popular due to availability of extensive data for analyses and their capability in modelling the spatial heterogeneity within catchments demetriou and punthakey 1998 mcmichael et al 2006 mcmichael and hope 2007 liu et al 2007 rahim et al 2012 wijesekara et al 2014 mike she model a widely used distributed deterministic physics based hydrological model has been applied in catchments with large spatial heterogeneities jain et al 1992 singh et al 1999 madsen 2003 gupta et al 2008 wang et al 2012 keilholz et al 2015 khwairakpam et al 2018 yang et al 2000 compared three distributed physics based hydrological models for characterization of spatial variability 300 m 300 m in the seki river catchment area of 703 km2 japan the study showed that mike she is an effective and flexible tool for representing spatial heterogeneity and describing detailed hydrological processes vis à vis topmodel and gb models geomorphology based hydrological model the study suggested that incorporating split sample another period validation test and detailed remote sensing and gis data could improve the prediction of stream flows henriksen et al 2003 highlighted the methodological issues related to calibration and validation of mike she establishment of performance criteria parametrization and assessment of model parameters the study emphasized the need of testing calibrated distributed physics based models against independent observed data using split sample tests another period and proxy basin tests another area sahoo et al 2006 calibrated parameters of a lumped mike she model and recommended that consideration of physical heterogeneity in the model parameters could improve the runoff prediction within catchments huang et al 2010 examined hydrological processes in the tarim river basin using lumped mike she mike 11 model and recommended that continuous data in large time scale can describe the long term variations of hydrologic components within the study region rahim et al 2012 simulated total water balance of the paya indah wetlands watershed 242 21 km2 malasiya using distributed mike she grid sizes 200 m 200 m while calibrating the model from observed stream flows only within the watershed qin et al 2013 used an auto calibration tool developed by madsen 2003 in mike she for optimization of groundwater level predictions in large north china plain at monthly and annual time scales the simulated groundwater levels in the wells were compared satisfactorily with observed water levels average rmse of 19 44 m within the study region the water balance of the study area revealed that saturated depths in ground water aquifers were declining in the study region and potential evapotranspiration was higher than precipitation throughout the simulation period the study also recommended that water use efficiency could be improved by changing the cropping pattern and water saving technologies in the north china foster and allen 2015 predicted spatial seasonal interaction of ground water and surface water and predicted water balance of cowichan watershed area 930 km2 canada using detailed remote sensing data base and mike she mike 11 modelling tool vo and goursbille 2016 applied mike she mike 11 model on the vu gia thu bon catchment area of 10 350 km2 for predictions of stream flow and water levels the model was calibrated for distributed manning s m and soil parameter ks with lumped saturated zone parameter kh for period 1991 2000 the study suggested that surface water and groundwater predictions are affected significantly by the choice of distributed manning s m in the model the study acknowledged the source of uncertainty due to coarser resolution of topographic data lack of measured cross sections spatial variation of rainfall data and quality of groundwater data within the catchment loliyana and patel 2018 simulated hydrological processes for the yerli sub catchment in tapi basin india using a coupled mike she mike 11 model the sensitivity of thirteen lumped model parameters of overland unsaturated saturated flow were analyzed while simulating the runoﬀ volume peak runoﬀ at the catchment outlet and groundwater levels within the catchment the study suggested that further consideration of grid to grid variations in leaf area index lai root depth rd soil properties aquifer and unsaturated soil parameters could improve the performance of the model the distributed hydrological model presented in the current study addresses the following research objectives a calibration of a distributed hydrological model using multi objective approach i e minimizing the error in prediction of stream flows and ground water levels within the sub catchment the applicability of the calibrated model is validated with the data of independent period within the sub catchment b effect of grid size on performance of the distributed hydrological has been investigated with reference to accuracy and computational efforts the performance of the model is also compared with another model wherein the parameters of saturated zone were considered as lumped in nature c the calibrated and validated hydrological model has been used for assessing the impact of land use changes on water balance within the sub catchment 2 study area the tapi river is one of the major west flowing rivers of india draining into the arabian sea and passes through madhya pradesh maharashtra and gujarat states having a catchment area of 65 145 km2 the entire tapi basin can be subdivided into three sub basins i e upper tapi basin from origin to hathnur dam 29 430 km2 middle tapi basin from hathnur dam to gidhade stream gauging station 25 320 km2 and lower tapi basin from gidhade stream gauging station to the arabian sea 10395 km2 timbadiya et al 2015 there are two major reservoirs viz ukai and hathnur upstream of ukai reservoirs on the tapi river to meet the domestic industrial and irrigation demands in their respective command areas the purna river is one of the major tributaries of tapi river having its confluence just upstream of hathnur reservoir originating from gawilgarh steep mountains of the eastern satpura range betul district of madhya pradesh see fig 1 the purna river catchment lies between latitudes 20 08 31 to 21 40 30 n and longitudes 75 56 25 to 77 56 46 e having basaltic geology in peripheral part and alluvial formation in central region the ten major tributaries pedhi kate purna chandrabhaga mohali morna mun wan ghan biswa and wagar also drain into the purna river length 360 km up to the confluence with the tapi river the study area of present study includes the purna river catchment up to the yerli stream gauging station would be referred as yerli sub catchment in further discussions draining through the madhya pradesh and maharashtra states the elevation in the sub catchment varies from 213 to 1171 m above the mean sea level fig 2 a the northern portion of the yerli sub catchment has mountainous topography with the highest elevation point 1171 m located at the north east of the sub catchment conversely the point of minimum elevation is at 213 m located at the outlet of sub catchment yerli gauging station the mean elevation of the sub catchment is 372 6 m the area elevation curve i e hypsometric curve fig 2b highlight the flat characteristics of the yerli sub catchment it is evident that 65 of sub catchment area is below mean elevation of 372 6 m while 80 is below 450 m the soils in the sub catchment are mainly dark brown clayey loamy fine textured and sticky in nature the principal rock formation consists of quaternary sediments alluvial deposits upper gondwanas and deccan basalt underlying hills and terraces in the study area average annual 1991 2004 potential evapotranspiration pet in the catchment is 1715 mm and its aridity index i e ratio of rainfall to pet is 0 46 the climate in the study area is thus classified as semi arid as the potential evapotranspiration is 1 6 times to 3 17 times of annual rainfall loliyana and patel 2018 the daily minimum and maximum temperature ranges from 10 c in january month to 48 c in may month respectively in the sub catchment the major land use class in the sub catchment is agriculture followed by deciduous forest some of major crops grown are cotton sorghum green gram black gram pigeon pea during the kharif season while horse gram safflower and wheat are cultivated during the rabi season in the study area jain and tambe 2012 3 material the details of input data viz topographical climatic hydrologic land use land cover soil geological and groundwater data for distributed hydrological modelling of the study domain are described in table 1 square grids of size 500 m 500 m 63 524 finite difference grids has been generated over the study area in formulation of mike she model fig 2a includes the shuttle radar topography mission srtm digital elevation model dem describing the topographical variation and river drainage network in the yerli sub catchment the thiessen polygon approach has been used to obtain the areal rainfall over the sub catchment using the data of seventeen rain gauge stations fig 2c the weather data of three stations falling in three districts were used to obtain time series of pet of respective stations using penman s method penman 1948 the time series of pet of each weather station were distributed to different grids depending upon their locations in respective districts in the sub catchment fig 2d for computation of actual evapotranspiration aet from the model the hydrological data including observed stream flows and water levels and river cross sections at yerli gopalkheda and lakhpuri stream gauging stations were used to calibrate and assess the performance of coupled mike she and mike 11 hydrological model the satellite imagery of year 2000 were classified in erdas imagine and nine types of land use cover were identified fig 3 a the classified land use cover were in turn used for obtaining the distributed manning s roughness parameter m inverse of manning s roughness coefficient n for computation of overland flow in the model also the variation of lai and rd of crops were temporally distributed within the sub catchment as per the cropping pattern in the study areas see fig 3b 3c the soil series data were digitized in vector format in arcgis 10 6 as shown in fig 3d e the available soil data for the study area frequently comprises of 37 soil series whereas soil within each series differ significantly based on local variances such as geology and topography fig 3d e shows the soil characteristics i e class and depth present within the yerli sub catchment the alluvium around 7800 km2 41 area and basalt are the parent material of the soils in the yerli sub catchment while clayey and loamy soils are the dominant surface texture present within the sub catchment the vertisols soil in purna alluvium contains 50 clay out of this 30 60 is fine clay 0 2 mm the fine clay to total clay ratio in the catchment ranged between 0 3 and 0 6 and usually increases with depth the high content of expansive clay vertisols in the sub catchment known as montmorillonite forms deep cracks in dry period of the year the alternate shrinking and swelling characteristics of such soil causes self mulching wherein the soil material is mixed evenly within the soil mass the occurrence of the sub surface salinity in the sub catchment could be ascribed due to cracking nature of these soils the soil depths in the yerli sub catchment varies from 10 to 150 cm presence of soil of high productivity 62 with moderate erosion 67 indicates the dominance of agricultural activity in the sub catchment jain and tambe 2012 the exploratory well data table 1 showing the profile of soil and rock formation below the ground have been used for creating bathymetry of groundwater zone up to 200 m while the data of observation wells were used for comparing simulated water levels in distributed hydrological modeling in present modelling approach the lower boundary of the saturated zone has been considered as bed rock surface basalt rock layer varies from 38 m to 200 m additionally four geological units sand gravel clay sandy clay and weather basalt have been added as geological lenses discontinuous vertical layers within the bathymetry of saturated zone in the mike she set up the alluvial deposit ranges from the ages of lower pleistocene to recent quaternary age within the yerli sub catchment fig 4 a illustrates the detailed information of geological lithology in the yerli sub catchment the alluvium which covers 7800 km2 area of the yerli sub catchment forms the principal water bearing formation wherein granular zones were encountered at various depths in unconfined as well as confined conditions the purna alluvium can be divided into two hydrogeological layers i e younger alluvium extending down to around 80 m depth below the ground level forming potential aquifer comprising alternate beds of clay and sand and older alluvium extending up to 300 m depth below the ground level cgwb cgwb districts report 2013 see fig 4 the cgwb report 2013 indicates that basalt deposits are available as weathered jointed vesicular massive basaltic formation where water bearing formation exists in semi confined to confined conditions the weathered and fractured parts of basaltic chips exist up to 15 20 m 20 25 m and 15 20 m depth below the ground level in amravati akola and buldana districts respectively wherein ground water occurs under unconfined to confined conditions the basaltic region at deeper level occurring at confined conditions does not form the potential aquifer region in purna alluvium around 2760 km2 area 17 4 of total area is affected by inland salinity as shown in fig 4a the sample geological cross sections aa bb cc dd and ee obtained from lithology information of exploratory and observation wells are included in fig 4b f while observing the lithologs along aa right bank of purna river fig 4b it is seen that deep geological layer mainly consists of basaltic formation followed by clayey formation at upper levels the geological formation nearing to the ground surface include thin sand gravel and sandy clay layers along bb left bank of purna river fig 4c the geological formation is dominated by basaltic and weather basalt formations the layer close to the ground surface is dominated by thin sandy clay clay and sand gravel layers due to existence of thick layers of clay in alluvium of yerli sub catchment on either side of river see fig 4d e and f there is slow movement of groundwater in the region the hydrological parameters related to ground water aquifers viz horizontal hydraulic conductivity kxx vertical hydraulic conductivity kyy specific yield sy and specific storage ss had been selected for calibration in present study 4 model description and methodology 4 1 mike she mike 11 model mike she système hydrologique europeén is a distributed physics based deterministic macro scale hydrological model operates at grid level the hydrological model is capable of both continuous and event based analyses to predict water yield in large complex catchments with varying land use soils and management practices dhi 2017 it offers flexibility in choosing various alternative for modelling each component of the hydrological processes in mike she a finite difference method is used in solving 2d saint venant equations as diffusive wave approximation i e partial differential equations describing the overland flow process 2 layer water balance for computation of unsaturated flow 3d darcy s equation for estimating saturated flow and analytical solutions are used to compute interception and actual evapotranspiration within the study domain dhi 2017 in mike 11 1 dimensional saint venant s kinematic wave routing equations are solved for computation of river discharge and water levels at q and h points respectively in the flow domain dhi 2017 the mike she hydrological model and mike 11 hydrodynamic model are coupled at h points along the river network while the flow exchange between the surface water in the river and groundwater is estimated as the difference in water table level in the adjacent grid cell and the river cell multiplied by the conductance value between the two cells the snowmelt component has been ignored in current study as the study area sub catchment is free from snowfall complete description of mike she mike 11 modelling system are available else where dhi 2017 4 2 methodology the process description of mike she in developing distributed hydrological model including overlaying of background maps simulation set up creation of model set up loading of dem meteorological and lulc parameters input coupling of mike she and mike 11 specifying the parameters of unsaturated and saturated zones boundary conditions specifying output variables autocalibration of model parameters and finally the validation of calibrated hydrological model is shown in the form of flow chart see fig 5 further the details of irrigation and groundwater extraction components initial and boundary conditions including time steps for simulation are described elsewhere loliyana and patel 2018 the performance of the mike she and mike 11 coupled model under calibration validation stages were evaluated by comparing simulated stream flow values with their corresponding observed values in terms of statistical performance indices like mean absolute error mae correlation coefficient r nash sutcliffe efficiency nse root mean squared error rmse and efficiency index ei loliyana and patel 2015 5 selection of model parameters the distributed manning s m values to each grid were assigned as per land use and land cover lulc fig 3a while the leaf area index lai and root zone depth rd and crop coefficient kc were distributed as per cropping vegetation pattern within the sub catchment see fig 3 a and b the manning s m values for different land use pattern in sub catchment were selected from available information in literature engman 1986 chow 1959 vieux 2001 kothyari et al 2010 the lai rd and kc are key parameters of crop vegetation used for estimation of actual evapotranspiration for the mike she model the values of lai and rd were selected based on available information in literature foster and allen 2015 wijesekara et al 2014 allen et al 1998 for different crops and stages of their growth based on the cropping pattern jain and tambe 2012 in the sub catchment the lai rd and kc values of crops and fallow lands were estimated for different months in a year and the time series of their respective values for the whole year were given as inputs into the mike she model see fig 3b c the lai rd and kc values for other land use land covers except for crop and fallow land were given constant values for the whole year as per the values based on their classification from satellite imagery of year 2000 see table in fig 3a also the value of drainage depth dd and drainage time constant dt of saturated zones were taken from loliyana and patel 2018 as they were found non sensitive in the previous study the parameter et surface depth is a calibrated parameter which is used for estimation of aet in the two layer water balance method being used in present study the remaining model parameters et uz and sz components of the mike she model forty one parameters were calibrated and validated for simulation of stream flows at the yerli stream gauging station and groundwater levels within the sub catchment for periods 1991 1998 and 1999 2004 respectively see table 2 initial values of the model parameters were chosen based on existing conditions in the sub catchment available information in the literature maidment 1988 todd and mays 2004 and subramanya 2011 the auto calibration was preferred as the information available about the catchment hydrology was limited mike she mike 11 uses the auto calibration process based on shuffle complex evolution sce algorithm for multiple objective functions and multi objective optimization the objective functions are i minimization of rmse in prediction of stream flow at the outlet of sub catchment i e minimum rmse of daily discharges at yerli station and ii rmse of groundwater levels within the sub catchment total 1500 model evaluations were performed for ascertaining optimal values of forty one calibration parameters the optimal values of the model parameters were obtained by plotting the rmse values of daily stream flows and groundwater levels and picking the model parameters for the run out of 1500 model evaluation demonstrating the minimal rmses of both the objective functions fig 6 the details of optional model parameters are included in table 2 in table 2 the calibrated value of et surface depth representing the thickness of capillary fringe above ground water table has been found to be 1 52 m relatively higher values of capillary fringe is indicative of high clay content in the soil of the yerli sub catchment in unsaturated zone θs in the range of 0 30 0 51 indicates maximum water available in the root zone for evapotranspiration and percolation in clay loam sandy clay sandy loam and silty clay soils whereas θfc 0 43 0 29 0 27 0 11 0 33 demonstrate the maximum water content available in the root zone to meet the vegetative evapotranspirational requirements for clay loam sandy clay sandy loam and silty clay soils respectively θwp 0 28 0 15 0 16 0 06 0 19 specifies the limit of water contents in unsaturated zone up to which the plants can extract moisture from the soil without wilting for clay loam sandy clay sandy loam and silty clay soils respectively the higher values of θs θfc and θwp for clay soil represent more water stored in pores of such soil due to high porosity while lower values in the sandy soil are due to less pore space available between particles of such soil the ks 6 9 10 8 8 7 10 5 4 3 10 6 1 6 10 4 5 7 10 7 m s indicate hydraulic conductivity of clay loam sandy clay sandy loam and silty clay soils respectively in the sub catchment in the unsaturated zone the lower hydraulic conductivity values in unsaturated zone indicates the dominance of agriculture and clayey soil within the sub catchment the clayey soil has high vales of et depth and more water in the pores in root zone which is responsible for higher evapotranspiration from the unsaturated zone in the sub catchment sandy soil is having higher hydraulic conductivity vis à vis other type of soils due to better connectivity of pore spaces in the former soil which may lead to high infiltration rate for saturated zone calibrated values of kxx 4 2 10 3 4 8 10 7 9 3 10 6 5 9 10 6 6 1 10 6 m s and kyy 6 1 10 4 7 2 10 8 4 9 10 7 2 7 10 7 3 6 10 7 m s exhibit horizontal and vertical hydraulic conductivity for sand gravel clay sandy clay weather basalt and basalt geological layers respectively the groundwater movement in the sub catchment in general is sluggish due to presence of low permeable alternate bed layers like clay and weather basalt in the major part of the sub catchment much longer residence time in the aquifers is in turn responsible for larger evapotranspiration losses in the sub catchment the dd exhibit the recession part of discharge hydrograph whereas dt governs the velocity of the drainage water from saturated zone higher values of dd and dt indicate larger contribution from base flow and higher drainage velocity i e large peak respectively the dd 1 4 m and dt 10 7 s 1 indicate moderate influence of base flow in stream runoff and slow drainage velocity due to relatively shallow water table depth near the bank of purna river and larger clay content in the soil loliyana and patel 2018 in order to assess the significance of model parameters in hydrological modelling the variation of parameter sets for 1500 model evaluations attained through auto calibration along with balanced optimum value of parameters are included in fig 7 a b for unsaturated and saturated zones respectively for examining the behaviour of the model parameters on the model results the standardized parameter values s as indicated in fig 7a b were obtained using eq 1 1 standardized parameter value s x x min x max x min here x value of model parameter for a particular evaluation out of evaluation set generated in auto calibration xmax and xmin maximum and minimum values of the model parameters generated in auto calibration process the parameters were standardized to have their all values in the feasible range of 0 1 from fig 7 a and b it is evident that standardized values of model parameters vary widely the parameters with lesser variation in the model evaluation sets can be considered more important in generation of the model output small variations in such parameters may cause significant variations in the model output the saturated hydraulic conductivity of unsaturated zone and horizontal as well as vertical hydraulic conductivity of saturated zone are less variable indicating that these parameters are more significant for general model behaviour i e generating model results in terms of stream flow and groundwater levels the less variability in hydraulic conductivity parameters may be owing to predominance of agricultural land and clayey soil in unsaturated zone wherein thick alternate sandy clay and clay bed layers forms the major part of the sub catchment particularly in the alluvium plain area the low hydraulic conductivity of soil geological layers in unsaturated saturated zone of the sub catchment may lead to slow movement of subsurface flows into the stream flows such slow movement of unsaturated saturated water is one of possible reasons for high evapotranspiration in the sub catchment such high evapotranspiration in combination of leached and weathered basalt from upper part of the sub catchment may be leading to high inland salinity in the alluvial plain along the banks of the purna river jain and tambe 2012 the parameters of unsaturated porous media like et surface depth θs θfc and θwp are relatively less significant vis à vis saturated hydraulic conductivity see fig 7 a similarly the parameters of saturated soil like sy and ss are less significant vis à vis hydraulic conductivity in x and y directions in generating the model outputs for saturated zone see fig 7 b 6 results and discussion the stream flows groundwater levels soil moisture within the root zone and components of water in surface sub surface and base flow were simulated with optimally calibrated parameters for calibration and validation period of 1991 1998 and 1999 2004 respectively the performance of calibrated model has been assessed with wide range of statistical performance indices i e mae rmse r nse and ei see table 3 and graphical plots are shown in figs 8 12 6 1 performance of calibrated model in prediction of stream flows 6 1 1 stream flows at terminal stream gauging station in table 3 the hydrological model shows satisfactory performance in terms of acceptable range of statistical parameters refsgaard and knudsen 1996 lorup et al 1998 moriasi et al 2007 wang et al 2012 in simulation of stream flows at monthly and daily time scales at the outlet of sub catchment for both calibration 1991 1998 and validation split sampling periods 1999 2004 the model slightly under predicts the stream flows during wet period and over predicts during the lean dry period at the outlet of sub catchments fig 8d such inconsistence performance in the model is possibly due to effect of minor storage structures which plays significant roles in the lean period the effects of such storage structures are getting diminished during the wet period as they are almost filled during the beginning of monsoon period the performance of calibrated model can be enhanced further if it represents the topography of the sub catchment at finer scale the distributed physics based model was not implemented at finer scale 250 m 250 m or so in the modelling due to limitation on part of available computational resources in the laboratory previously the model was developed using lumped parameters for unsaturated and saturated zones while using the finer grids of size 250 m 250 m loliyana and patel 2018 the computational effort invested in the previous modelling loliyana and patel 2018 and current modelling approaches are included in table 4 from table 4 it is clearly seen that distributed model even at coarser grids 500 m 500 m takes larger simulation time vis a vis distributed model with finer grids 250 m 250 m with lumped unsaturated and saturated flow parameters loliyana and patel 2018 the relative performance of the hydrological models i e mike she mike 11 model loliyana and patel 2018 and mike she mike 11 model of present study is included in table 5 the distributed physics based model with coarser grid 500 m 500 m gives better performance vis à vis distributed physics based model 250 m 250 m with lumped unsaturated and saturated flow parameters loliyana and patel 2018 table 5 6 1 2 stream flows at internal stream gauging stations to establish the applicability of calibrated distributed hydrological model in simulation of the stream flows within the sub catchment the calibrated model has been validated for two stream gauging stations namely gopalkheda and lakhpuri stream gauging stations upstream of yerli stream gauging station along the purna river within the sub catchment for period 1999 2004 the statistical performance indices are indicated in table 3 for assessing the quantitative performance of the model in prediction of stream flows at respective stream gauging stations the statistical performance indices for prediction of stream flows at internal stream gauging station are within satisfactory range of respective indices as described at footnote of table 3 the stream flow hydrographs of observed and simulated results on daily scale are graphically shown in fig 8 a b for years 1999 and 2002 respectively for gopalkheda stream gauging station also observed and simulated monthly hydrographs for simulation periods are included in fig 8 c the month wise observed and simulated runoff are included in fig 8 d from 8 a c it is seen that the peak outflows are invariably higher than simulated flows particularly for year 1999 on other hand simulated stream flows are invariably higher than observed flows for the dry lean period the performance of the model in prediction of monthly flows are better than daily flows thus developed model would be better suited in planning water resources at monthly scale within the sub catchment the low peak events are overpredicted from the model during the lean period wherein the minor storage structures account for significant storages such storage structures were specifically not taken into consideration in the modelling the same effects are seen in fig 8 d wherein early part of the monsoon june month storages in minor storage structures are developed in the sub catchment and thus available observed runoff are less than simulated runoff at the outlet of sub catchment the reverse is true for later part of monsoon july september months wherein such storage structures start releasing the flows in the streams as they are filled in early part of the monsoon in fig 8 e and f the stream flows at gopalkheda gauging station are classified as low flows q50 medium flows q10 q50 and high flows q10 for dependable flows 50 50 10 and 10 respectively clausen and biggs 2000 swain and patra 2017 respectively the higher values of ei 0 96 for daily and 1 00 for monthly time scales indicates satisfactory simulation of flow duration curve while comparing observed and simulated dependable flows for validation period fig 8 e f it is seen that performance of calibrated model is satisfactory for both high and medium flows 50 dependable flow vis à vis low flows 50 dependable flow the magnitude of high low and medium dependable flows observed and simulated at daily time scale for yerli gopalkheda and lakhpuri stations are attributed in table 6 it is evident that peak flow q0 1 quantiles are invariably under predicted while medium and low flow quantiles exhibit satisfactory agreement between observed and simulated stream flows the performance of the model in prediction of stream flows for dry period high dependable flows vis à vis wet period low dependable flows is rather poor due to non consideration of minor storage structure as stated earlier within the sub catchment further low stream flows 1 0 m3 s have been simulated as higher values than corresponding observed flows in the same range such inconsistency is attributed to the artefact of mike she mike 11 that does not allow for a river stream to dry out during the dry periods dai et al 2010 wang et al 2012 overall the calibrated model has been able to simulate the stream flows satisfactorily within the sub catchment from tables 3 5 and 6 the performance of distributed physics based model has been found to be significantly better due to consideration of grid to grid variations in lai rd distributed soil properties in unsaturated zone and distributed parameters of aquifers for saturated zone the results of stream flow prediction at both daily and monthly scales can be improved further by a integrating finer finite difference grid sizes i e 500 m 500 m due to availability fine resolution data within the sub catchment b consideration of high resolution gridded rainfall and pet data on daily scale as inputs into the model c including details of minor hydraulic structures within the sub catchment d incorporating refined cross sectional data of river and its tributaries 6 2 prediction of groundwater levels the performance of calibrated hydrological model has also been assessed using observed water levels of 49 observation wells within the study region see fig 9 from fig 9 it is evident that performance of the hydrological model is satisfactory in simulation of groundwater levels in hilly transition and flat areas within the sub catchment the model simulates ground water levels more effectively for pre monsoon period vis à vis post monsoon period also the hydrological model gives better performance in simulating the ground water levels in the plain areas vis à vis transitional and hilly areas for both calibration 1991 1998 and validation 1999 2004 periods the rmse values vary from 0 5 to 4 4 m 1 2 8 69 m and 1 9 14 2 m respectively in plain transition and hilly areas for pre monsoon season while rmse ranges between 0 8 and 5 2 m 1 8 14 6 m and 0 8 14 9 m in plain transition and hilly areas respectively for post monsoon season the performance trends of the hydrological model for both calibration and validation periods have been found to be similar see fig 9 fig 10 depicts comparison between observed and simulated time series of groundwater levels at few selected locations of wells within the sub catchment the inferior estimation of groundwater levels in hilly and transitional areas are due to a simulated groundwater levels were computed at the center of the model grids whereas actual observation wells may be located away from the grid centers b model is not able to account for the local heterogeneity at the scale smaller than the model grid size c non availability of comprehensive data of actual wells functioning in the agricultural fields village and urban centers the locations of wells were integrated in the model by presuming their locations at the center of each tehsil and village urban centers for irrigation and domestic industrial needs respectively d consideration of 1 d vertical flow in unsaturated zone may not be appropriate for hilly transitional areas due to existence of large differences in altitude at spatial scales and heterogeneity related to soil properties and geological units i e aquifer characteristics overall there is a significant improvement in prediction of groundwater levels within the sub catchment fig 10 compared to results reported during previous modeling exercise of lumped model loliyana and patel 2018 due to consideration of distributed vegetation soil and geological parameters within the sub catchment 6 3 prediction of soil moisture within the sub catchment prediction of soil moisture in the root zone was essential within the sub catchment which has the predominance of agricultural activities and semi arid in characteristics the spatially distributed approach used in present study defines very clearly the role of varying soil texture in prediction of soil moisture within the sub catchment the average soil moisture contents captured from satellite on monthly time scale table 1 are compared with simulated average soil moisture contents within the sub catchment with distributed soil parameters for period 1991 2004 the temporal variations of spatially averaged satellite based moisture content and simulated moisture content from the calibrated model for period 1991 2004 within bhaisdehi amravati malegaon and akola tehsils are depicted in fig 11 a d respectively for aforesaid input conditions from fig 11 it can clearly evident that simulated moisture contents from the models follow the trends of observed moisture contents especially for bhainsdehi amravati and akola tehsils the simulated moisture content at particular location may be subjected to the nature of input data namely rainfall potential evapotranspiration and soil moisture holding capacity at that particular location fig 12 a d depict scatter plots of simulated and satellite based soil moisture contents spatially averaged at monthly time scale for four tehsils within yerli sub catchment it is worth noting that average soil moisture contents at field capacity θfc and permanent wilting point θwp as reported by nbss lup for year 1994 within the sub catchment were 0 33 and 0 23 respectively nbss lup soil survey report 2008 from fig 11 a and b and 12 a b it is apparent that simulated and satellite based moisture contents are invariably higher than reported θwp for amravati and bhainsdehi tehsils even for dry period during october june months on other hand in malegaon and akola tehsils the simulated moisture contents are significantly lower than θwp for the dry months see fig 11 c and d and fig 12c d which require special attention of water resource managers for maintaining the moisture contents above threshold levels to obtain the desired crop yield the inconsistencies in prediction results of soil moisture content within the sub catchment even using distributed soil properties might be due to a consideration of two layer water balance et uz method vis à vis richard s equation b consideration of station based rainfall input vis à vis gridded rainfall data input c scaling heterogeneity of simulated and observed satellite based data d non consideration of local heterogeneity in soil and hydro geological characteristics at the scale finer than the model grid size e non applicability of 1 d vertical flow conditions in true sense in unsaturated zone for hilly transitional areas owing to large differences in altitude at spatial scales and heterogeneity in terms of soil and hydro geological properties for instance prediction of soil moisture contents in the malegaon tehsil falling in the transitional area of hilly to flood plain topography is much inferior on temporal basis fig 11c also for the same area the simulated gridded moisture contents are significantly under predicted see fig 12c during 1991 2004 the improved goodness of fit r2 on scatter plots is evidently seen for plain area i e akola tehsil r2 0 69 vis à vis transitional topographical area i e malegaon tehsil r2 0 47 in prediction of soil moisture within the sub catchment such information on the soil moisture contents within the root zone would be useful in planning the irrigation facilities within the sub catchment 6 4 water balance of yerli sub catchment water balance of a river basin includes the balance of input volume extraction volumes and volume of water available in the system and depends upon climatic and physiographic characteristics of the river basin the annual water balance components in unit mm of the yerli sub catchment for the calibration 1991 1998 and validation 1999 2004 periods derived as final output from the distributed model are depicted in table 7 fig 13 a and c indicate the annual water balance for years 1998 and 2000 respectively while fig 13 b and d represent the water balance for calibration and validation period respectively from fig 12 a and c and table 7 it is inferred that during dry years changes in water storage δs within the sub catchment becomes more negative deficient as during period of deficient rainfall in the sub catchment the water requirements are fulfilled due to excessive extraction of water from saturated unsaturated zones the estimated total water balance errors are in order of 0 86 and 1 09 for calibration 1991 1998 and validation 1999 2004 periods respectively table 7 indicating the adequacy of hydrological model in simulation of all hydrological processes stream flows aet unsaturated flow and groundwater flow satisfactorily the simulated surface runoff in terms of total inflow into the system for calibration and validation periods are 14 13 and 16 83 correspondingly the high evapotranspiration during calibration 78 of inflows and validation periods 77 of inflows are due to extensive agricultural activities within the sub catchment with semi arid characteristics an effective strategy in improving water use efficiency is to introduce water saving technologies like sprinkler irrigation drip irrigation and implement crop rotation policy within the sub catchment such measures may result in reducing the inundation irrigation within the sub catchment also it is noted that hydrological model developed in the current study simulated less water balance errors 0 86 for calibration and 1 09 for validation periods within the sub catchment table 7 compared to the water balance errors 1 14 for calibration and 1 27 for validation periods reported in the previous study loliyana and patel 2018 6 5 impact of changes in land use irrigation demand and groundwater draft on total water balance within the sub catchment the developed hydrological model has been used further in assessing the impact of land use changes groundwater draft and irrigation demands on total water balance within the sub catchment for periods i e 2005 2009 and 2010 2012 the classified land use land cover images of years 2009 and 2012 were used for further analyses to explore their linkages with hydrological processes during the simulation period of 2005 2009 and 2010 12 within the sub catchment the land use land cover was classified into nine major classes and their variability in terms of percentage of total area are shown in table 8 from table 8 it is evident that deciduous forest and scrubland were prominently converted into agricultural and urban lands across the sub catchment also it is reported in previous studies sharma et al 2018 that there is significant change in the demography of the sub catchment the population growth between two decades 1991 2011 was found to be 31 29 34 14 and 37 11 of amravati akola and buldana districts which is responsible for increase in irrigation water requirements domestic and industrial water demands within the sub catchment such reduction in forest cover increase in agricultural and urban land and increase in ground water irrigation draft and domestic industrial demand can have varied hydrological impacts which need to be investigated using the hydrological model developed in current study from table 9 it is seen that the simulated over land flow olr for periods 2005 2009 and 2009 2011 are 18 93 and 18 21 respectively of total inflow into the sub catchment which is relatively higher than overland flow of calibration 1991 1998 and validation 1999 2004 periods also the simulated values of evapotranspiration for period 2005 2009 71 19 and 2010 2012 71 10 are lesser than calibration and validation periods such increase olr and decrease in evapotranspiration are due to decrease in deciduous forest area and urbanization of the sub catchment further the decrease in available storage in root zone δuz is relatively higher during 2005 2009 2 08 and 2010 2012 2 72 due to increase in agricultural area within the sub catchment the positive storage in saturated zone δsz during 2005 2009 and 2010 2012 may be due to increase in surface storage for irrigation due to large olr and less pressure on ground water extraction from the saturated zone the exact reason for such positive storage in saturated zone in aforesaid period could not be investigated in current study due to non availability of surface storage and pattern of ground water extraction in the corresponding period 6 6 variation of simulated aet within yerli sub catchment the et on the land and vegetation surface is a central component of water cycle of a river basin understanding of et is useful in strengthening water use planning and management practices within a catchment the aet is basically governed by lai rd and soil properties of the catchment apart from meteorological characteristics like temperature wind velocity and solar radiation within a catchment the rd along with soil properties govern the ability of the vegetation to draw the water from soil the deeper roots can draw the soil moisture from bottom soil layers as and when the soil moisture is limited in upper layers the aet for all the months during the calibration 1991 1998 and validation periods 1999 2004 were estimated from the model the simulated aet for dry 2003 and wet 1994 years during winter january summer may and monsoon august months are shown in fig 14 from fig 14 it is apparent that simulated aet is highest during the august month followed by summer may month the maximum value of aet during the august month is due to availability of extensive vegetation and surface water during monsoon month in the sub catchment vis à vis during may and january months it is also noted that simulated aet for may month is lower than august month even though pet for former is higher than latter month identically such model outputs may be valuable for the local irrigation planners and autorities for management practices of water resources within the sub catchment 6 7 perspective of further for study the following research considerations need to be explored further in line with the work reported in present study i the performance of distributed physics based coupled mike she mike 11 model can be improved further while selecting the finer grids 500 m 500 m grid size provided available data are at finer resolutions by augmenting additional computational efforts in simulation of stream flows and groundwater levels within the sub catchment ii it would be interesting to derive the model with alternatives inputs i e finer scale gridded rainfall and potential evapotranspiration data to assess its influence at catchment scale hydrological modelling refining the developed model with aquifer boundary information in the saturated zone could improve the results in prediction of water balance within sub catchment detailed cross sections of purna river and its tributaries can be incorporated for more realistic output from the distributed model depending upon the data availability the richard s method can be incorporated in the hydrological model instead of using two layer water balance method for more descriptive information of vertical soil moisture profile and recharging the groundwater zone 7 conclusions the extensive data base of the study area i e yerli sub catchment of tapi basin india have been created by gathering information from different sources enumerated in table 1 few distributed parameters of the model were estimated as per the land use cover cropping pattern in the sub catchment while others were calibrated using multi objective optimization approach based on shuffle complex evolution sce algorithm the optimally calibrated parameters were in turn used in distributed physics based hydrological model for simulation of complete water balance of the study area the simulated stream flows and groundwater levels within the sub catchment were validated satisfactorily for available independent data of year 1999 2004 the key findings of current study are briefed as follows i the simulated stream flows and groundwater levels from distributed hydrological models have been found to be highly variable pertaining to saturated hydraulic conductivity ks of unsaturated zone horizontal kxx and vertical kyy hydraulic conductivity of saturated porous media instead the moisture content at saturation θs field capacity θfc wilting point θwp specific yield sy and specific storage ss have been found to be less significant in generating the stream flows and groundwater levels within the sub catchment ii distributed hydrological model with coarser grids 500 m 500 m performs better than distributed model with finer grids 250 m 250 m having lumped unsaturated and saturated flow parameters loliyana and patel 2018 in simulating stream flows and groundwater levels at the outlet and within sub catchment respectively iii the calibrated distributed model has been able to simulate stream flows and ground water levels satisfactorily for split sampling and proxy sampling tests rmse 98 79 m3 s r 0 84 nse 0 70 and ei 0 99 the calibrated hydrological model based on stream flows at the outlet of sub catchment even performs satisfactorily in simulating the flows within the sub catchment iv the calibrated model has been able to simulate soil moisture contents satisfactorily within the sub catchment particularly for alluvial plain the performance of the model requires further improvement in simulating soil moisture contents over hilly and transitional areas while incorporating richard s equation in unsaturated flow modeling v the simulated water balance of the semi arid yerli sub catchment revealed a high value of aet 78 of rainfall during the calibration and validation periods the percentage error in the water balance of calibrated and validated periods have been found to be 0 86 and 1 09 respectively which is marginally better than previous modelling results loliyana and patel 2018 the part of study area in the plain region have been reported to be saline due to combined effects of slow movement of sub surface flow high evapotranspiration and leaching of weather basalt from the hilly region vi the calibrated and validated hydrological model has been applied during application periods 2005 2009 and 2010 2012 it is seen that reduction in the deciduous forest cover and increase in urbanization during the application periods have led to increase in surface runoff and decrease in evapotranspiration within the sub catchment declaration of competing interest there is no any conflict of interest for the current manuscript content submitted by the authors acknowledgement authors are thankful to mhrd npiu teqip ii for providing the funding through centre of excellence coe project on water resources and flood management centre at svnit under which present investigation has been undertaken authors are also thankful to india meteorological department imd national remote sensing centre nrsc hyderabad national bureau of soil survey and land use planning nbss lup nagpur central water commission cwc tapi division for providing the data for present study appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104677 
26036,a physics based distributed integrated hydrological model in prediction of water balance of a semi arid catchment in india v d loliyana a c p l patel b a mechatronics systems private limited pune 411058 maharashtra india mechatronics systems private limited pune maharashtra 411058 india former research scholar center of excellence on water resources and flood management department of civil engineering sardar vallabhbhai national institute of technology svnit surat 395001 gujarat india b department of civil engineering sardar vallabhbhai national institute of technology svnit surat 395007 gujarat india department of civil engineering sardar vallabhbhai national institute of technology svnit surat gujarat 395007 india professor department of civil engineering sardar vallabhbhai national institute of technology svnit surat 395007 gujarat india c center of excellence on water resources and flood management department of civil engineering sardar vallabhbhai national institute of technology svnit surat 395007 gujarat india center of excellence on water resources and flood management department of civil engineering sardar vallabhbhai national institute of technology svnit surat gujarat 395007 india former research scholar center of excellence on water resources and flood management department of civil engineering sardar vallabhbhai national institute of technology svnit surat 395007 gujarat india corresponding author a physics based distributed coupled integrated hydrological model mike she mike 11 has been calibrated using a multi objective optimization approach for minimization of errors in prediction of stream flows and groundwater levels within a semi arid yerli sub catchment of upper tapi basin in india the performance of the calibrated model has been found to improve significantly in prediction of stream flows and groundwater levels within the sub catchment while incorporating distributed manning s roughness coefficient vegetation soil and geologic parameters in the model also the model has been able to simulate the soil moisture satisfactorily for the areas falling within the plain area of the sub catchment analyses of water balance period 1991 1998 1999 2004 2005 2009 and 2010 2012 revealed that developed integrated hydrological model could simulate better dynamics of hydrological processes in terms of change in land use groundwater draft and irrigation practices to maximize stakeholder sustainability graphical abstract image 1 keywords mike she mike 11 stream flows groundwater levels soil moisture content water balance yerli sub catchment software and data availability 1 mike she mike 11 software https www mikepoweredbydhi com used for simulation of spatially distributed hydrologic variables of the study area 2 arcgis 10 6 software http www esri in used for the preparation of gis database of the study area 3 erdas imagine 10 0 software https www hexagongeospatial com used for the analysis of satellite images spatially such as land use cover classification of the study area 4 archydro groundwater software https www aquaveo com used for the development of bathymetry of hydrogeological units of the study area the academic licenses of aforesaid softwares are available at computational hydraulics laboratory department civil engineering svnit surat india the softwares run on microsoft windows computer with no special hardware requirements data sources for all utilized explanatory variables are provided in table 1 please contact the corresponding author for any further information at premlalpatel1966 gmail com 1 introduction the conjunctive use of surface water and ground water is essentially required for sustainable management of water resources across river basins the unprecedented population growth in india during recent years particularly due to extensive urbanization led to over exploitation of surface and groundwater resources within the river basins the information on spatio temporal distributions of surface water moisture within the root zones and ground water levels would help water resource managers and agronomists for integrated management of water and optimizing the cropping pattern in the river basins lumped hydrological models have the inherent limitations to provide information of stream flows only at the outlet of catchments while physics based distributed integrated hydrological models can provide detailed description of hydrological processes within the catchments refsgaard 1997 modelling of surface unsaturated and saturated flows necessitate rigorous calibration of distributed parameters for prediction of stream flows groundwater levels and water balance within catchments henriksen et al 2003 vázquez et al 2009 foster and allen 2015 in the recent past physics based distributed hydrological models have become popular due to availability of extensive data for analyses and their capability in modelling the spatial heterogeneity within catchments demetriou and punthakey 1998 mcmichael et al 2006 mcmichael and hope 2007 liu et al 2007 rahim et al 2012 wijesekara et al 2014 mike she model a widely used distributed deterministic physics based hydrological model has been applied in catchments with large spatial heterogeneities jain et al 1992 singh et al 1999 madsen 2003 gupta et al 2008 wang et al 2012 keilholz et al 2015 khwairakpam et al 2018 yang et al 2000 compared three distributed physics based hydrological models for characterization of spatial variability 300 m 300 m in the seki river catchment area of 703 km2 japan the study showed that mike she is an effective and flexible tool for representing spatial heterogeneity and describing detailed hydrological processes vis à vis topmodel and gb models geomorphology based hydrological model the study suggested that incorporating split sample another period validation test and detailed remote sensing and gis data could improve the prediction of stream flows henriksen et al 2003 highlighted the methodological issues related to calibration and validation of mike she establishment of performance criteria parametrization and assessment of model parameters the study emphasized the need of testing calibrated distributed physics based models against independent observed data using split sample tests another period and proxy basin tests another area sahoo et al 2006 calibrated parameters of a lumped mike she model and recommended that consideration of physical heterogeneity in the model parameters could improve the runoff prediction within catchments huang et al 2010 examined hydrological processes in the tarim river basin using lumped mike she mike 11 model and recommended that continuous data in large time scale can describe the long term variations of hydrologic components within the study region rahim et al 2012 simulated total water balance of the paya indah wetlands watershed 242 21 km2 malasiya using distributed mike she grid sizes 200 m 200 m while calibrating the model from observed stream flows only within the watershed qin et al 2013 used an auto calibration tool developed by madsen 2003 in mike she for optimization of groundwater level predictions in large north china plain at monthly and annual time scales the simulated groundwater levels in the wells were compared satisfactorily with observed water levels average rmse of 19 44 m within the study region the water balance of the study area revealed that saturated depths in ground water aquifers were declining in the study region and potential evapotranspiration was higher than precipitation throughout the simulation period the study also recommended that water use efficiency could be improved by changing the cropping pattern and water saving technologies in the north china foster and allen 2015 predicted spatial seasonal interaction of ground water and surface water and predicted water balance of cowichan watershed area 930 km2 canada using detailed remote sensing data base and mike she mike 11 modelling tool vo and goursbille 2016 applied mike she mike 11 model on the vu gia thu bon catchment area of 10 350 km2 for predictions of stream flow and water levels the model was calibrated for distributed manning s m and soil parameter ks with lumped saturated zone parameter kh for period 1991 2000 the study suggested that surface water and groundwater predictions are affected significantly by the choice of distributed manning s m in the model the study acknowledged the source of uncertainty due to coarser resolution of topographic data lack of measured cross sections spatial variation of rainfall data and quality of groundwater data within the catchment loliyana and patel 2018 simulated hydrological processes for the yerli sub catchment in tapi basin india using a coupled mike she mike 11 model the sensitivity of thirteen lumped model parameters of overland unsaturated saturated flow were analyzed while simulating the runoﬀ volume peak runoﬀ at the catchment outlet and groundwater levels within the catchment the study suggested that further consideration of grid to grid variations in leaf area index lai root depth rd soil properties aquifer and unsaturated soil parameters could improve the performance of the model the distributed hydrological model presented in the current study addresses the following research objectives a calibration of a distributed hydrological model using multi objective approach i e minimizing the error in prediction of stream flows and ground water levels within the sub catchment the applicability of the calibrated model is validated with the data of independent period within the sub catchment b effect of grid size on performance of the distributed hydrological has been investigated with reference to accuracy and computational efforts the performance of the model is also compared with another model wherein the parameters of saturated zone were considered as lumped in nature c the calibrated and validated hydrological model has been used for assessing the impact of land use changes on water balance within the sub catchment 2 study area the tapi river is one of the major west flowing rivers of india draining into the arabian sea and passes through madhya pradesh maharashtra and gujarat states having a catchment area of 65 145 km2 the entire tapi basin can be subdivided into three sub basins i e upper tapi basin from origin to hathnur dam 29 430 km2 middle tapi basin from hathnur dam to gidhade stream gauging station 25 320 km2 and lower tapi basin from gidhade stream gauging station to the arabian sea 10395 km2 timbadiya et al 2015 there are two major reservoirs viz ukai and hathnur upstream of ukai reservoirs on the tapi river to meet the domestic industrial and irrigation demands in their respective command areas the purna river is one of the major tributaries of tapi river having its confluence just upstream of hathnur reservoir originating from gawilgarh steep mountains of the eastern satpura range betul district of madhya pradesh see fig 1 the purna river catchment lies between latitudes 20 08 31 to 21 40 30 n and longitudes 75 56 25 to 77 56 46 e having basaltic geology in peripheral part and alluvial formation in central region the ten major tributaries pedhi kate purna chandrabhaga mohali morna mun wan ghan biswa and wagar also drain into the purna river length 360 km up to the confluence with the tapi river the study area of present study includes the purna river catchment up to the yerli stream gauging station would be referred as yerli sub catchment in further discussions draining through the madhya pradesh and maharashtra states the elevation in the sub catchment varies from 213 to 1171 m above the mean sea level fig 2 a the northern portion of the yerli sub catchment has mountainous topography with the highest elevation point 1171 m located at the north east of the sub catchment conversely the point of minimum elevation is at 213 m located at the outlet of sub catchment yerli gauging station the mean elevation of the sub catchment is 372 6 m the area elevation curve i e hypsometric curve fig 2b highlight the flat characteristics of the yerli sub catchment it is evident that 65 of sub catchment area is below mean elevation of 372 6 m while 80 is below 450 m the soils in the sub catchment are mainly dark brown clayey loamy fine textured and sticky in nature the principal rock formation consists of quaternary sediments alluvial deposits upper gondwanas and deccan basalt underlying hills and terraces in the study area average annual 1991 2004 potential evapotranspiration pet in the catchment is 1715 mm and its aridity index i e ratio of rainfall to pet is 0 46 the climate in the study area is thus classified as semi arid as the potential evapotranspiration is 1 6 times to 3 17 times of annual rainfall loliyana and patel 2018 the daily minimum and maximum temperature ranges from 10 c in january month to 48 c in may month respectively in the sub catchment the major land use class in the sub catchment is agriculture followed by deciduous forest some of major crops grown are cotton sorghum green gram black gram pigeon pea during the kharif season while horse gram safflower and wheat are cultivated during the rabi season in the study area jain and tambe 2012 3 material the details of input data viz topographical climatic hydrologic land use land cover soil geological and groundwater data for distributed hydrological modelling of the study domain are described in table 1 square grids of size 500 m 500 m 63 524 finite difference grids has been generated over the study area in formulation of mike she model fig 2a includes the shuttle radar topography mission srtm digital elevation model dem describing the topographical variation and river drainage network in the yerli sub catchment the thiessen polygon approach has been used to obtain the areal rainfall over the sub catchment using the data of seventeen rain gauge stations fig 2c the weather data of three stations falling in three districts were used to obtain time series of pet of respective stations using penman s method penman 1948 the time series of pet of each weather station were distributed to different grids depending upon their locations in respective districts in the sub catchment fig 2d for computation of actual evapotranspiration aet from the model the hydrological data including observed stream flows and water levels and river cross sections at yerli gopalkheda and lakhpuri stream gauging stations were used to calibrate and assess the performance of coupled mike she and mike 11 hydrological model the satellite imagery of year 2000 were classified in erdas imagine and nine types of land use cover were identified fig 3 a the classified land use cover were in turn used for obtaining the distributed manning s roughness parameter m inverse of manning s roughness coefficient n for computation of overland flow in the model also the variation of lai and rd of crops were temporally distributed within the sub catchment as per the cropping pattern in the study areas see fig 3b 3c the soil series data were digitized in vector format in arcgis 10 6 as shown in fig 3d e the available soil data for the study area frequently comprises of 37 soil series whereas soil within each series differ significantly based on local variances such as geology and topography fig 3d e shows the soil characteristics i e class and depth present within the yerli sub catchment the alluvium around 7800 km2 41 area and basalt are the parent material of the soils in the yerli sub catchment while clayey and loamy soils are the dominant surface texture present within the sub catchment the vertisols soil in purna alluvium contains 50 clay out of this 30 60 is fine clay 0 2 mm the fine clay to total clay ratio in the catchment ranged between 0 3 and 0 6 and usually increases with depth the high content of expansive clay vertisols in the sub catchment known as montmorillonite forms deep cracks in dry period of the year the alternate shrinking and swelling characteristics of such soil causes self mulching wherein the soil material is mixed evenly within the soil mass the occurrence of the sub surface salinity in the sub catchment could be ascribed due to cracking nature of these soils the soil depths in the yerli sub catchment varies from 10 to 150 cm presence of soil of high productivity 62 with moderate erosion 67 indicates the dominance of agricultural activity in the sub catchment jain and tambe 2012 the exploratory well data table 1 showing the profile of soil and rock formation below the ground have been used for creating bathymetry of groundwater zone up to 200 m while the data of observation wells were used for comparing simulated water levels in distributed hydrological modeling in present modelling approach the lower boundary of the saturated zone has been considered as bed rock surface basalt rock layer varies from 38 m to 200 m additionally four geological units sand gravel clay sandy clay and weather basalt have been added as geological lenses discontinuous vertical layers within the bathymetry of saturated zone in the mike she set up the alluvial deposit ranges from the ages of lower pleistocene to recent quaternary age within the yerli sub catchment fig 4 a illustrates the detailed information of geological lithology in the yerli sub catchment the alluvium which covers 7800 km2 area of the yerli sub catchment forms the principal water bearing formation wherein granular zones were encountered at various depths in unconfined as well as confined conditions the purna alluvium can be divided into two hydrogeological layers i e younger alluvium extending down to around 80 m depth below the ground level forming potential aquifer comprising alternate beds of clay and sand and older alluvium extending up to 300 m depth below the ground level cgwb cgwb districts report 2013 see fig 4 the cgwb report 2013 indicates that basalt deposits are available as weathered jointed vesicular massive basaltic formation where water bearing formation exists in semi confined to confined conditions the weathered and fractured parts of basaltic chips exist up to 15 20 m 20 25 m and 15 20 m depth below the ground level in amravati akola and buldana districts respectively wherein ground water occurs under unconfined to confined conditions the basaltic region at deeper level occurring at confined conditions does not form the potential aquifer region in purna alluvium around 2760 km2 area 17 4 of total area is affected by inland salinity as shown in fig 4a the sample geological cross sections aa bb cc dd and ee obtained from lithology information of exploratory and observation wells are included in fig 4b f while observing the lithologs along aa right bank of purna river fig 4b it is seen that deep geological layer mainly consists of basaltic formation followed by clayey formation at upper levels the geological formation nearing to the ground surface include thin sand gravel and sandy clay layers along bb left bank of purna river fig 4c the geological formation is dominated by basaltic and weather basalt formations the layer close to the ground surface is dominated by thin sandy clay clay and sand gravel layers due to existence of thick layers of clay in alluvium of yerli sub catchment on either side of river see fig 4d e and f there is slow movement of groundwater in the region the hydrological parameters related to ground water aquifers viz horizontal hydraulic conductivity kxx vertical hydraulic conductivity kyy specific yield sy and specific storage ss had been selected for calibration in present study 4 model description and methodology 4 1 mike she mike 11 model mike she système hydrologique europeén is a distributed physics based deterministic macro scale hydrological model operates at grid level the hydrological model is capable of both continuous and event based analyses to predict water yield in large complex catchments with varying land use soils and management practices dhi 2017 it offers flexibility in choosing various alternative for modelling each component of the hydrological processes in mike she a finite difference method is used in solving 2d saint venant equations as diffusive wave approximation i e partial differential equations describing the overland flow process 2 layer water balance for computation of unsaturated flow 3d darcy s equation for estimating saturated flow and analytical solutions are used to compute interception and actual evapotranspiration within the study domain dhi 2017 in mike 11 1 dimensional saint venant s kinematic wave routing equations are solved for computation of river discharge and water levels at q and h points respectively in the flow domain dhi 2017 the mike she hydrological model and mike 11 hydrodynamic model are coupled at h points along the river network while the flow exchange between the surface water in the river and groundwater is estimated as the difference in water table level in the adjacent grid cell and the river cell multiplied by the conductance value between the two cells the snowmelt component has been ignored in current study as the study area sub catchment is free from snowfall complete description of mike she mike 11 modelling system are available else where dhi 2017 4 2 methodology the process description of mike she in developing distributed hydrological model including overlaying of background maps simulation set up creation of model set up loading of dem meteorological and lulc parameters input coupling of mike she and mike 11 specifying the parameters of unsaturated and saturated zones boundary conditions specifying output variables autocalibration of model parameters and finally the validation of calibrated hydrological model is shown in the form of flow chart see fig 5 further the details of irrigation and groundwater extraction components initial and boundary conditions including time steps for simulation are described elsewhere loliyana and patel 2018 the performance of the mike she and mike 11 coupled model under calibration validation stages were evaluated by comparing simulated stream flow values with their corresponding observed values in terms of statistical performance indices like mean absolute error mae correlation coefficient r nash sutcliffe efficiency nse root mean squared error rmse and efficiency index ei loliyana and patel 2015 5 selection of model parameters the distributed manning s m values to each grid were assigned as per land use and land cover lulc fig 3a while the leaf area index lai and root zone depth rd and crop coefficient kc were distributed as per cropping vegetation pattern within the sub catchment see fig 3 a and b the manning s m values for different land use pattern in sub catchment were selected from available information in literature engman 1986 chow 1959 vieux 2001 kothyari et al 2010 the lai rd and kc are key parameters of crop vegetation used for estimation of actual evapotranspiration for the mike she model the values of lai and rd were selected based on available information in literature foster and allen 2015 wijesekara et al 2014 allen et al 1998 for different crops and stages of their growth based on the cropping pattern jain and tambe 2012 in the sub catchment the lai rd and kc values of crops and fallow lands were estimated for different months in a year and the time series of their respective values for the whole year were given as inputs into the mike she model see fig 3b c the lai rd and kc values for other land use land covers except for crop and fallow land were given constant values for the whole year as per the values based on their classification from satellite imagery of year 2000 see table in fig 3a also the value of drainage depth dd and drainage time constant dt of saturated zones were taken from loliyana and patel 2018 as they were found non sensitive in the previous study the parameter et surface depth is a calibrated parameter which is used for estimation of aet in the two layer water balance method being used in present study the remaining model parameters et uz and sz components of the mike she model forty one parameters were calibrated and validated for simulation of stream flows at the yerli stream gauging station and groundwater levels within the sub catchment for periods 1991 1998 and 1999 2004 respectively see table 2 initial values of the model parameters were chosen based on existing conditions in the sub catchment available information in the literature maidment 1988 todd and mays 2004 and subramanya 2011 the auto calibration was preferred as the information available about the catchment hydrology was limited mike she mike 11 uses the auto calibration process based on shuffle complex evolution sce algorithm for multiple objective functions and multi objective optimization the objective functions are i minimization of rmse in prediction of stream flow at the outlet of sub catchment i e minimum rmse of daily discharges at yerli station and ii rmse of groundwater levels within the sub catchment total 1500 model evaluations were performed for ascertaining optimal values of forty one calibration parameters the optimal values of the model parameters were obtained by plotting the rmse values of daily stream flows and groundwater levels and picking the model parameters for the run out of 1500 model evaluation demonstrating the minimal rmses of both the objective functions fig 6 the details of optional model parameters are included in table 2 in table 2 the calibrated value of et surface depth representing the thickness of capillary fringe above ground water table has been found to be 1 52 m relatively higher values of capillary fringe is indicative of high clay content in the soil of the yerli sub catchment in unsaturated zone θs in the range of 0 30 0 51 indicates maximum water available in the root zone for evapotranspiration and percolation in clay loam sandy clay sandy loam and silty clay soils whereas θfc 0 43 0 29 0 27 0 11 0 33 demonstrate the maximum water content available in the root zone to meet the vegetative evapotranspirational requirements for clay loam sandy clay sandy loam and silty clay soils respectively θwp 0 28 0 15 0 16 0 06 0 19 specifies the limit of water contents in unsaturated zone up to which the plants can extract moisture from the soil without wilting for clay loam sandy clay sandy loam and silty clay soils respectively the higher values of θs θfc and θwp for clay soil represent more water stored in pores of such soil due to high porosity while lower values in the sandy soil are due to less pore space available between particles of such soil the ks 6 9 10 8 8 7 10 5 4 3 10 6 1 6 10 4 5 7 10 7 m s indicate hydraulic conductivity of clay loam sandy clay sandy loam and silty clay soils respectively in the sub catchment in the unsaturated zone the lower hydraulic conductivity values in unsaturated zone indicates the dominance of agriculture and clayey soil within the sub catchment the clayey soil has high vales of et depth and more water in the pores in root zone which is responsible for higher evapotranspiration from the unsaturated zone in the sub catchment sandy soil is having higher hydraulic conductivity vis à vis other type of soils due to better connectivity of pore spaces in the former soil which may lead to high infiltration rate for saturated zone calibrated values of kxx 4 2 10 3 4 8 10 7 9 3 10 6 5 9 10 6 6 1 10 6 m s and kyy 6 1 10 4 7 2 10 8 4 9 10 7 2 7 10 7 3 6 10 7 m s exhibit horizontal and vertical hydraulic conductivity for sand gravel clay sandy clay weather basalt and basalt geological layers respectively the groundwater movement in the sub catchment in general is sluggish due to presence of low permeable alternate bed layers like clay and weather basalt in the major part of the sub catchment much longer residence time in the aquifers is in turn responsible for larger evapotranspiration losses in the sub catchment the dd exhibit the recession part of discharge hydrograph whereas dt governs the velocity of the drainage water from saturated zone higher values of dd and dt indicate larger contribution from base flow and higher drainage velocity i e large peak respectively the dd 1 4 m and dt 10 7 s 1 indicate moderate influence of base flow in stream runoff and slow drainage velocity due to relatively shallow water table depth near the bank of purna river and larger clay content in the soil loliyana and patel 2018 in order to assess the significance of model parameters in hydrological modelling the variation of parameter sets for 1500 model evaluations attained through auto calibration along with balanced optimum value of parameters are included in fig 7 a b for unsaturated and saturated zones respectively for examining the behaviour of the model parameters on the model results the standardized parameter values s as indicated in fig 7a b were obtained using eq 1 1 standardized parameter value s x x min x max x min here x value of model parameter for a particular evaluation out of evaluation set generated in auto calibration xmax and xmin maximum and minimum values of the model parameters generated in auto calibration process the parameters were standardized to have their all values in the feasible range of 0 1 from fig 7 a and b it is evident that standardized values of model parameters vary widely the parameters with lesser variation in the model evaluation sets can be considered more important in generation of the model output small variations in such parameters may cause significant variations in the model output the saturated hydraulic conductivity of unsaturated zone and horizontal as well as vertical hydraulic conductivity of saturated zone are less variable indicating that these parameters are more significant for general model behaviour i e generating model results in terms of stream flow and groundwater levels the less variability in hydraulic conductivity parameters may be owing to predominance of agricultural land and clayey soil in unsaturated zone wherein thick alternate sandy clay and clay bed layers forms the major part of the sub catchment particularly in the alluvium plain area the low hydraulic conductivity of soil geological layers in unsaturated saturated zone of the sub catchment may lead to slow movement of subsurface flows into the stream flows such slow movement of unsaturated saturated water is one of possible reasons for high evapotranspiration in the sub catchment such high evapotranspiration in combination of leached and weathered basalt from upper part of the sub catchment may be leading to high inland salinity in the alluvial plain along the banks of the purna river jain and tambe 2012 the parameters of unsaturated porous media like et surface depth θs θfc and θwp are relatively less significant vis à vis saturated hydraulic conductivity see fig 7 a similarly the parameters of saturated soil like sy and ss are less significant vis à vis hydraulic conductivity in x and y directions in generating the model outputs for saturated zone see fig 7 b 6 results and discussion the stream flows groundwater levels soil moisture within the root zone and components of water in surface sub surface and base flow were simulated with optimally calibrated parameters for calibration and validation period of 1991 1998 and 1999 2004 respectively the performance of calibrated model has been assessed with wide range of statistical performance indices i e mae rmse r nse and ei see table 3 and graphical plots are shown in figs 8 12 6 1 performance of calibrated model in prediction of stream flows 6 1 1 stream flows at terminal stream gauging station in table 3 the hydrological model shows satisfactory performance in terms of acceptable range of statistical parameters refsgaard and knudsen 1996 lorup et al 1998 moriasi et al 2007 wang et al 2012 in simulation of stream flows at monthly and daily time scales at the outlet of sub catchment for both calibration 1991 1998 and validation split sampling periods 1999 2004 the model slightly under predicts the stream flows during wet period and over predicts during the lean dry period at the outlet of sub catchments fig 8d such inconsistence performance in the model is possibly due to effect of minor storage structures which plays significant roles in the lean period the effects of such storage structures are getting diminished during the wet period as they are almost filled during the beginning of monsoon period the performance of calibrated model can be enhanced further if it represents the topography of the sub catchment at finer scale the distributed physics based model was not implemented at finer scale 250 m 250 m or so in the modelling due to limitation on part of available computational resources in the laboratory previously the model was developed using lumped parameters for unsaturated and saturated zones while using the finer grids of size 250 m 250 m loliyana and patel 2018 the computational effort invested in the previous modelling loliyana and patel 2018 and current modelling approaches are included in table 4 from table 4 it is clearly seen that distributed model even at coarser grids 500 m 500 m takes larger simulation time vis a vis distributed model with finer grids 250 m 250 m with lumped unsaturated and saturated flow parameters loliyana and patel 2018 the relative performance of the hydrological models i e mike she mike 11 model loliyana and patel 2018 and mike she mike 11 model of present study is included in table 5 the distributed physics based model with coarser grid 500 m 500 m gives better performance vis à vis distributed physics based model 250 m 250 m with lumped unsaturated and saturated flow parameters loliyana and patel 2018 table 5 6 1 2 stream flows at internal stream gauging stations to establish the applicability of calibrated distributed hydrological model in simulation of the stream flows within the sub catchment the calibrated model has been validated for two stream gauging stations namely gopalkheda and lakhpuri stream gauging stations upstream of yerli stream gauging station along the purna river within the sub catchment for period 1999 2004 the statistical performance indices are indicated in table 3 for assessing the quantitative performance of the model in prediction of stream flows at respective stream gauging stations the statistical performance indices for prediction of stream flows at internal stream gauging station are within satisfactory range of respective indices as described at footnote of table 3 the stream flow hydrographs of observed and simulated results on daily scale are graphically shown in fig 8 a b for years 1999 and 2002 respectively for gopalkheda stream gauging station also observed and simulated monthly hydrographs for simulation periods are included in fig 8 c the month wise observed and simulated runoff are included in fig 8 d from 8 a c it is seen that the peak outflows are invariably higher than simulated flows particularly for year 1999 on other hand simulated stream flows are invariably higher than observed flows for the dry lean period the performance of the model in prediction of monthly flows are better than daily flows thus developed model would be better suited in planning water resources at monthly scale within the sub catchment the low peak events are overpredicted from the model during the lean period wherein the minor storage structures account for significant storages such storage structures were specifically not taken into consideration in the modelling the same effects are seen in fig 8 d wherein early part of the monsoon june month storages in minor storage structures are developed in the sub catchment and thus available observed runoff are less than simulated runoff at the outlet of sub catchment the reverse is true for later part of monsoon july september months wherein such storage structures start releasing the flows in the streams as they are filled in early part of the monsoon in fig 8 e and f the stream flows at gopalkheda gauging station are classified as low flows q50 medium flows q10 q50 and high flows q10 for dependable flows 50 50 10 and 10 respectively clausen and biggs 2000 swain and patra 2017 respectively the higher values of ei 0 96 for daily and 1 00 for monthly time scales indicates satisfactory simulation of flow duration curve while comparing observed and simulated dependable flows for validation period fig 8 e f it is seen that performance of calibrated model is satisfactory for both high and medium flows 50 dependable flow vis à vis low flows 50 dependable flow the magnitude of high low and medium dependable flows observed and simulated at daily time scale for yerli gopalkheda and lakhpuri stations are attributed in table 6 it is evident that peak flow q0 1 quantiles are invariably under predicted while medium and low flow quantiles exhibit satisfactory agreement between observed and simulated stream flows the performance of the model in prediction of stream flows for dry period high dependable flows vis à vis wet period low dependable flows is rather poor due to non consideration of minor storage structure as stated earlier within the sub catchment further low stream flows 1 0 m3 s have been simulated as higher values than corresponding observed flows in the same range such inconsistency is attributed to the artefact of mike she mike 11 that does not allow for a river stream to dry out during the dry periods dai et al 2010 wang et al 2012 overall the calibrated model has been able to simulate the stream flows satisfactorily within the sub catchment from tables 3 5 and 6 the performance of distributed physics based model has been found to be significantly better due to consideration of grid to grid variations in lai rd distributed soil properties in unsaturated zone and distributed parameters of aquifers for saturated zone the results of stream flow prediction at both daily and monthly scales can be improved further by a integrating finer finite difference grid sizes i e 500 m 500 m due to availability fine resolution data within the sub catchment b consideration of high resolution gridded rainfall and pet data on daily scale as inputs into the model c including details of minor hydraulic structures within the sub catchment d incorporating refined cross sectional data of river and its tributaries 6 2 prediction of groundwater levels the performance of calibrated hydrological model has also been assessed using observed water levels of 49 observation wells within the study region see fig 9 from fig 9 it is evident that performance of the hydrological model is satisfactory in simulation of groundwater levels in hilly transition and flat areas within the sub catchment the model simulates ground water levels more effectively for pre monsoon period vis à vis post monsoon period also the hydrological model gives better performance in simulating the ground water levels in the plain areas vis à vis transitional and hilly areas for both calibration 1991 1998 and validation 1999 2004 periods the rmse values vary from 0 5 to 4 4 m 1 2 8 69 m and 1 9 14 2 m respectively in plain transition and hilly areas for pre monsoon season while rmse ranges between 0 8 and 5 2 m 1 8 14 6 m and 0 8 14 9 m in plain transition and hilly areas respectively for post monsoon season the performance trends of the hydrological model for both calibration and validation periods have been found to be similar see fig 9 fig 10 depicts comparison between observed and simulated time series of groundwater levels at few selected locations of wells within the sub catchment the inferior estimation of groundwater levels in hilly and transitional areas are due to a simulated groundwater levels were computed at the center of the model grids whereas actual observation wells may be located away from the grid centers b model is not able to account for the local heterogeneity at the scale smaller than the model grid size c non availability of comprehensive data of actual wells functioning in the agricultural fields village and urban centers the locations of wells were integrated in the model by presuming their locations at the center of each tehsil and village urban centers for irrigation and domestic industrial needs respectively d consideration of 1 d vertical flow in unsaturated zone may not be appropriate for hilly transitional areas due to existence of large differences in altitude at spatial scales and heterogeneity related to soil properties and geological units i e aquifer characteristics overall there is a significant improvement in prediction of groundwater levels within the sub catchment fig 10 compared to results reported during previous modeling exercise of lumped model loliyana and patel 2018 due to consideration of distributed vegetation soil and geological parameters within the sub catchment 6 3 prediction of soil moisture within the sub catchment prediction of soil moisture in the root zone was essential within the sub catchment which has the predominance of agricultural activities and semi arid in characteristics the spatially distributed approach used in present study defines very clearly the role of varying soil texture in prediction of soil moisture within the sub catchment the average soil moisture contents captured from satellite on monthly time scale table 1 are compared with simulated average soil moisture contents within the sub catchment with distributed soil parameters for period 1991 2004 the temporal variations of spatially averaged satellite based moisture content and simulated moisture content from the calibrated model for period 1991 2004 within bhaisdehi amravati malegaon and akola tehsils are depicted in fig 11 a d respectively for aforesaid input conditions from fig 11 it can clearly evident that simulated moisture contents from the models follow the trends of observed moisture contents especially for bhainsdehi amravati and akola tehsils the simulated moisture content at particular location may be subjected to the nature of input data namely rainfall potential evapotranspiration and soil moisture holding capacity at that particular location fig 12 a d depict scatter plots of simulated and satellite based soil moisture contents spatially averaged at monthly time scale for four tehsils within yerli sub catchment it is worth noting that average soil moisture contents at field capacity θfc and permanent wilting point θwp as reported by nbss lup for year 1994 within the sub catchment were 0 33 and 0 23 respectively nbss lup soil survey report 2008 from fig 11 a and b and 12 a b it is apparent that simulated and satellite based moisture contents are invariably higher than reported θwp for amravati and bhainsdehi tehsils even for dry period during october june months on other hand in malegaon and akola tehsils the simulated moisture contents are significantly lower than θwp for the dry months see fig 11 c and d and fig 12c d which require special attention of water resource managers for maintaining the moisture contents above threshold levels to obtain the desired crop yield the inconsistencies in prediction results of soil moisture content within the sub catchment even using distributed soil properties might be due to a consideration of two layer water balance et uz method vis à vis richard s equation b consideration of station based rainfall input vis à vis gridded rainfall data input c scaling heterogeneity of simulated and observed satellite based data d non consideration of local heterogeneity in soil and hydro geological characteristics at the scale finer than the model grid size e non applicability of 1 d vertical flow conditions in true sense in unsaturated zone for hilly transitional areas owing to large differences in altitude at spatial scales and heterogeneity in terms of soil and hydro geological properties for instance prediction of soil moisture contents in the malegaon tehsil falling in the transitional area of hilly to flood plain topography is much inferior on temporal basis fig 11c also for the same area the simulated gridded moisture contents are significantly under predicted see fig 12c during 1991 2004 the improved goodness of fit r2 on scatter plots is evidently seen for plain area i e akola tehsil r2 0 69 vis à vis transitional topographical area i e malegaon tehsil r2 0 47 in prediction of soil moisture within the sub catchment such information on the soil moisture contents within the root zone would be useful in planning the irrigation facilities within the sub catchment 6 4 water balance of yerli sub catchment water balance of a river basin includes the balance of input volume extraction volumes and volume of water available in the system and depends upon climatic and physiographic characteristics of the river basin the annual water balance components in unit mm of the yerli sub catchment for the calibration 1991 1998 and validation 1999 2004 periods derived as final output from the distributed model are depicted in table 7 fig 13 a and c indicate the annual water balance for years 1998 and 2000 respectively while fig 13 b and d represent the water balance for calibration and validation period respectively from fig 12 a and c and table 7 it is inferred that during dry years changes in water storage δs within the sub catchment becomes more negative deficient as during period of deficient rainfall in the sub catchment the water requirements are fulfilled due to excessive extraction of water from saturated unsaturated zones the estimated total water balance errors are in order of 0 86 and 1 09 for calibration 1991 1998 and validation 1999 2004 periods respectively table 7 indicating the adequacy of hydrological model in simulation of all hydrological processes stream flows aet unsaturated flow and groundwater flow satisfactorily the simulated surface runoff in terms of total inflow into the system for calibration and validation periods are 14 13 and 16 83 correspondingly the high evapotranspiration during calibration 78 of inflows and validation periods 77 of inflows are due to extensive agricultural activities within the sub catchment with semi arid characteristics an effective strategy in improving water use efficiency is to introduce water saving technologies like sprinkler irrigation drip irrigation and implement crop rotation policy within the sub catchment such measures may result in reducing the inundation irrigation within the sub catchment also it is noted that hydrological model developed in the current study simulated less water balance errors 0 86 for calibration and 1 09 for validation periods within the sub catchment table 7 compared to the water balance errors 1 14 for calibration and 1 27 for validation periods reported in the previous study loliyana and patel 2018 6 5 impact of changes in land use irrigation demand and groundwater draft on total water balance within the sub catchment the developed hydrological model has been used further in assessing the impact of land use changes groundwater draft and irrigation demands on total water balance within the sub catchment for periods i e 2005 2009 and 2010 2012 the classified land use land cover images of years 2009 and 2012 were used for further analyses to explore their linkages with hydrological processes during the simulation period of 2005 2009 and 2010 12 within the sub catchment the land use land cover was classified into nine major classes and their variability in terms of percentage of total area are shown in table 8 from table 8 it is evident that deciduous forest and scrubland were prominently converted into agricultural and urban lands across the sub catchment also it is reported in previous studies sharma et al 2018 that there is significant change in the demography of the sub catchment the population growth between two decades 1991 2011 was found to be 31 29 34 14 and 37 11 of amravati akola and buldana districts which is responsible for increase in irrigation water requirements domestic and industrial water demands within the sub catchment such reduction in forest cover increase in agricultural and urban land and increase in ground water irrigation draft and domestic industrial demand can have varied hydrological impacts which need to be investigated using the hydrological model developed in current study from table 9 it is seen that the simulated over land flow olr for periods 2005 2009 and 2009 2011 are 18 93 and 18 21 respectively of total inflow into the sub catchment which is relatively higher than overland flow of calibration 1991 1998 and validation 1999 2004 periods also the simulated values of evapotranspiration for period 2005 2009 71 19 and 2010 2012 71 10 are lesser than calibration and validation periods such increase olr and decrease in evapotranspiration are due to decrease in deciduous forest area and urbanization of the sub catchment further the decrease in available storage in root zone δuz is relatively higher during 2005 2009 2 08 and 2010 2012 2 72 due to increase in agricultural area within the sub catchment the positive storage in saturated zone δsz during 2005 2009 and 2010 2012 may be due to increase in surface storage for irrigation due to large olr and less pressure on ground water extraction from the saturated zone the exact reason for such positive storage in saturated zone in aforesaid period could not be investigated in current study due to non availability of surface storage and pattern of ground water extraction in the corresponding period 6 6 variation of simulated aet within yerli sub catchment the et on the land and vegetation surface is a central component of water cycle of a river basin understanding of et is useful in strengthening water use planning and management practices within a catchment the aet is basically governed by lai rd and soil properties of the catchment apart from meteorological characteristics like temperature wind velocity and solar radiation within a catchment the rd along with soil properties govern the ability of the vegetation to draw the water from soil the deeper roots can draw the soil moisture from bottom soil layers as and when the soil moisture is limited in upper layers the aet for all the months during the calibration 1991 1998 and validation periods 1999 2004 were estimated from the model the simulated aet for dry 2003 and wet 1994 years during winter january summer may and monsoon august months are shown in fig 14 from fig 14 it is apparent that simulated aet is highest during the august month followed by summer may month the maximum value of aet during the august month is due to availability of extensive vegetation and surface water during monsoon month in the sub catchment vis à vis during may and january months it is also noted that simulated aet for may month is lower than august month even though pet for former is higher than latter month identically such model outputs may be valuable for the local irrigation planners and autorities for management practices of water resources within the sub catchment 6 7 perspective of further for study the following research considerations need to be explored further in line with the work reported in present study i the performance of distributed physics based coupled mike she mike 11 model can be improved further while selecting the finer grids 500 m 500 m grid size provided available data are at finer resolutions by augmenting additional computational efforts in simulation of stream flows and groundwater levels within the sub catchment ii it would be interesting to derive the model with alternatives inputs i e finer scale gridded rainfall and potential evapotranspiration data to assess its influence at catchment scale hydrological modelling refining the developed model with aquifer boundary information in the saturated zone could improve the results in prediction of water balance within sub catchment detailed cross sections of purna river and its tributaries can be incorporated for more realistic output from the distributed model depending upon the data availability the richard s method can be incorporated in the hydrological model instead of using two layer water balance method for more descriptive information of vertical soil moisture profile and recharging the groundwater zone 7 conclusions the extensive data base of the study area i e yerli sub catchment of tapi basin india have been created by gathering information from different sources enumerated in table 1 few distributed parameters of the model were estimated as per the land use cover cropping pattern in the sub catchment while others were calibrated using multi objective optimization approach based on shuffle complex evolution sce algorithm the optimally calibrated parameters were in turn used in distributed physics based hydrological model for simulation of complete water balance of the study area the simulated stream flows and groundwater levels within the sub catchment were validated satisfactorily for available independent data of year 1999 2004 the key findings of current study are briefed as follows i the simulated stream flows and groundwater levels from distributed hydrological models have been found to be highly variable pertaining to saturated hydraulic conductivity ks of unsaturated zone horizontal kxx and vertical kyy hydraulic conductivity of saturated porous media instead the moisture content at saturation θs field capacity θfc wilting point θwp specific yield sy and specific storage ss have been found to be less significant in generating the stream flows and groundwater levels within the sub catchment ii distributed hydrological model with coarser grids 500 m 500 m performs better than distributed model with finer grids 250 m 250 m having lumped unsaturated and saturated flow parameters loliyana and patel 2018 in simulating stream flows and groundwater levels at the outlet and within sub catchment respectively iii the calibrated distributed model has been able to simulate stream flows and ground water levels satisfactorily for split sampling and proxy sampling tests rmse 98 79 m3 s r 0 84 nse 0 70 and ei 0 99 the calibrated hydrological model based on stream flows at the outlet of sub catchment even performs satisfactorily in simulating the flows within the sub catchment iv the calibrated model has been able to simulate soil moisture contents satisfactorily within the sub catchment particularly for alluvial plain the performance of the model requires further improvement in simulating soil moisture contents over hilly and transitional areas while incorporating richard s equation in unsaturated flow modeling v the simulated water balance of the semi arid yerli sub catchment revealed a high value of aet 78 of rainfall during the calibration and validation periods the percentage error in the water balance of calibrated and validated periods have been found to be 0 86 and 1 09 respectively which is marginally better than previous modelling results loliyana and patel 2018 the part of study area in the plain region have been reported to be saline due to combined effects of slow movement of sub surface flow high evapotranspiration and leaching of weather basalt from the hilly region vi the calibrated and validated hydrological model has been applied during application periods 2005 2009 and 2010 2012 it is seen that reduction in the deciduous forest cover and increase in urbanization during the application periods have led to increase in surface runoff and decrease in evapotranspiration within the sub catchment declaration of competing interest there is no any conflict of interest for the current manuscript content submitted by the authors acknowledgement authors are thankful to mhrd npiu teqip ii for providing the funding through centre of excellence coe project on water resources and flood management centre at svnit under which present investigation has been undertaken authors are also thankful to india meteorological department imd national remote sensing centre nrsc hyderabad national bureau of soil survey and land use planning nbss lup nagpur central water commission cwc tapi division for providing the data for present study appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104677 
26037,bayesian networks bns are commonly used to model socio ecological systems as their graphical structure supports participatory modelling they can integrate quantitative data and qualitative knowledge and account for uncertainty although the spatial and temporal dimensions are important in socio ecological systems there is a lack of openly available and easy to use tools to run bns with spatial data over time here we present gbay gbay ethz ch an online platform where users can link their bns to spatial data run the network iteratively to incorporate dynamics and feedbacks and add geo processing calculations to account for spatial interactions we demonstrate the use of this tool on the examples of a modelling a regulating ecosystem service where we account for neighbourhood effects and land use decisions where we include regional level boundary conditions the gbay platform supports users with its graphical interface and instructive wiki page and provides a step towards more accessible and flexible socio ecological modelling keywords bayesian networks online tool ecosystem services land use decisions spatial interactions 1 introduction as ecosystems undergo changes that jeopardize their capacity to provide essential services to society cardinale et al 2012 foley et al 2005 natural resource managers and landscape planners face challenging decisions on sustainable landscape development wu 2013 modellers aim to support these decisions e g through mapping ecosystem services and assessing trade offs between them carpenter et al 2009 or predicting scenarios of future land use carpenter et al 2015 verkerk et al 2018 however modelling complex socio ecological systems requires integrating various types of information hamilton et al 2015a such as earth observation and in situ data empirical or process based models and socio economic data models are often associated with high uncertainties due to both the inherent variability of socio ecological systems and the common lack of data ascough et al 2008 ropero et al 2013 at the same time local experts and stakeholders often have valuable knowledge about their socio ecological systems and involving them in the modelling process facilitates communication and learning ruckelshaus et al 2013 voinov and bousquet 2010 involving stakeholders and producing credible results that can support decision making requires a flexible and transparent modelling process jakeman et al 2006 voinov et al 2016 an increasingly common approach to deal with these challenges is the use of bayesian networks bns directed graphs where variables are linked through conditional probabilities marcot and penman 2019 key advantages of bns include their capacity to integrate qualitative and quantitative information their explicit treatment of uncertainty and their graphical structure uusitalo 2007 the graphical structure of a bn represents causalities in the modelled system which increases modelling transparency in comparison to black box e g empirical models jakeman et al 2006 and facilitates communication with stakeholders voinov and bousquet 2010 for example co developing bns with stakeholders has been used to address ambiguities in water management henriksen et al 2012 and to build a common understanding of an agricultural socio ecological system salliou et al 2017 a bn of forest ecosystem services provided a common language for experts from different fields thus supporting planning gonzalez redin et al 2016 different types of information can be integrated in a bn since the links between variables in a bn can be quantified individually borsuk et al 2004 often information on some components of the modelled system is already available in the form of empirical and process based models which can be translated to conditional probabilities borsuk et al 2004 stritih et al 2019 bns can also learn relationships between variables directly from data stelzenmüller et al 2010 such as remote sensing dlamini 2010 water quality measurements ames et al 2005 or species observations hamilton et al 2015b when data are scarce or unavailable they can be supplemented with expert knowledge ames et al 2005 borsuk et al 2004 hamilton et al 2015b pollino et al 2007 the probabilistic structure of bns means that uncertainties are explicit and propagated through the network socio ecological systems are inherently complex and variable leading to high uncertainties that are exacerbated by limited data availability regan et al 2002 it is particularly important to consider these uncertainties in risk assessments where unlikely extreme events are relevant grêt regamey and straub 2006 mcdonald et al 2016 bns can be used to identify knowledge gaps hamilton et al 2015b stritih et al 2019 and can easily be updated as soon as new information becomes available hamilton et al 2015b in environmental applications the spatial and temporal components are often crucial carpenter et al 2009 the spatial composition of ecosystems and land use in landscapes is essential to their function and needs to be taken into account when trying to understand landscape change or identify trade offs or synergies between ecosystem services nelson et al 2009 raudsepp hearne et al 2010 therefore models of socio ecological systems are often spatially explicit spatially explicit bns where the network is linked to a raster have been used to model scenarios of future land use carpenter et al 2015 celio et al 2014 and map ecosystem services gonzalez redin et al 2016 grêt regamey et al 2013 landuyt et al 2013 villa et al 2014 the temporal dimension has been addressed less frequently in bn modelling as bns are most commonly static and the construction of dynamic bns is often seen as cumbersome uusitalo 2007 dynamic bns use the time sliced approach kjaerulff and madsen 2013 where each variable of the system is represented by a separate node in each time step resulting in a copy of a network for each time slice with temporal links between these iterations this approach can be used to model landscape changes over time chee et al 2016 in most spatial applications of bns so far the models have been run for every individual pixel of a raster a major limitation of this approach is that is fails to take into account spatial interactions landuyt et al 2015 stritih et al 2019 and cross scale effects which often have an important influence both on ecological and socio economic processes peters et al 2007 for example a habitat is only suitable for a species if it is large enough to support a viable population or connected to other habitats farmers decisions to cultivate a parcel of land may depend on the decisions of other farmers or an overarching policy that prescribes certain amounts of ecological set aside to be eligible for subsidies celio and grêt regamey 2016 in case of ecosystem services such as flood protection or pollination the provision and demand for the services do not occur at the same location bagstad et al 2013 and the provision of services is related to the spatial composition of ecosystems in the landscape grêt regamey et al 2014 syrbe and walz 2012 therefore interactions across space at different levels should be taken into account when modelling socio ecological systems several tools have been developed to run bns with spatial data see table 1 but most do not explicitly support iterative inference over time feedback loops or spatial interactions for such more complex applications modellers typically use the api of common bn software packages such as hugin or netica pérez miñana 2016 to link their bns to spatial data celio et al 2014 chee et al 2016 grêt regamey et al 2013 sun and müller 2013 however there is a lack of openly available and easy to use tools i e including a graphical user interface which would allow users to run spatially explicit bns over multiple time steps in this paper we present gbay an online platform with a simple graphical interface that links bns to spatial data users can run their bns iteratively over multiple time steps with raster or vector data in addition the platform includes the possibility to account for spatial interactions such as neighbourhood effects we describe the architecture of the platform and its use furthermore we illustrate how accounting for effects at different spatial scales such as neighbourhood effects and regional boundary conditions can help improve the realism and reduce uncertainties in models of ecosystem services and land use change we discuss the advantages of bns and the gbay platform as well as the limitations of this modelling approach and ongoing challenges 2 methods 2 1 bayesian networks a bayesian network is a directed acyclic graph with an underlying joint probability distribution jensen 2001 kjaerulff and madsen 2013 pearl 1988 it consists of nodes representing variables each with a set of mutually exclusive states the states of a node can be categorical e g land use types or quantitative e g the distance to the nearest forest the links between nodes represent the directed causal relationships or dependencies between these nodes e g x y the joint probability distribution p x y of the nodes is condensed in conditional probability tables cpts which contain the probability distribution of each node for each combination of its parent nodes states the probability that node y is in state y can be calculated by summing its conditional probabilities over the states x of its parent nodes p y y xp y y x x p x x in a process called marginalization kjaerulff and madsen 2013 fig 1 shows an example of a bn which predicts land cover change in a system where meadow abandonment leads to forest encroachment the future land cover lc t1 is a child node of the current land cover lc t0 and the intensity of agricultural use the causal relationships between the nodes are quantified in the cpt of node lc t1 which specifies the belief that the future land cover will be either a meadow or a forest for each combination of the states of its parent nodes once the bn is compiled it can be updated for specific cases by adding evidence evidence can be data e g when we know the type of land cover in a pixel or scenarios e g when we explore what happens in a system if agricultural use changes when we know the state of a node with 100 certainty this is called hard evidence e g that the agricultural intensity of a pixel is low while soft evidence contains some uncertainty and is in the form of a probability distribution e g our observation indicates that the land cover is a meadow with 70 probability and a forest with 30 probability when evidence is added to the network the joint probability distribution is updated through a process called inference which results in a posterior probability distribution ppd of all the nodes in the network thus providing information about the expected most likely state of target nodes as well as the associated uncertainty jensen 2001 evidence can also be propagated along a chain of nodes e g x y z according to the chain rule p x y z p z y p y x p x and from child nodes to parent nodes for example in the network in fig 1 knowledge about current land cover could be used to infer the past land cover 2 2 coupling bns and spatial data with gbay here we present gbay bayesian networks with geo data an online tool to link a bn to spatial data and run a process over multiple time steps fig 2 illustrates the functionalities of the gbay platform spatial data is used as evidence on specific nodes in a bn inference is then performed for each pixel or object of the input data where the output is a probability distribution across the possible states of target nodes for each spatial unit the outputs of inference can be used as inputs in the next iteration to account for temporal dynamics see section 2 3 in addition spatial inputs or outputs can be processed with a python script to account for spatial interactions at different scales see section 2 4 the gbay platform consists of an online graphical user interface fig 3 where the users can upload a network in the dne format developed in netica or a similar bn software the uploaded network is visualized in the gui where users can select one or more target nodes the ppd of which they wish to calculate spatial data can be added to the network in the form of raster a geotiff file for each input node or vector files a shapefile or geodatabase with attributes corresponding to input nodes by dragging the file to the designated location in the network or by using the menu provided for each node gbay can take into account both hard and soft evidence see table 2 to set hard evidence the input raster or attribute table of the vector data contains only one value per pixel or object for soft evidence the input raster or vector file has a band or attribute for each state of the input node in addition users can set non spatial hard or soft evidence for the whole area by simply clicking on the state of the node or entering the soft evidence probabilities all configurations links iterations hard and soft evidence including the corresponding geo data can be saved and reloaded later if necessary the output has the same geometry as the input spatial files and contains the probability of each state of the target node for each spatial unit i e the whole ppd in a multi band raster or attribute table as well as information about the most likely state in addition shannon s evenness index of the ppd is calculated j h hmax where h i 1 n p i l o g 2 p i hmax log2 n where pi is the probability of state i and n is the number of states the index is a standardized measure of entropy which expresses uncertainty and can be compared between nodes with different numbers of states marcot 2012 it has values between 0 and 1 where 1 denotes a uniform distribution between all possible states maximum uncertainty and 0 denotes complete certainty about the state of the node for continuous target nodes the output additionally contains information about the mean median and standard deviation of the ppd running bns with large spatial data can be computationally intensive at the moment gbay runs on a virtual server ubuntu 16 04 4 with 6 cores at 3 ghz and its processing speed depends on the size of the network and spatial data for example when running a network of 17 nodes and 1128 cpt rows with four input rasters gbay can process around 4000 pixels per second when processing large networks or datasets users can receive the outputs via email in case of a browser timeout user data including bns spatial data and scripts are automatically deleted from the server after one day 2 3 temporal dynamics through iterations bayesian networks usually represent a static state of the studied system and one of their major drawbacks is that they cannot incorporate feedback loops uusitalo 2007 this limitation can be overcome by dynamic bns using the so called time slicing approach kjaerulff and madsen 2013 where each time step is represented by a separate network however developing such dynamic bns can be very cumbersome uusitalo 2007 in gbay a simplified version of the time slicing approach is implemented where the bn is run iteratively in multiple time steps and the outputs of one time step are used as inputs to the next for example when modelling land cover change we start with a map of current land cover lc t0 during one time step land cover change takes place and through inference we obtain the probability distribution of land cover after the first time step lc t1 e g after 5 years this lc t1 then becomes the input for starting land use in the second time step in other words the result of one iteration is used as starting condition for a second iteration see celio et al 2014 for an example on the gbay platform bns can be run iteratively by specifying temporal links between nodes representing time steps and the number of iterations for example if the output node lc t1 is selected as a link node an arrow appears that can be connected to the corresponding input node lc t0 multiple links can be used reflecting different variables that are connected over time 2 4 multi scale processes using python scripts the gbay platform can account for spatial processes at different levels corresponding to different types of geoprocessing operations tomlin 1994 in the basic mode of gbay inference is performed at the local level for each individual pixel or object however gbay also provides the option to consider processes at different levels calculations across scales can be implemented by running an intermediate processing python script indicated with a script icon in fig 2 at the focal level a python script can be used to take into account the neighbouring pixels or objects e g to obtain the land cover of neighbouring pixels within a specified window or calculate the distance to the nearest pixel of a specified land cover type in the land cover change example forest encroachment on a meadow depends on the distance to the nearest forest patch which can be calculated from the input land cover raster using a python script see appendix a this information can be used to set new evidence on a node e g distance to forest at the zonal level the python script evaluates pixels or objects across the whole study area for example to check whether regional boundary conditions have been reached an example of such boundary conditions is a minimum percentage of a specific land use category defined by an agricultural policy a python script can be run before performing inference i e to calculate spatial evidence such as focal statistics based on the input data or in between iterations it can also be used to modify evidences over time e g to implement a policy that changes between time steps gbay currently supports intermediate processing scripts written in python using openly available libraries including gdal ogr numpy and math python was chosen as the language of the intermediate processing scripts since it is one of the most widely used programming languages with a large community particularly in spatial modelling and provides many open access libraries a set of scripts to model spatial interactions at the focal and zonal levels are available on the gbay wiki and can be downloaded and adapted in addition advanced users can develop their own scripts where the input and output format must match the format used by gbay a list of nodes containing an array of probabilities across states for every pixel or object see appendix a for details it is important to note that the processing time of gbay increases when more complex geoprocessing is performed two examples of bn models that incorporate spatial interactions are described in more detail below 2 5 case studies 2 5 1 avalanche protection in davos accounting for neighbourhood effects protection from snow avalanches is one of the most important ecosystem services provided by forests in the swiss alps grêt regamey et al 2008 an avalanche release is less likely inside a forest bebi et al 2009 and forests also reduce the mass and velocity of avalanches that flow through them feistl et al 2014 the release and size of avalanches depend on terrain characteristics and snow conditions the protection capacity of the forest is related to its structure and species composition and the value of the service depends on the risk to settlements and infrastructure a bn was used to combine earth observation data on terrain and forest structure existing process based and empirical models about the avalanche process and expert knowledge about risk factors the bn was run with spatial data to map the provision and demand for avalanche protection in the region of davos switzerland stritih et al 2019 the resulting maps of avalanche protection contain large uncertainties and a sensitivity analysis was used to identify the key sources of uncertainty in the model one of the main sources of uncertainty was the definition of potential release areas of avalanches the probability of an avalanche release depends on topography slope curvature terrain roughness as well as snow conditions in the bn the topographical factors were combined using fuzzy logic veitinger et al 2016 for each factor a membership function describing the probability that a pixel belongs to a potential release area as a function of the factor was defined by experts the membership function describes the probability that a pixel belongs to a potential release area as a function of the factor for example the factor of slope has a trapezoid like membership function where avalanche releases can occur on slopes between ca 28 and 55 but the release probability is highest between 35 and 45 the factors of slope curvature and terrain roughness were then combined using a fuzzy and operator for details see veitinger et al 2016 to fill the cpt of the node release this way an avalanche release probability can be calculated for each pixel of the study area this pixel based approach neglects the interactions between neighbouring spatial units i e whether a release pixel is connected to other release pixels however the probability of an avalanche release depends on the size of the potential release area bühler et al 2013 an avalanche release can only occur when there is a sufficient volume of snow to be released which depends on the amount of snow i e the avalanche release depth which is estimated using a probability distribution of maximum new snow based on long term observations slf 2017 and the size of the release area in order to incorporate neighbourhood effects we implemented an updated version of the bn from stritih et al 2019 in gbay the model is implemented in two iterations see fig 4 first the avalanche release probability of each individual pixel is calculated based on its slope curvature and roughness then these probabilities are used as an input to a python script that calculates the size of the release area spatial metrics such as patch sizes are commonly calculated based on boolean class memberships either a pixel is a release area or it is not however since the definition of release areas is uncertain such an area calculation would depend on an arbitrary threshold probability e g 50 at which we consider a pixel to be in a release area to avoid this problem we used a fuzzy geographical area calculation fonte and lodwick 2004 we defined a set of probability thresholds α between 0 and 1 for each threshold all pixels with p release α were considered to be release pixels and adjoining release pixels form a release area a release area size was calculated for each α and based on the different sizes for different threshold probabilities we could estimate a probability distribution of release area size see appendix b for an illustration in the second iteration the probability distribution of release area size was used as soft evidence on the node release area size combined with the maximum new snow height the release area defined whether the snow volume release area new snow height was sufficient for an avalanche release if the snow volume was below the threshold for small snow avalanches as defined by the canadian classification of avalanche sizes slf 2018 we assumed that the release will not occur setting the release corrected probability to zero the updated bn model of avalanche protection was run in gbay with spatial inputs at a 5 m resolution for the dischma valley in davos the whole network is illustrated in appendix b 1 the release probability the total provision of avalanche protection and the associated uncertainty were calculated and compared with the results of the previous model that did not account for release area size 2 5 2 implementing boundary conditions for land use change in the entlebuch unesco biosphere land use decisions have a strong impact on landscape development and are influenced by an interplay of biophysical and socio economic factors policies and personal preferences celio and grêt regamey 2016 used a participatory approach to develop a model of farmers decisions and resulting land use change in the entlebuch unesco biosphere in the canton of lucerne switzerland after identifying potential factors influencing land use decisions through literature review an expert group was formed the experts weighted the influencing factors to find a subset of the most relevant variables and defined the causal relations between them then they defined node states and the conditional probabilities the bn was updated with local actors knowledge and validated through a review by experts celio et al 2012 for a detailed description of the participatory modelling process see celio et al 2014 the resulting bn predicts land use change based on biophysical factors such as slope and potential natural vegetation agricultural policy amount and types of direct payments zoning e g vicinity to a residential area and individual farmers characteristics such as their education whether they have a part time business and their view on ecological policies see fig 5 the land use change probabilities are defined for a time step of 5 years and the bn can be run iteratively to model longer periods the bn was used to model scenarios of agriculture policy ap old agricultural direct payments or the more ecology oriented agricultural policy implemented in 2014 and farmer characteristics production or ecology oriented farmers the scenario maps illustrated the trends of the different combinations of aps and actor characteristics however the scenarios were calculated only taking into account individual parcel information not considering limitations on the regional scale such as prescribed minimum amounts of specific land use types to support cattle production the cell level approach means that the exogenous limits of farmers decisions were neglected in order to account for the regional boundary conditions we adapted the bn developed by celio and grêt regamey 2016 for agricultural land use and implemented it in gbay the limits of land use change were defined based on the maximum number of cattle grazing per hectare as defined by the federal office for the environment 2013 assuming that the number of cattle in the region remains constant we estimated the minimum area of extensive medium and intensive agricultural land required to fulfil this legal obligation this limited the conversion of agriculture to other land use types through extensification and abandonment when certain minimum areas of agricultural land use had been reached no further cells were converted to other land use types this boundary condition was implemented at the end of every iteration time step of the network in gbay using a python script we checked the amount of extensive medium intensive and intensive agriculture cells across the whole study area if the required amount of a certain agricultural land use category was not reached the script searched among the cells that had been converted from other land use categories to find those where the change was least likely and converted them back to their previous probability distribution until the minimum area of the category was reached in other words land use change was prevented by the minimum amount condition in those cells where it was least likely to occur this roll back mechanism is explained in more detail in appendix c 3 case study results 3 1 avalanche protection we mapped the provision of avalanche protection and associated uncertainty in the dischma valley in davos using a bn adapted from stritih et al 2019 since the definition of avalanche release areas was a major source of uncertainty in the model we adapted the model to account for neighbourhood effects in the release process fig 6 a and 6b show the resulting maps with and without accounting for spatial interactions where the colours indicate the mean value of avalanche protection provision expressed in height of snow stopped and the uncertainty entropy of the posterior probability distribution the most important areas providing avalanche protection are steeper densely forested areas but the model shows a high spatial heterogeneity and high uncertainty in the basic model without neighbourhood effects the mean coefficient of variation across the whole study area amounts to 95 when taking the size of the release area into account the spatial pattern remained similar but the uncertainty was reduced mean cv of 87 see appendix b table b 2 fig 6c and 6d show the release probability without and with the correction for spatial interactions release area size the bn that accounts for the release size results in fewer release areas fig 6d as smaller areas are less likely to reach a sufficient volume of snow for an avalanche release in addition in areas that are originally assigned a low release probability the probability is additionally reduced as they are unlikely to form part of a large release area thus a clearer spatial pattern of potential release areas emerges with the mean entropy uncertainty of the release probability map reduced from 29 to 19 see appendix b table b 2 3 2 land use decisions the bn of agricultural land use decisions in the entlebuch was run in the iterative mode in gbay with and without the inclusion of boundary conditions minimum area of medium and intensive agriculture due to legal requirements for cattle breeding the resulting land use maps and distribution of land use types are shown for two scenarios production oriented farmers with the old direct payment system and ecology oriented farmers with the new agricultural policy across three time steps fig 7 in both scenarios the boundary conditions had an effect on the final land use change in the ecology oriented scenario the farmers decisions drive extensification leading to a rapid loss of intensive agricultural land when no limits are implemented when the boundary conditions are implemented the minimum is reached very quickly within one time step preventing further land use change in the production oriented scenario the boundary conditions have a smaller effect as farmers are more likely to maintain their intensive agriculture however the medium intensive plots are converted to extensive use if the minimum limits are not implemented 4 discussion in this paper we presented gbay an openly available online platform for spatially and temporally explicit bayesian networks the platform offers an easy to use gui to run bns with spatial data over multiple time steps as such it aims to facilitate spatial bn modelling of socio ecological systems by including the temporal component and spatial interactions as well as making it more accessible to practitioners bn models can be used to integrate different types of information account for uncertainty and can facilitate participatory modelling in the following we discuss how the gbay platform can help users draw on these advantages as well as the associated challenges and limitations in addition we discuss the implications of our case studies for landscape planning 4 1 integrating information across scales data on socio ecological systems is becoming increasingly available through sources such as earth observation and social media and information is also available in the form of local actors or expert knowledge bns are well suited to integrating these different types of information as is illustrated in the avalanche protection case study where remote sensing inputs were combined with process based empirical models and expert knowledge stritih et al 2019 however while bns are commonly used to integrate information about a static system the temporal and spatial are often not explicitly represented in bns although they are essential in most socio ecological systems hamilton et al 2015a the gbay platform provides the possibility to incorporate dynamics by using the iterative bn approach which can include feedback loops thus addressing one of the major limitations of bn models kelly letcher et al 2013 uusitalo 2007 however the iterative bns are mainly suitable for systems where one or few variables change over time e g land use change while other variables act as drivers of this change such as state and transition models see chee et al 2016 and feedbacks only occur between time steps for more complex dynamic interactions other modelling approaches such as coupled component models or system dynamic models may be more appropriate kelly letcher et al 2013 lauf et al 2012 schreinemachers and berger 2011 in addition to the temporal component gbay can also account for processes that occur at different spatial scales or organizational levels socio ecological systems are influenced by different processes at different scales verburg et al 2004 and interactions between these processes across scales can result in non linear dynamics or threshold effects peters et al 2007 in bn models processes at higher organisational levels e g regional policies market conditions climate are often represented by a node in the network celio et al 2014 grêt regamey et al 2013 kleemann et al 2017 but potential feedbacks from the lower to the higher level are not accounted for using the python module in gbay the cumulative effects at the local level can be calculated and used to update the higher level node in the next time step for example while land use decisions the level of individual parcels depend on regional policies rapid land use change across many parcels may in turn affect the policies in a feedback effect that can be accounted for in gbay while the python module increases modelling flexibility and allows us to incorporate spatial interactions or boundary conditions the intermediate calculations used to modify bn inputs or outputs should be compatible with the probabilistic logic of bns the explicit treatment of uncertainties is a major advantage of bns but it is challenging to include the information about the whole probability distribution per each pixel in spatial calculations a simple approach is to set a threshold probability each pixel with a probability of forest above 50 is considered a forest in a neighbourhood calculation however this means a loss of information about the probability distribution and results can be strongly affected by the arbitrary threshold arnot et al 2004 to deal with this fuzzy landscape metrics can be applied to account for uncertain membership in a class e g land cover arnot et al 2004 fonte and lodwick 2004 such as the fuzzy area calculation used in the avalanche protection example however these do not account for variability in spatial processes e g flows in ecosystem services assessments the directional flow between service providing and receiving areas is important to account for es flows in space in a probabilistic manner johnson et al 2012 have used a combination of bns and agent based models that simulate the flow of es units such an approach would add an additional level of complexity but offers a probabilistic perspective on spatial processes that should be addressed 4 2 dealing with uncertainty socio ecological models often contain high uncertainties partly due to limited data measurement errors and subjective judgement but partly also related to the inherent spatial and temporal variability of the modelled systems regan et al 2002 these uncertainties should be acknowledged and taken into account in decision making maier et al 2008 a major advantage of bns is that uncertainties can be explicitly accounted for and propagated through the models stritih et al 2019 uusitalo 2007 the output of inference in gbay contains a probability distribution across all the possible states of the target node for each pixel or object of the study area which means that the output uncertainty can be quantified in a spatially explicit way gbay automatically provides the user with measures of uncertainty including the entropy marcot 2012 and standard deviation for continuous nodes of the probability distribution these can be used to map uncertainties as demonstrated in the avalanche protection case study see fig 6 mapping uncertainties is particularly important when these are spatially heterogeneous such as in remote sensing based classifications petrou et al 2013 although maps of uncertainty can be easily generated in gbay interpreting them is not straightforward landuyt et al 2015 the way in which spatial uncertainties are visualized may have a strong effect on how they are understood by end users kunz et al 2011 in the avalanche protection example fig 6 we used a bivariate map to depict both the modelled value and associated uncertainty with darker colours indicating higher uncertainty thus drawing attention to areas of high uncertainty kunz et al 2011 in other applications it may be more appropriate to emphasize areas of higher certainty by linking uncertainty to transparency or fuzziness since different types of users prefer different visualizations of uncertainty maceachren et al 2005 gbay is currently limited to providing data on uncertainty which users can use in their preferred visualization mode 4 3 increasing the accessibility and transparency of bn modelling involving stakeholders in modelling socio ecological systems can increase the credibility of model results and support learning jakeman et al 2006 voinov and bousquet 2010 a key requirement for credible participatory modelling is transparency voinov and bousquet 2010 bns have been promoted as a tool for participatory modelling due to their transparent model structure and capacity to incorporate expert knowledge bromley 2005 this type of use is demonstrated in our land use decision case study where the model was co developed with experts from different fields and updated with local stakeholder knowledge however participatory modelling is an iterative process and models should be updated as new information becomes available which is often not within the frame of research projects models are more likely to have an impact on decision making when local experts and decision makers take ownership of the model jakeman et al 2006 and can generate new results as new information becomes available in an iterative process ruckelshaus et al 2013 open access and easy to use web based tools can support the adoption of models by local experts and practitioners voinov et al 2016 although the structure of a bn model is in itself transparent and many graphical tools are available to develop bns the application of bns to spatial data usually requires programming skills to use the api of bn software such as netica or hugin pérez miñana 2016 the gbay platform aims to reduce this gap and make spatial bns more accessible to a wider range of users because of its simple user interface users without programming experience can use gbay to link their bns with spatial data this is supported by the gbay wiki page wiki gbay ethz ch with instructions examples of bns and associated data that can be downloaded to test the platform nonetheless developing a bn is not straightforward model co development with stakeholders is a time consuming process and it is important to ensure stakeholder diversity and consider group dynamics voinov and bousquet 2010 when experts are asked to parametrize a bn model challenges include potential biases kuhnert et al 2010 fatigue during elicitation of extensive cpts das 2004 and over confidence speirs bridge et al 2010 when learning a bn from data the quality of the model is limited by the quality and amount of data available hamilton et al 2015b because of such challenges making bn modelling more accessible will require not only tools such as gbay but also training and capacity building among potential users although the code of gbay is published it is based on the proprietary netica api norsys 2010 in addition the platform is not designed for bn development and requires users to upload their own bns in the dne format as developed in netica genie bayesfusion 2017 or a similar bn software netica is currently the most commonly used bn software in the ecosystem service modelling community pérez miñana 2016 and to our knowledge no open source software currently offers a graphical interface for bn development with comparable functionalities including the integration of discrete and continuous nodes learning from data and sensitivity analyses the development of such an open source software would be an important step towards increasing the accessibility and transparency of bn modelling 4 4 implications for environmental management and landscape planning processes our case studies on ecosystem service mapping and land use decision modelling demonstrated the use of gbay for spatial bns incorporating focal neighbourhood effects and zonal boundary conditions accounting for such spatial interactions can help to reduce uncertainties improve model realism and take into account knowledge at different scales or organizational levels high uncertainties in ecosystem services maps limit their usability as a support for decision makers andrew et al 2015 schulp et al 2014 in the case of the avalanche protection service section 3 1 1 considering neighbourhood effects between pixels in potential avalanche release zones reduces overall uncertainty by excluding areas that are too small to produce an avalanche release however due to the fuzzy geographical area calculation algorithm the corrected release probability is also reduced in large areas of low p release which could lead to neglecting large releases that occur only under very extreme conditions thus adding the area condition likely reduces false positives i e increases the specificity of detecting release areas but may also lead to more false negatives some release areas may be excluded higher levels of specificity in detecting potential release areas may be useful to identify the forest patches that play the most important role in preventing avalanche releases which is important in prioritizing the management of these protection forests teich and bebi 2009 however the purpose of hazard risk mapping it is also important to consider releases that only occur in extreme snowfall conditions with very low probabilities although validation data on these extreme events is lacking bühler et al 2018 better estimates of extreme scenarios could be achieved by running the bn for a scenario with high new snow or choosing a low threshold for pixels to be considered part of a release area in the land use decision case study section 3 1 2 taking into account boundary conditions offers a more regional perspective where farmers decisions are limited by regulations in other words the more realistic representation limits the option space the constrained model may be more useful for short term forecasts of landscape development under the assumption that boundary conditions will stay constant while unconstrained exploratory models can better represent the whole range of possible futures maier et al 2016 rounsevell and metzger 2010 modelling more extreme scenarios may be useful to clearly observe the impacts of different scenarios of agricultural policy and farmers characteristics and may offer a wider perspective on potential solutions in landscape planning processes hence combining both perspectives i e with and without boundary conditions and observing the differences between them can yield additional insights in our case study the comparison demonstrates how strongly the decision making of farmers at their plot level is constrained by larger scale regulations in both cases studies the appropriate choice of method e g considering boundary conditions or not and the interpretation of results will depend on the needs of decision makers which highlights the need to involve stakeholders and decision makers in the modelling process voinov and bousquet 2010 tools such as gbay can contribute to the flexibility and accessibility of modelling socio ecological systems over time and space and thus have the potential to support decision makers in environmental management and landscape planning software availability name gbay bayesian networks with geo data developed by orencio robaina enrico celio ana stritih sven erik rabe eth zürich plus availability online at gbay ethz ch free for non commercial use software requirements netica norsys or similar software to create bayesian networks programming language web interface in html javascript back end in c using the netica api python to support intermediate processing scripts source code https github com ethzplus gbay instructions and examples available at wiki gbay ethz ch declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the european union s horizon 2020 research and innovation programme ecopotential project grant agreement no 641762 and by the swiss programme for research on global issues for development r4d programme funded by the swiss national science foundation snsf and the swiss agency for development and cooperation sdc grant no 100400 152167 the authors would like to thank ralph sonderegger for graphic design yves bühler and peter bebi for discussion on avalanche modelling and two anonymous reviewers for constructive comments the authors are grateful to the cantonal administration of lucerne for the provision of spatial data appendix a python scripts in order to account for spatial interactions and processes and different scales users can implement a python script in gbay in order to be compatible with gbay the uploaded script file needs to implement a function named process with the following definition process gdaldataseth dataset list nodes data int iteration the inputs to the process function are gdaldataseth dataset contains the metadata of the spatial data being processes e g raster spatial extent pixel size and projection gbay uses gdal to operate with spatial data list nodes data contains the data of the nodes that are used as inputs for the geoprocessing script the node data is stored as a python dictionary with three keys o name str name of the node o type int type of the node py discrete py continuous py discrete if omitted o data list list of probabilities between 0 and 100 of each state for each raster cell or object int iteration the number of the current iteration which can be used if some inputs should be modified over time the process function should return a list of nodes with the updated node likelihoods if the output node is discrete or node values if the node is continuous the python script should import the node utils python module which contains functions to validate the output and to read and write node information as well as other packages used by the script e g gdal scipy gbay stores the probabilities of the nodes selected by the user as to be used by the python script creates the nodes data list and runs the process function then it runs a function to validate whether the output complies with the nodes data format and if it does it will set the node probabilities as returned by the function this happens at the beginning of the processing and at the end of each iteration in case the results are not validated e g the data types are incompatible or total probability does not add up to 100 or in case an error occurs in the execution of the script gbay will print out the error message and ignore the output data it may also occur that the results are correctly formatted but invalid form the bn perspective e g when trying to set a probability of a state that would be impossible according to the node s cpt and the evidence set on its parent in this case gbay will print out an error message from netica example besides the factors affecting future land cover described in fig 1 the transition of meadows to forest may also be affected by the distance to the nearest forest patch if a node distance forest is added to the network its values can be calculated based on the input land cover map directly in gbay using a python script image 1 appendix b bayesian network for avalanche protection to model the provision of avalanche protection we used a model adapted from stritih et al 2019 see figure b 1 the model was modified to account for neighbourhood effects in the avalanche release process where a pixel is only accounted as a potential avalanche release if it is part of a sufficiently large release area we used a python script to calculate fuzzy release areas based on release probabilities as illustrated below this led to lower uncertainty in the definition of release areas and in the total provision of avalanche protection see table b 2 fig b 1 bayesian network used to model the provision of avalanche protection adapted from stritih et al 2019 the bn uses inputs indicated with a thicker frame from remote sensing and avalanche data to infer about the ecosystem structure and processes which determine the detrainment snow braking in the forest during an avalanche and prevention functions these functions are combined to express the total level of avalanche protection provision the orange arrow indicates where a python geoprocessing script is used to calculate the size of avalanche release areas from per pixel release probabilities fig b 1 fuzzy area calculation the raster figure b 2 shows the probability p release of each pixel belonging to a release area a fuzzy release area size is calculated for the pixel shown in red first the area is calculated for different threshold probabilities α where every pixel where p release α is considered part of the release area this results in a different size of release area for each probability table b 1 from which a cumulative probability distribution can be derived in this case the release area is between 4 and 19 pixels based on this probability distribution we can calculate the probabilities of the area belonging to a size class figure b 3 fig b 2 example raster of p release fig b 2 fig b 3 resulting cumulative probability distribution of area black line and the probability distribution of area in classes 1 5 5 10 10 15 15 20 pixels fig b 3 table b 1 area calculation for different α values table b 1 threshold probability α area 0 05 19 0 1 19 0 2 18 0 3 14 0 4 10 0 5 9 0 6 7 0 7 6 0 8 6 0 9 5 0 95 4 results uncertainty of total provision and release probability with and without accounting for release area size table b 2 mean uncertainty in total provision of avalanche protection and release probability across the whole study area expressed in coefficient of variation cv only for continuous nodes and entropy index with and without the correction for release area size neighbourhood effect table b 2 without neighbourhood correction with neighbourhood correction node cv uncertainty cv uncertainty provision 95 0 089 87 0 083 release 0 29 0 19 appendix c the roll back mechanism to implement boundary conditions for land use change in order to implement boundary conditions in the land use change model a minimum limit of extensive intensive and medium intensive land use to support the number of cattle in the region a python script was implemented in gbay at the end of every iteration the script checks the number of extensive medium intensive and intensive agriculture cells and if the frequency is below the defined minimum it converts cells which have the highest probability of being in those categories back to their previous probability distribution rolled back until the minimum frequency agriculture has been reached in case not enough cells of medium intensive agriculture are available to convert back to intensive agriculture due to the minimum limit in this land use category cells from a third category e g forest are changed back to medium intensive and medium intensive cells are changed back to intensive in a double roll back the mechanism is illustrated in figure c 1 fig c 1 representation of the roll back mechanism to ensure that the minimum frequencies of land use 1 and 2 are maintained during the first iteration of the land use change bn lu1 is converted to lu2 and lu2 changes to lu3 however if the frequency of lu1 and lu2 drops below the minimum limit the roll back mechanism is implemented to revert cells back to their previous probability distribution until the minimum is reached fig c 1 the conversion matrix table c 1 shows how many cells have been transferred to other land use categories due to the enforced conversion limits in the production oriented scenario for iterations time steps 2 and 3 in both the hill and mountain region certain parcels were rolled back in the hill region the number in brackets show how cells were initially rolled back from other to intensive and in the following from intensive to medium intensive to fulfil the restrictions double roll back table c 1 rollback mechanism induced by python script made explicit for the production oriented scenario in iteration 2 and 3 table c 1 iteration 2 production oriented source land use category region hill extensive med intensive intensive other sum target land use category extensive 54 54 med intensive 797 147 944 intensive 112 0 other region mountain extensive med intensive intensive other sum target land use category extensive 9 9 med intensive 0 intensive 0 other iteration 3 region hill extensive med intensive intensive other sum target land use category extensive 54 54 med intensive 112 1400 1512 intensive 112 0 other region mountain extensive med intensive intensive other sum target land use category extensive 0 med intensive 69544 5558 75102 intensive 0 other 
26037,bayesian networks bns are commonly used to model socio ecological systems as their graphical structure supports participatory modelling they can integrate quantitative data and qualitative knowledge and account for uncertainty although the spatial and temporal dimensions are important in socio ecological systems there is a lack of openly available and easy to use tools to run bns with spatial data over time here we present gbay gbay ethz ch an online platform where users can link their bns to spatial data run the network iteratively to incorporate dynamics and feedbacks and add geo processing calculations to account for spatial interactions we demonstrate the use of this tool on the examples of a modelling a regulating ecosystem service where we account for neighbourhood effects and land use decisions where we include regional level boundary conditions the gbay platform supports users with its graphical interface and instructive wiki page and provides a step towards more accessible and flexible socio ecological modelling keywords bayesian networks online tool ecosystem services land use decisions spatial interactions 1 introduction as ecosystems undergo changes that jeopardize their capacity to provide essential services to society cardinale et al 2012 foley et al 2005 natural resource managers and landscape planners face challenging decisions on sustainable landscape development wu 2013 modellers aim to support these decisions e g through mapping ecosystem services and assessing trade offs between them carpenter et al 2009 or predicting scenarios of future land use carpenter et al 2015 verkerk et al 2018 however modelling complex socio ecological systems requires integrating various types of information hamilton et al 2015a such as earth observation and in situ data empirical or process based models and socio economic data models are often associated with high uncertainties due to both the inherent variability of socio ecological systems and the common lack of data ascough et al 2008 ropero et al 2013 at the same time local experts and stakeholders often have valuable knowledge about their socio ecological systems and involving them in the modelling process facilitates communication and learning ruckelshaus et al 2013 voinov and bousquet 2010 involving stakeholders and producing credible results that can support decision making requires a flexible and transparent modelling process jakeman et al 2006 voinov et al 2016 an increasingly common approach to deal with these challenges is the use of bayesian networks bns directed graphs where variables are linked through conditional probabilities marcot and penman 2019 key advantages of bns include their capacity to integrate qualitative and quantitative information their explicit treatment of uncertainty and their graphical structure uusitalo 2007 the graphical structure of a bn represents causalities in the modelled system which increases modelling transparency in comparison to black box e g empirical models jakeman et al 2006 and facilitates communication with stakeholders voinov and bousquet 2010 for example co developing bns with stakeholders has been used to address ambiguities in water management henriksen et al 2012 and to build a common understanding of an agricultural socio ecological system salliou et al 2017 a bn of forest ecosystem services provided a common language for experts from different fields thus supporting planning gonzalez redin et al 2016 different types of information can be integrated in a bn since the links between variables in a bn can be quantified individually borsuk et al 2004 often information on some components of the modelled system is already available in the form of empirical and process based models which can be translated to conditional probabilities borsuk et al 2004 stritih et al 2019 bns can also learn relationships between variables directly from data stelzenmüller et al 2010 such as remote sensing dlamini 2010 water quality measurements ames et al 2005 or species observations hamilton et al 2015b when data are scarce or unavailable they can be supplemented with expert knowledge ames et al 2005 borsuk et al 2004 hamilton et al 2015b pollino et al 2007 the probabilistic structure of bns means that uncertainties are explicit and propagated through the network socio ecological systems are inherently complex and variable leading to high uncertainties that are exacerbated by limited data availability regan et al 2002 it is particularly important to consider these uncertainties in risk assessments where unlikely extreme events are relevant grêt regamey and straub 2006 mcdonald et al 2016 bns can be used to identify knowledge gaps hamilton et al 2015b stritih et al 2019 and can easily be updated as soon as new information becomes available hamilton et al 2015b in environmental applications the spatial and temporal components are often crucial carpenter et al 2009 the spatial composition of ecosystems and land use in landscapes is essential to their function and needs to be taken into account when trying to understand landscape change or identify trade offs or synergies between ecosystem services nelson et al 2009 raudsepp hearne et al 2010 therefore models of socio ecological systems are often spatially explicit spatially explicit bns where the network is linked to a raster have been used to model scenarios of future land use carpenter et al 2015 celio et al 2014 and map ecosystem services gonzalez redin et al 2016 grêt regamey et al 2013 landuyt et al 2013 villa et al 2014 the temporal dimension has been addressed less frequently in bn modelling as bns are most commonly static and the construction of dynamic bns is often seen as cumbersome uusitalo 2007 dynamic bns use the time sliced approach kjaerulff and madsen 2013 where each variable of the system is represented by a separate node in each time step resulting in a copy of a network for each time slice with temporal links between these iterations this approach can be used to model landscape changes over time chee et al 2016 in most spatial applications of bns so far the models have been run for every individual pixel of a raster a major limitation of this approach is that is fails to take into account spatial interactions landuyt et al 2015 stritih et al 2019 and cross scale effects which often have an important influence both on ecological and socio economic processes peters et al 2007 for example a habitat is only suitable for a species if it is large enough to support a viable population or connected to other habitats farmers decisions to cultivate a parcel of land may depend on the decisions of other farmers or an overarching policy that prescribes certain amounts of ecological set aside to be eligible for subsidies celio and grêt regamey 2016 in case of ecosystem services such as flood protection or pollination the provision and demand for the services do not occur at the same location bagstad et al 2013 and the provision of services is related to the spatial composition of ecosystems in the landscape grêt regamey et al 2014 syrbe and walz 2012 therefore interactions across space at different levels should be taken into account when modelling socio ecological systems several tools have been developed to run bns with spatial data see table 1 but most do not explicitly support iterative inference over time feedback loops or spatial interactions for such more complex applications modellers typically use the api of common bn software packages such as hugin or netica pérez miñana 2016 to link their bns to spatial data celio et al 2014 chee et al 2016 grêt regamey et al 2013 sun and müller 2013 however there is a lack of openly available and easy to use tools i e including a graphical user interface which would allow users to run spatially explicit bns over multiple time steps in this paper we present gbay an online platform with a simple graphical interface that links bns to spatial data users can run their bns iteratively over multiple time steps with raster or vector data in addition the platform includes the possibility to account for spatial interactions such as neighbourhood effects we describe the architecture of the platform and its use furthermore we illustrate how accounting for effects at different spatial scales such as neighbourhood effects and regional boundary conditions can help improve the realism and reduce uncertainties in models of ecosystem services and land use change we discuss the advantages of bns and the gbay platform as well as the limitations of this modelling approach and ongoing challenges 2 methods 2 1 bayesian networks a bayesian network is a directed acyclic graph with an underlying joint probability distribution jensen 2001 kjaerulff and madsen 2013 pearl 1988 it consists of nodes representing variables each with a set of mutually exclusive states the states of a node can be categorical e g land use types or quantitative e g the distance to the nearest forest the links between nodes represent the directed causal relationships or dependencies between these nodes e g x y the joint probability distribution p x y of the nodes is condensed in conditional probability tables cpts which contain the probability distribution of each node for each combination of its parent nodes states the probability that node y is in state y can be calculated by summing its conditional probabilities over the states x of its parent nodes p y y xp y y x x p x x in a process called marginalization kjaerulff and madsen 2013 fig 1 shows an example of a bn which predicts land cover change in a system where meadow abandonment leads to forest encroachment the future land cover lc t1 is a child node of the current land cover lc t0 and the intensity of agricultural use the causal relationships between the nodes are quantified in the cpt of node lc t1 which specifies the belief that the future land cover will be either a meadow or a forest for each combination of the states of its parent nodes once the bn is compiled it can be updated for specific cases by adding evidence evidence can be data e g when we know the type of land cover in a pixel or scenarios e g when we explore what happens in a system if agricultural use changes when we know the state of a node with 100 certainty this is called hard evidence e g that the agricultural intensity of a pixel is low while soft evidence contains some uncertainty and is in the form of a probability distribution e g our observation indicates that the land cover is a meadow with 70 probability and a forest with 30 probability when evidence is added to the network the joint probability distribution is updated through a process called inference which results in a posterior probability distribution ppd of all the nodes in the network thus providing information about the expected most likely state of target nodes as well as the associated uncertainty jensen 2001 evidence can also be propagated along a chain of nodes e g x y z according to the chain rule p x y z p z y p y x p x and from child nodes to parent nodes for example in the network in fig 1 knowledge about current land cover could be used to infer the past land cover 2 2 coupling bns and spatial data with gbay here we present gbay bayesian networks with geo data an online tool to link a bn to spatial data and run a process over multiple time steps fig 2 illustrates the functionalities of the gbay platform spatial data is used as evidence on specific nodes in a bn inference is then performed for each pixel or object of the input data where the output is a probability distribution across the possible states of target nodes for each spatial unit the outputs of inference can be used as inputs in the next iteration to account for temporal dynamics see section 2 3 in addition spatial inputs or outputs can be processed with a python script to account for spatial interactions at different scales see section 2 4 the gbay platform consists of an online graphical user interface fig 3 where the users can upload a network in the dne format developed in netica or a similar bn software the uploaded network is visualized in the gui where users can select one or more target nodes the ppd of which they wish to calculate spatial data can be added to the network in the form of raster a geotiff file for each input node or vector files a shapefile or geodatabase with attributes corresponding to input nodes by dragging the file to the designated location in the network or by using the menu provided for each node gbay can take into account both hard and soft evidence see table 2 to set hard evidence the input raster or attribute table of the vector data contains only one value per pixel or object for soft evidence the input raster or vector file has a band or attribute for each state of the input node in addition users can set non spatial hard or soft evidence for the whole area by simply clicking on the state of the node or entering the soft evidence probabilities all configurations links iterations hard and soft evidence including the corresponding geo data can be saved and reloaded later if necessary the output has the same geometry as the input spatial files and contains the probability of each state of the target node for each spatial unit i e the whole ppd in a multi band raster or attribute table as well as information about the most likely state in addition shannon s evenness index of the ppd is calculated j h hmax where h i 1 n p i l o g 2 p i hmax log2 n where pi is the probability of state i and n is the number of states the index is a standardized measure of entropy which expresses uncertainty and can be compared between nodes with different numbers of states marcot 2012 it has values between 0 and 1 where 1 denotes a uniform distribution between all possible states maximum uncertainty and 0 denotes complete certainty about the state of the node for continuous target nodes the output additionally contains information about the mean median and standard deviation of the ppd running bns with large spatial data can be computationally intensive at the moment gbay runs on a virtual server ubuntu 16 04 4 with 6 cores at 3 ghz and its processing speed depends on the size of the network and spatial data for example when running a network of 17 nodes and 1128 cpt rows with four input rasters gbay can process around 4000 pixels per second when processing large networks or datasets users can receive the outputs via email in case of a browser timeout user data including bns spatial data and scripts are automatically deleted from the server after one day 2 3 temporal dynamics through iterations bayesian networks usually represent a static state of the studied system and one of their major drawbacks is that they cannot incorporate feedback loops uusitalo 2007 this limitation can be overcome by dynamic bns using the so called time slicing approach kjaerulff and madsen 2013 where each time step is represented by a separate network however developing such dynamic bns can be very cumbersome uusitalo 2007 in gbay a simplified version of the time slicing approach is implemented where the bn is run iteratively in multiple time steps and the outputs of one time step are used as inputs to the next for example when modelling land cover change we start with a map of current land cover lc t0 during one time step land cover change takes place and through inference we obtain the probability distribution of land cover after the first time step lc t1 e g after 5 years this lc t1 then becomes the input for starting land use in the second time step in other words the result of one iteration is used as starting condition for a second iteration see celio et al 2014 for an example on the gbay platform bns can be run iteratively by specifying temporal links between nodes representing time steps and the number of iterations for example if the output node lc t1 is selected as a link node an arrow appears that can be connected to the corresponding input node lc t0 multiple links can be used reflecting different variables that are connected over time 2 4 multi scale processes using python scripts the gbay platform can account for spatial processes at different levels corresponding to different types of geoprocessing operations tomlin 1994 in the basic mode of gbay inference is performed at the local level for each individual pixel or object however gbay also provides the option to consider processes at different levels calculations across scales can be implemented by running an intermediate processing python script indicated with a script icon in fig 2 at the focal level a python script can be used to take into account the neighbouring pixels or objects e g to obtain the land cover of neighbouring pixels within a specified window or calculate the distance to the nearest pixel of a specified land cover type in the land cover change example forest encroachment on a meadow depends on the distance to the nearest forest patch which can be calculated from the input land cover raster using a python script see appendix a this information can be used to set new evidence on a node e g distance to forest at the zonal level the python script evaluates pixels or objects across the whole study area for example to check whether regional boundary conditions have been reached an example of such boundary conditions is a minimum percentage of a specific land use category defined by an agricultural policy a python script can be run before performing inference i e to calculate spatial evidence such as focal statistics based on the input data or in between iterations it can also be used to modify evidences over time e g to implement a policy that changes between time steps gbay currently supports intermediate processing scripts written in python using openly available libraries including gdal ogr numpy and math python was chosen as the language of the intermediate processing scripts since it is one of the most widely used programming languages with a large community particularly in spatial modelling and provides many open access libraries a set of scripts to model spatial interactions at the focal and zonal levels are available on the gbay wiki and can be downloaded and adapted in addition advanced users can develop their own scripts where the input and output format must match the format used by gbay a list of nodes containing an array of probabilities across states for every pixel or object see appendix a for details it is important to note that the processing time of gbay increases when more complex geoprocessing is performed two examples of bn models that incorporate spatial interactions are described in more detail below 2 5 case studies 2 5 1 avalanche protection in davos accounting for neighbourhood effects protection from snow avalanches is one of the most important ecosystem services provided by forests in the swiss alps grêt regamey et al 2008 an avalanche release is less likely inside a forest bebi et al 2009 and forests also reduce the mass and velocity of avalanches that flow through them feistl et al 2014 the release and size of avalanches depend on terrain characteristics and snow conditions the protection capacity of the forest is related to its structure and species composition and the value of the service depends on the risk to settlements and infrastructure a bn was used to combine earth observation data on terrain and forest structure existing process based and empirical models about the avalanche process and expert knowledge about risk factors the bn was run with spatial data to map the provision and demand for avalanche protection in the region of davos switzerland stritih et al 2019 the resulting maps of avalanche protection contain large uncertainties and a sensitivity analysis was used to identify the key sources of uncertainty in the model one of the main sources of uncertainty was the definition of potential release areas of avalanches the probability of an avalanche release depends on topography slope curvature terrain roughness as well as snow conditions in the bn the topographical factors were combined using fuzzy logic veitinger et al 2016 for each factor a membership function describing the probability that a pixel belongs to a potential release area as a function of the factor was defined by experts the membership function describes the probability that a pixel belongs to a potential release area as a function of the factor for example the factor of slope has a trapezoid like membership function where avalanche releases can occur on slopes between ca 28 and 55 but the release probability is highest between 35 and 45 the factors of slope curvature and terrain roughness were then combined using a fuzzy and operator for details see veitinger et al 2016 to fill the cpt of the node release this way an avalanche release probability can be calculated for each pixel of the study area this pixel based approach neglects the interactions between neighbouring spatial units i e whether a release pixel is connected to other release pixels however the probability of an avalanche release depends on the size of the potential release area bühler et al 2013 an avalanche release can only occur when there is a sufficient volume of snow to be released which depends on the amount of snow i e the avalanche release depth which is estimated using a probability distribution of maximum new snow based on long term observations slf 2017 and the size of the release area in order to incorporate neighbourhood effects we implemented an updated version of the bn from stritih et al 2019 in gbay the model is implemented in two iterations see fig 4 first the avalanche release probability of each individual pixel is calculated based on its slope curvature and roughness then these probabilities are used as an input to a python script that calculates the size of the release area spatial metrics such as patch sizes are commonly calculated based on boolean class memberships either a pixel is a release area or it is not however since the definition of release areas is uncertain such an area calculation would depend on an arbitrary threshold probability e g 50 at which we consider a pixel to be in a release area to avoid this problem we used a fuzzy geographical area calculation fonte and lodwick 2004 we defined a set of probability thresholds α between 0 and 1 for each threshold all pixels with p release α were considered to be release pixels and adjoining release pixels form a release area a release area size was calculated for each α and based on the different sizes for different threshold probabilities we could estimate a probability distribution of release area size see appendix b for an illustration in the second iteration the probability distribution of release area size was used as soft evidence on the node release area size combined with the maximum new snow height the release area defined whether the snow volume release area new snow height was sufficient for an avalanche release if the snow volume was below the threshold for small snow avalanches as defined by the canadian classification of avalanche sizes slf 2018 we assumed that the release will not occur setting the release corrected probability to zero the updated bn model of avalanche protection was run in gbay with spatial inputs at a 5 m resolution for the dischma valley in davos the whole network is illustrated in appendix b 1 the release probability the total provision of avalanche protection and the associated uncertainty were calculated and compared with the results of the previous model that did not account for release area size 2 5 2 implementing boundary conditions for land use change in the entlebuch unesco biosphere land use decisions have a strong impact on landscape development and are influenced by an interplay of biophysical and socio economic factors policies and personal preferences celio and grêt regamey 2016 used a participatory approach to develop a model of farmers decisions and resulting land use change in the entlebuch unesco biosphere in the canton of lucerne switzerland after identifying potential factors influencing land use decisions through literature review an expert group was formed the experts weighted the influencing factors to find a subset of the most relevant variables and defined the causal relations between them then they defined node states and the conditional probabilities the bn was updated with local actors knowledge and validated through a review by experts celio et al 2012 for a detailed description of the participatory modelling process see celio et al 2014 the resulting bn predicts land use change based on biophysical factors such as slope and potential natural vegetation agricultural policy amount and types of direct payments zoning e g vicinity to a residential area and individual farmers characteristics such as their education whether they have a part time business and their view on ecological policies see fig 5 the land use change probabilities are defined for a time step of 5 years and the bn can be run iteratively to model longer periods the bn was used to model scenarios of agriculture policy ap old agricultural direct payments or the more ecology oriented agricultural policy implemented in 2014 and farmer characteristics production or ecology oriented farmers the scenario maps illustrated the trends of the different combinations of aps and actor characteristics however the scenarios were calculated only taking into account individual parcel information not considering limitations on the regional scale such as prescribed minimum amounts of specific land use types to support cattle production the cell level approach means that the exogenous limits of farmers decisions were neglected in order to account for the regional boundary conditions we adapted the bn developed by celio and grêt regamey 2016 for agricultural land use and implemented it in gbay the limits of land use change were defined based on the maximum number of cattle grazing per hectare as defined by the federal office for the environment 2013 assuming that the number of cattle in the region remains constant we estimated the minimum area of extensive medium and intensive agricultural land required to fulfil this legal obligation this limited the conversion of agriculture to other land use types through extensification and abandonment when certain minimum areas of agricultural land use had been reached no further cells were converted to other land use types this boundary condition was implemented at the end of every iteration time step of the network in gbay using a python script we checked the amount of extensive medium intensive and intensive agriculture cells across the whole study area if the required amount of a certain agricultural land use category was not reached the script searched among the cells that had been converted from other land use categories to find those where the change was least likely and converted them back to their previous probability distribution until the minimum area of the category was reached in other words land use change was prevented by the minimum amount condition in those cells where it was least likely to occur this roll back mechanism is explained in more detail in appendix c 3 case study results 3 1 avalanche protection we mapped the provision of avalanche protection and associated uncertainty in the dischma valley in davos using a bn adapted from stritih et al 2019 since the definition of avalanche release areas was a major source of uncertainty in the model we adapted the model to account for neighbourhood effects in the release process fig 6 a and 6b show the resulting maps with and without accounting for spatial interactions where the colours indicate the mean value of avalanche protection provision expressed in height of snow stopped and the uncertainty entropy of the posterior probability distribution the most important areas providing avalanche protection are steeper densely forested areas but the model shows a high spatial heterogeneity and high uncertainty in the basic model without neighbourhood effects the mean coefficient of variation across the whole study area amounts to 95 when taking the size of the release area into account the spatial pattern remained similar but the uncertainty was reduced mean cv of 87 see appendix b table b 2 fig 6c and 6d show the release probability without and with the correction for spatial interactions release area size the bn that accounts for the release size results in fewer release areas fig 6d as smaller areas are less likely to reach a sufficient volume of snow for an avalanche release in addition in areas that are originally assigned a low release probability the probability is additionally reduced as they are unlikely to form part of a large release area thus a clearer spatial pattern of potential release areas emerges with the mean entropy uncertainty of the release probability map reduced from 29 to 19 see appendix b table b 2 3 2 land use decisions the bn of agricultural land use decisions in the entlebuch was run in the iterative mode in gbay with and without the inclusion of boundary conditions minimum area of medium and intensive agriculture due to legal requirements for cattle breeding the resulting land use maps and distribution of land use types are shown for two scenarios production oriented farmers with the old direct payment system and ecology oriented farmers with the new agricultural policy across three time steps fig 7 in both scenarios the boundary conditions had an effect on the final land use change in the ecology oriented scenario the farmers decisions drive extensification leading to a rapid loss of intensive agricultural land when no limits are implemented when the boundary conditions are implemented the minimum is reached very quickly within one time step preventing further land use change in the production oriented scenario the boundary conditions have a smaller effect as farmers are more likely to maintain their intensive agriculture however the medium intensive plots are converted to extensive use if the minimum limits are not implemented 4 discussion in this paper we presented gbay an openly available online platform for spatially and temporally explicit bayesian networks the platform offers an easy to use gui to run bns with spatial data over multiple time steps as such it aims to facilitate spatial bn modelling of socio ecological systems by including the temporal component and spatial interactions as well as making it more accessible to practitioners bn models can be used to integrate different types of information account for uncertainty and can facilitate participatory modelling in the following we discuss how the gbay platform can help users draw on these advantages as well as the associated challenges and limitations in addition we discuss the implications of our case studies for landscape planning 4 1 integrating information across scales data on socio ecological systems is becoming increasingly available through sources such as earth observation and social media and information is also available in the form of local actors or expert knowledge bns are well suited to integrating these different types of information as is illustrated in the avalanche protection case study where remote sensing inputs were combined with process based empirical models and expert knowledge stritih et al 2019 however while bns are commonly used to integrate information about a static system the temporal and spatial are often not explicitly represented in bns although they are essential in most socio ecological systems hamilton et al 2015a the gbay platform provides the possibility to incorporate dynamics by using the iterative bn approach which can include feedback loops thus addressing one of the major limitations of bn models kelly letcher et al 2013 uusitalo 2007 however the iterative bns are mainly suitable for systems where one or few variables change over time e g land use change while other variables act as drivers of this change such as state and transition models see chee et al 2016 and feedbacks only occur between time steps for more complex dynamic interactions other modelling approaches such as coupled component models or system dynamic models may be more appropriate kelly letcher et al 2013 lauf et al 2012 schreinemachers and berger 2011 in addition to the temporal component gbay can also account for processes that occur at different spatial scales or organizational levels socio ecological systems are influenced by different processes at different scales verburg et al 2004 and interactions between these processes across scales can result in non linear dynamics or threshold effects peters et al 2007 in bn models processes at higher organisational levels e g regional policies market conditions climate are often represented by a node in the network celio et al 2014 grêt regamey et al 2013 kleemann et al 2017 but potential feedbacks from the lower to the higher level are not accounted for using the python module in gbay the cumulative effects at the local level can be calculated and used to update the higher level node in the next time step for example while land use decisions the level of individual parcels depend on regional policies rapid land use change across many parcels may in turn affect the policies in a feedback effect that can be accounted for in gbay while the python module increases modelling flexibility and allows us to incorporate spatial interactions or boundary conditions the intermediate calculations used to modify bn inputs or outputs should be compatible with the probabilistic logic of bns the explicit treatment of uncertainties is a major advantage of bns but it is challenging to include the information about the whole probability distribution per each pixel in spatial calculations a simple approach is to set a threshold probability each pixel with a probability of forest above 50 is considered a forest in a neighbourhood calculation however this means a loss of information about the probability distribution and results can be strongly affected by the arbitrary threshold arnot et al 2004 to deal with this fuzzy landscape metrics can be applied to account for uncertain membership in a class e g land cover arnot et al 2004 fonte and lodwick 2004 such as the fuzzy area calculation used in the avalanche protection example however these do not account for variability in spatial processes e g flows in ecosystem services assessments the directional flow between service providing and receiving areas is important to account for es flows in space in a probabilistic manner johnson et al 2012 have used a combination of bns and agent based models that simulate the flow of es units such an approach would add an additional level of complexity but offers a probabilistic perspective on spatial processes that should be addressed 4 2 dealing with uncertainty socio ecological models often contain high uncertainties partly due to limited data measurement errors and subjective judgement but partly also related to the inherent spatial and temporal variability of the modelled systems regan et al 2002 these uncertainties should be acknowledged and taken into account in decision making maier et al 2008 a major advantage of bns is that uncertainties can be explicitly accounted for and propagated through the models stritih et al 2019 uusitalo 2007 the output of inference in gbay contains a probability distribution across all the possible states of the target node for each pixel or object of the study area which means that the output uncertainty can be quantified in a spatially explicit way gbay automatically provides the user with measures of uncertainty including the entropy marcot 2012 and standard deviation for continuous nodes of the probability distribution these can be used to map uncertainties as demonstrated in the avalanche protection case study see fig 6 mapping uncertainties is particularly important when these are spatially heterogeneous such as in remote sensing based classifications petrou et al 2013 although maps of uncertainty can be easily generated in gbay interpreting them is not straightforward landuyt et al 2015 the way in which spatial uncertainties are visualized may have a strong effect on how they are understood by end users kunz et al 2011 in the avalanche protection example fig 6 we used a bivariate map to depict both the modelled value and associated uncertainty with darker colours indicating higher uncertainty thus drawing attention to areas of high uncertainty kunz et al 2011 in other applications it may be more appropriate to emphasize areas of higher certainty by linking uncertainty to transparency or fuzziness since different types of users prefer different visualizations of uncertainty maceachren et al 2005 gbay is currently limited to providing data on uncertainty which users can use in their preferred visualization mode 4 3 increasing the accessibility and transparency of bn modelling involving stakeholders in modelling socio ecological systems can increase the credibility of model results and support learning jakeman et al 2006 voinov and bousquet 2010 a key requirement for credible participatory modelling is transparency voinov and bousquet 2010 bns have been promoted as a tool for participatory modelling due to their transparent model structure and capacity to incorporate expert knowledge bromley 2005 this type of use is demonstrated in our land use decision case study where the model was co developed with experts from different fields and updated with local stakeholder knowledge however participatory modelling is an iterative process and models should be updated as new information becomes available which is often not within the frame of research projects models are more likely to have an impact on decision making when local experts and decision makers take ownership of the model jakeman et al 2006 and can generate new results as new information becomes available in an iterative process ruckelshaus et al 2013 open access and easy to use web based tools can support the adoption of models by local experts and practitioners voinov et al 2016 although the structure of a bn model is in itself transparent and many graphical tools are available to develop bns the application of bns to spatial data usually requires programming skills to use the api of bn software such as netica or hugin pérez miñana 2016 the gbay platform aims to reduce this gap and make spatial bns more accessible to a wider range of users because of its simple user interface users without programming experience can use gbay to link their bns with spatial data this is supported by the gbay wiki page wiki gbay ethz ch with instructions examples of bns and associated data that can be downloaded to test the platform nonetheless developing a bn is not straightforward model co development with stakeholders is a time consuming process and it is important to ensure stakeholder diversity and consider group dynamics voinov and bousquet 2010 when experts are asked to parametrize a bn model challenges include potential biases kuhnert et al 2010 fatigue during elicitation of extensive cpts das 2004 and over confidence speirs bridge et al 2010 when learning a bn from data the quality of the model is limited by the quality and amount of data available hamilton et al 2015b because of such challenges making bn modelling more accessible will require not only tools such as gbay but also training and capacity building among potential users although the code of gbay is published it is based on the proprietary netica api norsys 2010 in addition the platform is not designed for bn development and requires users to upload their own bns in the dne format as developed in netica genie bayesfusion 2017 or a similar bn software netica is currently the most commonly used bn software in the ecosystem service modelling community pérez miñana 2016 and to our knowledge no open source software currently offers a graphical interface for bn development with comparable functionalities including the integration of discrete and continuous nodes learning from data and sensitivity analyses the development of such an open source software would be an important step towards increasing the accessibility and transparency of bn modelling 4 4 implications for environmental management and landscape planning processes our case studies on ecosystem service mapping and land use decision modelling demonstrated the use of gbay for spatial bns incorporating focal neighbourhood effects and zonal boundary conditions accounting for such spatial interactions can help to reduce uncertainties improve model realism and take into account knowledge at different scales or organizational levels high uncertainties in ecosystem services maps limit their usability as a support for decision makers andrew et al 2015 schulp et al 2014 in the case of the avalanche protection service section 3 1 1 considering neighbourhood effects between pixels in potential avalanche release zones reduces overall uncertainty by excluding areas that are too small to produce an avalanche release however due to the fuzzy geographical area calculation algorithm the corrected release probability is also reduced in large areas of low p release which could lead to neglecting large releases that occur only under very extreme conditions thus adding the area condition likely reduces false positives i e increases the specificity of detecting release areas but may also lead to more false negatives some release areas may be excluded higher levels of specificity in detecting potential release areas may be useful to identify the forest patches that play the most important role in preventing avalanche releases which is important in prioritizing the management of these protection forests teich and bebi 2009 however the purpose of hazard risk mapping it is also important to consider releases that only occur in extreme snowfall conditions with very low probabilities although validation data on these extreme events is lacking bühler et al 2018 better estimates of extreme scenarios could be achieved by running the bn for a scenario with high new snow or choosing a low threshold for pixels to be considered part of a release area in the land use decision case study section 3 1 2 taking into account boundary conditions offers a more regional perspective where farmers decisions are limited by regulations in other words the more realistic representation limits the option space the constrained model may be more useful for short term forecasts of landscape development under the assumption that boundary conditions will stay constant while unconstrained exploratory models can better represent the whole range of possible futures maier et al 2016 rounsevell and metzger 2010 modelling more extreme scenarios may be useful to clearly observe the impacts of different scenarios of agricultural policy and farmers characteristics and may offer a wider perspective on potential solutions in landscape planning processes hence combining both perspectives i e with and without boundary conditions and observing the differences between them can yield additional insights in our case study the comparison demonstrates how strongly the decision making of farmers at their plot level is constrained by larger scale regulations in both cases studies the appropriate choice of method e g considering boundary conditions or not and the interpretation of results will depend on the needs of decision makers which highlights the need to involve stakeholders and decision makers in the modelling process voinov and bousquet 2010 tools such as gbay can contribute to the flexibility and accessibility of modelling socio ecological systems over time and space and thus have the potential to support decision makers in environmental management and landscape planning software availability name gbay bayesian networks with geo data developed by orencio robaina enrico celio ana stritih sven erik rabe eth zürich plus availability online at gbay ethz ch free for non commercial use software requirements netica norsys or similar software to create bayesian networks programming language web interface in html javascript back end in c using the netica api python to support intermediate processing scripts source code https github com ethzplus gbay instructions and examples available at wiki gbay ethz ch declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the european union s horizon 2020 research and innovation programme ecopotential project grant agreement no 641762 and by the swiss programme for research on global issues for development r4d programme funded by the swiss national science foundation snsf and the swiss agency for development and cooperation sdc grant no 100400 152167 the authors would like to thank ralph sonderegger for graphic design yves bühler and peter bebi for discussion on avalanche modelling and two anonymous reviewers for constructive comments the authors are grateful to the cantonal administration of lucerne for the provision of spatial data appendix a python scripts in order to account for spatial interactions and processes and different scales users can implement a python script in gbay in order to be compatible with gbay the uploaded script file needs to implement a function named process with the following definition process gdaldataseth dataset list nodes data int iteration the inputs to the process function are gdaldataseth dataset contains the metadata of the spatial data being processes e g raster spatial extent pixel size and projection gbay uses gdal to operate with spatial data list nodes data contains the data of the nodes that are used as inputs for the geoprocessing script the node data is stored as a python dictionary with three keys o name str name of the node o type int type of the node py discrete py continuous py discrete if omitted o data list list of probabilities between 0 and 100 of each state for each raster cell or object int iteration the number of the current iteration which can be used if some inputs should be modified over time the process function should return a list of nodes with the updated node likelihoods if the output node is discrete or node values if the node is continuous the python script should import the node utils python module which contains functions to validate the output and to read and write node information as well as other packages used by the script e g gdal scipy gbay stores the probabilities of the nodes selected by the user as to be used by the python script creates the nodes data list and runs the process function then it runs a function to validate whether the output complies with the nodes data format and if it does it will set the node probabilities as returned by the function this happens at the beginning of the processing and at the end of each iteration in case the results are not validated e g the data types are incompatible or total probability does not add up to 100 or in case an error occurs in the execution of the script gbay will print out the error message and ignore the output data it may also occur that the results are correctly formatted but invalid form the bn perspective e g when trying to set a probability of a state that would be impossible according to the node s cpt and the evidence set on its parent in this case gbay will print out an error message from netica example besides the factors affecting future land cover described in fig 1 the transition of meadows to forest may also be affected by the distance to the nearest forest patch if a node distance forest is added to the network its values can be calculated based on the input land cover map directly in gbay using a python script image 1 appendix b bayesian network for avalanche protection to model the provision of avalanche protection we used a model adapted from stritih et al 2019 see figure b 1 the model was modified to account for neighbourhood effects in the avalanche release process where a pixel is only accounted as a potential avalanche release if it is part of a sufficiently large release area we used a python script to calculate fuzzy release areas based on release probabilities as illustrated below this led to lower uncertainty in the definition of release areas and in the total provision of avalanche protection see table b 2 fig b 1 bayesian network used to model the provision of avalanche protection adapted from stritih et al 2019 the bn uses inputs indicated with a thicker frame from remote sensing and avalanche data to infer about the ecosystem structure and processes which determine the detrainment snow braking in the forest during an avalanche and prevention functions these functions are combined to express the total level of avalanche protection provision the orange arrow indicates where a python geoprocessing script is used to calculate the size of avalanche release areas from per pixel release probabilities fig b 1 fuzzy area calculation the raster figure b 2 shows the probability p release of each pixel belonging to a release area a fuzzy release area size is calculated for the pixel shown in red first the area is calculated for different threshold probabilities α where every pixel where p release α is considered part of the release area this results in a different size of release area for each probability table b 1 from which a cumulative probability distribution can be derived in this case the release area is between 4 and 19 pixels based on this probability distribution we can calculate the probabilities of the area belonging to a size class figure b 3 fig b 2 example raster of p release fig b 2 fig b 3 resulting cumulative probability distribution of area black line and the probability distribution of area in classes 1 5 5 10 10 15 15 20 pixels fig b 3 table b 1 area calculation for different α values table b 1 threshold probability α area 0 05 19 0 1 19 0 2 18 0 3 14 0 4 10 0 5 9 0 6 7 0 7 6 0 8 6 0 9 5 0 95 4 results uncertainty of total provision and release probability with and without accounting for release area size table b 2 mean uncertainty in total provision of avalanche protection and release probability across the whole study area expressed in coefficient of variation cv only for continuous nodes and entropy index with and without the correction for release area size neighbourhood effect table b 2 without neighbourhood correction with neighbourhood correction node cv uncertainty cv uncertainty provision 95 0 089 87 0 083 release 0 29 0 19 appendix c the roll back mechanism to implement boundary conditions for land use change in order to implement boundary conditions in the land use change model a minimum limit of extensive intensive and medium intensive land use to support the number of cattle in the region a python script was implemented in gbay at the end of every iteration the script checks the number of extensive medium intensive and intensive agriculture cells and if the frequency is below the defined minimum it converts cells which have the highest probability of being in those categories back to their previous probability distribution rolled back until the minimum frequency agriculture has been reached in case not enough cells of medium intensive agriculture are available to convert back to intensive agriculture due to the minimum limit in this land use category cells from a third category e g forest are changed back to medium intensive and medium intensive cells are changed back to intensive in a double roll back the mechanism is illustrated in figure c 1 fig c 1 representation of the roll back mechanism to ensure that the minimum frequencies of land use 1 and 2 are maintained during the first iteration of the land use change bn lu1 is converted to lu2 and lu2 changes to lu3 however if the frequency of lu1 and lu2 drops below the minimum limit the roll back mechanism is implemented to revert cells back to their previous probability distribution until the minimum is reached fig c 1 the conversion matrix table c 1 shows how many cells have been transferred to other land use categories due to the enforced conversion limits in the production oriented scenario for iterations time steps 2 and 3 in both the hill and mountain region certain parcels were rolled back in the hill region the number in brackets show how cells were initially rolled back from other to intensive and in the following from intensive to medium intensive to fulfil the restrictions double roll back table c 1 rollback mechanism induced by python script made explicit for the production oriented scenario in iteration 2 and 3 table c 1 iteration 2 production oriented source land use category region hill extensive med intensive intensive other sum target land use category extensive 54 54 med intensive 797 147 944 intensive 112 0 other region mountain extensive med intensive intensive other sum target land use category extensive 9 9 med intensive 0 intensive 0 other iteration 3 region hill extensive med intensive intensive other sum target land use category extensive 54 54 med intensive 112 1400 1512 intensive 112 0 other region mountain extensive med intensive intensive other sum target land use category extensive 0 med intensive 69544 5558 75102 intensive 0 other 
26038,simulating karst spring discharge and land use change impacts in a recharge area is strategic for water resource management in many countries worldwide we introduce a user friendly modeling environment by integrating the recently proposed lukars land use change modeling in karst systems model into the freewat free and open source software tools for water resource management framework lukars is a rainfall discharge model for karst systems that simulates the impact of land use changes by changing the area of dominant hydrotopes i e landscape units with homogeneous soil and land use properties freewat is a free and open source toolkit for water resource management implemented in qgis desktop application the integration of lukars into freewat takes advantage of the gis capabilities to map visualize and change the relevant hydrotope shapefiles freewat provides a modular framework for pre and post processing tools that facilitate the setup calibration analysis storage and sharing of lukars models 1 introduction mathematical models are tools which are commonly applied to simulate different processes of the hydrologic cycle casper et al 2019 chiogna et al 2018 hartmann et al 2014a tuo et al 2018 typically these models are based on sophisticated mathematical frameworks developed and applied by researchers and water experts in recent years more and more open source codes are available to model various processes of the hydrologic cycle arnold et al 1998 borsi et al 2013 winston 2009 their complex nature makes them not intuitive for what relates to their applicability in particular those people who need to apply these tools e g water resources managers are not necessarily familiar with programming languages and are thus often not able to apply these models de filippis et al 2017 in the years 2015 2017 the european union funded a coordinated initiative to overcome this limitation within the horizon 2020 program the freewat project www freewat eu in freewat researchers from various european countries developed a free and open source plugin including tools for water resource management such as a hydrologic model and time series analysis methods in the qgis environment rossetto et al 2018 the aim of this plugin is to use qgis as a convenient and intuitive graphical user interface gui that enables practitioners to deal with sophisticated software generics rossetto et al 2019 although freewat still can be considered as new its broad applicability was already highlighted in multiple case studies such as the management of coastal aquifers modeling of groundwater surface water interactions and the assessment of climate change impacts on groundwater resources de filippis et al 2020 freewat comprises three main modules in which different tools are provided the pre processing module the simulation codes module and the post processing module so far the simulation codes module contains various packages of the modflow 2005 family harbaugh 2005 and modflow owhm hanson et al 2014 all of them being coded for physically based flow and transport modeling primarily in groundwater systems the modflow family of codes is only applicable for groundwater dominated systems but not e g for rainfall runoff modeling in the particular case of karst hydrology rainfall runoff models are commonly applied for spring discharge predictions mazzilli et al 2012 ollivier et al 2020 and to enhance the understanding of hydrological processes in karst aquifers duran et al 2020 sivelle et al 2019 karst aquifers are widely used as drinking water supply both in europe and worldwide and hence require appropriate model based management chen et al 2017a stevanović 2019 although promising physically based model codes for the distributed simulation of karst aquifer processes exist chen et al 2017b giese et al 2018 henson et al 2018 reimann et al 2011 reimann and hill 2009 and initiatives started to improve their applicability berthelin et al 2020 berthelin and hartmann 2020 a reasonable implementation of these models still is often restricted by data limitations jukić and denić jukić 2009 ladouche et al 2014 various types of rainfall runoff karst models exist with different conceptual approaches one common approach is to consider different combinations of the dominant flow compartments i e conduits and matrix as distinct buckets chang et al 2017 fleury et al 2007 jourde et al 2015 for this conceptual idea of representing karst systems mazzilli et al 2019 proposed an open source gui that is intuitively applicable also by practioners another way to conceptualize the behavior of karst systems in lumped models is to consider the complexity of recharge processes and the infiltration to slow and quickflow paths bittner et al 2018 hartmann et al 2012 ollivier et al 2020 given that no gui for this type of conceptual approach exists the implementation of such karst aquifer model in freewat makes this framework more comprehensive and applicable for a broader range of water resource management issues we want to contribute to the freewat modeling framework by implementing the recently proposed lukars model bittner et al 2018 2020 teixeira parente et al 2019 into the qgis integrated freewat environment lukars is a rainfall runoff model that can be used to predict spring discharge in karst system and was developed to perform land use change impact studies in karstic environments this bucket type model is based on the integration of dominant hydrotopes in a recharge area which are distinct spatial entities defined by homogeneous soil and land use properties arnold et al 1998 each hydrotope shows a specific hydrological response to recharge events depending on its specific properties since spatially distributed properties need to be defined for each hydrotope when setting up a lukars model integrating lukars in a gis environment can make the visualization application and evaluation of the model more intuitive moreover lukars is one of the few karst hydrologic models that takes into consideration the possibility of modeling land use change impacts on karst spring discharge this feature offers a possible solution to a timely challenge of the karst modeler community hartmann et al 2014b therefore we consider the gis integration of lukars into freewat as a valuable contribution to provide applicable and open source tools for water resources management in section 2 we describe the concept of lukars and how the technical integration of the model into the qgis freewat environment was performed in section 3 we show how lukars fits into the existing modular structure of freewat for a better reproducibility we showcase the lukars tool in freewat with a case study application finally we discuss and conclude our efforts in section 4 and 5 2 materials and methods in this section we provide a short description about the conceptual idea of lukars fig 1 a and b the related assumptions and the flow components that are simulated by the model moreover we provide a technical summary how the integration of lukars into freewat was achieved 2 1 concept of lukars the concept of lukars assumes that the dominant flow components in a karst system i e the quickflow through conduits and fractures and a slow flow through the matrix are controlled by the physical properties of the shallow subsurface i e the soil epikarst system if the soil properties of a hydrotope are characterized by shallow soils with a coarse grained texture e g hyd 2 in fig 1a the quickflow paths are better connected and the hydrotope shows a fast and intense response to a given input signal e g precipitation in a hydrotope dominated by deep and more fine textured soils e g hyd 4 in fig 1a the quickflow is less intense and matrix infiltration is more constant in lukars each hydrotope is represented by a distinct bucket that has three different flow components fig 1b i e the quickflow qhyd the infiltration into the rock matrix qis feeding the baseflow storage b and a secondary spring discharge qsec qhyd is activated once a hydrotope specific storage value emax was exceeded and stops after the hydrotope storage reaches a lower threshold emin by that a hysteretic behavior of the soil epikarst system is simulated qis and qsec are both implemented based on linear transfer functions in contrast to qis qsec is only active if the hydrotope storage is higher than a defined threshold for secondary spring discharge esec for more information about the mathematical details the interested reader is referred to the works of bittner et al 2018 teixeira parente et al 2019 and bittner et al 2020 where lukars was applied for the case study of waidhofen an der ybbs fig 1c 2 2 lukars integration in freewat the goal of the lukars integration was to take advantage of the modular framework of freewat and using the provided pre and post processing tools this concept is shown in fig 2 which highlights the use of the gis to create and visualize the hydrotope shapefiles as well as the freewat input data management in the pre processing step in the post processing the integration of lukars benefits from the existing time series analysis tools in freewat and the gis functionalities to store the hydrotope parameters in shapefiles the integration of all freewat tools into the qgis environment is performed with the python programming language although most of qgis is written in c it allows for connecting python scripts as standalone tools and plugins via pyqgis bhatt et al 2014 during the installation process of qgis a folder directory is created in which additional plugins such as freewat can be pasted in this way qgis knows what has to be loaded when starting following the guidelines provided on the project webpage a freewat folder containing all relevant python scripts should be dropped in this directory this directory establishes the connection between qgis freewat and lukars lukars is stored in a single folder containing all necessary python scripts to be added to the mentioned freewat folder most of the lukars code was programmed object oriented with python classes this folder is connected to the plugin by calling it in the overall plugin operating code freewat py then when starting qgis lukars appears in the freewat drop down menu in the qgis toolbar it is planned that the lukars model comes with one of the next official releases of freewat however interested persons can also download the required lukars python scripts from github https github com dbittner87 lukars a detailed description where these utilities need to be added in the qgis freewat folder structure is also provided there 3 using lukars in freewat in order to make the use of lukars user friendly general steps that need to be taken to perform modeling tasks with freewat should be similar for all models integrated in this framework in the following we explain the relevant steps during the pre processing modeling and post processing phases we showcase this application for the kerschbaum spring case study used in bittner et al 2018 the kerschbaum springshed is a small scale pre alpine and dolomite dominated aquifer system close to waidhofen a d ybbs austria fig 1c the recharge area is affected by anthropogenic impacts in form of increasing mining activities in order to investigate the hydrological impacts of this land use change bittner et al 2018 developed the lukars model for more information about this case study area we refer to the work of narany et al 2019 the required data files to perform this simulation can be downloaded from github 3 1 pre processing in order to run lukars time series data with a daily temporal resolution at least precipitation and spring discharge and hydrotope shapefiles vector data are needed similar to the use of modflow in freewat the first step to be done is to go to model setup create model this step is required in order to define the length and time units set the working directory and to define the coordinate reference system crs after finishing this initial step freewat creates different model tables as data objects in which the defined information is stored in the next step we need to load all relevant input time series in the observation analysis tool oat cannata et al 2018 cannata and neumann 2017 the oat library includes two classes the oat sensor and the oat method class we use the oat sensor class to load and store the time series needed for lukars the advantage of using oat at this pre processing step is that the time series can be stored in a sqlite database if we save the qgis project after loading the time series with oat we do not have to load them again once we resume our lukars project in freewat as time series input lukars requires at least a precipitation and a discharge time series with daily temporal resolution optionally it is possible to load a temperature time series that is used to run a temperature based evapotranspiration model thornthwaite 1948 and a snow model degree day method martinec 1960 in order to get a daily evapotranspiration time series from the monthly values obtained with the thornthwaite method the monthly evapotranspiration is divided by the number of days in a month the resulting daily evapotranspiration value is assumed to be representative for the 15th day of a month then a daily evapotranspiration time series is obtained through linear interpolation between each month the selection of these modeling approaches is based on the fact that they provide reliable results for the kerschbaum spring recharge area as shown in the work of bittner et al 2018 it is planned that further different approaches will be integrated in future versions for our case study example we loaded daily time series of spring discharge precipitation and temperature in the next step we can either load or generate hydrotope shapefiles for the kerschbaum case study the hydrotope shapefiles already include the relevant model parameters introduced in table 1 if these shapefiles have to be generated there are different methods to create them e g by koeck and hochbichler 2012 or the german association for water wastewater and waste deutsche vereinigung für wasserwirtschaft abwasser und abfall dwa dwa 2018 in bittner et al 2018 we used a modified hydrotope map of the one created by koeck and hochbichler 2012 the map we use here see fig 3 a was created using a recently proposed hydropedological fieldguide by dwa 2018 the dwa 2018 method has the advantage that it can also be used to derive ranges for some of the lumped parameters required in lukars bittner et al 2020 so far there is no automatic routine to generate the hydrotope shapefiles it has to be noted that for using lukars in freewat each hydrotope needs to be stored in a separate shapefile we decided to store them in different shapefiles in order to simplify their individual manipulation their distinct manual calibration as well as the process of loading the hydrotopes in the lukars freewat interface these shapefiles need to be projected in the same crs as previously defined in the model tables since the crs is needed to automatically calculate the exact area each lukars hydrotope covers in a regarded recharge area 3 2 model application after the hydrotope shapefiles and the required time series have been loaded we can start using the lukars model in freewat the first interface that is loaded is shown in fig 3b since the time series are stored as oat sensors the interface recognizes them and we just need to select the right one for each input then we have to define the total number of hydrotopes which will be loaded in lukars note that in the current version the maximum number of possible hydrotopes is limited to four this is due to the fact that each hydrotope has seven calibration parameters in teixeira parente et al 2019 it was shown that the parameter uncertainties when having four hydrotopes can still be handled but so far we do not know how this changes when more than four hydrotopes need to be calibrated for the kerschbaum spring case study we have four hydrotopes that have to be loaded these are hyd q hyd 1 hyd 2 and hyd 3 the order in which we load the hydrotopes is not crucial for more information about the hydrotopes we refer to bittner et al 2020 in the right part of the interface fig 3b we define the warm up calibration and validation period if the chosen periods overlap an error message occurs to warn the user and the starting dates of each period will adapted automatically for example if the calibration period overlaps with the warm up period the starting date of the calibration period is set to the end date of the warm up period for our case study the following periods are defined as in bittner et al 2018 warm up period from 01 01 2001 to 12 31 2005 calibration period from 01 01 2006 to 31 12 2006 and validation period from 01 01 2007 to 31 12 2007 fig 3 in the lower part of the interface the optional snow evapotranspiration and interception modules can be activated the snow module makes use of the degree day method martinec 1960 and requires setting a melt factor f mm d 1 c 1 as well as a temperature threshold t f c for melt for each hydrotope the interception module requires setting a maximum interception value i max mm that depends on the respective land cover of the hydrotope for a given event the effective rainfall is calculated by subtracting i max from the measured precipitation value bittner et al 2018 the evapotranspiration module does not require the definition of any further parameters after everything was defined it is possible to advance to the define parameters interface after proving the validity of all input check if input is valid the function to check the validity of the input helps the users recognizing errors before starting the model calibration the define parameters interface is shown in fig 3c three different ways for setting the hydrotope parameters are integrated in lukars if no parameter set exists for a recharge area to be modeled the first choice would be to manually define the parameters another option is to load a parameter set from a csv file a template how this file should be constructed is provided on github the third option is to load the parameters from the hydrotope shapefiles for the kerschbaum spring example we provide the parameters in table 1 once all parameters were defined the run model button initiates a lukars simulation with the given parameter set and a new window opens that shows the simulated and the observed hydrograph of a regarded karst spring 3 3 post processing the newly opened window allows to visually compare the simulated and the observed time series fig 3d moreover the window is split into two frames one for the calibration and one for the validation period for both periods two objective evaluation criteria are calculated namely the mean absolute error mae and the nash sutcliffe efficency nash and sutcliffe 1970 in case those criteria do not show an acceptable goodness of fit for the calibration period we can go back to the define parameters interface and change the parameters during this trial and error calibration the modeler should only focus on the goodness of fit in the calibration window of course in order to better focus on parts of the simulation period we can zoom into a period of interest e g a high flow event to see which properties of the measured time series are matched by our simulated time series and which are not this calibration procedure can be continued until an acceptable parameter set was found once such a parameter set is identified we can save the graph as an image file which can then be shared with stakeholders or included in a report also the simulated time series can be exported as a txt or csv file using the save simulation results function in the define parameters interface finally we can save the hydrotope parameters to the attribute table of the respective hydrotope shapefiles by applying the write parameters to shp file s function in the define parameters interface this allows to easily share the relevant model files with stakeholders and to apply the model on other computers without the need to define the parameters from scratch once having written the parameters to the shapefiles they can directly be loaded to the define parameters interface by clicking load parameters from shapefile s it is important to note that no automatic calibration procedure is coupled to lukars in freewat so far but will be included in future releases additional post processing tools are provided in the freewat environment in particular in oat the oat method integrates several statistical analysis tools that can be used to compare the simulated and observed time series for more details about these tools the interested reader is referred to rossetto et al 2018 3 4 land use change impact modeling once a reliable parameter set was identified we can use this parameter set to simulate the hydrological impacts of a land use change on karst spring discharge to do so modified hydrotope shapefiles are needed that include the respective land use change compared to the original set of hydrotopes several ways exist how land use change scenarios and modified hydrotope shapefiles can be generated e g participatory approaches mehdi et al 2018 or repetitive field mapping campaigns dwa 2018 another possibility which was chosen by bittner et al 2018 is to derive land use changes from change detection analysis with orthophotos in order to copy the parameter set found during calibration to the modified hydrotope shapefiles we can use the copy from vector layer tool in freewat this function copies the attribute tables from the original hydrotopes to the new ones by that we can avoid to manually enter the parameters in lukars again using the modified hydrotopes a lukars run can be performed and the effects of the respective land use change on spring discharge can be evaluated as an example we provide such a set of hydrotope shapefiles including the land use change modeled in bittner et al 2018 on github 4 discussion given that the freewat project ended in 2017 the maintenance of the initiative now depends on joint efforts by the developers group and those who like to contribute to it in our study we highlight that previous research studies and related outcomes can act as a platform for further developments and enhancements the lukars integration in freewat provides a gui for a lumped karst aquifer model making its application more intuitive for stakeholders dealing with water management issues in karst systems possible stakeholders who may use lukars in the future are researchers water resources managers public authorities engineers and non governmental organizations they can be reached by the broad number of persons who already apply freewat and are informed about new releases by social media and stakeholders workshops lukars represents a complementary gui for karst hydrologic modeling since the way how it conceptualizes karst aquifers is different as compared to the karstmod platform introduced by mazzilli et al 2019 the integration approach was chosen following the state of the art of software development i e object oriented thus the python model classes and the implemented functions can easily be accessed applied and adapted by interested persons who are familiar with programming nevertheless some improvements are envisaged for future releases in order to enhance the applicability of lukars in freewat so far the generation of hydrotope shapefiles is based on field mapping campaigns using different mapping approaches however a map based generation of hydrotope shapefiles should be made possible if detailed information about soil physical properties and land use is available in form of raster and or vector data we plan to create an automated procedure that can process available soil and land use data in order to generate hydrotope shapefiles within a gis environment a limitation of the current lukars version in freewat is that it requires manual calibration however lumped karst aquifer models are typically calibrated with automatic calibration routines hartmann et al 2017 teixeira parente et al 2019 it is foreseen to implement an automatic calibration framework in the modular structure of freewat such that it will be usable also for other models available in the platform for example we believe that implementing the safe toolbox by pianosi et al 2015 into the freewat framework would be very welcome by the user community finally the current version of lukars allows for the definition of up to four hydrotopes larger scale studies may require a larger number of hydrotopes however this would lead to a rapid increase of fitting parameters and the risk of overfitting the opportunity of increasing the number of hydrotopes should be tested for different case studies e g using the world karst spring hydrograph wokas database presented by olarinoye et al 2020 and made available along with an automatic sensitivity and parametric uncertainty toolbox 5 conclusions in this paper we introduce a user friendly environment for modeling the impacts of land use changes on the spring discharge of a karst system we integrated the lukars model into the freewat environment which is a water resources management toolkit implemented as a plugin in qgis the advantages of the lukars integration following the tight coupled freewat approach are that 1 qgis serves as a gui and provides a useful platform to map store and change the dominant hydrotopes in a recharge area of interest 2 freewat s modular framework for hydrological modeling i e pre processing simulation codes and post processing represents a tailored framework to bring in new model codes that make the freewat environment more comprehensive 3 stakeholders can use the measured and simulated time series obtained with the lukars model and apply all analysis tools present in freewat we want to emphasize that freewat provides a valuable environment with open source modeling and analysis tools that can easily be applied by stakeholders the continuous discussions in this group of researchers and practitioners guarantees that the framework improves while remaining applicable moreover this close cooperation also makes sure that only tools will be integrated which are considered beneficial for freewat and its end users declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research is a result of the unmix project uncertainties due to boundary conditions in predicting mixing in groundwater which is supported by deutsche forschungsgemeinschaft dfg through tum international graduate school for science and engineering igsse gsc 81 the authors refer to the interreg central europe project proline ce and to the interreg central europe project boderec ce both funded by erdf d b acknowledges financial support by the feldstipendium of the deutsche hydrologische gesellschaft dhg g c also acknowledges the support of the stiftungsfonds für umweltökonomie und nachhaltigkeit gmbh sun the water works in waidhofen a d ybbs kindly provided the orthophoto as well as the discharge precipitation and temperature data recorded in the kerschbaum spring recharge area 
26038,simulating karst spring discharge and land use change impacts in a recharge area is strategic for water resource management in many countries worldwide we introduce a user friendly modeling environment by integrating the recently proposed lukars land use change modeling in karst systems model into the freewat free and open source software tools for water resource management framework lukars is a rainfall discharge model for karst systems that simulates the impact of land use changes by changing the area of dominant hydrotopes i e landscape units with homogeneous soil and land use properties freewat is a free and open source toolkit for water resource management implemented in qgis desktop application the integration of lukars into freewat takes advantage of the gis capabilities to map visualize and change the relevant hydrotope shapefiles freewat provides a modular framework for pre and post processing tools that facilitate the setup calibration analysis storage and sharing of lukars models 1 introduction mathematical models are tools which are commonly applied to simulate different processes of the hydrologic cycle casper et al 2019 chiogna et al 2018 hartmann et al 2014a tuo et al 2018 typically these models are based on sophisticated mathematical frameworks developed and applied by researchers and water experts in recent years more and more open source codes are available to model various processes of the hydrologic cycle arnold et al 1998 borsi et al 2013 winston 2009 their complex nature makes them not intuitive for what relates to their applicability in particular those people who need to apply these tools e g water resources managers are not necessarily familiar with programming languages and are thus often not able to apply these models de filippis et al 2017 in the years 2015 2017 the european union funded a coordinated initiative to overcome this limitation within the horizon 2020 program the freewat project www freewat eu in freewat researchers from various european countries developed a free and open source plugin including tools for water resource management such as a hydrologic model and time series analysis methods in the qgis environment rossetto et al 2018 the aim of this plugin is to use qgis as a convenient and intuitive graphical user interface gui that enables practitioners to deal with sophisticated software generics rossetto et al 2019 although freewat still can be considered as new its broad applicability was already highlighted in multiple case studies such as the management of coastal aquifers modeling of groundwater surface water interactions and the assessment of climate change impacts on groundwater resources de filippis et al 2020 freewat comprises three main modules in which different tools are provided the pre processing module the simulation codes module and the post processing module so far the simulation codes module contains various packages of the modflow 2005 family harbaugh 2005 and modflow owhm hanson et al 2014 all of them being coded for physically based flow and transport modeling primarily in groundwater systems the modflow family of codes is only applicable for groundwater dominated systems but not e g for rainfall runoff modeling in the particular case of karst hydrology rainfall runoff models are commonly applied for spring discharge predictions mazzilli et al 2012 ollivier et al 2020 and to enhance the understanding of hydrological processes in karst aquifers duran et al 2020 sivelle et al 2019 karst aquifers are widely used as drinking water supply both in europe and worldwide and hence require appropriate model based management chen et al 2017a stevanović 2019 although promising physically based model codes for the distributed simulation of karst aquifer processes exist chen et al 2017b giese et al 2018 henson et al 2018 reimann et al 2011 reimann and hill 2009 and initiatives started to improve their applicability berthelin et al 2020 berthelin and hartmann 2020 a reasonable implementation of these models still is often restricted by data limitations jukić and denić jukić 2009 ladouche et al 2014 various types of rainfall runoff karst models exist with different conceptual approaches one common approach is to consider different combinations of the dominant flow compartments i e conduits and matrix as distinct buckets chang et al 2017 fleury et al 2007 jourde et al 2015 for this conceptual idea of representing karst systems mazzilli et al 2019 proposed an open source gui that is intuitively applicable also by practioners another way to conceptualize the behavior of karst systems in lumped models is to consider the complexity of recharge processes and the infiltration to slow and quickflow paths bittner et al 2018 hartmann et al 2012 ollivier et al 2020 given that no gui for this type of conceptual approach exists the implementation of such karst aquifer model in freewat makes this framework more comprehensive and applicable for a broader range of water resource management issues we want to contribute to the freewat modeling framework by implementing the recently proposed lukars model bittner et al 2018 2020 teixeira parente et al 2019 into the qgis integrated freewat environment lukars is a rainfall runoff model that can be used to predict spring discharge in karst system and was developed to perform land use change impact studies in karstic environments this bucket type model is based on the integration of dominant hydrotopes in a recharge area which are distinct spatial entities defined by homogeneous soil and land use properties arnold et al 1998 each hydrotope shows a specific hydrological response to recharge events depending on its specific properties since spatially distributed properties need to be defined for each hydrotope when setting up a lukars model integrating lukars in a gis environment can make the visualization application and evaluation of the model more intuitive moreover lukars is one of the few karst hydrologic models that takes into consideration the possibility of modeling land use change impacts on karst spring discharge this feature offers a possible solution to a timely challenge of the karst modeler community hartmann et al 2014b therefore we consider the gis integration of lukars into freewat as a valuable contribution to provide applicable and open source tools for water resources management in section 2 we describe the concept of lukars and how the technical integration of the model into the qgis freewat environment was performed in section 3 we show how lukars fits into the existing modular structure of freewat for a better reproducibility we showcase the lukars tool in freewat with a case study application finally we discuss and conclude our efforts in section 4 and 5 2 materials and methods in this section we provide a short description about the conceptual idea of lukars fig 1 a and b the related assumptions and the flow components that are simulated by the model moreover we provide a technical summary how the integration of lukars into freewat was achieved 2 1 concept of lukars the concept of lukars assumes that the dominant flow components in a karst system i e the quickflow through conduits and fractures and a slow flow through the matrix are controlled by the physical properties of the shallow subsurface i e the soil epikarst system if the soil properties of a hydrotope are characterized by shallow soils with a coarse grained texture e g hyd 2 in fig 1a the quickflow paths are better connected and the hydrotope shows a fast and intense response to a given input signal e g precipitation in a hydrotope dominated by deep and more fine textured soils e g hyd 4 in fig 1a the quickflow is less intense and matrix infiltration is more constant in lukars each hydrotope is represented by a distinct bucket that has three different flow components fig 1b i e the quickflow qhyd the infiltration into the rock matrix qis feeding the baseflow storage b and a secondary spring discharge qsec qhyd is activated once a hydrotope specific storage value emax was exceeded and stops after the hydrotope storage reaches a lower threshold emin by that a hysteretic behavior of the soil epikarst system is simulated qis and qsec are both implemented based on linear transfer functions in contrast to qis qsec is only active if the hydrotope storage is higher than a defined threshold for secondary spring discharge esec for more information about the mathematical details the interested reader is referred to the works of bittner et al 2018 teixeira parente et al 2019 and bittner et al 2020 where lukars was applied for the case study of waidhofen an der ybbs fig 1c 2 2 lukars integration in freewat the goal of the lukars integration was to take advantage of the modular framework of freewat and using the provided pre and post processing tools this concept is shown in fig 2 which highlights the use of the gis to create and visualize the hydrotope shapefiles as well as the freewat input data management in the pre processing step in the post processing the integration of lukars benefits from the existing time series analysis tools in freewat and the gis functionalities to store the hydrotope parameters in shapefiles the integration of all freewat tools into the qgis environment is performed with the python programming language although most of qgis is written in c it allows for connecting python scripts as standalone tools and plugins via pyqgis bhatt et al 2014 during the installation process of qgis a folder directory is created in which additional plugins such as freewat can be pasted in this way qgis knows what has to be loaded when starting following the guidelines provided on the project webpage a freewat folder containing all relevant python scripts should be dropped in this directory this directory establishes the connection between qgis freewat and lukars lukars is stored in a single folder containing all necessary python scripts to be added to the mentioned freewat folder most of the lukars code was programmed object oriented with python classes this folder is connected to the plugin by calling it in the overall plugin operating code freewat py then when starting qgis lukars appears in the freewat drop down menu in the qgis toolbar it is planned that the lukars model comes with one of the next official releases of freewat however interested persons can also download the required lukars python scripts from github https github com dbittner87 lukars a detailed description where these utilities need to be added in the qgis freewat folder structure is also provided there 3 using lukars in freewat in order to make the use of lukars user friendly general steps that need to be taken to perform modeling tasks with freewat should be similar for all models integrated in this framework in the following we explain the relevant steps during the pre processing modeling and post processing phases we showcase this application for the kerschbaum spring case study used in bittner et al 2018 the kerschbaum springshed is a small scale pre alpine and dolomite dominated aquifer system close to waidhofen a d ybbs austria fig 1c the recharge area is affected by anthropogenic impacts in form of increasing mining activities in order to investigate the hydrological impacts of this land use change bittner et al 2018 developed the lukars model for more information about this case study area we refer to the work of narany et al 2019 the required data files to perform this simulation can be downloaded from github 3 1 pre processing in order to run lukars time series data with a daily temporal resolution at least precipitation and spring discharge and hydrotope shapefiles vector data are needed similar to the use of modflow in freewat the first step to be done is to go to model setup create model this step is required in order to define the length and time units set the working directory and to define the coordinate reference system crs after finishing this initial step freewat creates different model tables as data objects in which the defined information is stored in the next step we need to load all relevant input time series in the observation analysis tool oat cannata et al 2018 cannata and neumann 2017 the oat library includes two classes the oat sensor and the oat method class we use the oat sensor class to load and store the time series needed for lukars the advantage of using oat at this pre processing step is that the time series can be stored in a sqlite database if we save the qgis project after loading the time series with oat we do not have to load them again once we resume our lukars project in freewat as time series input lukars requires at least a precipitation and a discharge time series with daily temporal resolution optionally it is possible to load a temperature time series that is used to run a temperature based evapotranspiration model thornthwaite 1948 and a snow model degree day method martinec 1960 in order to get a daily evapotranspiration time series from the monthly values obtained with the thornthwaite method the monthly evapotranspiration is divided by the number of days in a month the resulting daily evapotranspiration value is assumed to be representative for the 15th day of a month then a daily evapotranspiration time series is obtained through linear interpolation between each month the selection of these modeling approaches is based on the fact that they provide reliable results for the kerschbaum spring recharge area as shown in the work of bittner et al 2018 it is planned that further different approaches will be integrated in future versions for our case study example we loaded daily time series of spring discharge precipitation and temperature in the next step we can either load or generate hydrotope shapefiles for the kerschbaum case study the hydrotope shapefiles already include the relevant model parameters introduced in table 1 if these shapefiles have to be generated there are different methods to create them e g by koeck and hochbichler 2012 or the german association for water wastewater and waste deutsche vereinigung für wasserwirtschaft abwasser und abfall dwa dwa 2018 in bittner et al 2018 we used a modified hydrotope map of the one created by koeck and hochbichler 2012 the map we use here see fig 3 a was created using a recently proposed hydropedological fieldguide by dwa 2018 the dwa 2018 method has the advantage that it can also be used to derive ranges for some of the lumped parameters required in lukars bittner et al 2020 so far there is no automatic routine to generate the hydrotope shapefiles it has to be noted that for using lukars in freewat each hydrotope needs to be stored in a separate shapefile we decided to store them in different shapefiles in order to simplify their individual manipulation their distinct manual calibration as well as the process of loading the hydrotopes in the lukars freewat interface these shapefiles need to be projected in the same crs as previously defined in the model tables since the crs is needed to automatically calculate the exact area each lukars hydrotope covers in a regarded recharge area 3 2 model application after the hydrotope shapefiles and the required time series have been loaded we can start using the lukars model in freewat the first interface that is loaded is shown in fig 3b since the time series are stored as oat sensors the interface recognizes them and we just need to select the right one for each input then we have to define the total number of hydrotopes which will be loaded in lukars note that in the current version the maximum number of possible hydrotopes is limited to four this is due to the fact that each hydrotope has seven calibration parameters in teixeira parente et al 2019 it was shown that the parameter uncertainties when having four hydrotopes can still be handled but so far we do not know how this changes when more than four hydrotopes need to be calibrated for the kerschbaum spring case study we have four hydrotopes that have to be loaded these are hyd q hyd 1 hyd 2 and hyd 3 the order in which we load the hydrotopes is not crucial for more information about the hydrotopes we refer to bittner et al 2020 in the right part of the interface fig 3b we define the warm up calibration and validation period if the chosen periods overlap an error message occurs to warn the user and the starting dates of each period will adapted automatically for example if the calibration period overlaps with the warm up period the starting date of the calibration period is set to the end date of the warm up period for our case study the following periods are defined as in bittner et al 2018 warm up period from 01 01 2001 to 12 31 2005 calibration period from 01 01 2006 to 31 12 2006 and validation period from 01 01 2007 to 31 12 2007 fig 3 in the lower part of the interface the optional snow evapotranspiration and interception modules can be activated the snow module makes use of the degree day method martinec 1960 and requires setting a melt factor f mm d 1 c 1 as well as a temperature threshold t f c for melt for each hydrotope the interception module requires setting a maximum interception value i max mm that depends on the respective land cover of the hydrotope for a given event the effective rainfall is calculated by subtracting i max from the measured precipitation value bittner et al 2018 the evapotranspiration module does not require the definition of any further parameters after everything was defined it is possible to advance to the define parameters interface after proving the validity of all input check if input is valid the function to check the validity of the input helps the users recognizing errors before starting the model calibration the define parameters interface is shown in fig 3c three different ways for setting the hydrotope parameters are integrated in lukars if no parameter set exists for a recharge area to be modeled the first choice would be to manually define the parameters another option is to load a parameter set from a csv file a template how this file should be constructed is provided on github the third option is to load the parameters from the hydrotope shapefiles for the kerschbaum spring example we provide the parameters in table 1 once all parameters were defined the run model button initiates a lukars simulation with the given parameter set and a new window opens that shows the simulated and the observed hydrograph of a regarded karst spring 3 3 post processing the newly opened window allows to visually compare the simulated and the observed time series fig 3d moreover the window is split into two frames one for the calibration and one for the validation period for both periods two objective evaluation criteria are calculated namely the mean absolute error mae and the nash sutcliffe efficency nash and sutcliffe 1970 in case those criteria do not show an acceptable goodness of fit for the calibration period we can go back to the define parameters interface and change the parameters during this trial and error calibration the modeler should only focus on the goodness of fit in the calibration window of course in order to better focus on parts of the simulation period we can zoom into a period of interest e g a high flow event to see which properties of the measured time series are matched by our simulated time series and which are not this calibration procedure can be continued until an acceptable parameter set was found once such a parameter set is identified we can save the graph as an image file which can then be shared with stakeholders or included in a report also the simulated time series can be exported as a txt or csv file using the save simulation results function in the define parameters interface finally we can save the hydrotope parameters to the attribute table of the respective hydrotope shapefiles by applying the write parameters to shp file s function in the define parameters interface this allows to easily share the relevant model files with stakeholders and to apply the model on other computers without the need to define the parameters from scratch once having written the parameters to the shapefiles they can directly be loaded to the define parameters interface by clicking load parameters from shapefile s it is important to note that no automatic calibration procedure is coupled to lukars in freewat so far but will be included in future releases additional post processing tools are provided in the freewat environment in particular in oat the oat method integrates several statistical analysis tools that can be used to compare the simulated and observed time series for more details about these tools the interested reader is referred to rossetto et al 2018 3 4 land use change impact modeling once a reliable parameter set was identified we can use this parameter set to simulate the hydrological impacts of a land use change on karst spring discharge to do so modified hydrotope shapefiles are needed that include the respective land use change compared to the original set of hydrotopes several ways exist how land use change scenarios and modified hydrotope shapefiles can be generated e g participatory approaches mehdi et al 2018 or repetitive field mapping campaigns dwa 2018 another possibility which was chosen by bittner et al 2018 is to derive land use changes from change detection analysis with orthophotos in order to copy the parameter set found during calibration to the modified hydrotope shapefiles we can use the copy from vector layer tool in freewat this function copies the attribute tables from the original hydrotopes to the new ones by that we can avoid to manually enter the parameters in lukars again using the modified hydrotopes a lukars run can be performed and the effects of the respective land use change on spring discharge can be evaluated as an example we provide such a set of hydrotope shapefiles including the land use change modeled in bittner et al 2018 on github 4 discussion given that the freewat project ended in 2017 the maintenance of the initiative now depends on joint efforts by the developers group and those who like to contribute to it in our study we highlight that previous research studies and related outcomes can act as a platform for further developments and enhancements the lukars integration in freewat provides a gui for a lumped karst aquifer model making its application more intuitive for stakeholders dealing with water management issues in karst systems possible stakeholders who may use lukars in the future are researchers water resources managers public authorities engineers and non governmental organizations they can be reached by the broad number of persons who already apply freewat and are informed about new releases by social media and stakeholders workshops lukars represents a complementary gui for karst hydrologic modeling since the way how it conceptualizes karst aquifers is different as compared to the karstmod platform introduced by mazzilli et al 2019 the integration approach was chosen following the state of the art of software development i e object oriented thus the python model classes and the implemented functions can easily be accessed applied and adapted by interested persons who are familiar with programming nevertheless some improvements are envisaged for future releases in order to enhance the applicability of lukars in freewat so far the generation of hydrotope shapefiles is based on field mapping campaigns using different mapping approaches however a map based generation of hydrotope shapefiles should be made possible if detailed information about soil physical properties and land use is available in form of raster and or vector data we plan to create an automated procedure that can process available soil and land use data in order to generate hydrotope shapefiles within a gis environment a limitation of the current lukars version in freewat is that it requires manual calibration however lumped karst aquifer models are typically calibrated with automatic calibration routines hartmann et al 2017 teixeira parente et al 2019 it is foreseen to implement an automatic calibration framework in the modular structure of freewat such that it will be usable also for other models available in the platform for example we believe that implementing the safe toolbox by pianosi et al 2015 into the freewat framework would be very welcome by the user community finally the current version of lukars allows for the definition of up to four hydrotopes larger scale studies may require a larger number of hydrotopes however this would lead to a rapid increase of fitting parameters and the risk of overfitting the opportunity of increasing the number of hydrotopes should be tested for different case studies e g using the world karst spring hydrograph wokas database presented by olarinoye et al 2020 and made available along with an automatic sensitivity and parametric uncertainty toolbox 5 conclusions in this paper we introduce a user friendly environment for modeling the impacts of land use changes on the spring discharge of a karst system we integrated the lukars model into the freewat environment which is a water resources management toolkit implemented as a plugin in qgis the advantages of the lukars integration following the tight coupled freewat approach are that 1 qgis serves as a gui and provides a useful platform to map store and change the dominant hydrotopes in a recharge area of interest 2 freewat s modular framework for hydrological modeling i e pre processing simulation codes and post processing represents a tailored framework to bring in new model codes that make the freewat environment more comprehensive 3 stakeholders can use the measured and simulated time series obtained with the lukars model and apply all analysis tools present in freewat we want to emphasize that freewat provides a valuable environment with open source modeling and analysis tools that can easily be applied by stakeholders the continuous discussions in this group of researchers and practitioners guarantees that the framework improves while remaining applicable moreover this close cooperation also makes sure that only tools will be integrated which are considered beneficial for freewat and its end users declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research is a result of the unmix project uncertainties due to boundary conditions in predicting mixing in groundwater which is supported by deutsche forschungsgemeinschaft dfg through tum international graduate school for science and engineering igsse gsc 81 the authors refer to the interreg central europe project proline ce and to the interreg central europe project boderec ce both funded by erdf d b acknowledges financial support by the feldstipendium of the deutsche hydrologische gesellschaft dhg g c also acknowledges the support of the stiftungsfonds für umweltökonomie und nachhaltigkeit gmbh sun the water works in waidhofen a d ybbs kindly provided the orthophoto as well as the discharge precipitation and temperature data recorded in the kerschbaum spring recharge area 
26039,deep uncertainty in future climate socio economic and technological conditions poses a great challenge to medium long term decision making recently several approaches have been proposed to identify solutions that are robust with respect to a large ensemble of deeply uncertain future scenarios in this paper we introduce ross robust optimal scenario selection a novel algorithm that uses an active learning approach for adaptively selecting the smallest scenario subset to be included into a robust optimization process ross contributes a twofold novelty in the field of robust optimization under deep uncertainty first it allows the computational requirements for the generation of robust solutions to be considerably reduced with respect to traditional optimization methods second it allows the identification of the most informative regions of the scenario set containing the scenarios to be included in the optimization process for generating a robust solution we test ross on the real case study of robust planning of an off grid hybrid energy system combining diesel generation with renewable energy sources and storage technologies results show that ross enables computational requirements to be reduced between 23 to 84 compared with traditional robust optimization methods depending on the complexity of the robustness metrics considered it is also able to identify very small regions of the scenario set containing the most informative scenarios for generating a robust solution keywords robust optimization deep uncertainty active learning robust planning hybrid energy systems 1 introduction changes in future climatic and socio economic conditions as well as rapid technological innovation represent sources of uncertainty that significantly influence medium long term decision making in different fields e g water management infrastructure planning energy systems design harrison et al 2015 maier et al 2016 identifying planning solutions or management strategies for complex environmental systems can thus be extremely challenging as the consequences of a decision in terms of system performance strongly depend on the external uncertain conditions that will actually unfold in the future uusitalo et al 2015 mcphail et al 2018 when information on the probability of occurrence of future uncertain conditions is available and the uncertainty can be described using a stochastic model the problem is defined as decision making under risk french 1986 when instead such probability of occurrence is unknown and a set membership description of the uncertain future conditions is the only available information the problem is classified as decision making under deep uncertainty lempert and schlesinger 2000 lempert 2002 dessai et al 2009 several approaches have been proposed to support decision making under deep uncertainty see herman et al 2015 and maier et al 2016 for a review the most widely adopted approaches include robust decision making rdm lempert 2002 lempert et al 2003 bryant and lempert 2010 hall et al 2012 decision scaling brown et al 2012 moody and brown 2013 ghile et al 2014 poff et al 2015 and many objective robust decision making mordm kasprzyk et al 2013 herman et al 2014 paton et al 2014 all these approaches aim at evaluating the robustness of different planning or management alternatives over an ensemble of plausible future scenarios focusing on understanding and retrieving the future conditions for which a given alternative fails or not these conditions which highlight the main system vulnerabilities are then considered for re designing improved solutions within an iterative stress testing process converging to the identification of the most robust strategy lempert et al 2006 groves and lempert 2007 following the taxonomy proposed by herman et al 2015 these approaches can be classified based on the methods adopted for i identifying alternatives e g pre specified e g tingstad et al 2014 or optimally generated e g quinn et al 2018 ii sampling scenarios e g top down e g mahmoud et al 2009 or bottom up e g nazemi and wheater 2014 methods iii quantifying robustness measures e g regret or satisficing measures e g lempert and collins 2007 and iv identifying key uncertainties through sensitivity analysis e g scenario discovery e g bryant and lempert 2010 singh et al 2014 irrespective of which of the above approaches is used the following sequential steps have to be performed i generation of alternative solutions or policies ii sampling of plausible future scenarios and iii robustness analysis since the above robustness analysis is conducted on pre defined solutions there is no guarantee the most robust solutions are identified to partially overcome this problem recent studies focus on scenario selection methods for identifying vulnerable or problematic scenarios to be included within the alternative generation phase in order to enhance the robustness of the candidate solutions watson and kasprzyk 2017 eker and kwakkel 2018 in particular alternatives are first generated by preforming multiple optimization runs considering the identified scenarios independently and then re evaluated in a robustness assessment over a large ensemble of scenarios however in this case the design of most robust solutions is strictly dependent on the scenarios selected and used to optimize the alternatives robust optimization ro methods overcome these issues by identifying solutions that maximize robustness over a range of plausible future scenarios this is achieved by combining robustness calculations and formal optimization processes identifying solutions that perform satisfactorily over a wide range of deeply uncertain scenarios however this requires the performance of each alternative to be simulated at each of these scenarios making the optimization process computationally demanding or even intractable especially when the number of scenarios increases from tens to several thousand roach et al 2016 this is the reason why most studies that combine robustness assessment and the use of formal optimization for alternative generation perform the robustness assessment after the completion of the optimization process kasprzyk et al 2013 paton et al 2014 beh et al 2015 studies incorporating robustness as an explicit objective in the optimization problem usually consider a limited number of scenarios hamarat et al 2014 basupi and kapelan 2015 kwakkel et al 2015 giuliani and castelletti 2016 kwakkel et al 2016b zeff et al 2016 shavazipour and stewart 2019 however some of these works explicitly highlight as a key research challenge the necessity of handling a large number of scenarios by reducing computational requirements through scenario reduction algorithms or novel solution methods kwakkel et al 2016b shavazipour and stewart 2019 to overcome the computational issues of ro methods some studies use computationally efficient metamodels castelletti et al 2012b as surrogates of the hi fidelity computationally expensive simulation models that are generally used in the calculation of robustness values within the optimization process yan and minsker 2011 broad et al 2015 beh et al 2017 these metamodels are usually black box models that map future uncertain conditions and decision variables onto robustness values while their use can increase the computational efficiency of the optimization process significantly their black box nature means that the underlying system dynamics are not able to be represented explicitly castelletti et al 2011 2012a making it difficult to obtain insight into the plausible future conditions that are likely to result in system failure an alternative approach to increasing the computational efficiency of robust optimization methods is to adopt sampling techniques to extract a smaller scenario subset to be included in the robustness calculation this subset can be pre specified trindade et al 2017 gold et al 2019 or randomly generated at each iteration of the optimization process kasprzyk et al 2012 giuliani et al 2014 while consideration of a smaller number of scenarios reduces computational requirements it also introduces the possibility that the robustness of the solution identified in the optimization process is significantly different from the one resulting from the consideration of the entire scenario set in order to overcome the shortcomings of previous studies incorporating robustness as an optimization objective outlined above we introduce ross robust optimal scenario selection a novel algorithm that uses an active learning approach cohn et al 1996 to identify the smallest scenario subset for which the robustness of the optimized solution is similar or even equal to that obtained by considering the entire scenario set thereby increasing computational efficiency by reducing the number of scenarios over which solution performance has to be evaluated active learning is a sub field of machine learning as part of which the learning algorithm is directly responsible for acquiring its training data set through i experiments on the real systems or ii running simulations of generative models to obtain the desired outputs for new unknown inputs in our case the active learning principles are applied for filtering an existing data set i e scenario set in order to select a concise sub set of sufficiently informative scenarios for generating robust solutions ross contributes a twofold novelty in the field of robust optimization under deep uncertainty first it allows to considerably reduce the computational requirements for the generation of robust solutions with respect to traditional optimization methods and state of the art scenario selection techniques second it iteratively explores the entire scenario set within the robust optimization process in order to identify its most informative regions for designing a robust solution similar to scenario discovery in some cases these regions also highlight the main system vulnerabilities i e scenarios for which the system performs poorly that should be then considered for designing robust solutions in order to demonstrate the utility of ross we apply it to the robust planning of a hybrid energy system for the ustica island italy this case study is ideally suited to demonstrate the capabilities of ross as it is focused on the optimal design of a hybrid energy system including different types of renewable energy sources res which is subject to deep uncertainties associated with future changes in a number of climatic variables e g solar radiation wind speed temperature in addition due to the very high number of future scenarios the consideration of robustness as an optimization objective is computationally demanding we first assess the ability of ross of iteratively identifying the regions of the scenario set that contain the most informative scenarios for generating a robust solution as highlighted by different studies giuliani and castelletti 2016 kwakkel et al 2016a mcphail et al 2018 defining the robustness of a solution constitutes itself a meta problem i e deciding how to decide schneller and sphicas 1983 as it strictly depends on the attitude of the decision makers in facing and reacting to future uncertain conditions to address this aspect we repeat our analysis for multiple robustness metrics and explore how the most informative regions of the scenario set and the resulting robust solution change according to the robustness metrics selected highlighting the relative influence of different uncertain drivers on the robustness of the solution finally we evaluate the computational advantages of using ross by comparing it with a traditional robust optimization method i e brute force method which considers the entire scenario set within the optimization process and a state of the art scenario selection method i e random sampling method trindade et al 2017 that randomly extracts a small scenario subset to be included into the optimization process the remainder of this paper is organized as follows section 2 presents the adopted methods and the ross algorithm section 3 introduces the case study providing a description of the model adopted to simulate the electricity system and defining the experimental settings section 4 shows the numerical results and section 5 provides conclusions and suggestions for further research 2 methods and tools ross robust optimal scenario selection is an active learning algorithm designed to identify the smallest scenario subset for generating robust solutions fig 1 the main inputs to the algorithm are a set of future plausible scenarios f a robustness metric ψ readers should refer to mcphail et al 2018 for an overview of the most adopted robustness metrics used to calculate the performance of an alternative over the scenarios in f and the cardinality n 0 of the scenario subset to be included in the optimization process as a rule of thumb n 0 is defined according to the robustness metric selected as the minimum number of scenarios needed to mathematically compute the robustness metric see section 3 2 for further details however for complex dynamic systems affected by a large set of co varying uncertain factors this number could be too small for identifying a robust solution in this case n 0 has to be increased iteratively and the algorithm relaunched until a robust solution is found as outputs ross delivers the robust planning solution and the regions of the scenario set containing the most informative scenarios i e the scenarios that have the largest influence on robustness values 2 1 robust optimal scenario selection ross adopts an active learning al algorithm that iteratively moves a gaussian distribution function defined on the scenario set f towards the regions containing the most informative scenarios these are the scenarios for which the robustness of the optimized solution is similar ideally equal to that obtained when robustness is calculated over the entire scenario set following rachelson et al 2011 we choose gaussian distribution functions because they i are simple to update and ii allow to assign a different probability of extraction to each scenario this allows the algorithm to shrink the gaussian towards the most informative regions of the scenario set assigning a higher probability of extraction to the most informative scenarios in order to be rather confident that such scenarios are included within the optimization process most of the times the algorithm algorithm 1 starts by initializing a n k variate gaussian distribution d where n n 0 is the cardinality of the scenario subset and k is the dimension of the scenario set f then n points are sampled according to the distribution d which correspond to a single n k dimensional sample from d to ensure these sampled points align with members of the scenario set we normalize each uncertain variables between 0 and 1 in order to equally weighting the different uncertain drivers and we perform a nearest neighbour search in the scenario space with respect to the euclidean distance between the candidate points and the scenarios in f the n selected scenarios constitute the scenario subset f ˆ n for which a robust solution a is identified via a formal robust optimization process using the selected robustness metric ψ as an objective the degree of similarity between the robustness values calculated over the selected scenario subset and over the entire scenario set is then calculated using the following metric 1 r r o p t a f ˆ n r r e v a l a f r r e v a l a f 100 where r o p t a f ˆ n is the robustness of the optimal solution a calculated over the scenario subset f ˆ n and r r e v a l a f is the robustness of the optimal solution a calculated over the entire scenario set f the score r to be maximized quantifies the absolute difference in percentage between the robustness of the optimized solution calculated over the scenario subset extracted from the gaussian distribution d and the one computed over the entire scenario set f the lower this difference the higher the score when the score r equals zero the two robustness calculations match and the solution optimized over the scenario subset is also robust with respect to the entire scenario set it is worth noting that considering the robustness value computed over the entire scenario set f as a target to be achieved is needed here for testing the algorithm and verifying that it works correctly however in general this target could be externally imposed without necessarily being generated by optimizing over the entire scenario set in order to assess the ability of the gaussian distribution d to identify the most informative scenarios the above mentioned steps i e scenario sampling nearest neighbour search robust optimization score computation are repeated for n e x p times we thus obtain a set φ f ˆ n r of n e x p elements each composed of n scenarios and the corresponding r score to identify the scenarios that allow the highest score to be obtained φ is ordered descendingly with respect to the evaluation score r and its first p n e x p elements are selected and included in φ then the scenarios composing φ are used to update the distribution d this corresponds to computing the n k variate average vector and the n k n k co variance matrix for the scenarios in φ the al procedure iterates until a maximum number of iterations m is reached or when the 0 1 quantile of the n e x p scores reaches zero in the latter case we can claim that ross is at convergence as 90 of the n e x p extractions from d identifies scenarios that allow the maximum r score value to be obtained namely zero the purpose of the al algorithm thus consists of evolving the parameters i e mean variance of a n k variate gaussian distribution d so as to maximize its capability of extracting scenarios that allow high r score values to be obtained if ross does not achieve convergence within m iterations the number of scenarios included in the robust optimization process is too small for identifying a robust solution in this case the subset cardinality n 0 has to be increased and the algorithm relaunched it is worth noting that the al parameters n e x p and p are strictly dependent on the dimension n k of the distribution d as this latter is updated considering p n e x p elements of φ in addition the convergence of the distribution d towards the optimal regions of the scenario set is driven by the ratio p n e x p once p is defined the number of experiments n e x p has to be selected balancing the convergence speed and the ability to explore the entire scenario set in the following we provide some general rules that allow to set ross parameters without conducting a sensitivity analysis in particular m can be set as a very high value as a convergence criterion based on the r score is implemented to terminate the algorithm p usually ranges between 0 1 and 0 3 low p values allow to reach the convergence earlier but usually need a higher number of extractions at each iteration higher n e x p value in order to obtain a sufficient number of scenarios for updating the gaussian distribution high p values reduce the speed of convergence but a lower number of extractions at each iteration is needed to obtain a sufficient number of scenarios for updating the gaussian distribution finally n e x p is set accordingly to p and the dimensions of the gaussian distribution in particular the higher the dimensions and the lower p are the higher n e x p should be we found that n e x p values should range between 100 and 300 depending on the above mentioned parameters even if its value does not particularly affect the ability of the algorithm to reach the convergence 3 case study as part of the case study ross is applied to identify least cost hybrid energy system designs that are robust with respect to plausible future changes in the main climate drivers i e solar radiation wind speed temperature for the small italian island of ustica the surface area of the island is 8 km 2 and it is located about 50 km north of sicily in the mediterranean sea fig 2a ustica has a resident population of 1 559 inhabitants which nearly doubles during the summer touristic months electricity is produced entirely by 5 diesel generators with a total installed capacity of 4 6 mw household consumption accounts for nearly 70 of the annual electricity demand with the remaining 30 covered by a desalination plant built in 2016 to satisfy the entire water demand due to the high seasonal variability of the electricity demand caused by high touristic fluxes in the summer months diesel generators are over sized to cater to the summer peaking demand even if energy supply security is guaranteed the energy system of the island is scarcely sustainable from both environmental and economic points of view in order to improve the sustainability of this costly and inefficient system the design of a hybrid energy system combining diesel generation with res and storage technologies is considered as a potential solution the high natural solar and wind resources of the island can be exploited through the installation of micro wind turbines and photovoltaic pv systems coupled with storage technologies in order to produce clean energy at lower costs however the performance of such a hybrid energy system over a medium long term horizon is strongly affected by future uncertainty in the main climate variables e g solar radiation wind speed temperature thus considering this uncertainty in the identification of the optimal hybrid system design is essential to generate solutions that are robust with respect to a wide range of plausible future conditions these attributes make ustica an ideal case study for testing ross as mentioned in the introduction 3 1 model electricity supply and demand are represented by means of a mathematical model fig 2b simulating the electricity grid dynamics using an hourly time step at each time step t the electricity is generated by the planned res technologies i e pv and wind and the 5 existing diesel generators to meet a total load l t o t t composed of the base electrical load l b t and the load of the desalination plant l d e s t when the potential electricity output e p o t t exceeds the electrical load l t o t t the batteries are charged according to their maximum charge and capacity constraints if a non storable electricity surplus e s u r t is generated a res power curtailment is applied when the electrical load l t o t t exceeds the potential electricity output batteries are discharged according to their maximum discharge and capacity constraints after having discharged the batteries if the total load is not met the diesel units are forced to produce electricity in order to ensure the load is always completely covered the reader could refer to giudici et al 2019 for a detailed description of the equations governing the electricity production from pv wind and diesel generators a description of the batteries dynamics not included in giudici et al 2019 is provided in supplementary material whereas the formulation of the optimization problem is reported in the following paragraph formulation of the optimization problem we focus on identifying the configuration of the hybrid energy system a that minimizes the objective function j a over a given simulation horizon the optimization problem is formulated as follows 2 a a r g min a j a where j a represents the net present cost and is calculated as follows 3 j a c c a p a y 1 h δ y c g r i d c o p e r y a c r e p y a c s a l y a where all costs occurring at each hour t throughout the simulation horizon are aggregated on a yearly basis y h is the number of years of the simulation horizon c c a p a are the capital costs c g r i d are the costs for the management of the electricity grid and c o p e r y a c r e p y a c s a l y a are the operational replacement and salvage costs at year y respectively it is worth noting that problem 2 is the classical formulation of the optimization problem that does not account for the effects of the uncertainty in the external drivers on the objective function see section 3 2 for the formulation of the robust optimization problem for each of the robustness metrics considered all costs except the capital ones are discounted using the following time varying coefficient 4 δ y 1 1 γ y where γ is the real discount rate calculated as a function of the nominal discount rate γ and the inflation rate φ 5 γ γ φ 1 φ the capital costs occur at the beginning of the simulation horizon and represent the investment to install the power technologies the replacement costs occur when a technology has to be substituted and the salvage costs are negative costs which are incurred at the end of the simulation horizon when one or more technologies have not reached the end of their lifetime finally the operational costs take into account both the cost of maintenance of each power technology and the cost of fuel net present cost is dependent on both the configuration of the hybrid energy system a and the climatic conditions i e wind speed solar radiation temperature which significantly affect the res power potential and consequently the electricity generation costs in particular we assume an electricity generation cost associated with res equal to zero the decision variables a of problem 2 are the pv capacity c p v and the number of wind turbines n w to be installed defined within the following feasibility sets c p v 100 2000 and n w 1 20 the upper bound of the feasibility sets is determined considering the small size of the island and the tight environmental constraints which strictly limit the maximum installable res capacity in this work we do not consider the number of batteries as a decision variable but we fix the capacity of the storage system to be equal to the pv capacity even if these simplifications potentially prevent the identification of the exact optimal number of batteries to be installed it constitutes a reasonable and conservative assumption i e empty batteries could store the hourly maximum pv power production this is because it ensures that the pv electricity surplus generated in the central hours of the day is stored in the batteries to enable the required load to be met during the night in addition it allows the size of the decision space to be reduced significantly by not considering bad or even infeasible solutions i e high number of batteries and low pv capacity we solve problem 2 using an exhaustive search within the feasibility set of the decision variables over a 1 year evaluation period assuming that external drivers do not change from one year to another during the simulation horizon to do this we sample the feasibility set of c p v with a 100 kw discretization step which is sufficiently fine to allow significant changes in the objective function to be captured giudici et al 2019 using this discretization the cardinality of the search space is equal to 400 the optimization is performed using an hourly simulation time step and the technical and economic parameters reported in supplementary material 3 2 experiment settings scenarios in this work we generate climate scenarios of wind speed solar radiation and temperature using an hybrid approach matrosov et al 2013 roach et al 2016 which combines top down haasnoot et al 2013 giuliani and castelletti 2016 and bottom up culley et al 2016 bertoni et al 2019 methods by first estimating future conditions from climate models and then enlarging the range of plausible future scenarios in order to stress test the system of interest we first consider climate projections generated by five different euro cordex scenarios see www euro cordex net defined as combinations of representative concentration pathways rcps ipcc 2014 global circulation models gcms and regional circulation models rcms table 1 these scenarios have a spatial resolution of 0 11 degrees and provide projections for the period 2006 2100 to resolve the mismatch between the spatial resolution of rcms and that of our study site we apply a statistical downscaling method based on quantile mapping boe et al 2007 estimating a correction function between the observations of the climate variables at the local scale and the rcm output over the control period 1971 2005 since our model simulates the system over a reference year we consider each projected year as a single scenario then we enlarge the mean annual variability of wind speed and solar radiation for stress testing our system under more variable conditions in order to identify potential vulnerabilities in particular we enlarge the variability of solar radiation by 2 and the variability of wind speed by 10 these values have been arbitrarily selected to capture more extreme conditions than the ones predicted by the euro cordex climate projections but which are still feasible and reliable for our specific case study we focus on these two variables as the most uncertain and the ones that are most likely to significantly affect planning decisions as solar radiation and wind speed are the main drivers determining pv and wind power potential for a given installed capacity at the end of the scenario generation procedure we obtain 3125 scenarios of hourly values of wind speed solar radiation and temperature composing the scenario set f considered in ross this scenario set is shown in fig 3 where each point represents the mean annual value of each climate variable according to a specific scenario wind speed is clearly the driver with the largest variability with mean annual values that range between 3 4 and 7 m s solar radiation ranges between 253 8 and 272 7 w m 2 and temperature values vary from 18 2 to 24 5 c robustness metrics the robustness metrics we consider include minimax this metric identifies the alternative a that attains the best performance in the worst case 6 a a r g min a max ξ j a w this metric usually associated with a pessimistic point of view selects the alternative assuming that the worst future conditions will be realized wald 1950 minimin this metric identifies the alternative a that attains the best performance in the best case 7 a a r g min a min ξ j a w this metric usually associated with an optimistic point of view selects the alternative assuming that the best future conditions will be realized wald 1950 hurwicz this metric called the optimism pessimism rule combines the first two metrics identifying the alternative a that attains the best weighted sum of the performance obtained in the worst case and in the best case 8 a a r g min a α max ξ j a w 1 α min ξ j a w where 0 α 1 is a weight that specifies the relative importance associated with the realization of the worst or best future condition depending on the choice of the weight α which is related to the level of risk aversion of the decision maker this metric can be associated with a more or less pessimistic or optimistic point of view in this work we consider α 0 66 hurwicz 1953 laplace this metric called the principle of insufficient reason selects the alternative a that attains the best expected performance over the n future scenarios 9 a a r g min a 1 n i 1 n j a w i this metric suggests risk neutrality of the decision maker and implicitly assumes that each future scenario could be realized with the same probability laplace 1951 mean variance this metric tries to balance the expected performance and its variability over future scenarios identifying the alternative a such that 10 a a r g min a 1 n i 1 n j a w i v a r ξ j a w this metric is based on the assumption that an optimal robust solution is one whose performance is not very sensitive to which plausible future will be realized hamarat et al 2014 however the main disadvantages of this metric are that it is not always monotonically increasing ray et al 2014 and positive and negative deviations from the mean are treated equally takriti and ahmed 2004 ross parameterization we implement ross see section 2 1 for our case study using the following parameters m 20 n e x p 100 p 0 1 m represents the maximum number of al iterations n e x p the number of scenario samplings from the gaussian distribution d at each al iteration and p a fraction of n e x p identifying the best scenarios used to update the gaussian distribution d these parameters have been set taking into account the specific characteristics of our case study and indications reported at the end of section 2 1 in contrast the parameter n 0 which identifies the cardinality of the scenario subset is set according to the robustness metric selected it is worth noting that for the minimax minimin and hurwicz metrics the cardinality n 0 defines exactly the minimum number of scenarios to search for following their intrinsic formulation in particular for the minimax and minimin metrics n 0 is equal to 1 as both require system performance to be evaluated for the worst and the best scenario respectively for the hurwicz metric n 0 is equal to 2 as it requires system performance to be evaluated for a weighted sum of the worst and the best scenario for the laplace and mean variance metrics n 0 is set equal to 2 as this represents the minimum number of scenarios needed to mathematically compute the metrics even though in theory this value of n 0 could be too small for identifying a robust solution and would therefore have to be increased iteratively until such a robust solution is found see section 2 this is not the case of our specific case study finally the parameter k which represents the dimension of the scenario set f is equal to 3 since we consider three different climate drivers 3 3 computational experiments ross is run for each of the five robustness metrics considered using the settings defined in section 3 2 first we assess the ability of ross to identify the most informative sub regions of the scenario set by analysing the evolution of the r score and the gaussian distribution d throughout the m al iterations section 4 1 at each al iteration ross calculates n e x p different r scores corresponding to the n e x p scenario subset extractions from the same gaussian distribution d since the best and the worst scores are strongly affected by outliers we assume that ross is at convergence as soon as the 0 1 quantile of the scores i e value under which only 10 of the scores lies reaches zero see section 2 1 secondly in order to identify the key drivers that mostly influence system robustness we explore how the most informative regions of the scenario set and the resulting robust solution change according to the robustness metric selected by comparing the different gaussian distributions at convergence section 4 2 finally we assess the advantages of using ross by comparing it with a traditional robust optimization method i e brute force method which evaluates all candidate solutions over the entire scenario set and a state of the art scenario selection method i e random sampling method trindade et al 2017 which randomly extracts a small scenario subset to be included in the optimization process section 4 3 since the brute force method always guarantees the identification of the most robust solution we perform the comparison by focusing on the computational requirements calculated as the number of system simulations needed to obtain a robust solution in particular the computational requirements of the brute force method are calculated multiplying the number of scenarios included in the robust optimization process i e 3125 by the number of candidate solutions in the search space namely 400 see section 3 1 resulting in a total of 1 250 000 simulations with ross this number depends on the metric considered and is calculated by multiplying the number of al iterations needed to reach the convergence by the subset cardinality n 0 and the number of extraction n e x p at each al iteration ross solves n e x p robust optimization problems considering n scenarios it is worth noting that we do not consider the simulations needed to re evaluate the solutions over the entire scenario set within the calculation of ross computational requirements as we assume to know a priori the target performance resulted from that re evaluation see section 2 1 for further details in order to compare ross with the random sampling method we perform two different experiments focusing on the minimax metric and evaluating the results in terms of both computational requirements and r score in the first experiment we perform a robust optimization randomly extracting the same number of scenarios evaluated by ross for reaching the convergence and we compute the r score as percentage difference between the robustness calculated over that scenarios and the one obtained considering the entire scenario set since the extraction is performed randomly we repeat the robust optimization 100 times in order to reliably infer the probability of identifying the robust solution and compute the 0 1 quantile of the r scores used to assess ross convergence in the second experiment we perform the comparison by fixing the computational requirements to the ones needed by ross for reaching the convergence in this case we performed 10 robust optimizations assumed to be sufficient to filter the uncertainty in the random sampling by including a number of randomly extracted scenarios equal to the number of scenarios evaluated by ross divided by 10 the computational experiments are performed on a personal computer with a 2 5 ghz intel core i5 3210m processor with 2 cores and 8 gb system ram for each system simulation the model takes about 0 05 s for a total number of computational hours ranging from 2 77 to 13 33 depending on the robustness metric considered 4 numerical results 4 1 ability to identify the most informative sub regions of the scenario set numerical results show that best and worst scores as well as the 0 1 quantile of the n e x p scores rapidly increase with the number of al iterations leading ross to reach convergence in a small number of al iterations 12 maximum for all the robustness metrics considered fig 4 this means that ross effectively moves towards the regions of the scenario set f containing the most informative scenarios for generating a robust solution in particular convergence is reached earlier for the minimax and minimin metrics fig 4a b which are characterized by a subset cardinality of n 0 1 and consequently a gaussian distribution with n k 3 dimensions only the other metrics fig 4c e are characterized by a subset cardinality of n 0 2 and thus a gaussian distribution with n k 6 dimensions due to the higher dimensions of the gaussian distribution convergence is reached later in these cases another interesting aspect related to algorithm convergence concerns the value the worst score grey lines in fig 4 assumes in the first al iterations as can be seen this is strongly dependent on the metric considered and indicates the degree to which the robustness of the optimal solution is sensitive to the scenario subset considered for example if we consider the laplace metric fig 4d the worst score at the first al iteration is about 5 meaning that independently from the scenarios extracted the robustness of the optimal solution computed over that scenarios does not differ too much from the one computed over the entire scenario set on the contrary for the mean variance metric fig 4e the worst score at the first al iteration is about 230 in this case the difference between the robustness of the optimal solution calculated for the scenario subset and the one computed for the entire scenario set is highly sensitive to the scenarios extracted making the identification of the most informative regions of the scenario set more challenging this different behaviour is related to both the system dynamics of our specific case study and the intrinsic formulation of the robustness metric in particular the mean variance metric has a more complex formulation compared with the laplace one as it evaluates the robustness of a solution considering a combination of the average performance and the performance dispersion over the entire scenario set making it difficult to identify a smaller number of scenarios able to generate a robustness value that is similar to the one obtained considering the entire scenario set the rapid increase in r score towards convergence is driven by the evolution of the multivariate gaussian distribution d according to which the scenarios to be included in the optimization process are extracted results show that the gaussian distribution rapidly shrinks in the most informative regions of the scenario set the mean identifies the most interesting regions and the variance tends to decrease throughout the al iterations fig 5 in particular fig 5a refers to the minimax metric and fig 5b to the hurwicz metric see supplementary material for the other metrics for illustration purposes we show the gaussian distribution projected in the space of wind speed and solar radiation the two drivers that mostly influence system performance each grey point represents a plausible scenario and each black circle represents two dimensions of the multivariate gaussian distribution corresponding to the two variables i e wind speed and solar radiation characterizing a scenario with the centre representing the mean and the radius three times the standard deviation i e 99 7 of extraction probability is included in the circle the upper and bottom panels refer to the first al iteration and the al iteration at convergence respectively the middle panels show an intermediate al iteration it is worth noting that for the minimax metric there is only one circle evolving towards convergence as the cardinality of the scenario subset is equal to 1 fig 5a for the hurwicz metric there are two circles as the cardinality of the scenario subset is equal to 2 we can observe that at the first al iteration the gaussian distribution is initialized covering the entire scenario set so that there is no need to precondition the algorithm to evolve towards particular regions of the scenario set for the same reason the two circles of the hurwicz metric completely overlap it is interesting to note that the variance of the gaussian distribution at convergence for the minimax metric is higher compared with that of the hurwicz metric this is mainly due to the fact that in this case the distribution moves outside the scenario set where no scenarios exist as a consequence irrespective of which point is extracted from that distribution the nearest neighbour search see section 2 1 always identifies the same small number of scenarios included in the circle representing the distribution 4 2 ability to identify most informative scenarios and robust solutions for different robustness metrics in order to explore how the most informative regions of the scenario set change according to the robustness metric selected we show the gaussian distribution at convergence projected in the space of wind speed and solar radiation for the five metrics considered fig 6 the gaussian distribution circles and the corresponding most likely extracted points are represented with different colours according to the robustness metric we can observe that the distribution associated with the minimax metric purple circle which aims at selecting the best solution in the worst case identifies a region characterized by low wind speed and low solar radiation in our case these conditions represent the worst possible conditions as they significantly reduce the renewable power potential and increase the cost for electricity production the robust solution can be thus determined by evaluating the system performance over one of the scenarios included in the purple circle not surprisingly the distribution associated with the minimin metric green circle which focuses on determining the best solution in the best case identifies a region characterized by high wind speed and high solar radiation these conditions allow the highest renewable power potential to be attained and consequently lower cost for electricity production the most informative regions associated with the laplace metric yellow circles include scenarios with average values of wind speed and solar radiation coherent with the metric definition this result shows that the average system performance evaluated over the entire scenario set is almost equal to that evaluated over a small scenario subset characterized by average future conditions this behaviour suggests a direct correlation between the values of wind speed and solar radiation and system performance in terms of net present cost if we consider the hurwicz metric the solution that performs better over a weighted combination of worst and best case can be obtained by extracting scenarios from two regions red circles that present average values of solar radiation and average and low values of wind speed one of these regions almost overlaps the one identified by the laplace metric the scenarios that are most likely to be extracted by both laplace and hurwicz metrics are represented as orange points the other region is instead located between minimax and laplace as the highest weight i e 0 66 is associated with the worst case see section 3 2 we can observe that in this case the two scenarios to be included in the optimization process are extracted from the two regions of the scenario set that present different values of wind speed but almost the same values of solar radiation this suggests that wind speed is the driver that most influences system performance for a given res installed capacity it is worth noting that even if the hurwicz metric computes a weighted some of worst and best case performance the most informative regions do not converge to the ones obtained by the minimax and minimin metrics this is mainly due to the fact that multiple scenario pairs that allow to obtain the same weighted sum as the one calculated on the worst and the best scenarios might exist since the algorithm evolution stops when the difference between the weighted sum calculated considering the extracted scenarios equals the one obtained considering the worst and the best scenarios over the entire scenario set the regions identified at convergence could differ from the two obtained considering the minimax and minimin metrics however the obtained regions can be in any case considered as equally informative as the two extreme regions finally the regions identified for the mean variance metric blue circles which aims to select the solution that performs better on average and at the same time has a low variance if re evaluated over the entire scenario set present medium low values of solar radiation and average values of wind speed in particular unlike the laplace metric which focuses on average performance only in this case the two regions are further apart in order to capture the variability of wind speed which is the driver that most affects the system performance of a given solution in conclusion the robust solutions obtained for the different robustness metrics are reported in table 2 in terms of pv capacity c p v and number of wind turbines n w as expected the minimax and mean variance metrics result in the installation of the minimum number i e 1 of wind turbines the first metric i e minimax focuses on the worst scenario which is characterized by very low values of wind speed as a consequence installing a higher number of wind turbines would cause an increase in the capital costs without having any benefit associated with higher renewable energy production the second metric i e mean variance focuses on minimizing the variability of the solution performance when re evaluated over the entire scenario set in this case the variability in renewable power generation due to the high variability in wind speed would increase with the number of wind turbines installed leading to highly variable solution performance conversely the minimin metric results in the installation of the maximum number i e 20 of wind turbines in order to fully exploit the high values of wind speed associated with the best scenario the hurwicz and laplace metrics result in the selection of an intermediate number of wind turbines 10 for hurwicz and 12 for laplace in particular the solution associated with the hurwicz metric is characterized by a slightly lower number of wind turbines as it assigns a higher weight to the worst scenario in contrast the robust solution in terms of pv capacity is selected in a completely different way the minimax and mean variance metrics result in the installation of the maximum allowable pv capacity i e 2000 kw whereas the minimin metric results in the installation of a very low pv capacity i e 900 kw both the hurwicz and laplace metrics result in the installation of a medium high pv capacity namely 1700 kw and 1600 kw respectively this is mainly due to the relative low marginal cost of pv with respect to that of the wind turbines and the very small variability of solar radiation when compared with that of wind speed 4 3 benchmarking with brute force and random sampling methods we compare ross with the brute force method by assessing the ross computational efficiency gain calculated as percentage reduction of system simulations with respect to the brute force method for the robustness metrics considered comparison results show that the computational efficiency gain obtained using ross ranges from 23 to 84 depending on the complexity of the robustness metric and the cardinality n 0 of the scenario subset this result suggests that in all cases ross succeeds in considerably reducing the computational requirements of solving robust optimization problems by selecting the most informative scenarios to be considered within the optimization process table 3 shows the computational requirements see section 3 3 for details on the calculations for each metric considered and the corresponding computational efficiency gain calculated as the percentage reduction in required system simulations with respect to the brute force method the total computational time of ross ranges from 2 77 h for the minimin metric 84 of reduction to 13 33 h for the mean variance metric 23 of reduction compared with approximately 17 h for the brute force method such computational efficiency gains become more significant when considering applications on more complex systems characterized by a higher number of scenarios and where simulation based optimization techniques are adopted to find the robust solutions such as the growing number of studies relying on evolutionary algorithms to solve robust optimization problems trindade et al 2017 gold et al 2019 trindade et al 2019 in order to compare ross with the random sampling method we first perform 100 runs of the random sampling method extracting the same number of scenarios evaluated by ross for reaching the convergence i e 700 for the minimax metric comparison results show that random sampling method identifies the most robust solution r score equal to 0 with a probability of 24 compared to the 90 probability of ross in addition the 0 1 quantile of the r scores computed in the 100 runs of the random sampling method is equal to 0 3 compared to the 0 of ross at convergence 7th iteration for the minimax metric it is worth noting that even if the difference between random sampling and ross in terms of 0 1 quantile of the r scores is very small random sampling reaches this performance with a 100 times higher computational time with respect to ross i e 100 robust optimizations performed by including 700 scenarios each moreover if a 0 1 quantile of the r score equal to 0 3 can be considered acceptable ross convergence would be reached at the 4th iteration further reducing its computational requirements only 400 scenarios should be evaluated secondly we fix the computational requirements to the ones needed by ross for reaching the convergence i e 700 system simulation for the minimax metric and we perform 10 runs of the random sampling method extracting a number of scenarios equal to the number of scenarios evaluated by ross divided by 10 i e 70 scenarios results show a strong degradation of the performance of random sampling method which never identifies the most robust solution r score never reaches 0 and obtains a 0 1 quantile of the r scores equal to 3 29 in summary despite random sampling method allows generating robust solutions by including a small scenario subset within the optimization phase multiple runs of the algorithm are needed to assess its reliability in identifying a robust solution thus leading to higher computational requirements compared to ross for the same level of robustness conversely if the two algorithms are compared with exactly the same computational effort the performance of the random sampling method significantly degrades 5 conclusions in this paper we propose ross robust optimal scenario selection a novel algorithm to identify the smallest scenario subset to be included in the optimal generation of robust solutions under deep uncertainty once the set of deeply uncertain scenarios the robustness metric and the cardinality of the scenario subset have been defined ross uses an active learning algorithm to adaptively identify the regions of the scenario set that include scenarios for which the robustness of the optimized solution is similar or even equal to that obtained by considering the entire scenario set first we assess the ability of ross to iteratively identify the regions of the scenario set that contain the most informative scenarios for generating robust solutions secondly we explore how the most informative regions of the scenario set and the resulted robust solution change according to the robustness metrics selected highlighting the relative influence of different uncertain drivers on the robustness of the most robust solutions finally we evaluate the advantages of using ross by comparing it with a traditional robust optimization method i e brute force method which considers the entire scenario set within the optimization process and a state of the art scenario selection method i e random sampling method trindade et al 2017 that randomly extracts a small scenario subset to be included into the optimization process we tested ross on the robust planning of a hybrid energy system on the ustica island italy the problem consists in the identification of the least cost photovoltaic pv and wind capacity under deep uncertainty in the main climatic variables affecting the system namely wind speed solar radiation and temperature results show that ross rapidly reaches convergence maximum 12 iterations for the more complex mean variance metric by identifying the regions of the scenario set containing the most informative scenarios for generating a robust solution this results in a computational efficiency gain varying from 23 mean variance metric to 84 minimin metric compared with brute force method which is achieved by solving multiple optimization problems considering small scenario subsets it is worth noting that these significant computational efficiency gains are obtained considering the minimum subset cardinality needed to compute the robustness metrics however as already mentioned in section 2 for complex dynamic systems affected by a large set of co varying uncertain factors this subset cardinality could be too small for identifying a robust solution in this case the algorithm has to relaunched by iteratively increasing the subset cardinality until a robust solution is found leading to an increase in computational requirements and consequently a potential decrease in the computational efficiency gain comparison with random sampling method shows that despite this latter allows generating robust solutions by including a small scenario subset within the optimization phase multiple runs of the algorithm are needed to assess its reliability in identifying a robust solution thus leading to higher computational requirements compared to ross for the same level of robustness conversely if the two algorithms are compared with exactly the same computational effort the performance of the random sampling method significantly degrades the regions identified at the end of the active learning procedure highlight the deeply uncertain future conditions that mostly influence the system robustness depending on the robustness metric considered these regions are characterized by different values of the uncertain drivers for example the minimax metric which aims at selecting the best solution in the worst case identified a region characterized by low wind speed and low solar radiation as these conditions reduce the renewable power potential while increasing the cost for electricity production conversely the minimin metric which focuses on determining the best solution in the best case identified a region characterized by high wind speed and high solar radiation the other metrics identified regions in between the extreme ones highlighted by the minimax and minimin metrics showing that wind speed represents the deeply uncertain driver that most significantly affects system performance and the robustness of the solution by abstracting from this specific case study ross has the ability to significantly improve existing state of the art robust optimization methods with respect to two important aspects first ross allows the high computational requirements associated with the inclusion of robustness within the optimization process to be reduced significantly by selecting a small scenario subset as the most informative for generating robust solutions second ross identifies the key uncertain drivers to which system performance is most sensitive as well as the values of the drives over which this occurs in an adaptive manner using active learning these improvements can lead to significant advantages especially when complex dynamic systems where decisions are affected by a large ensemble of deeply uncertain co varying factors are involved in these cases system non linearity as well as unpredictable behaviours pose great challenges in predicting how the system would perform in response to uncertain changing conditions and consequently in identifying system vulnerabilities and robust solutions one of the main limitations of the current version of ross is that in contrast to much of the state of the art works that consider system robustness with respect to multiple objectives it allows to solve robust optimization problems considering a single objective only however this limitation could be partially overcome by iteratively solving a robust multi objective optimization problem and computing the r score as the hypervolume between the obtained pareto front and a target pareto front reflecting the target performance to be ideally achieved in this case the algorithm would try to identify that regions of the scenario set that are the most informative for generating a pareto front that is as close as possible to the target one another ross limitation concerns its inability to guarantee that the identified solution is robust also with respect to other uncertainties or scenarios outside the priors considered in the scenario set however these uncertainties if known could be included within the scenario set and ross ability of identifying the most informative scenarios would be even more exploited for generating a robust solution further reducing computational time with respect to traditional robust optimization methods further research efforts will focus on testing ross for i solving robust joint planning and management optimization problems where the optimal system design strictly depends on the adopted control strategy and vice versa and ii analysing more complex systems characterized by a large set of climate socioeconomic and technological uncertain and potentially interdependent drivers in this latter case when dealing with continuous uncertainties e g scenarios of continuous variables such as temperature wind speed electricity demand as in our case study a multi variate gaussian distribution can be employed to filter the multi dimensional scenario set the dimensions correspond to the number of uncertainties to be considered when instead the uncertainties to be considered are categorical e g system parameters model structures a categorical distribution e g bernoulli for 2 categories could be adopted to assign a probability to each category this categorical distribution evolves throughout the active learning iterations by concentrating the highest probability on those categories that allow to obtain a robust solution declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements federico giudici has been financed by the research fund for the italian electrical system in compliance with the decree of april 16 2018 appendix a supplementary data supplementary material related to this article can be found online at https doi org 10 1016 j envsoft 2020 104681 appendix a supplementary data the following is the supplementary material related to this article mmc s1 
26039,deep uncertainty in future climate socio economic and technological conditions poses a great challenge to medium long term decision making recently several approaches have been proposed to identify solutions that are robust with respect to a large ensemble of deeply uncertain future scenarios in this paper we introduce ross robust optimal scenario selection a novel algorithm that uses an active learning approach for adaptively selecting the smallest scenario subset to be included into a robust optimization process ross contributes a twofold novelty in the field of robust optimization under deep uncertainty first it allows the computational requirements for the generation of robust solutions to be considerably reduced with respect to traditional optimization methods second it allows the identification of the most informative regions of the scenario set containing the scenarios to be included in the optimization process for generating a robust solution we test ross on the real case study of robust planning of an off grid hybrid energy system combining diesel generation with renewable energy sources and storage technologies results show that ross enables computational requirements to be reduced between 23 to 84 compared with traditional robust optimization methods depending on the complexity of the robustness metrics considered it is also able to identify very small regions of the scenario set containing the most informative scenarios for generating a robust solution keywords robust optimization deep uncertainty active learning robust planning hybrid energy systems 1 introduction changes in future climatic and socio economic conditions as well as rapid technological innovation represent sources of uncertainty that significantly influence medium long term decision making in different fields e g water management infrastructure planning energy systems design harrison et al 2015 maier et al 2016 identifying planning solutions or management strategies for complex environmental systems can thus be extremely challenging as the consequences of a decision in terms of system performance strongly depend on the external uncertain conditions that will actually unfold in the future uusitalo et al 2015 mcphail et al 2018 when information on the probability of occurrence of future uncertain conditions is available and the uncertainty can be described using a stochastic model the problem is defined as decision making under risk french 1986 when instead such probability of occurrence is unknown and a set membership description of the uncertain future conditions is the only available information the problem is classified as decision making under deep uncertainty lempert and schlesinger 2000 lempert 2002 dessai et al 2009 several approaches have been proposed to support decision making under deep uncertainty see herman et al 2015 and maier et al 2016 for a review the most widely adopted approaches include robust decision making rdm lempert 2002 lempert et al 2003 bryant and lempert 2010 hall et al 2012 decision scaling brown et al 2012 moody and brown 2013 ghile et al 2014 poff et al 2015 and many objective robust decision making mordm kasprzyk et al 2013 herman et al 2014 paton et al 2014 all these approaches aim at evaluating the robustness of different planning or management alternatives over an ensemble of plausible future scenarios focusing on understanding and retrieving the future conditions for which a given alternative fails or not these conditions which highlight the main system vulnerabilities are then considered for re designing improved solutions within an iterative stress testing process converging to the identification of the most robust strategy lempert et al 2006 groves and lempert 2007 following the taxonomy proposed by herman et al 2015 these approaches can be classified based on the methods adopted for i identifying alternatives e g pre specified e g tingstad et al 2014 or optimally generated e g quinn et al 2018 ii sampling scenarios e g top down e g mahmoud et al 2009 or bottom up e g nazemi and wheater 2014 methods iii quantifying robustness measures e g regret or satisficing measures e g lempert and collins 2007 and iv identifying key uncertainties through sensitivity analysis e g scenario discovery e g bryant and lempert 2010 singh et al 2014 irrespective of which of the above approaches is used the following sequential steps have to be performed i generation of alternative solutions or policies ii sampling of plausible future scenarios and iii robustness analysis since the above robustness analysis is conducted on pre defined solutions there is no guarantee the most robust solutions are identified to partially overcome this problem recent studies focus on scenario selection methods for identifying vulnerable or problematic scenarios to be included within the alternative generation phase in order to enhance the robustness of the candidate solutions watson and kasprzyk 2017 eker and kwakkel 2018 in particular alternatives are first generated by preforming multiple optimization runs considering the identified scenarios independently and then re evaluated in a robustness assessment over a large ensemble of scenarios however in this case the design of most robust solutions is strictly dependent on the scenarios selected and used to optimize the alternatives robust optimization ro methods overcome these issues by identifying solutions that maximize robustness over a range of plausible future scenarios this is achieved by combining robustness calculations and formal optimization processes identifying solutions that perform satisfactorily over a wide range of deeply uncertain scenarios however this requires the performance of each alternative to be simulated at each of these scenarios making the optimization process computationally demanding or even intractable especially when the number of scenarios increases from tens to several thousand roach et al 2016 this is the reason why most studies that combine robustness assessment and the use of formal optimization for alternative generation perform the robustness assessment after the completion of the optimization process kasprzyk et al 2013 paton et al 2014 beh et al 2015 studies incorporating robustness as an explicit objective in the optimization problem usually consider a limited number of scenarios hamarat et al 2014 basupi and kapelan 2015 kwakkel et al 2015 giuliani and castelletti 2016 kwakkel et al 2016b zeff et al 2016 shavazipour and stewart 2019 however some of these works explicitly highlight as a key research challenge the necessity of handling a large number of scenarios by reducing computational requirements through scenario reduction algorithms or novel solution methods kwakkel et al 2016b shavazipour and stewart 2019 to overcome the computational issues of ro methods some studies use computationally efficient metamodels castelletti et al 2012b as surrogates of the hi fidelity computationally expensive simulation models that are generally used in the calculation of robustness values within the optimization process yan and minsker 2011 broad et al 2015 beh et al 2017 these metamodels are usually black box models that map future uncertain conditions and decision variables onto robustness values while their use can increase the computational efficiency of the optimization process significantly their black box nature means that the underlying system dynamics are not able to be represented explicitly castelletti et al 2011 2012a making it difficult to obtain insight into the plausible future conditions that are likely to result in system failure an alternative approach to increasing the computational efficiency of robust optimization methods is to adopt sampling techniques to extract a smaller scenario subset to be included in the robustness calculation this subset can be pre specified trindade et al 2017 gold et al 2019 or randomly generated at each iteration of the optimization process kasprzyk et al 2012 giuliani et al 2014 while consideration of a smaller number of scenarios reduces computational requirements it also introduces the possibility that the robustness of the solution identified in the optimization process is significantly different from the one resulting from the consideration of the entire scenario set in order to overcome the shortcomings of previous studies incorporating robustness as an optimization objective outlined above we introduce ross robust optimal scenario selection a novel algorithm that uses an active learning approach cohn et al 1996 to identify the smallest scenario subset for which the robustness of the optimized solution is similar or even equal to that obtained by considering the entire scenario set thereby increasing computational efficiency by reducing the number of scenarios over which solution performance has to be evaluated active learning is a sub field of machine learning as part of which the learning algorithm is directly responsible for acquiring its training data set through i experiments on the real systems or ii running simulations of generative models to obtain the desired outputs for new unknown inputs in our case the active learning principles are applied for filtering an existing data set i e scenario set in order to select a concise sub set of sufficiently informative scenarios for generating robust solutions ross contributes a twofold novelty in the field of robust optimization under deep uncertainty first it allows to considerably reduce the computational requirements for the generation of robust solutions with respect to traditional optimization methods and state of the art scenario selection techniques second it iteratively explores the entire scenario set within the robust optimization process in order to identify its most informative regions for designing a robust solution similar to scenario discovery in some cases these regions also highlight the main system vulnerabilities i e scenarios for which the system performs poorly that should be then considered for designing robust solutions in order to demonstrate the utility of ross we apply it to the robust planning of a hybrid energy system for the ustica island italy this case study is ideally suited to demonstrate the capabilities of ross as it is focused on the optimal design of a hybrid energy system including different types of renewable energy sources res which is subject to deep uncertainties associated with future changes in a number of climatic variables e g solar radiation wind speed temperature in addition due to the very high number of future scenarios the consideration of robustness as an optimization objective is computationally demanding we first assess the ability of ross of iteratively identifying the regions of the scenario set that contain the most informative scenarios for generating a robust solution as highlighted by different studies giuliani and castelletti 2016 kwakkel et al 2016a mcphail et al 2018 defining the robustness of a solution constitutes itself a meta problem i e deciding how to decide schneller and sphicas 1983 as it strictly depends on the attitude of the decision makers in facing and reacting to future uncertain conditions to address this aspect we repeat our analysis for multiple robustness metrics and explore how the most informative regions of the scenario set and the resulting robust solution change according to the robustness metrics selected highlighting the relative influence of different uncertain drivers on the robustness of the solution finally we evaluate the computational advantages of using ross by comparing it with a traditional robust optimization method i e brute force method which considers the entire scenario set within the optimization process and a state of the art scenario selection method i e random sampling method trindade et al 2017 that randomly extracts a small scenario subset to be included into the optimization process the remainder of this paper is organized as follows section 2 presents the adopted methods and the ross algorithm section 3 introduces the case study providing a description of the model adopted to simulate the electricity system and defining the experimental settings section 4 shows the numerical results and section 5 provides conclusions and suggestions for further research 2 methods and tools ross robust optimal scenario selection is an active learning algorithm designed to identify the smallest scenario subset for generating robust solutions fig 1 the main inputs to the algorithm are a set of future plausible scenarios f a robustness metric ψ readers should refer to mcphail et al 2018 for an overview of the most adopted robustness metrics used to calculate the performance of an alternative over the scenarios in f and the cardinality n 0 of the scenario subset to be included in the optimization process as a rule of thumb n 0 is defined according to the robustness metric selected as the minimum number of scenarios needed to mathematically compute the robustness metric see section 3 2 for further details however for complex dynamic systems affected by a large set of co varying uncertain factors this number could be too small for identifying a robust solution in this case n 0 has to be increased iteratively and the algorithm relaunched until a robust solution is found as outputs ross delivers the robust planning solution and the regions of the scenario set containing the most informative scenarios i e the scenarios that have the largest influence on robustness values 2 1 robust optimal scenario selection ross adopts an active learning al algorithm that iteratively moves a gaussian distribution function defined on the scenario set f towards the regions containing the most informative scenarios these are the scenarios for which the robustness of the optimized solution is similar ideally equal to that obtained when robustness is calculated over the entire scenario set following rachelson et al 2011 we choose gaussian distribution functions because they i are simple to update and ii allow to assign a different probability of extraction to each scenario this allows the algorithm to shrink the gaussian towards the most informative regions of the scenario set assigning a higher probability of extraction to the most informative scenarios in order to be rather confident that such scenarios are included within the optimization process most of the times the algorithm algorithm 1 starts by initializing a n k variate gaussian distribution d where n n 0 is the cardinality of the scenario subset and k is the dimension of the scenario set f then n points are sampled according to the distribution d which correspond to a single n k dimensional sample from d to ensure these sampled points align with members of the scenario set we normalize each uncertain variables between 0 and 1 in order to equally weighting the different uncertain drivers and we perform a nearest neighbour search in the scenario space with respect to the euclidean distance between the candidate points and the scenarios in f the n selected scenarios constitute the scenario subset f ˆ n for which a robust solution a is identified via a formal robust optimization process using the selected robustness metric ψ as an objective the degree of similarity between the robustness values calculated over the selected scenario subset and over the entire scenario set is then calculated using the following metric 1 r r o p t a f ˆ n r r e v a l a f r r e v a l a f 100 where r o p t a f ˆ n is the robustness of the optimal solution a calculated over the scenario subset f ˆ n and r r e v a l a f is the robustness of the optimal solution a calculated over the entire scenario set f the score r to be maximized quantifies the absolute difference in percentage between the robustness of the optimized solution calculated over the scenario subset extracted from the gaussian distribution d and the one computed over the entire scenario set f the lower this difference the higher the score when the score r equals zero the two robustness calculations match and the solution optimized over the scenario subset is also robust with respect to the entire scenario set it is worth noting that considering the robustness value computed over the entire scenario set f as a target to be achieved is needed here for testing the algorithm and verifying that it works correctly however in general this target could be externally imposed without necessarily being generated by optimizing over the entire scenario set in order to assess the ability of the gaussian distribution d to identify the most informative scenarios the above mentioned steps i e scenario sampling nearest neighbour search robust optimization score computation are repeated for n e x p times we thus obtain a set φ f ˆ n r of n e x p elements each composed of n scenarios and the corresponding r score to identify the scenarios that allow the highest score to be obtained φ is ordered descendingly with respect to the evaluation score r and its first p n e x p elements are selected and included in φ then the scenarios composing φ are used to update the distribution d this corresponds to computing the n k variate average vector and the n k n k co variance matrix for the scenarios in φ the al procedure iterates until a maximum number of iterations m is reached or when the 0 1 quantile of the n e x p scores reaches zero in the latter case we can claim that ross is at convergence as 90 of the n e x p extractions from d identifies scenarios that allow the maximum r score value to be obtained namely zero the purpose of the al algorithm thus consists of evolving the parameters i e mean variance of a n k variate gaussian distribution d so as to maximize its capability of extracting scenarios that allow high r score values to be obtained if ross does not achieve convergence within m iterations the number of scenarios included in the robust optimization process is too small for identifying a robust solution in this case the subset cardinality n 0 has to be increased and the algorithm relaunched it is worth noting that the al parameters n e x p and p are strictly dependent on the dimension n k of the distribution d as this latter is updated considering p n e x p elements of φ in addition the convergence of the distribution d towards the optimal regions of the scenario set is driven by the ratio p n e x p once p is defined the number of experiments n e x p has to be selected balancing the convergence speed and the ability to explore the entire scenario set in the following we provide some general rules that allow to set ross parameters without conducting a sensitivity analysis in particular m can be set as a very high value as a convergence criterion based on the r score is implemented to terminate the algorithm p usually ranges between 0 1 and 0 3 low p values allow to reach the convergence earlier but usually need a higher number of extractions at each iteration higher n e x p value in order to obtain a sufficient number of scenarios for updating the gaussian distribution high p values reduce the speed of convergence but a lower number of extractions at each iteration is needed to obtain a sufficient number of scenarios for updating the gaussian distribution finally n e x p is set accordingly to p and the dimensions of the gaussian distribution in particular the higher the dimensions and the lower p are the higher n e x p should be we found that n e x p values should range between 100 and 300 depending on the above mentioned parameters even if its value does not particularly affect the ability of the algorithm to reach the convergence 3 case study as part of the case study ross is applied to identify least cost hybrid energy system designs that are robust with respect to plausible future changes in the main climate drivers i e solar radiation wind speed temperature for the small italian island of ustica the surface area of the island is 8 km 2 and it is located about 50 km north of sicily in the mediterranean sea fig 2a ustica has a resident population of 1 559 inhabitants which nearly doubles during the summer touristic months electricity is produced entirely by 5 diesel generators with a total installed capacity of 4 6 mw household consumption accounts for nearly 70 of the annual electricity demand with the remaining 30 covered by a desalination plant built in 2016 to satisfy the entire water demand due to the high seasonal variability of the electricity demand caused by high touristic fluxes in the summer months diesel generators are over sized to cater to the summer peaking demand even if energy supply security is guaranteed the energy system of the island is scarcely sustainable from both environmental and economic points of view in order to improve the sustainability of this costly and inefficient system the design of a hybrid energy system combining diesel generation with res and storage technologies is considered as a potential solution the high natural solar and wind resources of the island can be exploited through the installation of micro wind turbines and photovoltaic pv systems coupled with storage technologies in order to produce clean energy at lower costs however the performance of such a hybrid energy system over a medium long term horizon is strongly affected by future uncertainty in the main climate variables e g solar radiation wind speed temperature thus considering this uncertainty in the identification of the optimal hybrid system design is essential to generate solutions that are robust with respect to a wide range of plausible future conditions these attributes make ustica an ideal case study for testing ross as mentioned in the introduction 3 1 model electricity supply and demand are represented by means of a mathematical model fig 2b simulating the electricity grid dynamics using an hourly time step at each time step t the electricity is generated by the planned res technologies i e pv and wind and the 5 existing diesel generators to meet a total load l t o t t composed of the base electrical load l b t and the load of the desalination plant l d e s t when the potential electricity output e p o t t exceeds the electrical load l t o t t the batteries are charged according to their maximum charge and capacity constraints if a non storable electricity surplus e s u r t is generated a res power curtailment is applied when the electrical load l t o t t exceeds the potential electricity output batteries are discharged according to their maximum discharge and capacity constraints after having discharged the batteries if the total load is not met the diesel units are forced to produce electricity in order to ensure the load is always completely covered the reader could refer to giudici et al 2019 for a detailed description of the equations governing the electricity production from pv wind and diesel generators a description of the batteries dynamics not included in giudici et al 2019 is provided in supplementary material whereas the formulation of the optimization problem is reported in the following paragraph formulation of the optimization problem we focus on identifying the configuration of the hybrid energy system a that minimizes the objective function j a over a given simulation horizon the optimization problem is formulated as follows 2 a a r g min a j a where j a represents the net present cost and is calculated as follows 3 j a c c a p a y 1 h δ y c g r i d c o p e r y a c r e p y a c s a l y a where all costs occurring at each hour t throughout the simulation horizon are aggregated on a yearly basis y h is the number of years of the simulation horizon c c a p a are the capital costs c g r i d are the costs for the management of the electricity grid and c o p e r y a c r e p y a c s a l y a are the operational replacement and salvage costs at year y respectively it is worth noting that problem 2 is the classical formulation of the optimization problem that does not account for the effects of the uncertainty in the external drivers on the objective function see section 3 2 for the formulation of the robust optimization problem for each of the robustness metrics considered all costs except the capital ones are discounted using the following time varying coefficient 4 δ y 1 1 γ y where γ is the real discount rate calculated as a function of the nominal discount rate γ and the inflation rate φ 5 γ γ φ 1 φ the capital costs occur at the beginning of the simulation horizon and represent the investment to install the power technologies the replacement costs occur when a technology has to be substituted and the salvage costs are negative costs which are incurred at the end of the simulation horizon when one or more technologies have not reached the end of their lifetime finally the operational costs take into account both the cost of maintenance of each power technology and the cost of fuel net present cost is dependent on both the configuration of the hybrid energy system a and the climatic conditions i e wind speed solar radiation temperature which significantly affect the res power potential and consequently the electricity generation costs in particular we assume an electricity generation cost associated with res equal to zero the decision variables a of problem 2 are the pv capacity c p v and the number of wind turbines n w to be installed defined within the following feasibility sets c p v 100 2000 and n w 1 20 the upper bound of the feasibility sets is determined considering the small size of the island and the tight environmental constraints which strictly limit the maximum installable res capacity in this work we do not consider the number of batteries as a decision variable but we fix the capacity of the storage system to be equal to the pv capacity even if these simplifications potentially prevent the identification of the exact optimal number of batteries to be installed it constitutes a reasonable and conservative assumption i e empty batteries could store the hourly maximum pv power production this is because it ensures that the pv electricity surplus generated in the central hours of the day is stored in the batteries to enable the required load to be met during the night in addition it allows the size of the decision space to be reduced significantly by not considering bad or even infeasible solutions i e high number of batteries and low pv capacity we solve problem 2 using an exhaustive search within the feasibility set of the decision variables over a 1 year evaluation period assuming that external drivers do not change from one year to another during the simulation horizon to do this we sample the feasibility set of c p v with a 100 kw discretization step which is sufficiently fine to allow significant changes in the objective function to be captured giudici et al 2019 using this discretization the cardinality of the search space is equal to 400 the optimization is performed using an hourly simulation time step and the technical and economic parameters reported in supplementary material 3 2 experiment settings scenarios in this work we generate climate scenarios of wind speed solar radiation and temperature using an hybrid approach matrosov et al 2013 roach et al 2016 which combines top down haasnoot et al 2013 giuliani and castelletti 2016 and bottom up culley et al 2016 bertoni et al 2019 methods by first estimating future conditions from climate models and then enlarging the range of plausible future scenarios in order to stress test the system of interest we first consider climate projections generated by five different euro cordex scenarios see www euro cordex net defined as combinations of representative concentration pathways rcps ipcc 2014 global circulation models gcms and regional circulation models rcms table 1 these scenarios have a spatial resolution of 0 11 degrees and provide projections for the period 2006 2100 to resolve the mismatch between the spatial resolution of rcms and that of our study site we apply a statistical downscaling method based on quantile mapping boe et al 2007 estimating a correction function between the observations of the climate variables at the local scale and the rcm output over the control period 1971 2005 since our model simulates the system over a reference year we consider each projected year as a single scenario then we enlarge the mean annual variability of wind speed and solar radiation for stress testing our system under more variable conditions in order to identify potential vulnerabilities in particular we enlarge the variability of solar radiation by 2 and the variability of wind speed by 10 these values have been arbitrarily selected to capture more extreme conditions than the ones predicted by the euro cordex climate projections but which are still feasible and reliable for our specific case study we focus on these two variables as the most uncertain and the ones that are most likely to significantly affect planning decisions as solar radiation and wind speed are the main drivers determining pv and wind power potential for a given installed capacity at the end of the scenario generation procedure we obtain 3125 scenarios of hourly values of wind speed solar radiation and temperature composing the scenario set f considered in ross this scenario set is shown in fig 3 where each point represents the mean annual value of each climate variable according to a specific scenario wind speed is clearly the driver with the largest variability with mean annual values that range between 3 4 and 7 m s solar radiation ranges between 253 8 and 272 7 w m 2 and temperature values vary from 18 2 to 24 5 c robustness metrics the robustness metrics we consider include minimax this metric identifies the alternative a that attains the best performance in the worst case 6 a a r g min a max ξ j a w this metric usually associated with a pessimistic point of view selects the alternative assuming that the worst future conditions will be realized wald 1950 minimin this metric identifies the alternative a that attains the best performance in the best case 7 a a r g min a min ξ j a w this metric usually associated with an optimistic point of view selects the alternative assuming that the best future conditions will be realized wald 1950 hurwicz this metric called the optimism pessimism rule combines the first two metrics identifying the alternative a that attains the best weighted sum of the performance obtained in the worst case and in the best case 8 a a r g min a α max ξ j a w 1 α min ξ j a w where 0 α 1 is a weight that specifies the relative importance associated with the realization of the worst or best future condition depending on the choice of the weight α which is related to the level of risk aversion of the decision maker this metric can be associated with a more or less pessimistic or optimistic point of view in this work we consider α 0 66 hurwicz 1953 laplace this metric called the principle of insufficient reason selects the alternative a that attains the best expected performance over the n future scenarios 9 a a r g min a 1 n i 1 n j a w i this metric suggests risk neutrality of the decision maker and implicitly assumes that each future scenario could be realized with the same probability laplace 1951 mean variance this metric tries to balance the expected performance and its variability over future scenarios identifying the alternative a such that 10 a a r g min a 1 n i 1 n j a w i v a r ξ j a w this metric is based on the assumption that an optimal robust solution is one whose performance is not very sensitive to which plausible future will be realized hamarat et al 2014 however the main disadvantages of this metric are that it is not always monotonically increasing ray et al 2014 and positive and negative deviations from the mean are treated equally takriti and ahmed 2004 ross parameterization we implement ross see section 2 1 for our case study using the following parameters m 20 n e x p 100 p 0 1 m represents the maximum number of al iterations n e x p the number of scenario samplings from the gaussian distribution d at each al iteration and p a fraction of n e x p identifying the best scenarios used to update the gaussian distribution d these parameters have been set taking into account the specific characteristics of our case study and indications reported at the end of section 2 1 in contrast the parameter n 0 which identifies the cardinality of the scenario subset is set according to the robustness metric selected it is worth noting that for the minimax minimin and hurwicz metrics the cardinality n 0 defines exactly the minimum number of scenarios to search for following their intrinsic formulation in particular for the minimax and minimin metrics n 0 is equal to 1 as both require system performance to be evaluated for the worst and the best scenario respectively for the hurwicz metric n 0 is equal to 2 as it requires system performance to be evaluated for a weighted sum of the worst and the best scenario for the laplace and mean variance metrics n 0 is set equal to 2 as this represents the minimum number of scenarios needed to mathematically compute the metrics even though in theory this value of n 0 could be too small for identifying a robust solution and would therefore have to be increased iteratively until such a robust solution is found see section 2 this is not the case of our specific case study finally the parameter k which represents the dimension of the scenario set f is equal to 3 since we consider three different climate drivers 3 3 computational experiments ross is run for each of the five robustness metrics considered using the settings defined in section 3 2 first we assess the ability of ross to identify the most informative sub regions of the scenario set by analysing the evolution of the r score and the gaussian distribution d throughout the m al iterations section 4 1 at each al iteration ross calculates n e x p different r scores corresponding to the n e x p scenario subset extractions from the same gaussian distribution d since the best and the worst scores are strongly affected by outliers we assume that ross is at convergence as soon as the 0 1 quantile of the scores i e value under which only 10 of the scores lies reaches zero see section 2 1 secondly in order to identify the key drivers that mostly influence system robustness we explore how the most informative regions of the scenario set and the resulting robust solution change according to the robustness metric selected by comparing the different gaussian distributions at convergence section 4 2 finally we assess the advantages of using ross by comparing it with a traditional robust optimization method i e brute force method which evaluates all candidate solutions over the entire scenario set and a state of the art scenario selection method i e random sampling method trindade et al 2017 which randomly extracts a small scenario subset to be included in the optimization process section 4 3 since the brute force method always guarantees the identification of the most robust solution we perform the comparison by focusing on the computational requirements calculated as the number of system simulations needed to obtain a robust solution in particular the computational requirements of the brute force method are calculated multiplying the number of scenarios included in the robust optimization process i e 3125 by the number of candidate solutions in the search space namely 400 see section 3 1 resulting in a total of 1 250 000 simulations with ross this number depends on the metric considered and is calculated by multiplying the number of al iterations needed to reach the convergence by the subset cardinality n 0 and the number of extraction n e x p at each al iteration ross solves n e x p robust optimization problems considering n scenarios it is worth noting that we do not consider the simulations needed to re evaluate the solutions over the entire scenario set within the calculation of ross computational requirements as we assume to know a priori the target performance resulted from that re evaluation see section 2 1 for further details in order to compare ross with the random sampling method we perform two different experiments focusing on the minimax metric and evaluating the results in terms of both computational requirements and r score in the first experiment we perform a robust optimization randomly extracting the same number of scenarios evaluated by ross for reaching the convergence and we compute the r score as percentage difference between the robustness calculated over that scenarios and the one obtained considering the entire scenario set since the extraction is performed randomly we repeat the robust optimization 100 times in order to reliably infer the probability of identifying the robust solution and compute the 0 1 quantile of the r scores used to assess ross convergence in the second experiment we perform the comparison by fixing the computational requirements to the ones needed by ross for reaching the convergence in this case we performed 10 robust optimizations assumed to be sufficient to filter the uncertainty in the random sampling by including a number of randomly extracted scenarios equal to the number of scenarios evaluated by ross divided by 10 the computational experiments are performed on a personal computer with a 2 5 ghz intel core i5 3210m processor with 2 cores and 8 gb system ram for each system simulation the model takes about 0 05 s for a total number of computational hours ranging from 2 77 to 13 33 depending on the robustness metric considered 4 numerical results 4 1 ability to identify the most informative sub regions of the scenario set numerical results show that best and worst scores as well as the 0 1 quantile of the n e x p scores rapidly increase with the number of al iterations leading ross to reach convergence in a small number of al iterations 12 maximum for all the robustness metrics considered fig 4 this means that ross effectively moves towards the regions of the scenario set f containing the most informative scenarios for generating a robust solution in particular convergence is reached earlier for the minimax and minimin metrics fig 4a b which are characterized by a subset cardinality of n 0 1 and consequently a gaussian distribution with n k 3 dimensions only the other metrics fig 4c e are characterized by a subset cardinality of n 0 2 and thus a gaussian distribution with n k 6 dimensions due to the higher dimensions of the gaussian distribution convergence is reached later in these cases another interesting aspect related to algorithm convergence concerns the value the worst score grey lines in fig 4 assumes in the first al iterations as can be seen this is strongly dependent on the metric considered and indicates the degree to which the robustness of the optimal solution is sensitive to the scenario subset considered for example if we consider the laplace metric fig 4d the worst score at the first al iteration is about 5 meaning that independently from the scenarios extracted the robustness of the optimal solution computed over that scenarios does not differ too much from the one computed over the entire scenario set on the contrary for the mean variance metric fig 4e the worst score at the first al iteration is about 230 in this case the difference between the robustness of the optimal solution calculated for the scenario subset and the one computed for the entire scenario set is highly sensitive to the scenarios extracted making the identification of the most informative regions of the scenario set more challenging this different behaviour is related to both the system dynamics of our specific case study and the intrinsic formulation of the robustness metric in particular the mean variance metric has a more complex formulation compared with the laplace one as it evaluates the robustness of a solution considering a combination of the average performance and the performance dispersion over the entire scenario set making it difficult to identify a smaller number of scenarios able to generate a robustness value that is similar to the one obtained considering the entire scenario set the rapid increase in r score towards convergence is driven by the evolution of the multivariate gaussian distribution d according to which the scenarios to be included in the optimization process are extracted results show that the gaussian distribution rapidly shrinks in the most informative regions of the scenario set the mean identifies the most interesting regions and the variance tends to decrease throughout the al iterations fig 5 in particular fig 5a refers to the minimax metric and fig 5b to the hurwicz metric see supplementary material for the other metrics for illustration purposes we show the gaussian distribution projected in the space of wind speed and solar radiation the two drivers that mostly influence system performance each grey point represents a plausible scenario and each black circle represents two dimensions of the multivariate gaussian distribution corresponding to the two variables i e wind speed and solar radiation characterizing a scenario with the centre representing the mean and the radius three times the standard deviation i e 99 7 of extraction probability is included in the circle the upper and bottom panels refer to the first al iteration and the al iteration at convergence respectively the middle panels show an intermediate al iteration it is worth noting that for the minimax metric there is only one circle evolving towards convergence as the cardinality of the scenario subset is equal to 1 fig 5a for the hurwicz metric there are two circles as the cardinality of the scenario subset is equal to 2 we can observe that at the first al iteration the gaussian distribution is initialized covering the entire scenario set so that there is no need to precondition the algorithm to evolve towards particular regions of the scenario set for the same reason the two circles of the hurwicz metric completely overlap it is interesting to note that the variance of the gaussian distribution at convergence for the minimax metric is higher compared with that of the hurwicz metric this is mainly due to the fact that in this case the distribution moves outside the scenario set where no scenarios exist as a consequence irrespective of which point is extracted from that distribution the nearest neighbour search see section 2 1 always identifies the same small number of scenarios included in the circle representing the distribution 4 2 ability to identify most informative scenarios and robust solutions for different robustness metrics in order to explore how the most informative regions of the scenario set change according to the robustness metric selected we show the gaussian distribution at convergence projected in the space of wind speed and solar radiation for the five metrics considered fig 6 the gaussian distribution circles and the corresponding most likely extracted points are represented with different colours according to the robustness metric we can observe that the distribution associated with the minimax metric purple circle which aims at selecting the best solution in the worst case identifies a region characterized by low wind speed and low solar radiation in our case these conditions represent the worst possible conditions as they significantly reduce the renewable power potential and increase the cost for electricity production the robust solution can be thus determined by evaluating the system performance over one of the scenarios included in the purple circle not surprisingly the distribution associated with the minimin metric green circle which focuses on determining the best solution in the best case identifies a region characterized by high wind speed and high solar radiation these conditions allow the highest renewable power potential to be attained and consequently lower cost for electricity production the most informative regions associated with the laplace metric yellow circles include scenarios with average values of wind speed and solar radiation coherent with the metric definition this result shows that the average system performance evaluated over the entire scenario set is almost equal to that evaluated over a small scenario subset characterized by average future conditions this behaviour suggests a direct correlation between the values of wind speed and solar radiation and system performance in terms of net present cost if we consider the hurwicz metric the solution that performs better over a weighted combination of worst and best case can be obtained by extracting scenarios from two regions red circles that present average values of solar radiation and average and low values of wind speed one of these regions almost overlaps the one identified by the laplace metric the scenarios that are most likely to be extracted by both laplace and hurwicz metrics are represented as orange points the other region is instead located between minimax and laplace as the highest weight i e 0 66 is associated with the worst case see section 3 2 we can observe that in this case the two scenarios to be included in the optimization process are extracted from the two regions of the scenario set that present different values of wind speed but almost the same values of solar radiation this suggests that wind speed is the driver that most influences system performance for a given res installed capacity it is worth noting that even if the hurwicz metric computes a weighted some of worst and best case performance the most informative regions do not converge to the ones obtained by the minimax and minimin metrics this is mainly due to the fact that multiple scenario pairs that allow to obtain the same weighted sum as the one calculated on the worst and the best scenarios might exist since the algorithm evolution stops when the difference between the weighted sum calculated considering the extracted scenarios equals the one obtained considering the worst and the best scenarios over the entire scenario set the regions identified at convergence could differ from the two obtained considering the minimax and minimin metrics however the obtained regions can be in any case considered as equally informative as the two extreme regions finally the regions identified for the mean variance metric blue circles which aims to select the solution that performs better on average and at the same time has a low variance if re evaluated over the entire scenario set present medium low values of solar radiation and average values of wind speed in particular unlike the laplace metric which focuses on average performance only in this case the two regions are further apart in order to capture the variability of wind speed which is the driver that most affects the system performance of a given solution in conclusion the robust solutions obtained for the different robustness metrics are reported in table 2 in terms of pv capacity c p v and number of wind turbines n w as expected the minimax and mean variance metrics result in the installation of the minimum number i e 1 of wind turbines the first metric i e minimax focuses on the worst scenario which is characterized by very low values of wind speed as a consequence installing a higher number of wind turbines would cause an increase in the capital costs without having any benefit associated with higher renewable energy production the second metric i e mean variance focuses on minimizing the variability of the solution performance when re evaluated over the entire scenario set in this case the variability in renewable power generation due to the high variability in wind speed would increase with the number of wind turbines installed leading to highly variable solution performance conversely the minimin metric results in the installation of the maximum number i e 20 of wind turbines in order to fully exploit the high values of wind speed associated with the best scenario the hurwicz and laplace metrics result in the selection of an intermediate number of wind turbines 10 for hurwicz and 12 for laplace in particular the solution associated with the hurwicz metric is characterized by a slightly lower number of wind turbines as it assigns a higher weight to the worst scenario in contrast the robust solution in terms of pv capacity is selected in a completely different way the minimax and mean variance metrics result in the installation of the maximum allowable pv capacity i e 2000 kw whereas the minimin metric results in the installation of a very low pv capacity i e 900 kw both the hurwicz and laplace metrics result in the installation of a medium high pv capacity namely 1700 kw and 1600 kw respectively this is mainly due to the relative low marginal cost of pv with respect to that of the wind turbines and the very small variability of solar radiation when compared with that of wind speed 4 3 benchmarking with brute force and random sampling methods we compare ross with the brute force method by assessing the ross computational efficiency gain calculated as percentage reduction of system simulations with respect to the brute force method for the robustness metrics considered comparison results show that the computational efficiency gain obtained using ross ranges from 23 to 84 depending on the complexity of the robustness metric and the cardinality n 0 of the scenario subset this result suggests that in all cases ross succeeds in considerably reducing the computational requirements of solving robust optimization problems by selecting the most informative scenarios to be considered within the optimization process table 3 shows the computational requirements see section 3 3 for details on the calculations for each metric considered and the corresponding computational efficiency gain calculated as the percentage reduction in required system simulations with respect to the brute force method the total computational time of ross ranges from 2 77 h for the minimin metric 84 of reduction to 13 33 h for the mean variance metric 23 of reduction compared with approximately 17 h for the brute force method such computational efficiency gains become more significant when considering applications on more complex systems characterized by a higher number of scenarios and where simulation based optimization techniques are adopted to find the robust solutions such as the growing number of studies relying on evolutionary algorithms to solve robust optimization problems trindade et al 2017 gold et al 2019 trindade et al 2019 in order to compare ross with the random sampling method we first perform 100 runs of the random sampling method extracting the same number of scenarios evaluated by ross for reaching the convergence i e 700 for the minimax metric comparison results show that random sampling method identifies the most robust solution r score equal to 0 with a probability of 24 compared to the 90 probability of ross in addition the 0 1 quantile of the r scores computed in the 100 runs of the random sampling method is equal to 0 3 compared to the 0 of ross at convergence 7th iteration for the minimax metric it is worth noting that even if the difference between random sampling and ross in terms of 0 1 quantile of the r scores is very small random sampling reaches this performance with a 100 times higher computational time with respect to ross i e 100 robust optimizations performed by including 700 scenarios each moreover if a 0 1 quantile of the r score equal to 0 3 can be considered acceptable ross convergence would be reached at the 4th iteration further reducing its computational requirements only 400 scenarios should be evaluated secondly we fix the computational requirements to the ones needed by ross for reaching the convergence i e 700 system simulation for the minimax metric and we perform 10 runs of the random sampling method extracting a number of scenarios equal to the number of scenarios evaluated by ross divided by 10 i e 70 scenarios results show a strong degradation of the performance of random sampling method which never identifies the most robust solution r score never reaches 0 and obtains a 0 1 quantile of the r scores equal to 3 29 in summary despite random sampling method allows generating robust solutions by including a small scenario subset within the optimization phase multiple runs of the algorithm are needed to assess its reliability in identifying a robust solution thus leading to higher computational requirements compared to ross for the same level of robustness conversely if the two algorithms are compared with exactly the same computational effort the performance of the random sampling method significantly degrades 5 conclusions in this paper we propose ross robust optimal scenario selection a novel algorithm to identify the smallest scenario subset to be included in the optimal generation of robust solutions under deep uncertainty once the set of deeply uncertain scenarios the robustness metric and the cardinality of the scenario subset have been defined ross uses an active learning algorithm to adaptively identify the regions of the scenario set that include scenarios for which the robustness of the optimized solution is similar or even equal to that obtained by considering the entire scenario set first we assess the ability of ross to iteratively identify the regions of the scenario set that contain the most informative scenarios for generating robust solutions secondly we explore how the most informative regions of the scenario set and the resulted robust solution change according to the robustness metrics selected highlighting the relative influence of different uncertain drivers on the robustness of the most robust solutions finally we evaluate the advantages of using ross by comparing it with a traditional robust optimization method i e brute force method which considers the entire scenario set within the optimization process and a state of the art scenario selection method i e random sampling method trindade et al 2017 that randomly extracts a small scenario subset to be included into the optimization process we tested ross on the robust planning of a hybrid energy system on the ustica island italy the problem consists in the identification of the least cost photovoltaic pv and wind capacity under deep uncertainty in the main climatic variables affecting the system namely wind speed solar radiation and temperature results show that ross rapidly reaches convergence maximum 12 iterations for the more complex mean variance metric by identifying the regions of the scenario set containing the most informative scenarios for generating a robust solution this results in a computational efficiency gain varying from 23 mean variance metric to 84 minimin metric compared with brute force method which is achieved by solving multiple optimization problems considering small scenario subsets it is worth noting that these significant computational efficiency gains are obtained considering the minimum subset cardinality needed to compute the robustness metrics however as already mentioned in section 2 for complex dynamic systems affected by a large set of co varying uncertain factors this subset cardinality could be too small for identifying a robust solution in this case the algorithm has to relaunched by iteratively increasing the subset cardinality until a robust solution is found leading to an increase in computational requirements and consequently a potential decrease in the computational efficiency gain comparison with random sampling method shows that despite this latter allows generating robust solutions by including a small scenario subset within the optimization phase multiple runs of the algorithm are needed to assess its reliability in identifying a robust solution thus leading to higher computational requirements compared to ross for the same level of robustness conversely if the two algorithms are compared with exactly the same computational effort the performance of the random sampling method significantly degrades the regions identified at the end of the active learning procedure highlight the deeply uncertain future conditions that mostly influence the system robustness depending on the robustness metric considered these regions are characterized by different values of the uncertain drivers for example the minimax metric which aims at selecting the best solution in the worst case identified a region characterized by low wind speed and low solar radiation as these conditions reduce the renewable power potential while increasing the cost for electricity production conversely the minimin metric which focuses on determining the best solution in the best case identified a region characterized by high wind speed and high solar radiation the other metrics identified regions in between the extreme ones highlighted by the minimax and minimin metrics showing that wind speed represents the deeply uncertain driver that most significantly affects system performance and the robustness of the solution by abstracting from this specific case study ross has the ability to significantly improve existing state of the art robust optimization methods with respect to two important aspects first ross allows the high computational requirements associated with the inclusion of robustness within the optimization process to be reduced significantly by selecting a small scenario subset as the most informative for generating robust solutions second ross identifies the key uncertain drivers to which system performance is most sensitive as well as the values of the drives over which this occurs in an adaptive manner using active learning these improvements can lead to significant advantages especially when complex dynamic systems where decisions are affected by a large ensemble of deeply uncertain co varying factors are involved in these cases system non linearity as well as unpredictable behaviours pose great challenges in predicting how the system would perform in response to uncertain changing conditions and consequently in identifying system vulnerabilities and robust solutions one of the main limitations of the current version of ross is that in contrast to much of the state of the art works that consider system robustness with respect to multiple objectives it allows to solve robust optimization problems considering a single objective only however this limitation could be partially overcome by iteratively solving a robust multi objective optimization problem and computing the r score as the hypervolume between the obtained pareto front and a target pareto front reflecting the target performance to be ideally achieved in this case the algorithm would try to identify that regions of the scenario set that are the most informative for generating a pareto front that is as close as possible to the target one another ross limitation concerns its inability to guarantee that the identified solution is robust also with respect to other uncertainties or scenarios outside the priors considered in the scenario set however these uncertainties if known could be included within the scenario set and ross ability of identifying the most informative scenarios would be even more exploited for generating a robust solution further reducing computational time with respect to traditional robust optimization methods further research efforts will focus on testing ross for i solving robust joint planning and management optimization problems where the optimal system design strictly depends on the adopted control strategy and vice versa and ii analysing more complex systems characterized by a large set of climate socioeconomic and technological uncertain and potentially interdependent drivers in this latter case when dealing with continuous uncertainties e g scenarios of continuous variables such as temperature wind speed electricity demand as in our case study a multi variate gaussian distribution can be employed to filter the multi dimensional scenario set the dimensions correspond to the number of uncertainties to be considered when instead the uncertainties to be considered are categorical e g system parameters model structures a categorical distribution e g bernoulli for 2 categories could be adopted to assign a probability to each category this categorical distribution evolves throughout the active learning iterations by concentrating the highest probability on those categories that allow to obtain a robust solution declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements federico giudici has been financed by the research fund for the italian electrical system in compliance with the decree of april 16 2018 appendix a supplementary data supplementary material related to this article can be found online at https doi org 10 1016 j envsoft 2020 104681 appendix a supplementary data the following is the supplementary material related to this article mmc s1 
