index,text
550,this work presents a new lattice boltzmann model for steady and unsteady two dimensional shallow water flows compared to previous lattice boltzmann based shallow water models the proposed method uses a consistent characteristic speed in the pressure term and in the viscosity to preserve the isotropy of viscosity the relaxation rates for the different cumulants have to be decoupled this is only possible by using multiple relaxation rates the recovery of the correct viscosity is investigated by a convergence study based on the decay of a taylor green vortex results from shallow water models using the proposed collision operator are then compared to those derived from standard bgk approach and from a continuous model keywords shallow water collision operator cumulant convergence abbreviations lbm lattice boltzmann method swe shallow water equations lb sw lattice boltzmann shallow water co collision operator srt single relaxation time mrt multi relaxation time bgk bhatnagar gross krook cumlb model cumulant lattice boltzmann model bgk sw model bgk shallow water model pdf particle distribution function d2q9 two dimensional lattice pattern with 9 speed directions 1 introduction the lattice boltzmann method lbm is a mesoscopic method closely related to kinetic theory of gases it is a versatile method and it has been extensively applied in different fields such as turbulent flow krafczyk et al 2003 far et al 2016 multiphase falcucci et al 2010b 2010a flow in complex geometries ubertini et al 2003 di francesco et al 2013 in porous media yang et al 2016 and thermal flows zarghami et al 2014 however it is not so common to use the lbm approach to simulate large scale hydraulic problems such as flooding events di francesco et al 2016 dam breaks biscarini et al 2016 and propagation of tsunamis the lbm applies a stream and collide algorithm in which particles move on a cartesian lattice and collide at lattice nodes once an appropriate lattice or velocity set has been chosen the physics have to be implemented in the collision operator co which is the central part of the model a widespread solution is based on a single relaxation time srt approach bgk method qian et al 1992 characterized by a relatively simple implementation the particle distribution relaxes towards an equilibrium function with a rate chosen to match the viscosity of the modeled fluid in order to maximize the number of adjustable parameters and increase both stability and accuracy some authors suggest to use a multiple relaxation times mrt co d humières 1994 even though it was demonstrated that the mrt co improves the stability range of the lbm it introduces an additional galilean invariance violation and hyper viscosity problems when compared to the bgk co geier et al 2015 the first problem is due to the definition of the moments in fact the mrt method translates the pdf into a set of raw moments geier et al 2015 using a linear transformation each moment is orthogonal to all the others and relaxes with its own rate however the orthogonality of the moments is not reference frame independent krafczyk et al 2015 and orthogonality is not maintained if the reference frame is changed therefore the original mrt method further violates the galilean invariance compared to a bgk srt model by applying the collision in terms of cumulants instead of raw moments the cumulant lbm geier 2006 overcomes the violation of galilean invariance removing also the hyper viscosity that can be generated using for example central moments in the collision operator geier et al 2015 thus in this work we adopt the cumulant collision operator for the simulation of the shallow water equations in order to apply the lbm kinetic approach to large scale hydraulic problems and to increase stability and accuracy in the solution of the fluid dynamic problem the shallow water equations swe are obtained from the navier stokes equations by integrating over the water column from the ground to the free surface and assuming that the pressure along the vertical can be replaced by the hydrostatic pressure they have been extensively applied to the simulation of large scale hydraulic problems using standard numerical methods biscarini et al 2010 the swe were first studied with the lbm by salmon 1999 and dellar 2002 while zhou provided a comprehensive revision of the method zhou 2004 thought some works presenting simulations of large scale problems thommes et al 2007 shafiai 2011 prestininzi et al 2013 are available in literature the issue of the effective applicability of a shallow water lattice boltzmann model to complex hydraulic problems remains substantially open in term of efficiency stability and accuracy most of the applications of the lbm for shallow water flows are based on the standard bgk approach bhatnagar et al 1954 and only a small number of authors peng 2012 liu 2009 de rosis 2017 applied multiple relaxation time collision operators to the problem the objective of this work is to test the shallow water model using a non conventional mrt collision operator and compare it to the standard bgk approach in this work a new model based on cumulants is originally described and its principal and innovative features and the main theoretical differences to the standard bgk are underlined in order to retain the characteristics of the isotropy of the model the equilibrium cumulants and corresponding relaxation rates are here defined and mathematically derived taking into account the dependence of the characteristic speed on the water depth the paper is organized as follows section 2 is a short introduction to the governing shallow water equations and lattice boltzmann shallow water models in particular section 2 1 provides information on the lb models solving shallow water equations with the description of the different cos and section 2 3 gives an extensive description of the cumulant model and its implementation procedure numerical results and model performance are presented in section 3 in particular a comparison between the bgk model and the cumulant model is performed and the main differences are investigated especially in section 3 4 and section 3 5 the test case of the stoker dam break and the fennema chaudhry dam break are used to show the different stability characteristics of the bgk and the innovative approach finally section 4 presents conclusions 2 methods the swe are derived from the three dimensional incompressible navier stokes equations they are valid for problems in which the vertical dynamics of the fluid can be neglected toro 2001 in particular the swe are derived using an integration over the depth in order to obtain vertically averaged quantities the pressure distribution in the vertical direction z p z is supposed to be hydrostatic 1 p z ρ g where ρ is the fluid water density and g is the gravity acceleration the superscript indicates physical variables to distinguish them from variables in lattice units l u a system of 2d shallow water equations as in zhou 2004 can be written in the following form 2 h t h u j x j 0 3 u j 1 h z b z b h u d z 4 h u i t h u i u j x j g x i h 2 2 ν 2 h u i x j x j f i where i and j indicate the coordinate axis direction in 2d space h is the water depth u is the velocity ν is the kinematic viscosity zb is the bed elevation f i is the external force in the i direction the external force term can be written as follows 5 f i f p i f s i f w i f c i where f p i is the force due to gravity and is equal to g h z b x i f s i is the bed shear stress defined as 6 f s i c b u i u i u i where c b g n f 2 h 1 3 represents the friction factor with manning s coefficient n f at the seabed the terms f w i and f c i are respectively the wind shear stress and the force representing the coriolis effect in this work shallow water flows are modeled using a mesoscopic approach for the tracking of free surface dynamics flow characteristics are described by the evolution of a particle distribution function f x t pdf on a regularly spaced domain lattice pattern using the discrete lattice boltzmann equation 7 f α x e a δ t t δ t f α x t ω α f α α 1 m where x is the position of the particle in the discretized space at time t fα x t and eα are the particle distribution functions and the set of discrete speeds along the m allowed lattice directions the key steps in lbm are streaming and collision in eq 7 f α x e a δ t t δ t f α x t represents the streaming process and ω α is the collision operator fα represents the external forces the lattice pattern dnqm dn stands for n dimensions while qm stands for m speeds in lbm has the two functions to represent the points of the grid and to determine the particle directions of the motion in the present work we adopt the d2q9 pattern with the following set of velocities e α α 0 8 wolf gladrow 2004 e 0 0 0 e 1 1 0 e 2 0 1 e 3 1 0 e 4 0 1 e 5 1 1 e 6 1 1 e 7 1 1 e 8 1 1 in lbm the macroscopic properties of the fluid can be expressed using the raw moments of the distribution the generic raw moment mαβ can be expressed as 8 m α β i j i α j β f i j i j 1 0 1 where the sum of α and β indices represents the moment order the indices i and j represent the miller indices and following a method primarily used in crystallography cambridge university 2008 allow to express speeds directions 2 1 lattice boltzmann shallow water model in isothermal lattice boltzmann models the characteristic speed is a parameter set to a constant that maximizes isotropy the characteristic speed is usually identified with the speed of sound for the d2q9 lattice the speed of sound is equal to c s 1 3 lattice units this fixed characteristic speed is an important simplification of the lattice boltzmann method as it eliminates the need to fully resolve the energy flux tensor in velocity space geier and pasquali 2018 to understand this point we have to recall that the momentum distribution gives rise to an infinite hierarchy of moment equations due to the limited number of lattice velocities only very few of these moment equations are independent of each other on the d2q9 lattice the moments m 30 and m 03 are not independently adjustable quantities physically they are part of the heat flux tensor the trick applied in the lattice boltzmann method to avoid a larger set of discrete velocities in which these moments were independent is to chose a specific temperature and hence characteristic speed for which the moments m 30 and m 03 recover their correct physical value this is only the case for c s 1 3 which explains why this constant characteristic speed is chosen a detailed mathematical derivation is found in appendix h of geier et al 2015 in the swe the speed of surface waves takes the place of the speed of sound characteristic speed in the original lbm in fact in lattice boltzmann shallow water lb sw models the characteristic speed cs is not constant but a function of the fluid elevation h and the gravity acceleration g 9 c s 2 g h 2 this is a direct consequence of the equation of state p 1 2 g h 2 dellar 2002 where p indicates the macroscopic value of the pressure this poses a problem to the applicability of the lbm to the swe as the characteristic speed is no longer constant the errors in the discretization of the third order moments do no longer cancel automatically geier and pasquali 2018 this is important as the characteristic speed influences the viscosity in fact the kinematic viscosity transport coefficient of the fluid ν is linked to the relaxation rate ω and to the characteristic speed 10 ν c s 2 1 ω 1 2 the macroscopic properties water depth h and velocity field ui of the flow are computed from the raw moments m 00 m 10 m 01 respectively 11 h α 0 8 f α u i 1 h α 0 8 e α i f α α 0 8 during the collision update rules are applied at each node depending only on the state of the pdf on the node the collision has to conserve mass and momentum such that m 00 m 10 and m 01 are quantities that do not change during this step 2 1 1 from lattice units to physical units in lb simulations the grid spacing δx and the elementary lattice time step δt are linked through the physical value of the characteristic speed c s c s δ x δ t then δ t c s c s δ x s as reported in wolf gladrow 2004 the characteristic speed represents the velocity at which the waves travel recalling that in swe lbm the characteristic speed is defined as in equation 9 it follows that the value of the lattice time step δt in seconds s is given by δ t g h 2 g h 2 δ x where g and g are the value of the gravity acceleration in lattice and physical units respectively in the practical implementation h can be maintained equal to h then the expression of the lattice time step δt becomes 12 δ t g g δ x s the froude number fr of the physical model matches the lattice one 13 f r v g h v δ x δ t g δ x δ t 2 h δ x v δ x δ t δ x δ t g h f r 2 2 single relaxation time models srt models solving the shallow water equations are generally based on a bgk co salmon 1999 zhou 2004 assuming that collision is the process that returns particles to the state of the local maxwellian equilibrium 14 ω α f α f α e q τ where ω α is the collision operator and τ is the collision mean free time authors dealing with the solution of the shallow water equations using the bgk approach geveler et al 2011 frandsen 2008 suggest to express the equilibrium pdf as a power series in macroscopic velocities up to second order assuring mass and momentum conservation 15 f α e q h 5 g h 2 6 2 h 3 u u α 0 β h g h 6 1 3 e α u 1 2 e α u 2 1 6 u u α 1 8 where u u v with u and v that represent the velocity components β assumes the value 1 if α 1 4 and 1 4 otherwise de rosis 2017 in the d2q9 model the relation 8 implies there are only nine independent moments and in particular m 30 m 10 in the srt shallow water model bgk sw the characteristic speed is decoupled from the third order moment m 30 the model uses a c s 2 different from g h 2 in the diffusion process this can be evinced by observing the equilibrium equations used the part related to the viscosity appears equal to the one adopted in a standard two dimensional lb model while the part related to the density is different and takes into account the dependency of the characteristic speed c s 2 on the depth of the water h eq 15 one of the properties of these models is therefore the decoupling of the viscosity from the characteristic speed to assure isotropy in the next paragraph we propose and derive the cumulant collision operator where the physical link between the characteristic speed and viscosity is maintained and the isotropy of the viscosity is restored by applying different relaxation rates to different second order cumulants 2 3 cumulant model while the original mrt model performs collision on moments the cumulant model cumlb performs the collision on cumulants cumulants quantify the deviation of a distribution from a gaussian distribution by design cumulants are statistically independent of each other cumulants are conveniently obtained from central moments defined as 16 κ α β i j i u α j v β f i j i j 1 0 1 where the subscripts i and j indicate the corresponding components of the speed vectors of the pdf for the d2q9 pattern used in this work all but one of the available cumulants are identical to the corresponding central moments in particular 17 c 00 κ 00 c 10 κ 10 c 01 κ 01 c 20 κ 20 c 02 κ 02 c 11 κ 11 c 12 κ 12 c 21 κ 21 starting from fourth order cumulants differ from central moments geier et al 2015 the d2q9 pattern used in this work has only one independent fourth order cumulant 18 c 22 κ 22 κ 20 κ 02 2 κ 11 2 h then equilibrium cumulants are identical to equilibrium central moments geier 2006 except for the fourth order cumulant that is zero the equilibrium cumulants for a d2q9 scheme are given by 19 c 00 h c 10 0 c 01 0 c 20 c s 2 h c 02 c s 2 h c 11 0 c 12 0 c 21 0 c 22 0 the cumlb method is implemented by transforming the distributions to cumulants before the collision using the following equations 20 κ 00 f 7 f 3 f 6 f 4 f 0 f 2 f 8 f 1 f 5 κ 10 0 κ 01 0 κ 20 1 u 2 f 7 1 u 2 f 3 1 u 2 f 6 u 2 f 4 u 2 f 0 u 2 f 2 1 u 2 f 8 1 u 2 f 1 1 u 2 f 5 κ 02 1 v 2 f 7 v 2 f 3 1 v 2 f 6 1 v 2 f 4 v 2 f 0 1 v 2 f 2 1 v 2 f 8 v 2 f 1 1 v 2 f 5 κ 11 1 u 1 v f 7 1 u v f 3 1 u 1 v f 6 u 1 v f 4 u v f 0 u 1 v f 2 1 u 1 v f 8 1 u v f 1 1 u 1 v f 5 κ 21 1 u 2 1 v f 7 1 u 2 v f 3 1 u 2 1 v f 6 u 2 1 v f 4 u 2 v f 0 u 2 1 v f 2 1 u 2 1 v f 8 1 u 2 v f 1 1 u 2 1 v f 5 κ 12 1 u 1 v 2 f 7 1 u v 2 f 3 1 u 1 v 2 f 6 u 1 v 2 f 4 u v 2 f 0 u 1 v 2 f 2 1 u 1 v 2 f 8 1 u v 2 f 1 1 u 1 v 2 f 5 κ 22 1 u 2 1 v 2 f 7 1 u 2 v 2 f 3 1 u 2 1 v 2 f 6 u 2 1 v 2 f 4 u 2 v 2 f 0 u 2 1 v 2 f 2 1 u 2 1 v 2 f 8 1 u 2 v 2 f 1 1 u 2 1 v 2 f 5 in the collision step cumulants are relaxed following the equations 21 c α β p c c α β ω α β c α β c α β e q where c α β e q is the equilibrium cumulant and c α β p c is the post collision one a cumulant related to the definition of the value of the transport coefficient ν is c 11 while the corresponding cumulants obtained from the rotational invariance constraint geier 2006 are c 20 and c 02 then in order to retain the isotropy of the model the latter cumulants are relaxed together 22 c 20 02 p c c 20 02 ω 20 02 c 20 c 20 e q c 02 c 02 e q c 20 02 p c c 20 02 1 ω 20 02 with c 20 02 c 20 c 02 and c 20 02 c 20 c 02 the relaxation rates related to the kinematic viscosity are ω 11 and ω 20 02 23 ω 11 1 3 ν 0 5 ω 20 02 ω 11 with ω 20 02 ω 20 ω 02 the only relaxation rates that influence the viscosity to leading order are ω 11 and ω 20 02 the relaxation rate ω 20 02 can be imposed equal to unity or related to the bulk viscosity the remaining relaxation rates are free parameters and can be chosen in the range 0 2 to improve stability or accuracy in section 3 3 5 we investigate some options for the choice of these relaxation rates finally post collision cumulants are transformed to distributions 24 f 0 κ 20 κ 22 2 κ 12 u κ 02 1 u 2 2 κ 21 v 4 κ 11 u v κ 20 v 2 κ 00 1 u 2 1 v 2 f 1 1 2 κ 20 κ 22 κ 00 u κ 02 u κ 00 u 2 κ 02 u 2 κ 12 1 2 u 2 κ 11 v 2 κ 21 v 4 κ 11 u v κ 20 v 2 κ 00 u v 2 κ 00 u 2 v 2 f 2 1 2 κ 02 κ 22 2 κ 11 u 2 κ 12 u κ 02 u 2 κ 00 v κ 20 v 4 κ 11 u v κ 00 u 2 v κ 00 v 2 κ 20 v 2 κ 00 u 2 v 2 κ 21 1 2 v f 3 1 2 κ 12 κ 20 κ 22 κ 00 u κ 02 u 2 κ 12 u κ 00 u 2 κ 02 u 2 2 κ 11 v 2 κ 21 v 4 κ 11 u v κ 20 v 2 κ 00 u v 2 κ 00 u 2 v 2 f 4 1 2 κ 02 k 21 κ 22 2 κ 11 u 2 κ 12 u κ 02 u 2 κ 00 v κ 20 v 2 κ 21 v 4 κ 11 u v κ 00 u 2 v κ 00 v 2 κ 20 v 2 κ 00 u 2 v 2 f 5 1 4 κ 12 κ 21 κ 22 κ 02 u 2 κ 12 u κ 02 u 2 κ 20 v 2 κ 21 v κ 00 u v κ 00 u 2 v κ 20 v 2 κ 00 u v 2 κ 00 u 2 v 2 κ 11 1 2 u 1 2 v f 6 1 4 κ 21 κ 22 κ 02 u κ 02 u 2 κ 12 1 2 u κ 20 v 2 κ 21 v κ 00 u v κ 00 u 2 v κ 20 v 2 κ 00 u v 2 κ 00 u 2 v 2 κ 11 1 2 u 1 2 v f 7 1 4 κ 21 κ 22 κ 02 u κ 02 u 2 κ 12 1 2 u κ 20 v 2 κ 21 v κ 00 u v κ 00 u 2 v κ 20 v 2 κ 00 u v 2 κ 00 u 2 v 2 κ 11 1 2 u 1 2 v f 8 1 4 κ 12 κ 21 κ 22 κ 02 u 2 κ 12 u κ 02 u 2 κ 20 v 2 κ 21 v κ 00 u v κ 00 u 2 v κ 20 v 2 κ 00 u v 2 κ 00 u 2 v 2 κ 11 1 2 u 1 2 v 2 3 1 specific characteristics of cumulant co based model in the d2q9 model relation 8 implies there are only nine independent moments and in particular 25 m 30 m 10 the coincidence of higher order moments with lower order ones is known as aliasing referring to the theory of cumulants geier et al 2015 or similarly to the taylor expansion of the maxwell boltzmann distribution up to second order in the mach number chen and doolen 1998 it can be shown that the third order equilibrium moment m 30 e q is equal to 26 m 30 e q 3 c s 2 u u 3 h if the normalized value of m 30 e q is considered and the high order term u 3 is neglected the moment m 30 e q becomes 27 m 30 e q 3 c s 2 m 10 e q 3 c s 2 m 30 e q this explains why in an isothermal single phase lattice boltzmann model the characteristic speed is considered constant and equal to 1 3 as already pointed out the characteristic speed cs in a shallow water lb model is not constant but variable with the depth of the water dellar 2002 the relationship 25 is hence not fulfilled by a swe lbm all previous models solved this problem by decoupling the viscosity from the characteristic speed in that way the equilibrium distribution function does no longer represent an approximation of the maxwellian in what follows we propose an alternative solution to the problem that respects the dependence of the viscosity on the characteristic speed and captures the first nine moments of the maxwellian equilibrium exactly since the aliasing relation 25 can no longer be fulfilled the isotropy of the viscosity has be restored by the use of different relaxation rates for the two moments controlling viscosity hence the following strategy is adopted the characteristic speed is assumed variable following eq 9 the expression of c s 2 enters in the definition of cumulants of the equilibrium in the collision step the relaxation rate of 2nd order cumulants are adjusted in order to retain the isotropy during the collision 28 ω 11 2 3 g h 1 ω 1 2 1 2 1 and 29 ω 20 02 2 3 1 ω 1 2 1 g h 2 1 2 1 where the ω is related to the kinematic viscosity of the fluid eq 10 3 results 3 1 convergence study in diffusive scaling as the two possible strategies to solve the swe with lbm differ in the way they impose an isotropic viscosity we show here the performance of the respective models in recovering the target viscosity to this end we conduct a convergence study section 3 2 and 3 3 by measuring the error in diffusive scaling the time step scales proportionally to the square of the grid spacing δt δx 2 the setting allows to maintain a constant value of reynolds number at constant viscosity under grid refinement the asymptotic behavior of the measured viscosity νm is determined by fitting the logarithm of the amplitude of a decaying wave to a linear function the slope is related to the measured viscosity through the square of the wave vector the normalized error of the viscosity erν with respect to the theoretical viscosity ν c s 2 τ 1 2 δ x 2 δ t is defined as e r ν ν m ν ν the phase lag can be considered a measure of the level of galilean invariance gi in the model thus the gi of the proposed method is investigated by calculating the phase lag er φ every turn when the wave comes back to its original position 3 2 shear wave test in the shear wave test the asymptotic behavior in diffusive scaling δt δx 2 of a one dimensional decaying shear wave is investigated the dimensions of the domain are l 3 nodes with l varying from 32 to 256 nodes periodic boundary conditions are used the simulation was run for 20000 l l 0 2 time steps and the wave amplitude was measured every 1000 l l 0 2 time steps in order to avoid any influence of the initial conditions on the asymptotic decay the values of viscosity and phase were compared to the measurement obtained after 10000 l l 0 2 time steps the velocity is scaled with l l 0 the initial conditions are given by u x t 0 u 0 l 0 l v x t 0 v 0 l 0 l sin 2 π x l with l 0 32 u 0 0 01 and v 0 0 1 all the quantities are expressed in lattice units l u the physical values are l 0δx u 0 δx δt v 0 δx δt with δ x 1m and δt given by the relation 12 the analytical solution of the problem is given by v x t v 0 l 0 l sin 2 π x l e ν t 2 π l 2 hereafter the results of the cumlb and bgk sw are shown fig 1 the error in viscosity in the cumlb model is comparable with the ones of bgk sw for example with ν 0 01 and h 1 in the cumulant model the normalized error in viscosity is 0 00284 for l 32 nodes and 0 0000443 for l 256 nodes in the bgk sw it is generally slightly higher 0 0029 for l 32 nodes and 0 0000453 for l 256 nodes moreover all the models show a second order convergence with the increase of the resolution the error in phase phase lag is measured when the wave should have come back to its original position the number of time steps after which the wave should return is equal to u 0 l 0 l the error in phase lag shows a fourth order accuracy it should be clarified that a fourth order phase lag is due to the alignment of the wave with the grid and that in general the method is second order accurate geier et al 2015 the value of the phase lag is similar in cumlb and bgk sw model 3 3 taylor green vortex test due to its simplicity in both initial and boundary conditions the taylor green vortex has been studied extensively and serves as a well established reference and benchmark test problem for numerical simulation it allows a straightforward validation of the code and is ideally suited for a structured grid approach altmann et al 2013 the decay of a taylor green vortex in a fully periodic domain is investigated to assess the accuracy of the transport coefficient viscosity and the phase lag computations are performed considering a domain of variable length l and width w different scenarios are based on values of l equal to 32 64 128 256 δx and width w equal to 48 96 192 and 384 δx respectively the difference in the dimensions of the domain l w allows to check for possible defects in the isotropy of the model and for the presence of preferential directions at the beginning of the simulation the values of the velocity along the x axis and y axis and of water depth are 30 u x y t 0 u 0 l 0 l u l 0 l k y k x cos k x x sin k y y 31 v x y t 0 u l 0 l k x k y sin k x x cos k y y 32 h x y t 0 h 0 1 u 2 l 0 2 l 2 4 c s 2 k y k x cos 2 k x x k x k y sin 2 k y y having indicated with kx and ky the components of the wave vectors 33 k x 2 π l k y 4 π 3 l k x k y 3 2 l 0 is set equal to 32 u 0 and u are chosen considering two different velocity configurations namely slow set and fast set for the slow set u 0 0 01 δ x δ t and u 0 00035 δ x δ t while for the fast set u 0 0 096 δ x δ t and u 0 0035 δ x δ t the value of the grid spacing is δ x 1 m the value of δt is given by the eq 12 the viscosity value is then defined as ν c s 2 τ 1 2 δ x 2 δ t an analytical solution of the wave velocity along the y axis is given by 34 v x y t u l 0 l k x k y sin k x x cos k y y e t t d where td decay time of the wave is equal to 35 t d 1 ν k x 2 k y 2 the aforementioned initial conditions are set as in krüger et al 2010 with the difference that our domain is not isotropic simulations are run with viscosities ν 0 01 0 001 0 0001 and different depths h 1 0 5 0 1 the simulation setup uses the diffusive scaling in such a way velocities and times are always multiplied by the factor l 0 l and l 0 l 2 respectively 3 3 1 error in viscosity the asymptotic behavior of the viscosity of the cumlb and bgk sw has been compared for various values of viscosities ν and depths of the water h to take the variations in the characteristic speed into account the logarithm of the normalized viscosity error is plotted against the logarithm of the number of nodes for h 1 the trend of the viscosity error slope is in between second and third order convergence for the cumlb model in the cumulant model with slow velocity set and ν 0 01 the slope is 2 35 fig 2 case a but it approaches 2 for lower values of h fig 2 case b and case c if stable the bgk sw is always characterized by a slope trend equal to 2 0 first of all it is noteworthy that the bgk sw model is characterized by a more limited range of stability in fact the bgk model becomes unstable for a value of h equal to 1 0 for all the considered viscosities ν 0 01 0 001 0 0001 fig 2 case a d and g the bgk sw results are slightly more accurate than those of the cumlb model for low depths only if the slow velocity set is considered i e fig 2 case c h and i conversely for the fast velocity set the cumlb model wins for h 0 5 the cumlb model is more accurate with ν 0 01 and ν 0 001 fig 2 case b and e bgk with ν 0 0001 fig 2 case h furthermore it should be noted that generally in cumlb model the error in viscosity increases with the reduction of viscosity as already discussed in geier et al 2015 on the other hand the viscosity error in the bgk sw changes in a limited manner with the reduction of viscosity as expected simulations performed with a fast velocity set are characterized by some cases missing because of instability fig 3 cases d g and i are missing furthermore all the models show a viscosity error much higher than for the slow velocity for example taking into consideration results of cumlb with ν 0 01 and h 1 0 an increment in the fast set of translational velocity u 0 and of the amplitude of the velocity of the wave u by a factor of about tenth leads to a forty times increase of the error for comparison fig 2 case a and fig 3 case a nevertheless it has to be pointed out that if the model remains stable errors can be considered essentially low in all the cases taking a maximum value of 0 015 for cumlb model and 0 02 for bgk sw respectively 3 3 2 phase lag measure in all the models examined the error in phase is defined by a slope of the trend equal to 2 as generally expected in lattice boltzmann models as already observed in the previous section 3 3 1 the instability that characterizes the bgk sw and starts from depths higher than 0 5 does not allow to measures the phase for h 1 cases a d and g of fig 4 and 5 at slow velocities fig 4 the error in phase of the cumlb model appears to be slightly influenced by the depth and viscosity values in fact it evidently fig 4 case a b and c increases if the depth decreases for example in a 256 384 nodes domain the difference of the error for the viscosities ν 0 01 and ν 0 0001 is about 2 5 for h 1 and about 40 for h 0 1 on the other hand the bgk model is characterized by an error in phase less variable with depth and viscosity as it was already observed in the section regarding the viscosity error section 3 3 1 for h 0 1 viscosity ν 0 0001 l 32 and l 32 the cumulant model is not stable in all the models the phase lag increases with the velocity fig 5 for example in the cumulant model the error for the fast velocities set is about 0 5 h 1 0 4 h 0 5 and 0 3 h 0 1 higher than for the slow set the error was calculated considering the medium value for all the viscosities in the bgk sw this difference is not seen and the phase lag grows by 0 5 for all the depths it has to be pointed out that in bgk sw the error does not change significantly with the change in the viscosity value for example fig 5 case b e and h if h 1 and ν 0 001 fig 5 case d cumlb exhibits an higher accuracy in phase 3 3 3 results for high viscosities results from the previous sections clearly exhibit the unstable behavior of bgk for h between 0 5 and 1 with all the viscosities taken into consideration bgk returns to stable solutions for ν 0 1 and h 1 fig 7 maintaining the trend already observed in sections 3 3 1 and 3 3 2 in such a case the cumulant model continues to display a stable behavior for a viscosity value ν 0 1 and h 1 fig 6 analyzing the simulation results not shown in this work it was noted that the trend illustrated in the previous cases fig 2 3 4 5 was maintained in fact an accuracy decrease both for bgk and cumlb can be put in evidence adopting high viscosities in the evaluation of the errors for viscosities higher than ν 0 01 a shorter time sampling of measurements has to be taken into consideration than for the lower viscosities as the decay time drops exponentially with the viscosity for a viscosity equal to ν 0 1 the sampling interval chosen is 100 l l 0 2 3 3 4 observations about stability range in this work a new mrt collision operator is presented and applied to the solution of shallow water equations as a first result and validation a study on accuracy and stability of the cumlb model against the standard bgk model is proposed and discussed in particular the taylor green vortex test in a rectangular domain has been considered significant to evaluate the behavior of the models also from the point of view of isotropy as this flow has velocity components not aligned with any of the primary axes in fig 6 and 7 the stability range of the models is shown the stability range changes for the different values of depths of the water h and translational velocity u 0 to define the range of stability an intermediate velocities value between the fast and slow set was considered with u 0 0 05 and wave amplitude u 0 00175 circles indicate the points where the simulation was performed when stable all models show a second order convergence in viscosity error and in phase error the h characterized by the most stable behavior is 0 5 here the simulations are stable for all the value of the viscosities taken into consideration if the value of the depth moves towards lower or higher values the stability properties change in particular for low depths h 0 1 both models are stable for the lowest viscosities ν 0 001 and ν 0 0001 only if using low translational velocities for depths going towards the value of 1 0 the cumlb model is always stable for the slow set of translational velocities for the fast set we have to arrive to a viscosity value equal to ν 0 01 to have a stable behavior it is clear from figs 6 and 7 that the cumlb model is characterized by a wider stability range in fact the bgk becomes unstable for h values between 0 5 and 1 several areas are totally missing in the bgk stability range graph bgk starts to become stable again only for high viscosities ν 0 1 and depths h 1 3 3 5 choices of relaxation rates parameters it needs to be clarified that all the results presented so far were obtained with the relaxation rates for the bulk mode the third order and fourth order cumulant set to one this choice is known to be particularly stable but it is also known that this is not the most accurate choice some particular combinations of relaxation rates can lead to drastic improvement of accuracy dubois and lallemand 2011 geier et al 2017 to investigate the effect of using different values of relaxation rates two approaches were taken into consideration under relaxation and over relaxation of third order cumulants the under relaxation of third order cumulants can be obtained by means of the ginzburg coefficient ginzburg showed that by selecting an appropriate combination of the odd and even rates it was possible to obtain the correct solution for the poiseuille and couette flow test cases d humières and ginzburg 2009 for the d2q9 model the ginzburg coefficient becomes λ 1 ω 11 1 2 1 ω 21 1 2 we selected the simple values λ 1 4 and λ 1 6 in the case of the taylor green vortex test both values of λ lead to a significant accuracy decrease in viscosity and in phase different results were achieved with the over relaxation of third order cumulants it can be obtained imposing ω 21 ω 11 in some cases this choice of parameters gave better results than under relaxation for example taking into account the error in phase section 3 3 2 we have observed an improvement in accuracy the best results are obtained for the highest viscosity ν 0 01 and the lowest height h 0 1 considering the low velocity set the improvement of the over relaxation with respect to the case ω 21 1 is about 39 considering the high velocity set the improvement is about 32 however the over relaxed cumulant does not always enhance the accuracy in viscosity a decisive improvement is only observed in a limited number of cases moreover it was found that the effects of over relaxation on stability are not always positive in fact despite a few cases where the stability improves for low heights it becomes worse in particular for a range of heights between 0 5 and 1 3 4 stoker dam break one of the test cases of a dam break flow was given from stoker 1957 and it has become a standard benchmark for the shallow water equations delestre et al 2013 hereafter the one dimensional stoker dam break is presented in order to compare the bgk model with the new mrt model the numerical results of the cumlb model are compared with the non stationary analytical solution at a certain time a fluid domain of 200 m 200 m was taken into account at the boundaries no slip is imposed fig 8 simulation setup the bed is flat and frictionless at t 0 the flow is at rest and characterized by two different water levels hl 10 m and hr 5 m with the presence of a step wise discontinuity at x 100 m the initial conditions are set to u x 0 m s e v e r y w h e r e h l 10 m 200 m x 100 m h r 5 m 100 m x 0 m the instantaneous breach of the dam leads to a transient flow consisting of two waves the one reflects from the discontinuity to the region with the higher water level the other is a shock wave moving in the opposite direction at time t 6 s a sensitivity analysis for different grid spacings was carried out for the cumlb model the relaxation rate τ was set to 0 85 and various δx 1 0 5 0 25 m were considered in the flat regions a good agreement between the simulation and the analytical solution is observed due to numerical diffusion the slopes are less well recovered but the agreement improves with higher resolution fig 8 it is also possible to note a slight oscillation at the shock front this phenomenon common in discrete approximations and analogous to gibbs oscillations arfken and weber 1972 is an obstacle to the stability of the lb schemes the oscillation becomes stronger with lower viscosity the gibbs effect is highlighted in the following graphs where the trend of h and u is shown for different times 2 4 6 and 8 s figs 9 and 10 the value of the relaxation rate is set to 0 6 which is lower than in previous case δx is set to 1 m the oscillations are especially evident in the case of the bgk model due to the strong gibbs oscillations the bgk model becomes unstable before the end of the simulation at time t 15 s after the bounce back of the wave at the wall we point out that the value of the grid spacing does not influence the extent of the oscillations to further investigate the stability properties of the cumulant co at different viscosities we gradually decrease the value of τ and δx results shows that using a δx 0 125 m the simulation is still stable for τ 0 5001 corresponding to a physical viscosity ν 5 05 10 6 m2 s which is close to the value of water the threshold value of τ for bgk co is 0 65 corresponding to ν 7 5 10 3 m2 s which is three orders of magnitude higher than the one reached for the cumulant co moreover a lower value of τ corresponds to simulation results closer to the analytical solution with regard to the knees fig 11 3 5 asymmetrical dam break of fennema chaudhry the fennema chaudhry dam break has been used extensively in literature as a test case for the shallow water equations fennema and chaudhry 1990 the spatial domain is a 200 m 200 m flat region without friction at the bed and a dam in the middle biscarini et al 2010 the numerical parameters of the simulation are δx 0 1 m δt 0 0082 s and τ 0 7 at t 0 the level of the water surface is set 10 m for the upstream region and 5 m for the downstream region fig 12 simulation setup the asymmetrical dam break was simulated using the cumulant co and the bgk sw model the value of viscosity in this example is artificially increased and differs from that of water this is due to the fact that the bgk co is unstable for lower value of τ and a comparison of results is feasible only in the stability range of both investigated models moreover outcomes were compared to results from a finite volume numerical model riverflow 2d murillo and garcía navarro 2010 that implements non linear shallow water equations and is based on adaptive triangular meshes the model solves mass and momentum conservation equations in a plane obtained by depth averaging navier stokes equations as most of 2d models available in literature this model neglects the diffusion of momentum due to viscosity υ 2 h u i x j x j a comparison with the continuous model will be therefore only qualitative as observed from figs 13 and 14 the bgk sw model and the cumlb model show similar results and the agreement between these solutions and the riverflow 2d solution is satisfactory the simulations with the cumulant and the bgk co start to differ significantly from each other after the impact of the wave with the east wall in fact the cumlb model continues to exhibit a stable behavior at the impact t 11 5 s after the reflection of the wave t 14 s and at the impact with the dam t 23 3 s fig 15 while the bgk sw model becomes unstable this is a further evidence of the stability improvement in lattice shallow water schemes due to the introduction of the cumulant co fig 16 shows the good agreement between the water depth hydrographs of riverflow 2d model and cumlb model at the two point p1 and p2 fig 12 located at the middle of the breach 4 conclusions in this work an alternative approach to solve the shallow water equations with the lattice boltzmann model was proposed and investigated all previous lbm swe models used a decoupling of the viscosity from the characteristic speed to assure isotropy in this work the physical link between the characteristic speed and viscosity has been maintained the isotropy of the viscosity has been restored by applying different relaxation rates to different second order cumulants it was confirmed that the method maintained a correct viscosity with second order convergence the proposed methodology overcomes the stability problems of bgk co for low values of viscosity allowing for the correct simulation of natural phenomena such as propagation of floods dam breaks that involve the propagation of water studying the breaking dam example it was also confirmed that the new approach compares favorable to the classical bgk approach it is possible to conclude that the cumulant co is a promising tool to overcome issues of the bgk model its higher stability properties make it more suitable for numerical simulation of shallow water equations since our approach is based on different relaxation rates for different cumulants it cannot be implemented within a single relaxation time bgk framework however the mechanism to sustain isotropy used in the bgk method can be implemented also in a mrt cascaded framework based on central moments as has recently be done by de rosis 2017 his method is consistent with the bgk operator as the latter is recovered exactly if all the moments relax with a common frequency in the cumulant framework the two methods can even be combined venturi 2018 which provides an additional degree of freedom to further improve stability and or accuracy declaration of competing interest the authors declare no potential conflict of interests acknowledgments this work was supported by the italian ministry of education university and research under prin grant no 20154ehyw9 combined numerical and experimental methodology for fluid structure interaction in free surface flows under impulsive loading 
550,this work presents a new lattice boltzmann model for steady and unsteady two dimensional shallow water flows compared to previous lattice boltzmann based shallow water models the proposed method uses a consistent characteristic speed in the pressure term and in the viscosity to preserve the isotropy of viscosity the relaxation rates for the different cumulants have to be decoupled this is only possible by using multiple relaxation rates the recovery of the correct viscosity is investigated by a convergence study based on the decay of a taylor green vortex results from shallow water models using the proposed collision operator are then compared to those derived from standard bgk approach and from a continuous model keywords shallow water collision operator cumulant convergence abbreviations lbm lattice boltzmann method swe shallow water equations lb sw lattice boltzmann shallow water co collision operator srt single relaxation time mrt multi relaxation time bgk bhatnagar gross krook cumlb model cumulant lattice boltzmann model bgk sw model bgk shallow water model pdf particle distribution function d2q9 two dimensional lattice pattern with 9 speed directions 1 introduction the lattice boltzmann method lbm is a mesoscopic method closely related to kinetic theory of gases it is a versatile method and it has been extensively applied in different fields such as turbulent flow krafczyk et al 2003 far et al 2016 multiphase falcucci et al 2010b 2010a flow in complex geometries ubertini et al 2003 di francesco et al 2013 in porous media yang et al 2016 and thermal flows zarghami et al 2014 however it is not so common to use the lbm approach to simulate large scale hydraulic problems such as flooding events di francesco et al 2016 dam breaks biscarini et al 2016 and propagation of tsunamis the lbm applies a stream and collide algorithm in which particles move on a cartesian lattice and collide at lattice nodes once an appropriate lattice or velocity set has been chosen the physics have to be implemented in the collision operator co which is the central part of the model a widespread solution is based on a single relaxation time srt approach bgk method qian et al 1992 characterized by a relatively simple implementation the particle distribution relaxes towards an equilibrium function with a rate chosen to match the viscosity of the modeled fluid in order to maximize the number of adjustable parameters and increase both stability and accuracy some authors suggest to use a multiple relaxation times mrt co d humières 1994 even though it was demonstrated that the mrt co improves the stability range of the lbm it introduces an additional galilean invariance violation and hyper viscosity problems when compared to the bgk co geier et al 2015 the first problem is due to the definition of the moments in fact the mrt method translates the pdf into a set of raw moments geier et al 2015 using a linear transformation each moment is orthogonal to all the others and relaxes with its own rate however the orthogonality of the moments is not reference frame independent krafczyk et al 2015 and orthogonality is not maintained if the reference frame is changed therefore the original mrt method further violates the galilean invariance compared to a bgk srt model by applying the collision in terms of cumulants instead of raw moments the cumulant lbm geier 2006 overcomes the violation of galilean invariance removing also the hyper viscosity that can be generated using for example central moments in the collision operator geier et al 2015 thus in this work we adopt the cumulant collision operator for the simulation of the shallow water equations in order to apply the lbm kinetic approach to large scale hydraulic problems and to increase stability and accuracy in the solution of the fluid dynamic problem the shallow water equations swe are obtained from the navier stokes equations by integrating over the water column from the ground to the free surface and assuming that the pressure along the vertical can be replaced by the hydrostatic pressure they have been extensively applied to the simulation of large scale hydraulic problems using standard numerical methods biscarini et al 2010 the swe were first studied with the lbm by salmon 1999 and dellar 2002 while zhou provided a comprehensive revision of the method zhou 2004 thought some works presenting simulations of large scale problems thommes et al 2007 shafiai 2011 prestininzi et al 2013 are available in literature the issue of the effective applicability of a shallow water lattice boltzmann model to complex hydraulic problems remains substantially open in term of efficiency stability and accuracy most of the applications of the lbm for shallow water flows are based on the standard bgk approach bhatnagar et al 1954 and only a small number of authors peng 2012 liu 2009 de rosis 2017 applied multiple relaxation time collision operators to the problem the objective of this work is to test the shallow water model using a non conventional mrt collision operator and compare it to the standard bgk approach in this work a new model based on cumulants is originally described and its principal and innovative features and the main theoretical differences to the standard bgk are underlined in order to retain the characteristics of the isotropy of the model the equilibrium cumulants and corresponding relaxation rates are here defined and mathematically derived taking into account the dependence of the characteristic speed on the water depth the paper is organized as follows section 2 is a short introduction to the governing shallow water equations and lattice boltzmann shallow water models in particular section 2 1 provides information on the lb models solving shallow water equations with the description of the different cos and section 2 3 gives an extensive description of the cumulant model and its implementation procedure numerical results and model performance are presented in section 3 in particular a comparison between the bgk model and the cumulant model is performed and the main differences are investigated especially in section 3 4 and section 3 5 the test case of the stoker dam break and the fennema chaudhry dam break are used to show the different stability characteristics of the bgk and the innovative approach finally section 4 presents conclusions 2 methods the swe are derived from the three dimensional incompressible navier stokes equations they are valid for problems in which the vertical dynamics of the fluid can be neglected toro 2001 in particular the swe are derived using an integration over the depth in order to obtain vertically averaged quantities the pressure distribution in the vertical direction z p z is supposed to be hydrostatic 1 p z ρ g where ρ is the fluid water density and g is the gravity acceleration the superscript indicates physical variables to distinguish them from variables in lattice units l u a system of 2d shallow water equations as in zhou 2004 can be written in the following form 2 h t h u j x j 0 3 u j 1 h z b z b h u d z 4 h u i t h u i u j x j g x i h 2 2 ν 2 h u i x j x j f i where i and j indicate the coordinate axis direction in 2d space h is the water depth u is the velocity ν is the kinematic viscosity zb is the bed elevation f i is the external force in the i direction the external force term can be written as follows 5 f i f p i f s i f w i f c i where f p i is the force due to gravity and is equal to g h z b x i f s i is the bed shear stress defined as 6 f s i c b u i u i u i where c b g n f 2 h 1 3 represents the friction factor with manning s coefficient n f at the seabed the terms f w i and f c i are respectively the wind shear stress and the force representing the coriolis effect in this work shallow water flows are modeled using a mesoscopic approach for the tracking of free surface dynamics flow characteristics are described by the evolution of a particle distribution function f x t pdf on a regularly spaced domain lattice pattern using the discrete lattice boltzmann equation 7 f α x e a δ t t δ t f α x t ω α f α α 1 m where x is the position of the particle in the discretized space at time t fα x t and eα are the particle distribution functions and the set of discrete speeds along the m allowed lattice directions the key steps in lbm are streaming and collision in eq 7 f α x e a δ t t δ t f α x t represents the streaming process and ω α is the collision operator fα represents the external forces the lattice pattern dnqm dn stands for n dimensions while qm stands for m speeds in lbm has the two functions to represent the points of the grid and to determine the particle directions of the motion in the present work we adopt the d2q9 pattern with the following set of velocities e α α 0 8 wolf gladrow 2004 e 0 0 0 e 1 1 0 e 2 0 1 e 3 1 0 e 4 0 1 e 5 1 1 e 6 1 1 e 7 1 1 e 8 1 1 in lbm the macroscopic properties of the fluid can be expressed using the raw moments of the distribution the generic raw moment mαβ can be expressed as 8 m α β i j i α j β f i j i j 1 0 1 where the sum of α and β indices represents the moment order the indices i and j represent the miller indices and following a method primarily used in crystallography cambridge university 2008 allow to express speeds directions 2 1 lattice boltzmann shallow water model in isothermal lattice boltzmann models the characteristic speed is a parameter set to a constant that maximizes isotropy the characteristic speed is usually identified with the speed of sound for the d2q9 lattice the speed of sound is equal to c s 1 3 lattice units this fixed characteristic speed is an important simplification of the lattice boltzmann method as it eliminates the need to fully resolve the energy flux tensor in velocity space geier and pasquali 2018 to understand this point we have to recall that the momentum distribution gives rise to an infinite hierarchy of moment equations due to the limited number of lattice velocities only very few of these moment equations are independent of each other on the d2q9 lattice the moments m 30 and m 03 are not independently adjustable quantities physically they are part of the heat flux tensor the trick applied in the lattice boltzmann method to avoid a larger set of discrete velocities in which these moments were independent is to chose a specific temperature and hence characteristic speed for which the moments m 30 and m 03 recover their correct physical value this is only the case for c s 1 3 which explains why this constant characteristic speed is chosen a detailed mathematical derivation is found in appendix h of geier et al 2015 in the swe the speed of surface waves takes the place of the speed of sound characteristic speed in the original lbm in fact in lattice boltzmann shallow water lb sw models the characteristic speed cs is not constant but a function of the fluid elevation h and the gravity acceleration g 9 c s 2 g h 2 this is a direct consequence of the equation of state p 1 2 g h 2 dellar 2002 where p indicates the macroscopic value of the pressure this poses a problem to the applicability of the lbm to the swe as the characteristic speed is no longer constant the errors in the discretization of the third order moments do no longer cancel automatically geier and pasquali 2018 this is important as the characteristic speed influences the viscosity in fact the kinematic viscosity transport coefficient of the fluid ν is linked to the relaxation rate ω and to the characteristic speed 10 ν c s 2 1 ω 1 2 the macroscopic properties water depth h and velocity field ui of the flow are computed from the raw moments m 00 m 10 m 01 respectively 11 h α 0 8 f α u i 1 h α 0 8 e α i f α α 0 8 during the collision update rules are applied at each node depending only on the state of the pdf on the node the collision has to conserve mass and momentum such that m 00 m 10 and m 01 are quantities that do not change during this step 2 1 1 from lattice units to physical units in lb simulations the grid spacing δx and the elementary lattice time step δt are linked through the physical value of the characteristic speed c s c s δ x δ t then δ t c s c s δ x s as reported in wolf gladrow 2004 the characteristic speed represents the velocity at which the waves travel recalling that in swe lbm the characteristic speed is defined as in equation 9 it follows that the value of the lattice time step δt in seconds s is given by δ t g h 2 g h 2 δ x where g and g are the value of the gravity acceleration in lattice and physical units respectively in the practical implementation h can be maintained equal to h then the expression of the lattice time step δt becomes 12 δ t g g δ x s the froude number fr of the physical model matches the lattice one 13 f r v g h v δ x δ t g δ x δ t 2 h δ x v δ x δ t δ x δ t g h f r 2 2 single relaxation time models srt models solving the shallow water equations are generally based on a bgk co salmon 1999 zhou 2004 assuming that collision is the process that returns particles to the state of the local maxwellian equilibrium 14 ω α f α f α e q τ where ω α is the collision operator and τ is the collision mean free time authors dealing with the solution of the shallow water equations using the bgk approach geveler et al 2011 frandsen 2008 suggest to express the equilibrium pdf as a power series in macroscopic velocities up to second order assuring mass and momentum conservation 15 f α e q h 5 g h 2 6 2 h 3 u u α 0 β h g h 6 1 3 e α u 1 2 e α u 2 1 6 u u α 1 8 where u u v with u and v that represent the velocity components β assumes the value 1 if α 1 4 and 1 4 otherwise de rosis 2017 in the d2q9 model the relation 8 implies there are only nine independent moments and in particular m 30 m 10 in the srt shallow water model bgk sw the characteristic speed is decoupled from the third order moment m 30 the model uses a c s 2 different from g h 2 in the diffusion process this can be evinced by observing the equilibrium equations used the part related to the viscosity appears equal to the one adopted in a standard two dimensional lb model while the part related to the density is different and takes into account the dependency of the characteristic speed c s 2 on the depth of the water h eq 15 one of the properties of these models is therefore the decoupling of the viscosity from the characteristic speed to assure isotropy in the next paragraph we propose and derive the cumulant collision operator where the physical link between the characteristic speed and viscosity is maintained and the isotropy of the viscosity is restored by applying different relaxation rates to different second order cumulants 2 3 cumulant model while the original mrt model performs collision on moments the cumulant model cumlb performs the collision on cumulants cumulants quantify the deviation of a distribution from a gaussian distribution by design cumulants are statistically independent of each other cumulants are conveniently obtained from central moments defined as 16 κ α β i j i u α j v β f i j i j 1 0 1 where the subscripts i and j indicate the corresponding components of the speed vectors of the pdf for the d2q9 pattern used in this work all but one of the available cumulants are identical to the corresponding central moments in particular 17 c 00 κ 00 c 10 κ 10 c 01 κ 01 c 20 κ 20 c 02 κ 02 c 11 κ 11 c 12 κ 12 c 21 κ 21 starting from fourth order cumulants differ from central moments geier et al 2015 the d2q9 pattern used in this work has only one independent fourth order cumulant 18 c 22 κ 22 κ 20 κ 02 2 κ 11 2 h then equilibrium cumulants are identical to equilibrium central moments geier 2006 except for the fourth order cumulant that is zero the equilibrium cumulants for a d2q9 scheme are given by 19 c 00 h c 10 0 c 01 0 c 20 c s 2 h c 02 c s 2 h c 11 0 c 12 0 c 21 0 c 22 0 the cumlb method is implemented by transforming the distributions to cumulants before the collision using the following equations 20 κ 00 f 7 f 3 f 6 f 4 f 0 f 2 f 8 f 1 f 5 κ 10 0 κ 01 0 κ 20 1 u 2 f 7 1 u 2 f 3 1 u 2 f 6 u 2 f 4 u 2 f 0 u 2 f 2 1 u 2 f 8 1 u 2 f 1 1 u 2 f 5 κ 02 1 v 2 f 7 v 2 f 3 1 v 2 f 6 1 v 2 f 4 v 2 f 0 1 v 2 f 2 1 v 2 f 8 v 2 f 1 1 v 2 f 5 κ 11 1 u 1 v f 7 1 u v f 3 1 u 1 v f 6 u 1 v f 4 u v f 0 u 1 v f 2 1 u 1 v f 8 1 u v f 1 1 u 1 v f 5 κ 21 1 u 2 1 v f 7 1 u 2 v f 3 1 u 2 1 v f 6 u 2 1 v f 4 u 2 v f 0 u 2 1 v f 2 1 u 2 1 v f 8 1 u 2 v f 1 1 u 2 1 v f 5 κ 12 1 u 1 v 2 f 7 1 u v 2 f 3 1 u 1 v 2 f 6 u 1 v 2 f 4 u v 2 f 0 u 1 v 2 f 2 1 u 1 v 2 f 8 1 u v 2 f 1 1 u 1 v 2 f 5 κ 22 1 u 2 1 v 2 f 7 1 u 2 v 2 f 3 1 u 2 1 v 2 f 6 u 2 1 v 2 f 4 u 2 v 2 f 0 u 2 1 v 2 f 2 1 u 2 1 v 2 f 8 1 u 2 v 2 f 1 1 u 2 1 v 2 f 5 in the collision step cumulants are relaxed following the equations 21 c α β p c c α β ω α β c α β c α β e q where c α β e q is the equilibrium cumulant and c α β p c is the post collision one a cumulant related to the definition of the value of the transport coefficient ν is c 11 while the corresponding cumulants obtained from the rotational invariance constraint geier 2006 are c 20 and c 02 then in order to retain the isotropy of the model the latter cumulants are relaxed together 22 c 20 02 p c c 20 02 ω 20 02 c 20 c 20 e q c 02 c 02 e q c 20 02 p c c 20 02 1 ω 20 02 with c 20 02 c 20 c 02 and c 20 02 c 20 c 02 the relaxation rates related to the kinematic viscosity are ω 11 and ω 20 02 23 ω 11 1 3 ν 0 5 ω 20 02 ω 11 with ω 20 02 ω 20 ω 02 the only relaxation rates that influence the viscosity to leading order are ω 11 and ω 20 02 the relaxation rate ω 20 02 can be imposed equal to unity or related to the bulk viscosity the remaining relaxation rates are free parameters and can be chosen in the range 0 2 to improve stability or accuracy in section 3 3 5 we investigate some options for the choice of these relaxation rates finally post collision cumulants are transformed to distributions 24 f 0 κ 20 κ 22 2 κ 12 u κ 02 1 u 2 2 κ 21 v 4 κ 11 u v κ 20 v 2 κ 00 1 u 2 1 v 2 f 1 1 2 κ 20 κ 22 κ 00 u κ 02 u κ 00 u 2 κ 02 u 2 κ 12 1 2 u 2 κ 11 v 2 κ 21 v 4 κ 11 u v κ 20 v 2 κ 00 u v 2 κ 00 u 2 v 2 f 2 1 2 κ 02 κ 22 2 κ 11 u 2 κ 12 u κ 02 u 2 κ 00 v κ 20 v 4 κ 11 u v κ 00 u 2 v κ 00 v 2 κ 20 v 2 κ 00 u 2 v 2 κ 21 1 2 v f 3 1 2 κ 12 κ 20 κ 22 κ 00 u κ 02 u 2 κ 12 u κ 00 u 2 κ 02 u 2 2 κ 11 v 2 κ 21 v 4 κ 11 u v κ 20 v 2 κ 00 u v 2 κ 00 u 2 v 2 f 4 1 2 κ 02 k 21 κ 22 2 κ 11 u 2 κ 12 u κ 02 u 2 κ 00 v κ 20 v 2 κ 21 v 4 κ 11 u v κ 00 u 2 v κ 00 v 2 κ 20 v 2 κ 00 u 2 v 2 f 5 1 4 κ 12 κ 21 κ 22 κ 02 u 2 κ 12 u κ 02 u 2 κ 20 v 2 κ 21 v κ 00 u v κ 00 u 2 v κ 20 v 2 κ 00 u v 2 κ 00 u 2 v 2 κ 11 1 2 u 1 2 v f 6 1 4 κ 21 κ 22 κ 02 u κ 02 u 2 κ 12 1 2 u κ 20 v 2 κ 21 v κ 00 u v κ 00 u 2 v κ 20 v 2 κ 00 u v 2 κ 00 u 2 v 2 κ 11 1 2 u 1 2 v f 7 1 4 κ 21 κ 22 κ 02 u κ 02 u 2 κ 12 1 2 u κ 20 v 2 κ 21 v κ 00 u v κ 00 u 2 v κ 20 v 2 κ 00 u v 2 κ 00 u 2 v 2 κ 11 1 2 u 1 2 v f 8 1 4 κ 12 κ 21 κ 22 κ 02 u 2 κ 12 u κ 02 u 2 κ 20 v 2 κ 21 v κ 00 u v κ 00 u 2 v κ 20 v 2 κ 00 u v 2 κ 00 u 2 v 2 κ 11 1 2 u 1 2 v 2 3 1 specific characteristics of cumulant co based model in the d2q9 model relation 8 implies there are only nine independent moments and in particular 25 m 30 m 10 the coincidence of higher order moments with lower order ones is known as aliasing referring to the theory of cumulants geier et al 2015 or similarly to the taylor expansion of the maxwell boltzmann distribution up to second order in the mach number chen and doolen 1998 it can be shown that the third order equilibrium moment m 30 e q is equal to 26 m 30 e q 3 c s 2 u u 3 h if the normalized value of m 30 e q is considered and the high order term u 3 is neglected the moment m 30 e q becomes 27 m 30 e q 3 c s 2 m 10 e q 3 c s 2 m 30 e q this explains why in an isothermal single phase lattice boltzmann model the characteristic speed is considered constant and equal to 1 3 as already pointed out the characteristic speed cs in a shallow water lb model is not constant but variable with the depth of the water dellar 2002 the relationship 25 is hence not fulfilled by a swe lbm all previous models solved this problem by decoupling the viscosity from the characteristic speed in that way the equilibrium distribution function does no longer represent an approximation of the maxwellian in what follows we propose an alternative solution to the problem that respects the dependence of the viscosity on the characteristic speed and captures the first nine moments of the maxwellian equilibrium exactly since the aliasing relation 25 can no longer be fulfilled the isotropy of the viscosity has be restored by the use of different relaxation rates for the two moments controlling viscosity hence the following strategy is adopted the characteristic speed is assumed variable following eq 9 the expression of c s 2 enters in the definition of cumulants of the equilibrium in the collision step the relaxation rate of 2nd order cumulants are adjusted in order to retain the isotropy during the collision 28 ω 11 2 3 g h 1 ω 1 2 1 2 1 and 29 ω 20 02 2 3 1 ω 1 2 1 g h 2 1 2 1 where the ω is related to the kinematic viscosity of the fluid eq 10 3 results 3 1 convergence study in diffusive scaling as the two possible strategies to solve the swe with lbm differ in the way they impose an isotropic viscosity we show here the performance of the respective models in recovering the target viscosity to this end we conduct a convergence study section 3 2 and 3 3 by measuring the error in diffusive scaling the time step scales proportionally to the square of the grid spacing δt δx 2 the setting allows to maintain a constant value of reynolds number at constant viscosity under grid refinement the asymptotic behavior of the measured viscosity νm is determined by fitting the logarithm of the amplitude of a decaying wave to a linear function the slope is related to the measured viscosity through the square of the wave vector the normalized error of the viscosity erν with respect to the theoretical viscosity ν c s 2 τ 1 2 δ x 2 δ t is defined as e r ν ν m ν ν the phase lag can be considered a measure of the level of galilean invariance gi in the model thus the gi of the proposed method is investigated by calculating the phase lag er φ every turn when the wave comes back to its original position 3 2 shear wave test in the shear wave test the asymptotic behavior in diffusive scaling δt δx 2 of a one dimensional decaying shear wave is investigated the dimensions of the domain are l 3 nodes with l varying from 32 to 256 nodes periodic boundary conditions are used the simulation was run for 20000 l l 0 2 time steps and the wave amplitude was measured every 1000 l l 0 2 time steps in order to avoid any influence of the initial conditions on the asymptotic decay the values of viscosity and phase were compared to the measurement obtained after 10000 l l 0 2 time steps the velocity is scaled with l l 0 the initial conditions are given by u x t 0 u 0 l 0 l v x t 0 v 0 l 0 l sin 2 π x l with l 0 32 u 0 0 01 and v 0 0 1 all the quantities are expressed in lattice units l u the physical values are l 0δx u 0 δx δt v 0 δx δt with δ x 1m and δt given by the relation 12 the analytical solution of the problem is given by v x t v 0 l 0 l sin 2 π x l e ν t 2 π l 2 hereafter the results of the cumlb and bgk sw are shown fig 1 the error in viscosity in the cumlb model is comparable with the ones of bgk sw for example with ν 0 01 and h 1 in the cumulant model the normalized error in viscosity is 0 00284 for l 32 nodes and 0 0000443 for l 256 nodes in the bgk sw it is generally slightly higher 0 0029 for l 32 nodes and 0 0000453 for l 256 nodes moreover all the models show a second order convergence with the increase of the resolution the error in phase phase lag is measured when the wave should have come back to its original position the number of time steps after which the wave should return is equal to u 0 l 0 l the error in phase lag shows a fourth order accuracy it should be clarified that a fourth order phase lag is due to the alignment of the wave with the grid and that in general the method is second order accurate geier et al 2015 the value of the phase lag is similar in cumlb and bgk sw model 3 3 taylor green vortex test due to its simplicity in both initial and boundary conditions the taylor green vortex has been studied extensively and serves as a well established reference and benchmark test problem for numerical simulation it allows a straightforward validation of the code and is ideally suited for a structured grid approach altmann et al 2013 the decay of a taylor green vortex in a fully periodic domain is investigated to assess the accuracy of the transport coefficient viscosity and the phase lag computations are performed considering a domain of variable length l and width w different scenarios are based on values of l equal to 32 64 128 256 δx and width w equal to 48 96 192 and 384 δx respectively the difference in the dimensions of the domain l w allows to check for possible defects in the isotropy of the model and for the presence of preferential directions at the beginning of the simulation the values of the velocity along the x axis and y axis and of water depth are 30 u x y t 0 u 0 l 0 l u l 0 l k y k x cos k x x sin k y y 31 v x y t 0 u l 0 l k x k y sin k x x cos k y y 32 h x y t 0 h 0 1 u 2 l 0 2 l 2 4 c s 2 k y k x cos 2 k x x k x k y sin 2 k y y having indicated with kx and ky the components of the wave vectors 33 k x 2 π l k y 4 π 3 l k x k y 3 2 l 0 is set equal to 32 u 0 and u are chosen considering two different velocity configurations namely slow set and fast set for the slow set u 0 0 01 δ x δ t and u 0 00035 δ x δ t while for the fast set u 0 0 096 δ x δ t and u 0 0035 δ x δ t the value of the grid spacing is δ x 1 m the value of δt is given by the eq 12 the viscosity value is then defined as ν c s 2 τ 1 2 δ x 2 δ t an analytical solution of the wave velocity along the y axis is given by 34 v x y t u l 0 l k x k y sin k x x cos k y y e t t d where td decay time of the wave is equal to 35 t d 1 ν k x 2 k y 2 the aforementioned initial conditions are set as in krüger et al 2010 with the difference that our domain is not isotropic simulations are run with viscosities ν 0 01 0 001 0 0001 and different depths h 1 0 5 0 1 the simulation setup uses the diffusive scaling in such a way velocities and times are always multiplied by the factor l 0 l and l 0 l 2 respectively 3 3 1 error in viscosity the asymptotic behavior of the viscosity of the cumlb and bgk sw has been compared for various values of viscosities ν and depths of the water h to take the variations in the characteristic speed into account the logarithm of the normalized viscosity error is plotted against the logarithm of the number of nodes for h 1 the trend of the viscosity error slope is in between second and third order convergence for the cumlb model in the cumulant model with slow velocity set and ν 0 01 the slope is 2 35 fig 2 case a but it approaches 2 for lower values of h fig 2 case b and case c if stable the bgk sw is always characterized by a slope trend equal to 2 0 first of all it is noteworthy that the bgk sw model is characterized by a more limited range of stability in fact the bgk model becomes unstable for a value of h equal to 1 0 for all the considered viscosities ν 0 01 0 001 0 0001 fig 2 case a d and g the bgk sw results are slightly more accurate than those of the cumlb model for low depths only if the slow velocity set is considered i e fig 2 case c h and i conversely for the fast velocity set the cumlb model wins for h 0 5 the cumlb model is more accurate with ν 0 01 and ν 0 001 fig 2 case b and e bgk with ν 0 0001 fig 2 case h furthermore it should be noted that generally in cumlb model the error in viscosity increases with the reduction of viscosity as already discussed in geier et al 2015 on the other hand the viscosity error in the bgk sw changes in a limited manner with the reduction of viscosity as expected simulations performed with a fast velocity set are characterized by some cases missing because of instability fig 3 cases d g and i are missing furthermore all the models show a viscosity error much higher than for the slow velocity for example taking into consideration results of cumlb with ν 0 01 and h 1 0 an increment in the fast set of translational velocity u 0 and of the amplitude of the velocity of the wave u by a factor of about tenth leads to a forty times increase of the error for comparison fig 2 case a and fig 3 case a nevertheless it has to be pointed out that if the model remains stable errors can be considered essentially low in all the cases taking a maximum value of 0 015 for cumlb model and 0 02 for bgk sw respectively 3 3 2 phase lag measure in all the models examined the error in phase is defined by a slope of the trend equal to 2 as generally expected in lattice boltzmann models as already observed in the previous section 3 3 1 the instability that characterizes the bgk sw and starts from depths higher than 0 5 does not allow to measures the phase for h 1 cases a d and g of fig 4 and 5 at slow velocities fig 4 the error in phase of the cumlb model appears to be slightly influenced by the depth and viscosity values in fact it evidently fig 4 case a b and c increases if the depth decreases for example in a 256 384 nodes domain the difference of the error for the viscosities ν 0 01 and ν 0 0001 is about 2 5 for h 1 and about 40 for h 0 1 on the other hand the bgk model is characterized by an error in phase less variable with depth and viscosity as it was already observed in the section regarding the viscosity error section 3 3 1 for h 0 1 viscosity ν 0 0001 l 32 and l 32 the cumulant model is not stable in all the models the phase lag increases with the velocity fig 5 for example in the cumulant model the error for the fast velocities set is about 0 5 h 1 0 4 h 0 5 and 0 3 h 0 1 higher than for the slow set the error was calculated considering the medium value for all the viscosities in the bgk sw this difference is not seen and the phase lag grows by 0 5 for all the depths it has to be pointed out that in bgk sw the error does not change significantly with the change in the viscosity value for example fig 5 case b e and h if h 1 and ν 0 001 fig 5 case d cumlb exhibits an higher accuracy in phase 3 3 3 results for high viscosities results from the previous sections clearly exhibit the unstable behavior of bgk for h between 0 5 and 1 with all the viscosities taken into consideration bgk returns to stable solutions for ν 0 1 and h 1 fig 7 maintaining the trend already observed in sections 3 3 1 and 3 3 2 in such a case the cumulant model continues to display a stable behavior for a viscosity value ν 0 1 and h 1 fig 6 analyzing the simulation results not shown in this work it was noted that the trend illustrated in the previous cases fig 2 3 4 5 was maintained in fact an accuracy decrease both for bgk and cumlb can be put in evidence adopting high viscosities in the evaluation of the errors for viscosities higher than ν 0 01 a shorter time sampling of measurements has to be taken into consideration than for the lower viscosities as the decay time drops exponentially with the viscosity for a viscosity equal to ν 0 1 the sampling interval chosen is 100 l l 0 2 3 3 4 observations about stability range in this work a new mrt collision operator is presented and applied to the solution of shallow water equations as a first result and validation a study on accuracy and stability of the cumlb model against the standard bgk model is proposed and discussed in particular the taylor green vortex test in a rectangular domain has been considered significant to evaluate the behavior of the models also from the point of view of isotropy as this flow has velocity components not aligned with any of the primary axes in fig 6 and 7 the stability range of the models is shown the stability range changes for the different values of depths of the water h and translational velocity u 0 to define the range of stability an intermediate velocities value between the fast and slow set was considered with u 0 0 05 and wave amplitude u 0 00175 circles indicate the points where the simulation was performed when stable all models show a second order convergence in viscosity error and in phase error the h characterized by the most stable behavior is 0 5 here the simulations are stable for all the value of the viscosities taken into consideration if the value of the depth moves towards lower or higher values the stability properties change in particular for low depths h 0 1 both models are stable for the lowest viscosities ν 0 001 and ν 0 0001 only if using low translational velocities for depths going towards the value of 1 0 the cumlb model is always stable for the slow set of translational velocities for the fast set we have to arrive to a viscosity value equal to ν 0 01 to have a stable behavior it is clear from figs 6 and 7 that the cumlb model is characterized by a wider stability range in fact the bgk becomes unstable for h values between 0 5 and 1 several areas are totally missing in the bgk stability range graph bgk starts to become stable again only for high viscosities ν 0 1 and depths h 1 3 3 5 choices of relaxation rates parameters it needs to be clarified that all the results presented so far were obtained with the relaxation rates for the bulk mode the third order and fourth order cumulant set to one this choice is known to be particularly stable but it is also known that this is not the most accurate choice some particular combinations of relaxation rates can lead to drastic improvement of accuracy dubois and lallemand 2011 geier et al 2017 to investigate the effect of using different values of relaxation rates two approaches were taken into consideration under relaxation and over relaxation of third order cumulants the under relaxation of third order cumulants can be obtained by means of the ginzburg coefficient ginzburg showed that by selecting an appropriate combination of the odd and even rates it was possible to obtain the correct solution for the poiseuille and couette flow test cases d humières and ginzburg 2009 for the d2q9 model the ginzburg coefficient becomes λ 1 ω 11 1 2 1 ω 21 1 2 we selected the simple values λ 1 4 and λ 1 6 in the case of the taylor green vortex test both values of λ lead to a significant accuracy decrease in viscosity and in phase different results were achieved with the over relaxation of third order cumulants it can be obtained imposing ω 21 ω 11 in some cases this choice of parameters gave better results than under relaxation for example taking into account the error in phase section 3 3 2 we have observed an improvement in accuracy the best results are obtained for the highest viscosity ν 0 01 and the lowest height h 0 1 considering the low velocity set the improvement of the over relaxation with respect to the case ω 21 1 is about 39 considering the high velocity set the improvement is about 32 however the over relaxed cumulant does not always enhance the accuracy in viscosity a decisive improvement is only observed in a limited number of cases moreover it was found that the effects of over relaxation on stability are not always positive in fact despite a few cases where the stability improves for low heights it becomes worse in particular for a range of heights between 0 5 and 1 3 4 stoker dam break one of the test cases of a dam break flow was given from stoker 1957 and it has become a standard benchmark for the shallow water equations delestre et al 2013 hereafter the one dimensional stoker dam break is presented in order to compare the bgk model with the new mrt model the numerical results of the cumlb model are compared with the non stationary analytical solution at a certain time a fluid domain of 200 m 200 m was taken into account at the boundaries no slip is imposed fig 8 simulation setup the bed is flat and frictionless at t 0 the flow is at rest and characterized by two different water levels hl 10 m and hr 5 m with the presence of a step wise discontinuity at x 100 m the initial conditions are set to u x 0 m s e v e r y w h e r e h l 10 m 200 m x 100 m h r 5 m 100 m x 0 m the instantaneous breach of the dam leads to a transient flow consisting of two waves the one reflects from the discontinuity to the region with the higher water level the other is a shock wave moving in the opposite direction at time t 6 s a sensitivity analysis for different grid spacings was carried out for the cumlb model the relaxation rate τ was set to 0 85 and various δx 1 0 5 0 25 m were considered in the flat regions a good agreement between the simulation and the analytical solution is observed due to numerical diffusion the slopes are less well recovered but the agreement improves with higher resolution fig 8 it is also possible to note a slight oscillation at the shock front this phenomenon common in discrete approximations and analogous to gibbs oscillations arfken and weber 1972 is an obstacle to the stability of the lb schemes the oscillation becomes stronger with lower viscosity the gibbs effect is highlighted in the following graphs where the trend of h and u is shown for different times 2 4 6 and 8 s figs 9 and 10 the value of the relaxation rate is set to 0 6 which is lower than in previous case δx is set to 1 m the oscillations are especially evident in the case of the bgk model due to the strong gibbs oscillations the bgk model becomes unstable before the end of the simulation at time t 15 s after the bounce back of the wave at the wall we point out that the value of the grid spacing does not influence the extent of the oscillations to further investigate the stability properties of the cumulant co at different viscosities we gradually decrease the value of τ and δx results shows that using a δx 0 125 m the simulation is still stable for τ 0 5001 corresponding to a physical viscosity ν 5 05 10 6 m2 s which is close to the value of water the threshold value of τ for bgk co is 0 65 corresponding to ν 7 5 10 3 m2 s which is three orders of magnitude higher than the one reached for the cumulant co moreover a lower value of τ corresponds to simulation results closer to the analytical solution with regard to the knees fig 11 3 5 asymmetrical dam break of fennema chaudhry the fennema chaudhry dam break has been used extensively in literature as a test case for the shallow water equations fennema and chaudhry 1990 the spatial domain is a 200 m 200 m flat region without friction at the bed and a dam in the middle biscarini et al 2010 the numerical parameters of the simulation are δx 0 1 m δt 0 0082 s and τ 0 7 at t 0 the level of the water surface is set 10 m for the upstream region and 5 m for the downstream region fig 12 simulation setup the asymmetrical dam break was simulated using the cumulant co and the bgk sw model the value of viscosity in this example is artificially increased and differs from that of water this is due to the fact that the bgk co is unstable for lower value of τ and a comparison of results is feasible only in the stability range of both investigated models moreover outcomes were compared to results from a finite volume numerical model riverflow 2d murillo and garcía navarro 2010 that implements non linear shallow water equations and is based on adaptive triangular meshes the model solves mass and momentum conservation equations in a plane obtained by depth averaging navier stokes equations as most of 2d models available in literature this model neglects the diffusion of momentum due to viscosity υ 2 h u i x j x j a comparison with the continuous model will be therefore only qualitative as observed from figs 13 and 14 the bgk sw model and the cumlb model show similar results and the agreement between these solutions and the riverflow 2d solution is satisfactory the simulations with the cumulant and the bgk co start to differ significantly from each other after the impact of the wave with the east wall in fact the cumlb model continues to exhibit a stable behavior at the impact t 11 5 s after the reflection of the wave t 14 s and at the impact with the dam t 23 3 s fig 15 while the bgk sw model becomes unstable this is a further evidence of the stability improvement in lattice shallow water schemes due to the introduction of the cumulant co fig 16 shows the good agreement between the water depth hydrographs of riverflow 2d model and cumlb model at the two point p1 and p2 fig 12 located at the middle of the breach 4 conclusions in this work an alternative approach to solve the shallow water equations with the lattice boltzmann model was proposed and investigated all previous lbm swe models used a decoupling of the viscosity from the characteristic speed to assure isotropy in this work the physical link between the characteristic speed and viscosity has been maintained the isotropy of the viscosity has been restored by applying different relaxation rates to different second order cumulants it was confirmed that the method maintained a correct viscosity with second order convergence the proposed methodology overcomes the stability problems of bgk co for low values of viscosity allowing for the correct simulation of natural phenomena such as propagation of floods dam breaks that involve the propagation of water studying the breaking dam example it was also confirmed that the new approach compares favorable to the classical bgk approach it is possible to conclude that the cumulant co is a promising tool to overcome issues of the bgk model its higher stability properties make it more suitable for numerical simulation of shallow water equations since our approach is based on different relaxation rates for different cumulants it cannot be implemented within a single relaxation time bgk framework however the mechanism to sustain isotropy used in the bgk method can be implemented also in a mrt cascaded framework based on central moments as has recently be done by de rosis 2017 his method is consistent with the bgk operator as the latter is recovered exactly if all the moments relax with a common frequency in the cumulant framework the two methods can even be combined venturi 2018 which provides an additional degree of freedom to further improve stability and or accuracy declaration of competing interest the authors declare no potential conflict of interests acknowledgments this work was supported by the italian ministry of education university and research under prin grant no 20154ehyw9 combined numerical and experimental methodology for fluid structure interaction in free surface flows under impulsive loading 
551,current computational power allows the modelling of complex time dependent flows in particular shallow water flows are frequently simulated to achieve solutions for different problems in the field of civil and environmental engineering the mesh resolution and related computational cost are primarily associated to the scale of the flow structures to be investigated nonetheless turbulence also plays an important role on computational cost since it participates in the generation shedding and support of such flow structures different mathematical models can be considered to numerically simulate the turbulence of unsteady shallow flows depending on the degree of turbulent scale resolution required depending on the adopted approach the level of accuracy required i e the range of scales that must be resolved with a low diffusive and dispersive error is different such accuracy namely the dispersive and dissipative characteristic is directly related with the numerical scheme used to discretize the equations in finite volume schemes the range of scales of turbulent motion that a numerical model can accurately resolve strongly depends on the riemann solver used via its intrinsic numerical diffusion apart from the order of accuracy and degrees of freedom of the method in this work we aim at the analysis of two well known riemann solvers in the framework of the classical shallow water equations i e considering the full convective terms and neglecting dissipation the aroe and hlls solvers an important difference between the aroe and hlls solvers is the numerical diffusion inherent to each of them this artificial diffusion combined with the mesh resolution determine the cut off scale resolved by each numerical technique for this purpose we assess the suitability of each solver by means of the analysis of the kinetic energy cascade of the numerical solution using a double shear layer configuration this analysis is combined with the examination of the analytical expression of he approximate solution for a shear wave provided by the aforementioned solvers the study herein presented allows to assess whether or not all the relevant turbulent flow structures are resolved and if the phenomenon of interest is thus accurately modeled the numerical results evidence that a diffusive profile appears at the shear line during the first steps of the simulation determining the duration of the linear regime prior to the turbulent motion the strength of this profile shown to be higher for the hlls solver is associated to the numerical diffusion of the solver the analysis of the energy cascade also agrees with this observation keywords shallow flows turbulence spectrum riemann solvers hyperbolic flux shear layer 1 introduction the shallow water equations swe are a common choice to model shallow environmental flows where the flow depth is much smaller than the horizontal scale jirka and uijttewaal 2003 uijttewaal 2014 turbulence plays a fundamental role in the hydrodynamics of such flows turbulent shallow flows are mainly governed by 2d horizontal large scale vortices caused by local variations of the velocity field apart from these 2d large vortical structures small scale 3d turbulence mainly produced by the interaction of the flowing water with the solid boundaries is also present nadaoka and yagi 1998 for the numerical resolution of turbulent shallow water flows the depth averaged reynolds averaged navier stokes da rans methodology is the most preferred approach due to its simple numerical implementation and computational efficiency rodi 1993 cea et al 2007 wu et al 2004 the da rans methodology is based on the swe with extra diffusive terms which account for the effect of the fluctuating terms i e the 3d turbulent scale of the flow in the transport of mean quantities the diffusive terms are modelled calculated by means of a closure formulation that allows to model the reynolds stress term in terms of the mean flow due to the development of the computing capabilities in the last decades more sophisticated approaches such as depth averaged urans da urans and or large eddy simulation da les have been introduced nadaoka and yagi 1998 hinterberger et al 2007 van prooijen and uijttewaal 2009 navas montilla et al 2019 these techniques aim at the resolution of the large scale vortical structures i e 2d horizontal vortices rather than modelling their effect in the mean flow by means of a closure relationship in the models discussed before small scale 3d turbulence is always modelled by a closure relationship since a depth averaged 2d approach is considered nevertheless the treatment of 2d horizontal turbulence is done differently for each of them 2d vortical structures are normally resolved when considering the da urans and da les methodologies whereas they are modelled when considering the da rans approach when aiming at the resolution of turbulence the solving capabilities i e numerical dissipation and dispersion of the numerical schemes used for the discretization of the equations determine the quality of the solution e g the range of wavenumbers which are accurately resolved a low numerically induced dissipation and dispersion is a must to guarantee an accurate resolution of a turbulent flow field with sharp gradients in the velocity field maulik and san 2018 san and kara 2015 to assess the suitability of a numerical model for the simulation of turbulence the focus must be put on the evaluation of the numerical resolution of the convective terms the methods herein presented will thus aim at the analysis of the numerical resolution of the 2d swe in their full dynamic wave neglecting all dissipative effects e g friction molecular viscosity and turbulent diffusion shock capturing finite volume schemes are amongst the most common techniques used for the resolution of the swe roe 1981 juez et al 2015 fraccarollo and toro 1995 toro 2001 they allow an accurate resolution of traveling gravity waves and they are able to exactly preserve the rankine hugoniot relations under certain conditions to satisfy equilibrium solutions this family of methods requires the computation of the so called numerical fluxes at cell interfaces which are latter used to update the solution in time such fluxes are normally computed using approximate riemann solvers such as the aroe and hlls solvers murillo and garcía navarro 2010 2012 navas montilla and murillo 2016 the difference between the aroe and hlls solvers is that the aroe solver is a complete i e it represents the full eigenstructure of the jacobian of the flux and linear solver whereas the hlls solver is an incomplete i e shock structures are solved but the contact wave is estimated and non linear solver concerning turbulence resolving capabilities the fact of using an incomplete solver such as the hlls involves the production of artificial numerical diffusion also called numerical viscosity which would eventually attenuate the turbulent fluctuations at medium and large wave numbers high frequencies therefore the hlls solver is a priori not a good candidate for the resolution of turbulent shallow water flows it is possible to find some previous work where the aroe and hlls solvers are assessed for the euler equations see for instance san and kara 2015 however an exhaustive comparison between both solvers in the framework of shallow water turbulence has not been yet presented in this work we aim at the analysis of the aroe and hlls solvers in terms of numerical diffusion for the resolution of the convective part of the classical swe the analysis is carried out for different orders of accuracy based on the weno ader approach navas montilla and murillo 2018 navas montilla et al 2019 see castro and toro 2008 vignoli et al 2008 caleffi et al 2006 for more examples using two different 2d shear layer problems as a benchmark an ideal shear layer and a turbulent double shear layer the latter adapted from san and kara 2015 consists of a homogeneous initial flow field with a discontinuity in the streamwise velocity which is perturbed using a sinusoidal first mode variation of the initial spanwise velocity the presence of such perturbation triggers a non linear turbulent evolution of the solution i e a kelvin helmholtz instability the suitability of the solvers is assessed by means of the analysis of the kinetic energy cascade of the numerical solution the energy dissipation rates in the numerical solution will be compared with the theoretical dissipation rates according to kolmogorov s theory kraichnan 1967 san and kara 2015 in the analysis herein considered 3d turbulence will be neglected regarding turbulence modelling of the unresolved 2d scales we will not consider the use of any turbulence model in order to adequately analyze the resolving capabilities of the schemes i e an infinite reynolds number will be considered 2 the mathematical model the swe are based on a 2d depth averaged and hydrostatic model suitable for free surface flows where the vertical dimension is much smaller than the longitudinal dimensions the derivation of the differential formulation of the swe is obtained by applying the reynolds transport theorem to the equation for the conservation of mass and momentum within an integration volume of infinitesimal size navas montilla et al 2019 the swe are written in matrix form as follows 1 u t f u x g u y s with 2 u h h u h v f h u h u 2 1 2 g h 2 h u v g h v h v u h v 2 1 2 g h 2 where g is the acceleration of gravity h is the water depth hu is the depth averaged unitary discharge in the x direction and hv the depth averaged unitary discharge in the y direction the longitudinal and transverse velocities u and v are depth averaged mean components in the term of the definition of the reynolds decomposition the water depth h also corresponds to a mean value the methods herein described are based on the assumption that the convective part of the system in equation 1 is hyperbolic godlewski and raviart 2013 the term s s z s f includes the source terms which involve the stress exerted by the bottom topography sz and by the bed roughness sf such sources are also called bed slope and friction source terms respectively and are written as 3 s z 0 g h d z d x g h d z d y s f 0 c f u u c f u v where u u 2 v 2 is the velocity magnitude z z x y represents the bottom topography which is fixed in time and cf is the friction coefficient computed using manning s formulation as follows 4 c f g n 2 h 1 3 note that this work aims at the evaluation of the numerical schemes for the discretization the convective terms the viscous molecular and turbulent diffusion term have not been considered in equation 1 in order to avoid any extra i e modelled diffusion and to properly examine the artificial diffusion added by the schemes 3 numerical model augmented weno ader scheme in cartesian grid let us consider the system of conservation laws in 1 2 to compose the following initial boundary value problem ibvp 5 pdes u t f g s ic u x 0 u x x ω bc u x t u ω x t x ω defined in the domain ω 0 t where ω a b c d is the spatial domain the initial condition is given by u x and the boundary condition by u ω x t the spatial domain is discretized in nx ny volume cells defined as ω ij ω such that ω i j 1 n ω i j cells are defined as 6 ω i j x i 1 2 x i 1 2 y j 1 2 y j 1 2 i 1 n x j 1 n y and cell sizes denoted by ϑ ij will be considered constant and equal to ϑ i j δ x 2 for a regular cartesian grid where δ x δ y inside each cell at time tn the conserved quantities are generally defined as cell averages as 7 u i j n 1 ϑ i j ω i j u x t n d a i 1 n x j 1 n y where d a d x d y integration of the system in 5 over the discrete domain ω ij δt where δ t t n 1 t n yields to 8 u i j n 1 u i j n 1 ϑ i j 0 δ t ω i j f g n d l d t 1 ϑ i j 0 δ t ω i j s d a d t where dl is the differential length for a cartesian grid the following fully discrete updating formula is obtained navas montilla et al 2019 9 u i j n 1 u i j n δ t δ x 2 f i 1 2 j f i 1 2 j δ t δ x 2 g i j 1 2 g i j 1 2 δ t δ x 2 s i j where f i 1 2 j and g i j 1 2 are the numerical fluxes at cell interfaces and 10 s i j 1 δ t 0 δ t x i 1 2 x i 1 2 y i 1 2 y i 1 2 s d y d x d τ is the approximation of the space time integral of the source terms the time step δt is computed dynamically according to the cfl condition to preserve the stability of the numerical solution the numerical fluxes are computed as the space time integral of the numerical fluxes over the cell edges to construct a numerical scheme of order 2 k 1 th it is sufficient to approximate such integrals using a 2 k 1 th order gaussian quadrature thus requiring k quadrature points for instance the numerical flux f i 1 2 j is computed as follows 11 f i 1 2 j δ x 2 q 1 k w q f i 1 2 j q where wq are the gaussian weights inside the interval 1 1 at the q 1 k quadrature points along the cell edge and f i 1 2 j q the numerical fluxes at each of these points computed by means of the resolution of the cauchy problem the numerical fluxes are computed solving an arbitrary order approximation of the cauchy problem at the quadrature points along cell interfaces this is given by the so called drp which is defined in the x direction for the numerical fluxes on the east and west interfaces and in the y direction for those fluxes on the north and south interfaces it is worth noting that the source term is included in the definition of the drp according to navas montilla and murillo 2016 the drp k defined in the x direction at the interface i 1 2 and quadrature point q reads as navas montilla and murillo 2016 12 u t f u x s i 1 2 u x t 0 u i j x y i 1 2 j q x 0 u i 1 j x y i 1 2 j q x 0 where u ij x y and u i 1 j x y are smooth spatial reconstructions defined using the weno method such functions are evaluated at the particular location where the drp is defined y y i 1 2 j q on the other hand s i 1 2 represents the integral of the source term at cell interfaces which only is non zero when considering geometric source terms e g bed elevation source term it is computed as 13 s i 1 2 1 δ t 0 δ t x i 1 2 x i 1 2 s d x d τ the solution for the drp in 12 is constructed using the flux expansion approach as 14 f i 1 2 j q f i 1 2 j q 0 k 1 k f i 1 2 j q k δ t k k 1 f i 1 2 j q f i 1 2 j q 0 k 1 k f i 1 2 j q k δ t k k 1 where f i 1 2 j q 0 f i 1 2 j q k f i 1 2 j q 0 and f i 1 2 j q k are computed by solving the drp k in this work the lfs aroe and lfs hlls solver will be considered navas montilla and murillo 2016 such solvers are next summarized 3 1 lfs aroe solver the lfs aroe solver is a complete linear solver that accounts for the full wave structure of the system when using the lfs aroe solver navas montilla and murillo 2016 the coefficients of 14 read as 15 f i 1 2 k f i e k m 1 n λ λ α k β k i 1 2 m e i 1 2 m k 0 k f i 1 2 k f i 1 w k m 1 n λ λ α k β k i 1 2 m e i 1 2 m k 0 k where f i e k and f i 1 w k are the left and right hand limits to the cell edge of the physical flux k 0 and their k th time derivatives α k are the wave strengths β k the source strengths and λ and e i 1 2 m the approximate wave celerities and eigenvectors defined using roe s averages roe 1981 the computation of the aforementioned quantities is detailed in navas montilla and murillo 2018 the fluxes g i j 1 2 q and g i j 1 2 q are computed analogously 3 2 lfs hlls solver the lfs hlls solver is an incomplete non linear solver that approaches the wave structure of the system by a 2 wave structure when using the lfs hlls solver navas montilla and murillo 2016 the coefficients of 14 read as 16 f i 1 2 k f i e k if λ 1 0 f i e k s u b if λ 1 0 λ 2 f i i 1 w k s i 1 2 k if λ 2 0 f i 1 2 k f i e k s i 1 2 k if λ 1 0 f i 1 w k s u b if λ 1 0 λ 2 f i i 1 w k if λ 2 0 where the hlls fluxes are given by 17 f i e k s u b λ 2 f i e k λ 1 f i 1 w k λ 1 λ 2 δ u i 1 2 k λ 1 s i 1 2 k λ 2 h i 1 2 k λ 2 λ 1 18 f i 1 w k s u b λ 2 f i e k λ 1 f i 1 w k λ 1 λ 2 δ u i 1 2 k λ 2 s i 1 2 k λ 1 h i 1 2 k λ 2 λ 1 where u i e k and u i 1 w k are the left and right hand limits to the cell edge of the conserved quantities k 0 and their k th time derivatives and h i 1 2 k a matrix related to the source term further details can be found in navas montilla and murillo 2016 4 resolution of an ideal shear layer with analytical solution 4 1 analytical solution let us consider an ideal shear layer as depicted in fig 1 defined inside the domain ω 0 l 0 h r 2 by a one dimensional flow in the streamwise direction x composed by two regions ω1 and ω2 of different velocity the shear discontinuity is located at y s h 2 the streamwise velocity is given by 19 u x y u 1 if x y ω 1 u 2 if x y ω 2 v x y 0 h x y h 0 x y ω as the water depth is constant in the whole domain and equal to h 0 the wave celerity yields to c 0 g h 0 we take in count the homogeneous swe i e bed slope friction and diffusion terms are neglected in 1 to derive the analytical solution for the shear layer problem in 19 the analytical relations between variables across the shear discontinuity at ys are provided by the rankine hugoniot rh conditions which are derived for rp in the y direction defined at the discontinuity the piecewise constant initial data for such rp are given by u 1 h 0 h 0 u 1 0 and u 2 h 0 h 0 u 2 0 generally the rh conditions read δ g s δ u where s is the velocity of the wave note that δ stands for the discrete difference operator between states 1 and 2 in this case as the shear discontinuity is stationary s 0 and the rh relations are simplified to δ g 0 yielding to 20 δ h v 0 21 δ h v 2 1 2 g h 2 0 22 δ h v u 0 from equation 21 it is verified that the flow is in equilibrium since v 1 v 2 0 and h 1 h 2 h 0 on the other hand from equation 22 it is observed that any combination of shear velocities at each side of the discontinuity u 1 and u 2 is possible this solution is admissible as the transverse velocity is zero and also due to the absence of viscosity in the equations there is no mechanism of transference of momentum across the shear layer under the aforementioned conditions and the defined regions is thus decoupled from each other this condition can be intuitively understood by remembering that the 2d swe can satisfy a free slip boundary condition between layers characterized by different flow velocity 4 2 analysis of the discrete solution 1 st order aroe and hlls schemes according to the previous section it is expected that the mixing process across the shear layer will be first determined by the choice of the computational mesh only when cell interfaces are aligned with the shear layer or perpendicular to it the theoretical equilibrium can be satisfied otherwise the decomposition of the velocity onto the local basis relative to the cell interface will enforce the presence of a non zero velocity in the direction normal to the interface additionally the mixing process will be determined by the choice of the numerical flux function even for cartesian grids a poor resolution of the shear waves across the interface may eventually produce an intense diffusive mixing across the shear layer only when using a riemann solver that includes the contact discontinuity linked to the shear wave in the internal structure of the approximate solution such as the aroe solver the exact equilibrium is satisfied in this section the expressions of the shear momentum numerical flux provided by the 1 st order aroe and hlls schemes in cartesian grid are derived and compared the presence of source terms in the equations will be hereafter neglected these numerical schemes will be thus reduced to the traditional roe and hll schemes the numerical fluxes obtained by solving the rp in the y direction with the discontinuity located at y j 1 2 h 2 g j 1 2 and g j 1 2 will coincide and will be hereafter denoted by g j 1 2 note that the first and third components of the numerical flux will be zero as the normal velocity across the interface is nil and the water depth is constant hence only the shear component of the momentum flux hereafter denoted by h v u j 1 2 will be studied the dependence upon the x direction is omitted for the sake of clarity the approximation of the shear momentum numerical flux h v u j 1 2 for the resolution of the x split rp defined at the shear discontinuity with the aroe solver is constructed using eq 15 as follows 23 h v u j 1 2 h v u j 1 v u δ h 2 v δ h δ h v 2 c u δ h u u δ h c u δ h 2 v δ h δ h v 2 c u inserting v 0 and δ h 0 in eq 23 yields to 24 h v u j 1 2 h v u j 1 0 such way no shear momentum will be transferred across the interface in the numerical solution and the discrete equilibrium will be preserved the approximation of the shear momentum numerical flux h v u j 1 2 based on the hlls solver is constructed using eq 18 as follows 25 h v u j 1 2 c h v u j v c h v u j 1 c v c h u j 1 h u j c v c inserting v 0 c c 0 and h v u j h v u j 1 0 in eq 25 yields to 26 h v u j 1 2 c 0 h u j 1 h u j 2 note that h v u j 1 2 is not nil and a viscous effect will appear across the shear layer contrary to what was stated by the aroe solver in 24 let us consider the updating scheme in 9 for hu j in 1 st order version the scheme will be defined locally in the y direction and the only contributions to the cell average will be the shear fluxes in 26 dependence upon the x coordinate will be omitted note that the solution in the x direction is uniform and the fluxes in this direction are in equilibrium the scheme can be written as 27 h u j n 1 h u j n δ t δ y h v u j 1 2 h v u j 1 2 and inserting 26 in 27 the following equation is obtained 28 h u j n 1 h u j n δ t δ y c 0 2 h u j 1 2 h u j h u j 1 rearranging it yields to 29 h u j n 1 h u j n δ t c 0 δ y 2 h u j 1 2 h u j h u j 1 δ y 2 which represents a diffusion equation for the transverse discharge the equivalent partial differential equation for the transverse discharge in the cartesian reference frame can be written using taylor power series expansion in time of h u j n 1 h u j 1 and h u j 1 and the cauchy kowalevsky theorem leading to 30 h u t c 0 δ y 2 2 h u y 2 δ y 2 12 4 h u y 4 c δ y δ t 2 4 4 h u y 4 and assuming h as constant if we discard the higher order truncation error terms the equivalent pde can be written as 31 h u t ν h l l s 2 h u y 2 where 32 ν h l l s c 0 δ y 2 is the numerical viscosity inherent to the hlls solver 5 computational results 5 1 resolution of an ideal shear layer let us consider a numerical test case given by a shear layer configuration as defined in section 4 the domain is defined by h l 1 and the initial condition is given by u 1 1 u 2 1 and h 0 1 according to 19 the boundary conditions are considered solid walls friction bed slope and diffusion both turbulent and molecular are neglected based on such boundary and initial conditions the analytical solution for the streamwise unitary discharge of this ideal shear layer is given by 33 h u y t 1 if 0 y 0 5 1 if 0 5 y 1 according to the previous findings the aroe solver should provide the exact solution in 33 conversely the numerical solution of the 1 st order hlls solver is given by the equivalent pde in 31 the analytical solution of 31 is given by 34 h u y t 5 10 1 e r f y 0 5 2 ν t where ν is computed using 32 in fig 2 the exact and numerical solutions provided by the aroe and hlls solver are compared the numerical solution at t 0 2 is depicted for grid sizes δ x 0 025 δ x 0 0125 δ x 0 0065 δ x 0 001 the analytical solution of the equivalent pde for the 1 st order hlls solver is also depicted for such grid sizes it is evidenced that the aroe solver is able to maintain the initial equilibrium providing the exact solution whereas the hlls solver introduces artificial viscosity in the shear waves as indicated in section 4 2 it is verified that the numerical solution for the 1 st order hlls solver matches the analytical solution in 34 as the order of the numerical scheme is increased the numerical diffusion introduced by such scheme is reduced 5 2 numerical evaluation of the solvers using a double shear layer 5 2 1 problem set up the problem considered here is adapted from san and kara 2015 and is composed of a double shear layer the flow is typically unstable all through the shear layer and it leads to the generation of turbulent flow structures such as the kelvin helmholtz instabilities such kelvin helmholtz instabilities convert the initial linear perturbations into complex turbulent structures that ultimately determine the mass exchange across the different jets layers in the flow san and kara 2015 the computation of this flow is of high complexity and it requires a high accurate resolution of the fluxes in order to capture the generation and evolution of vortices at different scales the double shear layer problem is herein used as a benchmark to evaluate the numerical diffusion of the proposed solvers aroe and hlls the computational domain is given by ω 0 1 0 1 the initial condition is defined as h x y 1 0 and with 35 u x y u s if 0 25 x 0 75 u s otherwise which corresponds to a double shear layer as depicted in fig 3 note that us is the layer flow velocity yet to be defined for each case cyclic boundary conditions are considered in all cases and a 1 st mode sinusoidal perturbation is introduced as san and kara 2015 36 v x y 0 01 sin 2 π x the solution first evolves following a linear regime which breaks down at a certain time tb leading to a nonlinear evolution skinner and aaron 2019 note that the choice of a shear layer composed of two regions with equal velocity magnitude and opposite sign helps to parametrize the strength of the shear layer in terms of a unique parameter us furthermore under this configuration the overall advection of the 2d vortices in the streamwise direction is negligible and their centers remain steady with the flow swirling around them this flow configuration was adopted in san and kara 2015 and in other works therein referenced 5 2 2 computation of the numerical energy cascade the analysis of the kinetic energy cascade in 2d turbulent flows is a powerful tool to determine whether the selected numerical scheme can provide a trustworthy representation of the turbulence spectrum theoretically the 2d kinetic energy cascade in the inertial range for the system in 1 is of the form e k k 3 where e is the kinetic energy function and k the wave number san and kara 2015 sometimes an inverse cascade can also be observed at small wave numbers being in this case e k k 5 3 note that this inverse cascade may not be noticeable if the source of excitation of the flow is not triggered at a particular wave number to compute the numerical energy cascade the velocity components are first transformed to the frequency domain using the fast fourier transform fft algorithm yielding to u kx ky and v kx ky with kx and ky the wave numbers in each cartesian direction note that the transformed quantities must be properly normalized using the number of samples number of cells then the kinetic energy in the frequency domain is computed as e k x k y 0 5 u k x k y 2 v k x k y 2 by an appropriate change to polar coordinates we can express e kx ky as e k θ where k is the composite wave number computed as k s q r t k x 2 k y 2 and θ is the phase to obtain a 1d representation of the energy cascade a phase average of the kinetic energy is computed as follows 37 e k 1 2 π 0 2 π e k θ d θ 5 2 3 case 1 homogeneous double shear layer this first case considers a homogeneous double shear layer problem i e z x y 0 two configurations defined by a different us velocity and considering the initial perturbation in eq 36 are studied and displayed in table 1 the numerical solution is computed using the 1 st order and 3 rd order aroe and hlls weno ader scheme with a cfl number of 0 45 four different computational grids composed of 125 125 250 250 500 500 and 1000 1000 cells are used the simulation time is set to t 5 s satisfying t tb to determine tb in the numerical solution a dimensionless normalized transverse velocity is defined as v max us where v max max v i j is the maximum value of the transverse velocity in the domain configuration 1 a is used as a benchmark to assess the impact of the numerical scheme in the length of the linear regime i e tb as well as the turbulent patterns generated afterward the evolution in time of v max us computed by both schemes in all the grids for 1 st and 3 rd order of accuracy is presented in fig 4 furthermore the numerical values of tb for the 3 rd order aroe and hlls shemes are presented in table 2 for the highest resolution grid and 3 rd order aroe and hlls solvers the spatial vorticity distribution is depicted in fig 5 it is observed that the choice of the order of accuracy i e 1 st order or 3 rd order the riemann solver i e aroe or hlls and the computational grid have a strong impact on the length of the linear regime and subsequent transition to the turbulent regime fig 4 shows that tb is reduced as the order is increased and or the grid is refined furthermore it is observed that the hlls solver which does not account for the shear wave yields to longer tb than the aroe scheme the reason is that the hlls solver produces a larger viscous profile at initial times see fig 5 as discussed in the previous section which prevents the initial perturbation from triggering the nonlinear regime at earlier times it is worth pointing out that in the limit of the mesh resolution i e a very fine grid tb should tend to zero as the model does not include any physical viscosity that would enforce a viscous profile independent of the scheme and grid and an asymptotic tb 0 when analyzing the evolution of tb with respect to the mesh size the numerical results reveal that the parameter δtb defined as the difference of tb in two consecutive grids see table 2 grows proportionally to the mesh refinement ratio that is δtb doubles as the cell size doubles it must be noted that this relation is independent to the choice of the solver the ability of the scheme to resolve turbulent flows is assessed by comparing the computed energy cascade with the theoretical energy cascade san and kara 2015 numerical errors e g diffusion and dispersion errors are known to cause a deviation of the computed energy cascade from the theoretical spectrum e g a different tendency slope figs 6 and 7 report the computed energy cascades for cases 1 a and 1 b at t 3 s results were obtained with the 1 st order and 3 rd order aroe and hlls weno ader numerical schemes using the same grids as in the previous sections 125 125 250 250 500 500 and 1000 1000 note that e k k 3 is plotted instead of e k in order to better examine the region of the spectrum where e k k 3 such way the energy spectrum in the k 3 region must appear flat and helps in the detection of any energy deviation the numerical results point out several details i a 1 st order scheme is not adequate for the simulation of 2d turbulent flows of this type since the computed energy cascade shows a strong decay at relatively low wavenumbers ii the difference between the numerical solutions provided by the aroe and hlls solver is remarkable when using a 1 st order scheme while both solvers become more alike when moving to 3 rd order of accuracy iii considering the 3 rd order schemes those using the aroe solver are able to accurately reproduce a wider region of the spectrum i e they follow a k 3 tendency those based on the hlls solver exhibit a higher dissipation rate even at low wave numbers looking at the energy cascades computed with the finest mesh it is observed that the energy dissipation increases from t 3 s to t 5 s in case 1 a the opposite behavior is noticed in case 1 b this may be due to the presence of shock waves in case 1 b for such case the initial longitudinal velocity of the flow is sufficiently high to develop a turbulent pattern with the presence of shock waves which contain energy at high frequencies such shock waves are clearly observed in the water surface elevation at t 5 as depicted in fig 8 where the solution for cases 1 a and 1 b are compared vortices observed in fig 8 are physically explained through the mass and angular momentum conservation equations angular momentum is the product of mass velocity and the distance to the axis of rotation thus fluid particles close to the axis of rotation have to spin faster to compensate the smaller distance the local increasing velocity is in turn compensated with a lower water level to guarantee mass conservation the magnitude of such vortices and local water level depressions are related with the joint combination of initial and boundary conditions which are able to trigger some initial angular momentum the analysis of the energy cascade is key to determine the accuracy of the numerical solution observing fig 5 we could initially consider that the hlls solver is able to well reproduce the dynamics of the shear layer two main counter rotating vortices are generated in the computational domain however the energy cascade analysis shows the flaws of such superficial analysis since the flow energy is not dissipated at the physical rate at which it should do it the proposed model implemented with a hlls solver does resolve a lower range of turbulent scales than the version based on the aroe solver it can only capture the dominant low wave numbers i e the ones which correspond to the coherent vortex shedding based on the numerical results obtained for test cases 1 a and 1 b the cutoff wavenumbers for the 3 rd order and with the aroe and hlls weno ader schemes would be in the range k 10 20 and k 4 6 respectively such wavenumbers are related to the smallest flow features that the scheme can resolve and are of the order of the smallest structures observed in fig 5 the smallest structure size resolved can be estimated transforming the cutoff wavenumbers into a length l l 2 k c with l 1 the length of the domain for the aroe and hlls solvers such cutoff length yields to l 0 025 and l 0 1 the relation 1 4 between both cutoff lengths is approximately observed between the width of the viscous shear layers displayed in fig 5 5 2 4 case 2 double shear layer with bed variations a double shear layer including a smoothly varying bed elevation is considered in this section the initial condition in this case is set as h x y z x y 1 where the bed level is given by a sinusoidal variation 38 z x y 0 1 cos 2 π y 0 1 and v x y is given in eq 36 the initial condition for the streamwise velocity is given by u s 1 0 m s the aim of this test is to assess the performance of the numerical scheme in the resolution of 2d turbulence including the presence of bed source terms the given bed profile is smooth and it does not present sharp gradients therefore it is expected that the energy spectrum of the numerical solution of this case and the case 1 b same configuration without bed variation should be alike the correct discretization of the bed source term should not provide any artificial deviation of the energy spectrum decay the numerical solution at t 5 s computed using the 3 rd order aroe weno ader scheme and 1000 1000 cells is depicted in fig 9 two large coherent horizontal vortices are observed on both sides of the domain a comparison of the energy spectrum of the numerical solution of this case and of the case 1 b is presented in fig 10 the energy distribution within the palette of wavenumbers is very similar for both cases showing that the numerical scheme is not adding any significant extra dissipation or spurious high frequency modes 6 conclusions a depth averaged 2d shallow water hydrodynamic model based on a high order approximation of the numerical solution is evaluated for the resolution of unsteady turbulent shear layers within this framework two numerical solvers with different orders of accuracy are studied and implemented to assess their performance for the resolution of viscous and turbulent shear flows the accuracy of the solvers is assessed by means of a the analytical study of the numerical diffusion introduced by the solver associated to the resolution of the shear wave and b the analysis of the kinetic energy cascade of the numerical solution which is found to be related to other relevant indicators the size of the initial viscous profile and the time to develop the turbulent regime the results herein reported show that the ability of modeling a coherent shedding of vortices does not imply that the numerical solver is dissipating energy at the level at which it should be in order to explain the effect of the numerical diffusion in the resolution of turbulent shear flows an ideal case that considers an unperturbed shear flow configuration is first considered both analytical and numerical results evidence that the hlls solver incorporates an additional source of artificial dissipation due to the poorly resolved contact wave under ideal shear flow conditions this issue is related to the generation of a wide diffusive profile across the discontinuity which is proportional to the cell size when considering turbulent i e perturbed shear flow cases it is observed that the generation of such viscous profile across the discontinuity in the beginning of the simulation prevents the perturbed flow from a fast evolution to the turbulent regime this is equivalent to having a very low reynolds number note that this reynolds number is based on numerical diffusion only as the solution evolves in time the effect of the perturbation grows triggering the nonlinear transition to the turbulent regime i e the numerically induced reynolds number increases the time at which the transition from linear to turbulent regime is produced was measured showing that it is shorter for aroe based schemes than for the hlls versions as expected this is because the aroe solver induce a much narrower viscous profile than the hlls solver furthermore the spectrum of the turbulent flow field provided by the numerical solution was analyzed by means of the kinetic energy cascade it is observed that the schemes based on the aroe solver allows to properly resolve a wider range of the spectrum than those based on the hlls solver a close relation between the cutoff wavenumber the width of the initial viscous profile and the time for the generation of the turbulent regime was found the presence of smooth bed variations is analyzed in terms of the energy spectrum it is shown that the proposed scheme does not lead to spurious features in the numerical solution in presence of bed variation the numerical results evidence that the 3 rd order aroe weno ader scheme can be selected as discretization method in the framework of da urans simulation when aiming at resolving only the larger horizontal vortical structures on the other hand the hlls version of the scheme introduces too much diffusion even in the larger scales in this case a higher order of accuracy or a higher mesh resolution should be used the use of a 1 st order of accuracy for this purpose is proved to be undesirable credit authorship contribution statement a navas montilla conceptualization methodology software writing original draft c juez conceptualization methodology validation writing original draft declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was funded bythe spanish ministry of science and innovation under the research project pgc2018 094341 b i00 navas montilla by the gobierno de aragón through fondo social europeo feder 2014 2020 construyendo europa desde aragón navas montilla and by the h2020 msca if 2018 programme marie sklodowska curie actions of the european union under rea grant agreement number 834329 sediland juez the first author would like to thank dr omer san san fluid dynamics group school of mechanical and aerospace engineering oklahoma state university for some fruitful discussions about the topic and the detailed clarifications about his work 
551,current computational power allows the modelling of complex time dependent flows in particular shallow water flows are frequently simulated to achieve solutions for different problems in the field of civil and environmental engineering the mesh resolution and related computational cost are primarily associated to the scale of the flow structures to be investigated nonetheless turbulence also plays an important role on computational cost since it participates in the generation shedding and support of such flow structures different mathematical models can be considered to numerically simulate the turbulence of unsteady shallow flows depending on the degree of turbulent scale resolution required depending on the adopted approach the level of accuracy required i e the range of scales that must be resolved with a low diffusive and dispersive error is different such accuracy namely the dispersive and dissipative characteristic is directly related with the numerical scheme used to discretize the equations in finite volume schemes the range of scales of turbulent motion that a numerical model can accurately resolve strongly depends on the riemann solver used via its intrinsic numerical diffusion apart from the order of accuracy and degrees of freedom of the method in this work we aim at the analysis of two well known riemann solvers in the framework of the classical shallow water equations i e considering the full convective terms and neglecting dissipation the aroe and hlls solvers an important difference between the aroe and hlls solvers is the numerical diffusion inherent to each of them this artificial diffusion combined with the mesh resolution determine the cut off scale resolved by each numerical technique for this purpose we assess the suitability of each solver by means of the analysis of the kinetic energy cascade of the numerical solution using a double shear layer configuration this analysis is combined with the examination of the analytical expression of he approximate solution for a shear wave provided by the aforementioned solvers the study herein presented allows to assess whether or not all the relevant turbulent flow structures are resolved and if the phenomenon of interest is thus accurately modeled the numerical results evidence that a diffusive profile appears at the shear line during the first steps of the simulation determining the duration of the linear regime prior to the turbulent motion the strength of this profile shown to be higher for the hlls solver is associated to the numerical diffusion of the solver the analysis of the energy cascade also agrees with this observation keywords shallow flows turbulence spectrum riemann solvers hyperbolic flux shear layer 1 introduction the shallow water equations swe are a common choice to model shallow environmental flows where the flow depth is much smaller than the horizontal scale jirka and uijttewaal 2003 uijttewaal 2014 turbulence plays a fundamental role in the hydrodynamics of such flows turbulent shallow flows are mainly governed by 2d horizontal large scale vortices caused by local variations of the velocity field apart from these 2d large vortical structures small scale 3d turbulence mainly produced by the interaction of the flowing water with the solid boundaries is also present nadaoka and yagi 1998 for the numerical resolution of turbulent shallow water flows the depth averaged reynolds averaged navier stokes da rans methodology is the most preferred approach due to its simple numerical implementation and computational efficiency rodi 1993 cea et al 2007 wu et al 2004 the da rans methodology is based on the swe with extra diffusive terms which account for the effect of the fluctuating terms i e the 3d turbulent scale of the flow in the transport of mean quantities the diffusive terms are modelled calculated by means of a closure formulation that allows to model the reynolds stress term in terms of the mean flow due to the development of the computing capabilities in the last decades more sophisticated approaches such as depth averaged urans da urans and or large eddy simulation da les have been introduced nadaoka and yagi 1998 hinterberger et al 2007 van prooijen and uijttewaal 2009 navas montilla et al 2019 these techniques aim at the resolution of the large scale vortical structures i e 2d horizontal vortices rather than modelling their effect in the mean flow by means of a closure relationship in the models discussed before small scale 3d turbulence is always modelled by a closure relationship since a depth averaged 2d approach is considered nevertheless the treatment of 2d horizontal turbulence is done differently for each of them 2d vortical structures are normally resolved when considering the da urans and da les methodologies whereas they are modelled when considering the da rans approach when aiming at the resolution of turbulence the solving capabilities i e numerical dissipation and dispersion of the numerical schemes used for the discretization of the equations determine the quality of the solution e g the range of wavenumbers which are accurately resolved a low numerically induced dissipation and dispersion is a must to guarantee an accurate resolution of a turbulent flow field with sharp gradients in the velocity field maulik and san 2018 san and kara 2015 to assess the suitability of a numerical model for the simulation of turbulence the focus must be put on the evaluation of the numerical resolution of the convective terms the methods herein presented will thus aim at the analysis of the numerical resolution of the 2d swe in their full dynamic wave neglecting all dissipative effects e g friction molecular viscosity and turbulent diffusion shock capturing finite volume schemes are amongst the most common techniques used for the resolution of the swe roe 1981 juez et al 2015 fraccarollo and toro 1995 toro 2001 they allow an accurate resolution of traveling gravity waves and they are able to exactly preserve the rankine hugoniot relations under certain conditions to satisfy equilibrium solutions this family of methods requires the computation of the so called numerical fluxes at cell interfaces which are latter used to update the solution in time such fluxes are normally computed using approximate riemann solvers such as the aroe and hlls solvers murillo and garcía navarro 2010 2012 navas montilla and murillo 2016 the difference between the aroe and hlls solvers is that the aroe solver is a complete i e it represents the full eigenstructure of the jacobian of the flux and linear solver whereas the hlls solver is an incomplete i e shock structures are solved but the contact wave is estimated and non linear solver concerning turbulence resolving capabilities the fact of using an incomplete solver such as the hlls involves the production of artificial numerical diffusion also called numerical viscosity which would eventually attenuate the turbulent fluctuations at medium and large wave numbers high frequencies therefore the hlls solver is a priori not a good candidate for the resolution of turbulent shallow water flows it is possible to find some previous work where the aroe and hlls solvers are assessed for the euler equations see for instance san and kara 2015 however an exhaustive comparison between both solvers in the framework of shallow water turbulence has not been yet presented in this work we aim at the analysis of the aroe and hlls solvers in terms of numerical diffusion for the resolution of the convective part of the classical swe the analysis is carried out for different orders of accuracy based on the weno ader approach navas montilla and murillo 2018 navas montilla et al 2019 see castro and toro 2008 vignoli et al 2008 caleffi et al 2006 for more examples using two different 2d shear layer problems as a benchmark an ideal shear layer and a turbulent double shear layer the latter adapted from san and kara 2015 consists of a homogeneous initial flow field with a discontinuity in the streamwise velocity which is perturbed using a sinusoidal first mode variation of the initial spanwise velocity the presence of such perturbation triggers a non linear turbulent evolution of the solution i e a kelvin helmholtz instability the suitability of the solvers is assessed by means of the analysis of the kinetic energy cascade of the numerical solution the energy dissipation rates in the numerical solution will be compared with the theoretical dissipation rates according to kolmogorov s theory kraichnan 1967 san and kara 2015 in the analysis herein considered 3d turbulence will be neglected regarding turbulence modelling of the unresolved 2d scales we will not consider the use of any turbulence model in order to adequately analyze the resolving capabilities of the schemes i e an infinite reynolds number will be considered 2 the mathematical model the swe are based on a 2d depth averaged and hydrostatic model suitable for free surface flows where the vertical dimension is much smaller than the longitudinal dimensions the derivation of the differential formulation of the swe is obtained by applying the reynolds transport theorem to the equation for the conservation of mass and momentum within an integration volume of infinitesimal size navas montilla et al 2019 the swe are written in matrix form as follows 1 u t f u x g u y s with 2 u h h u h v f h u h u 2 1 2 g h 2 h u v g h v h v u h v 2 1 2 g h 2 where g is the acceleration of gravity h is the water depth hu is the depth averaged unitary discharge in the x direction and hv the depth averaged unitary discharge in the y direction the longitudinal and transverse velocities u and v are depth averaged mean components in the term of the definition of the reynolds decomposition the water depth h also corresponds to a mean value the methods herein described are based on the assumption that the convective part of the system in equation 1 is hyperbolic godlewski and raviart 2013 the term s s z s f includes the source terms which involve the stress exerted by the bottom topography sz and by the bed roughness sf such sources are also called bed slope and friction source terms respectively and are written as 3 s z 0 g h d z d x g h d z d y s f 0 c f u u c f u v where u u 2 v 2 is the velocity magnitude z z x y represents the bottom topography which is fixed in time and cf is the friction coefficient computed using manning s formulation as follows 4 c f g n 2 h 1 3 note that this work aims at the evaluation of the numerical schemes for the discretization the convective terms the viscous molecular and turbulent diffusion term have not been considered in equation 1 in order to avoid any extra i e modelled diffusion and to properly examine the artificial diffusion added by the schemes 3 numerical model augmented weno ader scheme in cartesian grid let us consider the system of conservation laws in 1 2 to compose the following initial boundary value problem ibvp 5 pdes u t f g s ic u x 0 u x x ω bc u x t u ω x t x ω defined in the domain ω 0 t where ω a b c d is the spatial domain the initial condition is given by u x and the boundary condition by u ω x t the spatial domain is discretized in nx ny volume cells defined as ω ij ω such that ω i j 1 n ω i j cells are defined as 6 ω i j x i 1 2 x i 1 2 y j 1 2 y j 1 2 i 1 n x j 1 n y and cell sizes denoted by ϑ ij will be considered constant and equal to ϑ i j δ x 2 for a regular cartesian grid where δ x δ y inside each cell at time tn the conserved quantities are generally defined as cell averages as 7 u i j n 1 ϑ i j ω i j u x t n d a i 1 n x j 1 n y where d a d x d y integration of the system in 5 over the discrete domain ω ij δt where δ t t n 1 t n yields to 8 u i j n 1 u i j n 1 ϑ i j 0 δ t ω i j f g n d l d t 1 ϑ i j 0 δ t ω i j s d a d t where dl is the differential length for a cartesian grid the following fully discrete updating formula is obtained navas montilla et al 2019 9 u i j n 1 u i j n δ t δ x 2 f i 1 2 j f i 1 2 j δ t δ x 2 g i j 1 2 g i j 1 2 δ t δ x 2 s i j where f i 1 2 j and g i j 1 2 are the numerical fluxes at cell interfaces and 10 s i j 1 δ t 0 δ t x i 1 2 x i 1 2 y i 1 2 y i 1 2 s d y d x d τ is the approximation of the space time integral of the source terms the time step δt is computed dynamically according to the cfl condition to preserve the stability of the numerical solution the numerical fluxes are computed as the space time integral of the numerical fluxes over the cell edges to construct a numerical scheme of order 2 k 1 th it is sufficient to approximate such integrals using a 2 k 1 th order gaussian quadrature thus requiring k quadrature points for instance the numerical flux f i 1 2 j is computed as follows 11 f i 1 2 j δ x 2 q 1 k w q f i 1 2 j q where wq are the gaussian weights inside the interval 1 1 at the q 1 k quadrature points along the cell edge and f i 1 2 j q the numerical fluxes at each of these points computed by means of the resolution of the cauchy problem the numerical fluxes are computed solving an arbitrary order approximation of the cauchy problem at the quadrature points along cell interfaces this is given by the so called drp which is defined in the x direction for the numerical fluxes on the east and west interfaces and in the y direction for those fluxes on the north and south interfaces it is worth noting that the source term is included in the definition of the drp according to navas montilla and murillo 2016 the drp k defined in the x direction at the interface i 1 2 and quadrature point q reads as navas montilla and murillo 2016 12 u t f u x s i 1 2 u x t 0 u i j x y i 1 2 j q x 0 u i 1 j x y i 1 2 j q x 0 where u ij x y and u i 1 j x y are smooth spatial reconstructions defined using the weno method such functions are evaluated at the particular location where the drp is defined y y i 1 2 j q on the other hand s i 1 2 represents the integral of the source term at cell interfaces which only is non zero when considering geometric source terms e g bed elevation source term it is computed as 13 s i 1 2 1 δ t 0 δ t x i 1 2 x i 1 2 s d x d τ the solution for the drp in 12 is constructed using the flux expansion approach as 14 f i 1 2 j q f i 1 2 j q 0 k 1 k f i 1 2 j q k δ t k k 1 f i 1 2 j q f i 1 2 j q 0 k 1 k f i 1 2 j q k δ t k k 1 where f i 1 2 j q 0 f i 1 2 j q k f i 1 2 j q 0 and f i 1 2 j q k are computed by solving the drp k in this work the lfs aroe and lfs hlls solver will be considered navas montilla and murillo 2016 such solvers are next summarized 3 1 lfs aroe solver the lfs aroe solver is a complete linear solver that accounts for the full wave structure of the system when using the lfs aroe solver navas montilla and murillo 2016 the coefficients of 14 read as 15 f i 1 2 k f i e k m 1 n λ λ α k β k i 1 2 m e i 1 2 m k 0 k f i 1 2 k f i 1 w k m 1 n λ λ α k β k i 1 2 m e i 1 2 m k 0 k where f i e k and f i 1 w k are the left and right hand limits to the cell edge of the physical flux k 0 and their k th time derivatives α k are the wave strengths β k the source strengths and λ and e i 1 2 m the approximate wave celerities and eigenvectors defined using roe s averages roe 1981 the computation of the aforementioned quantities is detailed in navas montilla and murillo 2018 the fluxes g i j 1 2 q and g i j 1 2 q are computed analogously 3 2 lfs hlls solver the lfs hlls solver is an incomplete non linear solver that approaches the wave structure of the system by a 2 wave structure when using the lfs hlls solver navas montilla and murillo 2016 the coefficients of 14 read as 16 f i 1 2 k f i e k if λ 1 0 f i e k s u b if λ 1 0 λ 2 f i i 1 w k s i 1 2 k if λ 2 0 f i 1 2 k f i e k s i 1 2 k if λ 1 0 f i 1 w k s u b if λ 1 0 λ 2 f i i 1 w k if λ 2 0 where the hlls fluxes are given by 17 f i e k s u b λ 2 f i e k λ 1 f i 1 w k λ 1 λ 2 δ u i 1 2 k λ 1 s i 1 2 k λ 2 h i 1 2 k λ 2 λ 1 18 f i 1 w k s u b λ 2 f i e k λ 1 f i 1 w k λ 1 λ 2 δ u i 1 2 k λ 2 s i 1 2 k λ 1 h i 1 2 k λ 2 λ 1 where u i e k and u i 1 w k are the left and right hand limits to the cell edge of the conserved quantities k 0 and their k th time derivatives and h i 1 2 k a matrix related to the source term further details can be found in navas montilla and murillo 2016 4 resolution of an ideal shear layer with analytical solution 4 1 analytical solution let us consider an ideal shear layer as depicted in fig 1 defined inside the domain ω 0 l 0 h r 2 by a one dimensional flow in the streamwise direction x composed by two regions ω1 and ω2 of different velocity the shear discontinuity is located at y s h 2 the streamwise velocity is given by 19 u x y u 1 if x y ω 1 u 2 if x y ω 2 v x y 0 h x y h 0 x y ω as the water depth is constant in the whole domain and equal to h 0 the wave celerity yields to c 0 g h 0 we take in count the homogeneous swe i e bed slope friction and diffusion terms are neglected in 1 to derive the analytical solution for the shear layer problem in 19 the analytical relations between variables across the shear discontinuity at ys are provided by the rankine hugoniot rh conditions which are derived for rp in the y direction defined at the discontinuity the piecewise constant initial data for such rp are given by u 1 h 0 h 0 u 1 0 and u 2 h 0 h 0 u 2 0 generally the rh conditions read δ g s δ u where s is the velocity of the wave note that δ stands for the discrete difference operator between states 1 and 2 in this case as the shear discontinuity is stationary s 0 and the rh relations are simplified to δ g 0 yielding to 20 δ h v 0 21 δ h v 2 1 2 g h 2 0 22 δ h v u 0 from equation 21 it is verified that the flow is in equilibrium since v 1 v 2 0 and h 1 h 2 h 0 on the other hand from equation 22 it is observed that any combination of shear velocities at each side of the discontinuity u 1 and u 2 is possible this solution is admissible as the transverse velocity is zero and also due to the absence of viscosity in the equations there is no mechanism of transference of momentum across the shear layer under the aforementioned conditions and the defined regions is thus decoupled from each other this condition can be intuitively understood by remembering that the 2d swe can satisfy a free slip boundary condition between layers characterized by different flow velocity 4 2 analysis of the discrete solution 1 st order aroe and hlls schemes according to the previous section it is expected that the mixing process across the shear layer will be first determined by the choice of the computational mesh only when cell interfaces are aligned with the shear layer or perpendicular to it the theoretical equilibrium can be satisfied otherwise the decomposition of the velocity onto the local basis relative to the cell interface will enforce the presence of a non zero velocity in the direction normal to the interface additionally the mixing process will be determined by the choice of the numerical flux function even for cartesian grids a poor resolution of the shear waves across the interface may eventually produce an intense diffusive mixing across the shear layer only when using a riemann solver that includes the contact discontinuity linked to the shear wave in the internal structure of the approximate solution such as the aroe solver the exact equilibrium is satisfied in this section the expressions of the shear momentum numerical flux provided by the 1 st order aroe and hlls schemes in cartesian grid are derived and compared the presence of source terms in the equations will be hereafter neglected these numerical schemes will be thus reduced to the traditional roe and hll schemes the numerical fluxes obtained by solving the rp in the y direction with the discontinuity located at y j 1 2 h 2 g j 1 2 and g j 1 2 will coincide and will be hereafter denoted by g j 1 2 note that the first and third components of the numerical flux will be zero as the normal velocity across the interface is nil and the water depth is constant hence only the shear component of the momentum flux hereafter denoted by h v u j 1 2 will be studied the dependence upon the x direction is omitted for the sake of clarity the approximation of the shear momentum numerical flux h v u j 1 2 for the resolution of the x split rp defined at the shear discontinuity with the aroe solver is constructed using eq 15 as follows 23 h v u j 1 2 h v u j 1 v u δ h 2 v δ h δ h v 2 c u δ h u u δ h c u δ h 2 v δ h δ h v 2 c u inserting v 0 and δ h 0 in eq 23 yields to 24 h v u j 1 2 h v u j 1 0 such way no shear momentum will be transferred across the interface in the numerical solution and the discrete equilibrium will be preserved the approximation of the shear momentum numerical flux h v u j 1 2 based on the hlls solver is constructed using eq 18 as follows 25 h v u j 1 2 c h v u j v c h v u j 1 c v c h u j 1 h u j c v c inserting v 0 c c 0 and h v u j h v u j 1 0 in eq 25 yields to 26 h v u j 1 2 c 0 h u j 1 h u j 2 note that h v u j 1 2 is not nil and a viscous effect will appear across the shear layer contrary to what was stated by the aroe solver in 24 let us consider the updating scheme in 9 for hu j in 1 st order version the scheme will be defined locally in the y direction and the only contributions to the cell average will be the shear fluxes in 26 dependence upon the x coordinate will be omitted note that the solution in the x direction is uniform and the fluxes in this direction are in equilibrium the scheme can be written as 27 h u j n 1 h u j n δ t δ y h v u j 1 2 h v u j 1 2 and inserting 26 in 27 the following equation is obtained 28 h u j n 1 h u j n δ t δ y c 0 2 h u j 1 2 h u j h u j 1 rearranging it yields to 29 h u j n 1 h u j n δ t c 0 δ y 2 h u j 1 2 h u j h u j 1 δ y 2 which represents a diffusion equation for the transverse discharge the equivalent partial differential equation for the transverse discharge in the cartesian reference frame can be written using taylor power series expansion in time of h u j n 1 h u j 1 and h u j 1 and the cauchy kowalevsky theorem leading to 30 h u t c 0 δ y 2 2 h u y 2 δ y 2 12 4 h u y 4 c δ y δ t 2 4 4 h u y 4 and assuming h as constant if we discard the higher order truncation error terms the equivalent pde can be written as 31 h u t ν h l l s 2 h u y 2 where 32 ν h l l s c 0 δ y 2 is the numerical viscosity inherent to the hlls solver 5 computational results 5 1 resolution of an ideal shear layer let us consider a numerical test case given by a shear layer configuration as defined in section 4 the domain is defined by h l 1 and the initial condition is given by u 1 1 u 2 1 and h 0 1 according to 19 the boundary conditions are considered solid walls friction bed slope and diffusion both turbulent and molecular are neglected based on such boundary and initial conditions the analytical solution for the streamwise unitary discharge of this ideal shear layer is given by 33 h u y t 1 if 0 y 0 5 1 if 0 5 y 1 according to the previous findings the aroe solver should provide the exact solution in 33 conversely the numerical solution of the 1 st order hlls solver is given by the equivalent pde in 31 the analytical solution of 31 is given by 34 h u y t 5 10 1 e r f y 0 5 2 ν t where ν is computed using 32 in fig 2 the exact and numerical solutions provided by the aroe and hlls solver are compared the numerical solution at t 0 2 is depicted for grid sizes δ x 0 025 δ x 0 0125 δ x 0 0065 δ x 0 001 the analytical solution of the equivalent pde for the 1 st order hlls solver is also depicted for such grid sizes it is evidenced that the aroe solver is able to maintain the initial equilibrium providing the exact solution whereas the hlls solver introduces artificial viscosity in the shear waves as indicated in section 4 2 it is verified that the numerical solution for the 1 st order hlls solver matches the analytical solution in 34 as the order of the numerical scheme is increased the numerical diffusion introduced by such scheme is reduced 5 2 numerical evaluation of the solvers using a double shear layer 5 2 1 problem set up the problem considered here is adapted from san and kara 2015 and is composed of a double shear layer the flow is typically unstable all through the shear layer and it leads to the generation of turbulent flow structures such as the kelvin helmholtz instabilities such kelvin helmholtz instabilities convert the initial linear perturbations into complex turbulent structures that ultimately determine the mass exchange across the different jets layers in the flow san and kara 2015 the computation of this flow is of high complexity and it requires a high accurate resolution of the fluxes in order to capture the generation and evolution of vortices at different scales the double shear layer problem is herein used as a benchmark to evaluate the numerical diffusion of the proposed solvers aroe and hlls the computational domain is given by ω 0 1 0 1 the initial condition is defined as h x y 1 0 and with 35 u x y u s if 0 25 x 0 75 u s otherwise which corresponds to a double shear layer as depicted in fig 3 note that us is the layer flow velocity yet to be defined for each case cyclic boundary conditions are considered in all cases and a 1 st mode sinusoidal perturbation is introduced as san and kara 2015 36 v x y 0 01 sin 2 π x the solution first evolves following a linear regime which breaks down at a certain time tb leading to a nonlinear evolution skinner and aaron 2019 note that the choice of a shear layer composed of two regions with equal velocity magnitude and opposite sign helps to parametrize the strength of the shear layer in terms of a unique parameter us furthermore under this configuration the overall advection of the 2d vortices in the streamwise direction is negligible and their centers remain steady with the flow swirling around them this flow configuration was adopted in san and kara 2015 and in other works therein referenced 5 2 2 computation of the numerical energy cascade the analysis of the kinetic energy cascade in 2d turbulent flows is a powerful tool to determine whether the selected numerical scheme can provide a trustworthy representation of the turbulence spectrum theoretically the 2d kinetic energy cascade in the inertial range for the system in 1 is of the form e k k 3 where e is the kinetic energy function and k the wave number san and kara 2015 sometimes an inverse cascade can also be observed at small wave numbers being in this case e k k 5 3 note that this inverse cascade may not be noticeable if the source of excitation of the flow is not triggered at a particular wave number to compute the numerical energy cascade the velocity components are first transformed to the frequency domain using the fast fourier transform fft algorithm yielding to u kx ky and v kx ky with kx and ky the wave numbers in each cartesian direction note that the transformed quantities must be properly normalized using the number of samples number of cells then the kinetic energy in the frequency domain is computed as e k x k y 0 5 u k x k y 2 v k x k y 2 by an appropriate change to polar coordinates we can express e kx ky as e k θ where k is the composite wave number computed as k s q r t k x 2 k y 2 and θ is the phase to obtain a 1d representation of the energy cascade a phase average of the kinetic energy is computed as follows 37 e k 1 2 π 0 2 π e k θ d θ 5 2 3 case 1 homogeneous double shear layer this first case considers a homogeneous double shear layer problem i e z x y 0 two configurations defined by a different us velocity and considering the initial perturbation in eq 36 are studied and displayed in table 1 the numerical solution is computed using the 1 st order and 3 rd order aroe and hlls weno ader scheme with a cfl number of 0 45 four different computational grids composed of 125 125 250 250 500 500 and 1000 1000 cells are used the simulation time is set to t 5 s satisfying t tb to determine tb in the numerical solution a dimensionless normalized transverse velocity is defined as v max us where v max max v i j is the maximum value of the transverse velocity in the domain configuration 1 a is used as a benchmark to assess the impact of the numerical scheme in the length of the linear regime i e tb as well as the turbulent patterns generated afterward the evolution in time of v max us computed by both schemes in all the grids for 1 st and 3 rd order of accuracy is presented in fig 4 furthermore the numerical values of tb for the 3 rd order aroe and hlls shemes are presented in table 2 for the highest resolution grid and 3 rd order aroe and hlls solvers the spatial vorticity distribution is depicted in fig 5 it is observed that the choice of the order of accuracy i e 1 st order or 3 rd order the riemann solver i e aroe or hlls and the computational grid have a strong impact on the length of the linear regime and subsequent transition to the turbulent regime fig 4 shows that tb is reduced as the order is increased and or the grid is refined furthermore it is observed that the hlls solver which does not account for the shear wave yields to longer tb than the aroe scheme the reason is that the hlls solver produces a larger viscous profile at initial times see fig 5 as discussed in the previous section which prevents the initial perturbation from triggering the nonlinear regime at earlier times it is worth pointing out that in the limit of the mesh resolution i e a very fine grid tb should tend to zero as the model does not include any physical viscosity that would enforce a viscous profile independent of the scheme and grid and an asymptotic tb 0 when analyzing the evolution of tb with respect to the mesh size the numerical results reveal that the parameter δtb defined as the difference of tb in two consecutive grids see table 2 grows proportionally to the mesh refinement ratio that is δtb doubles as the cell size doubles it must be noted that this relation is independent to the choice of the solver the ability of the scheme to resolve turbulent flows is assessed by comparing the computed energy cascade with the theoretical energy cascade san and kara 2015 numerical errors e g diffusion and dispersion errors are known to cause a deviation of the computed energy cascade from the theoretical spectrum e g a different tendency slope figs 6 and 7 report the computed energy cascades for cases 1 a and 1 b at t 3 s results were obtained with the 1 st order and 3 rd order aroe and hlls weno ader numerical schemes using the same grids as in the previous sections 125 125 250 250 500 500 and 1000 1000 note that e k k 3 is plotted instead of e k in order to better examine the region of the spectrum where e k k 3 such way the energy spectrum in the k 3 region must appear flat and helps in the detection of any energy deviation the numerical results point out several details i a 1 st order scheme is not adequate for the simulation of 2d turbulent flows of this type since the computed energy cascade shows a strong decay at relatively low wavenumbers ii the difference between the numerical solutions provided by the aroe and hlls solver is remarkable when using a 1 st order scheme while both solvers become more alike when moving to 3 rd order of accuracy iii considering the 3 rd order schemes those using the aroe solver are able to accurately reproduce a wider region of the spectrum i e they follow a k 3 tendency those based on the hlls solver exhibit a higher dissipation rate even at low wave numbers looking at the energy cascades computed with the finest mesh it is observed that the energy dissipation increases from t 3 s to t 5 s in case 1 a the opposite behavior is noticed in case 1 b this may be due to the presence of shock waves in case 1 b for such case the initial longitudinal velocity of the flow is sufficiently high to develop a turbulent pattern with the presence of shock waves which contain energy at high frequencies such shock waves are clearly observed in the water surface elevation at t 5 as depicted in fig 8 where the solution for cases 1 a and 1 b are compared vortices observed in fig 8 are physically explained through the mass and angular momentum conservation equations angular momentum is the product of mass velocity and the distance to the axis of rotation thus fluid particles close to the axis of rotation have to spin faster to compensate the smaller distance the local increasing velocity is in turn compensated with a lower water level to guarantee mass conservation the magnitude of such vortices and local water level depressions are related with the joint combination of initial and boundary conditions which are able to trigger some initial angular momentum the analysis of the energy cascade is key to determine the accuracy of the numerical solution observing fig 5 we could initially consider that the hlls solver is able to well reproduce the dynamics of the shear layer two main counter rotating vortices are generated in the computational domain however the energy cascade analysis shows the flaws of such superficial analysis since the flow energy is not dissipated at the physical rate at which it should do it the proposed model implemented with a hlls solver does resolve a lower range of turbulent scales than the version based on the aroe solver it can only capture the dominant low wave numbers i e the ones which correspond to the coherent vortex shedding based on the numerical results obtained for test cases 1 a and 1 b the cutoff wavenumbers for the 3 rd order and with the aroe and hlls weno ader schemes would be in the range k 10 20 and k 4 6 respectively such wavenumbers are related to the smallest flow features that the scheme can resolve and are of the order of the smallest structures observed in fig 5 the smallest structure size resolved can be estimated transforming the cutoff wavenumbers into a length l l 2 k c with l 1 the length of the domain for the aroe and hlls solvers such cutoff length yields to l 0 025 and l 0 1 the relation 1 4 between both cutoff lengths is approximately observed between the width of the viscous shear layers displayed in fig 5 5 2 4 case 2 double shear layer with bed variations a double shear layer including a smoothly varying bed elevation is considered in this section the initial condition in this case is set as h x y z x y 1 where the bed level is given by a sinusoidal variation 38 z x y 0 1 cos 2 π y 0 1 and v x y is given in eq 36 the initial condition for the streamwise velocity is given by u s 1 0 m s the aim of this test is to assess the performance of the numerical scheme in the resolution of 2d turbulence including the presence of bed source terms the given bed profile is smooth and it does not present sharp gradients therefore it is expected that the energy spectrum of the numerical solution of this case and the case 1 b same configuration without bed variation should be alike the correct discretization of the bed source term should not provide any artificial deviation of the energy spectrum decay the numerical solution at t 5 s computed using the 3 rd order aroe weno ader scheme and 1000 1000 cells is depicted in fig 9 two large coherent horizontal vortices are observed on both sides of the domain a comparison of the energy spectrum of the numerical solution of this case and of the case 1 b is presented in fig 10 the energy distribution within the palette of wavenumbers is very similar for both cases showing that the numerical scheme is not adding any significant extra dissipation or spurious high frequency modes 6 conclusions a depth averaged 2d shallow water hydrodynamic model based on a high order approximation of the numerical solution is evaluated for the resolution of unsteady turbulent shear layers within this framework two numerical solvers with different orders of accuracy are studied and implemented to assess their performance for the resolution of viscous and turbulent shear flows the accuracy of the solvers is assessed by means of a the analytical study of the numerical diffusion introduced by the solver associated to the resolution of the shear wave and b the analysis of the kinetic energy cascade of the numerical solution which is found to be related to other relevant indicators the size of the initial viscous profile and the time to develop the turbulent regime the results herein reported show that the ability of modeling a coherent shedding of vortices does not imply that the numerical solver is dissipating energy at the level at which it should be in order to explain the effect of the numerical diffusion in the resolution of turbulent shear flows an ideal case that considers an unperturbed shear flow configuration is first considered both analytical and numerical results evidence that the hlls solver incorporates an additional source of artificial dissipation due to the poorly resolved contact wave under ideal shear flow conditions this issue is related to the generation of a wide diffusive profile across the discontinuity which is proportional to the cell size when considering turbulent i e perturbed shear flow cases it is observed that the generation of such viscous profile across the discontinuity in the beginning of the simulation prevents the perturbed flow from a fast evolution to the turbulent regime this is equivalent to having a very low reynolds number note that this reynolds number is based on numerical diffusion only as the solution evolves in time the effect of the perturbation grows triggering the nonlinear transition to the turbulent regime i e the numerically induced reynolds number increases the time at which the transition from linear to turbulent regime is produced was measured showing that it is shorter for aroe based schemes than for the hlls versions as expected this is because the aroe solver induce a much narrower viscous profile than the hlls solver furthermore the spectrum of the turbulent flow field provided by the numerical solution was analyzed by means of the kinetic energy cascade it is observed that the schemes based on the aroe solver allows to properly resolve a wider range of the spectrum than those based on the hlls solver a close relation between the cutoff wavenumber the width of the initial viscous profile and the time for the generation of the turbulent regime was found the presence of smooth bed variations is analyzed in terms of the energy spectrum it is shown that the proposed scheme does not lead to spurious features in the numerical solution in presence of bed variation the numerical results evidence that the 3 rd order aroe weno ader scheme can be selected as discretization method in the framework of da urans simulation when aiming at resolving only the larger horizontal vortical structures on the other hand the hlls version of the scheme introduces too much diffusion even in the larger scales in this case a higher order of accuracy or a higher mesh resolution should be used the use of a 1 st order of accuracy for this purpose is proved to be undesirable credit authorship contribution statement a navas montilla conceptualization methodology software writing original draft c juez conceptualization methodology validation writing original draft declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was funded bythe spanish ministry of science and innovation under the research project pgc2018 094341 b i00 navas montilla by the gobierno de aragón through fondo social europeo feder 2014 2020 construyendo europa desde aragón navas montilla and by the h2020 msca if 2018 programme marie sklodowska curie actions of the european union under rea grant agreement number 834329 sediland juez the first author would like to thank dr omer san san fluid dynamics group school of mechanical and aerospace engineering oklahoma state university for some fruitful discussions about the topic and the detailed clarifications about his work 
552,the quantitative validation of rainfall statistics obtained from space borne sensors and the evaluation of the associated uncertainty are hindered by i the limited coverage of rain gauge networks and weather radars at the ground and ii the different intrinsic statistical properties of point ground measurements and of area averaged remote sensing estimates this problem is particularly significant for extreme rainfall frequency analysis where the inference process is heavily impacted by observational uncertainty and short records of satellite quantitative rainfall estimates qpes here we develop an approach to the validation and correction of qpe extreme rainfall statistics over data scarce regions we employ a statistical technique for downscaling qpe probability density function spatial correlation structure and frequency of extreme events to sub grid scales so as to permit direct comparison with rain gauge point measurements extreme value modelling is performed using the metastatistical extreme value mev distribution which allows for the use of all available information from short qpe samples we test this framework over the conterminous united states conus providing a spatially extended characterization of the performance of the tropical rainfall measuring mission trmm multisatellite precipitation analysis tmpa dataset we then leverage this spatially explicit analysis to develop a non parametric model of the high quantile estimation error the model allows for the extrapolation of the qpe validation to ungauged locations we test this methodology by means of a cross validation approach using independent observations at the ground over the conus the results support the use of this approach to extreme rainfall estimation in ungauged areas paving the way to similar applications at the global scale keywords extreme rainfall rainfall downscaling gridded precipitation metastatistics list of symbols s generic linear spatial scale corresponding to spatial average over an area s 2 t generic integration time scale l linear spatial scale of the qpe grid cell l 0 linear spatial scale of rain gauge measurements h s daily rainfall accumulation magnitude at spatial averaging scale s hs ordinary rainfall amount realization at scale s i e rainfall magnitude in excess of the threshold q hs ordinary rainfall amount random variable q threshold used to determine ordinary rainfall events ns average yearly number of events at the spatial scale s nt number of daily rainfall observations in one year p r s wet fraction at spatial scale s ws weibull shape parameter at spatial scale s cs weibull scale parameter at spatial scale s σ s 2 variance of the rainfall process at spatial scale s β 0 β 0 l intermittency function γ 0 γ 0 l variance reduction function α shape parameter of the spatial correlation function ϵ transition parameter of the spatial correlation function d generic distance between two points in space ρ s s 1 s 2 ρ s d spatial correlation function at spatial scale s and distance d ρ h l h l correlation between local averages at scale l ρj areal averaged correlation value sampled between grid cells located at distance dj sse sum of squared errors α l shape parameter of the correlation function obtained from fit to tmpa gridded data ϵ l transition parameter of the correlation function obtained from fit to tmpa gridded data αg shape parameter of the correlation function obtained from fit to rain gauge data ϵ g transition parameter of the correlation function obtained from fit to rain gauge data γ 0 l variance reduction function computed from tmpa fitted correlation areal averaged γ 0 d variance reduction function computed from tmpa downscaled correlation γ 0 g variance reduction function computed from rain gauge fitted correlation c 0 g weibull scale parameter at a point from rain gauge data w 0 g weibull scale parameter at a point from rain gauge data n 0 g yearly number of events at a point from rain gauge data c 0 d weibull scale parameter at a point from downscaling w 0 d weibull scale parameter at a point from downscaling n 0 d yearly number of events at a point from downscaling c 0 c weibull scale parameter at a point corrected using the qrf error model w 0 c weibull scale parameter at a point corrected using the qrf error model n 0 c yearly number of events at a point corrected using the qrf error model ηz relative error between downscaled and gauge estimated values of a quantity z c w n or γ y response variable in the quantile regression forest algorithm realization of the r v y x multivariate explanatory variable in the quantile regression forest algorithm realization of the r v x ωi weights for the qrf algorithm k number of trees θ vector describe the tree structure σe standard deviation of elevation μe mean elevation h s m annual maxima daily rainfall at spatial scale s random variable c s i weibull scale parameter estimated for year i at spatial scale s w s i weibull shape parameter estimated for year i at spatial scale s n s i yearly number of events estimated for year i at spatial scale s pne non exceedance probability tr return time or average recurrence interval m number of years in the rainfall record used for extreme value analysis list of acronyms trmm tropical rainfall measuring mission tmpa trmm multisatellite precipitation analysis gpm global precipitation measurement mission imerg integrated multi satellite retrievals for gpm conus conterminous united states qpe quantitative precipitation estimates mev metatstatistical extreme value distribution gev generalized extreme value distribution qrf quantile regression forest rf random forest pot peak over threshold am annual maxima hpd hourly precipitation dataset noaa national oceanogaphic and atmospheric administration nasa national aeronautics and space administration sggc set of gauged grid cells used in the analysis 1 introduction quantitative precipitation estimates qpe from satellite borne sensors provide much needed information on the water cycle at the global scale and are an essential source of observations over large areas worldwide particularly where the density of rain gauge stations at the ground is low chen and wang 2018 kidd et al 2017 however while current algorithms try to optimally merge information from radar passive microwave and infrared sensors huffman et al 2007 2014 uncertainty in the rainfall retrievals from these instrumental sources inevitably propagates to the final multi sensor rainfall qpes the error structure of the resulting qpes is thus difficult to characterize as it depends on numerous variables including the specific sensor the precipitation type and surface characteristics huffman et al 2007 tian et al 2013 maggioni et al 2014 for this reason considerable effort has been devoted to the ground validation of satellite qpes based on the available observations at the ground among many others see nicholson et al 2003 su et al 2008 shen et al 2010 villarini 2010 chen et al 2013 bharti and singh 2015 manz et al 2017 monsieurs et al 2018 however the density of observational networks at the ground varies significantly around the world so that a spatially consistent characterization of the qpe error structure is to date very challenging over extended regions ideally the validation of satellite qpes should be performed by comparing the estimated rainfall rates or rainfall accumulation volumes with a ground truth dataset aggregated at the same integral domain in space and time peleg et al 2018 however this is in practice only possible with the use of exceptionally dense rain gauge networks of which a limited number exist worlwide e g see elliott et al 1993 villarini and krajewski 2007 foelsche et al 2017 pedersen et al 2010 peleg et al 2013 amitai et al 2012 villarini et al 2008 wang and wolff 2010 duan et al 2015 or by comparing qpes with ground radar estimates where these are available tao and barros 2010 kirstetter et al 2012 aghakouchak et al 2012 kirstetter et al 2013 marra et al 2017 these requirements severely hinder the qpe validation effort over complex terrain and poorly gauged areas the problem of validating qpe rainfall statistics is particularly challenging when the frequency of heavy rainfall is the variable of interest habib et al 2009 prakash et al 2016 while the global scale coverage and fine temporal scale of satellite qpes make them invaluable datasets for studying extreme rainfall at the global scale see e g zipser et al 2006 the inference on rainfall distributional tail properties is challenging due to the short length of homogenous records and to the observational uncertainty in satellite retrievals which inevitably produces large uncertainty in the estimated extreme rainfall quantiles recently the use of satellite qpe datasets for rainfall frequency analysis over ungauged regions and particularly for the estimation of intensity duration frequency curves has received increasing attention gado et al 2017 ombadi et al 2018 faridzad et al 2018 marra et al 2019 the main objective of this work is to contribute to bridging this gap by coupling a recently introduced downscaling technique zorzetto and marani 2019 with a model of the error for qpe derived rainfall statistics the key steps performed in this combined technique are i evaluation of key rainfall properties probability density function pdf spatial correlation structure intermittency and frequency of extremes from a gridded precipitation dataset ii downscaling these quantities so as to make a comparison possible with point measurements at the ground where these are available and iii developing a model of the error aimed at extrapolating the qpe error structure to locations where ground measurements are not available a key feature of the approach developed here is that it is specifically targeted towards the study of extreme rainfall frequency with important consquences for the use of satellite qpes for hydrological analysis and for studying the risk of natural hazards to this end we employ the metastatistical extreme value mev distribution marani and ignaccolo 2015 marra et al 2018 zorzetto et al 2016 an extreme value model which provides a connection between the pdf of rainfall accumulations and the frequency of extremes global scale applications of statistical value models to multi satellite qpe datasets have been performed before for example zhou et al 2015 apply the generalized extreme value distribution in order to map the average recurrence intervals of precipitation for real time precipitation monitoring demirdjian et al 2018 apply a recursive clustering algorithm which combined with the peak over threshold method e g coles et al 2001 reduces estimation uncertainty when applied to short remotely sensed datasets here we produce the first global scale maps of extreme value quantiles estimated with mev which i can reduce estimation uncertainty from short rainfall datasets as shown in zorzetto et al 2016 and ii by providing a link between extreme and non extreme rainfall statistics introduces a new way to study the effects of spatial averaging and qpe bias correction on the statistics of rainfall extremes overall the combined method of analysis developed here provides a novel framework for understanding how biases in qpe statistics propagate to rainfall frequency analysis and for designing suitable site specific corrections this last point has significant impact for ungauged areas worldwide where the density of stations or the length of the available records is not sufficient for obtaining reliable extreme value estimates with traditional statistical techniques the statistical framework outlined above is here developed and applied to a large domain covering the entire conterminous united states conus this domain is covered by a large number of gauges and ground radar derived precipitation products so that the performance of satellite sensors and qpe datasets have been extensively studied here e g aghakouchak et al 2011 chen et al 2013 prat and nelson 2015 this makes it the ideal domain for testing our methodology which does not make use of any ground observation for the downscaling and only requires sparse gauge observations or radar data for constructing the model for the error therefore ground rain gauge observations and comparison with previous studies can be used for independently testing the performance of the method and for validating our findings moreover the conus domain spans a broad range of climatic regimes and terrain types thus allowing us to extend our previous work zorzetto and marani 2019 and explore the variability of qpe rainfall statistics and the associated error structure over this broader domain here we focus our attention on the tropical rainfall measuring mission trmm multisatellite precipitation analysis tmpa 3b42 research version dataset which to date includes 20 years of continous data and even after the end of the trmm mission is a well tested source of information for hydrological applications however we note that the approach developed here is not product dependent and is suitable for a straightforward application to any gridded precipitation product such as the integrated multi satellite retrievals for gpm imerg product huffman et al 2014 or even reanalysis products and climate model outputs the manuscript is organized as follows in section 2 we present the statistical technique used to downscale rainfall statistics from gridded qpe datasets so as to enable a direct comparison with point gauge measurements building on this method in section 3 we develop a non parametric model for the error based on a quantile regression forest algorithm meinshausen 2006 the error model is then tested using a cross validation scheme so as to effectively simulate the prediction of qpe rainfall statistics over ungauged areas in section 4 we then present the statistical framework aimed at the large scale extreme rainfall value analysis based on gridded qpe precipitation products and discuss the current limitations in evaluating the results over poorly instrumented areas the extreme value estimates are then corrected and tested using the model of the error developed in section 3 results of these steps are presented throughout these sections and are followed by a discussion of our findings and by the conclusions drawn by this study section 5 2 spatial downscaling of the probability distribution of qpe magnitudes we start by addressing the scale gap between daily rainfall statistics estimated from tmpa gridded qpe fields and their counterpart derived from rain gauge point measurements for this purpose we employ a recently developed statistical downscaling technique zorzetto and marani 2019 based on the theory of random fields vanmarcke 2010 müller and thompson 2013 while this technique entails a number of assumptions it greatly extends the areas where validation of gridded qpes datasets is possible as it does not require the presence of a dense network of rainfall gauges or ground radars covering the location of interest for downscaling qpe statistics the main assumptions made in deriving this methodology are i local spatial homogeneity and isotropy of the rainfall field ii a taylor frozen turbulence hypothesis for obtaining the wet fraction at a point in space and iii the assumption that the distribution of daily rainfall events can be represented as a stretched exponential both at the point scale and at the grid cell scale albeit with varying parameters in the following we briefly summarize the downscaling method applied here for additional information on the derivation of the methods the reader is referred to zorzetto and marani 2019 we define as ordinary rainfall events all daily rainfall accumulations at a given temporal integration scale in excess of a fixed low threshold value here set to q 1 mm day we choose a fixed threshold to define ordinary daily rainfall events which is larger than both the detection limits of gauges and qpe data when aggregated at the daily timescale while at the same time low enough so as to include the bulk of the daily rainfall distribution in our analysis we denote with ns the yearly number of such events or wet days the wet fraction is thus defined as p r s n s n t with n t 366 the number of daily observations in each year the suffix s here indicates the linear spatial scale s at which a rainfall time series is averaged here s 0 indicates the point scale l 0 corresponding to rain gauge measurements the characteristic size of a rain gauge is of the order of l 0 10 4 km and s l corresponds to the linear characteristic scale of the gridded qpe to be downscaled l l x l y where lx and ly are the dimension of a qpe grid cell along the zonal and meridianal directions respectively since we are dealing with spatial domains which are well characterized by a single length scale here we use a single linear length scale s to characterize a random field averaged over an area s 2 we assume that in each year the ordinary rainfall event magnitudes observed at scale s h s h s q defined for daily rainfall totals h s q are realizations of a random variable hs with population ω h s 0 and marginal distribution described by a weibull or stretched exponential distribution defined by the cumulative probability function 1 p h s h s f h s 1 e h s c s w s here cs is a scale parameter with the same units of hs mm day here and ws is the dimensionless shape parameter of the distribution a recent study analyzed the tail properties of hourly rainfall accumulations over the conus papalexiou et al 2018 and found that weibull provides a satisfactory description of hourly rainfall observations complementing and reinforcing previous evidence for rain gauge daily accumulations at the global scale wilson and toumi 2005 while the weibull parameter cs represents the characteristic magnitude of daily rainfall events ws describes the decay of the tail of f hs such that values of ws 1 correspond to a sub exponential behavior i e the exceedance probability exhibits a heavy tail albeit with a characteristic scale an exponential decay is recovered for w s 1 and values of ws larger than unity correspond to faster than exponential decays this simple parametric model is assumed to describe the shape of the daily rainfall distribution of both rain gauge and satellite derived rainfall accumulations such that their respective difference is encoded in differences between the values of cs and ws at the different spatial scales 2 1 downscaling scheme zorzetto and marani 2019 showed that the parameters of the distribution of ordinary rainfall events in eq 1 aggregated at a fixed temporal scale and averaged at two different spatial scales can be linked by two equations which depend on the intermittency and spatial correlation of the rainfall field for example the stretched exponential parameters cl wl describing the ordinary rainfall pdf at the grid cell scale s l of the qpe dataset can be expressed in terms of the parameters c 0 and w 0 of the rainfall field at the point scale s 0 by the relations 2 γ 0 β 0 2 w 0 γ 2 w 0 γ 2 1 w 0 2 w l γ 2 w l γ 2 1 w l γ 0 1 p r l 3 c 0 w 0 2 β 0 2 c l w l 2 γ 2 1 w l γ 2 1 w 0 where γ denotes the gamma function p r l is the wet fraction of the ordinary rainfall process at the grid cell scale l γ 0 γ 0 l is the variance reduction factor i e the ratio of the variances σ l 2 and σ 0 2 of the process averaged at the two scales s l and s l 0 respectively and β 0 β 0 l p r l p r 0 or intermittency function is the ratio of the wet fraction at the two scales considered here while p r l can be directly computed from qpe time series γ 0 and β 0 are not known in the absence of rain gauge measurements since they do not only depend on the areal average process but also on the rainfall process at scale l 0 i e at a point in space 2 2 scalewise variation of the spatial correlation function however the variance reduction function γ 0 can be obtained from the spatial correlation of the rainfall field ρ s 1 s 2 here assumed to be quadrant symmetric vanmarcke 2010 where s 1 and s 2 are distances between two points measured along two coordinate axes as 4 γ 0 σ l 2 σ 0 2 4 l x 2 l y 2 0 l x 0 l y l x s 1 l y s 2 ρ s 1 s 2 d s 1 d s 2 where lx and ly are the spatial dimensions of the grid cell size ideally the point correlation ρ s 1 s 2 should be estimated using a sufficient number of rain gauges distributed in space as these are not available everywhere in fact they are lacking in most areas here we estimate it using the correlation between qpes time series sampled at grid cells within a 3 3 local neighborhood this correlation between spatially averaged values can be linked to the unknown correlation at the point scale by means of the following relation vanmarcke 2010 5 ρ h l h l k 0 3 l 0 3 1 k 1 l δ l x k l y l 4 δ l x l y where the set of distances l x k and l y l for k l 0 1 2 3 contains the information on the relative positions of pairs of cells within the neighborhood considered letting δx and δy be the distances between the two grid cells along the s 1 and s 2 coordinate directions then these distances are defined as l x 0 δ x l x l y 0 δ y l y l x 1 δ x l y 1 δ y l x 2 δ x l x l y 2 δ y l y l x 3 δ x l y 3 δ y and with the function δ a b defined as 6 δ a b 4 0 a 0 b a s 1 b s 2 ρ s 1 s 2 d s 1 d s 2 since the qpe dataset allows the computation of estimates of ρ h l h l we can obtain an estimate of the point scale correlation by assuming a parametric form for the spatial correlation function ρ s 1 s 2 and by numerically minimizing the sum of squared errors sse between its areal average value at the grid cell scale given by eq 5 and the values estimated from qpes we assume here an isotropic correlation function such that ρ s 1 s 2 ρ s 1 2 s 2 2 ρ d 7 ρ d ϵ α e α d ϵ d ϵ ϵ e d α d ϵ this formulation has an exponential kernel and a power law tail characterized by the exponent α the smooth transition between the two regimes occur at a distance d ϵ marani 2003 zorzetto and marani 2019 depending on the values of the parameters α and ϵ it allows for the description of both light and heavy tailed correlations with this assumption the sse becomes a function of the parameters α and ϵ 8 s s e ϵ α j 1 m ρ h l h l d j ϵ α ρ j 2 where the ρj are correlation estimates obtained from the qpe dataset using a local lattice of 3x3 grid cells centered over the location of interest using a larger lattice would increase the number of points available to estimate the correlation function but on the other hand would increase the likelihood of including in the analysis non homogeneous rainfall statistics which certaintly are present at the regional scale especially over a complex terrain for each pair of grid cells within this local domain the pearson correlation was computed between the respective qpe time series and the resulting values binned over a set m of distances dj here we minimize the sse in eq 8 by means of the differential evolution stochastic minimization algorithm storn and price 1997 as opposed to the deterministic algorithm used in zorzetto and marani 2019 the differential evolution minimization is particularly suited to avoid possible local minima in the α ϵ parametric space and thus more robust in minimizing a function which depends upon the experimental points ρj we seek the global minimum of eq 8 within the rectangular domain α 0 1 and ϵ 0 1000 km which are physically meaningful ranges of values while quantifying the uncertainty in the downscaled correlation parameters is a challenging task as the shape of objective function eq 8 varies with the observed values ρj we note that the downscaling methods only requires the functional of the correlation function defined in eq 4 which is an integral property of the correlation function over the pixel size therefore we expect γ 0 to be less sensitive to observational uncertainty than the single parameter values α and ϵ 2 3 scalewise variation of the wet fraction to obtain an estimate of the intermittency function β 0 l i e of the ratio between the yearly number of events for the ordinary rainfall process averaged at the grid cell scale l 0 with respect to the point scale we rely on an application of the taylor frozen turbulence hypothesis taylor 1938 deidda 2000 haerter et al 2015 introduced by zorzetto and marani 2019 this approximation enables us to use of information from the qpe dataset at smaller temporal scales up to 3 hours for the tmpa dataset to infer a property of the rainfall field namely the wet fraction at spatial scales smaller than the grid cell scale and at the daily timescale we aggregate tmpa rainfall fields at increasing spatial scales s and temporal scales t integrating in time and averaging in space and at each space time scale the wet fraction pr s t is computed as the fraction above q of the time series at scale s t we then extrapolate this quantity to the point scale l 0 by assuming that the function pr s t is locally linear in the s t plane i e that integration in time and averaging in space have the same effect on a property of the rainfall field the wet fraction here up to a constant factor which has the meaning of a local advection velocity this assumption holds exactly only in the case of a perfectly fractal rainfall field haerter et al 2015 but even though only approximate in general it offers a conceptually satisfactory way to infer the point scale wet fraction using only qpe data at the grid cell scale through this approach we obtain β 0 p r l p r 0 where p r l is simply obtained from the remote sensing qpe and p r 0 is the value that a linear extrapolation in the s t plane yields for daily rainfall at the rain gauge spatial scale l 0 note that in order to apply this technique we need a sufficiently fine scale resolution in time for the qpe dataset compared to the scale at which the downscaling analysis is performed 3 h versus daily here the reader is referred to zorzetto and marani 2019 for a more detailed description and application of the methodology 2 4 data an independent evaluation of the statistical structure of a qpe dataset using the approach described above requires observations from rain gauges at the ground we focus on the conus domain defined here as the domain over land within 22 n 50 n and 130 w 60 w over this domain we use observations from the network of hourly precipitation data hpd rain gauges of the national oceanogaphic and atmospheric administration noaa which covers the conus starting from 1948 ncep precipitation data were aggregated at the daily time scale by summing hourly accumulations in each day of the record defined starting from midnight we excluded from the analysis days with quality flagged or missing precipitation amounts the remotely sensed gridded precipitation product used in this study is tmpa 3b42 version 7 research version huffman et al 2007 2010 huffman and bolvin 2013 of which we use the entire record 1998 2018 so as to obtain the longest possible dataset for extreme value analysis note that starting in 2014 after the end of the trmm era rainfall retrievals from the gpm mission are used in tmpa estimates introducing some heterogeneity in the dataset rainfall rates obtained from the tmpa dataset are here assumed to represent the average rainfall rate for each 3 hr timestep these values were aggregated so as to compute rainfall accumulations at time scales raging from 3 h to 48 h as needed for estimating the intermittency of the rainfall field as discussed in section 2 3 with the 24 h totals used for our analysis at the daily time scale we note that discrepancies between gauge and tmpa daily totals may potentially arise in the presence of a pronounced daily precipitation cycle and as a consequence of the instantaneous nature as well as the timing of satellite rainfall retrievals libertino et al 2016 here we do not anticipate that this issue will play a relevant role in our analysis since we do not study the timing of specific events but only average statistics the spatial resolution of the data is 0 25 0 25 corresponding to a characteristic grid cell size of about 25km over the conus domain while here we interpret the tmpa rainfall rates as areal averages over the grid cell area see e g villarini and krajewski 2007 this is an approximation as these estimates are obtained by merging retrievals from different instrumental sources passive microwaves and infrared with different footprints additionally the monthly scale gauge correction is performed using precipitation gridded at scales larger than the spatial reolution of tmpa and therefore introduces larger scale information in the pixel scale qpe time series in essence any discrepancy stemming from this assumption will be included in the structure of the error that will be estimated as a result of our analysis in order to evaluate qpe statistics after downscaling we select a set of qpe grid cells from the tmpa dataset that are characterized by the presence of i at least one rain gauge within the grid cell with at least a 10 year record of daily rainfall and ii at least 4 rain gauges in a 5x5 pixel neighborhood around the location of interest which are used for producing an estimate of the local spatial correlation of the rainfall field this information is not used to train the downscaling method but only for validation purposes to be selected these rain gauge records must overlap for a temporal window of at least 2000 observations i e about 7 years of record this condition was chosen as a tradeoff between having enough data for reliably estimating the correlation between sites and including a large enough number of sites for testing purposes our results are not very sensitive to this choice as for most of the gauged sites in the dataset the record length is significantly longer than 7 years in the following we will refer to the set of sites meeting these conditions as the set of gauged grid cells or sggc 2 5 results of the downscaling method to study the spatial correlation of the rainfall field we focus on two metrics the ratio between the two parameters of the correlation function ϵ α and the variance reduction function γ 0 the first is a characteristic spatial scale describing how the correlation decays with distance for very large values of ϵ this is exactly the spatial integral scale of the field which is not necessarily finite depending on the values of α while γ 0 is an integral property of the correlation function between the point and the grid cell scale γ 0 is the main quantity of interest here as it connects the variance of the rainfall accumulations at different spatial averaging scales and it is the only quantity dependent on the correlation function that is directly needed in downscaling the pdf of ordinary rainfall in eqs 2 and 3 we note that three different cases can be identified when downscaling the spatial correlation function from gridded qpes when the correlation scale of the rainfall field at the ground is small compared to the spatial averaging scale ϵ αl 1 then the correlation structure of the continuous process is in large part hidden by the averaging process and we expect it cannot be completely recoverable from the qpe dataset alone this occurs only in a very limited subset of the sggc sites examined here we find that the ratio ϵ α is smaller than 25km only at 33 of the 860 sggc sites where downscaling is performed 3 8 of all cases these are primarily locations characterized by a complex terrain and it is reasonable to think that in these cases the correlation at the point scale is not retrievable from qpe alone we therefore exclude these locations from the following analysis conversely when the correlation of the field is much larger than the grid scale over which averaging is performed most of the information about the correlation function is preserved even after averaging in this case we argue that the effect of and need for a downscaling process may be limited because the variables being averaged are highly correlated and thus they tend to behave very similarly to their spatil average this case also does not occur frequently with only 8 out of the 860 sggc sites examined here exhibiting ratios ϵ α larger than 200km the largest ratio being around 8 in the intermediate case between these two situations when the correlation distance of the rainfall field is comparable to or larger than the scale of averaging a substantial part of the information about the correlation structure of the continuous process at the point is preserved by the averaging process the application of the downscaling approach i e the minimization of eq 8 can produce in this case good estimates of the correlation structure at the point scale to provide a global metric describing the effect of correlation downscaling we compare the estimated values of ϵ α and γ 0 with the corresponding metric ϵ l α l obtained from fitting eq 7 to the empirical correlation estimates ρj dj obtained from a local lattice of qpe time series in turn using ϵ l and α l instead of ϵ and α in eq 4 we can infer an estimate γ 0 l of the variance reduction function value that we would obtain by using the grid cell scale correlation function instead of its downscaled version examination of the values of rainfall spatial correlation over the conus domain here indexed by longitudinal position see fig 1 a shows that the tmpa estimated correlation obtained by fitting eq 7 to the gridded qpes is larger than its estimates from point measurements at the ground this is as expected since we are comparing a correlation between averaged values with its point scale counterpart when we compare the downscaled correlation values obtained from the qpe dataset by minimizing eq 8 we see that some underestimation occurs primarily at the boundaries of the domain and chiefly for grid cells located in the proximity of the west coast this result is consistent with a previous study which investigated the ability of radar rainfall retrievals to reproduce the small scale rainfall variability gebremichael and krajewski 2004 however in the central part of the conus domain the zonal variability of the correlation appears to be reasonably captured by the tmpa estimates the application of the downscaling approach produces correlation values that are much closer to those from gauge observations suggesting that indeed the proposed downscaling procedure yields statistical properties that are close to those of point observations a similar result emerges if we turn our attention to the variance reduction function fig 1b for which we again notice how the downscaled results are closer to values from ground observations for the central part of the conus and less so along the east and west coasts a summary of the variability of the downscaled parameters over the sggc is provided by the scatter plots in fig 2 while no apparent bias appears for the values of γ 0 d with respect their counterpart estimated at the ground a significant variability is detected for the lowest values of γ 0 which correspond to grid cells located primarity in the mountainous part of the western united states fig 2a this scatter for low values of γ 0 confirms the increasing difficulty of correctly estimating the point correlation when its characteristic length scale becomes smaller compared to the averaging length l however note that the large majority of the sggc sites has values of γ 0 larger than 0 9 range in which the discrepancy between ground and downscaled values is limited as shown in panel 2a the comparison of the number of wet days n 0 d obtained from downscaling tmpa qpe s with gauge estimated values n 0 g indicates that the qpe dataset consistently underestimates the number of events recorded at the ground fig 2b and that this underestimation increases with the value of n 0 g while the number of events n 0 d is sistematically underestimated the scale parameter c 0 d appears to be significantly larger than its corresponding ground values fig 2c we argue that this feature of the qpe dataset is at least partially explained by the gauge correction applied to the tmpa 3b42 research version dataset gauge data are used to rescale tmpa monthly totals and thus if the number of missed events is appreciable such a correction will result in a deformation of the pdf of ordinary events with respect to the real one at the ground and will lead to the overestimation of the characteristic event size c 0 d so that monthly totals measured at the ground are preserved the downscaled shape parameter w 0 d appears to be quite variable with respect to its gauge estimated counterpart w 0 g fig 2d as was the case for γ 0 even though most of the values here are close to the identity line for some points corresponding to locations in the western united states the downscaled values of w 0 are significantly lower than gauge estimates meaning that for these locations the downscaled ordinary rainfall statistics exhibit a heavier tail i e an overestimated probability of intense events to study the spatial distribution of the discrepancy between qpe downscaled statistics and the corresponding values at the ground we define the relative errors 9 η z z 0 d z 0 g z 0 g where the variable of interest can be one of the following z c w n or γ the subscript 0 again refers to values at the point spatial scale l 0 and the subscripts d and g refer to the downscaled or rain gauge observed quantities respectively fig 3 reports the spatial variability of the relative errors ηz for the four parameters and the set of sggc sites the map of the relative errors ηγ in the variance reduction function over the conus is featured in panel 3a the variance reduction function γ 0 appears to be well captured by the downscaled correlation especially over the east coast and in the midwest regions of the conus some overestimation appears corresponding to the west coast while γ 0 tends to be generally underestimated with respect to ground values in the western conus the error in the yearly number of events appears to be larger on the west coast and in the north east whereas it is smaller in the south east and mid west regions of the usa fig 3b once the variance reduction function and intermittency functions are known the parameters describing the pdf of ordinary rainfall events at the point scale can be estimated by means of eqs 2 and 3 for the scale parameter c 0 some overestimation is found to occur especially in the northeastern and northwestern sectors of the conus while mostly underestimation occurs in the west fig 3c conversely the shape parameter w 0 appears to be underestimated in the west and overestimated in the east fig 3d together these results suggest that in the west downscaled qpe statistics exhibit heavier tails and lower mean compared to their ground counterparts while the opposite is true for the eastern usa and the pacific coast where both parameters are overestimated the characteristic event magnitude is overestimated but the tail of the distribution is lighter than ground estimates suggest these results are coherent with previous work such as prat and nelson 2015 which found that tmpa 3b42 shows a consistent underestimantion of the yearly number of events especially in the north east and middle atlantic regions of the conus therefore the results obtained here for the weibull scale parameter are also to be expected as the gauge correction applied to tmpa by rescaling the monthly total accumulations will determine an overestimation of the characteristic event size in the number of events is underestimated as we will discuss in the following section these distortions of the pdf of daily rainfall have relevant consequences for the estimation of extremes 3 a model of the error for qpe downscaled statistics as discussed above downscaling the pdf of rainfall accumulations allows for the direct validation and correction of remotely sensed qpe s statistics at gauged sites this result is useful in itself as downscaling allows for proper comparisons of quantities defined at the same scale and it reduces the density of the gauge networks required for validating qpe derived rainfall estimates however for many areas worldwide which are characterized by sparse or altogether absent ground stations it is of primary importance to provide the error analyses that can be applied to ungauged locations in the case of target sites located at a limited distance from gauged locations this objective can be pursued by building a geo spatial model of qpe error statistics here we investigate how qpe errors can be predicted from the information collected at gauged sites located at considerable distance from the target sites so that they are characterized by weak or no direct correlation to this end we develop a non parametric model of the error based on the quantile regression forest qrf algorithm meinshausen 2006 with the objective of inferring the relative errors in the downscaled parameters y η c η w or ηn at ungauged locations based on a set of variables describing the local rainfall regime and terrain type qrf has been applied before to study the error structure of passive microwave rainfall retrievals bhuiyan et al 2017 and constitutes a modification of the classic random forest rf algorithm introduced by breiman 2001 this modification of the rf algorithm is chosen for its ability to 1 describe the generally nonlinear relations between the error in the downscaled parameters and a set of explanatory features boulesteix et al 2012 2 limit the problem of overfitting breiman 2001 tyralis et al 2019 and 3 deal with possibly correlated predictor variables ziegler and könig 2014 predictions from these models are based on regression trees structures which encode the recursive partitioning of the predictor variable space in distinct and non overlapping regions in each terminal node of a tree the predicted value of the target variable is estimated as the average response in that region predictions of rf and qrf are ensemble values over a large number k of decision trees each trained on a bootstrap sample from the original dataset instead of predicting the expected value of the variable of interest for a given value of the predictor as is the case for the rf method qrf estimates the full conditional distribution of the response variable y given a value of x a possibly multi dimensional predictor vector reconstructing the entire conditional distribution can be useful here for i limiting the possible effect of outliers on the estimated conditional mean using the median value instead and ii constructing confidence intervals for the estimates of interest qrf estimates the empirical distribution function of y for a given value x of the explanatory variable x as meinshausen 2006 10 p y y x x i 1 n ω i x 1 y i y where 11 ω i x k 1 t 1 k ω i x θ t here k is the number of trees and each tree is built from an independent and identically distributed vector θ t t 1 k which encodes the information on which predictors are used as split point in each node of the tree the indicator function 1 y i y assumes the value 1 when the observation yi y and is equal to 0 otherwise the quantities ωi x θt are weights obtained from observation yi of the response variable for the tth tree they are defined as 12 ω i x θ t 1 x i r l x θ t j 1 n 1 j x j r l x θ y where again 1 denotes the indicator function so that they are zero if observation xi does not belong to leaf l x θt and are positive otherwise the leaf l x θt refers to the terminal node of tree t individuating the region r l x θ y obtained by partitioning the predictor variable space at each node split for the present application we select a set of explanatory features which are representative of the local rainfall climatology and which can all be obtained from a remotely sensed dataset without the need of observations at the ground for this purpose we identify as predictors the stretched exponential scale cl and shape wl parameters the average yearly number of events nl and the variance reduction function γ 0 as estimated from the qpe dataset additional predictors we consider to describe local environmental conditions are the mean elevation averaged over the qpe grid cell μe and the standard deviation of the elevation σe computed over a domain of size 1 25 x1 25 degrees centered over the location of interest in order to account nearby orographic features predictor variables were normalized by subtracting their averages and diving by their standard deviations elevation data were obtained from the global topographic map obtained from noaa national center for environmental information amante and eakins 2009 which provides elevation with respect to mean sea level globally at 1 arc minute resolution this information is here averaged so as to match the tmpa grid over the conus the qrf model is here applied by training k 2000 decision trees although experiments were carried out with different values of k to make sure the results were not overly sensitive to this choice we note that for their nature rf and qrf models should not be used in extrapolation outside the range of the predictors used for training instead they should be trained using a set of observations representative of the range in which we wish to obtain predictions therefore when applied to a heterogeneous region such as the conus one would like to understand if error predictions are possible based on sparse and low density network of training sites which is the working condition over many areas worlwide here we address this question by designing a cross validation procedure in which we take advantage of the relatively dense observational network available in this study over the conus to simulate how the method performs when trained using a low density gauge network and tested on an independent dataset first locations within the set of gauged sites sggc are randomly resampled then starting from a random location we construct a sample by examining sequentially all sggc sites and randomly inspecting and removing all gauged sites located at a distance less than 100km from any sites already included in the sample by doing so we obtain a sample size of about 175 sites the exact number varies at each realization and obtain a dataset in which selected sites are at most weakly spatially correlated thus allowing for proper calibration and testing of the error model second we randomly divide the stations obtained in stage 1 into two independent sets used for training the model and for independent testing respectively here we use 50 of the sites for calibration and the remining 50 for validation of the qrf model we repeat these two steps a number of times n g 20 in the analysis reported here and pool together the results to obtain a global measure of performance over the test sites extracted in each realization in each testing or training site we performed the downscaling of qpe statistics as described in the previous section 2 obtaining estimates of the parameters c 0 d and w 0 d as well as of the yearly number of events n 0 d at a point moreover at these sites independent rain gauge estimates of these quantities at the ground c 0 g w 0 g and n 0 g are also available therefore we can obtain a measure of the error in downscaling these variables by simply applying eq 9 results of this analysis are featured in fig 4 which compares the values of the errors ηc ηw and ηn predicted for the test sites with the true values of such errors obtained using gauge records at the ground in addition the lower panels in fig 4 show the values c 0 c w 0 c and n 0 c obtained by correcting the downscaled parameters using the qrf error model the qrf corrected values show a good agreement with their counterpart from rain gauges at the ground especially for the scale parameter c 0 c and the number of events n 0 c a somewhat larger scatter occurs for the shape parameter w 0 c overall this result constitutes a clear improvement with respect to the downscaled parameter values shown in fig 2 while using an ensemble of decision trees as done in the qrf algorithm allows to reduce prediction uncertainty and improve stability with respect to single decision trees these improvements occur at the expense of the interpretability of the model nevertheless it is possible to obtain an estimate of the relative importance of the predictor variables in our regression model the importance of a given predictor is computed as the mean decrease in the sum of squared residuals achieved at each node split by selecting that predictor james et al 2013 the importance of the features used in the qrf model to predict the three errors of interest ηc ηw and ηn is quantified in fig 5 the error in the number of events seems to depend primarily on the characteristic scale of ordinary rainfall events cl whereas for the error in the scale parameter ηc the most important features are the parameters of the ordinary rainfall distribution at the qpe grid cells scale in the case of the error in the shape parameter ηw the most important predictors are those representing local orography and the scale parameter cl the average number of events does not play a primary role for either of the three error variables and is only somewhat relevant for predicting the values of ηc however we note that these parameter importances should be interpreted with caution as the predictor variables are not completely independent between each other while this is not an issue for the qrf error predictions this should be kept in mind when examining parameter importances the dependence of the shape parameter error from elevation suggested by the qrf analysis is not a surprise given its spatial distribution shown examined in fig 3 however it is interesting to note that the role of orography seems to be more substantial in explaining errors in the tail of the distribution i e in w 0 rather than on the characteristic rainfall magnitude or in the yearly average number of events 4 the statistical distribution of extreme events from remotely sensed rainfall we turn here our attention to studying the probability distribution of extreme events based on tmpa qpe s building on the tools developed and tested in the previous sections to this end here we provide the first large scale application of the metastatistical extreme value mev distribution to remotely sensed qpe fields following marani and ignaccolo 2015 zorzetto et al 2016 zorzetto and marani 2019 we study the distribution of the annual maximum h s m among a variable number of independent and identically distributed ordinary rainfall events here at the daily time scale the parameters describing the distribution of ordinary events as well as the yearly number of events are themselves considered to be random variables whose realizations are estimated in each year of a rainfall record from a sample of m years of daily rainfall observations we thus estimate the cumulative probability distribution of the annual maximum according to the mev distribution 13 p h s m h ζ s h 1 m i 1 m f s i h n s i where for each year i 1 m in the rainfall time series the yearly number of ordinary events is n s i and p i h s h f s i h is the distribution of ordinary events in year i of the rainfall record which here we assume to be a stretched exponential with cumulative probability 14 f s i h 1 e h c s i w s i with yearly scale and shape parameters c s i and w s i and spatial averaging scale s the main idea behing the use of eq 13 for modelling daily rainfall accumulations extreme values is the recognition that low frequency variability in the rainfall generating mechanism can produce heavier tails than would be otherwise observed this effect is captured in eq 13 by averaging over parameter values estimated independently for each year in the record zorzetto et al 2016 while the weibull distribution is widely used for hourly to daily accumulations wilson and toumi 2005 papalexiou et al 2018 we note that eq 13 can be tailored to different parametric models for the probability distribution of ordinary rainfall events thus conferring to the mev distribution significant application flexibility for computing rainfall quantiles for a given cumulative probability pne or return time t r 1 1 p n e we can invert eq 13 and evaluate quantiles as 15 h s t r q ζ s 1 p n e parameters for the mev and generalized extreme value gev distributions were estimated by means of probability weighted moments greenwood et al 1979 and l moments hosking et al 1985 respectively for the purpose of computing extreme value statistics years with more than 10 of missing observations 36 data points at the daily scale were not included in the analysis as the selection of annual maxima values as well as the estimation of daily rainfall statistics would be potentially biased in the presence of relevant fractions of missing data 4 1 the mev distribution applied to tmpa qpe s the expected value of the daily rainfall accumulation corresponding to a 50 years return time was obtained by applying the mev distribution to the tmpa 3b42 v7 research version for the conus for comparison we produced the same estimate by fitting the generalized extreme value gev distribution e g coles et al 2001 to the series of annual maxima am extracted from the same dataset the large scale spatial features of extreme rainfall frequency over the conus are similarly captured by the two models fig 6 interestingly the spatial distribution of mev estimated quantiles is significantly smoother when compared with its gev counterpart this is consistent with the notion that mev by using all the observations available produces more stable estimates zorzetto et al 2016 that are also less sensitive to outliers inevitably present in remote sensing estimates as also noted in marra et al 2019 gev estimates are on the contrary dominated by a few large outlier values as manifest in the grainy texture of fig 6b we argue that this behavior originates from the fact that mev inferences are based upon the entire distribution of ordinary events determining a decreased sensitivity to biases in qpe estimates of large rainfall values compared to annual maxima or peaks over threshold approaches as discussed in mehran and aghakouchak 2014 while tmpa is able to capture rainfall from intense convective events the accuracy of the estimates progressively descreses when limiting the analysis to rainfall rates exceeding increasingly high threshold values a second observation is that the 50 year quantiles estimated by mev appear to be larger than those estimated by gev in many areas over the conus especially in the midwest this feature does not appear to be uniquely inherent to the statistical models used if we repeat the same analysis for the rain gauges from the noaa hourly precipitation dataset hpd aggregated at the same daily scale fig 6c d we find that the two statistical models yield much more similar results in this case although the 50 year quantile field obtained from mev remains spatially smoother that its gev counterpart it is reasonable that being the mev and gev distribution fitted using different parts of the dataset annual maxima in the case of the gev and the bulk of the distribution in the case of mev the extreme values estimated through the two formulations should respond differently to distortions in the pdf of the qpe magnitudes however a direct comparison of the extreme value quantiles reported in panels a b and c d of fig 6 is not feasible because of the scale discrepancy between qpe s and point rain gauge measurements eq 13 provides a direct link between properties of the ordinary daily rainfall distribution and the frequency of extreme values hence using downscaled parameters of the ordinary rainfall distribution inferred from remote sensing it can be used to directly compare the mev derived extremes at a point with those from independent rain gauge records and to possibly correct for emerging discrepancies here we test this idea by extending the cross validation method employed in section 3 for testing how the correction applied to the stretched exponential parameters affects extreme rainfall quantile estimates for each iteration in the cross validation scheme once the tmpa parameters c l i w l i and n l i for i 1 m have been computed over the test sites they are used to provide an estimate of the t r 50 years quantile h l t r by means of eq 15 analogously the downscaled c 0 d i w 0 d i n 0 d i and corrected c 0 c i w 0 c i n 0 c i values of the parameters are used to estimate the corresponding quantiles at a point h 0 d t r and h 0 c t r respectively for each testing site we independently compute the mev parameters c 0 g i w 0 g i n 0 g i and quantiles h 0 g t r from the rain gauge record at the ground we note that we estimate the parameters c s i w s i and the number of events n s i appearing in eq 13 separately for each year in the rainfall record so as to account for their inter annual variability we then downscale each yearly value of the parameters using the equations described in section 2 and correct them using the error model described in section 3 note that the downscaling and correction relations used are the same for each year in the record when downscaling the weibull parameters for estimating mev at subgrid scales we apply eqs 2 and 3 to each set of yearly parameter values separately using the constant values of the variance reduction function and intermittency function derived from the entire qpe time series available in principle the spatial correlation of precipitation may also vary year to year e g as a consequence of the varying frequency of different precipitation types however given the known difficulty of estimating values of correlation for skewed non gaussian processes such as precipitation at the daily time scale gebremichael and krajewski 2004 we did not include this possible source of inter annual variability in the model and computed correlation values from the entire qpe time series therefore in our application the intermittency of the rainfall field and the decrease of its variance with averaging scale are the same for each year of observations the downscaled mev parameters are then corrected for each year of the record using the median value of the relative error predicted by the qrf algorithm as for the parameter downscaling we do not explicitly correct for possible biases in the inter annual variability of the parameter estimates result of this comparison between tmpa downscaled and corrected quantiles with the corresponding rain gauge quantiles h 0 g is reported in fig 7 in the form of scatter plots panel 7a and pdf of the relative error between estimated values and reference rain gauge quantiles panel 7b we can see that qpe quantiles tend to consistently overestimate rain gauge quantiles even though they have a different nature as they are best interpreted as areal average quantiles over the pixel as opposed to values at the point this is clearly a byproduct of the biases observed in the tmpa estimated weibull parameters especially the overestimation of the scale parameter observed in fig 2c turning our attention to the downscaled values they exhibit an even more substantial overestimation with respect to the rain gauge benchmark this is expected as moving towards smaller averaging scales the fluctuations of the daily rainfall process tend to become more energetic as quantified by the increase in variance and decrease in the number of events captured by eqs 3 and 2 lastly the quantile values corrected by applying the qrf error model appear to be much closer to the ground estimates despite some variability remaining in the distribution of the estimates qpe statistics bias is greatly reduced and the pdf of relative errors exhibit in this case a clear peak around zero this encouraging result supports our choice of applying the same correction to each yearly parameter in the mev distribution without applying any specific correction to the inter annual variability of the parameters we note that the application of downscaling and bias correction techniques tend to have opposite effects on the estimated quantiles the downscaling tends to increase the magnitude of the 50 y e a r event in this case and the bias correction tends to decrese it on average even though this might seem counterintuitive and perhaps unnecessary we stress that this distinction between scale effects and errors is quite important as it is the difference between the downscaled and gauge values that should be minimized for example when designing new sensors and algorithms areal averages and point values should not be directly compared overall the combined application of downscaling and bias correction appears to provide good results and confirms that the bias correction performed on the parameters is relevant for correcting extreme value quantiles as a representative application of the error model we trained the qrf model with data from all the sggc sites and predicted the spatial distribution of the error over the conus fig 8 features the median relative errors in the weibull parameters panels 8 a and b and in the yearly number of events panel 8 c consistently with the observations from gauged sites these results clearly show spatial features such as the overestimation of the characteristic scale cd over the northeast us and the underestimation of the shape parameter wd in the western us predominantly controlled by orography after correcting the yearly mev parameters with the median qrf relativer error estimate one can compute the errors in mev estimated quantiles the spatial distribution of the error in the 50 year quantile is featured in fig 8 d which shows the quantile being generally overestimated over the conus this overestimation is particularly relevant in the western us where underestimation of the shape parameter over complex terrain produces extreme value distributions with tails significantly heavier than the corresponding ground estimates 4 2 the global distribution of rainfall extremes we are now in a position to extend the analysis of rainfall extremes over the conus to the entire tmpa domain which covers tropical subropical and mid latitude areas between 50 s and 50 n application of the mev distribution to the tmpa dataset is performed for each pixel independently as done for the conus and at the grid cell spatial scale the global distribution of extreme daily rainfall obtained from computing quantiles for a 50 year return period with the mev distribution is featured in fig 9 the most extreme rainfall values occur over subtropical regions with several hotspots over land which include south america and south east asia while the most intense quantiles are estimated to occur over ocean however the results obtained here by applying the error model should be regarded as representative of conditions over land only and validation over ocean should be pursued separately e g see sapiano and arkin 2009 prakash and gairola 2014 moreover the gauge correction of tmpa dataset performed at the monthly scale can hardly be performed over the ocean for lack of sufficient data this circumstance is expected to negatively impact the accuracy of qpe s over the ocean with respect to the accuracy over land when comparing quantiles estimated with gev and mev one can observe that i the main quantitative patterns of the two models estimates are quite coherent at the global scale and ii mev estimates exhibit a remarkable spatial coherence when compared to gev estimates while the estimation uncertainty of gev is heavily affected by the short dataset available here 20 years of annual maxima regionalization techniques are expected to decrease this uncertainty hosking and wallis 2005 bracken et al 2016 demirdjian et al 2018 schellander et al however we note that the reduction in estimation uncertainty achieved by using mev does not use information from nearby grid cells but uses the entire distribution of ordinary values from a single grid cell as an example we show a zoomed in representation of the 50 year event estimated for a domain in south east asia one of the regions characterized by most intense maximum rainfall accumulation according to the global scale analysis above 10 again mev estimates appear very coherent in space suggesting that the uncertainty intervals in quantile estimates are significantly reduced with respect to the gev model 5 discussion and conclusions in this paper we have described an approach for downscaling and validating satellite qpe statistics and for correcting and extending extreme value estimates over ungauged locations the methodology proposed here separately accounts for the scale difference between qpe gridded statistics and reference rain gauges at the ground and for errors in qpe estimates our cross validation results quantify estimation uncertainty and support the downscaling and correction procedures proposed suggesting that they can be applied to the entire conus and by extrapolation to many mid latitude areas at the global scale thus providing error estimates also where ground information is scarce the analysis over the conus provides useful insight over a range of different climatic regimes while optimally exploiting the ground observations available there for testing purposes we also note that the two main assumptions adopted here namely the adoption of the weibull distribution for daily rainfall accumulations across scales and of eq 7 for the spatial correlation can in principle be relaxed and tailored to local rainfall regimes as needed thus conferring to the proposed approach a greater flexibility while the weibull distribution has been advocated as a general model for the tail of the daily rainfall distribution at the global scale wilson and toumi 2005 the goodness of fit to daily values is expected to vary in different climate regions however the approach proposed here can be applied to other parametric models for daily rainfall provided that the first two moments exist the assumption of isotropic correlation function would be more difficult to relax as is the case for the taylor hypothesis used here to downscale the wet fraction of the rainfall process to the point while these are approximations a more refined description of the rainfall field would require the availability of information at the ground as our main objective here is to propose a methodology for application to data scarce regions we utilize these assumptions on the rainfall field to infer sub grid scale properties of the rainfall field from the qpe dataset alone one limitation encountered in section 2 5 concerns the downscaling of the spatial correlation function at locations where the correlation decays very rapidly with distance as is the case for sites characterized by particularly complex terrain while this situation occurs only for a small fraction of the sggc sites examined here these conditions may occur in many mountainous regions worldwide a possible way to address this limitation could be to develop spatially explicit models of the correlation based on local studies over complex terrain e g duan et al 2015 the results obtained in this study suggest that the gauge correction applied in order to obtain the tmpa research version dataset can lead to a significant deformation of the shape of the daily rainfall distribution this is especially relevant in areas where remote sensors fail to detect a significant fraction of events this consideration suggests the possibility of applying the qrf error model directly to the non adjusted real time datasets an interesting avenue of future research will be the application of this framework separately to the non adjusted precipitation statistics computed for different seasons this analysis would outline the performance of microwave retrievals in recovering rainfall statistics for different precipitation types and quantify their contributions to the discrepancies observed in the yearly rainfall distributions the aim of the downscaling approach proposed here is to infer rainfall statistics across spatial averaging scales at a fixed temporal integration scale daily in our application applications of the methodology to different temporal scales are possible and deserve further investigation extending the downscaling procedure to finer scales may encounter some limitations first the estimation of the spatial correlation becomes increasingly uncertain when moving to short time scales and the characteristic correlation length scale of the rainfall field decreases as discussed by gebremichael and krajewski 2004 at temporal scales where the correlation characteristic distance becomes smaller than the grid cell size the downscaling becomes unfeasible as discussed in section 2 5 second the taylor hypothesis approach introduced in zorzetto and marani 2019 and discussed here in section 2 3 requires information at temporal scales smaller that the temporal scale of interest in order to infer the wet fraction of the rainfall field at sub grid spatial scales therefore the shortest temporal scale at which the spatial downscaling analysis can be performed needs to be larger than the temporal resolution of the qpe dataset finally a third limitation concerns the use of eq 13 moving to progressively finer temporal scales the assumption of independent ordinary rainfall events becomes increasingly questionable as the temporal autocorrelation of the process becomes more significant therefore application of mev would require declustering of the observations e g as proposed by marra et al 2018 we also note that for the qrf error model to work adequately one should limit extrapolation of the qpe errors only to areas whose characteristics are represented in the training dataset however as we show in the cross validation experiment the model can be successfully trained over large areas using low density networks of rain gauges this was indeed the main objective of our study and it extends the domain where qpe validation is possible beyond the existing dense networks of rain gauges which tend to be available only in developed countries in densely populated areas and are lacking in arid locations and locations characterized by complex topography however where this information is not available the approach presented here provides a coherent quantification of estimation uncertainty accounting for the different intrinsic nature of spatially averaged satellite qpes and point observations at the ground the analyses proposed here can be used to inform the development of rain gauge networks covering a targeted set of sample climatic location to maximize the inference potential over ungauged sites an interesting research direction that builds on the results presented here is the possible extension of the methodology to a seasonal analysis of rainfall pdf s and correlation structure or possibly to a classification based on different precipitation types this type of differentiation could be useful either to assess more specifically the performance of remotely sensed qpes in different seasons and for different precipitation types and to study the distribution of extremes originated from multiple mechanisms credit authorship contribution statement enrico zorzetto data curation writing original draft writing review editing marco marani writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors acknowledge support from the national aeronautics and space administration fellowship nessf17 earth17f 0270 18 earth18r 0089 and 19 earth19r 27 and from venezia 2021 research grant provveditorato alle acque di venezia italy the qpe and rain gauge data sets used in this study are available online at https mirador gsfc nasa gov and https data nodc noaa gov respectively we wish to thank francesco marra and nadav peleg whose comments significantly improved the manuscript 
552,the quantitative validation of rainfall statistics obtained from space borne sensors and the evaluation of the associated uncertainty are hindered by i the limited coverage of rain gauge networks and weather radars at the ground and ii the different intrinsic statistical properties of point ground measurements and of area averaged remote sensing estimates this problem is particularly significant for extreme rainfall frequency analysis where the inference process is heavily impacted by observational uncertainty and short records of satellite quantitative rainfall estimates qpes here we develop an approach to the validation and correction of qpe extreme rainfall statistics over data scarce regions we employ a statistical technique for downscaling qpe probability density function spatial correlation structure and frequency of extreme events to sub grid scales so as to permit direct comparison with rain gauge point measurements extreme value modelling is performed using the metastatistical extreme value mev distribution which allows for the use of all available information from short qpe samples we test this framework over the conterminous united states conus providing a spatially extended characterization of the performance of the tropical rainfall measuring mission trmm multisatellite precipitation analysis tmpa dataset we then leverage this spatially explicit analysis to develop a non parametric model of the high quantile estimation error the model allows for the extrapolation of the qpe validation to ungauged locations we test this methodology by means of a cross validation approach using independent observations at the ground over the conus the results support the use of this approach to extreme rainfall estimation in ungauged areas paving the way to similar applications at the global scale keywords extreme rainfall rainfall downscaling gridded precipitation metastatistics list of symbols s generic linear spatial scale corresponding to spatial average over an area s 2 t generic integration time scale l linear spatial scale of the qpe grid cell l 0 linear spatial scale of rain gauge measurements h s daily rainfall accumulation magnitude at spatial averaging scale s hs ordinary rainfall amount realization at scale s i e rainfall magnitude in excess of the threshold q hs ordinary rainfall amount random variable q threshold used to determine ordinary rainfall events ns average yearly number of events at the spatial scale s nt number of daily rainfall observations in one year p r s wet fraction at spatial scale s ws weibull shape parameter at spatial scale s cs weibull scale parameter at spatial scale s σ s 2 variance of the rainfall process at spatial scale s β 0 β 0 l intermittency function γ 0 γ 0 l variance reduction function α shape parameter of the spatial correlation function ϵ transition parameter of the spatial correlation function d generic distance between two points in space ρ s s 1 s 2 ρ s d spatial correlation function at spatial scale s and distance d ρ h l h l correlation between local averages at scale l ρj areal averaged correlation value sampled between grid cells located at distance dj sse sum of squared errors α l shape parameter of the correlation function obtained from fit to tmpa gridded data ϵ l transition parameter of the correlation function obtained from fit to tmpa gridded data αg shape parameter of the correlation function obtained from fit to rain gauge data ϵ g transition parameter of the correlation function obtained from fit to rain gauge data γ 0 l variance reduction function computed from tmpa fitted correlation areal averaged γ 0 d variance reduction function computed from tmpa downscaled correlation γ 0 g variance reduction function computed from rain gauge fitted correlation c 0 g weibull scale parameter at a point from rain gauge data w 0 g weibull scale parameter at a point from rain gauge data n 0 g yearly number of events at a point from rain gauge data c 0 d weibull scale parameter at a point from downscaling w 0 d weibull scale parameter at a point from downscaling n 0 d yearly number of events at a point from downscaling c 0 c weibull scale parameter at a point corrected using the qrf error model w 0 c weibull scale parameter at a point corrected using the qrf error model n 0 c yearly number of events at a point corrected using the qrf error model ηz relative error between downscaled and gauge estimated values of a quantity z c w n or γ y response variable in the quantile regression forest algorithm realization of the r v y x multivariate explanatory variable in the quantile regression forest algorithm realization of the r v x ωi weights for the qrf algorithm k number of trees θ vector describe the tree structure σe standard deviation of elevation μe mean elevation h s m annual maxima daily rainfall at spatial scale s random variable c s i weibull scale parameter estimated for year i at spatial scale s w s i weibull shape parameter estimated for year i at spatial scale s n s i yearly number of events estimated for year i at spatial scale s pne non exceedance probability tr return time or average recurrence interval m number of years in the rainfall record used for extreme value analysis list of acronyms trmm tropical rainfall measuring mission tmpa trmm multisatellite precipitation analysis gpm global precipitation measurement mission imerg integrated multi satellite retrievals for gpm conus conterminous united states qpe quantitative precipitation estimates mev metatstatistical extreme value distribution gev generalized extreme value distribution qrf quantile regression forest rf random forest pot peak over threshold am annual maxima hpd hourly precipitation dataset noaa national oceanogaphic and atmospheric administration nasa national aeronautics and space administration sggc set of gauged grid cells used in the analysis 1 introduction quantitative precipitation estimates qpe from satellite borne sensors provide much needed information on the water cycle at the global scale and are an essential source of observations over large areas worldwide particularly where the density of rain gauge stations at the ground is low chen and wang 2018 kidd et al 2017 however while current algorithms try to optimally merge information from radar passive microwave and infrared sensors huffman et al 2007 2014 uncertainty in the rainfall retrievals from these instrumental sources inevitably propagates to the final multi sensor rainfall qpes the error structure of the resulting qpes is thus difficult to characterize as it depends on numerous variables including the specific sensor the precipitation type and surface characteristics huffman et al 2007 tian et al 2013 maggioni et al 2014 for this reason considerable effort has been devoted to the ground validation of satellite qpes based on the available observations at the ground among many others see nicholson et al 2003 su et al 2008 shen et al 2010 villarini 2010 chen et al 2013 bharti and singh 2015 manz et al 2017 monsieurs et al 2018 however the density of observational networks at the ground varies significantly around the world so that a spatially consistent characterization of the qpe error structure is to date very challenging over extended regions ideally the validation of satellite qpes should be performed by comparing the estimated rainfall rates or rainfall accumulation volumes with a ground truth dataset aggregated at the same integral domain in space and time peleg et al 2018 however this is in practice only possible with the use of exceptionally dense rain gauge networks of which a limited number exist worlwide e g see elliott et al 1993 villarini and krajewski 2007 foelsche et al 2017 pedersen et al 2010 peleg et al 2013 amitai et al 2012 villarini et al 2008 wang and wolff 2010 duan et al 2015 or by comparing qpes with ground radar estimates where these are available tao and barros 2010 kirstetter et al 2012 aghakouchak et al 2012 kirstetter et al 2013 marra et al 2017 these requirements severely hinder the qpe validation effort over complex terrain and poorly gauged areas the problem of validating qpe rainfall statistics is particularly challenging when the frequency of heavy rainfall is the variable of interest habib et al 2009 prakash et al 2016 while the global scale coverage and fine temporal scale of satellite qpes make them invaluable datasets for studying extreme rainfall at the global scale see e g zipser et al 2006 the inference on rainfall distributional tail properties is challenging due to the short length of homogenous records and to the observational uncertainty in satellite retrievals which inevitably produces large uncertainty in the estimated extreme rainfall quantiles recently the use of satellite qpe datasets for rainfall frequency analysis over ungauged regions and particularly for the estimation of intensity duration frequency curves has received increasing attention gado et al 2017 ombadi et al 2018 faridzad et al 2018 marra et al 2019 the main objective of this work is to contribute to bridging this gap by coupling a recently introduced downscaling technique zorzetto and marani 2019 with a model of the error for qpe derived rainfall statistics the key steps performed in this combined technique are i evaluation of key rainfall properties probability density function pdf spatial correlation structure intermittency and frequency of extremes from a gridded precipitation dataset ii downscaling these quantities so as to make a comparison possible with point measurements at the ground where these are available and iii developing a model of the error aimed at extrapolating the qpe error structure to locations where ground measurements are not available a key feature of the approach developed here is that it is specifically targeted towards the study of extreme rainfall frequency with important consquences for the use of satellite qpes for hydrological analysis and for studying the risk of natural hazards to this end we employ the metastatistical extreme value mev distribution marani and ignaccolo 2015 marra et al 2018 zorzetto et al 2016 an extreme value model which provides a connection between the pdf of rainfall accumulations and the frequency of extremes global scale applications of statistical value models to multi satellite qpe datasets have been performed before for example zhou et al 2015 apply the generalized extreme value distribution in order to map the average recurrence intervals of precipitation for real time precipitation monitoring demirdjian et al 2018 apply a recursive clustering algorithm which combined with the peak over threshold method e g coles et al 2001 reduces estimation uncertainty when applied to short remotely sensed datasets here we produce the first global scale maps of extreme value quantiles estimated with mev which i can reduce estimation uncertainty from short rainfall datasets as shown in zorzetto et al 2016 and ii by providing a link between extreme and non extreme rainfall statistics introduces a new way to study the effects of spatial averaging and qpe bias correction on the statistics of rainfall extremes overall the combined method of analysis developed here provides a novel framework for understanding how biases in qpe statistics propagate to rainfall frequency analysis and for designing suitable site specific corrections this last point has significant impact for ungauged areas worldwide where the density of stations or the length of the available records is not sufficient for obtaining reliable extreme value estimates with traditional statistical techniques the statistical framework outlined above is here developed and applied to a large domain covering the entire conterminous united states conus this domain is covered by a large number of gauges and ground radar derived precipitation products so that the performance of satellite sensors and qpe datasets have been extensively studied here e g aghakouchak et al 2011 chen et al 2013 prat and nelson 2015 this makes it the ideal domain for testing our methodology which does not make use of any ground observation for the downscaling and only requires sparse gauge observations or radar data for constructing the model for the error therefore ground rain gauge observations and comparison with previous studies can be used for independently testing the performance of the method and for validating our findings moreover the conus domain spans a broad range of climatic regimes and terrain types thus allowing us to extend our previous work zorzetto and marani 2019 and explore the variability of qpe rainfall statistics and the associated error structure over this broader domain here we focus our attention on the tropical rainfall measuring mission trmm multisatellite precipitation analysis tmpa 3b42 research version dataset which to date includes 20 years of continous data and even after the end of the trmm mission is a well tested source of information for hydrological applications however we note that the approach developed here is not product dependent and is suitable for a straightforward application to any gridded precipitation product such as the integrated multi satellite retrievals for gpm imerg product huffman et al 2014 or even reanalysis products and climate model outputs the manuscript is organized as follows in section 2 we present the statistical technique used to downscale rainfall statistics from gridded qpe datasets so as to enable a direct comparison with point gauge measurements building on this method in section 3 we develop a non parametric model for the error based on a quantile regression forest algorithm meinshausen 2006 the error model is then tested using a cross validation scheme so as to effectively simulate the prediction of qpe rainfall statistics over ungauged areas in section 4 we then present the statistical framework aimed at the large scale extreme rainfall value analysis based on gridded qpe precipitation products and discuss the current limitations in evaluating the results over poorly instrumented areas the extreme value estimates are then corrected and tested using the model of the error developed in section 3 results of these steps are presented throughout these sections and are followed by a discussion of our findings and by the conclusions drawn by this study section 5 2 spatial downscaling of the probability distribution of qpe magnitudes we start by addressing the scale gap between daily rainfall statistics estimated from tmpa gridded qpe fields and their counterpart derived from rain gauge point measurements for this purpose we employ a recently developed statistical downscaling technique zorzetto and marani 2019 based on the theory of random fields vanmarcke 2010 müller and thompson 2013 while this technique entails a number of assumptions it greatly extends the areas where validation of gridded qpes datasets is possible as it does not require the presence of a dense network of rainfall gauges or ground radars covering the location of interest for downscaling qpe statistics the main assumptions made in deriving this methodology are i local spatial homogeneity and isotropy of the rainfall field ii a taylor frozen turbulence hypothesis for obtaining the wet fraction at a point in space and iii the assumption that the distribution of daily rainfall events can be represented as a stretched exponential both at the point scale and at the grid cell scale albeit with varying parameters in the following we briefly summarize the downscaling method applied here for additional information on the derivation of the methods the reader is referred to zorzetto and marani 2019 we define as ordinary rainfall events all daily rainfall accumulations at a given temporal integration scale in excess of a fixed low threshold value here set to q 1 mm day we choose a fixed threshold to define ordinary daily rainfall events which is larger than both the detection limits of gauges and qpe data when aggregated at the daily timescale while at the same time low enough so as to include the bulk of the daily rainfall distribution in our analysis we denote with ns the yearly number of such events or wet days the wet fraction is thus defined as p r s n s n t with n t 366 the number of daily observations in each year the suffix s here indicates the linear spatial scale s at which a rainfall time series is averaged here s 0 indicates the point scale l 0 corresponding to rain gauge measurements the characteristic size of a rain gauge is of the order of l 0 10 4 km and s l corresponds to the linear characteristic scale of the gridded qpe to be downscaled l l x l y where lx and ly are the dimension of a qpe grid cell along the zonal and meridianal directions respectively since we are dealing with spatial domains which are well characterized by a single length scale here we use a single linear length scale s to characterize a random field averaged over an area s 2 we assume that in each year the ordinary rainfall event magnitudes observed at scale s h s h s q defined for daily rainfall totals h s q are realizations of a random variable hs with population ω h s 0 and marginal distribution described by a weibull or stretched exponential distribution defined by the cumulative probability function 1 p h s h s f h s 1 e h s c s w s here cs is a scale parameter with the same units of hs mm day here and ws is the dimensionless shape parameter of the distribution a recent study analyzed the tail properties of hourly rainfall accumulations over the conus papalexiou et al 2018 and found that weibull provides a satisfactory description of hourly rainfall observations complementing and reinforcing previous evidence for rain gauge daily accumulations at the global scale wilson and toumi 2005 while the weibull parameter cs represents the characteristic magnitude of daily rainfall events ws describes the decay of the tail of f hs such that values of ws 1 correspond to a sub exponential behavior i e the exceedance probability exhibits a heavy tail albeit with a characteristic scale an exponential decay is recovered for w s 1 and values of ws larger than unity correspond to faster than exponential decays this simple parametric model is assumed to describe the shape of the daily rainfall distribution of both rain gauge and satellite derived rainfall accumulations such that their respective difference is encoded in differences between the values of cs and ws at the different spatial scales 2 1 downscaling scheme zorzetto and marani 2019 showed that the parameters of the distribution of ordinary rainfall events in eq 1 aggregated at a fixed temporal scale and averaged at two different spatial scales can be linked by two equations which depend on the intermittency and spatial correlation of the rainfall field for example the stretched exponential parameters cl wl describing the ordinary rainfall pdf at the grid cell scale s l of the qpe dataset can be expressed in terms of the parameters c 0 and w 0 of the rainfall field at the point scale s 0 by the relations 2 γ 0 β 0 2 w 0 γ 2 w 0 γ 2 1 w 0 2 w l γ 2 w l γ 2 1 w l γ 0 1 p r l 3 c 0 w 0 2 β 0 2 c l w l 2 γ 2 1 w l γ 2 1 w 0 where γ denotes the gamma function p r l is the wet fraction of the ordinary rainfall process at the grid cell scale l γ 0 γ 0 l is the variance reduction factor i e the ratio of the variances σ l 2 and σ 0 2 of the process averaged at the two scales s l and s l 0 respectively and β 0 β 0 l p r l p r 0 or intermittency function is the ratio of the wet fraction at the two scales considered here while p r l can be directly computed from qpe time series γ 0 and β 0 are not known in the absence of rain gauge measurements since they do not only depend on the areal average process but also on the rainfall process at scale l 0 i e at a point in space 2 2 scalewise variation of the spatial correlation function however the variance reduction function γ 0 can be obtained from the spatial correlation of the rainfall field ρ s 1 s 2 here assumed to be quadrant symmetric vanmarcke 2010 where s 1 and s 2 are distances between two points measured along two coordinate axes as 4 γ 0 σ l 2 σ 0 2 4 l x 2 l y 2 0 l x 0 l y l x s 1 l y s 2 ρ s 1 s 2 d s 1 d s 2 where lx and ly are the spatial dimensions of the grid cell size ideally the point correlation ρ s 1 s 2 should be estimated using a sufficient number of rain gauges distributed in space as these are not available everywhere in fact they are lacking in most areas here we estimate it using the correlation between qpes time series sampled at grid cells within a 3 3 local neighborhood this correlation between spatially averaged values can be linked to the unknown correlation at the point scale by means of the following relation vanmarcke 2010 5 ρ h l h l k 0 3 l 0 3 1 k 1 l δ l x k l y l 4 δ l x l y where the set of distances l x k and l y l for k l 0 1 2 3 contains the information on the relative positions of pairs of cells within the neighborhood considered letting δx and δy be the distances between the two grid cells along the s 1 and s 2 coordinate directions then these distances are defined as l x 0 δ x l x l y 0 δ y l y l x 1 δ x l y 1 δ y l x 2 δ x l x l y 2 δ y l y l x 3 δ x l y 3 δ y and with the function δ a b defined as 6 δ a b 4 0 a 0 b a s 1 b s 2 ρ s 1 s 2 d s 1 d s 2 since the qpe dataset allows the computation of estimates of ρ h l h l we can obtain an estimate of the point scale correlation by assuming a parametric form for the spatial correlation function ρ s 1 s 2 and by numerically minimizing the sum of squared errors sse between its areal average value at the grid cell scale given by eq 5 and the values estimated from qpes we assume here an isotropic correlation function such that ρ s 1 s 2 ρ s 1 2 s 2 2 ρ d 7 ρ d ϵ α e α d ϵ d ϵ ϵ e d α d ϵ this formulation has an exponential kernel and a power law tail characterized by the exponent α the smooth transition between the two regimes occur at a distance d ϵ marani 2003 zorzetto and marani 2019 depending on the values of the parameters α and ϵ it allows for the description of both light and heavy tailed correlations with this assumption the sse becomes a function of the parameters α and ϵ 8 s s e ϵ α j 1 m ρ h l h l d j ϵ α ρ j 2 where the ρj are correlation estimates obtained from the qpe dataset using a local lattice of 3x3 grid cells centered over the location of interest using a larger lattice would increase the number of points available to estimate the correlation function but on the other hand would increase the likelihood of including in the analysis non homogeneous rainfall statistics which certaintly are present at the regional scale especially over a complex terrain for each pair of grid cells within this local domain the pearson correlation was computed between the respective qpe time series and the resulting values binned over a set m of distances dj here we minimize the sse in eq 8 by means of the differential evolution stochastic minimization algorithm storn and price 1997 as opposed to the deterministic algorithm used in zorzetto and marani 2019 the differential evolution minimization is particularly suited to avoid possible local minima in the α ϵ parametric space and thus more robust in minimizing a function which depends upon the experimental points ρj we seek the global minimum of eq 8 within the rectangular domain α 0 1 and ϵ 0 1000 km which are physically meaningful ranges of values while quantifying the uncertainty in the downscaled correlation parameters is a challenging task as the shape of objective function eq 8 varies with the observed values ρj we note that the downscaling methods only requires the functional of the correlation function defined in eq 4 which is an integral property of the correlation function over the pixel size therefore we expect γ 0 to be less sensitive to observational uncertainty than the single parameter values α and ϵ 2 3 scalewise variation of the wet fraction to obtain an estimate of the intermittency function β 0 l i e of the ratio between the yearly number of events for the ordinary rainfall process averaged at the grid cell scale l 0 with respect to the point scale we rely on an application of the taylor frozen turbulence hypothesis taylor 1938 deidda 2000 haerter et al 2015 introduced by zorzetto and marani 2019 this approximation enables us to use of information from the qpe dataset at smaller temporal scales up to 3 hours for the tmpa dataset to infer a property of the rainfall field namely the wet fraction at spatial scales smaller than the grid cell scale and at the daily timescale we aggregate tmpa rainfall fields at increasing spatial scales s and temporal scales t integrating in time and averaging in space and at each space time scale the wet fraction pr s t is computed as the fraction above q of the time series at scale s t we then extrapolate this quantity to the point scale l 0 by assuming that the function pr s t is locally linear in the s t plane i e that integration in time and averaging in space have the same effect on a property of the rainfall field the wet fraction here up to a constant factor which has the meaning of a local advection velocity this assumption holds exactly only in the case of a perfectly fractal rainfall field haerter et al 2015 but even though only approximate in general it offers a conceptually satisfactory way to infer the point scale wet fraction using only qpe data at the grid cell scale through this approach we obtain β 0 p r l p r 0 where p r l is simply obtained from the remote sensing qpe and p r 0 is the value that a linear extrapolation in the s t plane yields for daily rainfall at the rain gauge spatial scale l 0 note that in order to apply this technique we need a sufficiently fine scale resolution in time for the qpe dataset compared to the scale at which the downscaling analysis is performed 3 h versus daily here the reader is referred to zorzetto and marani 2019 for a more detailed description and application of the methodology 2 4 data an independent evaluation of the statistical structure of a qpe dataset using the approach described above requires observations from rain gauges at the ground we focus on the conus domain defined here as the domain over land within 22 n 50 n and 130 w 60 w over this domain we use observations from the network of hourly precipitation data hpd rain gauges of the national oceanogaphic and atmospheric administration noaa which covers the conus starting from 1948 ncep precipitation data were aggregated at the daily time scale by summing hourly accumulations in each day of the record defined starting from midnight we excluded from the analysis days with quality flagged or missing precipitation amounts the remotely sensed gridded precipitation product used in this study is tmpa 3b42 version 7 research version huffman et al 2007 2010 huffman and bolvin 2013 of which we use the entire record 1998 2018 so as to obtain the longest possible dataset for extreme value analysis note that starting in 2014 after the end of the trmm era rainfall retrievals from the gpm mission are used in tmpa estimates introducing some heterogeneity in the dataset rainfall rates obtained from the tmpa dataset are here assumed to represent the average rainfall rate for each 3 hr timestep these values were aggregated so as to compute rainfall accumulations at time scales raging from 3 h to 48 h as needed for estimating the intermittency of the rainfall field as discussed in section 2 3 with the 24 h totals used for our analysis at the daily time scale we note that discrepancies between gauge and tmpa daily totals may potentially arise in the presence of a pronounced daily precipitation cycle and as a consequence of the instantaneous nature as well as the timing of satellite rainfall retrievals libertino et al 2016 here we do not anticipate that this issue will play a relevant role in our analysis since we do not study the timing of specific events but only average statistics the spatial resolution of the data is 0 25 0 25 corresponding to a characteristic grid cell size of about 25km over the conus domain while here we interpret the tmpa rainfall rates as areal averages over the grid cell area see e g villarini and krajewski 2007 this is an approximation as these estimates are obtained by merging retrievals from different instrumental sources passive microwaves and infrared with different footprints additionally the monthly scale gauge correction is performed using precipitation gridded at scales larger than the spatial reolution of tmpa and therefore introduces larger scale information in the pixel scale qpe time series in essence any discrepancy stemming from this assumption will be included in the structure of the error that will be estimated as a result of our analysis in order to evaluate qpe statistics after downscaling we select a set of qpe grid cells from the tmpa dataset that are characterized by the presence of i at least one rain gauge within the grid cell with at least a 10 year record of daily rainfall and ii at least 4 rain gauges in a 5x5 pixel neighborhood around the location of interest which are used for producing an estimate of the local spatial correlation of the rainfall field this information is not used to train the downscaling method but only for validation purposes to be selected these rain gauge records must overlap for a temporal window of at least 2000 observations i e about 7 years of record this condition was chosen as a tradeoff between having enough data for reliably estimating the correlation between sites and including a large enough number of sites for testing purposes our results are not very sensitive to this choice as for most of the gauged sites in the dataset the record length is significantly longer than 7 years in the following we will refer to the set of sites meeting these conditions as the set of gauged grid cells or sggc 2 5 results of the downscaling method to study the spatial correlation of the rainfall field we focus on two metrics the ratio between the two parameters of the correlation function ϵ α and the variance reduction function γ 0 the first is a characteristic spatial scale describing how the correlation decays with distance for very large values of ϵ this is exactly the spatial integral scale of the field which is not necessarily finite depending on the values of α while γ 0 is an integral property of the correlation function between the point and the grid cell scale γ 0 is the main quantity of interest here as it connects the variance of the rainfall accumulations at different spatial averaging scales and it is the only quantity dependent on the correlation function that is directly needed in downscaling the pdf of ordinary rainfall in eqs 2 and 3 we note that three different cases can be identified when downscaling the spatial correlation function from gridded qpes when the correlation scale of the rainfall field at the ground is small compared to the spatial averaging scale ϵ αl 1 then the correlation structure of the continuous process is in large part hidden by the averaging process and we expect it cannot be completely recoverable from the qpe dataset alone this occurs only in a very limited subset of the sggc sites examined here we find that the ratio ϵ α is smaller than 25km only at 33 of the 860 sggc sites where downscaling is performed 3 8 of all cases these are primarily locations characterized by a complex terrain and it is reasonable to think that in these cases the correlation at the point scale is not retrievable from qpe alone we therefore exclude these locations from the following analysis conversely when the correlation of the field is much larger than the grid scale over which averaging is performed most of the information about the correlation function is preserved even after averaging in this case we argue that the effect of and need for a downscaling process may be limited because the variables being averaged are highly correlated and thus they tend to behave very similarly to their spatil average this case also does not occur frequently with only 8 out of the 860 sggc sites examined here exhibiting ratios ϵ α larger than 200km the largest ratio being around 8 in the intermediate case between these two situations when the correlation distance of the rainfall field is comparable to or larger than the scale of averaging a substantial part of the information about the correlation structure of the continuous process at the point is preserved by the averaging process the application of the downscaling approach i e the minimization of eq 8 can produce in this case good estimates of the correlation structure at the point scale to provide a global metric describing the effect of correlation downscaling we compare the estimated values of ϵ α and γ 0 with the corresponding metric ϵ l α l obtained from fitting eq 7 to the empirical correlation estimates ρj dj obtained from a local lattice of qpe time series in turn using ϵ l and α l instead of ϵ and α in eq 4 we can infer an estimate γ 0 l of the variance reduction function value that we would obtain by using the grid cell scale correlation function instead of its downscaled version examination of the values of rainfall spatial correlation over the conus domain here indexed by longitudinal position see fig 1 a shows that the tmpa estimated correlation obtained by fitting eq 7 to the gridded qpes is larger than its estimates from point measurements at the ground this is as expected since we are comparing a correlation between averaged values with its point scale counterpart when we compare the downscaled correlation values obtained from the qpe dataset by minimizing eq 8 we see that some underestimation occurs primarily at the boundaries of the domain and chiefly for grid cells located in the proximity of the west coast this result is consistent with a previous study which investigated the ability of radar rainfall retrievals to reproduce the small scale rainfall variability gebremichael and krajewski 2004 however in the central part of the conus domain the zonal variability of the correlation appears to be reasonably captured by the tmpa estimates the application of the downscaling approach produces correlation values that are much closer to those from gauge observations suggesting that indeed the proposed downscaling procedure yields statistical properties that are close to those of point observations a similar result emerges if we turn our attention to the variance reduction function fig 1b for which we again notice how the downscaled results are closer to values from ground observations for the central part of the conus and less so along the east and west coasts a summary of the variability of the downscaled parameters over the sggc is provided by the scatter plots in fig 2 while no apparent bias appears for the values of γ 0 d with respect their counterpart estimated at the ground a significant variability is detected for the lowest values of γ 0 which correspond to grid cells located primarity in the mountainous part of the western united states fig 2a this scatter for low values of γ 0 confirms the increasing difficulty of correctly estimating the point correlation when its characteristic length scale becomes smaller compared to the averaging length l however note that the large majority of the sggc sites has values of γ 0 larger than 0 9 range in which the discrepancy between ground and downscaled values is limited as shown in panel 2a the comparison of the number of wet days n 0 d obtained from downscaling tmpa qpe s with gauge estimated values n 0 g indicates that the qpe dataset consistently underestimates the number of events recorded at the ground fig 2b and that this underestimation increases with the value of n 0 g while the number of events n 0 d is sistematically underestimated the scale parameter c 0 d appears to be significantly larger than its corresponding ground values fig 2c we argue that this feature of the qpe dataset is at least partially explained by the gauge correction applied to the tmpa 3b42 research version dataset gauge data are used to rescale tmpa monthly totals and thus if the number of missed events is appreciable such a correction will result in a deformation of the pdf of ordinary events with respect to the real one at the ground and will lead to the overestimation of the characteristic event size c 0 d so that monthly totals measured at the ground are preserved the downscaled shape parameter w 0 d appears to be quite variable with respect to its gauge estimated counterpart w 0 g fig 2d as was the case for γ 0 even though most of the values here are close to the identity line for some points corresponding to locations in the western united states the downscaled values of w 0 are significantly lower than gauge estimates meaning that for these locations the downscaled ordinary rainfall statistics exhibit a heavier tail i e an overestimated probability of intense events to study the spatial distribution of the discrepancy between qpe downscaled statistics and the corresponding values at the ground we define the relative errors 9 η z z 0 d z 0 g z 0 g where the variable of interest can be one of the following z c w n or γ the subscript 0 again refers to values at the point spatial scale l 0 and the subscripts d and g refer to the downscaled or rain gauge observed quantities respectively fig 3 reports the spatial variability of the relative errors ηz for the four parameters and the set of sggc sites the map of the relative errors ηγ in the variance reduction function over the conus is featured in panel 3a the variance reduction function γ 0 appears to be well captured by the downscaled correlation especially over the east coast and in the midwest regions of the conus some overestimation appears corresponding to the west coast while γ 0 tends to be generally underestimated with respect to ground values in the western conus the error in the yearly number of events appears to be larger on the west coast and in the north east whereas it is smaller in the south east and mid west regions of the usa fig 3b once the variance reduction function and intermittency functions are known the parameters describing the pdf of ordinary rainfall events at the point scale can be estimated by means of eqs 2 and 3 for the scale parameter c 0 some overestimation is found to occur especially in the northeastern and northwestern sectors of the conus while mostly underestimation occurs in the west fig 3c conversely the shape parameter w 0 appears to be underestimated in the west and overestimated in the east fig 3d together these results suggest that in the west downscaled qpe statistics exhibit heavier tails and lower mean compared to their ground counterparts while the opposite is true for the eastern usa and the pacific coast where both parameters are overestimated the characteristic event magnitude is overestimated but the tail of the distribution is lighter than ground estimates suggest these results are coherent with previous work such as prat and nelson 2015 which found that tmpa 3b42 shows a consistent underestimantion of the yearly number of events especially in the north east and middle atlantic regions of the conus therefore the results obtained here for the weibull scale parameter are also to be expected as the gauge correction applied to tmpa by rescaling the monthly total accumulations will determine an overestimation of the characteristic event size in the number of events is underestimated as we will discuss in the following section these distortions of the pdf of daily rainfall have relevant consequences for the estimation of extremes 3 a model of the error for qpe downscaled statistics as discussed above downscaling the pdf of rainfall accumulations allows for the direct validation and correction of remotely sensed qpe s statistics at gauged sites this result is useful in itself as downscaling allows for proper comparisons of quantities defined at the same scale and it reduces the density of the gauge networks required for validating qpe derived rainfall estimates however for many areas worldwide which are characterized by sparse or altogether absent ground stations it is of primary importance to provide the error analyses that can be applied to ungauged locations in the case of target sites located at a limited distance from gauged locations this objective can be pursued by building a geo spatial model of qpe error statistics here we investigate how qpe errors can be predicted from the information collected at gauged sites located at considerable distance from the target sites so that they are characterized by weak or no direct correlation to this end we develop a non parametric model of the error based on the quantile regression forest qrf algorithm meinshausen 2006 with the objective of inferring the relative errors in the downscaled parameters y η c η w or ηn at ungauged locations based on a set of variables describing the local rainfall regime and terrain type qrf has been applied before to study the error structure of passive microwave rainfall retrievals bhuiyan et al 2017 and constitutes a modification of the classic random forest rf algorithm introduced by breiman 2001 this modification of the rf algorithm is chosen for its ability to 1 describe the generally nonlinear relations between the error in the downscaled parameters and a set of explanatory features boulesteix et al 2012 2 limit the problem of overfitting breiman 2001 tyralis et al 2019 and 3 deal with possibly correlated predictor variables ziegler and könig 2014 predictions from these models are based on regression trees structures which encode the recursive partitioning of the predictor variable space in distinct and non overlapping regions in each terminal node of a tree the predicted value of the target variable is estimated as the average response in that region predictions of rf and qrf are ensemble values over a large number k of decision trees each trained on a bootstrap sample from the original dataset instead of predicting the expected value of the variable of interest for a given value of the predictor as is the case for the rf method qrf estimates the full conditional distribution of the response variable y given a value of x a possibly multi dimensional predictor vector reconstructing the entire conditional distribution can be useful here for i limiting the possible effect of outliers on the estimated conditional mean using the median value instead and ii constructing confidence intervals for the estimates of interest qrf estimates the empirical distribution function of y for a given value x of the explanatory variable x as meinshausen 2006 10 p y y x x i 1 n ω i x 1 y i y where 11 ω i x k 1 t 1 k ω i x θ t here k is the number of trees and each tree is built from an independent and identically distributed vector θ t t 1 k which encodes the information on which predictors are used as split point in each node of the tree the indicator function 1 y i y assumes the value 1 when the observation yi y and is equal to 0 otherwise the quantities ωi x θt are weights obtained from observation yi of the response variable for the tth tree they are defined as 12 ω i x θ t 1 x i r l x θ t j 1 n 1 j x j r l x θ y where again 1 denotes the indicator function so that they are zero if observation xi does not belong to leaf l x θt and are positive otherwise the leaf l x θt refers to the terminal node of tree t individuating the region r l x θ y obtained by partitioning the predictor variable space at each node split for the present application we select a set of explanatory features which are representative of the local rainfall climatology and which can all be obtained from a remotely sensed dataset without the need of observations at the ground for this purpose we identify as predictors the stretched exponential scale cl and shape wl parameters the average yearly number of events nl and the variance reduction function γ 0 as estimated from the qpe dataset additional predictors we consider to describe local environmental conditions are the mean elevation averaged over the qpe grid cell μe and the standard deviation of the elevation σe computed over a domain of size 1 25 x1 25 degrees centered over the location of interest in order to account nearby orographic features predictor variables were normalized by subtracting their averages and diving by their standard deviations elevation data were obtained from the global topographic map obtained from noaa national center for environmental information amante and eakins 2009 which provides elevation with respect to mean sea level globally at 1 arc minute resolution this information is here averaged so as to match the tmpa grid over the conus the qrf model is here applied by training k 2000 decision trees although experiments were carried out with different values of k to make sure the results were not overly sensitive to this choice we note that for their nature rf and qrf models should not be used in extrapolation outside the range of the predictors used for training instead they should be trained using a set of observations representative of the range in which we wish to obtain predictions therefore when applied to a heterogeneous region such as the conus one would like to understand if error predictions are possible based on sparse and low density network of training sites which is the working condition over many areas worlwide here we address this question by designing a cross validation procedure in which we take advantage of the relatively dense observational network available in this study over the conus to simulate how the method performs when trained using a low density gauge network and tested on an independent dataset first locations within the set of gauged sites sggc are randomly resampled then starting from a random location we construct a sample by examining sequentially all sggc sites and randomly inspecting and removing all gauged sites located at a distance less than 100km from any sites already included in the sample by doing so we obtain a sample size of about 175 sites the exact number varies at each realization and obtain a dataset in which selected sites are at most weakly spatially correlated thus allowing for proper calibration and testing of the error model second we randomly divide the stations obtained in stage 1 into two independent sets used for training the model and for independent testing respectively here we use 50 of the sites for calibration and the remining 50 for validation of the qrf model we repeat these two steps a number of times n g 20 in the analysis reported here and pool together the results to obtain a global measure of performance over the test sites extracted in each realization in each testing or training site we performed the downscaling of qpe statistics as described in the previous section 2 obtaining estimates of the parameters c 0 d and w 0 d as well as of the yearly number of events n 0 d at a point moreover at these sites independent rain gauge estimates of these quantities at the ground c 0 g w 0 g and n 0 g are also available therefore we can obtain a measure of the error in downscaling these variables by simply applying eq 9 results of this analysis are featured in fig 4 which compares the values of the errors ηc ηw and ηn predicted for the test sites with the true values of such errors obtained using gauge records at the ground in addition the lower panels in fig 4 show the values c 0 c w 0 c and n 0 c obtained by correcting the downscaled parameters using the qrf error model the qrf corrected values show a good agreement with their counterpart from rain gauges at the ground especially for the scale parameter c 0 c and the number of events n 0 c a somewhat larger scatter occurs for the shape parameter w 0 c overall this result constitutes a clear improvement with respect to the downscaled parameter values shown in fig 2 while using an ensemble of decision trees as done in the qrf algorithm allows to reduce prediction uncertainty and improve stability with respect to single decision trees these improvements occur at the expense of the interpretability of the model nevertheless it is possible to obtain an estimate of the relative importance of the predictor variables in our regression model the importance of a given predictor is computed as the mean decrease in the sum of squared residuals achieved at each node split by selecting that predictor james et al 2013 the importance of the features used in the qrf model to predict the three errors of interest ηc ηw and ηn is quantified in fig 5 the error in the number of events seems to depend primarily on the characteristic scale of ordinary rainfall events cl whereas for the error in the scale parameter ηc the most important features are the parameters of the ordinary rainfall distribution at the qpe grid cells scale in the case of the error in the shape parameter ηw the most important predictors are those representing local orography and the scale parameter cl the average number of events does not play a primary role for either of the three error variables and is only somewhat relevant for predicting the values of ηc however we note that these parameter importances should be interpreted with caution as the predictor variables are not completely independent between each other while this is not an issue for the qrf error predictions this should be kept in mind when examining parameter importances the dependence of the shape parameter error from elevation suggested by the qrf analysis is not a surprise given its spatial distribution shown examined in fig 3 however it is interesting to note that the role of orography seems to be more substantial in explaining errors in the tail of the distribution i e in w 0 rather than on the characteristic rainfall magnitude or in the yearly average number of events 4 the statistical distribution of extreme events from remotely sensed rainfall we turn here our attention to studying the probability distribution of extreme events based on tmpa qpe s building on the tools developed and tested in the previous sections to this end here we provide the first large scale application of the metastatistical extreme value mev distribution to remotely sensed qpe fields following marani and ignaccolo 2015 zorzetto et al 2016 zorzetto and marani 2019 we study the distribution of the annual maximum h s m among a variable number of independent and identically distributed ordinary rainfall events here at the daily time scale the parameters describing the distribution of ordinary events as well as the yearly number of events are themselves considered to be random variables whose realizations are estimated in each year of a rainfall record from a sample of m years of daily rainfall observations we thus estimate the cumulative probability distribution of the annual maximum according to the mev distribution 13 p h s m h ζ s h 1 m i 1 m f s i h n s i where for each year i 1 m in the rainfall time series the yearly number of ordinary events is n s i and p i h s h f s i h is the distribution of ordinary events in year i of the rainfall record which here we assume to be a stretched exponential with cumulative probability 14 f s i h 1 e h c s i w s i with yearly scale and shape parameters c s i and w s i and spatial averaging scale s the main idea behing the use of eq 13 for modelling daily rainfall accumulations extreme values is the recognition that low frequency variability in the rainfall generating mechanism can produce heavier tails than would be otherwise observed this effect is captured in eq 13 by averaging over parameter values estimated independently for each year in the record zorzetto et al 2016 while the weibull distribution is widely used for hourly to daily accumulations wilson and toumi 2005 papalexiou et al 2018 we note that eq 13 can be tailored to different parametric models for the probability distribution of ordinary rainfall events thus conferring to the mev distribution significant application flexibility for computing rainfall quantiles for a given cumulative probability pne or return time t r 1 1 p n e we can invert eq 13 and evaluate quantiles as 15 h s t r q ζ s 1 p n e parameters for the mev and generalized extreme value gev distributions were estimated by means of probability weighted moments greenwood et al 1979 and l moments hosking et al 1985 respectively for the purpose of computing extreme value statistics years with more than 10 of missing observations 36 data points at the daily scale were not included in the analysis as the selection of annual maxima values as well as the estimation of daily rainfall statistics would be potentially biased in the presence of relevant fractions of missing data 4 1 the mev distribution applied to tmpa qpe s the expected value of the daily rainfall accumulation corresponding to a 50 years return time was obtained by applying the mev distribution to the tmpa 3b42 v7 research version for the conus for comparison we produced the same estimate by fitting the generalized extreme value gev distribution e g coles et al 2001 to the series of annual maxima am extracted from the same dataset the large scale spatial features of extreme rainfall frequency over the conus are similarly captured by the two models fig 6 interestingly the spatial distribution of mev estimated quantiles is significantly smoother when compared with its gev counterpart this is consistent with the notion that mev by using all the observations available produces more stable estimates zorzetto et al 2016 that are also less sensitive to outliers inevitably present in remote sensing estimates as also noted in marra et al 2019 gev estimates are on the contrary dominated by a few large outlier values as manifest in the grainy texture of fig 6b we argue that this behavior originates from the fact that mev inferences are based upon the entire distribution of ordinary events determining a decreased sensitivity to biases in qpe estimates of large rainfall values compared to annual maxima or peaks over threshold approaches as discussed in mehran and aghakouchak 2014 while tmpa is able to capture rainfall from intense convective events the accuracy of the estimates progressively descreses when limiting the analysis to rainfall rates exceeding increasingly high threshold values a second observation is that the 50 year quantiles estimated by mev appear to be larger than those estimated by gev in many areas over the conus especially in the midwest this feature does not appear to be uniquely inherent to the statistical models used if we repeat the same analysis for the rain gauges from the noaa hourly precipitation dataset hpd aggregated at the same daily scale fig 6c d we find that the two statistical models yield much more similar results in this case although the 50 year quantile field obtained from mev remains spatially smoother that its gev counterpart it is reasonable that being the mev and gev distribution fitted using different parts of the dataset annual maxima in the case of the gev and the bulk of the distribution in the case of mev the extreme values estimated through the two formulations should respond differently to distortions in the pdf of the qpe magnitudes however a direct comparison of the extreme value quantiles reported in panels a b and c d of fig 6 is not feasible because of the scale discrepancy between qpe s and point rain gauge measurements eq 13 provides a direct link between properties of the ordinary daily rainfall distribution and the frequency of extreme values hence using downscaled parameters of the ordinary rainfall distribution inferred from remote sensing it can be used to directly compare the mev derived extremes at a point with those from independent rain gauge records and to possibly correct for emerging discrepancies here we test this idea by extending the cross validation method employed in section 3 for testing how the correction applied to the stretched exponential parameters affects extreme rainfall quantile estimates for each iteration in the cross validation scheme once the tmpa parameters c l i w l i and n l i for i 1 m have been computed over the test sites they are used to provide an estimate of the t r 50 years quantile h l t r by means of eq 15 analogously the downscaled c 0 d i w 0 d i n 0 d i and corrected c 0 c i w 0 c i n 0 c i values of the parameters are used to estimate the corresponding quantiles at a point h 0 d t r and h 0 c t r respectively for each testing site we independently compute the mev parameters c 0 g i w 0 g i n 0 g i and quantiles h 0 g t r from the rain gauge record at the ground we note that we estimate the parameters c s i w s i and the number of events n s i appearing in eq 13 separately for each year in the rainfall record so as to account for their inter annual variability we then downscale each yearly value of the parameters using the equations described in section 2 and correct them using the error model described in section 3 note that the downscaling and correction relations used are the same for each year in the record when downscaling the weibull parameters for estimating mev at subgrid scales we apply eqs 2 and 3 to each set of yearly parameter values separately using the constant values of the variance reduction function and intermittency function derived from the entire qpe time series available in principle the spatial correlation of precipitation may also vary year to year e g as a consequence of the varying frequency of different precipitation types however given the known difficulty of estimating values of correlation for skewed non gaussian processes such as precipitation at the daily time scale gebremichael and krajewski 2004 we did not include this possible source of inter annual variability in the model and computed correlation values from the entire qpe time series therefore in our application the intermittency of the rainfall field and the decrease of its variance with averaging scale are the same for each year of observations the downscaled mev parameters are then corrected for each year of the record using the median value of the relative error predicted by the qrf algorithm as for the parameter downscaling we do not explicitly correct for possible biases in the inter annual variability of the parameter estimates result of this comparison between tmpa downscaled and corrected quantiles with the corresponding rain gauge quantiles h 0 g is reported in fig 7 in the form of scatter plots panel 7a and pdf of the relative error between estimated values and reference rain gauge quantiles panel 7b we can see that qpe quantiles tend to consistently overestimate rain gauge quantiles even though they have a different nature as they are best interpreted as areal average quantiles over the pixel as opposed to values at the point this is clearly a byproduct of the biases observed in the tmpa estimated weibull parameters especially the overestimation of the scale parameter observed in fig 2c turning our attention to the downscaled values they exhibit an even more substantial overestimation with respect to the rain gauge benchmark this is expected as moving towards smaller averaging scales the fluctuations of the daily rainfall process tend to become more energetic as quantified by the increase in variance and decrease in the number of events captured by eqs 3 and 2 lastly the quantile values corrected by applying the qrf error model appear to be much closer to the ground estimates despite some variability remaining in the distribution of the estimates qpe statistics bias is greatly reduced and the pdf of relative errors exhibit in this case a clear peak around zero this encouraging result supports our choice of applying the same correction to each yearly parameter in the mev distribution without applying any specific correction to the inter annual variability of the parameters we note that the application of downscaling and bias correction techniques tend to have opposite effects on the estimated quantiles the downscaling tends to increase the magnitude of the 50 y e a r event in this case and the bias correction tends to decrese it on average even though this might seem counterintuitive and perhaps unnecessary we stress that this distinction between scale effects and errors is quite important as it is the difference between the downscaled and gauge values that should be minimized for example when designing new sensors and algorithms areal averages and point values should not be directly compared overall the combined application of downscaling and bias correction appears to provide good results and confirms that the bias correction performed on the parameters is relevant for correcting extreme value quantiles as a representative application of the error model we trained the qrf model with data from all the sggc sites and predicted the spatial distribution of the error over the conus fig 8 features the median relative errors in the weibull parameters panels 8 a and b and in the yearly number of events panel 8 c consistently with the observations from gauged sites these results clearly show spatial features such as the overestimation of the characteristic scale cd over the northeast us and the underestimation of the shape parameter wd in the western us predominantly controlled by orography after correcting the yearly mev parameters with the median qrf relativer error estimate one can compute the errors in mev estimated quantiles the spatial distribution of the error in the 50 year quantile is featured in fig 8 d which shows the quantile being generally overestimated over the conus this overestimation is particularly relevant in the western us where underestimation of the shape parameter over complex terrain produces extreme value distributions with tails significantly heavier than the corresponding ground estimates 4 2 the global distribution of rainfall extremes we are now in a position to extend the analysis of rainfall extremes over the conus to the entire tmpa domain which covers tropical subropical and mid latitude areas between 50 s and 50 n application of the mev distribution to the tmpa dataset is performed for each pixel independently as done for the conus and at the grid cell spatial scale the global distribution of extreme daily rainfall obtained from computing quantiles for a 50 year return period with the mev distribution is featured in fig 9 the most extreme rainfall values occur over subtropical regions with several hotspots over land which include south america and south east asia while the most intense quantiles are estimated to occur over ocean however the results obtained here by applying the error model should be regarded as representative of conditions over land only and validation over ocean should be pursued separately e g see sapiano and arkin 2009 prakash and gairola 2014 moreover the gauge correction of tmpa dataset performed at the monthly scale can hardly be performed over the ocean for lack of sufficient data this circumstance is expected to negatively impact the accuracy of qpe s over the ocean with respect to the accuracy over land when comparing quantiles estimated with gev and mev one can observe that i the main quantitative patterns of the two models estimates are quite coherent at the global scale and ii mev estimates exhibit a remarkable spatial coherence when compared to gev estimates while the estimation uncertainty of gev is heavily affected by the short dataset available here 20 years of annual maxima regionalization techniques are expected to decrease this uncertainty hosking and wallis 2005 bracken et al 2016 demirdjian et al 2018 schellander et al however we note that the reduction in estimation uncertainty achieved by using mev does not use information from nearby grid cells but uses the entire distribution of ordinary values from a single grid cell as an example we show a zoomed in representation of the 50 year event estimated for a domain in south east asia one of the regions characterized by most intense maximum rainfall accumulation according to the global scale analysis above 10 again mev estimates appear very coherent in space suggesting that the uncertainty intervals in quantile estimates are significantly reduced with respect to the gev model 5 discussion and conclusions in this paper we have described an approach for downscaling and validating satellite qpe statistics and for correcting and extending extreme value estimates over ungauged locations the methodology proposed here separately accounts for the scale difference between qpe gridded statistics and reference rain gauges at the ground and for errors in qpe estimates our cross validation results quantify estimation uncertainty and support the downscaling and correction procedures proposed suggesting that they can be applied to the entire conus and by extrapolation to many mid latitude areas at the global scale thus providing error estimates also where ground information is scarce the analysis over the conus provides useful insight over a range of different climatic regimes while optimally exploiting the ground observations available there for testing purposes we also note that the two main assumptions adopted here namely the adoption of the weibull distribution for daily rainfall accumulations across scales and of eq 7 for the spatial correlation can in principle be relaxed and tailored to local rainfall regimes as needed thus conferring to the proposed approach a greater flexibility while the weibull distribution has been advocated as a general model for the tail of the daily rainfall distribution at the global scale wilson and toumi 2005 the goodness of fit to daily values is expected to vary in different climate regions however the approach proposed here can be applied to other parametric models for daily rainfall provided that the first two moments exist the assumption of isotropic correlation function would be more difficult to relax as is the case for the taylor hypothesis used here to downscale the wet fraction of the rainfall process to the point while these are approximations a more refined description of the rainfall field would require the availability of information at the ground as our main objective here is to propose a methodology for application to data scarce regions we utilize these assumptions on the rainfall field to infer sub grid scale properties of the rainfall field from the qpe dataset alone one limitation encountered in section 2 5 concerns the downscaling of the spatial correlation function at locations where the correlation decays very rapidly with distance as is the case for sites characterized by particularly complex terrain while this situation occurs only for a small fraction of the sggc sites examined here these conditions may occur in many mountainous regions worldwide a possible way to address this limitation could be to develop spatially explicit models of the correlation based on local studies over complex terrain e g duan et al 2015 the results obtained in this study suggest that the gauge correction applied in order to obtain the tmpa research version dataset can lead to a significant deformation of the shape of the daily rainfall distribution this is especially relevant in areas where remote sensors fail to detect a significant fraction of events this consideration suggests the possibility of applying the qrf error model directly to the non adjusted real time datasets an interesting avenue of future research will be the application of this framework separately to the non adjusted precipitation statistics computed for different seasons this analysis would outline the performance of microwave retrievals in recovering rainfall statistics for different precipitation types and quantify their contributions to the discrepancies observed in the yearly rainfall distributions the aim of the downscaling approach proposed here is to infer rainfall statistics across spatial averaging scales at a fixed temporal integration scale daily in our application applications of the methodology to different temporal scales are possible and deserve further investigation extending the downscaling procedure to finer scales may encounter some limitations first the estimation of the spatial correlation becomes increasingly uncertain when moving to short time scales and the characteristic correlation length scale of the rainfall field decreases as discussed by gebremichael and krajewski 2004 at temporal scales where the correlation characteristic distance becomes smaller than the grid cell size the downscaling becomes unfeasible as discussed in section 2 5 second the taylor hypothesis approach introduced in zorzetto and marani 2019 and discussed here in section 2 3 requires information at temporal scales smaller that the temporal scale of interest in order to infer the wet fraction of the rainfall field at sub grid spatial scales therefore the shortest temporal scale at which the spatial downscaling analysis can be performed needs to be larger than the temporal resolution of the qpe dataset finally a third limitation concerns the use of eq 13 moving to progressively finer temporal scales the assumption of independent ordinary rainfall events becomes increasingly questionable as the temporal autocorrelation of the process becomes more significant therefore application of mev would require declustering of the observations e g as proposed by marra et al 2018 we also note that for the qrf error model to work adequately one should limit extrapolation of the qpe errors only to areas whose characteristics are represented in the training dataset however as we show in the cross validation experiment the model can be successfully trained over large areas using low density networks of rain gauges this was indeed the main objective of our study and it extends the domain where qpe validation is possible beyond the existing dense networks of rain gauges which tend to be available only in developed countries in densely populated areas and are lacking in arid locations and locations characterized by complex topography however where this information is not available the approach presented here provides a coherent quantification of estimation uncertainty accounting for the different intrinsic nature of spatially averaged satellite qpes and point observations at the ground the analyses proposed here can be used to inform the development of rain gauge networks covering a targeted set of sample climatic location to maximize the inference potential over ungauged sites an interesting research direction that builds on the results presented here is the possible extension of the methodology to a seasonal analysis of rainfall pdf s and correlation structure or possibly to a classification based on different precipitation types this type of differentiation could be useful either to assess more specifically the performance of remotely sensed qpes in different seasons and for different precipitation types and to study the distribution of extremes originated from multiple mechanisms credit authorship contribution statement enrico zorzetto data curation writing original draft writing review editing marco marani writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors acknowledge support from the national aeronautics and space administration fellowship nessf17 earth17f 0270 18 earth18r 0089 and 19 earth19r 27 and from venezia 2021 research grant provveditorato alle acque di venezia italy the qpe and rain gauge data sets used in this study are available online at https mirador gsfc nasa gov and https data nodc noaa gov respectively we wish to thank francesco marra and nadav peleg whose comments significantly improved the manuscript 
553,the classical transient storage ts model is widely used to describe a non fickian solute transport process induced by the solute mass exchange between the main channel and the immobile zone it has been shown that the single rate ts model tends to underestimate the slower exchange that occurs in a deeper or longer hyporheic flow path this long term retention can be better described by the fractional mobile immobile model fmim however in a real world application this method usually overestimates the late time concentrations in a breakthrough curve btc which can be better described by the tempered time fractional model ttfm in this study we introduced a fractional in time derivative ts model ftts which can describe broad waiting times in a particle motion process first a fully implicit numerical scheme was applied to solve the ftts and the method was validated by comparing its results with the analytical solutions of the classical advection dispersion model ade and the fmim then the ftts was applied to fit the synthetic data generated by the stammt l and field tracer experiment data further a variance based global sensitivity analysis was performed to assess the influence rank of the parameters to the heavy tail of the btcs the results indicated that the ftts could fit the btcs generated by the ade and fmim well in synthetic cases the ftts could reproduce different heavy tailing btcs accurately in addition the ftts could well describe tracer data in natural streams and performed better than the ttfm the sensitivity analysis indicated that both the fractional in time term and the ts term in the ftts had important impacts on the tail of the btc which could make the ftts more flexible for describing the tailing phenomenon in a btc keywords fractional in time transient storage hyporheic zone tempered time fractional model 1 introduction modeling pollutant dispersion and transport from an accidental spill in open water systems is of considerable significance for environmental quality and health safety deng et al 2006 the temporary storage of solute in a river is often controlled by flow induced uptake in the immobile phase or the dead zones which would lead to the heavy tailing phenomenon in a breakthrough curve btc the late time behavior of btcs is critically important for a cleanup strategy especially when the pollutants are toxic as a result a wide range of models involving solute exchange with the dead zones have been proposed for predicting pollutant dispersion process in a natural river bencala and walters 1983 deng et al 2006 manson et al 2001 marion et al 2008 schmid 2003 schumer et al 2003 shen et al 2008 shen and phanikumar 2009 haggerty 2002 haggerty and gorelick 1995 most of these studies are based on the single rate transient storage ts model 1 c t v c x d 2 c x 2 q l a c l c ε c s c 2 c s t ε a a s c c s where ε is a first order mass transfer coefficient that denotes exchange between the main channel and the storage zones other important parameters in this ts model include the sizes areas of the main channel a and the dead zones as the dispersion coefficient d the lateral inflow rate ql the lateral inflow concentration cl and the average velocity v the transient storage zone is a conceptual representation of the slower flow paths in the stream which includes the in channel areas of the slow flow the surface dead zones and the hyporheic zones generally the single rate ts model can capture the tailing phenomenon more accurately than the ade but the evidence based on stream tracer data shows that solute transport processes often do not follow the classical single rate ts model hunt 2000 the reason is that the single rate ts model assumes an exponential decline of solute concentration which cannot describe the btcs with heavier power law tails haggerty et al 2000 that could be attributed to the presence of deeper hyporheic flow paths wörman et al 2002 with the understanding improvement for the role of hyporheic zones on a solute dispersion process more general forms for the ts model have been proposed for example briggs et al 2009 2010 introduced a two storage zones model to describe the interaction between the main channel and different types of storage zones e g surface and hyporheic storage zones but the two storage zones in this model are all single rate kinetics e g exponential residence time distribution rtd to reproduce various tailing behaviors a single storage zone should assume different types of rtds e g the power law rtd benson and meerschaert 2009 the multi rate mass transfer model mrmt could describe mass transfer between the main channel and any number of immobile domains with varying properties using a series of first order equations however to make accurate prediction using this model detailed measurements estimates of mass transfer processes corresponding to individual transfer rates are needed by extending the single rate ts model to include the space fractional dispersion shen and phanikumar 2009 derived a space fractional transient storage model fsts which has the following form 3 c t v c x d 1 σ 2 α c x α 1 σ 2 α c x α q l a c l c ε c s c 4 c s t ε a a s c s c where α is the order of the fractional derivative and σ 1 1 controls the skewness of the btc when σ 0 the solution of the fsts is skewed backward while when σ 0 the solution is skewed forward kelly and meerschaert 2017 zhang et al 2005 shen and phanikumar 2009 also showed that the fsts could well describe tracer data from a natural stream the success of fsts relies on its ability to describe heavy tailing btcs accurately which is due in part to space fractional dispersion however the physical interpretation of σ in the space fractional ade e g fsts when applied to streamflow remains controversial deng et al 2004 kelly and meerschaert 2017 zhang et al 2005 zhang et al 2009 the long term upstream displacement of the negatively skewed derivative model as noted by zhang et al 2009 is not physically based the controversy in space fractional models can be avoided by using the fractional in time derivative model fractional in time derivatives are used to describe long waiting times between random jumps or long resident times in the immobile zone schumer et al 2003 zhang et al 2017 the power law rtd tends to overestimate the late time concentration in the case without significant hyporheic zones which can be better described by the truncated power law scheme i e the tempered time fractional model ttfm meerschaert et al 2008 however this theory assumes that when trapped in a dead zone the solute will eventually return to the mobile zone within a limited time period when part of the solutes never come out of a storage zone the truncated theory is not fit anymore in fact different types of storage zones within a complex system can exhibit different characteristics and thus they may lead to different types of rtds the basic questions posed by this study are as follows 1 can the power law rtd combined with the exponential rtd create more flexible and accurate simulations than the ttfm 2 if so how do the two kinds of rtds or their corresponding parameters govern the tail of the btc 3 are there any interactions among the parameters to address these questions first we extended the single rate ts model into a fractional in time transient storage model ftts then we used the ftts to fit synthetic and observed data and the results were compared to those of the ttfm in addition we performed a variance based global sensitivity analysis gsa to the ftts since the gsa is an effective tool for quantifying the relative contribution of each parameter or each term in a mathematical model especially for a complex system with a high dimension of correlated parameters dai et al 2017 xia et al 2018 the rest of this paper is arranged as follows in section 2 we introduce the ftts and the numerical solving scheme which was verified in section 3 then we apply this model to study three synthetic cases generated by stammt l haggerty 2002 a solute transport code set for the mrmt in addition we compare the simulation results with tracer experiment data from a total of 8 reaches in 3 different rivers meanwhile a global sensitivity analysis for the ftts is also conducted in this section the results are discussed in section 4 finally we summarize our results in the conclusion section 2 methodology 2 1 the mrmt and fmim models the fundamental equations used in mrmt are haggerty and gorelick 1995 5 c m t i 1 n β i c i m i t v c m x d 2 c m x 2 6 c i m i t ε i c m c i m i where cm is the solute concentration in the mobile zone c im i is the concentration of the i th immobile zone and ε i is the first order exchange rate between the i th immobile zone and the mobile zone when n 1 the mrmt reduces to the single rate mass transfer model which is identical to the single rate ts model if we let ε i β i εa as and ignore the lateral inflow in eq 1 in the general form mrmt eq a6 in the appendix when b ε δ ε ε i and i 1 we get an exponential memory function 7 g t ε e ε t and the general function reduces to the single rate mass transfer model where δ is the delta function when we set a power law memory function 8 g t t γ γ 1 γ where γ is the gamma function we get the fractional mobile immobile fmim equation c m t t γ γ 1 γ γ c m t γ is the caputo fractional derivative of order γ schumer et al 2003 9 c m t β s γ c m t γ v c m x d 2 c m x 2 β s c m 0 x t γ γ 1 γ 0 γ 1 where β s is equal to β tot in eq a6 when the memory function takes the form 10 g t t e λ τ γ τ γ 1 γ 1 γ d τ the fmim evolves to the ttfm meerschaert et al 2008 11 c m t β s e λ t γ t γ e λ t c m β λ γ c m ν c m x d 2 c m x 2 β s c m x 0 t e λ τ τ γ 1 γ 1 γ d τ the term γ c t γ is the fractional derivative v is the velocity lt 1 and d l 2 t 1 is the dispersion coefficient when γ 1 the fmim reduces to the classic ade with a retardation factor of 1 β an analytical solution of eq 5 with a pulse initial condition in an unbounded domain was obtained using a stable subordinator density schumer et al 2003 and has been implemented in fracfit which is a robust fractional model toolkit kelly et al 2017 when the ade is generalized using a fractional in time derivative the underlying stochastic particle motion process is based on a power law waiting time distribution zhang et al 2009 or rtd schumer et al 2003 which is often used to describe sub diffusion due to mass exchange between the rapid and slow transport zones that are time nonlocal sun et al 2014 zhang et al 2015 in the ttfm λ 0 is the truncation parameter that controls the transition of the btc tail from a power law to exponential when t 1 λ the mobile zone concentration decreases as the power law decreases just as it does in the fmim at late times t 1 λ the tail of the mobile phase of the btc decreases exponentially 2 2 the fractional in time transient storage model in a real world flow field mass transfer between the mobile and the immobile zones can take on any number of different rates haggerty and gorelick 1995 which can be represented as a convolution of a memory function with mobile concentration transport i e eq a6 the memory function is a probability density function pdf that must be monotonically decreasing benson and meerschaert 2009 thus such an acceptable memory function can be a combination of a power law and an exponential rtd g t a t γ γ 1 γ d ε e ε t where a and d are the weighting coefficients satisfying a d 1 substituting this memory function into eq a6 gives 12 c m t β s a t γ γ 1 γ d ε e ε t c m t v c m x d 2 c m x 2 β s c m x 0 a t γ γ 1 γ d ε e ε t by taking into consideration the memory functions in eqs 9 and a4 eq 12 can be rewritten as 13 c m t a β s γ c m t γ d β s c i m t v c m x d 2 c m x 2 a β s t γ γ 1 γ 14 c i m t ε c m c i m for better separation and understanding we define β aβ s and rewrite the exponential term in the form of the single rate ts model eq 1 i e redefine as a dβ s and cs cim then eqs 13 and 14 i e ftts have the forms 15 c m t β γ c m t γ v c m x d 2 c m x 2 β c m 0 x t γ γ 1 γ ε c s c m 16 c s t ε a a s c m c s the weights of the two kinds of rtds are represented by β and as a for various values of β and as eq 15 allows either the decreasing exponential or power law functions to dominate various portions of the btc tails when as 0 the ftts reduces to the fmim model when we set β 0 the ftts reduces to the single rate mass transfer model when we set both as and β to 0 the ftts reduces to the classical ade model the riemann liouville rl fractional derivative of a constant is not zero which can cause problems in the boundary conditions such as the continuous slug release of a tracer of a known constant concentration in a stream the ftts introduced in this study is solved using the caputo definition we use the fully implicit scheme to solve eqs 15 and 16 the discretization of γ c t γ is the same as that in murio 2008 in this study we used the first type boundary as the upstream boundary and the second type boundary as the downstream boundary the resulting tridiagonal matrix system of equations was solved using the thomas algorithm press et al 2002 when γ 1 the numerical method reduces to the well known convergent fully implicit scheme which is unconditionally stable richtmyer and morton 1967 when γ 0 1 the implicit finite difference approximation of the time fractional equation has also been proven to be unconditionally stable by murio 2008 see their remarks 2 4 they also declared that their proof of stability and hence the convergence can be extended to other types of boundary conditions and the more general time fractional diffusion equations in one and higher space dimensions the error convergence rates of our numerical method are analyzed in appendix b 2 3 synthetic experiment a total of three datasets representing three kinds of heavy tails were generated using stammt l in which the user specified first order mass transfer coefficients are applied to a general one dimensional advection dispersion transport equation eq a1 several continuous distributions have been implemented in stammt l including a lognormal distribution of the diffusion rate see haggerty and gorelick 1998 and the gamma beta and power law distributions of first order mass transfer rates in this study we choose the single rate mass transfer model i 1 in eq 5 the lognormal distribution diffusion rate model and the power law distribution of the first order mass transfer model to generate the synthetic experimental datasets the single rate mass transfer model which has only one parameter ε governs the non local nature for the power law distribution of the first order mass exchange rate model b ε has the form see eq 27 a in haggertty et al 2000 17 b ε k 2 ε max k 2 ε min k 2 ε k 3 when k 2 and 18 b ε 1 ln ε max ε min ε when k 2 where k is the power law coefficient εmax is the maximal exchange rate and εmin is the minimal exchange rate b ε is zero when ε εmin and ε εmax in the lognormal distribution diffusion rate model the diffusion rate in the immobile zone is characterized by a lognormal density function which was defined by wörman et al 2002 as 19 b ω 1 2 π σ ω exp ln ω μ 2 2 σ 2 where σ and μ are the parameters of the lognormal function the parameter μ is the natural logarithm of the geometric mean of the distribution and σ is the standard deviation of the natural logarithms of the diffusion rate coefficient ω in this case the immobile domain governing equation is 20 c r ω t d α 2 c r ω r 2 where da is the apparent diffusion coefficient accounting for tortuosity and cr ω is the actual concentration in the immobile zone varying with distance r from the center to the edge of the immobile zone the spatially averaged immobile zone concentration is defined as 21 c i m ω 1 ε d n 0 ε d c r ω d r additional details can be found in haggerty and gorelick 1998 2 4 model parameter estimation the log based root mean squared error rmse was calculated for each model simulation as 22 r m s e i 1 n log 10 c s i m x t i log 10 c o b s x t i 2 n where n is the number of samples in each btc cobs is the observed data and csim is the simulated data as a result the lower values in a btc are associated with greater weights than those in the absence of the log transforming which is important for assessing anomalous transport characteristics when heavy tails occur at lower concentrations smaller rmse values indicate better agreement between the simulated and observed datasets because the absolute values of log 10 csim and log 10 cobs become very large when cobs and csim get close to zero we eliminate data points when csim and the corresponding cobs are less than 10 6 in calculating the rmse parameter estimation for both the stammt l generated synthetic data and the tracer experiments were accomplished using the parallel shuffled complex evolution sce algorithm duan et al 1993 muttil et al 2007 models with more parameters may fit the observations more accurately however increasing the number of parameters in a model would increase the complexity and the difficulty of parameter estimation therefore the small sample corrected akaike information metric aicc was used for model comparison the aicc which takes both the goodness of fit and the number of parameters into account is an effective method for model comparison and evaluation akpa and unuabonah 2011 anderson and phanikumar 2011 saffron et al 2006 xia et al 2018 23 aicc aic 2 m m 1 n m 1 where the aic is the akaike information criterion given by 24 aic n ln s n 2 m where n is the number of data points m is the number of model parameters and s is the error sum of squares which is log transformed similar to the log based rmse to give the same weight to the tail of the btcs smaller aicc values which may be negative suggest that the model is more justified by the data 2 5 parameter sensitivity study the first order sensitive indices si and the total sensitivity indices sti were calculated using monte carlo based estimators the first order sensitivity coefficient is expressed as 25 s i v x i e x i y x i v y where v x i e x i y x i is a variance based first order effect for a generic factor xi the i th factor x i denotes the matrix of all of the factors except xi e x i y xi is the mean of y which takes over all possible values of x i while keeping xi fixed the outer variance v x i takes over all possible values of xi the total effect index is 26 s t i e x i v x i y x i v y 1 v x i e x i y x i v y sti measures the first and higher order interactions of the factor xi dai and ye 2015 saltelli et al 2010 we used the quasi random sampling method developed by saltelli et al 2010 to simultaneously calculate si and sti a and b are two independent sampling matrices of the model parameters with size n k where k is the number of factors and n is the number of values for each parameter we introduce a matrix ci where all the columns are from b except the i th column which is from a each line of the three matrices is a combination of a set of parameter values by computing the model output with the input of the sample matrix we obtained three n 1 output vectors ya f a yb f b and y c i f c i then si and stican be calculated according to the method of homma and saltelli 1996 27 s i 1 n j 1 n y a j y c i j 1 n 2 j 1 n y a j j 1 n y b j 1 n j 1 n y a j 2 f 0 2 28 s t i 1 v y x i v y 1 1 n j 1 n y b j y c i j f 0 2 1 n j 1 n y a j 2 f 0 2 where f 0 2 1 n j 1 n y a j 2 since the output of the ftts is the data points of a btc we selected 500 data points within a time interval of 200 s from the output to represent the different parts of the btc i e the data points were averagely sampled at a time length of 100 000 seconds so the corresponding ya yb and y c i are matrices with the size of n 500 we assume that each parameter follows a uniform distribution within the specified range of values table 1 2 6 site descriptions in the synthetic experiment the hypothetical observation points were set at 360 m downstream from the spill points and the discharge q was set to 0 31 m 3s 1 for all cases and three field cases are simulated the red cedar river rcr in michigan usa the grand river gr also in michigan usa and the maocun subterranean river msr in guilin china the computational domain lengths of the ftts model for all cases are summarized in table 2 the tracer study of the rcr a fourth order stream in south central michigan was reported in phanikumar et al 2007 the rcr originates as an outflow from cedar lake michigan and flows through east lansing the study reach is between hagadorn bridge to the east and kalamazoo street bridge to the west the rcr meanders through the michigan state university msu campus over a stretch of approximately 5 km hagadorn bridge was selected as the injection point and samples were collected at three downstream sites the farm lane kellogg and kalamazoo bridges whose distances from the injection point are 1 4 km 3 1 km and 5 08 km respectively the gr tracer study was conducted by shen et al 2008 this river is a 420 km long tributary of lake michigan it originates from the city of grand rapids and extends to coopersville the tracer study was conducted on a 40 km stretch of the mainstream the injection point was at ann street bridge which is near downtown grand rapids sampling was carried out at four downstream sites 4 558 km 13 678 km 28 357 km and 37 608 km from the injection site more details are given in shen et al 2008 the msr is a more complex water system which is located in a typical karst area in guilin china the sodium fluorescein tracer study was conducted in the northern part of the msr the dye was injected at daguping sinkhole maocun sinkhole 585 m from the injection point was selected as the sampling point additional details are reported in huang et al 2017 3 results 3 1 comparison with the ade and fmim from the discussion in section 2 2 we knew that the ftts can reduce to the ade and fmim models when the special parameter values are set to verify the numerical scheme for solving the ftts we compared the results of the ftts with the corresponding analytical solutions of the ade and fmim schumer et al 2003 the parameter values set for the ftts ade and fmim are presented in table 3 fig 1 shows the comparison of the ade and ftts as can be seen the btc generated from the ftts agrees well with that from the ade the early time portion of the btc from the ftts is slightly higher than that from the ade the rmse of the comparison is 0 0041 similar results of comparison between the fmim and ftts are shown in fig 2 and the rmse is 0 0092 the deviation in the early time part of the btc may come from the numerical error which can be further reduced by decreasing the time and space steps another possible explanation could be that the numerical simulation is conducted in a finite domain while the analytical solutions for the ade and fmim assume an infinite domain in view of the small values of the rmse the accuracy of this numerical scheme is acceptable 3 2 simulation of the synthetic data the parameter values used in the single rate power law and lognormal mrmt models to generate the synthetic data sets are summarized in table 4 and the corresponding btcs are shown in fig 3 the concentration peaks and peak times of all btcs are approximately equal as can be seen the power law mrmt model has the heaviest tail which characterizes the long term mass exchange between the mainstream and hyporheic zones fig 4 shows the simulated btcs and the synthetic data generated by the single rate mrmt the optimized parameters are summarized in table 5 according to the results the ftts can reproduce the synthetic btc accurately rmse 0 0223 the optimized β is close to zero table 5 in the three cases corresponding to the minimum tailing of the btc which can be effectively described by the ts term in the lognormal case fig 5 the btc has a heavier tail than in the exponential case which means the solution has a longer mean migration time when being trapped in the immobile zones the ftts can capture the peak well but it slightly overestimates the early time and late time concentrations rmse 0 1269 fig 6 shows the simulated btc and the synthetic data from the power law mrmt as can be seen the ftts can also capture the heaviest tailing btc accurately rmse 0 1814 in summary the ftts can well reproduce different kinds of tailing btcs and exhibits strong adaptability and flexibility although there exists several slightly different definitions of the parameter β benson and meerschaert 2009 haggerty and gorelick 1995 schumer et al 2003 zhang et al 2007 a larger β corresponds to a larger fraction of immobile material in both the fmim and the mrmt in contrast to the exponential case the optimized β is largest in the power law case while as is small 0 0379 and only approximately one tenth of that of the exponential case thus this kind of heavy tail is mainly described by the fractional in time term the value of the optimized β for the lognormal case is larger than that of the exponential case indicating that the tailing effect can potentially be exhibited by both the fractional term and the ts term 3 2 simulation for natural streams 3 2 1 red cedar river the optimal parameter values for the ttfm and ftts models estimated for the tracer experiment conducted in the rcr are presented in table 6 fig 7 shows the comparison between the simulations and the observed data both models agree well with the field observations however the ttfm overestimates the late time data in reaches 1 and 2 which are better simulated by the ftts for reach 3 they show comparable accuracies and fit the observed data well the optimized parameters for the ttfm and ftts models table 6 exhibit the expected similarities and some important differences for both the ttfm and the ftts the optimized γ for reach 3 is smaller than that for reach 1 in contrast the optimized β for reach 3 is larger than that for reach 1 in addition the corresponding values of γ in the ttfm are smaller than those in the ftts for all 3 reaches in reaches 1 and 3 the optimized β in the fmim are larger than those in the ftts meanwhile for the ftts the as for reach 1 is the largest and the smallest for reach 3 the power law tail of the btc is dominated by the parameter γ a smaller γ value means a lower peak value of the btc and a heavier tail xia et al 2013 considering the definition of β we can conclude that part of the trapping effect is exhibited by the ts term in the ftts 3 2 2 grand river gr is a large river and the tracer experiments conducted in all 4 reaches do not show a significant hyporheic zone storage effect accordingly the observed btcs do not show conspicuously heavy tails the optimized parameters are listed in table 7 fig 8 shows a comparison of the simulation results with the observed data as we can see both the ttfm and ftts fit the peak and early time concentration well however the ttfm overestimates the tail of the btcs in reaches 1 3 which are better simulated by the ftts for reach 4 the ttfm is in better agreement in the late time but cannot capture the early time concentration well the optimized γ of the ttfm is smaller than that of the ftts the estimated parameter d in the ttfm model is considerably larger than that in the ftts model indicating that part of the scale dependent dispersion phenomenon garrard et al 2017 is explained by the ts term 3 2 3 maocun subterranean river in the msr experiment the observed btc is characterized by a relatively rapid concentration increase until it reaches the peak fig 9 subsequently the concentration decrease is slow and exhibits a multi peaked tail the multi peaking phenomenon indicates the river system is characterized by complex storage zone features the optimal parameter values are listed in table 8 the comparison of the simulations with experimental results clearly shows that the ftts has a greater advantage in simulating the multi peaked tail however both models cannot capture the local peaks in the btc similar to the ftts the space fractional ts model i e fsts also cannot capture the multi peak btc anderson and phanikumar 2011 although the power law rtd can be used to describe the heavy tail however in our knowledge none of these models can effective reproduce a multi peaked tail 3 3 parameter sensitivity the btcs generated by the ftts for different values of ε are presented in fig 10 other parameters were fixed as v 2 d 5 γ 0 8 β 0 4 andas 10 as we can see as ε increases more solutes are concentrated in the middle the temporal interval of 103 104s of the btc and the late time after 104s concentration decreases rapidly the sensitivity analysis results of the ftts are shown in fig 11 fig 12 shows the btcs generated from some random parameter combinations that were used to calculate si and sti the si for ε fig 11 e and γ fig 11 c both have small values at the early time and large values at the late time after 2000 s indicating that they have more important impacts on the tail of the btc while the difference is that si of γ keeps increasing with time while si of ε decreases with time this indicates that γ impacts the entire tail especially at very late time however ε has a greater impact on the concentration at time interval of about 2000 20 000 s than that in the late time thus the ftts is more flexible in describing the tailing phenomenon than the fmim the si of β has relatively large values in the early time and exhibits a sharp decrease after 2000 s which means that β has a greater influence on the leading edge of the btc than the tail the si of v fig 11 a d fig 11 b and as fig 11 f are small and show similar characteristics the si have relatively large values at the beginning of the btc and small values at the end of the btc however the sti of as fig 11 f is significantly larger than the si indicating that as has strong interactions with other parameters in addition the stivalues of all parameters show lager values in the early time before 2000 s than those at the late time after 2000 s a possible explanation for this is that the leading edge of the btc is steeper than the tail fig 12 which makes the concentration in the early time more sensitive to the total effects of each parameter 4 discussion in the general mrmt model eq a 6 the characteristics of immobile zones are directly described by a memory function the transport of solute in the mobile phase and the immobile phase can be described as a two state markov chain valocchi and quinodoz 1989 as special cases of the mrmt the stochastic process of the fmim can be viewed as a power law residence time immobile state in the immobile zone among mobile processes mobile states while the single rate ts model describes an exponential rtd benson and meerschaert 2009 the power law rtd typically overestimates solute concentration in cases without a significant heavy tail the exponential rtd however tends to underestimate the heavy tail of a btc when power law distributions occur with large truncation times exponential rtd one can obtain waiting time distributions with a finite mean and yield a tempered time fractional model i e the ttfm which tends to perform better than the fmim model meerschaert et al 2008 in our results the ftts performs better than the ttfm which over predicts the late time plume concentration in the cases without a significant hyporheic effect this may indicate that in the ftts the power law rtd balanced by the exponential rtd could be a more promising scheme than the exponentially truncated power law rtd it is well known that the exchange of water and solute between the main channel and the immobile zones would produce a delay in solute transport relative to mass center and lead to a long tailed btc boano et al 2007 further to that different types of immobile zones could lead to different rtd types short term retention which is represented by an exponential rtd is related to surface retention in pools and other slow regions of the open channel while power law rtds generally correspond to subsurface flow paths ensign and doyle 2005 aubeneau et al 2014 gooseff et al 2005 cardenas 2008 in addition even substrate type e g size and structural composition can control the characteristics of anomalous transport and how long they persist aubeneau et al 2014 for example in their tracer experiments payn et al 2008 demonstrated that the rtd exhibits power law behavior in a coarse bedded stream and exhibits exponential decay in a fine bedded stream one major criticism of the single type rtd model e g the single rate ts model and the fmim is that the general storage term is sensitive to both long term trapping and short term retention which essentially results in an ambiguous description of the immobile zones in a river system ts models containing more than one ts term with different mass exchange rates can also reproduce the heavy tailing btcs well as pointed out by hollenbec et al 1999 that two rate coefficients were insufficient in many cases i e more than two ts terms were needed for example aderson and phanikumar 2011 used a model that contains five ts terms to successfully reproduce a heavy tailing btc just like the fsts however when using this kind of model one needs to consider in advance how many ts terms should be used the ftts offers a new method for describing the transient storage effect and has the potential to relate the fractional term to the hyporheic zone and the ts term to the surface storage or fast exchange zone what is more the ftts may be helpful for reducing the chances of undue bias from an arbitrary choice of a transport model e g choosing a power law rtd for an exponential decline or vice versa when trapping is explicitly modeled in the ftts using the ts term it is the exponential short term retention rather than long term power law trapping occurs in the relatively immobile zones e g deeper hyporheic zones this long term heavy tailed trapping could be modeled by the fractional in time derivative parameter sensitivity analysis also shows that both the fractional term and the ts term have significant impacts on the tail of the btc and the ftts can reproduce a more flexible tail fig 10 thus the ftts exhibits strong flexibility tracer data from natural rivers would always include contributions from various types of immobile zones phanikumar et al 2007 combined the rcr tracer data with wavelet decomposition of acoustic doppler current profiler data to separate surface storage from hyporheic retention their results showed that reach 1 of the rcr was dominated by surface storage i e a fast exchange zone hyporheic exchange mainly contributed to the transient storage in reach 3 and reach 2 was characterized by both surface storage and hyporheic exchange consistent with their results the value of β in the ftts was the largest in reach 3 and the smallest in reach 1 compared to the ttfm the relatively smaller β and larger γ values in the ftts indicate that part of the trapping effect is described by the ts term the simulation results for the msr show that the ftts is a more promising model the best explanation for this is that the ftts has both the fractional and the ts terms that describe the trapping effect as the subterranean river is a more complex system coupled with different types of storage zones the btc could be a result of mixed rtds i e a combination of both power law and exponential rtds however it should be pointed out that neither the ftts nor the ttfm can capture the multi peaking phenomenon the reason may be that with a single parameter either ε or β the overall effect is simply the sum of the contributions from every immobile zone that are triggered simultaneously therefore the resultant btcs are not sensitive to the exact location of an immobile zone zhang et al 2009 although it is desirable to relate the estimated β and as a of the ftts to the physical characteristics e g the surface storage zone and the hyporheic storage zone of the open river system it is challenging the related methods always require that in addition to the btcs from the main channel the morphologic characteristics cross sectional stream velocity distributions and adjacent representative surface storage zones be collected briggs et al 2009 phanikumar et al 2007 the smart tracer could provide useful information haggerty et al 2008 and a clue for future tracer experiment research but in our study the tracers used were all traditional conservative tracers so very little detailed environmental information can be obtained from the btcs therefore our results should be taken as being suggestive of the modeling work rather than being conclusive at this moment further studies on tracer transport such as using smart tracers and ts zone types in the river are absolutely required 5 conclusions in this study we developed a fractional in time ts model using the sce algorithm the ftts was applied to describe different types of tailings in comparison with the ttfm model the ftts simulates more accurately in the ftts solute exchange with the immobile zone is described by two separate terms the fractional derivative in time term and the ts term the power law rtd described by the fractional in time model tends to overestimate the resident time of a tracer in contrast the single rate ts model assumes an exponential rtd that typically underestimates the resident time for the ftts model we proposed in this study however the power law rtd is well balanced with the exponential one thus the ftts could describe the heavy tailing btcs well meanwhile the ftts could be helpful for reducing undue bias for arbitrarily choosing a model e g choosing a power law rtd model for an exponential decline or vice versa whether this model is truly physically meaningful or just mathematically powerful dependents on whether the fractional term and the ts term could exactly represent the deep hyporheic zones and the surface storage zones respectively we may explore this in future work in addition we identified the deficiency of this model i e it cannot simulate a multi peaked btc which could usually be observed in karst areas therefore improvements for this model are needed in the future credit authorship contribution statement liwei sun conceptualization methodology jie niu software supervision bill x hu supervision chuanhao wu formal analysis heng dai declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was partially supported by national natural science foundation of china 41530316 nsf project of guangdong china under contract 2018a030313165 and the fundamental research funds for the central universities of china 11618340 data could be provided or uploaded to repository as requested from the authors we also thank fen huang from chinese academy of geological sciences for sharing the tracer experiment data for maocun subterranean river appendix a general form mrmt for the case of a continuous distribution of the rate coefficients the mrmt eq 5 can be written as a1 c m t β t o t 0 b ε c i m ε t d ε v c m x d 2 c m x 2 where b ε is the probability density function pdf of the exchange rate coefficients ε 0 is a random variable as shown in benson and meerschaert 2009 the pdf must be monotonically decreasing in each immobile zone there is an independent mass exchange rate a2 c i m ε t ε c m c i m ε the solution of eq a2 is a3 c i m ε ε e ε t c m c i m ε x 0 e ε t where denotes convolution by taking the derivative of eq a3 with respect to time one can get another form a4 c i m ε t ε e ε t c m t ε e ε t c m x 0 c i m ε x 0 substituting eq a4 into eq a1 produces c m t β t o t 0 b ε ε e ε t c m t d ε β t o t 0 b ε ε e ε t c m x 0 c i m ε x 0 d ε v c m x d 2 c m x 2 in an accidental release of contaminant or a typical tracer test the solute is injected into the mobile zone and the immobile zone is initially clean i e cim x 0 0 if we let a5 g t 0 b ε ε e ε t d ε eq a1 becomes a6 c m t β t o t g t c m t v c m x d 2 c m x 2 β t o t c m x 0 g t eq a6 is the general form of the mrmt in this formulation the memory function g t models how long the particles stay in the immobile zone benson and meerschaert 2009 appendix b error convergence rate analysis of the numerical scheme the error convergence rates of the fully implicated numerical scheme in this study were assessed based on the method of randall 2007 let e h denote the error in the calculation with grid spacing h as computed using the true solution or the analytical solution in this study we used the linear rmse to denote e h i e b1 e h i 1 n c a x t i c n h x t i 2 n if the method is of the pth order then we can expect b2 e h c h p o h p c h p when h is small enough in eq b1 ca is the result of the true solution and c n h is the result of the numerical solution with grid spacing h in this study the exact solution of the ftts is not available but we run the numerical scheme on a very fine grid h if we let c n h be the the fine grid solution then the approximate error is b3 e h i 1 n c n h x t i c n h x t i 2 n if the method is of the pth order accurate and h p h p e h is a very accurate estimate of e h to check the convergence properties of the numerical method we calculated the results on grids with spacing h h 2 and h 4 to estimate the order and the error convergence rate then we have h h 4 and b4 e h e h e h c h p c h p 4 p 1 c h p similarly b5 e h 2 2 p 1 c h p thus the ratio of the approximate errors is b6 r h e h e h 2 4 p 1 2 p 1 2 p 1 as shown in randall 2007 one can obtain a good estimate of p using b7 p log 2 r h 1 then the orders were analyzed for a two dimensional grid space dimension dx and time dimension dt for dx we chose 0 1 0 2 and 0 4 as the step size and dt was fixed at 0 5 for dt we chose 0 1 0 2 and 0 4 as the step size and dx was fixed at 0 5 the other parameters of the ftts were fixed as v 1 5 d 2 0 γ 0 95 β 2 ε 1e 4 and as 5 the resultant p for dt and dx were 0 73 and 1 72 respectively i e the error has a slower convergence rate in the time dimension than in the space dimension this may be due to the fact that the discretization scheme for the factional in time term has a lower order fig b1 shows the error convergence rates for dx and dt on a log log plot 
553,the classical transient storage ts model is widely used to describe a non fickian solute transport process induced by the solute mass exchange between the main channel and the immobile zone it has been shown that the single rate ts model tends to underestimate the slower exchange that occurs in a deeper or longer hyporheic flow path this long term retention can be better described by the fractional mobile immobile model fmim however in a real world application this method usually overestimates the late time concentrations in a breakthrough curve btc which can be better described by the tempered time fractional model ttfm in this study we introduced a fractional in time derivative ts model ftts which can describe broad waiting times in a particle motion process first a fully implicit numerical scheme was applied to solve the ftts and the method was validated by comparing its results with the analytical solutions of the classical advection dispersion model ade and the fmim then the ftts was applied to fit the synthetic data generated by the stammt l and field tracer experiment data further a variance based global sensitivity analysis was performed to assess the influence rank of the parameters to the heavy tail of the btcs the results indicated that the ftts could fit the btcs generated by the ade and fmim well in synthetic cases the ftts could reproduce different heavy tailing btcs accurately in addition the ftts could well describe tracer data in natural streams and performed better than the ttfm the sensitivity analysis indicated that both the fractional in time term and the ts term in the ftts had important impacts on the tail of the btc which could make the ftts more flexible for describing the tailing phenomenon in a btc keywords fractional in time transient storage hyporheic zone tempered time fractional model 1 introduction modeling pollutant dispersion and transport from an accidental spill in open water systems is of considerable significance for environmental quality and health safety deng et al 2006 the temporary storage of solute in a river is often controlled by flow induced uptake in the immobile phase or the dead zones which would lead to the heavy tailing phenomenon in a breakthrough curve btc the late time behavior of btcs is critically important for a cleanup strategy especially when the pollutants are toxic as a result a wide range of models involving solute exchange with the dead zones have been proposed for predicting pollutant dispersion process in a natural river bencala and walters 1983 deng et al 2006 manson et al 2001 marion et al 2008 schmid 2003 schumer et al 2003 shen et al 2008 shen and phanikumar 2009 haggerty 2002 haggerty and gorelick 1995 most of these studies are based on the single rate transient storage ts model 1 c t v c x d 2 c x 2 q l a c l c ε c s c 2 c s t ε a a s c c s where ε is a first order mass transfer coefficient that denotes exchange between the main channel and the storage zones other important parameters in this ts model include the sizes areas of the main channel a and the dead zones as the dispersion coefficient d the lateral inflow rate ql the lateral inflow concentration cl and the average velocity v the transient storage zone is a conceptual representation of the slower flow paths in the stream which includes the in channel areas of the slow flow the surface dead zones and the hyporheic zones generally the single rate ts model can capture the tailing phenomenon more accurately than the ade but the evidence based on stream tracer data shows that solute transport processes often do not follow the classical single rate ts model hunt 2000 the reason is that the single rate ts model assumes an exponential decline of solute concentration which cannot describe the btcs with heavier power law tails haggerty et al 2000 that could be attributed to the presence of deeper hyporheic flow paths wörman et al 2002 with the understanding improvement for the role of hyporheic zones on a solute dispersion process more general forms for the ts model have been proposed for example briggs et al 2009 2010 introduced a two storage zones model to describe the interaction between the main channel and different types of storage zones e g surface and hyporheic storage zones but the two storage zones in this model are all single rate kinetics e g exponential residence time distribution rtd to reproduce various tailing behaviors a single storage zone should assume different types of rtds e g the power law rtd benson and meerschaert 2009 the multi rate mass transfer model mrmt could describe mass transfer between the main channel and any number of immobile domains with varying properties using a series of first order equations however to make accurate prediction using this model detailed measurements estimates of mass transfer processes corresponding to individual transfer rates are needed by extending the single rate ts model to include the space fractional dispersion shen and phanikumar 2009 derived a space fractional transient storage model fsts which has the following form 3 c t v c x d 1 σ 2 α c x α 1 σ 2 α c x α q l a c l c ε c s c 4 c s t ε a a s c s c where α is the order of the fractional derivative and σ 1 1 controls the skewness of the btc when σ 0 the solution of the fsts is skewed backward while when σ 0 the solution is skewed forward kelly and meerschaert 2017 zhang et al 2005 shen and phanikumar 2009 also showed that the fsts could well describe tracer data from a natural stream the success of fsts relies on its ability to describe heavy tailing btcs accurately which is due in part to space fractional dispersion however the physical interpretation of σ in the space fractional ade e g fsts when applied to streamflow remains controversial deng et al 2004 kelly and meerschaert 2017 zhang et al 2005 zhang et al 2009 the long term upstream displacement of the negatively skewed derivative model as noted by zhang et al 2009 is not physically based the controversy in space fractional models can be avoided by using the fractional in time derivative model fractional in time derivatives are used to describe long waiting times between random jumps or long resident times in the immobile zone schumer et al 2003 zhang et al 2017 the power law rtd tends to overestimate the late time concentration in the case without significant hyporheic zones which can be better described by the truncated power law scheme i e the tempered time fractional model ttfm meerschaert et al 2008 however this theory assumes that when trapped in a dead zone the solute will eventually return to the mobile zone within a limited time period when part of the solutes never come out of a storage zone the truncated theory is not fit anymore in fact different types of storage zones within a complex system can exhibit different characteristics and thus they may lead to different types of rtds the basic questions posed by this study are as follows 1 can the power law rtd combined with the exponential rtd create more flexible and accurate simulations than the ttfm 2 if so how do the two kinds of rtds or their corresponding parameters govern the tail of the btc 3 are there any interactions among the parameters to address these questions first we extended the single rate ts model into a fractional in time transient storage model ftts then we used the ftts to fit synthetic and observed data and the results were compared to those of the ttfm in addition we performed a variance based global sensitivity analysis gsa to the ftts since the gsa is an effective tool for quantifying the relative contribution of each parameter or each term in a mathematical model especially for a complex system with a high dimension of correlated parameters dai et al 2017 xia et al 2018 the rest of this paper is arranged as follows in section 2 we introduce the ftts and the numerical solving scheme which was verified in section 3 then we apply this model to study three synthetic cases generated by stammt l haggerty 2002 a solute transport code set for the mrmt in addition we compare the simulation results with tracer experiment data from a total of 8 reaches in 3 different rivers meanwhile a global sensitivity analysis for the ftts is also conducted in this section the results are discussed in section 4 finally we summarize our results in the conclusion section 2 methodology 2 1 the mrmt and fmim models the fundamental equations used in mrmt are haggerty and gorelick 1995 5 c m t i 1 n β i c i m i t v c m x d 2 c m x 2 6 c i m i t ε i c m c i m i where cm is the solute concentration in the mobile zone c im i is the concentration of the i th immobile zone and ε i is the first order exchange rate between the i th immobile zone and the mobile zone when n 1 the mrmt reduces to the single rate mass transfer model which is identical to the single rate ts model if we let ε i β i εa as and ignore the lateral inflow in eq 1 in the general form mrmt eq a6 in the appendix when b ε δ ε ε i and i 1 we get an exponential memory function 7 g t ε e ε t and the general function reduces to the single rate mass transfer model where δ is the delta function when we set a power law memory function 8 g t t γ γ 1 γ where γ is the gamma function we get the fractional mobile immobile fmim equation c m t t γ γ 1 γ γ c m t γ is the caputo fractional derivative of order γ schumer et al 2003 9 c m t β s γ c m t γ v c m x d 2 c m x 2 β s c m 0 x t γ γ 1 γ 0 γ 1 where β s is equal to β tot in eq a6 when the memory function takes the form 10 g t t e λ τ γ τ γ 1 γ 1 γ d τ the fmim evolves to the ttfm meerschaert et al 2008 11 c m t β s e λ t γ t γ e λ t c m β λ γ c m ν c m x d 2 c m x 2 β s c m x 0 t e λ τ τ γ 1 γ 1 γ d τ the term γ c t γ is the fractional derivative v is the velocity lt 1 and d l 2 t 1 is the dispersion coefficient when γ 1 the fmim reduces to the classic ade with a retardation factor of 1 β an analytical solution of eq 5 with a pulse initial condition in an unbounded domain was obtained using a stable subordinator density schumer et al 2003 and has been implemented in fracfit which is a robust fractional model toolkit kelly et al 2017 when the ade is generalized using a fractional in time derivative the underlying stochastic particle motion process is based on a power law waiting time distribution zhang et al 2009 or rtd schumer et al 2003 which is often used to describe sub diffusion due to mass exchange between the rapid and slow transport zones that are time nonlocal sun et al 2014 zhang et al 2015 in the ttfm λ 0 is the truncation parameter that controls the transition of the btc tail from a power law to exponential when t 1 λ the mobile zone concentration decreases as the power law decreases just as it does in the fmim at late times t 1 λ the tail of the mobile phase of the btc decreases exponentially 2 2 the fractional in time transient storage model in a real world flow field mass transfer between the mobile and the immobile zones can take on any number of different rates haggerty and gorelick 1995 which can be represented as a convolution of a memory function with mobile concentration transport i e eq a6 the memory function is a probability density function pdf that must be monotonically decreasing benson and meerschaert 2009 thus such an acceptable memory function can be a combination of a power law and an exponential rtd g t a t γ γ 1 γ d ε e ε t where a and d are the weighting coefficients satisfying a d 1 substituting this memory function into eq a6 gives 12 c m t β s a t γ γ 1 γ d ε e ε t c m t v c m x d 2 c m x 2 β s c m x 0 a t γ γ 1 γ d ε e ε t by taking into consideration the memory functions in eqs 9 and a4 eq 12 can be rewritten as 13 c m t a β s γ c m t γ d β s c i m t v c m x d 2 c m x 2 a β s t γ γ 1 γ 14 c i m t ε c m c i m for better separation and understanding we define β aβ s and rewrite the exponential term in the form of the single rate ts model eq 1 i e redefine as a dβ s and cs cim then eqs 13 and 14 i e ftts have the forms 15 c m t β γ c m t γ v c m x d 2 c m x 2 β c m 0 x t γ γ 1 γ ε c s c m 16 c s t ε a a s c m c s the weights of the two kinds of rtds are represented by β and as a for various values of β and as eq 15 allows either the decreasing exponential or power law functions to dominate various portions of the btc tails when as 0 the ftts reduces to the fmim model when we set β 0 the ftts reduces to the single rate mass transfer model when we set both as and β to 0 the ftts reduces to the classical ade model the riemann liouville rl fractional derivative of a constant is not zero which can cause problems in the boundary conditions such as the continuous slug release of a tracer of a known constant concentration in a stream the ftts introduced in this study is solved using the caputo definition we use the fully implicit scheme to solve eqs 15 and 16 the discretization of γ c t γ is the same as that in murio 2008 in this study we used the first type boundary as the upstream boundary and the second type boundary as the downstream boundary the resulting tridiagonal matrix system of equations was solved using the thomas algorithm press et al 2002 when γ 1 the numerical method reduces to the well known convergent fully implicit scheme which is unconditionally stable richtmyer and morton 1967 when γ 0 1 the implicit finite difference approximation of the time fractional equation has also been proven to be unconditionally stable by murio 2008 see their remarks 2 4 they also declared that their proof of stability and hence the convergence can be extended to other types of boundary conditions and the more general time fractional diffusion equations in one and higher space dimensions the error convergence rates of our numerical method are analyzed in appendix b 2 3 synthetic experiment a total of three datasets representing three kinds of heavy tails were generated using stammt l in which the user specified first order mass transfer coefficients are applied to a general one dimensional advection dispersion transport equation eq a1 several continuous distributions have been implemented in stammt l including a lognormal distribution of the diffusion rate see haggerty and gorelick 1998 and the gamma beta and power law distributions of first order mass transfer rates in this study we choose the single rate mass transfer model i 1 in eq 5 the lognormal distribution diffusion rate model and the power law distribution of the first order mass transfer model to generate the synthetic experimental datasets the single rate mass transfer model which has only one parameter ε governs the non local nature for the power law distribution of the first order mass exchange rate model b ε has the form see eq 27 a in haggertty et al 2000 17 b ε k 2 ε max k 2 ε min k 2 ε k 3 when k 2 and 18 b ε 1 ln ε max ε min ε when k 2 where k is the power law coefficient εmax is the maximal exchange rate and εmin is the minimal exchange rate b ε is zero when ε εmin and ε εmax in the lognormal distribution diffusion rate model the diffusion rate in the immobile zone is characterized by a lognormal density function which was defined by wörman et al 2002 as 19 b ω 1 2 π σ ω exp ln ω μ 2 2 σ 2 where σ and μ are the parameters of the lognormal function the parameter μ is the natural logarithm of the geometric mean of the distribution and σ is the standard deviation of the natural logarithms of the diffusion rate coefficient ω in this case the immobile domain governing equation is 20 c r ω t d α 2 c r ω r 2 where da is the apparent diffusion coefficient accounting for tortuosity and cr ω is the actual concentration in the immobile zone varying with distance r from the center to the edge of the immobile zone the spatially averaged immobile zone concentration is defined as 21 c i m ω 1 ε d n 0 ε d c r ω d r additional details can be found in haggerty and gorelick 1998 2 4 model parameter estimation the log based root mean squared error rmse was calculated for each model simulation as 22 r m s e i 1 n log 10 c s i m x t i log 10 c o b s x t i 2 n where n is the number of samples in each btc cobs is the observed data and csim is the simulated data as a result the lower values in a btc are associated with greater weights than those in the absence of the log transforming which is important for assessing anomalous transport characteristics when heavy tails occur at lower concentrations smaller rmse values indicate better agreement between the simulated and observed datasets because the absolute values of log 10 csim and log 10 cobs become very large when cobs and csim get close to zero we eliminate data points when csim and the corresponding cobs are less than 10 6 in calculating the rmse parameter estimation for both the stammt l generated synthetic data and the tracer experiments were accomplished using the parallel shuffled complex evolution sce algorithm duan et al 1993 muttil et al 2007 models with more parameters may fit the observations more accurately however increasing the number of parameters in a model would increase the complexity and the difficulty of parameter estimation therefore the small sample corrected akaike information metric aicc was used for model comparison the aicc which takes both the goodness of fit and the number of parameters into account is an effective method for model comparison and evaluation akpa and unuabonah 2011 anderson and phanikumar 2011 saffron et al 2006 xia et al 2018 23 aicc aic 2 m m 1 n m 1 where the aic is the akaike information criterion given by 24 aic n ln s n 2 m where n is the number of data points m is the number of model parameters and s is the error sum of squares which is log transformed similar to the log based rmse to give the same weight to the tail of the btcs smaller aicc values which may be negative suggest that the model is more justified by the data 2 5 parameter sensitivity study the first order sensitive indices si and the total sensitivity indices sti were calculated using monte carlo based estimators the first order sensitivity coefficient is expressed as 25 s i v x i e x i y x i v y where v x i e x i y x i is a variance based first order effect for a generic factor xi the i th factor x i denotes the matrix of all of the factors except xi e x i y xi is the mean of y which takes over all possible values of x i while keeping xi fixed the outer variance v x i takes over all possible values of xi the total effect index is 26 s t i e x i v x i y x i v y 1 v x i e x i y x i v y sti measures the first and higher order interactions of the factor xi dai and ye 2015 saltelli et al 2010 we used the quasi random sampling method developed by saltelli et al 2010 to simultaneously calculate si and sti a and b are two independent sampling matrices of the model parameters with size n k where k is the number of factors and n is the number of values for each parameter we introduce a matrix ci where all the columns are from b except the i th column which is from a each line of the three matrices is a combination of a set of parameter values by computing the model output with the input of the sample matrix we obtained three n 1 output vectors ya f a yb f b and y c i f c i then si and stican be calculated according to the method of homma and saltelli 1996 27 s i 1 n j 1 n y a j y c i j 1 n 2 j 1 n y a j j 1 n y b j 1 n j 1 n y a j 2 f 0 2 28 s t i 1 v y x i v y 1 1 n j 1 n y b j y c i j f 0 2 1 n j 1 n y a j 2 f 0 2 where f 0 2 1 n j 1 n y a j 2 since the output of the ftts is the data points of a btc we selected 500 data points within a time interval of 200 s from the output to represent the different parts of the btc i e the data points were averagely sampled at a time length of 100 000 seconds so the corresponding ya yb and y c i are matrices with the size of n 500 we assume that each parameter follows a uniform distribution within the specified range of values table 1 2 6 site descriptions in the synthetic experiment the hypothetical observation points were set at 360 m downstream from the spill points and the discharge q was set to 0 31 m 3s 1 for all cases and three field cases are simulated the red cedar river rcr in michigan usa the grand river gr also in michigan usa and the maocun subterranean river msr in guilin china the computational domain lengths of the ftts model for all cases are summarized in table 2 the tracer study of the rcr a fourth order stream in south central michigan was reported in phanikumar et al 2007 the rcr originates as an outflow from cedar lake michigan and flows through east lansing the study reach is between hagadorn bridge to the east and kalamazoo street bridge to the west the rcr meanders through the michigan state university msu campus over a stretch of approximately 5 km hagadorn bridge was selected as the injection point and samples were collected at three downstream sites the farm lane kellogg and kalamazoo bridges whose distances from the injection point are 1 4 km 3 1 km and 5 08 km respectively the gr tracer study was conducted by shen et al 2008 this river is a 420 km long tributary of lake michigan it originates from the city of grand rapids and extends to coopersville the tracer study was conducted on a 40 km stretch of the mainstream the injection point was at ann street bridge which is near downtown grand rapids sampling was carried out at four downstream sites 4 558 km 13 678 km 28 357 km and 37 608 km from the injection site more details are given in shen et al 2008 the msr is a more complex water system which is located in a typical karst area in guilin china the sodium fluorescein tracer study was conducted in the northern part of the msr the dye was injected at daguping sinkhole maocun sinkhole 585 m from the injection point was selected as the sampling point additional details are reported in huang et al 2017 3 results 3 1 comparison with the ade and fmim from the discussion in section 2 2 we knew that the ftts can reduce to the ade and fmim models when the special parameter values are set to verify the numerical scheme for solving the ftts we compared the results of the ftts with the corresponding analytical solutions of the ade and fmim schumer et al 2003 the parameter values set for the ftts ade and fmim are presented in table 3 fig 1 shows the comparison of the ade and ftts as can be seen the btc generated from the ftts agrees well with that from the ade the early time portion of the btc from the ftts is slightly higher than that from the ade the rmse of the comparison is 0 0041 similar results of comparison between the fmim and ftts are shown in fig 2 and the rmse is 0 0092 the deviation in the early time part of the btc may come from the numerical error which can be further reduced by decreasing the time and space steps another possible explanation could be that the numerical simulation is conducted in a finite domain while the analytical solutions for the ade and fmim assume an infinite domain in view of the small values of the rmse the accuracy of this numerical scheme is acceptable 3 2 simulation of the synthetic data the parameter values used in the single rate power law and lognormal mrmt models to generate the synthetic data sets are summarized in table 4 and the corresponding btcs are shown in fig 3 the concentration peaks and peak times of all btcs are approximately equal as can be seen the power law mrmt model has the heaviest tail which characterizes the long term mass exchange between the mainstream and hyporheic zones fig 4 shows the simulated btcs and the synthetic data generated by the single rate mrmt the optimized parameters are summarized in table 5 according to the results the ftts can reproduce the synthetic btc accurately rmse 0 0223 the optimized β is close to zero table 5 in the three cases corresponding to the minimum tailing of the btc which can be effectively described by the ts term in the lognormal case fig 5 the btc has a heavier tail than in the exponential case which means the solution has a longer mean migration time when being trapped in the immobile zones the ftts can capture the peak well but it slightly overestimates the early time and late time concentrations rmse 0 1269 fig 6 shows the simulated btc and the synthetic data from the power law mrmt as can be seen the ftts can also capture the heaviest tailing btc accurately rmse 0 1814 in summary the ftts can well reproduce different kinds of tailing btcs and exhibits strong adaptability and flexibility although there exists several slightly different definitions of the parameter β benson and meerschaert 2009 haggerty and gorelick 1995 schumer et al 2003 zhang et al 2007 a larger β corresponds to a larger fraction of immobile material in both the fmim and the mrmt in contrast to the exponential case the optimized β is largest in the power law case while as is small 0 0379 and only approximately one tenth of that of the exponential case thus this kind of heavy tail is mainly described by the fractional in time term the value of the optimized β for the lognormal case is larger than that of the exponential case indicating that the tailing effect can potentially be exhibited by both the fractional term and the ts term 3 2 simulation for natural streams 3 2 1 red cedar river the optimal parameter values for the ttfm and ftts models estimated for the tracer experiment conducted in the rcr are presented in table 6 fig 7 shows the comparison between the simulations and the observed data both models agree well with the field observations however the ttfm overestimates the late time data in reaches 1 and 2 which are better simulated by the ftts for reach 3 they show comparable accuracies and fit the observed data well the optimized parameters for the ttfm and ftts models table 6 exhibit the expected similarities and some important differences for both the ttfm and the ftts the optimized γ for reach 3 is smaller than that for reach 1 in contrast the optimized β for reach 3 is larger than that for reach 1 in addition the corresponding values of γ in the ttfm are smaller than those in the ftts for all 3 reaches in reaches 1 and 3 the optimized β in the fmim are larger than those in the ftts meanwhile for the ftts the as for reach 1 is the largest and the smallest for reach 3 the power law tail of the btc is dominated by the parameter γ a smaller γ value means a lower peak value of the btc and a heavier tail xia et al 2013 considering the definition of β we can conclude that part of the trapping effect is exhibited by the ts term in the ftts 3 2 2 grand river gr is a large river and the tracer experiments conducted in all 4 reaches do not show a significant hyporheic zone storage effect accordingly the observed btcs do not show conspicuously heavy tails the optimized parameters are listed in table 7 fig 8 shows a comparison of the simulation results with the observed data as we can see both the ttfm and ftts fit the peak and early time concentration well however the ttfm overestimates the tail of the btcs in reaches 1 3 which are better simulated by the ftts for reach 4 the ttfm is in better agreement in the late time but cannot capture the early time concentration well the optimized γ of the ttfm is smaller than that of the ftts the estimated parameter d in the ttfm model is considerably larger than that in the ftts model indicating that part of the scale dependent dispersion phenomenon garrard et al 2017 is explained by the ts term 3 2 3 maocun subterranean river in the msr experiment the observed btc is characterized by a relatively rapid concentration increase until it reaches the peak fig 9 subsequently the concentration decrease is slow and exhibits a multi peaked tail the multi peaking phenomenon indicates the river system is characterized by complex storage zone features the optimal parameter values are listed in table 8 the comparison of the simulations with experimental results clearly shows that the ftts has a greater advantage in simulating the multi peaked tail however both models cannot capture the local peaks in the btc similar to the ftts the space fractional ts model i e fsts also cannot capture the multi peak btc anderson and phanikumar 2011 although the power law rtd can be used to describe the heavy tail however in our knowledge none of these models can effective reproduce a multi peaked tail 3 3 parameter sensitivity the btcs generated by the ftts for different values of ε are presented in fig 10 other parameters were fixed as v 2 d 5 γ 0 8 β 0 4 andas 10 as we can see as ε increases more solutes are concentrated in the middle the temporal interval of 103 104s of the btc and the late time after 104s concentration decreases rapidly the sensitivity analysis results of the ftts are shown in fig 11 fig 12 shows the btcs generated from some random parameter combinations that were used to calculate si and sti the si for ε fig 11 e and γ fig 11 c both have small values at the early time and large values at the late time after 2000 s indicating that they have more important impacts on the tail of the btc while the difference is that si of γ keeps increasing with time while si of ε decreases with time this indicates that γ impacts the entire tail especially at very late time however ε has a greater impact on the concentration at time interval of about 2000 20 000 s than that in the late time thus the ftts is more flexible in describing the tailing phenomenon than the fmim the si of β has relatively large values in the early time and exhibits a sharp decrease after 2000 s which means that β has a greater influence on the leading edge of the btc than the tail the si of v fig 11 a d fig 11 b and as fig 11 f are small and show similar characteristics the si have relatively large values at the beginning of the btc and small values at the end of the btc however the sti of as fig 11 f is significantly larger than the si indicating that as has strong interactions with other parameters in addition the stivalues of all parameters show lager values in the early time before 2000 s than those at the late time after 2000 s a possible explanation for this is that the leading edge of the btc is steeper than the tail fig 12 which makes the concentration in the early time more sensitive to the total effects of each parameter 4 discussion in the general mrmt model eq a 6 the characteristics of immobile zones are directly described by a memory function the transport of solute in the mobile phase and the immobile phase can be described as a two state markov chain valocchi and quinodoz 1989 as special cases of the mrmt the stochastic process of the fmim can be viewed as a power law residence time immobile state in the immobile zone among mobile processes mobile states while the single rate ts model describes an exponential rtd benson and meerschaert 2009 the power law rtd typically overestimates solute concentration in cases without a significant heavy tail the exponential rtd however tends to underestimate the heavy tail of a btc when power law distributions occur with large truncation times exponential rtd one can obtain waiting time distributions with a finite mean and yield a tempered time fractional model i e the ttfm which tends to perform better than the fmim model meerschaert et al 2008 in our results the ftts performs better than the ttfm which over predicts the late time plume concentration in the cases without a significant hyporheic effect this may indicate that in the ftts the power law rtd balanced by the exponential rtd could be a more promising scheme than the exponentially truncated power law rtd it is well known that the exchange of water and solute between the main channel and the immobile zones would produce a delay in solute transport relative to mass center and lead to a long tailed btc boano et al 2007 further to that different types of immobile zones could lead to different rtd types short term retention which is represented by an exponential rtd is related to surface retention in pools and other slow regions of the open channel while power law rtds generally correspond to subsurface flow paths ensign and doyle 2005 aubeneau et al 2014 gooseff et al 2005 cardenas 2008 in addition even substrate type e g size and structural composition can control the characteristics of anomalous transport and how long they persist aubeneau et al 2014 for example in their tracer experiments payn et al 2008 demonstrated that the rtd exhibits power law behavior in a coarse bedded stream and exhibits exponential decay in a fine bedded stream one major criticism of the single type rtd model e g the single rate ts model and the fmim is that the general storage term is sensitive to both long term trapping and short term retention which essentially results in an ambiguous description of the immobile zones in a river system ts models containing more than one ts term with different mass exchange rates can also reproduce the heavy tailing btcs well as pointed out by hollenbec et al 1999 that two rate coefficients were insufficient in many cases i e more than two ts terms were needed for example aderson and phanikumar 2011 used a model that contains five ts terms to successfully reproduce a heavy tailing btc just like the fsts however when using this kind of model one needs to consider in advance how many ts terms should be used the ftts offers a new method for describing the transient storage effect and has the potential to relate the fractional term to the hyporheic zone and the ts term to the surface storage or fast exchange zone what is more the ftts may be helpful for reducing the chances of undue bias from an arbitrary choice of a transport model e g choosing a power law rtd for an exponential decline or vice versa when trapping is explicitly modeled in the ftts using the ts term it is the exponential short term retention rather than long term power law trapping occurs in the relatively immobile zones e g deeper hyporheic zones this long term heavy tailed trapping could be modeled by the fractional in time derivative parameter sensitivity analysis also shows that both the fractional term and the ts term have significant impacts on the tail of the btc and the ftts can reproduce a more flexible tail fig 10 thus the ftts exhibits strong flexibility tracer data from natural rivers would always include contributions from various types of immobile zones phanikumar et al 2007 combined the rcr tracer data with wavelet decomposition of acoustic doppler current profiler data to separate surface storage from hyporheic retention their results showed that reach 1 of the rcr was dominated by surface storage i e a fast exchange zone hyporheic exchange mainly contributed to the transient storage in reach 3 and reach 2 was characterized by both surface storage and hyporheic exchange consistent with their results the value of β in the ftts was the largest in reach 3 and the smallest in reach 1 compared to the ttfm the relatively smaller β and larger γ values in the ftts indicate that part of the trapping effect is described by the ts term the simulation results for the msr show that the ftts is a more promising model the best explanation for this is that the ftts has both the fractional and the ts terms that describe the trapping effect as the subterranean river is a more complex system coupled with different types of storage zones the btc could be a result of mixed rtds i e a combination of both power law and exponential rtds however it should be pointed out that neither the ftts nor the ttfm can capture the multi peaking phenomenon the reason may be that with a single parameter either ε or β the overall effect is simply the sum of the contributions from every immobile zone that are triggered simultaneously therefore the resultant btcs are not sensitive to the exact location of an immobile zone zhang et al 2009 although it is desirable to relate the estimated β and as a of the ftts to the physical characteristics e g the surface storage zone and the hyporheic storage zone of the open river system it is challenging the related methods always require that in addition to the btcs from the main channel the morphologic characteristics cross sectional stream velocity distributions and adjacent representative surface storage zones be collected briggs et al 2009 phanikumar et al 2007 the smart tracer could provide useful information haggerty et al 2008 and a clue for future tracer experiment research but in our study the tracers used were all traditional conservative tracers so very little detailed environmental information can be obtained from the btcs therefore our results should be taken as being suggestive of the modeling work rather than being conclusive at this moment further studies on tracer transport such as using smart tracers and ts zone types in the river are absolutely required 5 conclusions in this study we developed a fractional in time ts model using the sce algorithm the ftts was applied to describe different types of tailings in comparison with the ttfm model the ftts simulates more accurately in the ftts solute exchange with the immobile zone is described by two separate terms the fractional derivative in time term and the ts term the power law rtd described by the fractional in time model tends to overestimate the resident time of a tracer in contrast the single rate ts model assumes an exponential rtd that typically underestimates the resident time for the ftts model we proposed in this study however the power law rtd is well balanced with the exponential one thus the ftts could describe the heavy tailing btcs well meanwhile the ftts could be helpful for reducing undue bias for arbitrarily choosing a model e g choosing a power law rtd model for an exponential decline or vice versa whether this model is truly physically meaningful or just mathematically powerful dependents on whether the fractional term and the ts term could exactly represent the deep hyporheic zones and the surface storage zones respectively we may explore this in future work in addition we identified the deficiency of this model i e it cannot simulate a multi peaked btc which could usually be observed in karst areas therefore improvements for this model are needed in the future credit authorship contribution statement liwei sun conceptualization methodology jie niu software supervision bill x hu supervision chuanhao wu formal analysis heng dai declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was partially supported by national natural science foundation of china 41530316 nsf project of guangdong china under contract 2018a030313165 and the fundamental research funds for the central universities of china 11618340 data could be provided or uploaded to repository as requested from the authors we also thank fen huang from chinese academy of geological sciences for sharing the tracer experiment data for maocun subterranean river appendix a general form mrmt for the case of a continuous distribution of the rate coefficients the mrmt eq 5 can be written as a1 c m t β t o t 0 b ε c i m ε t d ε v c m x d 2 c m x 2 where b ε is the probability density function pdf of the exchange rate coefficients ε 0 is a random variable as shown in benson and meerschaert 2009 the pdf must be monotonically decreasing in each immobile zone there is an independent mass exchange rate a2 c i m ε t ε c m c i m ε the solution of eq a2 is a3 c i m ε ε e ε t c m c i m ε x 0 e ε t where denotes convolution by taking the derivative of eq a3 with respect to time one can get another form a4 c i m ε t ε e ε t c m t ε e ε t c m x 0 c i m ε x 0 substituting eq a4 into eq a1 produces c m t β t o t 0 b ε ε e ε t c m t d ε β t o t 0 b ε ε e ε t c m x 0 c i m ε x 0 d ε v c m x d 2 c m x 2 in an accidental release of contaminant or a typical tracer test the solute is injected into the mobile zone and the immobile zone is initially clean i e cim x 0 0 if we let a5 g t 0 b ε ε e ε t d ε eq a1 becomes a6 c m t β t o t g t c m t v c m x d 2 c m x 2 β t o t c m x 0 g t eq a6 is the general form of the mrmt in this formulation the memory function g t models how long the particles stay in the immobile zone benson and meerschaert 2009 appendix b error convergence rate analysis of the numerical scheme the error convergence rates of the fully implicated numerical scheme in this study were assessed based on the method of randall 2007 let e h denote the error in the calculation with grid spacing h as computed using the true solution or the analytical solution in this study we used the linear rmse to denote e h i e b1 e h i 1 n c a x t i c n h x t i 2 n if the method is of the pth order then we can expect b2 e h c h p o h p c h p when h is small enough in eq b1 ca is the result of the true solution and c n h is the result of the numerical solution with grid spacing h in this study the exact solution of the ftts is not available but we run the numerical scheme on a very fine grid h if we let c n h be the the fine grid solution then the approximate error is b3 e h i 1 n c n h x t i c n h x t i 2 n if the method is of the pth order accurate and h p h p e h is a very accurate estimate of e h to check the convergence properties of the numerical method we calculated the results on grids with spacing h h 2 and h 4 to estimate the order and the error convergence rate then we have h h 4 and b4 e h e h e h c h p c h p 4 p 1 c h p similarly b5 e h 2 2 p 1 c h p thus the ratio of the approximate errors is b6 r h e h e h 2 4 p 1 2 p 1 2 p 1 as shown in randall 2007 one can obtain a good estimate of p using b7 p log 2 r h 1 then the orders were analyzed for a two dimensional grid space dimension dx and time dimension dt for dx we chose 0 1 0 2 and 0 4 as the step size and dt was fixed at 0 5 for dt we chose 0 1 0 2 and 0 4 as the step size and dx was fixed at 0 5 the other parameters of the ftts were fixed as v 1 5 d 2 0 γ 0 95 β 2 ε 1e 4 and as 5 the resultant p for dt and dx were 0 73 and 1 72 respectively i e the error has a slower convergence rate in the time dimension than in the space dimension this may be due to the fact that the discretization scheme for the factional in time term has a lower order fig b1 shows the error convergence rates for dx and dt on a log log plot 
554,assimilating remote sensing observations into land surface models has become common practice to improve the accuracy of terrestrial water storage tws estimates such as soil moisture and groundwater for understanding the land surface interaction with the climate system as well as assessing regional and global water resources such remote sensing observations include soil moisture information from the l band soil moisture and ocean salinity smos and soil moisture active passive smap missions and tws information from the gravity recovery and climate experiment grace this study evaluates the benefit of assimilating them into the community atmosphere and biosphere land exchange cable land surface model the evaluation is conducted in the goulburn river catchment south east australia where various in situ soil moisture and groundwater level data are available for validating data assimilation da approaches it is found that the performance of da mainly depends on the type of observations that are assimilated the smos smap only assimilation sm da improves the top soil moisture but degrades the groundwater storage estimates whereas the grace only assimilation grace da improves only the groundwater component assimilating both observations multivariate da results in increased accuracy of both soil moisture and groundwater storage estimates these findings demonstrate the added value of multivariate da for simultaneously improving different model states thus leading to a more robust da system keywords smos smap grace enks cable multivariate data assimilation soil moisture groundwater 1 introduction accurate knowledge on terrestrial water storage tws is crucial for the assessment of climate variation and water resource availability entekhabi et al 1996 pitman 2003 rodell et al 2007 the accuracy of tws components e g soil moisture groundwater snow surface water simulated by land surface models lsm at high spatial resolution is commonly degraded by uncertainties in meteorological forcing model parameter calibration and land surface process representation moradkhani et al 2005 wood et al 2011 hydrologic information can also be obtained from satellite remote sensing observations e g kerr et al 2012 maurer et al 2003 tapley et al 2004 however tws components such as subsurface soil moisture and groundwater are usually not observed directly by in situ observations and the limited satellite coverage and sensing depths often restrict the reliability of the observations reichle et al 2008 data assimilation da can be used to combine various types of observations at different temporal and spatial resolutions with the model simulations according to the relative size of their errors reichle 2008 reichle et al 2008 da has been successfully applied in enhancing model estimated hydrologic components such as tws e g li et al 2012 soil moisture e g lievens et al 2015 groundwater e g tangdamrongsub et al 2018b snow e g andreadis and lettenmaier 2006 and runoff e g weerts and el serafy 2006 various satellite observations can be considered in the da system to improve the key components of the tws estimate for example surface soil moisture has an important role in the variability of the hydrological cycle and climate system entekhabi et al 1996 koster et al 2009 schumann et al 2009 and can be measured by l band radiometers i e from the soil moisture and ocean salinity smos kerr et al 2012 and soil moisture active passive entekhabi et al 2010 satellite missions chan et al 2016 both satellite missions provide global soil moisture products at a spatial resolution of 25 36 km representing the wetness in the top 0 5 cm soil layer approximately every 3 days the smos and smap radiometer data have been exploited in soil moisture data assimilation sm da systems over several river basins e g ahlergaarde western denmark ridler et al 2014 murray darling lievens et al 2015 continental australia e g tian et al 2017 the great lakes xu et al 2015 and north america e g blankenship et al 2016 these studies have demonstrated the benefits of sm da on both surface and root zone soil moisture components e g de lannoy and reichle 2016 tian et al 2017 xu et al 2015 however sm da has been found to have a negative impact on the groundwater storage estimate tian et al 2017 in addition to the surface soil moisture tws variations tws can be derived from gravity measurements by the gravity recovery and climate experiment grace satellite mission tapley et al 2004 the grace twin satellites measure changes of the earth s gravity field every month using a combination of several measurements including k band ranging accelerometer attitude and orbital data bettadpur 2012 because hydrological mass variations are dominant at a monthly time scale the grace data are commonly presented in terms of tws and have been used in a wide range of hydrological applications including data assimilation e g zaitchik et al 2008 eicker et al 2014 for drought detection e g houborg et al 2012 li et al 2012 kumar et al 2016 flood analysis reager et al 2015 groundwater loss analysis girotto et al 2017 tangdamrongsub et al 2018b and snow estimation forman et al 2012 su et al 2010 the benefit of grace da was observed particularly in deep storage components such as groundwater storage e g tangdamrongsub et al 2015 zaitchik et al 2008 however grace da is generally less effective in surface soil moisture improvement li et al 2012 tangdamrongsub et al 2017a tian et al 2017 the goal of multivariate da is to combine the strengths of sm da and grace da to simultaneously improve soil moisture and groundwater estimates tian et al 2017 elaborated this concept and showed that the accuracy of surface and deep storage components could be improved by the application of grace and smos data assimilation similarly kumar et al 2018 and jasinski et al 2019 applied multivariate da using multiple satellite soil moisture and snow products to improve the skills of model state estimates and climate assessment indicators kumar et al 2018 showed that the performance of da is improved with new satellite sensors based on these findings multivariate assimilation of grace and l band satellite soil moisture sensors e g smos smap is expected to lead to increased accuracy of soil moisture and groundwater estimates this study develops a multivariate da with grace smos and smap data to improve the accuracy of regional soil moisture and groundwater storage estimates the main research objective is to investigate the performance of multivariate da in simultaneously improving soil moisture and groundwater storage estimates different da schemes are developed to incorporate different observations into the da system simultaneously three different da scenarios subject to three different observation cases sm only grace only and both are evaluated in terms of estimating water storage e g surface and root zone soil moisture and groundwater the da approach is developed based on ensemble kalman smoother enks see e g dunne et al 2007 dong et al 2015 tian et al 2017 tangdamrongsub et al 2018b the lsm used in this study is the community atmosphere and biosphere land exchange cable decker 2015 the analysis is conducted over the goulburn river catchment rüdiger et al 2007 located in the eastern part of new south wales australia where extensive records of in situ soil moisture and groundwater are available from more than 20 sites throughout the catchment the da results are assessed by comparing them against the in situ data and the ensemble open loop estimate enol model run without da the evaluation is performed between january 2010 and december 2015 when grace smos smap from march 2015 and in situ data are available 2 materials 2 1 study area the goulburn river catchment is located in the south eastern part of the murray darling basin and has a sub humid or temperate climate fig 1 the catchment has a total area of 6540 km2 and consists of more than ten sub catchments including the krui and merriwa catchments where in situ soil moisture data are regularly recorded the catchment is maintained by the scaling and assimilation of soil moisture and streamflow sasmas project rüdiger et al 2007 http www eng newcastle edu au sasmas sasmas sasdata html the land cover of the catchment consists of a floodplain clear grassland crop in the northern part and a mountain range with dense vegetation in the south the northern part of the catchment is particularly suitable for satellite soil moisture remote sensing studies due to its low to moderate vegetation cover furthermore the clay content of the top soil layer 0 5 cm in the northern part is several times greater than in the south senanayake et al 2019 http www clw csiro au aclep soilandlandscapegrid higher variability in the top soil moisture can therefore be anticipated in the northern area the mean annual rainfall of the catchment is 700 mm and reaches 1100 mm in the higher altitude areas monthly mean minimum maximum temperatures reach approximately 16 30 c in summer and 2 14 c in winter no snowfall is presented in the catchment lsm simulations are expected to perform well over the catchment due to the absence of groundwater abstraction and streamflow control 2 2 land surface model setup the community atmosphere and biosphere land exchange cable land surface model is used to simulate daily volumetric soil moisture and groundwater storage at approximately 25 km resolution see fig 1 the model can be obtained from https trac nci org au trac cable and the model user guide and descriptions can be found in decker 2015 kowalczyk et al 2006 and ukkola et al 2016 cable estimates soil moisture in six separate layers in this study the soil thicknesses from the top to bottom compartments are set to 1 2 3 8 25 39 9 107 9 287 2 cm respectively in comparison with the in situ data see section 2 5 the combination of the first two model soil layers represents the 0 5 cm soil moisture component while the combination of the first three denotes the 0 30 cm component the forcing data used in cable are precipitation air temperature snowfall rate wind speed humidity surface pressure and shortwave and longwave downward radiation similar to tangdamrongsub et al 2018a the model is forced with meteorological input from the global land data assimilation system gldas rodell et al 2004 following the sensitivity study of tangdamrongsub et al 2018a gldas precipitation is replaced by data from the tropical rainfall measuring mission trmm huffman et al 2007 to improve the performance of the cable model two primary error sources of the lsm are the meteorological forcing data and the model parameters in the da process see section 3 the precipitation is perturbed based on the uncertainty provided by the trmm product huffman 1997 the shortwave radiation is perturbed using multiplicative white noise with 10 of the nominal values an additive white noise is used for the air temperature it is acknowledged that while a homoscedastic error would be more realistic for air temperature an offline sensitivity analysis found that the temperature error had only a marginal influence on the state estimates compared to e g precipitation the errors of forcing data are assumed to be spatially correlated as such an exponential correlation function is applied to the covariance matrix for each forcing variable the correlation lengths for forcing data were determined using variogram analysis and found to be approximately 25 km model parameters associated with soil moisture and groundwater components are also perturbed with a magnitude of 10 the selected model parameters are the fractions of clay sand silt and the drainage parameters that control the soil storage capacity and amount of subsurface runoff respectively both have a direct impact on the soil moisture and groundwater storages see table 2 in tangdamrongsub et al 2018a for more details the perturbation sizes of forcing data and parameters are determined based on the ensemble verification measures de lannoy et al 2006 mainly to allow an adequate spread of the ensemble between updates in the da process table 1 summarizes the forcing and parameter perturbation of this study note that the model state is not perturbed directly but rather perturbed as a result of model propagation associated with the perturbed forcing and perturbed model parameters as a result the correlation between soil layers is mainly controlled by lsm physics and there is no artificially additional imposed error correlation between soil layers 2 3 grace data processing the grace data release 05 rl05 provided by the center for space research csr the university of texas austin bettadpur 2012 is obtained between january 2010 and december 2015 the product consists of the monthly spherical harmonic coefficient shc complete up to degree and order 96 the full error variance covariance matrix is also provided as a part of the product the error matrix is only available up to june 2014 and the monthly average values are used for the missing months july 2014 dec 2015 the grace derived tws and its uncertainty over the goulburn catchment are computed following the approach in tangdamrongsub et al 2017b first the degree 1 coefficients shc provided by swenson et al 2008 are restored and the c20 term is replaced by the value estimated from the satellite laser ranging cheng and tapley 2004 second the long term mean january 2010 december 2015 is computed and removed from the monthly product to obtain the shc variations and the destriping swenson and wahr 2006 and 300 km radius gaussian smoothing filters jekeli 1981 are applied to the shc variations to suppress the high frequency noise third the tws variation tws is computed from the filtered shc variations using the method described by wahr et al 1998 because the grace derived tws shows no significant spatial variability over the study area the catchment averaged tws is used in this study finally a signal restoration e g chen et al 2014 is applied to the computed tws to restore the damped signal caused by the applied filters the method iteratively searches for the genuine tws using a forward model constructed solely from the grace data to be consistent with the model estimate the temporal mean value of tws january 2010 december 2015 from the cable estimate is added to the grace derived tws to obtain the absolute tws prior to the assimilation process finally the tws uncertainty is computed based on the grace full error variance covariance matrix using error propagation see e g tangdamrongsub et al 2017b as grace error is spatially correlated in nature swenson and wahr 2006 deriving the error from the available full covariance matrix represents a more realistic grace uncertainty compared to the application of a uniform error value e g tangdamrongsub et al 2015 2 4 satellite soil moisture observations the daily satellite soil moisture retrievals derived from the soil moisture and ocean salinity smos kerr et al 2012 and the soil moisture active passive smap entekhabi et al 2010 missions are used in this study smos data are obtained from the level 3 gridded product bitar et al 2017 provided by the centre aval de traitement des données smos catds https www catds fr operated for the centre national d etudes spatiales cnes by the french research institute for exploitation of the sea ifremer the data are available from 15 january 2010 to present with a spatial resolution of 25 km on the equal area scalable earth ease brodzik et al 2012 grid the smap data are retrieved from the level 3 version 4 radiometer global daily 36 km ease grid product spl3smp provided by the national snow and ice data center distributed active archive center nsidc daac https nsidc org data smap the product contains the volumetric soil moisture retrieved by the smap passive microwave radiometer available from 31 march 2015 to present for both smos and smap the data are resampled to a 25 km regular grid to reconcile the observations with the model grid space on days for which more than one sm retrieval is available the daily average is used to ensure consistency with the model time step following previous sm studies e g colliander et al 2017 lievens et al 2015 liu et al 2016 the measurement error of both smos and smap are both assumed to be 0 04 m3 m3 it is acknowledged that triple collocation analysis tca may potentially provide more accurate sm error estimates de lannoy and reichle 2016 however applying tca in sm da requires linear consistency between modeled and retrieved sm dong and crow 2018 this assumption has not yet been validated in practice therefore constant rather than tca based error estimates are used in this study the assimilation of satellite soil moisture data into the lsm requires the application of rescaling to reduce systematic bias that may be found between the model estimate and the observation crow et al 2005 reichle and koster 2004 the bias correction can be used to transform the observation into model space and reduce the inconsistency between their respective climatology koster et al 2009 renzullo et al 2014 in this study cumulative density function matching cdf matching reichle and koster 2004 is used to rescale satellite observation to lsm climatology the approach is applied separately for each model grid cell and each satellite data product with respect to its entire period 2 5 in situ data the in situ soil moisture and groundwater measurements between january 2010 and december 2015 are obtained from the ground observation networks for validation the in situ soil moisture data are provided by the sasmas network rüdiger et al 2007 data at each depth are provided in terms of volumetric soil moisture θ m3 m3 the 0 5 θ0 5cm and 0 30 cm θ0 30cm data are used in this study due to their compatibility with the model soil layers see section 2 2 in situ groundwater level data h are obtained from the department of primary industries dpi office of water nsw http www water nsw gov au groundwater storage gws simulated in the model can be converted to h if specific yield data are available however this is not the case for the goulburn catchment 3 methodology 3 1 ensemble open loop enol the enol is used as a reference to evaluate the performance and the uncertainty of the lsm outputs in the enol the forcing data u and model parameters α are perturbed see section 2 2 and the model propagation is performed without assimilation as 1 x t t 1 i f x t 1 i u t i α i where f is the model operator used to propagate the states from t 1 to t x is the model state vector and i 1 2 3 n denotes the index of ensemble member n in total in this paper the enol estimate is the ensemble mean of x t t 1 i note that the perturbed initial states are obtained by spinning up the model in enol mode for six years between 2004 and 2009 prior to the assimilation period in this study the state vector x consists of a total of seven variables soil moisture at six layers and one groundwater storage see section 3 2 for more details the contribution of the snow water and canopy water components to the total water storage in the goulburn catchment are negligible hence they are not included in the state vector followingtangdamrongsub et al 2017a an ensemble size of n 300 is used which is sufficient to ensure the effectiveness of da in the goulburn catchment 3 2 ensemble kalman smoother enks the enks consists of a forecast and analysis update step similar to the enol the states are propagated forward in time using the lsm in the forecast step the period of model propagation depends on the period of the assimilated observations e g approximately one month for grace a set of observations was computed by perturbing the measurement with its associated covariance r s burgers et al 1998 the subscript s denotes smoother e g s t l 1 t where l is the smoother window length the state vector is updated as 2 x s s i x s t l i k s y s i h x s t l i with 3 k s p e s h s t h s p e s h s t r e s 1 where y s i is a perturbed observation vector h s is an operator which relates the ensemble state x s t l i to the measurement vector y s i k is the kalman gain matrix and p e s and r e s are the ensemble error covariance matrices of the model and observation respectively note that the state variables from t l 1 to t are considered in the smoother case if the matrix a contains the ensemble states and a is the matrix of the same size as a and filled with the mean value computed from all ensemble members the ensemble error covariance matrix p e s can be computed as follows 4 p e s a a a a t n 1 similarly r e is computed as 5 r e s d d d d t n 1 where d stores the perturbed observation and d is the ensemble mean the da estimate is the ensemble mean of x s s i 3 3 design of the da schemes the different da schemes are developed to incorporate observations with different spatial temporal resolutions and error characteristics into the da system simultaneously three different da schemes are considered here fig 2 sm da only soil moisture is assimilated grace da only grace is assimilated and multivariate da both soil moisture and grace are assimilated as described in section 3 2 the state vector contains daily volumetric soil moisture of six different layers and groundwater storage components for a particular model grid cell j on a given day t the state vector can be defined as θ 1 j t θ 2 j t θ 3 j t θ 4 j t θ 5 j t θ 6 j t g w s j t t where θ is the volumetric soil moisture m3 m3 and gws is the groundwater storage m the state variables are obtained from the results of model propagation in the sm da fig 2a the soil moisture observations are assimilated every l 3 days on the model grid cell individually only smos data is used between january 2010 and february 2015 and the dimension of the state vector is mlx1 where m 7 is the number of the state variables the 3 day window allows the soil moisture observations to have full coverage over the goulburn catchment and yields the adequate ensemble spread between the updates the observation vector d contains the smos data with dimension lx1 the h s matrix is defined as 6 h s h s m j t 1 0 0 0 h s m j t 2 0 0 0 h s m j t 3 7 h s m j t s 1 s 2 0 0 0 0 0 where s 1 s 2 are the thickness of the first and second soil layers respectively the soil thickness is described in section 2 2 the h s matrix dimension lxml relates the smos observation to the top two soil layers bias correction is performed prior to the application of da to reduce the systematic error between the model estimated and the satellite retrieved soil moisture see section 3 3 when smap data are available e g from march 2015 the smos and smap data are assimilated into the lsm simultaneously lievens et al 2017 demonstrated that the joint sm da performed better than a single sm da case in the case of smos smap assimilation the dimension of h s and d are extended to 2lxml and 2lx1 respectively to include the measurement operator associated with the smap data in this study the errors in smos and smap data are assumed to be uncorrelated in the grace da fig 2b the model states are updated at a monthly time scale consistent with the grace temporal resolution the model state vector contains all model grid cells inside the blue polygon in fig 1 of daily state variables within approximately one month the state vector is also constructed from the results of model propagation the length of the vector is jlm where j is the number of grid cells in the study area and l 1 month the monthly time window used for each update is based on the time tag of the grace product as the monthly window used to produce a grace solution is not necessarily a calendar month l is different in each update and varies between 13 and 31 days following grace data used the observation vector y s is a 1 1 vector containing the monthly average values of the catchment mean tws the matrix h s is used to convert the volumetric soil moisture and groundwater storage into the catchment averaged tws of the month 8 h s h g t 1 h g t 2 h g t l 9 h g g j 1 g j 2 g j j 10 g j s 1 s 2 s 3 s 4 s 5 s 6 1 j l where s 1 s 6 are the thickness of each soil layer see section 2 2 in the multivariate da fig 2c the sm da and grace da schemes are combined the sm da is firstly performed step 1 in fig 2c and its updated state variables are used as the forecast state in the grace da step 2 it should be noted that unlike the 3d enkf reichle and koster 2003 satellite soil moisture observations are only used for correcting collocated soil moisture estimates however a recent study demonstrates that remote sensing observation error is highly structured in space suggesting a spatial correlation of soil moisture retrieval errors dong et al 2017 this complicates the accurate parameterization of the observation error matrix in a 3d updating da scheme hence the soil moisture retrievals are not used for correcting nearby grid cells 3 4 evaluation metrics the volumetric soil moisture estimates are validated with the in situ soil moisture and groundwater data in terms of temporal correlation ρ and unbiased root mean square difference ubrmsd entekhabi et al 2010 11 ρ x sim e x sim x obs e x obs x sim e x sim 2 x obs e x obs 2 12 u b r m s d e x sim e x sim x obs e x obs 2 where x sim and x obs are state vectors from simulation model estimate and observation e g satellite product in situ data respectively and e is the expectation operator all in situ soil moisture and groundwater data inside the same model grid cell fig 1 are averaged before the comparison this produces four grid cells of in situ soil moisture s1 s4 and four of in situ groundwater data g1 g4 note that only the temporal correlation between h and gws is used to evaluate the groundwater storage estimate against groundwater level due to the absence of accurate information on specific yield 4 results and discussion 4 1 impact of da on soil moisture estimate the top soil moisture θ0 5cm is estimated from the enol and three da scenarios sm only grace only and both the goodness of fit in terms of correlation is evaluated against the smos data fig 3 top row to investigate the impact of different da scenarios on the θ0 5cm estimates from fig 3 the sm da and the multivariate da deliver 0 1 0 15 higher averaged correlation values compared to the enol this is expected as the smos smap data are being integrated into the state estimate particularly into the θ0 5cm component by the applications of the sm da and multivariate da the kalman gain attempts to statistically optimize the fit between the θ0 5cm estimate and the smos smap observation resulting in an improved agreement between them similar behavior is also observed from the evaluation with the smap data not shown including the smos smap data in the assimilation system is proven necessary to improve the θ0 5cm estimate by contrast grace da reduces the correlation value by 0 1 the degradation is likely caused by the limited sensitivity of grace observations to top soil moisture the top soil component is strongly governed by high frequency meteorological forcing wu et al 2002 while grace can only observe monthly catchment averaged tws changes which is dominated by the low frequency variability of deep water storage components also the degradation of surface sm after assimilating grace suggests an inconsistency between the observed and modeled sm tws relationship as shown in fig 4 the modeled tws change is less sensitive to the modeled sm change compared to the corresponding observations therefore correcting the modeled tws to grace may over correct sm estimates and lead to degraded results clearly assimilating grace data alone cannot provide the high spatiotemporal variability essential for modeling the water storage in the top soil layer and the inclusion of grace data tends to have a negative impact on the θ0 5cm estimate all da cases reduce the uncertainty ensemble spread of the θ0 5cm estimate fig 3 bottom row compared to the enol the sm da and multivariate da reduce the uncertainty by a factor of three while the grace da reduces the uncertainty by a factor of 1 2 importantly the applications of the sm da and multivariate da also lead to an approximately three times lower uncertainty than the assigned smos smap uncertainty value in addition it is seen that the uncertainty of the θ0 5cm estimate is lower in the south eastern part of the catchment this is likely influenced by the lower field capacity associated with lower clay content in the southern region leading to a small variation of θ0 5cm and its uncertainty the spatial pattern of the uncertainty also explains the contribution of smos smap observation the update is likely limited in the south eastern part where the model uncertainty is small this is apparent in e g fig 3b where slightly lower correlation values are observed mostly in the south eastern region 4 2 impact of da on tws estimate the basin averaged tws of all three da cases is shown in fig 5 also the correlation with respect to grace is shown in fig 6 top row assimilating smos smap only yields a negative impact on the tws estimates resulting in a decreased agreement between the state estimate and the grace observation in the sm da the smoother underestimates the annual and inter annual variability of tws and reduces the averaged correlation value by 0 2 fig 6b the smoothers estimate a set of the ensemble by optimizing the kalman gain or likelihood function associated only with the θ0 5cm component while leaving the other storage components unconstrained computing the posterior estimate based on the resulted sample set produces an improved θ0 5cm estimate see also section 4 1 but does not necessarily improve the computation of total storage changes the degradation in tws may be due to the fact that the satellite sm observation does not provide information on the total column water which is crucial in the accurate distribution of the water through all stores in the grace da the constraint is applied to the entire water column leading to an improved agreement between the tws estimate and the grace observation the averaged correlation value is increased by 0 2 fig 6c the impact of the grace da is clearly seen in the tws adjustment before and after march 2012 to evaluate this the total mass variation in the two periods january 2010 march 2012 and april 2012 december 2015 is computed and shown in table 2 to determine the total mass of tws variation gton in each period the long term trend m year is first estimated and multiplied by the area of the goulburn catchment see section 2 1 the density of water and the number of years in that period respectively grace observes the increased mass estimate of 0 6 gton prior to april 2012 which is mainly induced by the 2010 2011 la niña rainfall see fig 5 the enol underestimates the mass estimate by 0 1 gton during this period the estimate is improved by the grace da leading to a 20 improvement in cross correlation between the adjusted mass estimate and grace data similar behavior is observed during the post la niña period after march 2012 when the grace da produces a 30 improvement in cross correlation unlike the grace da the sm da cannot improve the mass estimate in both periods due to e g the deficiency of deep water storage information necessary for the tws computation it is apparent that sm da and grace da are valuable for updating θ0 5cm and tws estimates respectively while they show limited benefit for the estimation of the other components the underlying strengths motivate the concept of assimilating the smos smap and grace observation simultaneously into the lsm in the multivariate da the θ0 5cm and tws components are adjusted toward the smos smap and grace observation respectively resulting in the final state estimates that agree with both observations the tws estimated with multivariate da agrees better with the grace observations by 0 12 in cross correlation fig 6d and simultaneously the θ0 5cm estimate presenting better correlation by 0 1 with smos smap data see fig 3b consequently the multivariate da improves the mass estimate during the la niña period table 2 the grace da and multivariate da reduce the tws uncertainty by more than a factor of 2 fig 6 bottom row as expected the sm da cannot deliver a reliable tws estimate as seen in the uncertainty which is approximately twice that obtained from the grace da and multivariate da 4 3 validation with in situ data 4 3 1 soil moisture the θ0 5cm variations estimated from all da case studies are validated against the in situ data at s1 s4 fig 7 the validation is conducted in terms of correlation and ubrmsd and the estimated values are shown in fig 8 cable performs remarkably well in the estimation of θ0 5cm and provides a good agreement with the in situ data at all locations with an averaged correlation value of 0 69 see enol in fig 8a the sm da and multivariate da increase the correlation value further by 7 from 0 69 to 0 73 and decrease the ubrmsd by 11 the improved result is anticipated since the satellite sm observation is used in the sm da and multivariate da by contrast the grace da shows an apparent negative impact on the θ0 5cm estimate see fig 8a b comparing to the enol the grace da overestimates θ0 5cm by a factor of 1 5 ubrmsd and decreases the correlation by 50 poor performance is due to the insensitivity of grace data to the signal associated with the top soil component as described in sections 4 1 and 4 2 the θ0 30cm variation is also validated against the in situ data with the statistical results shown in fig 8 bottom row cable provides a very accurate θ0 30cm component with a correlation value of almost 0 7 fig 8c unlike the θ0 5cm the sm da and multivariate da do not improve the correlation and ubrmsd values of the θ0 30cm estimate this is consistent previous studies that found that the benefit of surface sm da in root zone sm estimates depends on the accuracy of model physics dunne et al 2007 kumar et al 2009 in line with the analysis found in fig 4 grace da also reduces the quality of the θ0 30cm estimate seen from both metrics the benefit of including the smap data in the da system is evaluated the multivariate da results from two case studies using smap data between march and december 2015 are compared with the in situ data at s1 s2 and s4 fig 9 a c the in situ data at s3 are not available during this validation period in all locations the daily θ0 5cm estimates of the smos only assimilation and the smos smap assimilation are very similar and visibly show a better agreement with the in situ data comparing to the enol the correlation value is increased to almost 0 2 e g at s1 fig 9d and the highest correlation value is seen when the smap data is included in the da system 3 higher compared to the smos only assimilation the application of the smos smap assimilation also reduces the spurious peaks of the θ0 5cm estimate e g in october 2015 fig 9a b and november 2015 fig 9c leading to a better agreement with the in situ data evidently the smap data should be considered in the da process to maintain the accuracy in terms of agreement with the in situ data of the θ0 5cm estimate in the goulburn catchment 4 3 2 groundwater storage the gws estimates are compared with the in situ groundwater level anomalies h at g1 g4 fig 10 and the averaged correlation coefficients are shown in fig 11 in fig 10 the application of the sm da leads to an incorrect groundwater storage estimate with a large disagreement between the gws estimate and h particularly at g1 where the correlation value is as low as 0 6 the poor performance can be attributed to the lack of groundwater information in the satellite sm observation see sections 4 1 and 4 2 the h shows a very similar temporal variation in all g1 g4 locations the different scale between δgws and δh likely causes the visual phase shift seen in fig 10 applying a specific yield e g ranging between 0 and 1 to δh could reduce the magnitude of the right axis and led to the reduction of visual phase shift however the conversion is not performed due to the absence of specific yield as described in section 2 5 the temporal variations of h follow those of the tws estimate and the grace observations see fig 5 h and tws increases under the influence of the la niña rainfall in 2011 2012 and decreases afterward the similarity suggests that grace is sensitive to the signal of the groundwater store more than the shallow storage component in particular the groundwater level data h are correlated throughout the catchment with the cross correlation of 0 9 see fig 6 in tangdamrongsub et al 2017a the assimilation of grace data in both grace da and multivariate da increases the correlation between the gws estimate and h changes in each grid by a factor of 2 compared to the enol estimate the enol simulated gws shows smaller variations compared to the da estimate and h cable models the unconfined aquifer using a simple groundwater model decker 2015 decker and zeng 2009 niu et al 2007 vergnes et al 2012 that calculates the groundwater recharge based on the available water after vertical redistribution between the soil layers this simplification might lead to an enclosed groundwater component in the deep soil layer when the distributing water does not reach the defined field capacity in such a case groundwater recharge is not accounted for correctly and the groundwater storage changes become small the soil and groundwater components are not efficiently separated and the variation of the gws estimate is likely presented in the deep soil layer assimilating grace only always shows a better performance in the gws estimate and provides 29 higher average correlation compared to assimilating both grace and smos smap measurements in the multivariate da gws is updated by the grace da step 2 in fig 2c after the application of the sm da step 1 in fig 2c the application of the sm da in the multivariate da likely decreases the uncertainty of the state estimate which consequently reduces the contribution of grace in the analysis step of the grace da rescaling the grace uncertainty could increase the contribution of the grace observation e g tian et al 2017 5 conclusions this study evaluates three different da schemes to assimilate different combinations of satellite observations smos smap grace and both smos smap and grace in the goulburn catchment australia validation against the in situ data reveals that the performance of the da in estimating soil moisture and groundwater storage highly depends on the choice of the observation type the application of the sm da significantly improves the top 0 5 cm soil moisture but degrades the groundwater component whereas the grace da improves only the gws estimate applying the multivariate da simultaneously increases the accuracy of the soil moisture and groundwater storage estimates though at a slightly lesser degree of improvement compared to the single observation da case the application of the sm da underlines the importance of the smos smap data on the sm estimate by increasing the 0 5 cm correlation with in situ observations by up to 7 the benefit on the 0 30 cm soil moisture and groundwater component is minor or negative which is in line with several previous studies for example blankenship et al 2016 kolassa et al 2017 ridler et al 2014 and tian et al 2017 who reported a detrimental impact on the root zone and deep storage components sm da significantly reduces the uncertainty of storage in the top 0 5 cm soil layer but does not have an impact on the tws uncertainty the constraint solely in the top soil moisture component by the sm da does not necessarily have a positive effect on the entire water column we also found that assimilating both smos and smap data simultaneously is recommended in the goulburn catchment the advantage of multivariate sm da is also found in lievens et al 2017 kumar et al 2018 jasinski et al 2019 however it should be noted that smos and smap soil moisture may have potentially common systematic errors which may affect the observation error matrix future studies should explore the magnitude of smos smap error cross correlation and its impact on the da results the grace da demonstrates an outstanding example of improving the groundwater storage of the goulburn catchment particularly at a finer spatial resolution 25 km compared to grace s intrinsic resolution 100 km as the groundwater variation of the goulburn catchment is likely to be spatially correlated due to the large unconfined aquifer tangdamrongsub et al 2017a assimilating a coarser spatial scale tws from the grace observation can benefit the groundwater estimate even in the smaller individual grid cell grace da leads to the improved groundwater estimate by increasing the correlation to independent in situ groundwater level data however assimilating grace into lsm does not provide a positive impact on the top or surface sm components this is consistent with the conclusions of li et al 2012 and tian et al 2017 grace da significantly reduces the uncertainty of the tws estimate but has only a minor impact on the sm uncertainty it is known that grace is sensitive to the signal of the entire water column dominated by the processes in deeper layers the grace da might therefore adversely distribute the deep water storage signals into the shallow one multivariate da provides an improvement over both sm and gws estimates assimilating the satellite soil moisture and grace data together allows the high frequency components to be adjusted by the sm da while the low frequency signal is corrected by the grace da leading to the increased correlation values of both the 0 5 cm soil moisture by 7 and gws estimates by 65 compared to the independent in situ data however the multivariate da does not outperform the sm da or the grace da in the separate estimation of the 0 5 cm soil moisture and gws the da approach optimized the model states with multiple cost functions relevant to shallow and deep groundwater storage changes e g minimizing the residuals against both smos smap and grace resulting in an optimal solution that is not closer to one particular observation as also found by tian et al 2017 with the increased availability of satellite retrievals and ground measurement networks multivariate da can be an effective tool to exploit diverse observations the multivariate da presented in this study can be extended to include different types of new observations e g soil moisture from sentinel 1 lievens et al 2017 tws from grace follow on flechtner et al 2014 snow water equivalent from snowex kim 2017 with simple modification of the measurement operator as described in section 3 2 ongoing research is focused on the sensitivity to the selected window length l of the smoother dong et al 2015 and applications over regions with different climate conditions e g snow covered basins credit authorship contribution statement natthachet tangdamrongsub conceptualization methodology software validation writing original draft shin chan han supervision writing review editing in young yeo resources writing review editing jianzhi dong methodology writing review editing susan c steele dunne methodology writing review editing garry willgoose resources jeffrey p walker writing review editing decleration of competing interest none acknowledgment this work was funded by the university of newcastle to support nasa s grace and grace follow on projects as an international science team member and by the australian research council discovery project dp170102373 natthachet tangdamrongsub was supported by the nasa earth science division in support of the national climate assessment we thank awr s associated editor and three anonymous reviewers who provided insightful and constructive comments leading to a significant improvement of the paper data used in this study are publicly available with the access information provided in section 2 
554,assimilating remote sensing observations into land surface models has become common practice to improve the accuracy of terrestrial water storage tws estimates such as soil moisture and groundwater for understanding the land surface interaction with the climate system as well as assessing regional and global water resources such remote sensing observations include soil moisture information from the l band soil moisture and ocean salinity smos and soil moisture active passive smap missions and tws information from the gravity recovery and climate experiment grace this study evaluates the benefit of assimilating them into the community atmosphere and biosphere land exchange cable land surface model the evaluation is conducted in the goulburn river catchment south east australia where various in situ soil moisture and groundwater level data are available for validating data assimilation da approaches it is found that the performance of da mainly depends on the type of observations that are assimilated the smos smap only assimilation sm da improves the top soil moisture but degrades the groundwater storage estimates whereas the grace only assimilation grace da improves only the groundwater component assimilating both observations multivariate da results in increased accuracy of both soil moisture and groundwater storage estimates these findings demonstrate the added value of multivariate da for simultaneously improving different model states thus leading to a more robust da system keywords smos smap grace enks cable multivariate data assimilation soil moisture groundwater 1 introduction accurate knowledge on terrestrial water storage tws is crucial for the assessment of climate variation and water resource availability entekhabi et al 1996 pitman 2003 rodell et al 2007 the accuracy of tws components e g soil moisture groundwater snow surface water simulated by land surface models lsm at high spatial resolution is commonly degraded by uncertainties in meteorological forcing model parameter calibration and land surface process representation moradkhani et al 2005 wood et al 2011 hydrologic information can also be obtained from satellite remote sensing observations e g kerr et al 2012 maurer et al 2003 tapley et al 2004 however tws components such as subsurface soil moisture and groundwater are usually not observed directly by in situ observations and the limited satellite coverage and sensing depths often restrict the reliability of the observations reichle et al 2008 data assimilation da can be used to combine various types of observations at different temporal and spatial resolutions with the model simulations according to the relative size of their errors reichle 2008 reichle et al 2008 da has been successfully applied in enhancing model estimated hydrologic components such as tws e g li et al 2012 soil moisture e g lievens et al 2015 groundwater e g tangdamrongsub et al 2018b snow e g andreadis and lettenmaier 2006 and runoff e g weerts and el serafy 2006 various satellite observations can be considered in the da system to improve the key components of the tws estimate for example surface soil moisture has an important role in the variability of the hydrological cycle and climate system entekhabi et al 1996 koster et al 2009 schumann et al 2009 and can be measured by l band radiometers i e from the soil moisture and ocean salinity smos kerr et al 2012 and soil moisture active passive entekhabi et al 2010 satellite missions chan et al 2016 both satellite missions provide global soil moisture products at a spatial resolution of 25 36 km representing the wetness in the top 0 5 cm soil layer approximately every 3 days the smos and smap radiometer data have been exploited in soil moisture data assimilation sm da systems over several river basins e g ahlergaarde western denmark ridler et al 2014 murray darling lievens et al 2015 continental australia e g tian et al 2017 the great lakes xu et al 2015 and north america e g blankenship et al 2016 these studies have demonstrated the benefits of sm da on both surface and root zone soil moisture components e g de lannoy and reichle 2016 tian et al 2017 xu et al 2015 however sm da has been found to have a negative impact on the groundwater storage estimate tian et al 2017 in addition to the surface soil moisture tws variations tws can be derived from gravity measurements by the gravity recovery and climate experiment grace satellite mission tapley et al 2004 the grace twin satellites measure changes of the earth s gravity field every month using a combination of several measurements including k band ranging accelerometer attitude and orbital data bettadpur 2012 because hydrological mass variations are dominant at a monthly time scale the grace data are commonly presented in terms of tws and have been used in a wide range of hydrological applications including data assimilation e g zaitchik et al 2008 eicker et al 2014 for drought detection e g houborg et al 2012 li et al 2012 kumar et al 2016 flood analysis reager et al 2015 groundwater loss analysis girotto et al 2017 tangdamrongsub et al 2018b and snow estimation forman et al 2012 su et al 2010 the benefit of grace da was observed particularly in deep storage components such as groundwater storage e g tangdamrongsub et al 2015 zaitchik et al 2008 however grace da is generally less effective in surface soil moisture improvement li et al 2012 tangdamrongsub et al 2017a tian et al 2017 the goal of multivariate da is to combine the strengths of sm da and grace da to simultaneously improve soil moisture and groundwater estimates tian et al 2017 elaborated this concept and showed that the accuracy of surface and deep storage components could be improved by the application of grace and smos data assimilation similarly kumar et al 2018 and jasinski et al 2019 applied multivariate da using multiple satellite soil moisture and snow products to improve the skills of model state estimates and climate assessment indicators kumar et al 2018 showed that the performance of da is improved with new satellite sensors based on these findings multivariate assimilation of grace and l band satellite soil moisture sensors e g smos smap is expected to lead to increased accuracy of soil moisture and groundwater estimates this study develops a multivariate da with grace smos and smap data to improve the accuracy of regional soil moisture and groundwater storage estimates the main research objective is to investigate the performance of multivariate da in simultaneously improving soil moisture and groundwater storage estimates different da schemes are developed to incorporate different observations into the da system simultaneously three different da scenarios subject to three different observation cases sm only grace only and both are evaluated in terms of estimating water storage e g surface and root zone soil moisture and groundwater the da approach is developed based on ensemble kalman smoother enks see e g dunne et al 2007 dong et al 2015 tian et al 2017 tangdamrongsub et al 2018b the lsm used in this study is the community atmosphere and biosphere land exchange cable decker 2015 the analysis is conducted over the goulburn river catchment rüdiger et al 2007 located in the eastern part of new south wales australia where extensive records of in situ soil moisture and groundwater are available from more than 20 sites throughout the catchment the da results are assessed by comparing them against the in situ data and the ensemble open loop estimate enol model run without da the evaluation is performed between january 2010 and december 2015 when grace smos smap from march 2015 and in situ data are available 2 materials 2 1 study area the goulburn river catchment is located in the south eastern part of the murray darling basin and has a sub humid or temperate climate fig 1 the catchment has a total area of 6540 km2 and consists of more than ten sub catchments including the krui and merriwa catchments where in situ soil moisture data are regularly recorded the catchment is maintained by the scaling and assimilation of soil moisture and streamflow sasmas project rüdiger et al 2007 http www eng newcastle edu au sasmas sasmas sasdata html the land cover of the catchment consists of a floodplain clear grassland crop in the northern part and a mountain range with dense vegetation in the south the northern part of the catchment is particularly suitable for satellite soil moisture remote sensing studies due to its low to moderate vegetation cover furthermore the clay content of the top soil layer 0 5 cm in the northern part is several times greater than in the south senanayake et al 2019 http www clw csiro au aclep soilandlandscapegrid higher variability in the top soil moisture can therefore be anticipated in the northern area the mean annual rainfall of the catchment is 700 mm and reaches 1100 mm in the higher altitude areas monthly mean minimum maximum temperatures reach approximately 16 30 c in summer and 2 14 c in winter no snowfall is presented in the catchment lsm simulations are expected to perform well over the catchment due to the absence of groundwater abstraction and streamflow control 2 2 land surface model setup the community atmosphere and biosphere land exchange cable land surface model is used to simulate daily volumetric soil moisture and groundwater storage at approximately 25 km resolution see fig 1 the model can be obtained from https trac nci org au trac cable and the model user guide and descriptions can be found in decker 2015 kowalczyk et al 2006 and ukkola et al 2016 cable estimates soil moisture in six separate layers in this study the soil thicknesses from the top to bottom compartments are set to 1 2 3 8 25 39 9 107 9 287 2 cm respectively in comparison with the in situ data see section 2 5 the combination of the first two model soil layers represents the 0 5 cm soil moisture component while the combination of the first three denotes the 0 30 cm component the forcing data used in cable are precipitation air temperature snowfall rate wind speed humidity surface pressure and shortwave and longwave downward radiation similar to tangdamrongsub et al 2018a the model is forced with meteorological input from the global land data assimilation system gldas rodell et al 2004 following the sensitivity study of tangdamrongsub et al 2018a gldas precipitation is replaced by data from the tropical rainfall measuring mission trmm huffman et al 2007 to improve the performance of the cable model two primary error sources of the lsm are the meteorological forcing data and the model parameters in the da process see section 3 the precipitation is perturbed based on the uncertainty provided by the trmm product huffman 1997 the shortwave radiation is perturbed using multiplicative white noise with 10 of the nominal values an additive white noise is used for the air temperature it is acknowledged that while a homoscedastic error would be more realistic for air temperature an offline sensitivity analysis found that the temperature error had only a marginal influence on the state estimates compared to e g precipitation the errors of forcing data are assumed to be spatially correlated as such an exponential correlation function is applied to the covariance matrix for each forcing variable the correlation lengths for forcing data were determined using variogram analysis and found to be approximately 25 km model parameters associated with soil moisture and groundwater components are also perturbed with a magnitude of 10 the selected model parameters are the fractions of clay sand silt and the drainage parameters that control the soil storage capacity and amount of subsurface runoff respectively both have a direct impact on the soil moisture and groundwater storages see table 2 in tangdamrongsub et al 2018a for more details the perturbation sizes of forcing data and parameters are determined based on the ensemble verification measures de lannoy et al 2006 mainly to allow an adequate spread of the ensemble between updates in the da process table 1 summarizes the forcing and parameter perturbation of this study note that the model state is not perturbed directly but rather perturbed as a result of model propagation associated with the perturbed forcing and perturbed model parameters as a result the correlation between soil layers is mainly controlled by lsm physics and there is no artificially additional imposed error correlation between soil layers 2 3 grace data processing the grace data release 05 rl05 provided by the center for space research csr the university of texas austin bettadpur 2012 is obtained between january 2010 and december 2015 the product consists of the monthly spherical harmonic coefficient shc complete up to degree and order 96 the full error variance covariance matrix is also provided as a part of the product the error matrix is only available up to june 2014 and the monthly average values are used for the missing months july 2014 dec 2015 the grace derived tws and its uncertainty over the goulburn catchment are computed following the approach in tangdamrongsub et al 2017b first the degree 1 coefficients shc provided by swenson et al 2008 are restored and the c20 term is replaced by the value estimated from the satellite laser ranging cheng and tapley 2004 second the long term mean january 2010 december 2015 is computed and removed from the monthly product to obtain the shc variations and the destriping swenson and wahr 2006 and 300 km radius gaussian smoothing filters jekeli 1981 are applied to the shc variations to suppress the high frequency noise third the tws variation tws is computed from the filtered shc variations using the method described by wahr et al 1998 because the grace derived tws shows no significant spatial variability over the study area the catchment averaged tws is used in this study finally a signal restoration e g chen et al 2014 is applied to the computed tws to restore the damped signal caused by the applied filters the method iteratively searches for the genuine tws using a forward model constructed solely from the grace data to be consistent with the model estimate the temporal mean value of tws january 2010 december 2015 from the cable estimate is added to the grace derived tws to obtain the absolute tws prior to the assimilation process finally the tws uncertainty is computed based on the grace full error variance covariance matrix using error propagation see e g tangdamrongsub et al 2017b as grace error is spatially correlated in nature swenson and wahr 2006 deriving the error from the available full covariance matrix represents a more realistic grace uncertainty compared to the application of a uniform error value e g tangdamrongsub et al 2015 2 4 satellite soil moisture observations the daily satellite soil moisture retrievals derived from the soil moisture and ocean salinity smos kerr et al 2012 and the soil moisture active passive smap entekhabi et al 2010 missions are used in this study smos data are obtained from the level 3 gridded product bitar et al 2017 provided by the centre aval de traitement des données smos catds https www catds fr operated for the centre national d etudes spatiales cnes by the french research institute for exploitation of the sea ifremer the data are available from 15 january 2010 to present with a spatial resolution of 25 km on the equal area scalable earth ease brodzik et al 2012 grid the smap data are retrieved from the level 3 version 4 radiometer global daily 36 km ease grid product spl3smp provided by the national snow and ice data center distributed active archive center nsidc daac https nsidc org data smap the product contains the volumetric soil moisture retrieved by the smap passive microwave radiometer available from 31 march 2015 to present for both smos and smap the data are resampled to a 25 km regular grid to reconcile the observations with the model grid space on days for which more than one sm retrieval is available the daily average is used to ensure consistency with the model time step following previous sm studies e g colliander et al 2017 lievens et al 2015 liu et al 2016 the measurement error of both smos and smap are both assumed to be 0 04 m3 m3 it is acknowledged that triple collocation analysis tca may potentially provide more accurate sm error estimates de lannoy and reichle 2016 however applying tca in sm da requires linear consistency between modeled and retrieved sm dong and crow 2018 this assumption has not yet been validated in practice therefore constant rather than tca based error estimates are used in this study the assimilation of satellite soil moisture data into the lsm requires the application of rescaling to reduce systematic bias that may be found between the model estimate and the observation crow et al 2005 reichle and koster 2004 the bias correction can be used to transform the observation into model space and reduce the inconsistency between their respective climatology koster et al 2009 renzullo et al 2014 in this study cumulative density function matching cdf matching reichle and koster 2004 is used to rescale satellite observation to lsm climatology the approach is applied separately for each model grid cell and each satellite data product with respect to its entire period 2 5 in situ data the in situ soil moisture and groundwater measurements between january 2010 and december 2015 are obtained from the ground observation networks for validation the in situ soil moisture data are provided by the sasmas network rüdiger et al 2007 data at each depth are provided in terms of volumetric soil moisture θ m3 m3 the 0 5 θ0 5cm and 0 30 cm θ0 30cm data are used in this study due to their compatibility with the model soil layers see section 2 2 in situ groundwater level data h are obtained from the department of primary industries dpi office of water nsw http www water nsw gov au groundwater storage gws simulated in the model can be converted to h if specific yield data are available however this is not the case for the goulburn catchment 3 methodology 3 1 ensemble open loop enol the enol is used as a reference to evaluate the performance and the uncertainty of the lsm outputs in the enol the forcing data u and model parameters α are perturbed see section 2 2 and the model propagation is performed without assimilation as 1 x t t 1 i f x t 1 i u t i α i where f is the model operator used to propagate the states from t 1 to t x is the model state vector and i 1 2 3 n denotes the index of ensemble member n in total in this paper the enol estimate is the ensemble mean of x t t 1 i note that the perturbed initial states are obtained by spinning up the model in enol mode for six years between 2004 and 2009 prior to the assimilation period in this study the state vector x consists of a total of seven variables soil moisture at six layers and one groundwater storage see section 3 2 for more details the contribution of the snow water and canopy water components to the total water storage in the goulburn catchment are negligible hence they are not included in the state vector followingtangdamrongsub et al 2017a an ensemble size of n 300 is used which is sufficient to ensure the effectiveness of da in the goulburn catchment 3 2 ensemble kalman smoother enks the enks consists of a forecast and analysis update step similar to the enol the states are propagated forward in time using the lsm in the forecast step the period of model propagation depends on the period of the assimilated observations e g approximately one month for grace a set of observations was computed by perturbing the measurement with its associated covariance r s burgers et al 1998 the subscript s denotes smoother e g s t l 1 t where l is the smoother window length the state vector is updated as 2 x s s i x s t l i k s y s i h x s t l i with 3 k s p e s h s t h s p e s h s t r e s 1 where y s i is a perturbed observation vector h s is an operator which relates the ensemble state x s t l i to the measurement vector y s i k is the kalman gain matrix and p e s and r e s are the ensemble error covariance matrices of the model and observation respectively note that the state variables from t l 1 to t are considered in the smoother case if the matrix a contains the ensemble states and a is the matrix of the same size as a and filled with the mean value computed from all ensemble members the ensemble error covariance matrix p e s can be computed as follows 4 p e s a a a a t n 1 similarly r e is computed as 5 r e s d d d d t n 1 where d stores the perturbed observation and d is the ensemble mean the da estimate is the ensemble mean of x s s i 3 3 design of the da schemes the different da schemes are developed to incorporate observations with different spatial temporal resolutions and error characteristics into the da system simultaneously three different da schemes are considered here fig 2 sm da only soil moisture is assimilated grace da only grace is assimilated and multivariate da both soil moisture and grace are assimilated as described in section 3 2 the state vector contains daily volumetric soil moisture of six different layers and groundwater storage components for a particular model grid cell j on a given day t the state vector can be defined as θ 1 j t θ 2 j t θ 3 j t θ 4 j t θ 5 j t θ 6 j t g w s j t t where θ is the volumetric soil moisture m3 m3 and gws is the groundwater storage m the state variables are obtained from the results of model propagation in the sm da fig 2a the soil moisture observations are assimilated every l 3 days on the model grid cell individually only smos data is used between january 2010 and february 2015 and the dimension of the state vector is mlx1 where m 7 is the number of the state variables the 3 day window allows the soil moisture observations to have full coverage over the goulburn catchment and yields the adequate ensemble spread between the updates the observation vector d contains the smos data with dimension lx1 the h s matrix is defined as 6 h s h s m j t 1 0 0 0 h s m j t 2 0 0 0 h s m j t 3 7 h s m j t s 1 s 2 0 0 0 0 0 where s 1 s 2 are the thickness of the first and second soil layers respectively the soil thickness is described in section 2 2 the h s matrix dimension lxml relates the smos observation to the top two soil layers bias correction is performed prior to the application of da to reduce the systematic error between the model estimated and the satellite retrieved soil moisture see section 3 3 when smap data are available e g from march 2015 the smos and smap data are assimilated into the lsm simultaneously lievens et al 2017 demonstrated that the joint sm da performed better than a single sm da case in the case of smos smap assimilation the dimension of h s and d are extended to 2lxml and 2lx1 respectively to include the measurement operator associated with the smap data in this study the errors in smos and smap data are assumed to be uncorrelated in the grace da fig 2b the model states are updated at a monthly time scale consistent with the grace temporal resolution the model state vector contains all model grid cells inside the blue polygon in fig 1 of daily state variables within approximately one month the state vector is also constructed from the results of model propagation the length of the vector is jlm where j is the number of grid cells in the study area and l 1 month the monthly time window used for each update is based on the time tag of the grace product as the monthly window used to produce a grace solution is not necessarily a calendar month l is different in each update and varies between 13 and 31 days following grace data used the observation vector y s is a 1 1 vector containing the monthly average values of the catchment mean tws the matrix h s is used to convert the volumetric soil moisture and groundwater storage into the catchment averaged tws of the month 8 h s h g t 1 h g t 2 h g t l 9 h g g j 1 g j 2 g j j 10 g j s 1 s 2 s 3 s 4 s 5 s 6 1 j l where s 1 s 6 are the thickness of each soil layer see section 2 2 in the multivariate da fig 2c the sm da and grace da schemes are combined the sm da is firstly performed step 1 in fig 2c and its updated state variables are used as the forecast state in the grace da step 2 it should be noted that unlike the 3d enkf reichle and koster 2003 satellite soil moisture observations are only used for correcting collocated soil moisture estimates however a recent study demonstrates that remote sensing observation error is highly structured in space suggesting a spatial correlation of soil moisture retrieval errors dong et al 2017 this complicates the accurate parameterization of the observation error matrix in a 3d updating da scheme hence the soil moisture retrievals are not used for correcting nearby grid cells 3 4 evaluation metrics the volumetric soil moisture estimates are validated with the in situ soil moisture and groundwater data in terms of temporal correlation ρ and unbiased root mean square difference ubrmsd entekhabi et al 2010 11 ρ x sim e x sim x obs e x obs x sim e x sim 2 x obs e x obs 2 12 u b r m s d e x sim e x sim x obs e x obs 2 where x sim and x obs are state vectors from simulation model estimate and observation e g satellite product in situ data respectively and e is the expectation operator all in situ soil moisture and groundwater data inside the same model grid cell fig 1 are averaged before the comparison this produces four grid cells of in situ soil moisture s1 s4 and four of in situ groundwater data g1 g4 note that only the temporal correlation between h and gws is used to evaluate the groundwater storage estimate against groundwater level due to the absence of accurate information on specific yield 4 results and discussion 4 1 impact of da on soil moisture estimate the top soil moisture θ0 5cm is estimated from the enol and three da scenarios sm only grace only and both the goodness of fit in terms of correlation is evaluated against the smos data fig 3 top row to investigate the impact of different da scenarios on the θ0 5cm estimates from fig 3 the sm da and the multivariate da deliver 0 1 0 15 higher averaged correlation values compared to the enol this is expected as the smos smap data are being integrated into the state estimate particularly into the θ0 5cm component by the applications of the sm da and multivariate da the kalman gain attempts to statistically optimize the fit between the θ0 5cm estimate and the smos smap observation resulting in an improved agreement between them similar behavior is also observed from the evaluation with the smap data not shown including the smos smap data in the assimilation system is proven necessary to improve the θ0 5cm estimate by contrast grace da reduces the correlation value by 0 1 the degradation is likely caused by the limited sensitivity of grace observations to top soil moisture the top soil component is strongly governed by high frequency meteorological forcing wu et al 2002 while grace can only observe monthly catchment averaged tws changes which is dominated by the low frequency variability of deep water storage components also the degradation of surface sm after assimilating grace suggests an inconsistency between the observed and modeled sm tws relationship as shown in fig 4 the modeled tws change is less sensitive to the modeled sm change compared to the corresponding observations therefore correcting the modeled tws to grace may over correct sm estimates and lead to degraded results clearly assimilating grace data alone cannot provide the high spatiotemporal variability essential for modeling the water storage in the top soil layer and the inclusion of grace data tends to have a negative impact on the θ0 5cm estimate all da cases reduce the uncertainty ensemble spread of the θ0 5cm estimate fig 3 bottom row compared to the enol the sm da and multivariate da reduce the uncertainty by a factor of three while the grace da reduces the uncertainty by a factor of 1 2 importantly the applications of the sm da and multivariate da also lead to an approximately three times lower uncertainty than the assigned smos smap uncertainty value in addition it is seen that the uncertainty of the θ0 5cm estimate is lower in the south eastern part of the catchment this is likely influenced by the lower field capacity associated with lower clay content in the southern region leading to a small variation of θ0 5cm and its uncertainty the spatial pattern of the uncertainty also explains the contribution of smos smap observation the update is likely limited in the south eastern part where the model uncertainty is small this is apparent in e g fig 3b where slightly lower correlation values are observed mostly in the south eastern region 4 2 impact of da on tws estimate the basin averaged tws of all three da cases is shown in fig 5 also the correlation with respect to grace is shown in fig 6 top row assimilating smos smap only yields a negative impact on the tws estimates resulting in a decreased agreement between the state estimate and the grace observation in the sm da the smoother underestimates the annual and inter annual variability of tws and reduces the averaged correlation value by 0 2 fig 6b the smoothers estimate a set of the ensemble by optimizing the kalman gain or likelihood function associated only with the θ0 5cm component while leaving the other storage components unconstrained computing the posterior estimate based on the resulted sample set produces an improved θ0 5cm estimate see also section 4 1 but does not necessarily improve the computation of total storage changes the degradation in tws may be due to the fact that the satellite sm observation does not provide information on the total column water which is crucial in the accurate distribution of the water through all stores in the grace da the constraint is applied to the entire water column leading to an improved agreement between the tws estimate and the grace observation the averaged correlation value is increased by 0 2 fig 6c the impact of the grace da is clearly seen in the tws adjustment before and after march 2012 to evaluate this the total mass variation in the two periods january 2010 march 2012 and april 2012 december 2015 is computed and shown in table 2 to determine the total mass of tws variation gton in each period the long term trend m year is first estimated and multiplied by the area of the goulburn catchment see section 2 1 the density of water and the number of years in that period respectively grace observes the increased mass estimate of 0 6 gton prior to april 2012 which is mainly induced by the 2010 2011 la niña rainfall see fig 5 the enol underestimates the mass estimate by 0 1 gton during this period the estimate is improved by the grace da leading to a 20 improvement in cross correlation between the adjusted mass estimate and grace data similar behavior is observed during the post la niña period after march 2012 when the grace da produces a 30 improvement in cross correlation unlike the grace da the sm da cannot improve the mass estimate in both periods due to e g the deficiency of deep water storage information necessary for the tws computation it is apparent that sm da and grace da are valuable for updating θ0 5cm and tws estimates respectively while they show limited benefit for the estimation of the other components the underlying strengths motivate the concept of assimilating the smos smap and grace observation simultaneously into the lsm in the multivariate da the θ0 5cm and tws components are adjusted toward the smos smap and grace observation respectively resulting in the final state estimates that agree with both observations the tws estimated with multivariate da agrees better with the grace observations by 0 12 in cross correlation fig 6d and simultaneously the θ0 5cm estimate presenting better correlation by 0 1 with smos smap data see fig 3b consequently the multivariate da improves the mass estimate during the la niña period table 2 the grace da and multivariate da reduce the tws uncertainty by more than a factor of 2 fig 6 bottom row as expected the sm da cannot deliver a reliable tws estimate as seen in the uncertainty which is approximately twice that obtained from the grace da and multivariate da 4 3 validation with in situ data 4 3 1 soil moisture the θ0 5cm variations estimated from all da case studies are validated against the in situ data at s1 s4 fig 7 the validation is conducted in terms of correlation and ubrmsd and the estimated values are shown in fig 8 cable performs remarkably well in the estimation of θ0 5cm and provides a good agreement with the in situ data at all locations with an averaged correlation value of 0 69 see enol in fig 8a the sm da and multivariate da increase the correlation value further by 7 from 0 69 to 0 73 and decrease the ubrmsd by 11 the improved result is anticipated since the satellite sm observation is used in the sm da and multivariate da by contrast the grace da shows an apparent negative impact on the θ0 5cm estimate see fig 8a b comparing to the enol the grace da overestimates θ0 5cm by a factor of 1 5 ubrmsd and decreases the correlation by 50 poor performance is due to the insensitivity of grace data to the signal associated with the top soil component as described in sections 4 1 and 4 2 the θ0 30cm variation is also validated against the in situ data with the statistical results shown in fig 8 bottom row cable provides a very accurate θ0 30cm component with a correlation value of almost 0 7 fig 8c unlike the θ0 5cm the sm da and multivariate da do not improve the correlation and ubrmsd values of the θ0 30cm estimate this is consistent previous studies that found that the benefit of surface sm da in root zone sm estimates depends on the accuracy of model physics dunne et al 2007 kumar et al 2009 in line with the analysis found in fig 4 grace da also reduces the quality of the θ0 30cm estimate seen from both metrics the benefit of including the smap data in the da system is evaluated the multivariate da results from two case studies using smap data between march and december 2015 are compared with the in situ data at s1 s2 and s4 fig 9 a c the in situ data at s3 are not available during this validation period in all locations the daily θ0 5cm estimates of the smos only assimilation and the smos smap assimilation are very similar and visibly show a better agreement with the in situ data comparing to the enol the correlation value is increased to almost 0 2 e g at s1 fig 9d and the highest correlation value is seen when the smap data is included in the da system 3 higher compared to the smos only assimilation the application of the smos smap assimilation also reduces the spurious peaks of the θ0 5cm estimate e g in october 2015 fig 9a b and november 2015 fig 9c leading to a better agreement with the in situ data evidently the smap data should be considered in the da process to maintain the accuracy in terms of agreement with the in situ data of the θ0 5cm estimate in the goulburn catchment 4 3 2 groundwater storage the gws estimates are compared with the in situ groundwater level anomalies h at g1 g4 fig 10 and the averaged correlation coefficients are shown in fig 11 in fig 10 the application of the sm da leads to an incorrect groundwater storage estimate with a large disagreement between the gws estimate and h particularly at g1 where the correlation value is as low as 0 6 the poor performance can be attributed to the lack of groundwater information in the satellite sm observation see sections 4 1 and 4 2 the h shows a very similar temporal variation in all g1 g4 locations the different scale between δgws and δh likely causes the visual phase shift seen in fig 10 applying a specific yield e g ranging between 0 and 1 to δh could reduce the magnitude of the right axis and led to the reduction of visual phase shift however the conversion is not performed due to the absence of specific yield as described in section 2 5 the temporal variations of h follow those of the tws estimate and the grace observations see fig 5 h and tws increases under the influence of the la niña rainfall in 2011 2012 and decreases afterward the similarity suggests that grace is sensitive to the signal of the groundwater store more than the shallow storage component in particular the groundwater level data h are correlated throughout the catchment with the cross correlation of 0 9 see fig 6 in tangdamrongsub et al 2017a the assimilation of grace data in both grace da and multivariate da increases the correlation between the gws estimate and h changes in each grid by a factor of 2 compared to the enol estimate the enol simulated gws shows smaller variations compared to the da estimate and h cable models the unconfined aquifer using a simple groundwater model decker 2015 decker and zeng 2009 niu et al 2007 vergnes et al 2012 that calculates the groundwater recharge based on the available water after vertical redistribution between the soil layers this simplification might lead to an enclosed groundwater component in the deep soil layer when the distributing water does not reach the defined field capacity in such a case groundwater recharge is not accounted for correctly and the groundwater storage changes become small the soil and groundwater components are not efficiently separated and the variation of the gws estimate is likely presented in the deep soil layer assimilating grace only always shows a better performance in the gws estimate and provides 29 higher average correlation compared to assimilating both grace and smos smap measurements in the multivariate da gws is updated by the grace da step 2 in fig 2c after the application of the sm da step 1 in fig 2c the application of the sm da in the multivariate da likely decreases the uncertainty of the state estimate which consequently reduces the contribution of grace in the analysis step of the grace da rescaling the grace uncertainty could increase the contribution of the grace observation e g tian et al 2017 5 conclusions this study evaluates three different da schemes to assimilate different combinations of satellite observations smos smap grace and both smos smap and grace in the goulburn catchment australia validation against the in situ data reveals that the performance of the da in estimating soil moisture and groundwater storage highly depends on the choice of the observation type the application of the sm da significantly improves the top 0 5 cm soil moisture but degrades the groundwater component whereas the grace da improves only the gws estimate applying the multivariate da simultaneously increases the accuracy of the soil moisture and groundwater storage estimates though at a slightly lesser degree of improvement compared to the single observation da case the application of the sm da underlines the importance of the smos smap data on the sm estimate by increasing the 0 5 cm correlation with in situ observations by up to 7 the benefit on the 0 30 cm soil moisture and groundwater component is minor or negative which is in line with several previous studies for example blankenship et al 2016 kolassa et al 2017 ridler et al 2014 and tian et al 2017 who reported a detrimental impact on the root zone and deep storage components sm da significantly reduces the uncertainty of storage in the top 0 5 cm soil layer but does not have an impact on the tws uncertainty the constraint solely in the top soil moisture component by the sm da does not necessarily have a positive effect on the entire water column we also found that assimilating both smos and smap data simultaneously is recommended in the goulburn catchment the advantage of multivariate sm da is also found in lievens et al 2017 kumar et al 2018 jasinski et al 2019 however it should be noted that smos and smap soil moisture may have potentially common systematic errors which may affect the observation error matrix future studies should explore the magnitude of smos smap error cross correlation and its impact on the da results the grace da demonstrates an outstanding example of improving the groundwater storage of the goulburn catchment particularly at a finer spatial resolution 25 km compared to grace s intrinsic resolution 100 km as the groundwater variation of the goulburn catchment is likely to be spatially correlated due to the large unconfined aquifer tangdamrongsub et al 2017a assimilating a coarser spatial scale tws from the grace observation can benefit the groundwater estimate even in the smaller individual grid cell grace da leads to the improved groundwater estimate by increasing the correlation to independent in situ groundwater level data however assimilating grace into lsm does not provide a positive impact on the top or surface sm components this is consistent with the conclusions of li et al 2012 and tian et al 2017 grace da significantly reduces the uncertainty of the tws estimate but has only a minor impact on the sm uncertainty it is known that grace is sensitive to the signal of the entire water column dominated by the processes in deeper layers the grace da might therefore adversely distribute the deep water storage signals into the shallow one multivariate da provides an improvement over both sm and gws estimates assimilating the satellite soil moisture and grace data together allows the high frequency components to be adjusted by the sm da while the low frequency signal is corrected by the grace da leading to the increased correlation values of both the 0 5 cm soil moisture by 7 and gws estimates by 65 compared to the independent in situ data however the multivariate da does not outperform the sm da or the grace da in the separate estimation of the 0 5 cm soil moisture and gws the da approach optimized the model states with multiple cost functions relevant to shallow and deep groundwater storage changes e g minimizing the residuals against both smos smap and grace resulting in an optimal solution that is not closer to one particular observation as also found by tian et al 2017 with the increased availability of satellite retrievals and ground measurement networks multivariate da can be an effective tool to exploit diverse observations the multivariate da presented in this study can be extended to include different types of new observations e g soil moisture from sentinel 1 lievens et al 2017 tws from grace follow on flechtner et al 2014 snow water equivalent from snowex kim 2017 with simple modification of the measurement operator as described in section 3 2 ongoing research is focused on the sensitivity to the selected window length l of the smoother dong et al 2015 and applications over regions with different climate conditions e g snow covered basins credit authorship contribution statement natthachet tangdamrongsub conceptualization methodology software validation writing original draft shin chan han supervision writing review editing in young yeo resources writing review editing jianzhi dong methodology writing review editing susan c steele dunne methodology writing review editing garry willgoose resources jeffrey p walker writing review editing decleration of competing interest none acknowledgment this work was funded by the university of newcastle to support nasa s grace and grace follow on projects as an international science team member and by the australian research council discovery project dp170102373 natthachet tangdamrongsub was supported by the nasa earth science division in support of the national climate assessment we thank awr s associated editor and three anonymous reviewers who provided insightful and constructive comments leading to a significant improvement of the paper data used in this study are publicly available with the access information provided in section 2 
