index,text
25935,out of many approaches to landscape connectivity we choose patch mosaic flow neutral framework for modelling our new software system graphscape transforms a vector mosaic map into landscape graph finds shortest paths and combines them into minimum spanning tree the result is an optimal network of connections for a given set of patches to capture mobile agent properties we use two types of resistance assigned to patches and transfer resistance assigned to combinations of patch classes another alternative is step model when transfers occur as if on unweighted graph resulting mst is rendered by linear skeleton binary transit density measure assignable to patches continuous and summary statistics a novelty is compete workflow from vector based map to complex graph structures we describe concepts and operation of software graphscape can be used for ecological corridors planning identification of key areas of corridors overlap identification of the role of particular mobility factors assessment of patch isolation keywords landscape graph minimum spanning tree landscape metrics least cost distance patch corridor matrix model patch mosaic model 1 introduction landscape connectivity in the broad meaning of ability of the landscape to impede or to facilitate any kind of movement of any kind of living organisms among resource patches taylor et al 1993 is recognized as a key concept for understanding modelling management and protection of individual species as well as local and regional biodiversity as a whole it is important to emphasize that landscape connectivity depends on two groups of variables the first group includes the composition and configuration of the landscape i e the occurrence spatial distribution and boundary relations of all the ecosystem patches natural semi natural and anthropogenic this is a property that characterizes the landscape and is independent of the process under study the second group of factors influencing connectivity is related to the examined species or process and depends on the habitat requirements of species and the ability to move across patches ecosystems of different type ament et al 2014 in other words landscape connectivity is a function of a landscape permeability 1 1 permeability sensu singleton et al 2001 2002 refers to the degree to which whole landscapes encompassing a variety of natural semi natural and developed land cover types are compatible with wildlife needs and sustain ecological processes in contrast to landscape connectivity which characterizes the capacity of individual species to move between areas of habitat via corridors and linkage zones for any kind of animal movement e g daily movements of directional incidental and random character within the landscape seasonal migrations between different landscapes and regions dispersal from source ecosystems resulting in the growth of the species territorial range as well as in the interchange of individuals between local populations forming a single metapopulation ament et al 2014 the concept of landscape connectivity is also applied to other ecosystem processes such as active and passive plant species dispersal and genetic exchange colonization of unoccupied habitats disease spread as well as water energy and material transportation within and between ecosystems one of most important issues studied under the concept of connectivity is the modelling of linkages between focal habitats brás et al 2013 i e patches of ecosystems particularly suitable for a given species or focal areas e g nature protection areas fenu and pau 2018 there is no single generally accepted approach for modelling of such linkages because the result depends on the general landscape model adopted e g patch mosaic patch corridor matrix regular grid structural and functional properties assigned to patches as well as on route types and connectivity measures based on different theoretical foundations fortin et al 2012 ziółkowska et al 2014 according to van etten 2017 we may distinguish three main types of routes and as a consequence three types of distance measures in heterogeneous geographic spaces a the shortest line between two points as the crow flies b the least cost distance taking into account the obstacles and the local friction of the landscape as the wolf runs c the random walk with route finding as a stochastic process drunkard s walk terms of van etten 2017 regardless of the type of distance measure adopted three types of connectivity representations binary categorical and continuous may be considered ziółkowska et al 2014 according to calabrese and fagan 2004 all connectivity measures fall into three categories a structural metrics based solely on landscapes composition and configuration b potential connectivity metrics based on the landscape structure as well as information about the study organism s dispersal movement ability and habitat preferences c metrics of realized functional connectivity based on records of the real movements of individuals across the landscape all the approaches outlined above have been used in various ways in methods and algorithms for modelling connectivity these were based on different methodological background e g spatial statistics spatial regression and graph theory fortin et al 2012 the most promising and most widespread approach nowadays is based on graph theory nowadays according to galpern et al 2011 in most common applications patches of habitat are defined and distinguished from the matrix separately for each focal species and serve as the nodes vertices the connections among nodes called links linkages edges represent generally the distance between nodes since the beginning of the 21st century many systems have been developed to assess the degree of connectivity in the landscape table 1 most of the software was reviewed in papers by urban et al 2009 laita et al 2011 galpern et al 2011 and fortin et al 2012 they identified typical research questions and their associated graph construction and analysis methods with special emphasis on the conceptual model underlying these applications model parameterization and predictions available through graph analyses from our perspective all existing software can be combined into a few groups the first one includes solutions based on flows in pre determined connection networks regardless whether these are especially dedicated to landscape issues such as openfluid or ucinet or to general graph analysis such as pajek all other software is typically based on metapopulation models or landscape island models where the patches are islands of suitable habitat embedded in an inhospitable matrix nonhabitat the second group patch matrix model with dispersion rate introduces additional parameters for dispersion capacity of certain species e g conefor sensinode the third group cost distance raster models consists of software that handles only the suitability of matrix in each raster cell for a given species at last in the fourth group circuitscape software a least cost path is replaced by more general least cost flow moving across landscape circuitry some of the applications deliver a unique or a small set of best linkage s i e the areas promoting connectivity between habitats while others produce a quantitative evaluation of how suitable or probable each area is for the established connectivity goals brás et al 2013 however in the light of the above review it is worth recalling that one of the first papers to analyze the usefulness of the graph theory in landscape ecological studies cantwell and forman 1993 was focused on evaluating landscape configuration with less attention devoted to evaluating landscape connectivity the authors pointed out that nodes should represent landscape elements and linkages should represent common boundaries between elements the same theoretical principles were employed by ricotta et al 2000 who pointed out that a landscape graph may be constructed from any thematic map at any scale of observation ricotta et al 2000 also underlined that graph theoretic topological indices mainly the harary index may be used as measures of landscape connectivity urban et al 2009 mentioned the possibility to define a graph in which nodes represent habitat patches that tile the landscape and the links represent the likelihood that individuals moving within the patch might encounter the boundary to an adjacent patch and pass through lately fenu and pau 2018 proposed a very interesting approach in which suitable areas for ecological corridors were selected from a grid based resistance map where land cells were associated with a cost weighted distance cwd value but the analysis of the character of these corridor was conducted on the spatial graph model derived from a land cover vector map despite individual studies exploiting the vector representation of the landscape as a mosaic of patches this direction of research on landscape connectivity has never been popular especially when compared to the number of publications based on the raster approach and metapopulation theory island biogeography or patch corridor matrix model however among all the variety of systems used to assess landscape connectivity we miss the software referring to the classical landscape model as a mosaic of ecosystems patches with well defined borders represented on vector maps the main purpose of this article is to present a new software graphscape which enables analysis of landscape connectivity using the vector based patch mosaic landscape model together with its methodological assumptions and potential applications the proposed new software is justified for several reasons first of all many studies using raster data and graph approach move away from pixel based analyses towards patch based analyses e g hersperger 2006 de cola 2010 comber et al 2012 fenu and pau 2018 while there is a lack of publicly available software to determine landscape connectivity for such distinguished patch mosaic landscape secondly vector representation of polygons patches carries the advantages of better map precision being able to work with boundaries of differentiated shapes and avoiding the problem of a cell pixel size choice fenu and pau 2018 thirdly in the case of raster and vector representation of the same patch mosaic planar graphs should be identical fourthly the vector representation of patch mosaic makes it easier to take into account the role and nature of boundaries between adjacent patches 2 conceptual background 2 1 ecological aspect the starting point to our work was to view the landscape as a mosaic of patches ecosystems stored and processed in vector form this is in line with the postulates of cantwell and forman 1993 as it takes into account a the composition and spatial configuration of patches as basic elements of the landscape b is focused on interactions interdependencies and fluxes between adjacent elements and c allows to model corridors and evaluate connectivity in different spatial scales similar assumptions were made by vuilleumier and metzger 2006 for animal dispersal modelling we suggest that this approach is devoid of those limitations that may exist when working with the grid based landscape models which include the following i the existence of an a priori fixed scale of resolution ii the need to aggregate usually by averaging attributes of cells at the pre defined scale iii problems in representing linear objects and topology shape and relationships between distant objects vuilleumier and metzger 2006 laurini and thompson 1992 and iv poorly defined and difficult to operationalize concept of a border dunn 2010 also added that several pixel based connectivity metrics rely on algorithms that are affected by a regular grid induced bias we find the patch mosaic model uniquely suitable to parameterize the size and shape of the patches and the structure of the borders as factors affecting the network of connections within the landscape as animals and all other agents move within and between patches of different ecosystems we assume that the usability and permeability of patches as well as permeability of patch borders and contrast between adjacent patches determine the structural and potential landscape connectivity estreguil et al 2014 considering different empirical observations referred in the literature kindvall and petersson 2000 concluded that animal movement mainly migration dependence on patch geometry is probably a significant phenomenon in most patchy systems this dependence is a result of at least two animal species characteristics perceptual range and dispersal distance the perceptual range is the distance from which a particular landscape element i e a habitat patch can be perceived by a given animal kozakiewicz and szacki 1995 vuilleumier and metzger 2006 additionally patch edges are interfaces between two types of habitat edge effects may be positive or negative features for patch interior quality and for movement across a landscape baker 1996 ries and sisk 2004 ries et al 2004 estreguil et al 2014 according to these ecological concepts we build the mobility model around three characteristics of patches a usability and permeability which is assigned to the patch type and depends on the species or other mobile agent analyzed b individual resistance which is assigned separately to each patch and depends on intrinsic characteristics like size or shape not influenced by the species concerned c transfer resistance describing contrast between adjacent patches and depending solely on landscape configuration and ecosystem types the above input parameters either based on an expert estimation or measured on a map or in the field are the indirect indicators of species preferences and landscape constraints and make the modelling of landscape connectivity structural and partly potential according to terminology of calabrese and fagan 2004 more realistic 2 2 graph theory aspect topological structure of the landscape graph provides a framework for movement over which actual movement can happen this is not unlike a transportation network which provides possibilities for potential travelers the keyword here is potential because infrastructure layout is quite different matter from how the network is actually utilized over the same transportation network flow intensity and distribution varies widely within daily or weekly timeframe it is inspiring to see that an extensive research goes into modelling of transportation accessibility see the review of geurs and ritsema van eck 2001 where network potential is successfully analyzed without reference to flows we believe similar approach is possible in ecological modelling flows or fluxes in ecological parlance cannot be modelled without population dynamics trophic and competition factors with many analogies to transportation demand and supply this puts high demand on empirical data that often cannot be satisfied bergsten and zetterberg 2013 we propose more modest solution flow neutral model focused on possible rather than actual movement and on directions rather than magnitudes next we think that distance and other geometry concepts should be put into proper perspective when combined with graph theory first we have to point out that early work on landscape connectivity e g taylor et al 1993 urban and keitt 2001 established an overwhelming position of physical distance as a determinant of mobility it was the result of adopting patch corridor matrix approach and inevitable simplification of what is happening in inhospitable matrix zone in our opinion matrix deserves better recognition and there is more data than physical distance to be used in search of mobility patterns galpern et al 2011 place distance as a representation of graph edges on equal standing with dispersal and flux however these measures do not belong to the same logical category dispersal and flux are the effects directly observable on graph edges while distance is in more or less convoluted way one of determinants of these effects mixing graph theoretic and geometric terms leads to confusing concept of geometry of nodes and ramblings about node location within a patch fall et al 2007 go as far as to distinguish special class of spatial graphs with unique properties and thus propose geometric extensions to graph theory we think that spatial attribute describes the context of use much like dale and fortin 2010 rather than distinct mathematical entity conventional graph theory is rich enough for landscape studies and is reliable in terms of proofs and trusted algorithms therefore it should not be wrenched and extended beyond limits this is not to say that geographic thinking should be avoided we only advocate clear separation of concerns so that geometric terms and techniques are not used after crossing the boundary of graph domain after graph theory textbooks we use an abstract term weight as the final attribute of the graph edge when referring to algorithms and mechanics of the graph to cover the intricacies of weight construction before it is passed to graph algorithms we use the term weight function we place these concepts in a center of our method and by design channel to them as many factors as possible including distance mobility characteristics and terrain properties our approach avoids problematic issues related to graph assemblage and specifically edge construction we find distance thresholding as a method of edge creation e g urban and keitt 2001 fall et al 2007 baranyi et al 2011 too arbitrary and conflicting with graph theory requirements namely the freedom of choice given to optimization algorithms instead we propose edge creation either 1 based on fundamental and more sharply formulated criterion or 2 entirely unconstrained in our model we employ both approaches at the first level the criterion for edges is patch mosaic topology and at the second level fully connected graph is used 3 processing flow the current state of art and shortcomings described above led us to the other attitude more rooted in traditional graph theory we want to extend patch corridor matrix approach to make it more general and capable of handling landscape data in a uniform way in this section we outline a process of extraction of landscape structure useful for connectivity research and corridor design we describe a computational workflow leading from a vector based landscape patch mosaic to a minimum spanning tree an optimal connectivity structure according to graph theory with the mobility model as the key element incorporating ecological component we explain mathematical foundations data structures and algorithms used in this multi step approach 3 1 building topology an input data to our workflow is vector based map of patches areal units homogenous under study assumptions covering a continuous extent of earth surface which we call patch mosaic see fig 1 patch characteristics numeric and non numeric data are also provided but at this stage they are only preserved for further processing no patches are distinguished based on their attribute data especially habitat and matrix patches are treated equally local discontinuities are accepted unless they break the mosaic into separate disjoint parts we assume the input is devoid of topologic structure like esri shapefile format which stores geometric surfaces as self contained entities without topologic relations in case of continuous coverage borders of adjacent patches are then effectively duplicated the purpose of this step is to discover topologic relations and transform the mosaic into isomorphic structure of a landscape graph the landscape graph g v e made of a node set v and an edge set e is defined in such a way that 1 v 1 v 2 v n v p 1 p 2 p n p there is a one to one mapping of node set v onto patch set p 2 e e v a v b a d j p a p b t r u e an edge exists between nodes v a and v b if adjacency relation holds true for a pair of patches p a p b after having nodes 1 and edges 2 defined we proceed to adjacency relation 3 p a p b p p a d j p a p b t r u e i f s h a r e d b o r d e r p a p b f a l s e i f s h a r e d b o r d e r p a p b axiom 3 states that adjacency must be valid for every pair of patches and delegates the implementation to shared border function strictly graph definition ends at adj relation but we define additional function because it provides data for a special variant of the model described later shared border is a function those arguments are two patches p a and p b and the result is a set of multiline geometric objects or empty set an implementation runs along slightly different logic than axiomatic approach given above adjacency is not tested for each pair of patches as suggested by 3 rather shared border detection algorithm is run for the entire coverage an algorithm starts with matching all outline points against each other matched points are coalesced combined into one object with information on patch origin preserved next coalesced points are traced into strings representing single e g border a b on fig 2 or multiple border c d line segments with left and right patch attributes degenerated lines consisting of a single point are rejected result is stored in memory list sb now the algorithm proceeds to building an adjacency matrix am a binary symmetric matrix of p p size only below diagonal elements are physically stored am is initially filled with 0 s the algorithm performs full scan of sb list and for each sb i element finds corresponding am cell at position given by sb i left sb i right pair and sets its contents to 1 recurring assignments to the same cell do not change the result cells not identified by any sb element remain 0 we arrive at a basic graph representing topologic backbone of the landscape fig 3 it is unweighted because it has yet no information attached to edges and undirected because edges have no orientation towards either endpoint in subsequent processing the number of edges is never increased so this structure defines an upper limit of connectivity the analogy to the transportation infrastructure is clear it is important to emphasize that nodes obtained this way cannot be attributed geometric properties of any kind and any location in geographical space our software generates geometric points for visualization purposes geographic center of mass for concave polygons additionally corrected to remain within patch outline but their location is purely conventional and we do not derive any graph properties from them the same concerns edges which cannot be interpreted as the ground lines connecting locations 3 2 mobility model the next step of processing called mobility model is a parametric method of describing a mobile agent an entity performing a movement in the landscape living individual a group a species a substance etc we assume that the movement is controlled exclusively by graph topology and three types of properties assigned to graph nodes and edges called mobility factors distance relations receive no special treatment and if considered important must be included as one of these factors mobility model transforms initial unweighted graph which accounts for topology only into a weighted graph which accounts also for mobile agent s behavior a weight is a numeric value assigned to each edge and its magnitude reflects generalized cost or travel impedance between two nodes depending on the problem it may represent time energetic expenditure probability frequency or rate of transfer failure or inverse of transfer success etc the internal representation of the graph while being processed by mobility model is the travel matrix tm a floating point symmetric matrix of size n n nodes see fig 4 compared to binary am tm will be composed of numeric values with inverse interpretation low values will represent quick travel high values will represent slow travel infinity is assigned to no link cells it must be reminded that tm will refer only to direct connections between patches long range movement will be introduced later with paths 3 2 1 mobility factors the calculation of travel matrix is based on three components and corresponding three data sets patch class resistance patch individual resistance transfer resistance first two factors are the properties of patches third one defines the relationship between patches patch resistance is resistance to travel or alternatively a generalized cost of travel through a graph node as graph node represents a patch with non zero surface this surface can incur a resistance to mobile agent as a matter of convenience this influence has been split into two factors with patch class resistance pcr the resistance is assigned to whole classes of patches for instance land use classification type at once with patch individual resistance pir it is assigned to each patch individually for both resistances high values increase and low values decrease the weight assigned to transfer matrix cell apart from assignment method both factors share the same interpretation and performance in calculations class resistance is better suited to capture inherent behavior of mobile agent in response to certain terrain characteristics so it is appropriate when this behavior is consistent within entire classes of patches individual resistance provides a way of incorporating unique properties of patches which are difficult to generalize and fit into classification scheme it is the most suitable way to introduce patch size or geometric shape measures into the modelling process pcr table is supplied as an external tab delimited text file fig 5 pir is retrieved from one of the shapefile columns transfer resistance tr describes resistance to movement from patch to patch in mathematical terms it defines a relation between classes of patches accordingly this factor can be only assigned class wide like pcr here again high values increase and low values decrease the weight due to symmetric travel matrix design transfer resistance table needs only above diagonal elements fig 6 there is no predefined range of values for mobility factors except that negative numbers are converted to infinite resistance data is accepted as it is without any scaling we must emphasize that three mobility factors only define the maximum of options and not necessarily the best or obligatory program the model can run with any combination of factors 3 2 2 edge weight formula three mobility factors pcr pir and tr are combined into single weight value for travel matrix cell with the following formula 4 w i j p c r i p c r j 2 p i r i p i r j 2 t r i j with i standing for origin and j standing for destination patch we remind that the formula is valid only for adjacent patches in plain words the logic of computation is as follows 1 final weight is a product of combined patch resistance and transfer resistance 2 patch resistance is a sum of pcr and pir 3 patch resistance is relocated from nodes to edges the relation between patch resistance and transfer resistance is multiplicative so these two types contribute equally to the final result the relation between two kinds of patch resistance is additive so when both kinds are used they may together outweigh transfer resistance therefore an attention must be paid to proper scaling the most important part of 4 is rewriting of nodes to edges we do it to comply with standard graph algorithms which expect to find the cost of traversal only on edges edge value is derived from two neighboring nodes in equal parts half of origin patch resistance and half of destination patch resistance contribute to total edge resistance this simplistic rule is driven by important restriction no part of edge weight must be lost or added when combining paths for instance on fig 7 a total path weight of a c must be equal to the sum of a b and b c if this condition is not met graph algorithms cannot fairly compare alternative paths or trees and arrive at wrong optimization results 3 2 3 step model with border factor in some cases we may want to route movement without mobility factors this may be the case when mobility factors are unknown or we consider many contributing factors cancel each other in step model weight function is simplified so that it returns the same value for every transfer between adjacent patches or crossing the border subsequent path calculations will then return the count of steps between origin and destination in effect step model parallels ecological models based on unweighted graphs and may be useful for comparisons however this simple scheme must be improved to handle two fundamental drawbacks of routing on unweighted graph first the solution to the shortest path problem is not unique there can be many paths of equal length between two nodes with majority of alternative paths being the trivial variants of the base one the number of possibilities increases with the distance second minimum spanning tree solution may be not unique by the same token commonly used algorithms both sp and mst are not able to return the full set of equivalent solutions and return more or less random pick this effect is even more pronounced when algorithms are used in a sequence as in our modelling setup trying to solve the problem we rejected an option of modifying the algorithms to return multiple results in this case as many as sp set mst set as non standard and of little practical value we decided to force uniqueness by additional non topological criterion which is the geometric length of shared borders between patches the conjecture is that interaction intensity or opportunity for transfer between patches increases when they share a long border we included shared border information in weight function effectively tricking existing algorithms into bi criteria optimization our goal was not to mix two criteria evenly but to add supplementary criterion so that out of two solutions with the same number of steps the one with longer shared borders is selected a starting point was an unweighted formula w i j 1 we modified it to 5 w i j 1 b i j where b i j is a border factor between adjacent origin i and destination j patches defined as 6 b i j l i j l in 6 l i j is a length of a shared border between i th and j th patch and σl is the total length of shared borders in the coverage border factor causes weight of each edge to be lower than 1 0 by a small fraction the longer shared border the smaller weight is generated and easier the movement to assure that algorithms work correctly with the corrected weight function it is necessary to prove that secondary criterion does not interfere with the primary one so that border factor does not outweigh the number of steps for any resulting structure this is guaranteed by the way border factor is relativized to the total length of borders in equation 6 the sum of b never exceeds 1 0 for any path or spanning tree because there exists no path or spanning tree consisting of more than all edges of the graph 3 2 4 travel matrix assignment having weight function defined by either 4 or 5 we proceed to final calculation of travel matrix first we perform scalar inversion not to be confused with matrix inversion of adjacency matrix values to express topology in terms of resistance rather than adjacency 7 a m i j 1 f o r a m i j 0 1 f o r a m i j 1 tm is then a scalar product of inverted am and weight function values according to the following formula 8 t m i j a m i j 1 w i j am 1 term stands for an inelastic topology component no edge can appear between non adjacent patches even if weight function returns high value in further processing tm values will be the only source of graph edge weights however to keep consistency with graph terminology in the following text we will still use the term weight referring to graph edges 3 3 terminal set and reduction of graph on exit from mobility model travel matrix tm represents a full graph for the entire patch mosaic and has enough data to route the movement between arbitrary pair of nodes now we are going to reduce it to more compact structure taking into account that routing will be performed only for selected nodes we call this set of nodes a terminal set in patch corridor matrix paradigm this set corresponds to habitat patches but it may very well represent other focus areas like existing conservation patches in more elaborate scheme multiple terminal sets may be used to partition the patch mosaic into classes in any case terminal set is where the movement terminates and also where it originates we provide two ways of entering ts into the model one is by marking individual patches in a table and the other by selecting a class of patches based on a chosen variable typically we expect ts to be a fraction of all nodes but there is no predefined upper limit and it can include all graph nodes once the terminal set is established we are able to build a derived graph called terminal graph based on full graph g v e terminal graph g v e is specified as follows 9 v v new set of nodes is drawn from a set of full graph s nodes v is equivalent to terminal set 10 v v d e g r e e v v 1 each node v has a degree a number of incident edges equal to total number of nodes minus one in other words the graph is fully connected for n nodes the number of edges is n n 1 2 for instance six edges on fig 8 11 e e v a v b s h o r t e s t p a t h g v a v b terminal graph edge set e consists of edges e such that an edge between v a and v b is made of the shortest path between corresponding v a and v b in full graph g in order to implement above definition travel matrix is first compacted by removing the columns and rows corresponding to nodes not belonging to the terminal set then we proceed to computing shortest paths which will constitute terminal graph edges 3 4 computing shortest paths a path in the graph as opposed to walk or trail jungnickel 2013 is an ordered set of edges between given origin and destination nodes with additional requirement that both edges and intermediate nodes are unique this bans cycles within a path a path can be alternatively represented as an ordered list of nodes and this representation will be also in use among many possible paths between a pair of nodes shortest path is the one with a minimum sum of weights fig 9 path weight is the arithmetic sum of weights of its edges 12 w p a t h e p a t h w e i 1 n w e i formula 12 requires that weight function returns ratio scale values and edge to node rewriting has been done properly as outlined in 3 2 2 in our method shortest paths must be computed between every pair of nodes of terminal graph this is the most time consuming computational problem to solve in our workflow known under the name all pairs shortest paths like for many similar problems the core of the algorithm is dijkstra s shortest path algorithm dspa dspa is considered effective in practice and has an advantage that it not only returns the total weight but also enumerates a list of edges constituting the path our implementation computes a complete set of paths from single origin node to all destination nodes a dendrite in a single step dendrites are processed in parallel with very good scalability multithreading speed up is almost proportional to the number of processor cores an algorithm flawlessly accepts pre conditioned weights in case of a step model described earlier which effectively leads to bi criterion sps results from dspa are stored in two data structures terminal travel matrix ttm and shortest paths matrix spm both of tg tg size single cell of ttm holds the total weight of a path and single cell of spm a complete path in node representation ttm is equivalent of tm for the full graph with important difference its elements are valid for all nodes and not only adjacent nodes values do not occur by definition spm represents the mapping between full graph and terminal graph necessary to tie resulting mst to original coverage and later becomes the source of data for map displays as we process a fully connected graph both matrices should be dense a presence of empty cell indicates that the graph is composed of two or more subgraphs but an error is reported only if this prevents building the terminal graph both matrices are available for inspection 3 5 minimum spanning tree in the final step we are going to find of a minimum spanning tree of terminal graph this structure is composed of all nodes and selected edges of terminal graph in such way that connectivity is achieved at the minimum cost mst can be used to design an effective ecological corridor system or as a benchmark structure for actual movements and existing or planned conservation schemes we will show how mst can be translated to a patch oriented measure identifying connectivity hotspots 3 5 1 definition a spanning tree t v e is a subgraph of graph g v e consisting of all nodes of g and a subset of edges of g and having no cycles for a given graph g there may be many spanning trees t composed of different edge sets e the number of edges of any t is equal to v 1 and this is the least number of edges necessary to connect each and every node of g for weighted graph minimum spanning tree mst is the one with the minimum total weight see fig 10 weight summation for mst is carried out the same way as for paths as the number of edges of all spanning trees of a given graph g is equal mst cannot be uniquely determined in an unweighted graph this explains why we have put extra effort to border factors for step model section 3 2 3 3 5 2 implementation a graph of n vertices has n n 2 spanning trees e g over 1032 possibilities for 25 nodes we use prim s algorithm which execution time never grows faster than n 2 and in practice is much lower the principle of operation is to build mst edge by edge appending the shortest unused edge connecting to the recently added edge deo 1974 this behavior is typical for greedy algorithms prim s algorithm cannot be truly parallelized but compared to dspa it has much lower number of nodes to process therefore we leave it running single threaded the algorithm works on terminal graph represented by terminal travel matrix which is devoid of information on edge composition however we want mst to be expressed in terms of original full graph and ultimately patch mosaic elements therefore resulting mst edges are mapped back onto original graph fig 11 and it s edges reconstructed we arrive at minimum spanning tree composed of shortest paths so the outcome is combined result of two levels of our workflow driven by common optimization criterion additional intermediate nodes appearing now do not necessarily belong to terminal set but are still indispensable elements of the corridor system 3 5 3 outputs we provide several alternative methods of representing mst output these are elements list summary statistics transit density skeleton shortest mst edge the most basic output is the list of elements constituting the minimum spanning tree as it is easier to identify nodes than edges we use node representation for mst on fig 12 consisting of two edges the output would be a b c d d c b e f h the output uses node notation and terminology of the full graph so it is referred to as mst path summary statistics include sum mean and standard deviation of weight of paths regardless of whether the model is based on mobility factors or not step statistics are computed for comparison a raw listing although precise does not help to visualize the spatial layout of mst over the patch mosaic we want to know where mts is but the elements of mst are not distributed one per patch they may cross any single patch many times we introduce more general measure of transit density td as a count of mst paths crossing a patch it is computed with the following algorithm 13 td v 0 for all p mst for all v p td v td v 1 endfor endfor an algorithm initializes td values for all nodes all coverage patches to zero and then visits mst paths p one by one for each node v belonging to path p corresponding td value is incremented transit density measure is an effect of swapping the reference point from graph structure to patch mosaic so that mst becomes a property of each individual patch the new indicator is easily mappable measured on ratio scale and comparable with other patch metrics transit density is a measure of patch importance within a system of terminal patches or a contribution of a patch to connectivity provided by mst because it is always related to particular mst full name should read mst transit density but we use abbreviated form for simplicity in passing we note that algorithm 13 can be used not only for the mst it is mother of all scheme for measuring the occurrence of arbitrary structure or set within original graph including betweenness centrality thus bc can be perceived as density of a complete path set structure and another example could be the density of terminal graph the next kind of output is mst skeleton a pictorial straight line display of mst overlaid on a map fig 14 this method gives a quick comprehension of the extent and overall layout of mst but hides some detail because skeleton lines may overlap this is because they are not terminal to terminal lines but are made of paths two different trees may exhibit the same skeleton and may be distinguished only by different td pattern as illustrated on fig 13 against fig 12 the opposite is also true two final outputs describe the position of terminal patch within mst for every terminal graphscape lists a shortest path connecting it to mst and returns its length expressed in 1 weight units and 2 steps units our observation is that it is quite challenging to decipher mst topology in relation to underlying mosaic most difficult to handle are trees build on complex interweaving coverages where mst branches turn in seemingly random directions except for the simplest trees like on fig 14 several methods must work together skeleton lines provide the general layout and directions of branches transit density identifies branch joints additionally we provide an option to identify individual mst paths by clicking on a map or by selecting a path in the table and highlighting on a map 4 graphscape software we decided that our multi step procedure would be best implemented not with a mix of tools or scripts but with a self contained software system we have developed graphscape software which is gui based fast native code and more memory efficient than gis solutions graphscape processing logic fig 15 follows exactly the linear workflow described in preceding sections additionally it features built in gis functionality for mapping and interactive work topology checking tools as well as tabular display of all internal intermediate data like geometry data adjacency list shared borders patch resistance and connectivity raw travel matrix outputs are exportable in text raster and gis vector formats 5 test case 5 1 the problem and input data the problem analyzed in this case study concerns the determination of the ecological distance between patches of specific types of ecosystems and identification of optimal system of paths between them a sample analysis concerns a random map mosaic created on the basis of 703 points randomly distributed for which thiessen polygons were generated for each polygon patch one of the 12 categories of land cover buildings fields pastures wet meadows wet scrubland deciduous forest coniferous forest mixed forest dry scrubland ruderal vegetation sandy grasslands water was randomly assigned the adopted set of coordinates causes the map to correspond to an area of 7500 square kilometers a category of sandy grasslands 21 patches was selected as a terminal set and for these patches the analysis was carried out mobility factors were provided in two external tables patch class resistance table and transfer resistance table patch class resistance values were assigned arbitrarily but on the basis of habitat preferences of insects spiders and small mammals having the optimum occurrence on the sandy grasslands of north eastern poland transfer resistance values were also determined by an expert method in an arbitrary way but taking into account the structural contrast vegetation layer structures and difference in habitat conditions between the specific land cover classes patch individual resistance was derived from the patch area rescaled to the 0 1 range the procedure was run 8 times to cover all possible combinations of 3 mobility factors involved 5 2 output results results are presented in table 2 and fig 16 on a map minimum spanning tree is colored blue and the shading depends on transit density patches with the darkest shade render joint points of branches of the mst terminal set patches are colored translucent yellow and outlined variant a defines a simple step model without considering any mobility factor option h defines a model that takes into account all three pcr pir tm mobility factors options b g specify models with different combinations of mobility factors the analysis of spatial distribution shows that each variant results in a different arrangement of paths although some paths connecting pairs of terminal patches are repeated in different variants of the analysis only for one pair of terminal patches closest in spatial sense the path with the lowest ecological resistance is identical in all variants there is a general trend of path elongation measured by the number of steps with an increase in the number of mobility factors considered this effect is easy to explain because with step analysis all the patches and borders between the patches are equally valuable so by definition the path thus obtained must be the shortest on the other hand an increase in the number of mobility factors taken into account results in an increase in the number of patches with relatively higher resistance and as a result some of the patches lying on the path in step analysis are bypassed and the number of steps increases weight statistics cannot be directly compared between variants but it can be pointed out that coefficient of variation is very diverse and falls in the range of 29 47 and is not related to the number of mobility factors taken into account it is interesting to note that these values are higher than for coefficient of variation for step statistics which are in the range of 34 43 the path network presented in particular variants together with their formal numerical characteristics indicate some general relationships first of all depending on the mobility factors taken into account the set of pairs of terminal patches closest to each other changes secondly even if in different variants of analysis the same pair of patches is maintained the course of the path may be different and thirdly often two terminal patches located close to each other in the geometric sense are not connected with each other by a path but are connected with other more distant patches differences in the network of paths resulting from different analysis variants are also connected with differences in transit density td of individual patches as a count of mst paths crossing a patch in most cases and for most of the patches from terminal set the td is 1 and for the most patches from non terminal set it is zero on the other hand some patches have a higher td value table 3 it is worth noting that they belong to both sets of terminal and non terminal patches their role in the pathways within mst is special as they indicate the existence of bottlenecks in the network 6 concluding remarks graphscape software has several features that are not found together in any other available software dedicated to analyzing connection networks and determining connectivity in the landscape 1 the main input data is a map showing the typological and spatial heterogeneity of ecosystems presented in vector form 2 the path connecting two consecutive patches from the terminal set is not a single section but a route consisting of sections through individual patches belonging to the landscape matrix which is consistent with the view that landscapes generally exhibit greater heterogeneity or complexity than the traditional view of habitat patches embedded within a homogeneous matrix with 2019 3 the software allows to define transfer density not only for the patches from terminal set through which mst passes but also which is new for the matrix patches 4 the mst modelling in graphscape may take into account the resistance resulting from crossing a border the possibility of constructing graphs in which nodes represent all patches in the landscape and the links represent the likelihood of passing the boundary between adjacent patches was indicated by urban et al 2009 the use of a typological vector map as an input has its advantages and limitations the advantage is that discrete spatial units are distinguished which is consistent with the views that organisms live in ecosystems wiens et al 1985 and not in gradually changing environment for which the grid based structural representation squares or hexagons is an artificial construct dunn 2010 these differences in the assessment of the applicability of raster and vector approaches are an indirect echo of a longstanding discussion on the continuity and discontinuity of vegetation and the environment summarized among others by austin 2013 in addition this approach makes it easier to capture resistance when crossing borders and to distinguish areas with high td values also outside terminal set areas also important are the limitations some of which correspond to those for raster approach with raster approach the way of matrix representation can greatly affect assessments of connections ziółkowska et al 2014 on the other hand in vector approach the decisive impact on topological indices ricotta et al 2000 pathways and measurements has the classification scheme adopted for patch mosaic and the extent of the area analyzed we find this less disadvantageous because these are part of the model logic and better controlled from dunn s considerations 2010 referring to raster approaches it appears that an irregular lattice with a high level of isotropy may be the best representation of the landscape the vector map of ecosystems obviously has an irregular patch pattern but is rarely isotropic in particular the course of the paths forming the msp can be decisively influenced by very large very elongated or star shaped patches which tend to attract pathways in disproportionate manner in graphscape we provide a table to assess purely topological connectivity and to spot these irregularities these drawbacks can be minimized by preparing an input map accordingly in most cases it is sufficient to specify adequately detailed classification system of the ecosystems and expect that for example extensive forest complexes are divided into numerous smaller areas representing different forest types it is also possible to introduce arbitrary size thresholds for the individual patches but this solution although formally acceptable may distort the ecological interpretation of the results obtained a relatively save pre conditioning would be dividing quasi linear polygons like rivers based on with to length ratio with the result that shorter stretches would maintain existing barrier effect but curb down too extensive longitudinal movement the landscape graph created at the first stage of our workflow topology building has an isomorphic structure with an input map of ecosystems in formal terms it represents a class of minimum planar graphs i e those in which a link cannot cross another link and each node is connected only to its topological neighbors fall et al 2007 galpern et al 2011 it is worth noting that planar graphs are becoming increasingly popular in connectivity analyses dale 2017 they are generated either directly on raster data rayfield et al 2016 or on the basis of polygons obtained in a different way most often as a result of delaunay tessellation of a landscape and thiessen polygons margosian et al 2009 however none of the known approaches has so far used a vector based complete mosaic of ecosystem patches as an input material for analysis thiessen polygons can be an input to graphscape in cases when original data comes from point sources other than that planarity issues are rarely relevant to our workflow the main output of the graphscape software is a minimum spanning tree which according to fall et al 2007 identifies the connectivity backbone of a landscape resulting patch set naturally forms an input to next level of analysis involving dispersal flux traversability distances and weights clusters of patches from the terminal set the sensitivity to node removal etc de cola 2010 rego et al 2018 in graphscape we implemented only few basic measures because the result set is easily exportable and may be further processed in other software systems we did not however resign from visualization because it is indispensable in experimental phase as well as important from a practical point of view as it enables decision makers to intuitively assess the spatial configuration which can among others support the spatial planning process foltête et al 2014 the principal measure of mst in landscape graph is transit density td is similar to betweenness centrality freeman 1977 in the logic of computation the difference being the elements counted for bc these are all to all paths for td only paths building mst edges bc reflects unconstrained transit possibilities and td transit possibilities constrained to mst and driven by efficiency principle high loads of bc can occur anywhere in patch mosaic while high loads of td only on paths connecting terminal set so generally with more compact spatial extent however for some ts layouts td distribution may be far from obvious and may reveal intricate overlapping re use of the same patches between branches of mst as we stressed earlier td is always related to particular mst so single instance of td data cannot be considered a general purpose connectivity indicator for the whole mosaic it does not mean td cannot be used to reach for more general conclusions provided more elaborate setup td is very strong count type measure allowing for variety of operations like addition subtraction or ratio without losing ecological meaning to start with td is suitable not only for counting single tree s like shown in section 3 5 3 but also multiple trees occurrences we anticipate two broad classes of use cases mobility oriented and landscape oriented in the first one the terminal set is fixed and the model is run multiple times against different parameters of mobility for instance patch resistance a series of mst readouts as measured by td reveals the response to mobility tweaking or in another variant gives averaged more relaxed version of shortest paths and minimum spanning tree the latter variant could be used to alleviate the sharpness of optimization algorithms in the second scenario the goal is to get insight into the landscape rather than mobile agent this time mobility factors are fixed and the model is run against different terminal sets we loosen the assumption that ts is a set of core or distinguished patches depending on the ts partition logic different levels of overall connectivity can be tested up to the maximum level of whole patch mosaic which would be the case when mosaic subsets are disjoint and complete in this setup it is possible to introduce a key variable driving the process of partitioning the results would then reflect how partition driver influences connectivity metrics obtained in this way will be similar in nature but not identical to numerous raster based measures of graph structure described in rayfield et al 2011 and dale 2017 based on topology in symmetric graphs which in direct or indirect way describe different aspects of landscape connectivity graphscape is quite flexible tool and depending on the entered data the scale and extent of the map and its classification scheme with additional external tables defining mobility factors it can be used for the different types of problems in the simplest case without considering mobility factors and determining only step statistics the result obtained is one of the landscape configuration metrics and can be compared with such generally known metrics as to the nearest neighbor distance or proximity index in more complex cases when mobility factors are involved most of the issues concern landscape connectivity and modelling of ecological corridors in particular graphscape can be used for planning of ecological corridors in the landscape for selected groups of plant and animal species identification of the most important areas from the point of view of general landscape connectivity on the basis of the occurrence of patches where different paths intersect transit density identification of the role of particular mobility factors in the movement of organisms by comparing the modelled networks of connections with the actual routes of animal movement identification of critical areas from the path network point of view by comparing the path model with the distribution of barriers assessment of patch isolation as a complementary problem to connectivity at present graphscape is focused on the concept of minimum spanning tree which makes the analysis of connections between terminal patches one to one however we are convinced that one to many approach is possible along with variety of other graph structures and we are going to explore them in the future going in this direction will inevitably make optimization questions more and more important graph structures arise from optimization techniques and these demand clearly stated goals therefore with increasing precision of tools attention must gravitate towards values priorities and beneficiaries software availability software was created in embarcadero delphi programming language was object pascal developer was wojciech pomianowski wpo twarda pan pl distribution size is under 10 mb additional hardware or software components e g java or net framework are not required to run graphscape no installation is required graphscape with supporting material is available for free download at https www igipz pan pl graphscape html provided is 64 bit native code executable running under microsoft windows 7 and later declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements graphscape software has been developed under the project 2012 05 b st10 02173 financed by the poland national science center the authors would like to thank the anonymous reviewers whose comments and suggestions greatly improved the original manuscript 
25935,out of many approaches to landscape connectivity we choose patch mosaic flow neutral framework for modelling our new software system graphscape transforms a vector mosaic map into landscape graph finds shortest paths and combines them into minimum spanning tree the result is an optimal network of connections for a given set of patches to capture mobile agent properties we use two types of resistance assigned to patches and transfer resistance assigned to combinations of patch classes another alternative is step model when transfers occur as if on unweighted graph resulting mst is rendered by linear skeleton binary transit density measure assignable to patches continuous and summary statistics a novelty is compete workflow from vector based map to complex graph structures we describe concepts and operation of software graphscape can be used for ecological corridors planning identification of key areas of corridors overlap identification of the role of particular mobility factors assessment of patch isolation keywords landscape graph minimum spanning tree landscape metrics least cost distance patch corridor matrix model patch mosaic model 1 introduction landscape connectivity in the broad meaning of ability of the landscape to impede or to facilitate any kind of movement of any kind of living organisms among resource patches taylor et al 1993 is recognized as a key concept for understanding modelling management and protection of individual species as well as local and regional biodiversity as a whole it is important to emphasize that landscape connectivity depends on two groups of variables the first group includes the composition and configuration of the landscape i e the occurrence spatial distribution and boundary relations of all the ecosystem patches natural semi natural and anthropogenic this is a property that characterizes the landscape and is independent of the process under study the second group of factors influencing connectivity is related to the examined species or process and depends on the habitat requirements of species and the ability to move across patches ecosystems of different type ament et al 2014 in other words landscape connectivity is a function of a landscape permeability 1 1 permeability sensu singleton et al 2001 2002 refers to the degree to which whole landscapes encompassing a variety of natural semi natural and developed land cover types are compatible with wildlife needs and sustain ecological processes in contrast to landscape connectivity which characterizes the capacity of individual species to move between areas of habitat via corridors and linkage zones for any kind of animal movement e g daily movements of directional incidental and random character within the landscape seasonal migrations between different landscapes and regions dispersal from source ecosystems resulting in the growth of the species territorial range as well as in the interchange of individuals between local populations forming a single metapopulation ament et al 2014 the concept of landscape connectivity is also applied to other ecosystem processes such as active and passive plant species dispersal and genetic exchange colonization of unoccupied habitats disease spread as well as water energy and material transportation within and between ecosystems one of most important issues studied under the concept of connectivity is the modelling of linkages between focal habitats brás et al 2013 i e patches of ecosystems particularly suitable for a given species or focal areas e g nature protection areas fenu and pau 2018 there is no single generally accepted approach for modelling of such linkages because the result depends on the general landscape model adopted e g patch mosaic patch corridor matrix regular grid structural and functional properties assigned to patches as well as on route types and connectivity measures based on different theoretical foundations fortin et al 2012 ziółkowska et al 2014 according to van etten 2017 we may distinguish three main types of routes and as a consequence three types of distance measures in heterogeneous geographic spaces a the shortest line between two points as the crow flies b the least cost distance taking into account the obstacles and the local friction of the landscape as the wolf runs c the random walk with route finding as a stochastic process drunkard s walk terms of van etten 2017 regardless of the type of distance measure adopted three types of connectivity representations binary categorical and continuous may be considered ziółkowska et al 2014 according to calabrese and fagan 2004 all connectivity measures fall into three categories a structural metrics based solely on landscapes composition and configuration b potential connectivity metrics based on the landscape structure as well as information about the study organism s dispersal movement ability and habitat preferences c metrics of realized functional connectivity based on records of the real movements of individuals across the landscape all the approaches outlined above have been used in various ways in methods and algorithms for modelling connectivity these were based on different methodological background e g spatial statistics spatial regression and graph theory fortin et al 2012 the most promising and most widespread approach nowadays is based on graph theory nowadays according to galpern et al 2011 in most common applications patches of habitat are defined and distinguished from the matrix separately for each focal species and serve as the nodes vertices the connections among nodes called links linkages edges represent generally the distance between nodes since the beginning of the 21st century many systems have been developed to assess the degree of connectivity in the landscape table 1 most of the software was reviewed in papers by urban et al 2009 laita et al 2011 galpern et al 2011 and fortin et al 2012 they identified typical research questions and their associated graph construction and analysis methods with special emphasis on the conceptual model underlying these applications model parameterization and predictions available through graph analyses from our perspective all existing software can be combined into a few groups the first one includes solutions based on flows in pre determined connection networks regardless whether these are especially dedicated to landscape issues such as openfluid or ucinet or to general graph analysis such as pajek all other software is typically based on metapopulation models or landscape island models where the patches are islands of suitable habitat embedded in an inhospitable matrix nonhabitat the second group patch matrix model with dispersion rate introduces additional parameters for dispersion capacity of certain species e g conefor sensinode the third group cost distance raster models consists of software that handles only the suitability of matrix in each raster cell for a given species at last in the fourth group circuitscape software a least cost path is replaced by more general least cost flow moving across landscape circuitry some of the applications deliver a unique or a small set of best linkage s i e the areas promoting connectivity between habitats while others produce a quantitative evaluation of how suitable or probable each area is for the established connectivity goals brás et al 2013 however in the light of the above review it is worth recalling that one of the first papers to analyze the usefulness of the graph theory in landscape ecological studies cantwell and forman 1993 was focused on evaluating landscape configuration with less attention devoted to evaluating landscape connectivity the authors pointed out that nodes should represent landscape elements and linkages should represent common boundaries between elements the same theoretical principles were employed by ricotta et al 2000 who pointed out that a landscape graph may be constructed from any thematic map at any scale of observation ricotta et al 2000 also underlined that graph theoretic topological indices mainly the harary index may be used as measures of landscape connectivity urban et al 2009 mentioned the possibility to define a graph in which nodes represent habitat patches that tile the landscape and the links represent the likelihood that individuals moving within the patch might encounter the boundary to an adjacent patch and pass through lately fenu and pau 2018 proposed a very interesting approach in which suitable areas for ecological corridors were selected from a grid based resistance map where land cells were associated with a cost weighted distance cwd value but the analysis of the character of these corridor was conducted on the spatial graph model derived from a land cover vector map despite individual studies exploiting the vector representation of the landscape as a mosaic of patches this direction of research on landscape connectivity has never been popular especially when compared to the number of publications based on the raster approach and metapopulation theory island biogeography or patch corridor matrix model however among all the variety of systems used to assess landscape connectivity we miss the software referring to the classical landscape model as a mosaic of ecosystems patches with well defined borders represented on vector maps the main purpose of this article is to present a new software graphscape which enables analysis of landscape connectivity using the vector based patch mosaic landscape model together with its methodological assumptions and potential applications the proposed new software is justified for several reasons first of all many studies using raster data and graph approach move away from pixel based analyses towards patch based analyses e g hersperger 2006 de cola 2010 comber et al 2012 fenu and pau 2018 while there is a lack of publicly available software to determine landscape connectivity for such distinguished patch mosaic landscape secondly vector representation of polygons patches carries the advantages of better map precision being able to work with boundaries of differentiated shapes and avoiding the problem of a cell pixel size choice fenu and pau 2018 thirdly in the case of raster and vector representation of the same patch mosaic planar graphs should be identical fourthly the vector representation of patch mosaic makes it easier to take into account the role and nature of boundaries between adjacent patches 2 conceptual background 2 1 ecological aspect the starting point to our work was to view the landscape as a mosaic of patches ecosystems stored and processed in vector form this is in line with the postulates of cantwell and forman 1993 as it takes into account a the composition and spatial configuration of patches as basic elements of the landscape b is focused on interactions interdependencies and fluxes between adjacent elements and c allows to model corridors and evaluate connectivity in different spatial scales similar assumptions were made by vuilleumier and metzger 2006 for animal dispersal modelling we suggest that this approach is devoid of those limitations that may exist when working with the grid based landscape models which include the following i the existence of an a priori fixed scale of resolution ii the need to aggregate usually by averaging attributes of cells at the pre defined scale iii problems in representing linear objects and topology shape and relationships between distant objects vuilleumier and metzger 2006 laurini and thompson 1992 and iv poorly defined and difficult to operationalize concept of a border dunn 2010 also added that several pixel based connectivity metrics rely on algorithms that are affected by a regular grid induced bias we find the patch mosaic model uniquely suitable to parameterize the size and shape of the patches and the structure of the borders as factors affecting the network of connections within the landscape as animals and all other agents move within and between patches of different ecosystems we assume that the usability and permeability of patches as well as permeability of patch borders and contrast between adjacent patches determine the structural and potential landscape connectivity estreguil et al 2014 considering different empirical observations referred in the literature kindvall and petersson 2000 concluded that animal movement mainly migration dependence on patch geometry is probably a significant phenomenon in most patchy systems this dependence is a result of at least two animal species characteristics perceptual range and dispersal distance the perceptual range is the distance from which a particular landscape element i e a habitat patch can be perceived by a given animal kozakiewicz and szacki 1995 vuilleumier and metzger 2006 additionally patch edges are interfaces between two types of habitat edge effects may be positive or negative features for patch interior quality and for movement across a landscape baker 1996 ries and sisk 2004 ries et al 2004 estreguil et al 2014 according to these ecological concepts we build the mobility model around three characteristics of patches a usability and permeability which is assigned to the patch type and depends on the species or other mobile agent analyzed b individual resistance which is assigned separately to each patch and depends on intrinsic characteristics like size or shape not influenced by the species concerned c transfer resistance describing contrast between adjacent patches and depending solely on landscape configuration and ecosystem types the above input parameters either based on an expert estimation or measured on a map or in the field are the indirect indicators of species preferences and landscape constraints and make the modelling of landscape connectivity structural and partly potential according to terminology of calabrese and fagan 2004 more realistic 2 2 graph theory aspect topological structure of the landscape graph provides a framework for movement over which actual movement can happen this is not unlike a transportation network which provides possibilities for potential travelers the keyword here is potential because infrastructure layout is quite different matter from how the network is actually utilized over the same transportation network flow intensity and distribution varies widely within daily or weekly timeframe it is inspiring to see that an extensive research goes into modelling of transportation accessibility see the review of geurs and ritsema van eck 2001 where network potential is successfully analyzed without reference to flows we believe similar approach is possible in ecological modelling flows or fluxes in ecological parlance cannot be modelled without population dynamics trophic and competition factors with many analogies to transportation demand and supply this puts high demand on empirical data that often cannot be satisfied bergsten and zetterberg 2013 we propose more modest solution flow neutral model focused on possible rather than actual movement and on directions rather than magnitudes next we think that distance and other geometry concepts should be put into proper perspective when combined with graph theory first we have to point out that early work on landscape connectivity e g taylor et al 1993 urban and keitt 2001 established an overwhelming position of physical distance as a determinant of mobility it was the result of adopting patch corridor matrix approach and inevitable simplification of what is happening in inhospitable matrix zone in our opinion matrix deserves better recognition and there is more data than physical distance to be used in search of mobility patterns galpern et al 2011 place distance as a representation of graph edges on equal standing with dispersal and flux however these measures do not belong to the same logical category dispersal and flux are the effects directly observable on graph edges while distance is in more or less convoluted way one of determinants of these effects mixing graph theoretic and geometric terms leads to confusing concept of geometry of nodes and ramblings about node location within a patch fall et al 2007 go as far as to distinguish special class of spatial graphs with unique properties and thus propose geometric extensions to graph theory we think that spatial attribute describes the context of use much like dale and fortin 2010 rather than distinct mathematical entity conventional graph theory is rich enough for landscape studies and is reliable in terms of proofs and trusted algorithms therefore it should not be wrenched and extended beyond limits this is not to say that geographic thinking should be avoided we only advocate clear separation of concerns so that geometric terms and techniques are not used after crossing the boundary of graph domain after graph theory textbooks we use an abstract term weight as the final attribute of the graph edge when referring to algorithms and mechanics of the graph to cover the intricacies of weight construction before it is passed to graph algorithms we use the term weight function we place these concepts in a center of our method and by design channel to them as many factors as possible including distance mobility characteristics and terrain properties our approach avoids problematic issues related to graph assemblage and specifically edge construction we find distance thresholding as a method of edge creation e g urban and keitt 2001 fall et al 2007 baranyi et al 2011 too arbitrary and conflicting with graph theory requirements namely the freedom of choice given to optimization algorithms instead we propose edge creation either 1 based on fundamental and more sharply formulated criterion or 2 entirely unconstrained in our model we employ both approaches at the first level the criterion for edges is patch mosaic topology and at the second level fully connected graph is used 3 processing flow the current state of art and shortcomings described above led us to the other attitude more rooted in traditional graph theory we want to extend patch corridor matrix approach to make it more general and capable of handling landscape data in a uniform way in this section we outline a process of extraction of landscape structure useful for connectivity research and corridor design we describe a computational workflow leading from a vector based landscape patch mosaic to a minimum spanning tree an optimal connectivity structure according to graph theory with the mobility model as the key element incorporating ecological component we explain mathematical foundations data structures and algorithms used in this multi step approach 3 1 building topology an input data to our workflow is vector based map of patches areal units homogenous under study assumptions covering a continuous extent of earth surface which we call patch mosaic see fig 1 patch characteristics numeric and non numeric data are also provided but at this stage they are only preserved for further processing no patches are distinguished based on their attribute data especially habitat and matrix patches are treated equally local discontinuities are accepted unless they break the mosaic into separate disjoint parts we assume the input is devoid of topologic structure like esri shapefile format which stores geometric surfaces as self contained entities without topologic relations in case of continuous coverage borders of adjacent patches are then effectively duplicated the purpose of this step is to discover topologic relations and transform the mosaic into isomorphic structure of a landscape graph the landscape graph g v e made of a node set v and an edge set e is defined in such a way that 1 v 1 v 2 v n v p 1 p 2 p n p there is a one to one mapping of node set v onto patch set p 2 e e v a v b a d j p a p b t r u e an edge exists between nodes v a and v b if adjacency relation holds true for a pair of patches p a p b after having nodes 1 and edges 2 defined we proceed to adjacency relation 3 p a p b p p a d j p a p b t r u e i f s h a r e d b o r d e r p a p b f a l s e i f s h a r e d b o r d e r p a p b axiom 3 states that adjacency must be valid for every pair of patches and delegates the implementation to shared border function strictly graph definition ends at adj relation but we define additional function because it provides data for a special variant of the model described later shared border is a function those arguments are two patches p a and p b and the result is a set of multiline geometric objects or empty set an implementation runs along slightly different logic than axiomatic approach given above adjacency is not tested for each pair of patches as suggested by 3 rather shared border detection algorithm is run for the entire coverage an algorithm starts with matching all outline points against each other matched points are coalesced combined into one object with information on patch origin preserved next coalesced points are traced into strings representing single e g border a b on fig 2 or multiple border c d line segments with left and right patch attributes degenerated lines consisting of a single point are rejected result is stored in memory list sb now the algorithm proceeds to building an adjacency matrix am a binary symmetric matrix of p p size only below diagonal elements are physically stored am is initially filled with 0 s the algorithm performs full scan of sb list and for each sb i element finds corresponding am cell at position given by sb i left sb i right pair and sets its contents to 1 recurring assignments to the same cell do not change the result cells not identified by any sb element remain 0 we arrive at a basic graph representing topologic backbone of the landscape fig 3 it is unweighted because it has yet no information attached to edges and undirected because edges have no orientation towards either endpoint in subsequent processing the number of edges is never increased so this structure defines an upper limit of connectivity the analogy to the transportation infrastructure is clear it is important to emphasize that nodes obtained this way cannot be attributed geometric properties of any kind and any location in geographical space our software generates geometric points for visualization purposes geographic center of mass for concave polygons additionally corrected to remain within patch outline but their location is purely conventional and we do not derive any graph properties from them the same concerns edges which cannot be interpreted as the ground lines connecting locations 3 2 mobility model the next step of processing called mobility model is a parametric method of describing a mobile agent an entity performing a movement in the landscape living individual a group a species a substance etc we assume that the movement is controlled exclusively by graph topology and three types of properties assigned to graph nodes and edges called mobility factors distance relations receive no special treatment and if considered important must be included as one of these factors mobility model transforms initial unweighted graph which accounts for topology only into a weighted graph which accounts also for mobile agent s behavior a weight is a numeric value assigned to each edge and its magnitude reflects generalized cost or travel impedance between two nodes depending on the problem it may represent time energetic expenditure probability frequency or rate of transfer failure or inverse of transfer success etc the internal representation of the graph while being processed by mobility model is the travel matrix tm a floating point symmetric matrix of size n n nodes see fig 4 compared to binary am tm will be composed of numeric values with inverse interpretation low values will represent quick travel high values will represent slow travel infinity is assigned to no link cells it must be reminded that tm will refer only to direct connections between patches long range movement will be introduced later with paths 3 2 1 mobility factors the calculation of travel matrix is based on three components and corresponding three data sets patch class resistance patch individual resistance transfer resistance first two factors are the properties of patches third one defines the relationship between patches patch resistance is resistance to travel or alternatively a generalized cost of travel through a graph node as graph node represents a patch with non zero surface this surface can incur a resistance to mobile agent as a matter of convenience this influence has been split into two factors with patch class resistance pcr the resistance is assigned to whole classes of patches for instance land use classification type at once with patch individual resistance pir it is assigned to each patch individually for both resistances high values increase and low values decrease the weight assigned to transfer matrix cell apart from assignment method both factors share the same interpretation and performance in calculations class resistance is better suited to capture inherent behavior of mobile agent in response to certain terrain characteristics so it is appropriate when this behavior is consistent within entire classes of patches individual resistance provides a way of incorporating unique properties of patches which are difficult to generalize and fit into classification scheme it is the most suitable way to introduce patch size or geometric shape measures into the modelling process pcr table is supplied as an external tab delimited text file fig 5 pir is retrieved from one of the shapefile columns transfer resistance tr describes resistance to movement from patch to patch in mathematical terms it defines a relation between classes of patches accordingly this factor can be only assigned class wide like pcr here again high values increase and low values decrease the weight due to symmetric travel matrix design transfer resistance table needs only above diagonal elements fig 6 there is no predefined range of values for mobility factors except that negative numbers are converted to infinite resistance data is accepted as it is without any scaling we must emphasize that three mobility factors only define the maximum of options and not necessarily the best or obligatory program the model can run with any combination of factors 3 2 2 edge weight formula three mobility factors pcr pir and tr are combined into single weight value for travel matrix cell with the following formula 4 w i j p c r i p c r j 2 p i r i p i r j 2 t r i j with i standing for origin and j standing for destination patch we remind that the formula is valid only for adjacent patches in plain words the logic of computation is as follows 1 final weight is a product of combined patch resistance and transfer resistance 2 patch resistance is a sum of pcr and pir 3 patch resistance is relocated from nodes to edges the relation between patch resistance and transfer resistance is multiplicative so these two types contribute equally to the final result the relation between two kinds of patch resistance is additive so when both kinds are used they may together outweigh transfer resistance therefore an attention must be paid to proper scaling the most important part of 4 is rewriting of nodes to edges we do it to comply with standard graph algorithms which expect to find the cost of traversal only on edges edge value is derived from two neighboring nodes in equal parts half of origin patch resistance and half of destination patch resistance contribute to total edge resistance this simplistic rule is driven by important restriction no part of edge weight must be lost or added when combining paths for instance on fig 7 a total path weight of a c must be equal to the sum of a b and b c if this condition is not met graph algorithms cannot fairly compare alternative paths or trees and arrive at wrong optimization results 3 2 3 step model with border factor in some cases we may want to route movement without mobility factors this may be the case when mobility factors are unknown or we consider many contributing factors cancel each other in step model weight function is simplified so that it returns the same value for every transfer between adjacent patches or crossing the border subsequent path calculations will then return the count of steps between origin and destination in effect step model parallels ecological models based on unweighted graphs and may be useful for comparisons however this simple scheme must be improved to handle two fundamental drawbacks of routing on unweighted graph first the solution to the shortest path problem is not unique there can be many paths of equal length between two nodes with majority of alternative paths being the trivial variants of the base one the number of possibilities increases with the distance second minimum spanning tree solution may be not unique by the same token commonly used algorithms both sp and mst are not able to return the full set of equivalent solutions and return more or less random pick this effect is even more pronounced when algorithms are used in a sequence as in our modelling setup trying to solve the problem we rejected an option of modifying the algorithms to return multiple results in this case as many as sp set mst set as non standard and of little practical value we decided to force uniqueness by additional non topological criterion which is the geometric length of shared borders between patches the conjecture is that interaction intensity or opportunity for transfer between patches increases when they share a long border we included shared border information in weight function effectively tricking existing algorithms into bi criteria optimization our goal was not to mix two criteria evenly but to add supplementary criterion so that out of two solutions with the same number of steps the one with longer shared borders is selected a starting point was an unweighted formula w i j 1 we modified it to 5 w i j 1 b i j where b i j is a border factor between adjacent origin i and destination j patches defined as 6 b i j l i j l in 6 l i j is a length of a shared border between i th and j th patch and σl is the total length of shared borders in the coverage border factor causes weight of each edge to be lower than 1 0 by a small fraction the longer shared border the smaller weight is generated and easier the movement to assure that algorithms work correctly with the corrected weight function it is necessary to prove that secondary criterion does not interfere with the primary one so that border factor does not outweigh the number of steps for any resulting structure this is guaranteed by the way border factor is relativized to the total length of borders in equation 6 the sum of b never exceeds 1 0 for any path or spanning tree because there exists no path or spanning tree consisting of more than all edges of the graph 3 2 4 travel matrix assignment having weight function defined by either 4 or 5 we proceed to final calculation of travel matrix first we perform scalar inversion not to be confused with matrix inversion of adjacency matrix values to express topology in terms of resistance rather than adjacency 7 a m i j 1 f o r a m i j 0 1 f o r a m i j 1 tm is then a scalar product of inverted am and weight function values according to the following formula 8 t m i j a m i j 1 w i j am 1 term stands for an inelastic topology component no edge can appear between non adjacent patches even if weight function returns high value in further processing tm values will be the only source of graph edge weights however to keep consistency with graph terminology in the following text we will still use the term weight referring to graph edges 3 3 terminal set and reduction of graph on exit from mobility model travel matrix tm represents a full graph for the entire patch mosaic and has enough data to route the movement between arbitrary pair of nodes now we are going to reduce it to more compact structure taking into account that routing will be performed only for selected nodes we call this set of nodes a terminal set in patch corridor matrix paradigm this set corresponds to habitat patches but it may very well represent other focus areas like existing conservation patches in more elaborate scheme multiple terminal sets may be used to partition the patch mosaic into classes in any case terminal set is where the movement terminates and also where it originates we provide two ways of entering ts into the model one is by marking individual patches in a table and the other by selecting a class of patches based on a chosen variable typically we expect ts to be a fraction of all nodes but there is no predefined upper limit and it can include all graph nodes once the terminal set is established we are able to build a derived graph called terminal graph based on full graph g v e terminal graph g v e is specified as follows 9 v v new set of nodes is drawn from a set of full graph s nodes v is equivalent to terminal set 10 v v d e g r e e v v 1 each node v has a degree a number of incident edges equal to total number of nodes minus one in other words the graph is fully connected for n nodes the number of edges is n n 1 2 for instance six edges on fig 8 11 e e v a v b s h o r t e s t p a t h g v a v b terminal graph edge set e consists of edges e such that an edge between v a and v b is made of the shortest path between corresponding v a and v b in full graph g in order to implement above definition travel matrix is first compacted by removing the columns and rows corresponding to nodes not belonging to the terminal set then we proceed to computing shortest paths which will constitute terminal graph edges 3 4 computing shortest paths a path in the graph as opposed to walk or trail jungnickel 2013 is an ordered set of edges between given origin and destination nodes with additional requirement that both edges and intermediate nodes are unique this bans cycles within a path a path can be alternatively represented as an ordered list of nodes and this representation will be also in use among many possible paths between a pair of nodes shortest path is the one with a minimum sum of weights fig 9 path weight is the arithmetic sum of weights of its edges 12 w p a t h e p a t h w e i 1 n w e i formula 12 requires that weight function returns ratio scale values and edge to node rewriting has been done properly as outlined in 3 2 2 in our method shortest paths must be computed between every pair of nodes of terminal graph this is the most time consuming computational problem to solve in our workflow known under the name all pairs shortest paths like for many similar problems the core of the algorithm is dijkstra s shortest path algorithm dspa dspa is considered effective in practice and has an advantage that it not only returns the total weight but also enumerates a list of edges constituting the path our implementation computes a complete set of paths from single origin node to all destination nodes a dendrite in a single step dendrites are processed in parallel with very good scalability multithreading speed up is almost proportional to the number of processor cores an algorithm flawlessly accepts pre conditioned weights in case of a step model described earlier which effectively leads to bi criterion sps results from dspa are stored in two data structures terminal travel matrix ttm and shortest paths matrix spm both of tg tg size single cell of ttm holds the total weight of a path and single cell of spm a complete path in node representation ttm is equivalent of tm for the full graph with important difference its elements are valid for all nodes and not only adjacent nodes values do not occur by definition spm represents the mapping between full graph and terminal graph necessary to tie resulting mst to original coverage and later becomes the source of data for map displays as we process a fully connected graph both matrices should be dense a presence of empty cell indicates that the graph is composed of two or more subgraphs but an error is reported only if this prevents building the terminal graph both matrices are available for inspection 3 5 minimum spanning tree in the final step we are going to find of a minimum spanning tree of terminal graph this structure is composed of all nodes and selected edges of terminal graph in such way that connectivity is achieved at the minimum cost mst can be used to design an effective ecological corridor system or as a benchmark structure for actual movements and existing or planned conservation schemes we will show how mst can be translated to a patch oriented measure identifying connectivity hotspots 3 5 1 definition a spanning tree t v e is a subgraph of graph g v e consisting of all nodes of g and a subset of edges of g and having no cycles for a given graph g there may be many spanning trees t composed of different edge sets e the number of edges of any t is equal to v 1 and this is the least number of edges necessary to connect each and every node of g for weighted graph minimum spanning tree mst is the one with the minimum total weight see fig 10 weight summation for mst is carried out the same way as for paths as the number of edges of all spanning trees of a given graph g is equal mst cannot be uniquely determined in an unweighted graph this explains why we have put extra effort to border factors for step model section 3 2 3 3 5 2 implementation a graph of n vertices has n n 2 spanning trees e g over 1032 possibilities for 25 nodes we use prim s algorithm which execution time never grows faster than n 2 and in practice is much lower the principle of operation is to build mst edge by edge appending the shortest unused edge connecting to the recently added edge deo 1974 this behavior is typical for greedy algorithms prim s algorithm cannot be truly parallelized but compared to dspa it has much lower number of nodes to process therefore we leave it running single threaded the algorithm works on terminal graph represented by terminal travel matrix which is devoid of information on edge composition however we want mst to be expressed in terms of original full graph and ultimately patch mosaic elements therefore resulting mst edges are mapped back onto original graph fig 11 and it s edges reconstructed we arrive at minimum spanning tree composed of shortest paths so the outcome is combined result of two levels of our workflow driven by common optimization criterion additional intermediate nodes appearing now do not necessarily belong to terminal set but are still indispensable elements of the corridor system 3 5 3 outputs we provide several alternative methods of representing mst output these are elements list summary statistics transit density skeleton shortest mst edge the most basic output is the list of elements constituting the minimum spanning tree as it is easier to identify nodes than edges we use node representation for mst on fig 12 consisting of two edges the output would be a b c d d c b e f h the output uses node notation and terminology of the full graph so it is referred to as mst path summary statistics include sum mean and standard deviation of weight of paths regardless of whether the model is based on mobility factors or not step statistics are computed for comparison a raw listing although precise does not help to visualize the spatial layout of mst over the patch mosaic we want to know where mts is but the elements of mst are not distributed one per patch they may cross any single patch many times we introduce more general measure of transit density td as a count of mst paths crossing a patch it is computed with the following algorithm 13 td v 0 for all p mst for all v p td v td v 1 endfor endfor an algorithm initializes td values for all nodes all coverage patches to zero and then visits mst paths p one by one for each node v belonging to path p corresponding td value is incremented transit density measure is an effect of swapping the reference point from graph structure to patch mosaic so that mst becomes a property of each individual patch the new indicator is easily mappable measured on ratio scale and comparable with other patch metrics transit density is a measure of patch importance within a system of terminal patches or a contribution of a patch to connectivity provided by mst because it is always related to particular mst full name should read mst transit density but we use abbreviated form for simplicity in passing we note that algorithm 13 can be used not only for the mst it is mother of all scheme for measuring the occurrence of arbitrary structure or set within original graph including betweenness centrality thus bc can be perceived as density of a complete path set structure and another example could be the density of terminal graph the next kind of output is mst skeleton a pictorial straight line display of mst overlaid on a map fig 14 this method gives a quick comprehension of the extent and overall layout of mst but hides some detail because skeleton lines may overlap this is because they are not terminal to terminal lines but are made of paths two different trees may exhibit the same skeleton and may be distinguished only by different td pattern as illustrated on fig 13 against fig 12 the opposite is also true two final outputs describe the position of terminal patch within mst for every terminal graphscape lists a shortest path connecting it to mst and returns its length expressed in 1 weight units and 2 steps units our observation is that it is quite challenging to decipher mst topology in relation to underlying mosaic most difficult to handle are trees build on complex interweaving coverages where mst branches turn in seemingly random directions except for the simplest trees like on fig 14 several methods must work together skeleton lines provide the general layout and directions of branches transit density identifies branch joints additionally we provide an option to identify individual mst paths by clicking on a map or by selecting a path in the table and highlighting on a map 4 graphscape software we decided that our multi step procedure would be best implemented not with a mix of tools or scripts but with a self contained software system we have developed graphscape software which is gui based fast native code and more memory efficient than gis solutions graphscape processing logic fig 15 follows exactly the linear workflow described in preceding sections additionally it features built in gis functionality for mapping and interactive work topology checking tools as well as tabular display of all internal intermediate data like geometry data adjacency list shared borders patch resistance and connectivity raw travel matrix outputs are exportable in text raster and gis vector formats 5 test case 5 1 the problem and input data the problem analyzed in this case study concerns the determination of the ecological distance between patches of specific types of ecosystems and identification of optimal system of paths between them a sample analysis concerns a random map mosaic created on the basis of 703 points randomly distributed for which thiessen polygons were generated for each polygon patch one of the 12 categories of land cover buildings fields pastures wet meadows wet scrubland deciduous forest coniferous forest mixed forest dry scrubland ruderal vegetation sandy grasslands water was randomly assigned the adopted set of coordinates causes the map to correspond to an area of 7500 square kilometers a category of sandy grasslands 21 patches was selected as a terminal set and for these patches the analysis was carried out mobility factors were provided in two external tables patch class resistance table and transfer resistance table patch class resistance values were assigned arbitrarily but on the basis of habitat preferences of insects spiders and small mammals having the optimum occurrence on the sandy grasslands of north eastern poland transfer resistance values were also determined by an expert method in an arbitrary way but taking into account the structural contrast vegetation layer structures and difference in habitat conditions between the specific land cover classes patch individual resistance was derived from the patch area rescaled to the 0 1 range the procedure was run 8 times to cover all possible combinations of 3 mobility factors involved 5 2 output results results are presented in table 2 and fig 16 on a map minimum spanning tree is colored blue and the shading depends on transit density patches with the darkest shade render joint points of branches of the mst terminal set patches are colored translucent yellow and outlined variant a defines a simple step model without considering any mobility factor option h defines a model that takes into account all three pcr pir tm mobility factors options b g specify models with different combinations of mobility factors the analysis of spatial distribution shows that each variant results in a different arrangement of paths although some paths connecting pairs of terminal patches are repeated in different variants of the analysis only for one pair of terminal patches closest in spatial sense the path with the lowest ecological resistance is identical in all variants there is a general trend of path elongation measured by the number of steps with an increase in the number of mobility factors considered this effect is easy to explain because with step analysis all the patches and borders between the patches are equally valuable so by definition the path thus obtained must be the shortest on the other hand an increase in the number of mobility factors taken into account results in an increase in the number of patches with relatively higher resistance and as a result some of the patches lying on the path in step analysis are bypassed and the number of steps increases weight statistics cannot be directly compared between variants but it can be pointed out that coefficient of variation is very diverse and falls in the range of 29 47 and is not related to the number of mobility factors taken into account it is interesting to note that these values are higher than for coefficient of variation for step statistics which are in the range of 34 43 the path network presented in particular variants together with their formal numerical characteristics indicate some general relationships first of all depending on the mobility factors taken into account the set of pairs of terminal patches closest to each other changes secondly even if in different variants of analysis the same pair of patches is maintained the course of the path may be different and thirdly often two terminal patches located close to each other in the geometric sense are not connected with each other by a path but are connected with other more distant patches differences in the network of paths resulting from different analysis variants are also connected with differences in transit density td of individual patches as a count of mst paths crossing a patch in most cases and for most of the patches from terminal set the td is 1 and for the most patches from non terminal set it is zero on the other hand some patches have a higher td value table 3 it is worth noting that they belong to both sets of terminal and non terminal patches their role in the pathways within mst is special as they indicate the existence of bottlenecks in the network 6 concluding remarks graphscape software has several features that are not found together in any other available software dedicated to analyzing connection networks and determining connectivity in the landscape 1 the main input data is a map showing the typological and spatial heterogeneity of ecosystems presented in vector form 2 the path connecting two consecutive patches from the terminal set is not a single section but a route consisting of sections through individual patches belonging to the landscape matrix which is consistent with the view that landscapes generally exhibit greater heterogeneity or complexity than the traditional view of habitat patches embedded within a homogeneous matrix with 2019 3 the software allows to define transfer density not only for the patches from terminal set through which mst passes but also which is new for the matrix patches 4 the mst modelling in graphscape may take into account the resistance resulting from crossing a border the possibility of constructing graphs in which nodes represent all patches in the landscape and the links represent the likelihood of passing the boundary between adjacent patches was indicated by urban et al 2009 the use of a typological vector map as an input has its advantages and limitations the advantage is that discrete spatial units are distinguished which is consistent with the views that organisms live in ecosystems wiens et al 1985 and not in gradually changing environment for which the grid based structural representation squares or hexagons is an artificial construct dunn 2010 these differences in the assessment of the applicability of raster and vector approaches are an indirect echo of a longstanding discussion on the continuity and discontinuity of vegetation and the environment summarized among others by austin 2013 in addition this approach makes it easier to capture resistance when crossing borders and to distinguish areas with high td values also outside terminal set areas also important are the limitations some of which correspond to those for raster approach with raster approach the way of matrix representation can greatly affect assessments of connections ziółkowska et al 2014 on the other hand in vector approach the decisive impact on topological indices ricotta et al 2000 pathways and measurements has the classification scheme adopted for patch mosaic and the extent of the area analyzed we find this less disadvantageous because these are part of the model logic and better controlled from dunn s considerations 2010 referring to raster approaches it appears that an irregular lattice with a high level of isotropy may be the best representation of the landscape the vector map of ecosystems obviously has an irregular patch pattern but is rarely isotropic in particular the course of the paths forming the msp can be decisively influenced by very large very elongated or star shaped patches which tend to attract pathways in disproportionate manner in graphscape we provide a table to assess purely topological connectivity and to spot these irregularities these drawbacks can be minimized by preparing an input map accordingly in most cases it is sufficient to specify adequately detailed classification system of the ecosystems and expect that for example extensive forest complexes are divided into numerous smaller areas representing different forest types it is also possible to introduce arbitrary size thresholds for the individual patches but this solution although formally acceptable may distort the ecological interpretation of the results obtained a relatively save pre conditioning would be dividing quasi linear polygons like rivers based on with to length ratio with the result that shorter stretches would maintain existing barrier effect but curb down too extensive longitudinal movement the landscape graph created at the first stage of our workflow topology building has an isomorphic structure with an input map of ecosystems in formal terms it represents a class of minimum planar graphs i e those in which a link cannot cross another link and each node is connected only to its topological neighbors fall et al 2007 galpern et al 2011 it is worth noting that planar graphs are becoming increasingly popular in connectivity analyses dale 2017 they are generated either directly on raster data rayfield et al 2016 or on the basis of polygons obtained in a different way most often as a result of delaunay tessellation of a landscape and thiessen polygons margosian et al 2009 however none of the known approaches has so far used a vector based complete mosaic of ecosystem patches as an input material for analysis thiessen polygons can be an input to graphscape in cases when original data comes from point sources other than that planarity issues are rarely relevant to our workflow the main output of the graphscape software is a minimum spanning tree which according to fall et al 2007 identifies the connectivity backbone of a landscape resulting patch set naturally forms an input to next level of analysis involving dispersal flux traversability distances and weights clusters of patches from the terminal set the sensitivity to node removal etc de cola 2010 rego et al 2018 in graphscape we implemented only few basic measures because the result set is easily exportable and may be further processed in other software systems we did not however resign from visualization because it is indispensable in experimental phase as well as important from a practical point of view as it enables decision makers to intuitively assess the spatial configuration which can among others support the spatial planning process foltête et al 2014 the principal measure of mst in landscape graph is transit density td is similar to betweenness centrality freeman 1977 in the logic of computation the difference being the elements counted for bc these are all to all paths for td only paths building mst edges bc reflects unconstrained transit possibilities and td transit possibilities constrained to mst and driven by efficiency principle high loads of bc can occur anywhere in patch mosaic while high loads of td only on paths connecting terminal set so generally with more compact spatial extent however for some ts layouts td distribution may be far from obvious and may reveal intricate overlapping re use of the same patches between branches of mst as we stressed earlier td is always related to particular mst so single instance of td data cannot be considered a general purpose connectivity indicator for the whole mosaic it does not mean td cannot be used to reach for more general conclusions provided more elaborate setup td is very strong count type measure allowing for variety of operations like addition subtraction or ratio without losing ecological meaning to start with td is suitable not only for counting single tree s like shown in section 3 5 3 but also multiple trees occurrences we anticipate two broad classes of use cases mobility oriented and landscape oriented in the first one the terminal set is fixed and the model is run multiple times against different parameters of mobility for instance patch resistance a series of mst readouts as measured by td reveals the response to mobility tweaking or in another variant gives averaged more relaxed version of shortest paths and minimum spanning tree the latter variant could be used to alleviate the sharpness of optimization algorithms in the second scenario the goal is to get insight into the landscape rather than mobile agent this time mobility factors are fixed and the model is run against different terminal sets we loosen the assumption that ts is a set of core or distinguished patches depending on the ts partition logic different levels of overall connectivity can be tested up to the maximum level of whole patch mosaic which would be the case when mosaic subsets are disjoint and complete in this setup it is possible to introduce a key variable driving the process of partitioning the results would then reflect how partition driver influences connectivity metrics obtained in this way will be similar in nature but not identical to numerous raster based measures of graph structure described in rayfield et al 2011 and dale 2017 based on topology in symmetric graphs which in direct or indirect way describe different aspects of landscape connectivity graphscape is quite flexible tool and depending on the entered data the scale and extent of the map and its classification scheme with additional external tables defining mobility factors it can be used for the different types of problems in the simplest case without considering mobility factors and determining only step statistics the result obtained is one of the landscape configuration metrics and can be compared with such generally known metrics as to the nearest neighbor distance or proximity index in more complex cases when mobility factors are involved most of the issues concern landscape connectivity and modelling of ecological corridors in particular graphscape can be used for planning of ecological corridors in the landscape for selected groups of plant and animal species identification of the most important areas from the point of view of general landscape connectivity on the basis of the occurrence of patches where different paths intersect transit density identification of the role of particular mobility factors in the movement of organisms by comparing the modelled networks of connections with the actual routes of animal movement identification of critical areas from the path network point of view by comparing the path model with the distribution of barriers assessment of patch isolation as a complementary problem to connectivity at present graphscape is focused on the concept of minimum spanning tree which makes the analysis of connections between terminal patches one to one however we are convinced that one to many approach is possible along with variety of other graph structures and we are going to explore them in the future going in this direction will inevitably make optimization questions more and more important graph structures arise from optimization techniques and these demand clearly stated goals therefore with increasing precision of tools attention must gravitate towards values priorities and beneficiaries software availability software was created in embarcadero delphi programming language was object pascal developer was wojciech pomianowski wpo twarda pan pl distribution size is under 10 mb additional hardware or software components e g java or net framework are not required to run graphscape no installation is required graphscape with supporting material is available for free download at https www igipz pan pl graphscape html provided is 64 bit native code executable running under microsoft windows 7 and later declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements graphscape software has been developed under the project 2012 05 b st10 02173 financed by the poland national science center the authors would like to thank the anonymous reviewers whose comments and suggestions greatly improved the original manuscript 
25936,while large scale low cost sensor networks are now recording air pollutant concentrations at finer spatial and temporal scales than previously measured the large environmental data sets generated by these sensor networks can become overwhelming when considering the scientific skills required to analyze the data and generate interpretable results this paper summarizes the development of an open source r package airsensor and interactive web application dataviewer designed to address the environmental data science challenges of visualizing and understanding local air quality conditions with community networks of low cost air quality sensors airsensor allows users to access historical data add spatial metadata and create maps and plots for viewing community monitoring data the dataviewer application was developed to incorporate the functionality and plotting functions of the r package into a user friendly web experience that would serve as the primary source for data communication for community based organizations and citizen scientists graphical abstract image 1 keywords community air monitoring citizen scientist low cost air quality sensor open source r package particulate matter pm2 5 data interpretation software availability the airsensor r package version 0 5 was developed by mazama science and south coast aqmd airsensor is free and open source software available through the github repository https github com mazamascience airsensor tree version 0 5 mazama science maintains the package as part of its ongoing relationships with federal state and local air quality agencies airsensor version 0 5 was first released in 2019 under general public license v3 0 gpl 3 0 and runs on windows unix and macintosh operating systems airsensor was written in r and program files are less than 5 mbytes airsensor is designed to be used with r 3 3 and rstudio the dataviewer shiny application was developed by mazama science and south coast aqmd dataviewer is free and open source software available through the github repository https github com mazamascience airsensorshiny the dataviewer was first released in 2019 under general public license v3 0 gpl 3 0 and runs on windows unix and macintosh operating systems dataviewer was written in r and program files are less than 7 mbytes the dataviewer requires git apache docker r and r shiny server 1 introduction a paradigm shift in air quality monitoring is occurring with citizen scientists able to develop hyper local community monitoring networks to supplement the established regulatory monitoring networks that are designed for regional monitoring snyder et al 2013 these environmental monitoring networks are increasing in complexity size and resolution both spatial and temporal due to technological advances and cost reductions for environmental monitoring hardware connected internet of things iot devices and cloud computing citizen scientists can take an active role in monitoring air quality at the neighborhood level by installing low cost air quality sensors lcs that collect and report air pollutant data particulate matter pm is an air pollutant that is categorized based on size with fine particulate matter pm2 5 defined as particles with aerodynamic diameter less than 2 5 μm the ability to record and visualize hyper local data in an intuitive and informative interface will likely spawn an increase in interest and interaction with environmental data sets due to the locally relevant nature of the information on the other hand non intuitive or limited user interfaces and confusing user experiences may discourage citizen scientists from interacting with the collected data the increasing complexity size and resolution of today s environmental monitoring networks have created big data challenges leading to the emergence of a new field of study environmental data science gibert et al 2018 data science combines computer programming skills math and statistical knowledge and subject matter expertise conway 2013 free open source software foss platforms play a vital role in the progress of research towards developing new methods for addressing environmental data science challenges the r environment and python are two foss programing languages that are often used in environmental data science applications kadiyala and kumar 2017a 2017b open access to environmental data sets and related tools is foundational for environmental data science to thrive and develop environmental monitoring data can be considered open access when the data is available through a stable and consistent application programming interface api that allows software and application developers to build applications to display and report that data in transparent and meaningful ways environmental data scientists can access regulatory data via open api s e g airnow api openaq api to create custom web applications for displaying air monitoring am data airnow 2020 openaq 2020 these am data viewing websites are useful and provide information to the public at varying granularity spatially and temporally two examples of data viewing websites include the openaq map and the world s air pollution real time aqi waqi map which both display international air quality monitoring data openaq 2020 world air quality index project 2020 openaq uses a color scale fig s1 in the supplemental information si that deviates from the common air quality index aqi color scale to display air pollution concentrations a special feature in the waqi website is their use of calendar plots to display am information data viewing websites that display modeled or interpolated air pollutant or aqi values are also available breezometer 2020 iqair 2020 plume labs 2020 when displaying data from both regulatory grade instruments and lcs the source and type of data displayed should be readily apparent a lack of differentiating and identifying data sources may cause confusion for the end user especially if the lcs do not agree with nearby regulatory grade instrumentation with interpolated or modeled maps often the user is not readily aware of the input parameters used to model air quality data when viewing modeled air pollution information the viewer should be cautious especially when data sources are not readily apparent and input parameters whether defendable or questionable for the data model are unknown to the end user viewer hagler et al 2018 broadly the available sensor data viewing platforms are map centric with point values or interpolated modeled data displayed with options for viewing recent time series data resources for accessing and displaying data collected from networks of lcs are available though they vary in terms of software foss or proprietary what they provide and whether they are provided by the manufacturer a project team or through a citizen science model while many sensor manufacturers have software and platforms in place for ingesting storing and analyzing data that is generated from their respective sensors these are often proprietary and offered as a software as a service saas or platform as a service paas requiring accounts with monthly or annual subscriptions costs in contrast to the saas and paas business model several sensor resources are available for open access viewing of data collected from lcs networks these platforms include but are not limited to the habitatmap aircasting map air quality egg portal luft daten project map purpleair map smart citizen kit map and the uradmonitor network map air quality egg 2020 habitatmap 2020 luftdaten 2020 purpleair 2020 smart citizen kit 2020 uradmonitor 2020 purpleair provides open access to the data collected by the purpleair network of sensors through an api and provides open viewing and downloading of sensor data through the purpleair map the luft daten project is a citizen science project with lcs reporting to a map and invites programmers to collaborate in this foss development through github ok lab stuttgart 2020 when selecting a sensor in either the purpleair or luftdaten gui the user is currently limited to viewing only the last seven days of data in a time series plot and current data on the map accessed january 2020 to gain an understanding of the historical local am data the user is required to download process and visualize the data from these networks on their own which may be a limiting factor to those without the environmental data science skills needed to perform such analysis these sensor specific online resources for viewing sensor data often do not include the regulatory am data that may be publicly available through the airnow or openaq api and often do not indicate what if any quality control qc measures are taking place on the collected data before displaying publicly for community members to understand local air pollution trends a more in depth analysis of historical data is required while map centric guis work well for viewing real time data communities that monitor air quality in long term deployments need additional plotting and viewing capabilities to access and understand their local historical am data a data dashboard for viewing and analyzing historical data would provide citizen scientist with a better understanding of local air pollution levels particularly spatial and temporal air pollution trends for those with varying levels of technical data science programming skills several software resources are available that support individual data analysis of air quality data if data can be organized and loaded into a software system then a more in depth analysis can occur and custom visualizations can be produced foss software packages have been developed in the r and python environments specifically for accessing and visualizing freely available am data these include the r packages openair pwfslsmoke ropenaq and raqdm openair provides a useful package for developing visualizations from collected am data with functions to create calendar plots scatter plots and time variation plots along with wind roses pollution roses and bivariate polar plots if wind speed and direction data is available carslaw and ropkins 2012 carslaw and beevers 2013 if we use advanced analytical tools and access am data directly then we can facilitate more organized robust systematic and repeatable data processing analysis and visualization of lcs data furthermore using foss tools allows for increased iteration and development an example of this workflow would be the pwfslsmoke r package and the associated pm2 5 am web application developed as part of the airfire tools by the u s forest service usfs wildland fire air quality response program wfaqrp callahan et al 2019 air fire tools 2020 these tools were developed to access regulatory grade am data via the airnow api and display that data graphically to assist the usfs air resource advisors to gather air quality data and create air quality reports during wildfire smoke events the pwfslsmoke r package provides functions to download parse and plot am data and provides the back end software necessary to generate plots for displaying on the front end web application a similar model in which an r package is used for accessing and processing lcs data would save users time and would allow the development of custom functions for different approaches to qc and more complex historical data analysis which are gaps we see in the current offerings additionally the r package could provide the back end software to support a front end web application to display historical am data to provide communities with more useful analysis and visualizations of historical data this web application would allow community members to answer questions about their local environment which are not readily answered with the current offerings of real time maps with limited historical data analysis the objectives of the software development associated with this project were to build an foss r package and data viewer web application that would address the challenges identified with the data management and visualization of lcs networks deployed within the u s epa science to achieve results star grant project this paper summarizes the development of an r package and web application designed to address the environmental data science challenges created by deploying 400 lcs in 14 different communities we wanted an open source r package that would allow users to download sensor data add spatial metadata perform data fusion with other relevant data sets and create maps and plots for viewing data collected by am sensors we also wanted the package designed with functions so that minimal coding would be required to complete tasks understanding that many would prefer to interact with an online web application we wanted to build an application that would provide an interactive data experience allowing users to make selections and explore the community am data sets by generating pre defined data visuals based on their user input selections the south coast air quality management district south coast aqmd collaborated with mazama science to develop the r package airsensor and web application airsensor dataviewer dataviewer to meet these software development aims 2 methods software design and characteristics 2 1 community engagement in 2016 south coast aqmd was awarded a u s epa star grant titled engage educate and empower california communities on the use and applications of low cost air monitoring sensors under assistance agreement no r836184 south coast aqmd has engaged 14 california communities through a series of workshops to introduce the project provide technical guidance on sensor technology and deployment siting installation configuration and registration of air quality sensors review deployment progress and examine community data sets and provide software tools and resources for citizen scientists to engage with collected data sets and create informative data visualizations roughly 400 purpleair pa ii sensors purpleair llc usa were distributed to community members the on going engagement with the star grant sensor communities sgsc has provided the motivation to develop software tools to enhance the community members ability to interact with historical data and extract meaningful information about their local environment participants were not engaging with the data that often as is supported by the survey data which is most respondents only check their air quality data sometimes 36 as opposed to often 17 and everyday 5 in person discussions provided useful context to help us understand this by 1 reporting that data was difficult to access and download especially historic data and 2 sharing what they wished to do with the data for example after displaying a static time of day bar chart showing the diurnal pm2 5 trends during a community workshop one community group leader asked how do i generate that plot on a regular basis and share with my community members in one sgsc a sensor host wanted to know the best time of day to walk their dog to reduce their exposure to particulate pollution additionally multiple participants from different communities shared their difficulty downloading and analyzing the publicly accessible pa ii data especially with regards to the time date reformatting required for plotting in microsoft excel the survey responses along with the discussions with community members on the data science challenges provided the motivation to build additional software tools to address the difficulty and challenges posed by analyzing these large community am data sets increasing the number of data sharing events with effective data visualizations should provide participants with a better understanding of the principles of air quality their local air pollution and the proper use and application of lcs sandhaus et al 2019 table s1 in the si provides a summary of the environmental data science challenges that are addressed in this project 2 2 software tools r environment rstudio r packages and shiny the r environment is an integrated suite of software facilities that is designed on a simple yet effective computer programming language r the r environment provides tools and functions for data processing storage calculation and graphical display since r is designed essentially on a computer programming language users are able to add further functionality to existing packages by defining new functions and developing packages of functions the r environment 2019 rstudio a public benefit corporation provides a foss version of an integrated development environment ide for r which supports code execution debugging and workspace management rstudio 2019 allaire 2020 instructions for installing r and rstudio can be found on the web and in the literature kadiyala and kumar 2017b the fundamental unit of shareable code in r is a package packages bundle together r code data documentation and tests packages are sharable on the comprehensive r archive network cran which is the public clearing house for r packages cran hosts a wide variety of foss packages that allow researchers to collaborate and build upon already developed r code the development of airsensor built upon r packages available on cran most notably mazamaspatialutils openair pwfslsmoke and worldmet airsensor is designed to be used with r version 3 3 this paper describes version 0 5 of the airsensor package which is available on github the latest or master branch of airsensor is also available on github the airsensor package can be installed using the devtools package within r using the following code image 2 shiny is a foss r package that provides a framework for building interactive web applications shiny allows the user to turn r derived analysis and plots into interactive web applications without requiring html css or javascript programming shiny allows for the development of a web application for viewing and sharing data analytics since not all users would be comfortable using the r environment which does require coding r shiny was used to develop the dataviewer web application to provide an interactive data experience for community members that would prefer to interact with the sensor data in a web application rather than in the r programming environment 2 3 airsensor r package rather than describing each individual function in airsensor the following examples will showcase the three primary data objects available through the package how to apply quality control measures on the imported data and how to generate plots for each of the data objects a complete guide to airsensor functions and operations can be found within the r environment after the package has been loaded helpful r vignettes are also available within the package to provide the user with code examples for using the airsensor functions and working with the sensor data 2 3 1 data access extraction and data objects overview airsensor currently accesses data generated by purpleair sensors by collecting real time data from www purpleair com json and historical data from a thingspeak representational state transfer rest api extracted data is enhanced with spatial metadata and transformed into efficient data objects for downstream analytics the three primary data objects are the purple air synoptic pas purple air timeseries pat and airsensor sensor data objects functions exist for creating or loading data objects as well as manipulating and visualizing them an overview of the airsensor r package data access data objects and functions is provided in fig 1 after installing or loading the package a data archive repository can be set to access archived data data archives can be created that for specific sensor networks e g sgsc or for a specific geographic area e g southern california so that the r user can access and load historical data more efficiently from an archive rather than the thingspeak rest api the data archives developed for the sgsc are kept current with cron jobs cron jobs are time based jobs that can run commands at specific time intervals that are scheduled to run every hour to pull and add the most recent data to the archive the data archive for the sgsc is accessible at http smoke mazamascience com data purpleair and includes historical data starting from october 01 2017 a base archive can be set in airsensor by the following code image 3 2 3 2 purple air synoptic data object the purple air synoptic pas data object provides an instantaneous view of the measured values from a network of sensors a pas can be created from the json data available at www purpleair com json or can be loaded by accessing a data archive fig 1 at the time of this writing the time resolution of the pa ii sensors is 120 s and therefore a new pas data object would be available roughly every 120 s the available functions for manipulating pas data object include pas filter pas filterarea and pas filternear the pas data can be plotted on a map to display the instantaneous data collected by the sensor network with the pas leaflet and pas staticmap functions fig 2 shows a pas data object displayed on an interactive map using the pas leaflet function which maps sensor locations and colors the locations according to aqi the map is interactive in that the user can select an individual sensor and view the values recorded at that location for the time the pas object was created if a user is interested in loading specific states or air districts the user can apply filters when generating the pas data object the leaflet map can be modified with options for map tiles parameter displayed and what type of sensors to display i e inside or outside sensors fig 2 was produced by the following two lines of code image 4 2 3 3 data fusion enhancements data fusion with other relevant data sources provides benefits for custom analytics for performing data quality checks and for providing information on local weather conditions data fusion provides the ability to tell a more complete story about local air pollution by fusing collected sensor data with other publicly available data sets airsensor has been integrated with the pwfslsmoke r package for access to regulatory am data via the airnow api and integrated to the worldmet r package for access to the u s national oceanic and atmospheric administration noaa integrated surface database for meteorological data callahan et al 2019 carslaw 2019 these data fusion enhancements provide the ability to generate comparison plots between a lcs and the nearest regulatory grade instrument and allow for sensor data to be joined with nearest meteorological data so that wind roses pollution roses and bivariate polar plots can be generated to provide insights into local air pollution trends data fusion enhancements are performed on both the pat and sensor data objects 2 3 4 purple air timeseries pat data object and quality control functions the pat timeseries data object provides timeseries data on a per sensor basis data manipulation functions for the pat data object include filtering sampling and joining a pat can be loaded from a data archive using the pat load function or can be created from the purpleair thingspeak api with the pat createnew function the code example below loads a pat data object for a sensor in seal beach ca that was deployed as part of the sgsc deployments the pat data object includes data from january 01 to december 31 2018 subsequent example code and plots displaying the airsensor functions will be performed on this pat data object or a filtered pat data object created from the scsb 20 sensor the pat data object can be loaded into the r environment and filtered by date with the following code image 5 pat data objects can be processed for time averaging qc algorithms and outlier detection for removal or replacement the user can create their own framework for applying qc functions depending on their project requirements the pat aggregate function returns a data frame with aggregate statistics which are helpful for building out qc algorithms the aggregate statistics include the mean median standard deviation minimum maximum and count for the aggregate time period chosen note that the pa ii sensor node is manufactured with two identical oem original equipment manufacturer pm sensors model pms 5003 plantower china that report the same types and amounts of data and for reference purposes are labeled as channel a and channel b respectively for the paired channel a and b pm2 5 data columns the pat aggregate function also returns the t test statistic based on an unpaired two sample student s t test p value and degrees of freedom several built in qc algorithms are available in airsensor and are labeled as pat qc hourly ab 01 and hourly ab 02 the pat qc function allows the user to perform a first level qc check for values that are considered out of spec with regards to the manufacturer defined specifications for the acceptable ranges for pm2 5 temperature and humidity the purpleairqc hourly ab 00 function allows the user to perform an hourly average of the a and b sensor channels when sufficient sub hourly data exists for both channels within an hour the default min count for sub hourly data is set to 20 data points requiring a data recovery for a and b channels 66 for the current time resolution at 120 s no further qc is applied with this function note that the pa ii s time resolution has changed with firmware updates over time as firmware updates have not been performed across the board simultaneously for all sensors in the purpleair network the following dates are estimates for firmware releases and data resolution time resolution for data prior to february 2017 is 20 s from february 2017 to march 2017 is 40 s from march 2017 to may 2017 is 70 s from may 2017 to may 2019 is 80 s and data recorded after may 2019 is 120 s the function purpleairqc hourly ab 01 allows the user to perform an hourly average of the a and b sensor when sufficient sub hourly data exists and when data is considered statistically similar data is invalidated when 1 minimum count 20 values 2 when both the means of channels a and b are not statistically the same two sample t test p value 1e 4 and the mean difference between channels a and b is greater than 10 μg m3 and 3 when the mean difference between a and b is greater than 20 μg m3 for pm2 5 values less than 100 μg m3 these conditions assume that the air entering the channel a and b sensors is the same and therefore the means of the two channel measurements should be statistically similar when measurements from these sensor channels agree the user can have higher confidence in the lcs air quality measurements and the subsequent data averaging of the two oem sensors into one value the two sample t test is a statistical technique to determine whether the difference between two means is significant the default settings of these qc checks can be modified to adjust the qc check to individual project requirements additionally new qc functions can be created and airsensor users are encouraged to create their own custom qc functions and submit these functions to be added to the airsensor package through github the purpleairqc validationplot function creates a series of timeseries plots for channel a and b the difference between channel a and b t test p value min count and the hourly averaged final output fig 3 the airsensor pat outlier function provides an outlier detection function that allows the user to apply a rolling hampel filter to identify points that may be outliers and if desired replace those identified outliers with a rolling median value the hampel filter is an outlier detection technique that uses the median absolute deviation mad for each data point a median and standard deviation are calculated using neighborhood values within a sample window size if the mad of a single data point is a specified number of standard deviations threshold minimum from the median value for the sample window then the data point is flagged as an outlier the default values for the pat outlier function set the sample window 23 and the threshold minimum 8 adjusting the default parameters on the function for identifying outliers would adjust the number of points detected as outliers fig 4 provides an example of the pat outlier function with the potential outliers identified as red asterisks in this example a date filter was applied to the pat example previously generated to only include the june 27 to july 8 2018 time period that would be impacted by a special event 4th of july fireworks the outlier detection function appears to identify many of the one off high values as outliers but does not consider the elevated pm2 5 concentrations due to the fireworks to be outliers this function allows airsensor data users to quickly implement an outlier detection technique and visualize the results of their outlier detection function fig 4 was produced by the following r code image 6 data visualization functions for the pat include plotting raw data time series interactive time series multiplot time series a b temp rh comparison plot for channel a vs b and a comparison plot with regard to the nearest regulatory pm2 5 monitor the channel a and b pm2 5 timeseries data can be compared using the pat interalfit function as shown in fig 5 for scsb 20 the a and b sensors agree with each other with an r2 0 98 slope of 1 05 and an intercept of 0 8 since the two sensors perform similarly for 2018 time frame the blue timeseries points representing the b sensor are plotted over top of the red points representing the a sensor the code to generate the plot is image 7 the pat scatterplot function provides a multi panel scatterplot for variables in the pat data object with an example of the plot shown in fig 6 this plot allows the researcher to determine if there is a lack of correlation between the a and b sensor channels or if there are higher than expected correlations between pm2 5 concentrations and weather conditions temperature and humidity this plot also provides the timeseries and distribution of data points for pm temperature and humidity in fig 6 the distribution plots for the a and b sensor channels indicate pm2 5 concentrations for this sensor are typically less than 25 μg m3 the datetime column provides an indication of periods of downtime with a noticeable downtime seen in august and september of 2018 a sensor can also be compared to the nearest regulatory air monitoring station ams with the pat externalfit function fig 7 in this example the sensor is 3 1 km away from the regulatory ams equipped with a met one beta attenuation monitor bam which is a u s epa designated class iii fem eqpm 0308 170 for pm2 5 the time resolution of the regulatory pm2 5 data is hourly to match lcs data with the regulatory data this function uses the qc procedures previously described to hourly aggregate the sensor data the user can specify which qc algorithm to apply or create custom qc functions fig 7 indicates that while the sensor follows the typical daily pm2 5 trends of the nearby regulatory grade instrument for pm2 5 with r2 0 73 the sensor tends to estimate higher concentrations than the regulatory grade instrument this slope intercept offset could be due to a local emission source impacting this particular sensor location or could be due to sensor measurement bias error that has been identified in prior publications feenstra et al 2019 magi et al 2019 for the time series in fig 7 the purple colored points represent the 1 hr purpleair sensor data and the black colored points represent the regulatory grade instrument data if the two agree closely within an hour the black point would be plotted on top of the purple point for that hour the plot in fig 7 is created with the following image 8 the pat dygraph function returns an interactive time series plot for both channel a and b allowing the user to zoom in out and investigate date times when pm2 5 concentrations may be higher than normal fig 8 using the interactive time slider located below the plot allows the user to quickly zoom in to further investigate dates and times with particle pollution events with a small amount of code the dygraph provides a versatile interactive plot where the user can explore a large amount of data at customizable levels with the time slider and zoom in out features fig 8 is created with the following image 9 2 3 5 hourly qc data object sensor the sensor data object is generated on a per sensor basis from a pat data object with the pat createairsensor function the user will need to specify a pat data object time averaging period parameter channel qc algorithm and minimum count the qc algorithms applied in creating the sensor data object are described earlier in section 2 2 4 with regards to the qc functions that can be applied to a pat timeseries data object the functions for sensor data objects begin with sensor an example creating a sensor data object is shown in the code below image 10 plots available for the sensor data object within the airsensor package include a bivariate polar plot and pollution rose which wrap functions from the openair r package the meteorological data used to generate these plots is retrieved from the noaa worldmet r package the bivariate polar plot and pollution rose which are shown in fig 9 and fig 10 respectively provide the user with the ability to couple wind direction and wind speed with pm2 5 pollutant data to determine whether pollution events can be attributed to specific meteorological conditions and potentially identify pollution sources a more in depth analysis of these plots and their application in analyzing am datasets is accessible within the published literature on the open air r package development carslaw and ropkins 2012 and use of bivariate polar plots carslaw and beevers 2013 grange et al 2016 the pollution rose and polar plot are generated by the following code image 11 2 3 6 timestamp and time averaging for airsensor data objects and functions airsensor and airsensor functions have been designed to appropriately handle timestamps and various time zones of potential users users should understand how time stamps are stored and visualized within airsensor and take appropriate steps when creating and visualizing airsensor data objects especially if using plotting functions outside of the airsensor package to visualize data the purpleair api provides access to data stored in coordinated universal time utc the airsensor data objects pas pat and sensor all store data with a utc timestamp when creating or loading either a pat or a sensor data objects the user can specify the local time zone of the sensor selected if a time zone is not specified when creating a data object for a single day the date time parameters will be passed as utc which for a sensor located in the pacific time zone 8h utc would return a data object with data from 08 00 am to 08 00 am local time of the following day in airsensor time stamps are labeled and time averages are coded as time beginning for example a 1 hr time average with a timestamp of 14 00 would be an average of the data collected between 14 00 and 14 59 this holds true even with the 2 min time matched channel a and b sensor data available in airsensor the purpleair pa ii channel a and b sensors report at different times within a 120 sec time interval in airsensor the seconds are dropped and data from the a and b sensor are assigned to a 2 min time beginning time stamp for matching purposes between the two oem sensors within a pa ii sensor since data is stored as utc the plotting functions within airsensor are coded to appropriately apply time shifts based on the sensor s location time zone so that data will be plotted and displayed in the local time of that sensor s location 2 4 airsensor dataviewer web application 2 4 1 airsensor dataviewer overview the dataviewer application was developed to provide an online interactive data experience for the sgsc networks these communities and sensor names are listed in si table s2 this interactive web application provides access to the functionality of the airsensor r package citizen scientists that would not be able to download r and run code or scripts to access process and visualize community data are now able to visualize their community data through the dataviewer while the infrastructure to generate the types of plots that had resonated with community group members during the workshops was developed in the airsensor package the ability for community group members to use that infrastructure and generate visualizations in an interactive web application without writing a single piece of code is provided in the dataviewer application plots that generated the most interest with community group members including calendar plots concentration maps community time lapse videos and sensor performance plots between the a and b internal sensors and between the sensor and nearest reference pm2 5 monitor were prioritized for incorporation in the dataviewer the following sections will provide an overview of the back end infrastructure required for the dataviewer application and the methodology for the dataviewer color scale and timelapse videos the front end of the dataviewer which is the online web application and the primary point of interaction for community members is highlighted in the results section 2 4 2 cloud computing resources cloud computing provides computing services over the internet using a pay as you go pricing model computing services typically include computing power storage networking and analytics cloud computing can provide benefits by allowing programmers to focus on building new and innovative applications rather than acquiring and maintaining the infrastructure required for their computational needs the cloud can provide benefits with cost reductions for it infrastructure and can increase the scalability elasticity reliability and security of computational services in comparison to computation services provisioned locally or on premise azure which is microsoft s public cloud computing platform was used to support the computational requirements of the dataviewer application the application could also be run on another public cloud platform or on premise if desired the computation services required include running scheduled tasks cron jobs for creating data objects storing data in structured data directories and hosting the dataviewer application the data archive consists of a set of flat files defined by a simple directory and naming protocol with the data ingest scripts written in the r programming language a virtual machine vm was configured on azure with the structured directories for the data directories along with required software i e git apache docker and r a second vm was configured to host the dataviewer application fig 11 provides a simplified system architecture for the dataviewer application 2 4 3 dataviewer color scale determining an appropriate color scale for pollutant concentrations generated by lcs is challenging historically air quality has been colored according to the aqi with values ranging from 0 to 500 with six distinct color categories good green moderate yellow unhealthy for sensitive groups orange unhealthy red very unhealthy purple and hazardous maroon historically aqi has been calculated at 24 h averages due to the scientific information about air pollution exposure and public health in 2013 the u s epa released a new aqi calculation method nowcast reff method for pm2 5 that calculates aqi hourly based on the previous 12 h with the most recent hourly pollutant concentrations given larger weighting when air quality is changing rapidly mintz et al 2013 the u s epa in the air sensor toolbox suggested a new pilot version color concentration scale that could be used for 1 min high time resolution data from lcs u s environmental protection agency 2019 this scale uses four shades of blue for low medium high and very high pm2 5 concentrations and is shown in si fig s2 the scale from the airsensor toolbox was created for 1 min sensor data in contrast to this work in which lcs data is processed with qc algorithms and time averaged to 1 hr concentrations prior to being displayed in the dataviewer application furthermore the authors wanted to provide users with a clearer differentiation among the higher pollutant levels sometimes indicated by the sensors hence a new color scheme was developed for the dataviewer that includes 5 concentration categories represented by two colors blue and purple with variations in the hue and luminance as shown in table 1 2 4 4 ff mpeg and digital stills and video stills creation one of the desires of the community groups was to view historical time lapse concentration maps to view past air quality events in their communities to accomplish this task cron jobs run hourly to create video still images for each of the 14 sgsc these images are stored in the structured data directory in sequence and converted into mp4 video files using ffmpeg which is a foss dawes 2019 ffmpeg 2019 3 results 3 1 airsensor package the airsensor r package meets the community needs for those desiring to work with purpleair lcs data programmatically in the r environment through the airsensor package real time and historical data from the sgsc listed in si table s2 can be accessed loaded into r and visualized used pre built plotting functions these plotting functions allow the user to create useful and interactive plots that can be shared within a community group and deliver actionable information for the community members to answer questions like when is particle pollution highest in my community and what time of day or day of week would be best to plan an outdoor activity i e walk dog or golf game to potentially reduce my particle pollution exposure airsensor creates a data flow for the end user to create data objects for synoptic data time series data and qc hourly pm2 5 data with the functions of this r package highlighted in the methods section the user can easily create informative plots for community members to understand their local historical air quality trends with minimal coding required the airsensor r package and associated functions provide the necessary back end software analysis and plotting functions to create the front end dataviewer web application the dataviewer is usable and useful to a much broader segment of the public and is the primary point of interaction for community members to gain insights into their local air quality conditions this solution provides an example of how these types of tools and solutions can enhance public engagement with data from lcs networks 3 2 dataviewer application 3 2 1 user interface tabular structure and plotting features the dataviewer application version 0 9 7 has a hierarchical page and tab structure with 4 top level pages explore view data latest data and about the view data page is for viewing tabled data and provides the ability to download data in 3 to 30 day intervals on a per sensor basis for sgsc historical data can be accessed back to the start of the sgsc deployments october 01 2017 the view data page includes high resolution 2 min time matched pa ii pm2 5 data from the a and b sensor channels temperature f and relative humidity this data output provides the user with a clean time matched data set for the a and b sensor creating a similar data set outside of the airsensor r package or dataviewer application would likely be time consuming and difficult especially if the user were not proficient with microsoft excel or data science environments the latest data page provides visual access to the latest non qc data on a per sensor basis with timeseries plots provided for sensor channel a channel b humidity and temperature the about page provides an overview of the dataviewer its intended purpose qc procedures and a disclaimer message 3 2 2 explore page tabs and functionality the explore page has the most functionality for exploring and analyzing community am data and includes six tabs overview calendar raw data daily patterns compare and timelapse in the overview tab the user can select a community a single sensor sensor name a date end date and view past data with options for viewing the prior 3 7 15 or 30 days to the selected end date the overview tab fig 12 provides a map that displays the average pm2 5 for all sensors within the selected community for the time period selected 3 7 15 or 30 day average and a bar chart that displays hourly pm2 5 concentrations for the selected sensor this overview tab provides the user with access to historical pollutant concentrations for the user selected timeframe for their community and individual sensor by changing the date the user can quickly identify spatial differences between locations since the map indicates an average pm2 5 concentration for the entire timeframe chosen 3 30 days additionally the user can quickly scan the bar chart for when higher than typical pm2 5 concentrations were recorded for a particular sensor in the calendar tab a 1 year calendar plot is rendered for a single selected sensor the user selects a community sensor and date with a calendar plot being generated for the entire calendar year of the date selected fig 13 the calendar plot is interactive and when the user hovers over a date the 24 hr averaged pm2 5 concentration is displayed the calendar plot is easily understood by community members and provides an intuitive view of a complete year of pm2 5 data for a single sensor the calendar tab is great place to start when exploring a community data set to find dates with atypical 24 hr pm2 5 concentrations the user can then further examine these atypical pollution events at higher time resolution with other tabs available within the explore page the calendar plot especially resonated with the community members and sensor hosts and therefore was a priority for inclusion in the dataviewer application calculating and rendering the calendar plot is computationally expensive and may take a few moments to display when interacting with the dataviewer application but the result is well worth the wait for this informative plot throughout the workshops we received the most feedback and discussion from community members when showing the calendar plot the calendar plot triggered the audience and facilitated effective discussions during community workshops community members who would be more silent or could not recall as to what might have caused poor air quality in their community during the past several months were able to identify days with poor air quality and what might have caused them when they viewed the calendar plot with the color coded concentrations the raw data tab provides the raw time series data for channels a and b humidity and temperature below the time series plots the raw data tab provides a comparison between the channel a and b sensors with both a time series and a scatterplot that indicates the regression statistics between the channel a and b this functionality uses the pat internalfit function from the airsensor r package which was previously shown in fig 5 these comparison plots provide the user with the ability to check on the performance of an individual sensor by viewing how well the two internal raw sensors within the pa ii agree for the selected time period if a user is concerned with the performance of an individual sensor this tab can be used to determine if both the raw sensors are responding to changes in particle concentrations similarly low correlation and or a large slope intercept offset are indicative of a sensor performance issue and that one or both sensors may be experiencing a malfunction the daily patterns tab fig 14 provides a bar chart illustrating the diurnal trend for pm2 5 a pollution rose and a summary table for the noaa weather data for the date range selected the daily patterns bar chart provides the average concentration by hour of day with this tab the dataviewer user can determine on average what hour of the day has the highest and lowest particle pollution this plot helps to inform users as to historical trends within their community and provides information that the community member can infer what time of day may be best for scheduling physical activity to reduce particle pollution exposure based on historical air pollution trend data the pollution rose allows the user to determine if pollution can be attributed to specific meteorological conditions the compare tab provides a comparison between the sensor data and the nearest ams equipped with a continuous regulatory pm2 5 instrument the compare tab provides a map indicating the location of the sensor and nearest ams along with a timeseries and scatter plot comparison for the two data sources allowing the user to determine if the selected sensor follows the typical trends for pm2 5 recorded at the nearby regulatory ams for the date range selected the dataviewer application is using the airsensor pat externalfit plotting function which was shown prior in fig 7 while the distance between the regulatory monitor and the lcs is provided on the sensor monitor comparison timeseries plot the map provided in the dataviewer on this tab allows the user the opportunity to visualize the distance between and the spatial context of the two monitoring locations understanding the siting of the lcs and the regulatory ams is crucial to understanding the information provided by the comparison plot if either the sensor or regulatory monitor is installed in a near source environment i e near road the user should not expect the two measurements to agree the final tab in the explore page is the timelapse tab this tab provides the user with the ability to generate a 6 day timelapse pm2 5 concentration video on a per community basis fig 15 right clicking on the video allows the user to save a mp4 video to their computer and share if desired this timelapse concentration map allows the user to view pollution events that may have taken place within a community during a selected time frame and visualize the flow of pollutants through a community an informative approach to using this timelapse video is first to use the calendar plot feature to identify dates with elevated pm2 5 mass concentrations μg m3 after identifying those dates the user can then choose an inclusive date range to view the community timelapse to better understand the pollution event 4 discussion while online systems exist to view real time and recently recorded measurements foss tools for accessing processing and analyzing historical am data collected by lcs are less available to the public developing foss tools for archiving interpreting and communicating data from sensors has been identified as a concrete next step towards building a system for filling the air quality data gap pinder et al 2019 this work provides a foss r package and a web application designed to fill that gap by providing the software tools to view both real time and historical hyper local air quality information generated by lcs networks access to hyper local air quality information is expected to spawn an increased desire to interact with air quality information and allow community members to take appropriate actions based on results generated from their community monitoring networks the airsensor r package and dataviewer application provide a framework and data flow for communities to transform their community monitoring data sets into insightful information through interactive data experiences and data explorations when meaningful results and observations are formulated community members can take appropriate actions to reduce their exposure to air pollutants these actions could include planning transportation e g walk bike motor vehicle routes to reduce air pollution exposure and scheduling physical activity events e g golf game sporting practice sporting event during hours of the day or day of the week that have been identified to have lower pm2 5 pollution based on historical data analysis our experience with sharing the dataviewer with the community leaders and members participating in the project has been positive with users enjoying the interactive data experience provided within the dataviewer these community members have shared how this dataviewer provides them with the analysis capabilities to better understand their local air quality conditions plots that previously seemed out of reach due to required technical data analysis skills and coding experience are now readily available and generated with only a few selections and mouse clicks within the dataviewer application foss software developments provide efficiency by building a community of proactive data users around shared tools and allowing for multiple parties i e agencies entities individuals to contribute to software development and enhancing software functionalities this benefit has already been realized as with the usfs airfire group funding further developments to airsensor for functions to calculate state of health metrics designed to categorize whether sensors are functioning properly this information will be used in the context of wildfire air quality response foss allows for researchers to collaborate and build upon the foundation established in this development foss developments can also provide a high level of transparency in terms of data analysis and integrity as the end user is able to select which post processing steps are appropriate for their data analysis with foss tools and publicly available data sets researchers can reproduce data analysis techniques and develop additional functions with the interoperability associated with foss development 5 conclusions this novel work brings these software systems to the end users or community members in a foss format with all the advantages of open software developments not only is the end user able to access process and analyze historical sensor data but the user also has access to the source code and functions with the option to create their own custom functions for qc filters and advanced analytics allowing the community to build upon this existing work provides benefits to the sensing community as a whole developing this software in the r environment also provides for data fusion enrichment by coupling the collected am sensor data with meteorological data and regulatory am data through other open source packages in the r environment the airsensor package has established a foundation upon which further enhancements and refinements can be developed both airsensor and dataviewer source codes are available on github and the authors invite collaboration and input to help shape the airsensor open source project to best meet the needs of the air sensing community the airsensor r package is sensor specific working with any publicly registered purple air pa ii sensors the dataviewer solution is both sensor and project specific and therefore limited to the pa ii sensors deployed by south coast aqmd in sgsc the authors believe that the data flow works well for am sensor data with the data objects going from synoptic data to time series data and then to hourly qc sensor data the blueprint developed to make the dataviewer operational could be applied to other projects and communities to visualize data collected by their purpleair lcs networks the work discussed in this paper focused on the initial data handling and analysis capabilities required for a community am network of pm2 5 sensors planned future work will focus on several improvements to the airsensor r package the data archive database design and the dataviewer application the airsensor r package and archive will be improved by adding functionality to handle unique timeseries identifiers and incorporating pm1 and pm10 data additional plotting functionality will include enhancements to create multi sensor comparison plots and visualize sensor state of health metrics for both individual sensors and sensor networks additional enhancements to the r package may include developing models to provide hyper local air quality forecast for the community the dataviewer will be enhanced by improving the appearance usability data handling and performance of the application funding this research has been supported by a grant from the u s environmental protection agency s science to achieve results star program to the south coast air quality management district disclaimer statement this publication was developed under assistance agreement no r836184 awarded by the u s environmental protection agency to south coast aqmd it has not been formally reviewed by epa the views expressed in this document are solely those of the authors and do not necessarily reflect those of the u s epa the south coast aqmd and u s epa do not endorse any products or commercial services mentioned in this publication declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank dr jonathan callahan and hans martin at mazama science inc seattle wa for their collaboration and contributions in the development of the airsensor r package and the dataviewer application tools along with their valuable feedback on this manuscript the sensor data used and presented in this paper was collected by the air quality sensor performance evaluation center aq spec at south coast aqmd the authors would also like to thank the community groups leaders trainers coordinators and members sensor hosts that participated in the u s epa star grant and provided valuable feedback that allowed us to create and improve this work the authors thank ms emma ranheim who assisted in user testing the airsensor r package appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104832 
25936,while large scale low cost sensor networks are now recording air pollutant concentrations at finer spatial and temporal scales than previously measured the large environmental data sets generated by these sensor networks can become overwhelming when considering the scientific skills required to analyze the data and generate interpretable results this paper summarizes the development of an open source r package airsensor and interactive web application dataviewer designed to address the environmental data science challenges of visualizing and understanding local air quality conditions with community networks of low cost air quality sensors airsensor allows users to access historical data add spatial metadata and create maps and plots for viewing community monitoring data the dataviewer application was developed to incorporate the functionality and plotting functions of the r package into a user friendly web experience that would serve as the primary source for data communication for community based organizations and citizen scientists graphical abstract image 1 keywords community air monitoring citizen scientist low cost air quality sensor open source r package particulate matter pm2 5 data interpretation software availability the airsensor r package version 0 5 was developed by mazama science and south coast aqmd airsensor is free and open source software available through the github repository https github com mazamascience airsensor tree version 0 5 mazama science maintains the package as part of its ongoing relationships with federal state and local air quality agencies airsensor version 0 5 was first released in 2019 under general public license v3 0 gpl 3 0 and runs on windows unix and macintosh operating systems airsensor was written in r and program files are less than 5 mbytes airsensor is designed to be used with r 3 3 and rstudio the dataviewer shiny application was developed by mazama science and south coast aqmd dataviewer is free and open source software available through the github repository https github com mazamascience airsensorshiny the dataviewer was first released in 2019 under general public license v3 0 gpl 3 0 and runs on windows unix and macintosh operating systems dataviewer was written in r and program files are less than 7 mbytes the dataviewer requires git apache docker r and r shiny server 1 introduction a paradigm shift in air quality monitoring is occurring with citizen scientists able to develop hyper local community monitoring networks to supplement the established regulatory monitoring networks that are designed for regional monitoring snyder et al 2013 these environmental monitoring networks are increasing in complexity size and resolution both spatial and temporal due to technological advances and cost reductions for environmental monitoring hardware connected internet of things iot devices and cloud computing citizen scientists can take an active role in monitoring air quality at the neighborhood level by installing low cost air quality sensors lcs that collect and report air pollutant data particulate matter pm is an air pollutant that is categorized based on size with fine particulate matter pm2 5 defined as particles with aerodynamic diameter less than 2 5 μm the ability to record and visualize hyper local data in an intuitive and informative interface will likely spawn an increase in interest and interaction with environmental data sets due to the locally relevant nature of the information on the other hand non intuitive or limited user interfaces and confusing user experiences may discourage citizen scientists from interacting with the collected data the increasing complexity size and resolution of today s environmental monitoring networks have created big data challenges leading to the emergence of a new field of study environmental data science gibert et al 2018 data science combines computer programming skills math and statistical knowledge and subject matter expertise conway 2013 free open source software foss platforms play a vital role in the progress of research towards developing new methods for addressing environmental data science challenges the r environment and python are two foss programing languages that are often used in environmental data science applications kadiyala and kumar 2017a 2017b open access to environmental data sets and related tools is foundational for environmental data science to thrive and develop environmental monitoring data can be considered open access when the data is available through a stable and consistent application programming interface api that allows software and application developers to build applications to display and report that data in transparent and meaningful ways environmental data scientists can access regulatory data via open api s e g airnow api openaq api to create custom web applications for displaying air monitoring am data airnow 2020 openaq 2020 these am data viewing websites are useful and provide information to the public at varying granularity spatially and temporally two examples of data viewing websites include the openaq map and the world s air pollution real time aqi waqi map which both display international air quality monitoring data openaq 2020 world air quality index project 2020 openaq uses a color scale fig s1 in the supplemental information si that deviates from the common air quality index aqi color scale to display air pollution concentrations a special feature in the waqi website is their use of calendar plots to display am information data viewing websites that display modeled or interpolated air pollutant or aqi values are also available breezometer 2020 iqair 2020 plume labs 2020 when displaying data from both regulatory grade instruments and lcs the source and type of data displayed should be readily apparent a lack of differentiating and identifying data sources may cause confusion for the end user especially if the lcs do not agree with nearby regulatory grade instrumentation with interpolated or modeled maps often the user is not readily aware of the input parameters used to model air quality data when viewing modeled air pollution information the viewer should be cautious especially when data sources are not readily apparent and input parameters whether defendable or questionable for the data model are unknown to the end user viewer hagler et al 2018 broadly the available sensor data viewing platforms are map centric with point values or interpolated modeled data displayed with options for viewing recent time series data resources for accessing and displaying data collected from networks of lcs are available though they vary in terms of software foss or proprietary what they provide and whether they are provided by the manufacturer a project team or through a citizen science model while many sensor manufacturers have software and platforms in place for ingesting storing and analyzing data that is generated from their respective sensors these are often proprietary and offered as a software as a service saas or platform as a service paas requiring accounts with monthly or annual subscriptions costs in contrast to the saas and paas business model several sensor resources are available for open access viewing of data collected from lcs networks these platforms include but are not limited to the habitatmap aircasting map air quality egg portal luft daten project map purpleair map smart citizen kit map and the uradmonitor network map air quality egg 2020 habitatmap 2020 luftdaten 2020 purpleair 2020 smart citizen kit 2020 uradmonitor 2020 purpleair provides open access to the data collected by the purpleair network of sensors through an api and provides open viewing and downloading of sensor data through the purpleair map the luft daten project is a citizen science project with lcs reporting to a map and invites programmers to collaborate in this foss development through github ok lab stuttgart 2020 when selecting a sensor in either the purpleair or luftdaten gui the user is currently limited to viewing only the last seven days of data in a time series plot and current data on the map accessed january 2020 to gain an understanding of the historical local am data the user is required to download process and visualize the data from these networks on their own which may be a limiting factor to those without the environmental data science skills needed to perform such analysis these sensor specific online resources for viewing sensor data often do not include the regulatory am data that may be publicly available through the airnow or openaq api and often do not indicate what if any quality control qc measures are taking place on the collected data before displaying publicly for community members to understand local air pollution trends a more in depth analysis of historical data is required while map centric guis work well for viewing real time data communities that monitor air quality in long term deployments need additional plotting and viewing capabilities to access and understand their local historical am data a data dashboard for viewing and analyzing historical data would provide citizen scientist with a better understanding of local air pollution levels particularly spatial and temporal air pollution trends for those with varying levels of technical data science programming skills several software resources are available that support individual data analysis of air quality data if data can be organized and loaded into a software system then a more in depth analysis can occur and custom visualizations can be produced foss software packages have been developed in the r and python environments specifically for accessing and visualizing freely available am data these include the r packages openair pwfslsmoke ropenaq and raqdm openair provides a useful package for developing visualizations from collected am data with functions to create calendar plots scatter plots and time variation plots along with wind roses pollution roses and bivariate polar plots if wind speed and direction data is available carslaw and ropkins 2012 carslaw and beevers 2013 if we use advanced analytical tools and access am data directly then we can facilitate more organized robust systematic and repeatable data processing analysis and visualization of lcs data furthermore using foss tools allows for increased iteration and development an example of this workflow would be the pwfslsmoke r package and the associated pm2 5 am web application developed as part of the airfire tools by the u s forest service usfs wildland fire air quality response program wfaqrp callahan et al 2019 air fire tools 2020 these tools were developed to access regulatory grade am data via the airnow api and display that data graphically to assist the usfs air resource advisors to gather air quality data and create air quality reports during wildfire smoke events the pwfslsmoke r package provides functions to download parse and plot am data and provides the back end software necessary to generate plots for displaying on the front end web application a similar model in which an r package is used for accessing and processing lcs data would save users time and would allow the development of custom functions for different approaches to qc and more complex historical data analysis which are gaps we see in the current offerings additionally the r package could provide the back end software to support a front end web application to display historical am data to provide communities with more useful analysis and visualizations of historical data this web application would allow community members to answer questions about their local environment which are not readily answered with the current offerings of real time maps with limited historical data analysis the objectives of the software development associated with this project were to build an foss r package and data viewer web application that would address the challenges identified with the data management and visualization of lcs networks deployed within the u s epa science to achieve results star grant project this paper summarizes the development of an r package and web application designed to address the environmental data science challenges created by deploying 400 lcs in 14 different communities we wanted an open source r package that would allow users to download sensor data add spatial metadata perform data fusion with other relevant data sets and create maps and plots for viewing data collected by am sensors we also wanted the package designed with functions so that minimal coding would be required to complete tasks understanding that many would prefer to interact with an online web application we wanted to build an application that would provide an interactive data experience allowing users to make selections and explore the community am data sets by generating pre defined data visuals based on their user input selections the south coast air quality management district south coast aqmd collaborated with mazama science to develop the r package airsensor and web application airsensor dataviewer dataviewer to meet these software development aims 2 methods software design and characteristics 2 1 community engagement in 2016 south coast aqmd was awarded a u s epa star grant titled engage educate and empower california communities on the use and applications of low cost air monitoring sensors under assistance agreement no r836184 south coast aqmd has engaged 14 california communities through a series of workshops to introduce the project provide technical guidance on sensor technology and deployment siting installation configuration and registration of air quality sensors review deployment progress and examine community data sets and provide software tools and resources for citizen scientists to engage with collected data sets and create informative data visualizations roughly 400 purpleair pa ii sensors purpleair llc usa were distributed to community members the on going engagement with the star grant sensor communities sgsc has provided the motivation to develop software tools to enhance the community members ability to interact with historical data and extract meaningful information about their local environment participants were not engaging with the data that often as is supported by the survey data which is most respondents only check their air quality data sometimes 36 as opposed to often 17 and everyday 5 in person discussions provided useful context to help us understand this by 1 reporting that data was difficult to access and download especially historic data and 2 sharing what they wished to do with the data for example after displaying a static time of day bar chart showing the diurnal pm2 5 trends during a community workshop one community group leader asked how do i generate that plot on a regular basis and share with my community members in one sgsc a sensor host wanted to know the best time of day to walk their dog to reduce their exposure to particulate pollution additionally multiple participants from different communities shared their difficulty downloading and analyzing the publicly accessible pa ii data especially with regards to the time date reformatting required for plotting in microsoft excel the survey responses along with the discussions with community members on the data science challenges provided the motivation to build additional software tools to address the difficulty and challenges posed by analyzing these large community am data sets increasing the number of data sharing events with effective data visualizations should provide participants with a better understanding of the principles of air quality their local air pollution and the proper use and application of lcs sandhaus et al 2019 table s1 in the si provides a summary of the environmental data science challenges that are addressed in this project 2 2 software tools r environment rstudio r packages and shiny the r environment is an integrated suite of software facilities that is designed on a simple yet effective computer programming language r the r environment provides tools and functions for data processing storage calculation and graphical display since r is designed essentially on a computer programming language users are able to add further functionality to existing packages by defining new functions and developing packages of functions the r environment 2019 rstudio a public benefit corporation provides a foss version of an integrated development environment ide for r which supports code execution debugging and workspace management rstudio 2019 allaire 2020 instructions for installing r and rstudio can be found on the web and in the literature kadiyala and kumar 2017b the fundamental unit of shareable code in r is a package packages bundle together r code data documentation and tests packages are sharable on the comprehensive r archive network cran which is the public clearing house for r packages cran hosts a wide variety of foss packages that allow researchers to collaborate and build upon already developed r code the development of airsensor built upon r packages available on cran most notably mazamaspatialutils openair pwfslsmoke and worldmet airsensor is designed to be used with r version 3 3 this paper describes version 0 5 of the airsensor package which is available on github the latest or master branch of airsensor is also available on github the airsensor package can be installed using the devtools package within r using the following code image 2 shiny is a foss r package that provides a framework for building interactive web applications shiny allows the user to turn r derived analysis and plots into interactive web applications without requiring html css or javascript programming shiny allows for the development of a web application for viewing and sharing data analytics since not all users would be comfortable using the r environment which does require coding r shiny was used to develop the dataviewer web application to provide an interactive data experience for community members that would prefer to interact with the sensor data in a web application rather than in the r programming environment 2 3 airsensor r package rather than describing each individual function in airsensor the following examples will showcase the three primary data objects available through the package how to apply quality control measures on the imported data and how to generate plots for each of the data objects a complete guide to airsensor functions and operations can be found within the r environment after the package has been loaded helpful r vignettes are also available within the package to provide the user with code examples for using the airsensor functions and working with the sensor data 2 3 1 data access extraction and data objects overview airsensor currently accesses data generated by purpleair sensors by collecting real time data from www purpleair com json and historical data from a thingspeak representational state transfer rest api extracted data is enhanced with spatial metadata and transformed into efficient data objects for downstream analytics the three primary data objects are the purple air synoptic pas purple air timeseries pat and airsensor sensor data objects functions exist for creating or loading data objects as well as manipulating and visualizing them an overview of the airsensor r package data access data objects and functions is provided in fig 1 after installing or loading the package a data archive repository can be set to access archived data data archives can be created that for specific sensor networks e g sgsc or for a specific geographic area e g southern california so that the r user can access and load historical data more efficiently from an archive rather than the thingspeak rest api the data archives developed for the sgsc are kept current with cron jobs cron jobs are time based jobs that can run commands at specific time intervals that are scheduled to run every hour to pull and add the most recent data to the archive the data archive for the sgsc is accessible at http smoke mazamascience com data purpleair and includes historical data starting from october 01 2017 a base archive can be set in airsensor by the following code image 3 2 3 2 purple air synoptic data object the purple air synoptic pas data object provides an instantaneous view of the measured values from a network of sensors a pas can be created from the json data available at www purpleair com json or can be loaded by accessing a data archive fig 1 at the time of this writing the time resolution of the pa ii sensors is 120 s and therefore a new pas data object would be available roughly every 120 s the available functions for manipulating pas data object include pas filter pas filterarea and pas filternear the pas data can be plotted on a map to display the instantaneous data collected by the sensor network with the pas leaflet and pas staticmap functions fig 2 shows a pas data object displayed on an interactive map using the pas leaflet function which maps sensor locations and colors the locations according to aqi the map is interactive in that the user can select an individual sensor and view the values recorded at that location for the time the pas object was created if a user is interested in loading specific states or air districts the user can apply filters when generating the pas data object the leaflet map can be modified with options for map tiles parameter displayed and what type of sensors to display i e inside or outside sensors fig 2 was produced by the following two lines of code image 4 2 3 3 data fusion enhancements data fusion with other relevant data sources provides benefits for custom analytics for performing data quality checks and for providing information on local weather conditions data fusion provides the ability to tell a more complete story about local air pollution by fusing collected sensor data with other publicly available data sets airsensor has been integrated with the pwfslsmoke r package for access to regulatory am data via the airnow api and integrated to the worldmet r package for access to the u s national oceanic and atmospheric administration noaa integrated surface database for meteorological data callahan et al 2019 carslaw 2019 these data fusion enhancements provide the ability to generate comparison plots between a lcs and the nearest regulatory grade instrument and allow for sensor data to be joined with nearest meteorological data so that wind roses pollution roses and bivariate polar plots can be generated to provide insights into local air pollution trends data fusion enhancements are performed on both the pat and sensor data objects 2 3 4 purple air timeseries pat data object and quality control functions the pat timeseries data object provides timeseries data on a per sensor basis data manipulation functions for the pat data object include filtering sampling and joining a pat can be loaded from a data archive using the pat load function or can be created from the purpleair thingspeak api with the pat createnew function the code example below loads a pat data object for a sensor in seal beach ca that was deployed as part of the sgsc deployments the pat data object includes data from january 01 to december 31 2018 subsequent example code and plots displaying the airsensor functions will be performed on this pat data object or a filtered pat data object created from the scsb 20 sensor the pat data object can be loaded into the r environment and filtered by date with the following code image 5 pat data objects can be processed for time averaging qc algorithms and outlier detection for removal or replacement the user can create their own framework for applying qc functions depending on their project requirements the pat aggregate function returns a data frame with aggregate statistics which are helpful for building out qc algorithms the aggregate statistics include the mean median standard deviation minimum maximum and count for the aggregate time period chosen note that the pa ii sensor node is manufactured with two identical oem original equipment manufacturer pm sensors model pms 5003 plantower china that report the same types and amounts of data and for reference purposes are labeled as channel a and channel b respectively for the paired channel a and b pm2 5 data columns the pat aggregate function also returns the t test statistic based on an unpaired two sample student s t test p value and degrees of freedom several built in qc algorithms are available in airsensor and are labeled as pat qc hourly ab 01 and hourly ab 02 the pat qc function allows the user to perform a first level qc check for values that are considered out of spec with regards to the manufacturer defined specifications for the acceptable ranges for pm2 5 temperature and humidity the purpleairqc hourly ab 00 function allows the user to perform an hourly average of the a and b sensor channels when sufficient sub hourly data exists for both channels within an hour the default min count for sub hourly data is set to 20 data points requiring a data recovery for a and b channels 66 for the current time resolution at 120 s no further qc is applied with this function note that the pa ii s time resolution has changed with firmware updates over time as firmware updates have not been performed across the board simultaneously for all sensors in the purpleair network the following dates are estimates for firmware releases and data resolution time resolution for data prior to february 2017 is 20 s from february 2017 to march 2017 is 40 s from march 2017 to may 2017 is 70 s from may 2017 to may 2019 is 80 s and data recorded after may 2019 is 120 s the function purpleairqc hourly ab 01 allows the user to perform an hourly average of the a and b sensor when sufficient sub hourly data exists and when data is considered statistically similar data is invalidated when 1 minimum count 20 values 2 when both the means of channels a and b are not statistically the same two sample t test p value 1e 4 and the mean difference between channels a and b is greater than 10 μg m3 and 3 when the mean difference between a and b is greater than 20 μg m3 for pm2 5 values less than 100 μg m3 these conditions assume that the air entering the channel a and b sensors is the same and therefore the means of the two channel measurements should be statistically similar when measurements from these sensor channels agree the user can have higher confidence in the lcs air quality measurements and the subsequent data averaging of the two oem sensors into one value the two sample t test is a statistical technique to determine whether the difference between two means is significant the default settings of these qc checks can be modified to adjust the qc check to individual project requirements additionally new qc functions can be created and airsensor users are encouraged to create their own custom qc functions and submit these functions to be added to the airsensor package through github the purpleairqc validationplot function creates a series of timeseries plots for channel a and b the difference between channel a and b t test p value min count and the hourly averaged final output fig 3 the airsensor pat outlier function provides an outlier detection function that allows the user to apply a rolling hampel filter to identify points that may be outliers and if desired replace those identified outliers with a rolling median value the hampel filter is an outlier detection technique that uses the median absolute deviation mad for each data point a median and standard deviation are calculated using neighborhood values within a sample window size if the mad of a single data point is a specified number of standard deviations threshold minimum from the median value for the sample window then the data point is flagged as an outlier the default values for the pat outlier function set the sample window 23 and the threshold minimum 8 adjusting the default parameters on the function for identifying outliers would adjust the number of points detected as outliers fig 4 provides an example of the pat outlier function with the potential outliers identified as red asterisks in this example a date filter was applied to the pat example previously generated to only include the june 27 to july 8 2018 time period that would be impacted by a special event 4th of july fireworks the outlier detection function appears to identify many of the one off high values as outliers but does not consider the elevated pm2 5 concentrations due to the fireworks to be outliers this function allows airsensor data users to quickly implement an outlier detection technique and visualize the results of their outlier detection function fig 4 was produced by the following r code image 6 data visualization functions for the pat include plotting raw data time series interactive time series multiplot time series a b temp rh comparison plot for channel a vs b and a comparison plot with regard to the nearest regulatory pm2 5 monitor the channel a and b pm2 5 timeseries data can be compared using the pat interalfit function as shown in fig 5 for scsb 20 the a and b sensors agree with each other with an r2 0 98 slope of 1 05 and an intercept of 0 8 since the two sensors perform similarly for 2018 time frame the blue timeseries points representing the b sensor are plotted over top of the red points representing the a sensor the code to generate the plot is image 7 the pat scatterplot function provides a multi panel scatterplot for variables in the pat data object with an example of the plot shown in fig 6 this plot allows the researcher to determine if there is a lack of correlation between the a and b sensor channels or if there are higher than expected correlations between pm2 5 concentrations and weather conditions temperature and humidity this plot also provides the timeseries and distribution of data points for pm temperature and humidity in fig 6 the distribution plots for the a and b sensor channels indicate pm2 5 concentrations for this sensor are typically less than 25 μg m3 the datetime column provides an indication of periods of downtime with a noticeable downtime seen in august and september of 2018 a sensor can also be compared to the nearest regulatory air monitoring station ams with the pat externalfit function fig 7 in this example the sensor is 3 1 km away from the regulatory ams equipped with a met one beta attenuation monitor bam which is a u s epa designated class iii fem eqpm 0308 170 for pm2 5 the time resolution of the regulatory pm2 5 data is hourly to match lcs data with the regulatory data this function uses the qc procedures previously described to hourly aggregate the sensor data the user can specify which qc algorithm to apply or create custom qc functions fig 7 indicates that while the sensor follows the typical daily pm2 5 trends of the nearby regulatory grade instrument for pm2 5 with r2 0 73 the sensor tends to estimate higher concentrations than the regulatory grade instrument this slope intercept offset could be due to a local emission source impacting this particular sensor location or could be due to sensor measurement bias error that has been identified in prior publications feenstra et al 2019 magi et al 2019 for the time series in fig 7 the purple colored points represent the 1 hr purpleair sensor data and the black colored points represent the regulatory grade instrument data if the two agree closely within an hour the black point would be plotted on top of the purple point for that hour the plot in fig 7 is created with the following image 8 the pat dygraph function returns an interactive time series plot for both channel a and b allowing the user to zoom in out and investigate date times when pm2 5 concentrations may be higher than normal fig 8 using the interactive time slider located below the plot allows the user to quickly zoom in to further investigate dates and times with particle pollution events with a small amount of code the dygraph provides a versatile interactive plot where the user can explore a large amount of data at customizable levels with the time slider and zoom in out features fig 8 is created with the following image 9 2 3 5 hourly qc data object sensor the sensor data object is generated on a per sensor basis from a pat data object with the pat createairsensor function the user will need to specify a pat data object time averaging period parameter channel qc algorithm and minimum count the qc algorithms applied in creating the sensor data object are described earlier in section 2 2 4 with regards to the qc functions that can be applied to a pat timeseries data object the functions for sensor data objects begin with sensor an example creating a sensor data object is shown in the code below image 10 plots available for the sensor data object within the airsensor package include a bivariate polar plot and pollution rose which wrap functions from the openair r package the meteorological data used to generate these plots is retrieved from the noaa worldmet r package the bivariate polar plot and pollution rose which are shown in fig 9 and fig 10 respectively provide the user with the ability to couple wind direction and wind speed with pm2 5 pollutant data to determine whether pollution events can be attributed to specific meteorological conditions and potentially identify pollution sources a more in depth analysis of these plots and their application in analyzing am datasets is accessible within the published literature on the open air r package development carslaw and ropkins 2012 and use of bivariate polar plots carslaw and beevers 2013 grange et al 2016 the pollution rose and polar plot are generated by the following code image 11 2 3 6 timestamp and time averaging for airsensor data objects and functions airsensor and airsensor functions have been designed to appropriately handle timestamps and various time zones of potential users users should understand how time stamps are stored and visualized within airsensor and take appropriate steps when creating and visualizing airsensor data objects especially if using plotting functions outside of the airsensor package to visualize data the purpleair api provides access to data stored in coordinated universal time utc the airsensor data objects pas pat and sensor all store data with a utc timestamp when creating or loading either a pat or a sensor data objects the user can specify the local time zone of the sensor selected if a time zone is not specified when creating a data object for a single day the date time parameters will be passed as utc which for a sensor located in the pacific time zone 8h utc would return a data object with data from 08 00 am to 08 00 am local time of the following day in airsensor time stamps are labeled and time averages are coded as time beginning for example a 1 hr time average with a timestamp of 14 00 would be an average of the data collected between 14 00 and 14 59 this holds true even with the 2 min time matched channel a and b sensor data available in airsensor the purpleair pa ii channel a and b sensors report at different times within a 120 sec time interval in airsensor the seconds are dropped and data from the a and b sensor are assigned to a 2 min time beginning time stamp for matching purposes between the two oem sensors within a pa ii sensor since data is stored as utc the plotting functions within airsensor are coded to appropriately apply time shifts based on the sensor s location time zone so that data will be plotted and displayed in the local time of that sensor s location 2 4 airsensor dataviewer web application 2 4 1 airsensor dataviewer overview the dataviewer application was developed to provide an online interactive data experience for the sgsc networks these communities and sensor names are listed in si table s2 this interactive web application provides access to the functionality of the airsensor r package citizen scientists that would not be able to download r and run code or scripts to access process and visualize community data are now able to visualize their community data through the dataviewer while the infrastructure to generate the types of plots that had resonated with community group members during the workshops was developed in the airsensor package the ability for community group members to use that infrastructure and generate visualizations in an interactive web application without writing a single piece of code is provided in the dataviewer application plots that generated the most interest with community group members including calendar plots concentration maps community time lapse videos and sensor performance plots between the a and b internal sensors and between the sensor and nearest reference pm2 5 monitor were prioritized for incorporation in the dataviewer the following sections will provide an overview of the back end infrastructure required for the dataviewer application and the methodology for the dataviewer color scale and timelapse videos the front end of the dataviewer which is the online web application and the primary point of interaction for community members is highlighted in the results section 2 4 2 cloud computing resources cloud computing provides computing services over the internet using a pay as you go pricing model computing services typically include computing power storage networking and analytics cloud computing can provide benefits by allowing programmers to focus on building new and innovative applications rather than acquiring and maintaining the infrastructure required for their computational needs the cloud can provide benefits with cost reductions for it infrastructure and can increase the scalability elasticity reliability and security of computational services in comparison to computation services provisioned locally or on premise azure which is microsoft s public cloud computing platform was used to support the computational requirements of the dataviewer application the application could also be run on another public cloud platform or on premise if desired the computation services required include running scheduled tasks cron jobs for creating data objects storing data in structured data directories and hosting the dataviewer application the data archive consists of a set of flat files defined by a simple directory and naming protocol with the data ingest scripts written in the r programming language a virtual machine vm was configured on azure with the structured directories for the data directories along with required software i e git apache docker and r a second vm was configured to host the dataviewer application fig 11 provides a simplified system architecture for the dataviewer application 2 4 3 dataviewer color scale determining an appropriate color scale for pollutant concentrations generated by lcs is challenging historically air quality has been colored according to the aqi with values ranging from 0 to 500 with six distinct color categories good green moderate yellow unhealthy for sensitive groups orange unhealthy red very unhealthy purple and hazardous maroon historically aqi has been calculated at 24 h averages due to the scientific information about air pollution exposure and public health in 2013 the u s epa released a new aqi calculation method nowcast reff method for pm2 5 that calculates aqi hourly based on the previous 12 h with the most recent hourly pollutant concentrations given larger weighting when air quality is changing rapidly mintz et al 2013 the u s epa in the air sensor toolbox suggested a new pilot version color concentration scale that could be used for 1 min high time resolution data from lcs u s environmental protection agency 2019 this scale uses four shades of blue for low medium high and very high pm2 5 concentrations and is shown in si fig s2 the scale from the airsensor toolbox was created for 1 min sensor data in contrast to this work in which lcs data is processed with qc algorithms and time averaged to 1 hr concentrations prior to being displayed in the dataviewer application furthermore the authors wanted to provide users with a clearer differentiation among the higher pollutant levels sometimes indicated by the sensors hence a new color scheme was developed for the dataviewer that includes 5 concentration categories represented by two colors blue and purple with variations in the hue and luminance as shown in table 1 2 4 4 ff mpeg and digital stills and video stills creation one of the desires of the community groups was to view historical time lapse concentration maps to view past air quality events in their communities to accomplish this task cron jobs run hourly to create video still images for each of the 14 sgsc these images are stored in the structured data directory in sequence and converted into mp4 video files using ffmpeg which is a foss dawes 2019 ffmpeg 2019 3 results 3 1 airsensor package the airsensor r package meets the community needs for those desiring to work with purpleair lcs data programmatically in the r environment through the airsensor package real time and historical data from the sgsc listed in si table s2 can be accessed loaded into r and visualized used pre built plotting functions these plotting functions allow the user to create useful and interactive plots that can be shared within a community group and deliver actionable information for the community members to answer questions like when is particle pollution highest in my community and what time of day or day of week would be best to plan an outdoor activity i e walk dog or golf game to potentially reduce my particle pollution exposure airsensor creates a data flow for the end user to create data objects for synoptic data time series data and qc hourly pm2 5 data with the functions of this r package highlighted in the methods section the user can easily create informative plots for community members to understand their local historical air quality trends with minimal coding required the airsensor r package and associated functions provide the necessary back end software analysis and plotting functions to create the front end dataviewer web application the dataviewer is usable and useful to a much broader segment of the public and is the primary point of interaction for community members to gain insights into their local air quality conditions this solution provides an example of how these types of tools and solutions can enhance public engagement with data from lcs networks 3 2 dataviewer application 3 2 1 user interface tabular structure and plotting features the dataviewer application version 0 9 7 has a hierarchical page and tab structure with 4 top level pages explore view data latest data and about the view data page is for viewing tabled data and provides the ability to download data in 3 to 30 day intervals on a per sensor basis for sgsc historical data can be accessed back to the start of the sgsc deployments october 01 2017 the view data page includes high resolution 2 min time matched pa ii pm2 5 data from the a and b sensor channels temperature f and relative humidity this data output provides the user with a clean time matched data set for the a and b sensor creating a similar data set outside of the airsensor r package or dataviewer application would likely be time consuming and difficult especially if the user were not proficient with microsoft excel or data science environments the latest data page provides visual access to the latest non qc data on a per sensor basis with timeseries plots provided for sensor channel a channel b humidity and temperature the about page provides an overview of the dataviewer its intended purpose qc procedures and a disclaimer message 3 2 2 explore page tabs and functionality the explore page has the most functionality for exploring and analyzing community am data and includes six tabs overview calendar raw data daily patterns compare and timelapse in the overview tab the user can select a community a single sensor sensor name a date end date and view past data with options for viewing the prior 3 7 15 or 30 days to the selected end date the overview tab fig 12 provides a map that displays the average pm2 5 for all sensors within the selected community for the time period selected 3 7 15 or 30 day average and a bar chart that displays hourly pm2 5 concentrations for the selected sensor this overview tab provides the user with access to historical pollutant concentrations for the user selected timeframe for their community and individual sensor by changing the date the user can quickly identify spatial differences between locations since the map indicates an average pm2 5 concentration for the entire timeframe chosen 3 30 days additionally the user can quickly scan the bar chart for when higher than typical pm2 5 concentrations were recorded for a particular sensor in the calendar tab a 1 year calendar plot is rendered for a single selected sensor the user selects a community sensor and date with a calendar plot being generated for the entire calendar year of the date selected fig 13 the calendar plot is interactive and when the user hovers over a date the 24 hr averaged pm2 5 concentration is displayed the calendar plot is easily understood by community members and provides an intuitive view of a complete year of pm2 5 data for a single sensor the calendar tab is great place to start when exploring a community data set to find dates with atypical 24 hr pm2 5 concentrations the user can then further examine these atypical pollution events at higher time resolution with other tabs available within the explore page the calendar plot especially resonated with the community members and sensor hosts and therefore was a priority for inclusion in the dataviewer application calculating and rendering the calendar plot is computationally expensive and may take a few moments to display when interacting with the dataviewer application but the result is well worth the wait for this informative plot throughout the workshops we received the most feedback and discussion from community members when showing the calendar plot the calendar plot triggered the audience and facilitated effective discussions during community workshops community members who would be more silent or could not recall as to what might have caused poor air quality in their community during the past several months were able to identify days with poor air quality and what might have caused them when they viewed the calendar plot with the color coded concentrations the raw data tab provides the raw time series data for channels a and b humidity and temperature below the time series plots the raw data tab provides a comparison between the channel a and b sensors with both a time series and a scatterplot that indicates the regression statistics between the channel a and b this functionality uses the pat internalfit function from the airsensor r package which was previously shown in fig 5 these comparison plots provide the user with the ability to check on the performance of an individual sensor by viewing how well the two internal raw sensors within the pa ii agree for the selected time period if a user is concerned with the performance of an individual sensor this tab can be used to determine if both the raw sensors are responding to changes in particle concentrations similarly low correlation and or a large slope intercept offset are indicative of a sensor performance issue and that one or both sensors may be experiencing a malfunction the daily patterns tab fig 14 provides a bar chart illustrating the diurnal trend for pm2 5 a pollution rose and a summary table for the noaa weather data for the date range selected the daily patterns bar chart provides the average concentration by hour of day with this tab the dataviewer user can determine on average what hour of the day has the highest and lowest particle pollution this plot helps to inform users as to historical trends within their community and provides information that the community member can infer what time of day may be best for scheduling physical activity to reduce particle pollution exposure based on historical air pollution trend data the pollution rose allows the user to determine if pollution can be attributed to specific meteorological conditions the compare tab provides a comparison between the sensor data and the nearest ams equipped with a continuous regulatory pm2 5 instrument the compare tab provides a map indicating the location of the sensor and nearest ams along with a timeseries and scatter plot comparison for the two data sources allowing the user to determine if the selected sensor follows the typical trends for pm2 5 recorded at the nearby regulatory ams for the date range selected the dataviewer application is using the airsensor pat externalfit plotting function which was shown prior in fig 7 while the distance between the regulatory monitor and the lcs is provided on the sensor monitor comparison timeseries plot the map provided in the dataviewer on this tab allows the user the opportunity to visualize the distance between and the spatial context of the two monitoring locations understanding the siting of the lcs and the regulatory ams is crucial to understanding the information provided by the comparison plot if either the sensor or regulatory monitor is installed in a near source environment i e near road the user should not expect the two measurements to agree the final tab in the explore page is the timelapse tab this tab provides the user with the ability to generate a 6 day timelapse pm2 5 concentration video on a per community basis fig 15 right clicking on the video allows the user to save a mp4 video to their computer and share if desired this timelapse concentration map allows the user to view pollution events that may have taken place within a community during a selected time frame and visualize the flow of pollutants through a community an informative approach to using this timelapse video is first to use the calendar plot feature to identify dates with elevated pm2 5 mass concentrations μg m3 after identifying those dates the user can then choose an inclusive date range to view the community timelapse to better understand the pollution event 4 discussion while online systems exist to view real time and recently recorded measurements foss tools for accessing processing and analyzing historical am data collected by lcs are less available to the public developing foss tools for archiving interpreting and communicating data from sensors has been identified as a concrete next step towards building a system for filling the air quality data gap pinder et al 2019 this work provides a foss r package and a web application designed to fill that gap by providing the software tools to view both real time and historical hyper local air quality information generated by lcs networks access to hyper local air quality information is expected to spawn an increased desire to interact with air quality information and allow community members to take appropriate actions based on results generated from their community monitoring networks the airsensor r package and dataviewer application provide a framework and data flow for communities to transform their community monitoring data sets into insightful information through interactive data experiences and data explorations when meaningful results and observations are formulated community members can take appropriate actions to reduce their exposure to air pollutants these actions could include planning transportation e g walk bike motor vehicle routes to reduce air pollution exposure and scheduling physical activity events e g golf game sporting practice sporting event during hours of the day or day of the week that have been identified to have lower pm2 5 pollution based on historical data analysis our experience with sharing the dataviewer with the community leaders and members participating in the project has been positive with users enjoying the interactive data experience provided within the dataviewer these community members have shared how this dataviewer provides them with the analysis capabilities to better understand their local air quality conditions plots that previously seemed out of reach due to required technical data analysis skills and coding experience are now readily available and generated with only a few selections and mouse clicks within the dataviewer application foss software developments provide efficiency by building a community of proactive data users around shared tools and allowing for multiple parties i e agencies entities individuals to contribute to software development and enhancing software functionalities this benefit has already been realized as with the usfs airfire group funding further developments to airsensor for functions to calculate state of health metrics designed to categorize whether sensors are functioning properly this information will be used in the context of wildfire air quality response foss allows for researchers to collaborate and build upon the foundation established in this development foss developments can also provide a high level of transparency in terms of data analysis and integrity as the end user is able to select which post processing steps are appropriate for their data analysis with foss tools and publicly available data sets researchers can reproduce data analysis techniques and develop additional functions with the interoperability associated with foss development 5 conclusions this novel work brings these software systems to the end users or community members in a foss format with all the advantages of open software developments not only is the end user able to access process and analyze historical sensor data but the user also has access to the source code and functions with the option to create their own custom functions for qc filters and advanced analytics allowing the community to build upon this existing work provides benefits to the sensing community as a whole developing this software in the r environment also provides for data fusion enrichment by coupling the collected am sensor data with meteorological data and regulatory am data through other open source packages in the r environment the airsensor package has established a foundation upon which further enhancements and refinements can be developed both airsensor and dataviewer source codes are available on github and the authors invite collaboration and input to help shape the airsensor open source project to best meet the needs of the air sensing community the airsensor r package is sensor specific working with any publicly registered purple air pa ii sensors the dataviewer solution is both sensor and project specific and therefore limited to the pa ii sensors deployed by south coast aqmd in sgsc the authors believe that the data flow works well for am sensor data with the data objects going from synoptic data to time series data and then to hourly qc sensor data the blueprint developed to make the dataviewer operational could be applied to other projects and communities to visualize data collected by their purpleair lcs networks the work discussed in this paper focused on the initial data handling and analysis capabilities required for a community am network of pm2 5 sensors planned future work will focus on several improvements to the airsensor r package the data archive database design and the dataviewer application the airsensor r package and archive will be improved by adding functionality to handle unique timeseries identifiers and incorporating pm1 and pm10 data additional plotting functionality will include enhancements to create multi sensor comparison plots and visualize sensor state of health metrics for both individual sensors and sensor networks additional enhancements to the r package may include developing models to provide hyper local air quality forecast for the community the dataviewer will be enhanced by improving the appearance usability data handling and performance of the application funding this research has been supported by a grant from the u s environmental protection agency s science to achieve results star program to the south coast air quality management district disclaimer statement this publication was developed under assistance agreement no r836184 awarded by the u s environmental protection agency to south coast aqmd it has not been formally reviewed by epa the views expressed in this document are solely those of the authors and do not necessarily reflect those of the u s epa the south coast aqmd and u s epa do not endorse any products or commercial services mentioned in this publication declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank dr jonathan callahan and hans martin at mazama science inc seattle wa for their collaboration and contributions in the development of the airsensor r package and the dataviewer application tools along with their valuable feedback on this manuscript the sensor data used and presented in this paper was collected by the air quality sensor performance evaluation center aq spec at south coast aqmd the authors would also like to thank the community groups leaders trainers coordinators and members sensor hosts that participated in the u s epa star grant and provided valuable feedback that allowed us to create and improve this work the authors thank ms emma ranheim who assisted in user testing the airsensor r package appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104832 
25937,the authors developed a non proprietary web browser based open source software that allows users to visualize and evaluate hydrologic space time data in an interactive environment hydrovise is client side browser based software that interprets a configuration file to construct control elements in the graphical user interface for visualizations of space time data and model simulation evaluations it leverages the concept of three dimensional data cubes that facilitate query in space time and variable dimension s without the requirement for a database system using a configuration file users can define data sources as local file system resources and or external data sources e g online data services this capability makes hydrovise a flexible and portable solution where users can share their hydrologic data in an interactive web environment this paper provides the software description with four distinct example use cases including but not limited to time series data visualization and evaluation grid based and river network based data visualizations keywords web application data management environmental science hydrology evaluation software availability hydrovise is available under mit open source license terms and the source code and examples are available at the following github repository https github com hydrovise hydrovise for documentation and tutorials readers can refer to https github com hydrovise hydrovise wiki 1 introduction the availability of data and computational resources have brought unprecedented opportunities to the hydrologic modeling community for understanding the hydrologic cycle at the same time it has introduced new challenges that demand efficient approaches for visualization communication and data evaluations in recent years there has been significant progress in developing new tools and technologies for visualization analysis and handling real time data e g wong and kerkez 2016 swain et al 2016 brendel et al 2019 open source software oss provide unique opportunities for software development and it has potential advantages over their commercial counterparts an oss can benefit from developer interest user contributions feedback and frequent release ghapanchi et al 2014 that could further improve the user experience and lead to a successful open source solution midha and palvia 2012 identify the most critical factors in the success of oss as user base language translations responsibility assignment and modularity among others vitolo et al 2015 provide an overview of the current web technologies used for environmental big data several oss solutions used these technologies to develop web applications for water resources purposes swain et al 2015 however in most cases solutions have several external dependencies and require expertise or at least adequate knowledge of multiple programming languages frameworks these dependencies make solutions fragile as they complicate the deployment process and ultimately decrease software sustainability murugesan and gangadharan 2012 refer to software sustainability by quality attributes of a system such as modifiability re usability and portability swain et al 2016 developed tethys as a framework to lower the barrier in developing web applications for environmental data visualization and computations tethys uses python python foundation 2016 as the primary programming language with its server side deployment of django django software foundation 2018 it leverages other third party software tools that facilitate computing and visualizations it is a flexible solution for developing web applications in this framework but the user developer requires more than basic knowledge of python and other third party dependencies recently brendel et al 2019 developed sharks for retrieval visualization and analysis of hydrologic and meteorological data sharks uses the shiny apps framework and r programming language as its core programming language the sharks web application allows users to analyze hydrologic data over a user defined watershed previous software solutions for environmental data have made the deployment of web applications easier for environmental scientists however deployment of these solutions could be challenging for a non expert user because of various dependencies furthermore they still rely on a programming languages that are not designed natively for web development therefore for customized functionalities users need to have at least a basic knowledge of javascript css cascading style sheets and html hypertext markup language that are core languages for web application development many previous oss solutions developed for environmental science depend on database systems to handle the space time structure of the data swain et al 2015 provide a detailed list of solutions that employ spatial database systems the decision to use a database in an oss solution depends on the objective and design of the oss solution for example a database could be useful for complex queries and storing relational data on the other hand database may not be needed for hydrologic data visualizations however database as a hard requirement for a software solution adds further barrier for a user with limited database management expertise data access using data services have become more popular among environmental data providers facilitating data sharing and access for example cuahsi s consortium of universities for the advancement of hydrologic science hydroclient provides access to data on an interactive web based platform for the user selected region http data cuahsi org data services deployed by noaa national oceanic and atmospheric administration is another example of a web service that allows users access to extensive environmental observations in recent years advances in web technologies have added new features facilitating the development of applications from native web programming languages such as javascript walker and chapra 2014 for example standardization of javascript with modern ecmascript standard ecma 1999 significantly reduced compatibility issues on different browsers our motivation in this work was twofold first we aimed to develop a web based tool that facilitates the visualization and communication of space time hydrologic data second we intended to significantly lower code barriers for users with minimal web development knowledge therefore we present a non proprietary open source software for hydrologic science that we coined hydrovise hydrovise is a client side application developed by leveraging available web tools and using existing web programming languages it is configurable and deployable on a personal computer or a web server that does not require a supporting database server hydrovise allows users to visualize evaluate and share their hydrologic data in an interactive web environment by preparing the data organizing the data and preparing a configuration file for their project in the next section we describe the main components of the developed software data model and extension modules then we present example use cases common in the hydrologic community in section 4 we discuss the advantages and limitations of the current version of the developed software and its potential usages finally we provide a summary and discussion on future developments 2 software description hydrovise is an open source browser based client side software that lowers the code barrier in web based hydrologic data visualization and evaluations we selected javascript as its core programming language hydrovise is designed and developed as an entirely client side application for increasing the portability of the solution but it can also be deployed as server client if additional server side functionalities are required hydrovise integrates the functionalities of multiple external libraries and internal custom modules it consists of three main components modules configuration and graphical user interface gui modules are internal custom libraries including functions responsible for extension and orchestration of integrated third party libraries and web tools in hydrovise modules handle data acquisition visualizations and user interactions with the gui hydrovise custom libraries control third party libraries web tools and with the use of user defined project configuration files generate interactive map based reports we selected tools and libraries that are maintained regularly and are among the most popular open source javascript libraries table 1 summarizes the external libraries and their usage in hydrovise the project configuration file is a javascript object notation json file that contains a set of information definitions about data sources e g traces space time data marker locations and etc control elements in the gui and the styling of the visualizations the graphical user interface is a web browser based interface created dynamically by the initialization module the content of the gui depends on the information stored in the project configuration file it consists of a map a canvas for visualization of time series and spatial data and other control elements that allow navigating data in different dimensions fig 1 shows a schematic of the gui and structure of the configuration file the configuration objects are numbered with their corresponding elements on the gui further details on the configuration file is provided in the hydrovise documentation fig 2 shows the hydrovise workflow diagram the elements with white and gray background correspond to pre deployment and post deployment stages respectively pre deployment includes configuration and initialization the gui is generated dynamically by the initialization module using the configuration file in other words the absence or presence of control elements in the gui depends on the information provided in the configuration file after initialization user interactions are passed to modules by control elements in the gui modules refer to requested data source using event information that corresponds to a specific element of the data cube see section 2 1 and data are visualized on the gui 2 1 data model structure and types hydrovise adopts the concept of data cubes as its data model initially data cubes were introduced by gray et al 1997 for reducing the dimensions of data based on user queries maidment 2002 introduced the space time variable cube for referencing individual data to corresponding attributes later goodall et al 2008 implemented this concept in integrating time series from different sources for the national water information system nwis fig 3 shows a schematic of the data cubes where the user can query a chunk of data in a different dimension s the atomic element of the data cube can consist of a value a vector or a matrix previous uses of the data cube concept were implemented in a database system e g microsoft sql postgresql this approach is efficient for executing complex queries goodall et al 2008 however the construction and management of the database by itself demands expertise that not every user may have hydrovise eliminates the restricting requirement of a database by using flat file system for a file system implementation space time variable cubes are constructed by organizing the files in folders and sub folders to facilitate the navigation in selected dimension s this organization is defined in the configuration file by the user as a file path template for any given data source example file path template in the configuration file has the form template var time variable comid path format data 0 1 2 csv where curly brackets are substituted with time variable and space comid that user selects on the gui the dynamic path creator uses the path template and user selected variables to replace the placeholders in the template string and create the path to data source s a data source can be a local file a custom data interface using a database query or a web service hydrovise can be used to visualize time series of data associated to any spatial support extent including points lines or polygons for simplicity we selected the most common data types as the baseline data types for space time data e g radar based precipitation demir et al 2015 users can define the underlying grid geometry and the corresponding timestamps for the data for visualizations the data can be provided as comma separated value csv or binary files based on a common identifier comid table 2 summarizes the data types and formats used for handling time series static and space time data the users can also define static map layers e g kml s for adding contextual information to the map for the region of interest examples of these layers could be locations of hydrometeorological gauges stream and river network or administrative boundaries 2 2 map markers users can inspect interactive locations on the map interface by hovering over an object that shows a tooltip tooltips can be customized to show user defined attributes metrics or name of the observation site if available this feature allows users to access information about locations on the map without plotting the time series interactive locations on the map are specified as map markers map markers are a feature collection defined in a geojson formatted file properly described in the configuration file the underlying geometry could be a point line or polygon these spatial features are sensitive to user specified events e g click hover 2 3 visualizations spatial data e g points polygons etc are visualized on the map using the leaflet javascript library leaflet is an open source javascript library for interactive maps we selected leaflet because of its large open source community contributions and capability to extend functionality the basemaps for geographical locations or satellite imagery is provided in the configuration file as a tilemap data source leaflet library loads the map tiles depending on the zoom level and map extent hydrovise uses the plotly javascript library to visualize time series and available timestamps for space time data plotly is a data visualization library with a wide range of chart types we selected this library because of its user developer community and documentation the plotly data canvas is interactive that allows users to zoom pan and inspect data for a given observation point for space time data visualizations e g radar rainfall satellite based soil moisture we use the glify library developed as an extension for leaflet to leverage webgl web technology webgl employs graphical processor unit gpu for rendering graphics in web browsers it is a cross platform api that uses the opengl shading language glsl and runs in the html5 canvas element the visualization of space time data is controlled by a panel that shows available timestamps for each space time dataset when a user selects a timestamp for the spatial data data are visualized and added to the map inventory for additional layer control options data for a given space time dataset could be a geotiff raster file or geometry to be updated using csv or float32 binary file using the configuration file users can add animation control and define a custom color legend for each dataset 2 4 extension modules extension modules extend the core functionality of hydrovise these modules are enabled using user defined directives in the hydrovise configuration file we provide a list of implemented extension modules in the following sections 2 4 1 stage discharge converter the stage discharge relationship or rating curves are used to estimate discharge streamflow from stage observations we have developed an extension module that uses a non linear least square method to fit a polynomial to stage discharge data rating curve and use the fitted function to estimate discharge from stage observations or vice versa 2 4 2 flow categories flow categories or flood categories provide information on the severity of flow conditions for example in the united states the national weather service nws has classified flow stages into five categories nws defines these levels as major moderate minor flooding action near flood and normal no flooding state https water weather gov ahps forecasts php the united kingdom s flood information system https flood warning information service gov uk has a similar definition for flood levels these levels serve as a means of communicating flow conditions to the general public and early warning purposes we have included an extension module that allows users to incorporate flow level categories in streamflow and stage time series visualizations users can provide information about flow categories or historical flow conditions e g flow return periods they can visually inspect the hydrologic model predictions for flow categories 2 4 3 performance evaluations evaluation and validation provide insight into the strengths and limitations of models trying to replicate observed data performance evaluation of hydrologic model predictions helps identify the misrepresentations of the physical processes in a hydrologic model however this task can quickly become cumbersome as the number of dimensions increase we refer to dimensions as different model state variables model setups input rainfall forcings etc therefore it is essential to use interactive tools that allow decision makers model developers to compare data with model outputs using multiple performance metrics with a visual inspection of the predictions provides further confidence in model selection or performance assessment procedure bennett et al 2013 we have developed an extension module that performs evaluations for the user defined time period for selected performance metrics across the defined spatial domain the module incorporates the most common performance evaluation metrics such as kling gupta efficiency kge proposed by gupta et al 2009 and nash sutcliffe efficiency nse among several others also users can import their pre computed performance metrics to visualize their model outputs and their performance 2 4 4 ensemble time series navigator we developed an extension module utilizing plotly time series canvas that allows users to navigate and visualize ensemble time series for a user selected location this module shows the timestamp or the ensemble member number on time series canvas users can add or remove multiple time series to plotly canvas dynamically 2 4 5 watershed boundary outline this module allows users to define a data source for additional geometry related to a clicked spatial feature this geometry could be a watershed boundary or relevant geometry to the selected spatial element from map markers spatial data users can define the data source for the geometry in the configuration file the data source could be a path to the extracted watershed boundary or an extension module that extracts watershed boundaries from dem digital elevation model data sit et al 2019 2 5 deployment the minimum requirement for the implementation of hydrovise is a http server e g apache nginx compatible data types and a web browser potential users can copy the code from the github repository https github com hydrovise hydrovise to a webserver directory hydrovise works as an interpreter of the configuration file where it can serve multiple projects defined as different configuration files therefore providing the configuration file as a url to the interpreter in the browser will load the gui this feature allows users to share their deployed web link with anyone with internet access 3 example use cases in this section we provide four different example use cases of hydrovise implemented by using different configuration files each example illustrates the different capabilities of the software 3 1 time series data visualization and evaluation this example use case illustrates visualization and evaluation of streamflow time series live demos for this example use case is provided below 1 real time and historical usgs streamflow data browser http hydrovise com app config examples ex1 config usgs json 2 hydrologic model evaluations for smap satellite based soil moisture assimilation http hydrovise com app config examples ex1 config json we constructed a simple real time and historical usgs united states geological survey streamflow data browser using a configuration file the streamflow data are defined by creating a dynamic path to the usgs national water information system nwis web service usgs 2019 fig 4 shows an example illustration of a real time usgs streamflow data browser for a region of interest fig 4a and b show stage f t and streamflow m 3 s 1 time series using stage discharge converter module the data for usgs gauge observations are provided with a 15 min interval the basin boundary for a selected gauge is highlighted as light gray following the nws color convention flood categories are shown with magenta red orange and yellow colors defined as major moderate minor flooding and near flooding stages respectively as shown in fig 4 map inventory allows users to inspect the contextual map layers defined in the configuration hide show layer s and add their spatial data vector raster to current view the real time streamflow data browser can be used for monitoring streamflow at the observation locations provided by the user this example is useful for real time hydrologic model evaluations public and emergency management offices that coordinate the preparations for flood damage mitigation the evaluation module developed for hydrovise allows users to conduct hydrologic model performance assessments for a selected time period also users can visualize pre computed performance metrics by defining the data source in the configuration file for demonstration we compare streamflow predictions from the hlm model krajewski et al 2017 for two cases the first case is referred to as the open loop model prediction that does not include any update to model states the second case is a model prediction using data assimilation of smap satellite based soil moisture in hlm s top layer we use ensemble kalman filter evensen 2003 with time dependent variance for perturbations of initial soil moistures referred to as enkfv we show streamflow predictions at the usgs gauge observation locations for the state of iowa for the year 2015 as shown in fig 5 hydrovise populates summary maps of hydrologic model performance metrics for a selected year statistical measure and model setup in this example usgs locations are color coded based on kling gupta efficiency note that locations without reference data for the selected year are shown as light gray circle markers streamflow predictions after smap satellite based soil moisture assimilation fig 5b show improvement in majority of the usgs gauge locations compared to open loop model predictions fig 5a fig 6 illustrates the time series plot of the hlm model streamflow predictions for the two cases and usgs observation at cedar river at cedar rapids the color coded circle markers shows the peak timing difference of the streamflow predictions using enkfv and usgs observations 3 2 hydrologic assessment of precipitation forecasts this is an example hydrovise use case for visualization of streamflow forecasts the live demo for two cases in this example could be accessed at streamflow forecast time series for each issue time http hydrovise com app config examples ex2 config issue time json streamflow time series based on forecast lead time http hydrovise com app config examples ex2 config lead time json real time flood forecasting models use quantitative precipitation estimation qpe products for simulating historical streamflow and quantitative precipitation forecast qpf products for their forecasts flood forecasting models obtain the precipitation forcing from atmospheric models because of the chaotic nature of atmospheric flows atmospheric models use data assimilation techniques to account for new observations in model forecasts flood forecasting models update their streamflow forecasts as new precipitation forecasts are issued therefore it is essential to understand the streamflow forecasts in the context of precipitation forecasts for this example we used hourly accumulated mrms multi radar multi sensor and ifc iowa flood center qpe products for simulating historical streamflow for streamflow forecasts we use hrrr high resolution rapid refresh atmospheric model s benjamin et al 2016 qpf hrrr product is issued every hour with lead times starting from zero to 18 hour streamflow forecasts from hlm model are provided for the next five days starting from every precipitation forecast issue time we use the ensemble time series navigator described in section 2 4 4 to answer two key questions how good were streamflow predictions driven by precipitation forecasts at different issue time how do streamflow predictions evolve with the different precipitation forecast lead times to answer the first question we organized streamflow predictions by precipitation forecast issue times fig 7 shows usgs gauge observation and streamflow forecasts using mrms and ifc qpe and hrrr qpf products for a user selected issue time the time series show streamflow forecasts for the next five days from a selected forecast issue time as shown in this figure users can navigate to streamflow forecasts issued at any time for the selected usgs gauge location the extension module allows users to add and remove time series for a chosen product for visual comparisons with usgs gauge observations we answer the second question by organizing streamflow forecasts data by their lead time fig 8 illustrates usgs gauge observations and streamflow forecast time series for a 7 hour forecast lead time streamflow forecast time series corresponding to different lead times can be added to the time series plot shown in this figure this extension module allows users to evaluate the predictions based on forecast lead time 3 3 grid based hydrometeorological data visualization this example illustrates the visualization of a space time process on a regular grid the live demo for this example use case is available at http hydrovise com app config examples ex3 config json as one of the critical hydrologic variables soil moisture controls runoff production e g crow et al 2018 jadidoleslam et al 2019b and agricultural productivity e g hargreaves 1975 smos soil moisture ocean salinity and smap soil moisture active passive are the two satellite missions that provide soil moisture information by using remote sensing techniques e g kerr et al 2010 o neill et al 2015 in recent years several studies have been conducted for validation comparison and evaluation of satellite based soil moisture with field sensor observation networks e g colliander et al 2019 ma et al 2019 jackson et al 2012 tavakol et al 2019 an interactive web based tool can facilitate visualization and validation of satellite based soil moisture products that could complement current validation studies we provide an example use case of hydrovise for visualization of satellite ground based soil moisture and hydrologic model storage fig 9 shows soil moisture time series from smap smos field sensor observations and hydrologic model soil storage from the hillslope link model hlm krajewski et al 2017 smap data are posted on ease v2 grid brodzik et al 2012 which we use as the reference geometry shown with the light gray color for comparison smos satellite soil moisture data are gridded to the smap grid also we show model storage as percentiles to visualize the soil moisture sub grid variability originating from hlm model with higher spatial resolution iowa flood center operates a network of over 30 field soil moisture sensors that are shown with white circles field sensor soil moisture data are averaged for more than one collocated sensors in smap grid fig 9 shows that soil moisture oscillations from model and different products show good agreement for the selected grid highlighted on the map the sub grid variability decreases for higher median soil moisture values which is consistent with findings from previous studies e g famiglietti et al 2008 jadidoleslam et al 2019a fig 10 a illustrates smap satellite based soil moisture estimation over the state of iowa for a satellite overpass approximately at 6 00 p m on may 23 2016 fig 10b shows hourly mrms radar based rainfall map for at 3 00 p m on may 23 2016 on the same map higher soil moisture regions from smap in central iowa could be described by antecedent rainfall these figures illustrate the hydrovise ability to display raster data 3 4 river network based data visualization this example illustrates the visualization of a river network based data the live demo for this example use case is available at http hydrovise com app config examples ex4 config json portable version of this example use case is included in github examples folder the live demo can be accessed at http hydrovise com app config examples ex4 min config json a river network has self similar structure that describes surface water movement in a watershed or a region river network based visualizations can help users inspect the streamflow evolution in space and time in this example we use hydrovise to visualize flood potential over state of iowa s river network similarly it is possible to visualize other variables such as water quality and contaminant loads fig 11 shows the river network for the state of iowa and streamflow time series for a user selected river segment the network includes more than 100 000 streams with horton order three and above users can inspect streamflow time series for selected river segment s by providing time series data source in the configuration file flood potential index is an indicator for communicating the streamflow condition irrespective of upstream drainage area of a river segment it is defined as the ratio of streamflow to mean annual flood quintero et al 2020 1 i q q m a f where q is streamflow m 3 s 1 and q m a f is the mean annual flood for each river segment quintero et al 2020 derived a power law relationship for mean annual flood for the state of iowa 2 q m a f 3 12 a 0 57 where a is the upstream drainage area of the river segment we use eqs 1 2 to calculate flood potential maps fig 12 shows flood potential over the state of iowa during the 2016 september flooding event also hourly accumulated mrms radar rainfall for september 22 2016 at 3 00 a m is shown on map 4 discussion the core concept incorporated in hydrovise is the configuration file designed to include essential information about the map data sources visualization styling this feature decreases development effort for web based visualization and evaluation of hydrologic data this also allows for substantial code re use where one source code can serve multiple projects defined as different configuration files the target audience for hydrovise is entry level programmers graduate students research scientists and decision makers that have limited expertise in web development graduate students and researchers working on hydrology and water resources topics could use hydrovise to communicate their results with their mentors or colleagues in an interactive web based environment decision makers e g county emergency managers who rely on hydrologic data during weather related emergencies could inspect the observation data and hydrologic model forecasts using hydrovise however like any software hydrovise is also limited to its core objectives and scope therefore for custom functionality or additional features users must have basic knowledge of web development programming languages extension modules presented in this work allows users to visualize additional data gain insights on their hydrologic problem and evaluate data for a selected time period this ability helps users to identify spatial patterns through hydrologic data visualizations evaluation or validation jackson et al 2019 provide a review of performance metrics for hydrologic predictions in addition to data visualizations hydrovise incorporates multiple performance metrics for the evaluations while new performance metrics can be added to the evaluation extension module users can also use pre computed performance metrics calculated in python or matlab by other open source libraries e g jackson et al 2019 database and web mapping server e g geoserver implementations are useful for handling large space time data visualizations and sharing for example the iowa flood information system developed by the iowa flood center uses a postgresql database for storing and fetching data krajewski et al 2017 tethys uses database and web mapping servers for extensive environmental data swain et al 2016 however dependencies add layers of complexity to these solutions requiring more expertise also it takes more effort to securely host and deploy the solutions that have more dependencies on the other hand a broad community of users in environmental science may not need these tools for their applications therefore we used the concept of data cubes with a file system and a dynamic path creator to handle multi variate space time data using the file system our approach is not a substitution for a database it is an alternative for potential users that have limited knowledge or time to learn of database deployment therefore we included the options to define internal external data services as data sources a data service can be used to access data in a database or a chunk of data in other file formats such as hdf5 netcdf or other compressed data types given that user should have the expertise we have not implemented these data types as they could have different formats and dimensions this capability in hydrovise makes the solution minimal modular and extendible to reduce dependencies and data preparation workload we selected the most common data types such as csv geotiff geojson table 2 that could be easily written or read by users using free and open source software such as qgis python etc 5 summary and future developments our motivation in this work was to develop a practical tool for visualization of hydrologic data and the evaluation of hydrologic models that try to replicate observations we developed an open source software that lowers the barrier for implementation using a configuration rather than code based approach hydrovise allows users to conduct a qualitative and quantitative assessment on hydrologic data using its visualization and evaluation capabilities we have shown four different use cases for hydrologic data the first example illustrated time series data visualizations including real time usgs streamflow data and hydrologic model visualizations and performance evaluations second we provided a use case of a unique module that allows users to visualize forecast time series in the third example we demonstrated grid based multi product soil moisture data visualization in which we used field sensors satellite based and model soil moisture data finally we visualized space time network based hydrologic data by using hydrovise these are only a few example use cases to demonstrate the capabilities and capacity of the developed software hydrovise can accommodate other environmental data that have an inherent space time structure open access publications increase the transparency of the methods results and peer review process in publications however due to the lack of easy to deploy tools and software authors do not show the tendency to publish their companion dataset with their scientific findings hydrovise as an entirely client side and a portable solution could be leveraged for sharing data in open access journals along with published data for more transparency hydrovise is an open source software that could benefit from user community feedback feature requests and developer community for improvements and developments these developments could include adding sqlite for portable serverless database as back end and local file system and spatialite for adding geospatial analyses declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work is partially funded by nasa science utilization of the soil moisture active passive mission program grant nasa grant nnx16aq57g and a project funded under the provisions of section 104 of the water resources research act of 1984 annual base grants 104b distributed through the iowa water center as graduate student supplemental research competition with grant no 2020ia095b and mid america transportation center matc trb rip no 91994 5 with contract number 69a3551747107 we also gratefully acknowledge the user feedback provided by graduate students and research scientists working at the iowa flood center at the university of iowa 
25937,the authors developed a non proprietary web browser based open source software that allows users to visualize and evaluate hydrologic space time data in an interactive environment hydrovise is client side browser based software that interprets a configuration file to construct control elements in the graphical user interface for visualizations of space time data and model simulation evaluations it leverages the concept of three dimensional data cubes that facilitate query in space time and variable dimension s without the requirement for a database system using a configuration file users can define data sources as local file system resources and or external data sources e g online data services this capability makes hydrovise a flexible and portable solution where users can share their hydrologic data in an interactive web environment this paper provides the software description with four distinct example use cases including but not limited to time series data visualization and evaluation grid based and river network based data visualizations keywords web application data management environmental science hydrology evaluation software availability hydrovise is available under mit open source license terms and the source code and examples are available at the following github repository https github com hydrovise hydrovise for documentation and tutorials readers can refer to https github com hydrovise hydrovise wiki 1 introduction the availability of data and computational resources have brought unprecedented opportunities to the hydrologic modeling community for understanding the hydrologic cycle at the same time it has introduced new challenges that demand efficient approaches for visualization communication and data evaluations in recent years there has been significant progress in developing new tools and technologies for visualization analysis and handling real time data e g wong and kerkez 2016 swain et al 2016 brendel et al 2019 open source software oss provide unique opportunities for software development and it has potential advantages over their commercial counterparts an oss can benefit from developer interest user contributions feedback and frequent release ghapanchi et al 2014 that could further improve the user experience and lead to a successful open source solution midha and palvia 2012 identify the most critical factors in the success of oss as user base language translations responsibility assignment and modularity among others vitolo et al 2015 provide an overview of the current web technologies used for environmental big data several oss solutions used these technologies to develop web applications for water resources purposes swain et al 2015 however in most cases solutions have several external dependencies and require expertise or at least adequate knowledge of multiple programming languages frameworks these dependencies make solutions fragile as they complicate the deployment process and ultimately decrease software sustainability murugesan and gangadharan 2012 refer to software sustainability by quality attributes of a system such as modifiability re usability and portability swain et al 2016 developed tethys as a framework to lower the barrier in developing web applications for environmental data visualization and computations tethys uses python python foundation 2016 as the primary programming language with its server side deployment of django django software foundation 2018 it leverages other third party software tools that facilitate computing and visualizations it is a flexible solution for developing web applications in this framework but the user developer requires more than basic knowledge of python and other third party dependencies recently brendel et al 2019 developed sharks for retrieval visualization and analysis of hydrologic and meteorological data sharks uses the shiny apps framework and r programming language as its core programming language the sharks web application allows users to analyze hydrologic data over a user defined watershed previous software solutions for environmental data have made the deployment of web applications easier for environmental scientists however deployment of these solutions could be challenging for a non expert user because of various dependencies furthermore they still rely on a programming languages that are not designed natively for web development therefore for customized functionalities users need to have at least a basic knowledge of javascript css cascading style sheets and html hypertext markup language that are core languages for web application development many previous oss solutions developed for environmental science depend on database systems to handle the space time structure of the data swain et al 2015 provide a detailed list of solutions that employ spatial database systems the decision to use a database in an oss solution depends on the objective and design of the oss solution for example a database could be useful for complex queries and storing relational data on the other hand database may not be needed for hydrologic data visualizations however database as a hard requirement for a software solution adds further barrier for a user with limited database management expertise data access using data services have become more popular among environmental data providers facilitating data sharing and access for example cuahsi s consortium of universities for the advancement of hydrologic science hydroclient provides access to data on an interactive web based platform for the user selected region http data cuahsi org data services deployed by noaa national oceanic and atmospheric administration is another example of a web service that allows users access to extensive environmental observations in recent years advances in web technologies have added new features facilitating the development of applications from native web programming languages such as javascript walker and chapra 2014 for example standardization of javascript with modern ecmascript standard ecma 1999 significantly reduced compatibility issues on different browsers our motivation in this work was twofold first we aimed to develop a web based tool that facilitates the visualization and communication of space time hydrologic data second we intended to significantly lower code barriers for users with minimal web development knowledge therefore we present a non proprietary open source software for hydrologic science that we coined hydrovise hydrovise is a client side application developed by leveraging available web tools and using existing web programming languages it is configurable and deployable on a personal computer or a web server that does not require a supporting database server hydrovise allows users to visualize evaluate and share their hydrologic data in an interactive web environment by preparing the data organizing the data and preparing a configuration file for their project in the next section we describe the main components of the developed software data model and extension modules then we present example use cases common in the hydrologic community in section 4 we discuss the advantages and limitations of the current version of the developed software and its potential usages finally we provide a summary and discussion on future developments 2 software description hydrovise is an open source browser based client side software that lowers the code barrier in web based hydrologic data visualization and evaluations we selected javascript as its core programming language hydrovise is designed and developed as an entirely client side application for increasing the portability of the solution but it can also be deployed as server client if additional server side functionalities are required hydrovise integrates the functionalities of multiple external libraries and internal custom modules it consists of three main components modules configuration and graphical user interface gui modules are internal custom libraries including functions responsible for extension and orchestration of integrated third party libraries and web tools in hydrovise modules handle data acquisition visualizations and user interactions with the gui hydrovise custom libraries control third party libraries web tools and with the use of user defined project configuration files generate interactive map based reports we selected tools and libraries that are maintained regularly and are among the most popular open source javascript libraries table 1 summarizes the external libraries and their usage in hydrovise the project configuration file is a javascript object notation json file that contains a set of information definitions about data sources e g traces space time data marker locations and etc control elements in the gui and the styling of the visualizations the graphical user interface is a web browser based interface created dynamically by the initialization module the content of the gui depends on the information stored in the project configuration file it consists of a map a canvas for visualization of time series and spatial data and other control elements that allow navigating data in different dimensions fig 1 shows a schematic of the gui and structure of the configuration file the configuration objects are numbered with their corresponding elements on the gui further details on the configuration file is provided in the hydrovise documentation fig 2 shows the hydrovise workflow diagram the elements with white and gray background correspond to pre deployment and post deployment stages respectively pre deployment includes configuration and initialization the gui is generated dynamically by the initialization module using the configuration file in other words the absence or presence of control elements in the gui depends on the information provided in the configuration file after initialization user interactions are passed to modules by control elements in the gui modules refer to requested data source using event information that corresponds to a specific element of the data cube see section 2 1 and data are visualized on the gui 2 1 data model structure and types hydrovise adopts the concept of data cubes as its data model initially data cubes were introduced by gray et al 1997 for reducing the dimensions of data based on user queries maidment 2002 introduced the space time variable cube for referencing individual data to corresponding attributes later goodall et al 2008 implemented this concept in integrating time series from different sources for the national water information system nwis fig 3 shows a schematic of the data cubes where the user can query a chunk of data in a different dimension s the atomic element of the data cube can consist of a value a vector or a matrix previous uses of the data cube concept were implemented in a database system e g microsoft sql postgresql this approach is efficient for executing complex queries goodall et al 2008 however the construction and management of the database by itself demands expertise that not every user may have hydrovise eliminates the restricting requirement of a database by using flat file system for a file system implementation space time variable cubes are constructed by organizing the files in folders and sub folders to facilitate the navigation in selected dimension s this organization is defined in the configuration file by the user as a file path template for any given data source example file path template in the configuration file has the form template var time variable comid path format data 0 1 2 csv where curly brackets are substituted with time variable and space comid that user selects on the gui the dynamic path creator uses the path template and user selected variables to replace the placeholders in the template string and create the path to data source s a data source can be a local file a custom data interface using a database query or a web service hydrovise can be used to visualize time series of data associated to any spatial support extent including points lines or polygons for simplicity we selected the most common data types as the baseline data types for space time data e g radar based precipitation demir et al 2015 users can define the underlying grid geometry and the corresponding timestamps for the data for visualizations the data can be provided as comma separated value csv or binary files based on a common identifier comid table 2 summarizes the data types and formats used for handling time series static and space time data the users can also define static map layers e g kml s for adding contextual information to the map for the region of interest examples of these layers could be locations of hydrometeorological gauges stream and river network or administrative boundaries 2 2 map markers users can inspect interactive locations on the map interface by hovering over an object that shows a tooltip tooltips can be customized to show user defined attributes metrics or name of the observation site if available this feature allows users to access information about locations on the map without plotting the time series interactive locations on the map are specified as map markers map markers are a feature collection defined in a geojson formatted file properly described in the configuration file the underlying geometry could be a point line or polygon these spatial features are sensitive to user specified events e g click hover 2 3 visualizations spatial data e g points polygons etc are visualized on the map using the leaflet javascript library leaflet is an open source javascript library for interactive maps we selected leaflet because of its large open source community contributions and capability to extend functionality the basemaps for geographical locations or satellite imagery is provided in the configuration file as a tilemap data source leaflet library loads the map tiles depending on the zoom level and map extent hydrovise uses the plotly javascript library to visualize time series and available timestamps for space time data plotly is a data visualization library with a wide range of chart types we selected this library because of its user developer community and documentation the plotly data canvas is interactive that allows users to zoom pan and inspect data for a given observation point for space time data visualizations e g radar rainfall satellite based soil moisture we use the glify library developed as an extension for leaflet to leverage webgl web technology webgl employs graphical processor unit gpu for rendering graphics in web browsers it is a cross platform api that uses the opengl shading language glsl and runs in the html5 canvas element the visualization of space time data is controlled by a panel that shows available timestamps for each space time dataset when a user selects a timestamp for the spatial data data are visualized and added to the map inventory for additional layer control options data for a given space time dataset could be a geotiff raster file or geometry to be updated using csv or float32 binary file using the configuration file users can add animation control and define a custom color legend for each dataset 2 4 extension modules extension modules extend the core functionality of hydrovise these modules are enabled using user defined directives in the hydrovise configuration file we provide a list of implemented extension modules in the following sections 2 4 1 stage discharge converter the stage discharge relationship or rating curves are used to estimate discharge streamflow from stage observations we have developed an extension module that uses a non linear least square method to fit a polynomial to stage discharge data rating curve and use the fitted function to estimate discharge from stage observations or vice versa 2 4 2 flow categories flow categories or flood categories provide information on the severity of flow conditions for example in the united states the national weather service nws has classified flow stages into five categories nws defines these levels as major moderate minor flooding action near flood and normal no flooding state https water weather gov ahps forecasts php the united kingdom s flood information system https flood warning information service gov uk has a similar definition for flood levels these levels serve as a means of communicating flow conditions to the general public and early warning purposes we have included an extension module that allows users to incorporate flow level categories in streamflow and stage time series visualizations users can provide information about flow categories or historical flow conditions e g flow return periods they can visually inspect the hydrologic model predictions for flow categories 2 4 3 performance evaluations evaluation and validation provide insight into the strengths and limitations of models trying to replicate observed data performance evaluation of hydrologic model predictions helps identify the misrepresentations of the physical processes in a hydrologic model however this task can quickly become cumbersome as the number of dimensions increase we refer to dimensions as different model state variables model setups input rainfall forcings etc therefore it is essential to use interactive tools that allow decision makers model developers to compare data with model outputs using multiple performance metrics with a visual inspection of the predictions provides further confidence in model selection or performance assessment procedure bennett et al 2013 we have developed an extension module that performs evaluations for the user defined time period for selected performance metrics across the defined spatial domain the module incorporates the most common performance evaluation metrics such as kling gupta efficiency kge proposed by gupta et al 2009 and nash sutcliffe efficiency nse among several others also users can import their pre computed performance metrics to visualize their model outputs and their performance 2 4 4 ensemble time series navigator we developed an extension module utilizing plotly time series canvas that allows users to navigate and visualize ensemble time series for a user selected location this module shows the timestamp or the ensemble member number on time series canvas users can add or remove multiple time series to plotly canvas dynamically 2 4 5 watershed boundary outline this module allows users to define a data source for additional geometry related to a clicked spatial feature this geometry could be a watershed boundary or relevant geometry to the selected spatial element from map markers spatial data users can define the data source for the geometry in the configuration file the data source could be a path to the extracted watershed boundary or an extension module that extracts watershed boundaries from dem digital elevation model data sit et al 2019 2 5 deployment the minimum requirement for the implementation of hydrovise is a http server e g apache nginx compatible data types and a web browser potential users can copy the code from the github repository https github com hydrovise hydrovise to a webserver directory hydrovise works as an interpreter of the configuration file where it can serve multiple projects defined as different configuration files therefore providing the configuration file as a url to the interpreter in the browser will load the gui this feature allows users to share their deployed web link with anyone with internet access 3 example use cases in this section we provide four different example use cases of hydrovise implemented by using different configuration files each example illustrates the different capabilities of the software 3 1 time series data visualization and evaluation this example use case illustrates visualization and evaluation of streamflow time series live demos for this example use case is provided below 1 real time and historical usgs streamflow data browser http hydrovise com app config examples ex1 config usgs json 2 hydrologic model evaluations for smap satellite based soil moisture assimilation http hydrovise com app config examples ex1 config json we constructed a simple real time and historical usgs united states geological survey streamflow data browser using a configuration file the streamflow data are defined by creating a dynamic path to the usgs national water information system nwis web service usgs 2019 fig 4 shows an example illustration of a real time usgs streamflow data browser for a region of interest fig 4a and b show stage f t and streamflow m 3 s 1 time series using stage discharge converter module the data for usgs gauge observations are provided with a 15 min interval the basin boundary for a selected gauge is highlighted as light gray following the nws color convention flood categories are shown with magenta red orange and yellow colors defined as major moderate minor flooding and near flooding stages respectively as shown in fig 4 map inventory allows users to inspect the contextual map layers defined in the configuration hide show layer s and add their spatial data vector raster to current view the real time streamflow data browser can be used for monitoring streamflow at the observation locations provided by the user this example is useful for real time hydrologic model evaluations public and emergency management offices that coordinate the preparations for flood damage mitigation the evaluation module developed for hydrovise allows users to conduct hydrologic model performance assessments for a selected time period also users can visualize pre computed performance metrics by defining the data source in the configuration file for demonstration we compare streamflow predictions from the hlm model krajewski et al 2017 for two cases the first case is referred to as the open loop model prediction that does not include any update to model states the second case is a model prediction using data assimilation of smap satellite based soil moisture in hlm s top layer we use ensemble kalman filter evensen 2003 with time dependent variance for perturbations of initial soil moistures referred to as enkfv we show streamflow predictions at the usgs gauge observation locations for the state of iowa for the year 2015 as shown in fig 5 hydrovise populates summary maps of hydrologic model performance metrics for a selected year statistical measure and model setup in this example usgs locations are color coded based on kling gupta efficiency note that locations without reference data for the selected year are shown as light gray circle markers streamflow predictions after smap satellite based soil moisture assimilation fig 5b show improvement in majority of the usgs gauge locations compared to open loop model predictions fig 5a fig 6 illustrates the time series plot of the hlm model streamflow predictions for the two cases and usgs observation at cedar river at cedar rapids the color coded circle markers shows the peak timing difference of the streamflow predictions using enkfv and usgs observations 3 2 hydrologic assessment of precipitation forecasts this is an example hydrovise use case for visualization of streamflow forecasts the live demo for two cases in this example could be accessed at streamflow forecast time series for each issue time http hydrovise com app config examples ex2 config issue time json streamflow time series based on forecast lead time http hydrovise com app config examples ex2 config lead time json real time flood forecasting models use quantitative precipitation estimation qpe products for simulating historical streamflow and quantitative precipitation forecast qpf products for their forecasts flood forecasting models obtain the precipitation forcing from atmospheric models because of the chaotic nature of atmospheric flows atmospheric models use data assimilation techniques to account for new observations in model forecasts flood forecasting models update their streamflow forecasts as new precipitation forecasts are issued therefore it is essential to understand the streamflow forecasts in the context of precipitation forecasts for this example we used hourly accumulated mrms multi radar multi sensor and ifc iowa flood center qpe products for simulating historical streamflow for streamflow forecasts we use hrrr high resolution rapid refresh atmospheric model s benjamin et al 2016 qpf hrrr product is issued every hour with lead times starting from zero to 18 hour streamflow forecasts from hlm model are provided for the next five days starting from every precipitation forecast issue time we use the ensemble time series navigator described in section 2 4 4 to answer two key questions how good were streamflow predictions driven by precipitation forecasts at different issue time how do streamflow predictions evolve with the different precipitation forecast lead times to answer the first question we organized streamflow predictions by precipitation forecast issue times fig 7 shows usgs gauge observation and streamflow forecasts using mrms and ifc qpe and hrrr qpf products for a user selected issue time the time series show streamflow forecasts for the next five days from a selected forecast issue time as shown in this figure users can navigate to streamflow forecasts issued at any time for the selected usgs gauge location the extension module allows users to add and remove time series for a chosen product for visual comparisons with usgs gauge observations we answer the second question by organizing streamflow forecasts data by their lead time fig 8 illustrates usgs gauge observations and streamflow forecast time series for a 7 hour forecast lead time streamflow forecast time series corresponding to different lead times can be added to the time series plot shown in this figure this extension module allows users to evaluate the predictions based on forecast lead time 3 3 grid based hydrometeorological data visualization this example illustrates the visualization of a space time process on a regular grid the live demo for this example use case is available at http hydrovise com app config examples ex3 config json as one of the critical hydrologic variables soil moisture controls runoff production e g crow et al 2018 jadidoleslam et al 2019b and agricultural productivity e g hargreaves 1975 smos soil moisture ocean salinity and smap soil moisture active passive are the two satellite missions that provide soil moisture information by using remote sensing techniques e g kerr et al 2010 o neill et al 2015 in recent years several studies have been conducted for validation comparison and evaluation of satellite based soil moisture with field sensor observation networks e g colliander et al 2019 ma et al 2019 jackson et al 2012 tavakol et al 2019 an interactive web based tool can facilitate visualization and validation of satellite based soil moisture products that could complement current validation studies we provide an example use case of hydrovise for visualization of satellite ground based soil moisture and hydrologic model storage fig 9 shows soil moisture time series from smap smos field sensor observations and hydrologic model soil storage from the hillslope link model hlm krajewski et al 2017 smap data are posted on ease v2 grid brodzik et al 2012 which we use as the reference geometry shown with the light gray color for comparison smos satellite soil moisture data are gridded to the smap grid also we show model storage as percentiles to visualize the soil moisture sub grid variability originating from hlm model with higher spatial resolution iowa flood center operates a network of over 30 field soil moisture sensors that are shown with white circles field sensor soil moisture data are averaged for more than one collocated sensors in smap grid fig 9 shows that soil moisture oscillations from model and different products show good agreement for the selected grid highlighted on the map the sub grid variability decreases for higher median soil moisture values which is consistent with findings from previous studies e g famiglietti et al 2008 jadidoleslam et al 2019a fig 10 a illustrates smap satellite based soil moisture estimation over the state of iowa for a satellite overpass approximately at 6 00 p m on may 23 2016 fig 10b shows hourly mrms radar based rainfall map for at 3 00 p m on may 23 2016 on the same map higher soil moisture regions from smap in central iowa could be described by antecedent rainfall these figures illustrate the hydrovise ability to display raster data 3 4 river network based data visualization this example illustrates the visualization of a river network based data the live demo for this example use case is available at http hydrovise com app config examples ex4 config json portable version of this example use case is included in github examples folder the live demo can be accessed at http hydrovise com app config examples ex4 min config json a river network has self similar structure that describes surface water movement in a watershed or a region river network based visualizations can help users inspect the streamflow evolution in space and time in this example we use hydrovise to visualize flood potential over state of iowa s river network similarly it is possible to visualize other variables such as water quality and contaminant loads fig 11 shows the river network for the state of iowa and streamflow time series for a user selected river segment the network includes more than 100 000 streams with horton order three and above users can inspect streamflow time series for selected river segment s by providing time series data source in the configuration file flood potential index is an indicator for communicating the streamflow condition irrespective of upstream drainage area of a river segment it is defined as the ratio of streamflow to mean annual flood quintero et al 2020 1 i q q m a f where q is streamflow m 3 s 1 and q m a f is the mean annual flood for each river segment quintero et al 2020 derived a power law relationship for mean annual flood for the state of iowa 2 q m a f 3 12 a 0 57 where a is the upstream drainage area of the river segment we use eqs 1 2 to calculate flood potential maps fig 12 shows flood potential over the state of iowa during the 2016 september flooding event also hourly accumulated mrms radar rainfall for september 22 2016 at 3 00 a m is shown on map 4 discussion the core concept incorporated in hydrovise is the configuration file designed to include essential information about the map data sources visualization styling this feature decreases development effort for web based visualization and evaluation of hydrologic data this also allows for substantial code re use where one source code can serve multiple projects defined as different configuration files the target audience for hydrovise is entry level programmers graduate students research scientists and decision makers that have limited expertise in web development graduate students and researchers working on hydrology and water resources topics could use hydrovise to communicate their results with their mentors or colleagues in an interactive web based environment decision makers e g county emergency managers who rely on hydrologic data during weather related emergencies could inspect the observation data and hydrologic model forecasts using hydrovise however like any software hydrovise is also limited to its core objectives and scope therefore for custom functionality or additional features users must have basic knowledge of web development programming languages extension modules presented in this work allows users to visualize additional data gain insights on their hydrologic problem and evaluate data for a selected time period this ability helps users to identify spatial patterns through hydrologic data visualizations evaluation or validation jackson et al 2019 provide a review of performance metrics for hydrologic predictions in addition to data visualizations hydrovise incorporates multiple performance metrics for the evaluations while new performance metrics can be added to the evaluation extension module users can also use pre computed performance metrics calculated in python or matlab by other open source libraries e g jackson et al 2019 database and web mapping server e g geoserver implementations are useful for handling large space time data visualizations and sharing for example the iowa flood information system developed by the iowa flood center uses a postgresql database for storing and fetching data krajewski et al 2017 tethys uses database and web mapping servers for extensive environmental data swain et al 2016 however dependencies add layers of complexity to these solutions requiring more expertise also it takes more effort to securely host and deploy the solutions that have more dependencies on the other hand a broad community of users in environmental science may not need these tools for their applications therefore we used the concept of data cubes with a file system and a dynamic path creator to handle multi variate space time data using the file system our approach is not a substitution for a database it is an alternative for potential users that have limited knowledge or time to learn of database deployment therefore we included the options to define internal external data services as data sources a data service can be used to access data in a database or a chunk of data in other file formats such as hdf5 netcdf or other compressed data types given that user should have the expertise we have not implemented these data types as they could have different formats and dimensions this capability in hydrovise makes the solution minimal modular and extendible to reduce dependencies and data preparation workload we selected the most common data types such as csv geotiff geojson table 2 that could be easily written or read by users using free and open source software such as qgis python etc 5 summary and future developments our motivation in this work was to develop a practical tool for visualization of hydrologic data and the evaluation of hydrologic models that try to replicate observations we developed an open source software that lowers the barrier for implementation using a configuration rather than code based approach hydrovise allows users to conduct a qualitative and quantitative assessment on hydrologic data using its visualization and evaluation capabilities we have shown four different use cases for hydrologic data the first example illustrated time series data visualizations including real time usgs streamflow data and hydrologic model visualizations and performance evaluations second we provided a use case of a unique module that allows users to visualize forecast time series in the third example we demonstrated grid based multi product soil moisture data visualization in which we used field sensors satellite based and model soil moisture data finally we visualized space time network based hydrologic data by using hydrovise these are only a few example use cases to demonstrate the capabilities and capacity of the developed software hydrovise can accommodate other environmental data that have an inherent space time structure open access publications increase the transparency of the methods results and peer review process in publications however due to the lack of easy to deploy tools and software authors do not show the tendency to publish their companion dataset with their scientific findings hydrovise as an entirely client side and a portable solution could be leveraged for sharing data in open access journals along with published data for more transparency hydrovise is an open source software that could benefit from user community feedback feature requests and developer community for improvements and developments these developments could include adding sqlite for portable serverless database as back end and local file system and spatialite for adding geospatial analyses declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work is partially funded by nasa science utilization of the soil moisture active passive mission program grant nasa grant nnx16aq57g and a project funded under the provisions of section 104 of the water resources research act of 1984 annual base grants 104b distributed through the iowa water center as graduate student supplemental research competition with grant no 2020ia095b and mid america transportation center matc trb rip no 91994 5 with contract number 69a3551747107 we also gratefully acknowledge the user feedback provided by graduate students and research scientists working at the iowa flood center at the university of iowa 
25938,as the world becomes more urbanized and heavy precipitation events increase in frequency and intensity urban pluvial flooding is an emerging concern recent advances in hydrologic and hydraulic modeling high resolution quantitative precipitation forecasting and ensemble forecasting have improved the ability to evaluate flash flooding potential in urban areas the probabilistic urban flash flood information nexis puffin app integrates these components into a tool to evaluate the probability of an urban flash flood event and to identify specific infrastructure components at risk puffin uses a combination of deterministic and probabilistic precipitation forecasts to provide analyses over a range of spatial and temporal resolutions a case study for the city of roanoke virginia in the united states demonstrates how puffin can be used to assess the risk of urban flooding however with minimal modification the application can be adapted for implementation in other locations keywords quantitative precipitation forecasts ensemble forecasts hydrology hydraulics modeling urban flash flooding data analysis 1 introduction urban pluvial flooding has received considerable media attention in recent years due to its repetitive costly and systemic impacts in contrast to its fluvial and coastal counterparts urban flooding occurs more frequently and is caused when heavy precipitation collects on the land surface and exceeds the capacity of drainage systems to capture and convey stormwater galloway et al 2018 national academies of sciences and medicine 2019 furthermore the effects of urban flooding are not constrained to floodplains and areas near bodies of water galloway et al 2018 the severity extent and location of urban flooding are affected by many factors including land use land cover and drainage system design and condition national academies of sciences and medicine 2019 as urbanization continues globally and heavy precipitation events become more intense and frequent wuebbles et al 2017 urban flooding is projected to become an even more pervasive issue thus there is a need for improved tools for urban flash flood prediction predicting flash flooding in urban areas is challenging because of complex spatial variations in land use and land cover the lack of a well defined natural drainage network and interactions between man made structures including buildings channels culverts pipes roads tunnels and stormwater management infrastructure chahinian et al 2019 hapuarachchi et al 2011 javier et al 2007 rfcdmt 2003 the predominant method for flash flood prediction flash flood guidance ffg compares measured precipitation to established threshold values and was designed to predict flash flooding in small stream channels rfcdmt 2003 ffg threshold values are derived by running hydrological models to determine the precipitations and durations resulting in discharges that exceed bankfull conditions and flash flood watches and warnings are issued if measured precipitation exceeds the threshold historically the us national weather service nws implementation of ffg was based on lumped model parameters and basin averaged precipitation gourley et al 2014 however enhanced detection of the spatial variability of flash flooding has been achieved through the application of distributed hydrologic models for ffg in methods such as gridded flash flood guidance gffg schmidt et al 2007 distributed hydrological model threshold frequency dhm tf reed et al 2007 and the flooded locations and simulated hydrographs flash project gourley et al 2017 in addition the ability to evaluate flooding potential in urban areas has been improved due to advances in high spatial and temporal resolution quantitative precipitation estimation chen and chandrasekar 2015 cifelli et al 2018 javier et al 2007 rafieeinasab et al 2015 quantitative precipitation forecasts sharif et al 2006 yoon 2019 and ensemble forecasting lee et al 2013 schellart et al 2014 yang et al 2015 precipitation forecasts have also been used to estimate sewer flows and urban flood discharge and inundation using hydrology and hydraulics models lee et al 2013 schellart et al 2014 yoon 2019 despite the advances in high resolution quantitative precipitation forecasts ensemble forecasting and hydrology and hydraulics modeling there are few operational urban flash flood forecasting systems existing systems can be classified into two categories those based on pre simulated scenarios and those based on real time modeling henonin et al 2013 in systems utilizing pre simulated scenarios a combination of radar rain gauge and or water level measuring station data is used as input to a hydrologic model the outputs from the model are then used to select the most probable flooding scenario from a library of pre computed flood risk maps notable systems using this configuration include espada in nîmes france raymond et al 2006 and the texas medical center flood alert system fas in houston united states fang et al 2011 like those based on pre simulated scenarios systems utilizing real time modeling also use a combination of radar rain gauge and or water level measuring station data to run a hydrologic and hydraulic model in this method however the simulation results are used directly to evaluate the risk of flash flooding systems using this configuration include the dallas fort worth urban demonstration network in the united states habibi et al 2016 the asian institute of technology ait system in bangkok thailand mark and hosner 2004 and hidromet in barcelona spain montero et al 2010 it is worth noting that the drainage network is modeled in both the ait and hidromet systems existing urban flash flood forecasting systems have several important limitations first whereas probabilistic forecasts are routinely produced for use in evaluating the probability of fluvial flooding the existing urban flash flood forecasting systems are deterministic in nature thus they do not include any information about the uncertainty in their forecasts second real time modeling systems are computationally intensive and may require expensive high powered computing third systems that do not model the drainage network cannot identify which infrastructure components may be at risk for flooding finally the systems were developed for specific areas and may use local data streams thus they cannot be easily adapted and implemented in other locations to address the shortcomings in current urban flash flood prediction an application called the probabilistic urban flash flood information nexus puffin https bigbadcrad shinyapps io puffin was developed by integrating quantitative precipitation forecasts with real time modeling and data visualization and analysis tools puffin is tailored to stormwater managers emergency managers hydrologists and or meteorologists that need to evaluate the probability of an urban flash flood event in real time and to identify specific infrastructure components at risk the app incorporates data from multiple precipitation forecasts with varying spatial and temporal resolutions and provides immediately useful information for decision making the interface of the puffin application was designed with the objective of providing an intuitive platform for users with an understanding of basic hydrology and hydraulics although puffin was conceived and implemented for the city of roanoke virginia the application was developed to work for any location within the continental united states this paper describes the components of the puffin application and presents a case study demonstrating how puffin can be used to assess the risk of urban flash flooding 2 software implementation 2 1 architecture puffin was developed using the open source r language r was chosen over alternatives such as python because r is typically regarded as having superior data visualization features and because it includes packages that facilitate the retrieval and processing of meteorological data from the national oceanic and atmospheric administration noaa operational model archive and distribution system nomads server and interfacing with the environmental protection agency epa storm water management model swmm furthermore r facilitates the development of applications with its shiny application framework https shiny rstudio com which allows developers to create desktop and web based applications using entirely r code finally r and shiny are both free and open source so that others can update and customize puffin s publicly available source code brendel et al 2020a besides r puffin has two additional software dependencies first puffin requires the national centers for environmental prediction ncep wgrib2 program noaa 2019b to extract data from the quantitative precipitation forecast qpf datasets that cannot be downloaded directly into r from the noaa nomads server noaa 2019a second puffin requires the epa swmm software to perform the hydrology and hydraulics simulations both wgrib2 and swmm are freely available online and can be downloaded from noaa 2019b and epa 2020 respectively due to puffin s dependence on the wgrib2 and swmm programs extracting certain qpf datasets and performing the model simulations cannot be performed within a traditional cloud based shiny app instance thus puffin was originally conceived as a desktop based shiny app the desktop version of puffin can be launched in any r environment console r rgui rstudio etc and the application s source code is released online under the mit license brendel et al 2020a with this version puffin must be installed for each individual user and updates to the swmm model or puffin code must be repeated for each individual deployment in addition computations i e processing forecast data running swmm querying the results etc must be performed separately for each deployment consequently the desktop version of puffin is best suited for testing and small scale implementation for larger scale deployment puffin was reconfigured into a separate cloud based version overall the user experience in the cloud version of puffin is identical to that of the desktop version but the two versions differ in how they handle processing the qpf data and performing the model simulations fig 1 in the desktop configuration the shiny app performs all requisite computations i e processing forecast data running swmm and providing visualization and analysis features however in the cloud configuration the computational load is divided between a script hosted on a windows server and a web based shiny app the server script processes the forecast data performs the model simulations and stores the results in a mysql database whereas the shiny app queries the results and provides the visualization and analysis features mysql was chosen for data storage because it is free and open source it supports a wide variety of data types and it offers remote data access with high security features benefits of the cloud configuration of puffin include that it can be accessed online instead of requiring separate installations for each user and that it ensures that all users are using the same version of the app and swmm model in addition the response time of the shiny app interface is reduced because it is only required to retrieve the pre processed simulation results from the database however reliance of the cloud version on additional servers and databases makes it more complicated and expensive to set up than the desktop version 2 1 1 mechanics two components a user interface object and a reactive server function form puffin s shiny framework puffin s user interface was constructed using r code and customized with html css and javascript the application is controlled via input widgets e g checkboxes switches and dropdown menus that pass information to the shiny server function the shiny framework is based on a reactive programming model which updates outputs instantly as inputs are modified chang et al 2020 thus when users make selections or adjustments using puffin s input widgets the shiny server function immediately performs the requisite computations and updates the output objects e g maps tables and plots displayed in the user interface the mechanics of puffin are illustrated in fig 1 and described as follows puffin requires two files in order to retrieve the qpf data and perform the model simulations the swmm model file and a shapefile of the modeled subcatchments including the swmm id for each subcatchment puffin performs model simulations using whichever swmm model is saved in its model directory thus users can overwrite an existing model or replace an existing model with a model with a different name and puffin will use the new model when performing the simulations because swmm does not natively include a spatial reference the subcatchments shapefile is necessary for puffin to determine the location of the modeled watershed after identifying the location of the watershed puffin retrieves the autonowcaster anc high resolution rapid refresh hrrr high resolution ensemble forecast href and short range ensemble forecast sref forecasts section 2 3 1 for the area the data for each precipitation forecast is stored in a set of r data frames and puffin retrieves the precipitation accumulation data from these data frames as necessary to account for spatial variations in precipitation puffin creates an inverse distance weighted precipitation time series for each swmm subcatchment based on each precipitation forecast section 2 3 3 then puffin updates the swmm model settings and precipitation time series and runs the swmm model once for each for forecast i e anc hrrr href and each of the 26 sref ensemble members section 2 3 4 in the desktop version of puffin the simulation results from each swmm model run are joined and stored in independent r data frames for the subcatchment link and node results conversely in the cloud version of puffin the simulation results are stored in the mysql database then puffin queries and reshapes the data from the r data frames desktop version or mysql database cloud version as necessary for the various output and analysis features in the app s shiny user interface the settings page in puffin s shiny user interface provides users with various inputs to configure how the qpf data and swmm simulation results are processed see section 2 2 2 in the desktop version of puffin these inputs are used directly by the shiny app s reactive server function while retrieving the forecast data and running swmm however in the cloud version of puffin the script that retrieves the qpf data and runs the swmm model is hosted separately from the shiny app that provides the user interface see section 2 1 therefore to enable users to change puffin s settings from the web based shiny interface the settings are first saved to the mysql database and then retrieved by the windows server script before the script retrieves the qpf data and processes the model simulation results 2 1 2 implementation puffin has been deployed for the city of roanoke a medium sized urbanized area in southwest virginia the city has experienced recurring flash flooding in its central business district during brief intense rainfall see e g chittum 2017 this flooding has had detrimental economic effects for the city and has posed a hazard to health and safety thus puffin was conceived as an early warning system that the city stormwater managers and emergency managers could use to evaluate the probability of a flash flood occurring from an approaching storm and then communicate the information to the necessary stakeholders the cloud based configuration of puffin is currently deployed for the city online at https bigbadcrad shinyapps io puffin using the amazon web services aws https aws amazon com platform to host puffin s windows server and mysql database and the shinyapps io http www shinyapps io platform to host puffin s shiny app interface these platforms were chosen as low cost alternatives to configuring and maintaining city owned servers and databases for puffin in addition both platforms offer scalable hosting plans ranging from free options to professional options with increasing performance with higher tier plans under the current aws hosting plan puffin s windows server instance has one virtual cpu and 0 5 gb of ram and puffin s mysql database has two virtual cpus 1 gb of ram and 20 gb of available storage space with the current shinyapps io hosting plan each puffin instance has an available 8 gb of memory for computations to maximize the performance of puffin for concurrent users the shinyapps io server has been configured to rapidly trigger the addition of new worker processes and application instances to spread the computational load as usage of puffin increases and or more computational power or database storage is required e g the swmm model is updated to a larger more complex model or the time step for the model outputs is reduced then the aws and shinyapps io hosting plans can be upgraded as necessary 2 2 graphical user interface the puffin user interface consists of three pages home forecast and settings upon initialization of the application the puffin home screen fig 2 presents users with two buttons to access the forecast and settings pages the forecast page fig 3 includes a sidebar menu containing various inputs and controls as well as a main panel that contains puffin s map plot and table outputs the sidebar menu and main panel are dynamically linked and selections on the sidebar menu determine which swmm outputs are displayed summarized and mapped in the main panel the settings page section 2 2 2 includes inputs control how the qpf data is processed and to select which swmm parameters to simulate help buttons have been included throughout puffin s user interface and when clicked display modals with messages describing the various inputs outputs and options sections 2 2 1 2 2 2 describe the aforementioned components in greater detail 2 2 1 forecast page 2 2 1 1 sidebar menu the forecast page s sidebar menu contains inputs and options to control the interactive map object summary table forecast timeseries plot forecast peaks boxplot and group analysis table outputs sections 2 2 1 1 2 2 1 6 located in the page s main panel due to puffin s reactive shiny framework the outputs in the main panel are reactive to the sidebar menu and are immediately updated as selections and adjustments are made using the sidebar menu controls puffin includes two options to retrieve new precipitation forecasts section 2 3 1 first users can click the manual update button to manually have puffin check to see if any new forecasts have been released however users can also enable the auto update switch to have puffin automatically check for new forecasts every 5 min after puffin checks for new forecasts a modal is displayed to tell the user whether any new forecasts were released if new forecasts were released then puffin automatically retrieves the forecast data and a summary of the forecasts is displayed in a table in the sidebar menu fig 4 data in the summary table includes the name of the forecast product the time the forecast was released and the peak precipitation intensity in hr or mm hr depending on swmm model units users can set a peak precipitation intensity threshold on puffin s settings page section 2 2 2 and if any forecast s peak precipitation intensity exceeds that threshold value then a warning modal will be displayed to alert the user inputs fig 5 located in the sidebar menu allow users to customize which forecast products and swmm output parameters are displayed and summarized in the main panel outputs first a group of colored checkboxes allows users to select which forecast product results are used to symbolize the objects in the interactive map section 2 2 1 2 and are included in the main panel s plot and table outputs sections 2 2 1 3 2 2 1 6 below these inputs dropdown menus allow users to select the swmm output parameter e g flow rate capacity water depth summarized in the plot and table outputs for the model s subcatchments links and nodes finally slider inputs allow users to specify green yellow and red ranges for each parameter that control how the subcatchments links and nodes are symbolized in the interactive map and how the model results are analyzed for the table outputs sections 2 2 1 5 2 2 1 6 objects are assigned to the green yellow or red color group based on the object s overall peak simulated value maximum value from swmm simulations for each precipitation forecast for the selected output parameter the green range includes all objects with peak simulated values from zero to less than or equal to the first slider handle e g 0 2 for subcatchment rainfall rate in fig 5 the yellow range includes all objects with peak simulated values greater than the value of the first slider handle and less than or equal to the value of the second slider handle e g 0 5 for subcatchment rainfall rate in fig 5 and the red range includes all objects with peak simulated values greater than the value of the second slider handle the maximum value of the red range is the maximum simulated value for the parameter rounded up the interval between each selectable value on the slider is calculated based on the range between the minimum and maximum values on each slider two other inputs are included in the sidebar menu on the forecast page first a pair of dropdown menus allow users to locate a swmm object i e subcatchment link or node on the interactive map section 2 2 1 2 based on the object s swmm id after a user selects an object using the inputs puffin automatically zooms and centers the map to the object in addition puffin updates the main panel s object summary table forecast timeseries plot and forecast peaks boxplot outputs sections 2 2 1 3 2 2 1 5 to display the data for the selected object and parameter 2 2 1 2 interactive map puffin s forecast page fig 3 includes a large interactive map fig 6 in which users can zoom and pan around their modeled watershed to view the swmm model s subcatchments links and nodes a layer control menu is included in the map and allows users to toggle on off the swmm objects and their respective labels and switch the basemap between the default map a topographic map and satellite imagery by default subcatchments are symbolized in light blue links are symbolized in dark blue and nodes are symbolized in black however the objects can also be symbolized according to green yellow and red ranges for the selected swmm output parameter as specified using the dropdown menu and slider inputs in the sidebar menu section 2 2 1 1 fig 5 the map is also dynamically linked to the object summary table section 2 2 1 5 the forecast timeseries plot section 2 2 1 3 and the forecasts peaks boxplot section 2 2 1 4 thus if a user clicks on an object in the map then the table and plot outputs are updated to display the data for the selected object finally a toggle switch is included to display the spatial extent of the downloaded forecasts on the map fig 7 2 2 1 3 forecast timeseries plot a timeseries plot fig 8 is located prominently beneath the interactive map on puffin s forecast page fig 3 this plot is dynamically linked to the map and the sidebar menu inputs and displays the swmm simulation results for the swmm object i e subcatchment link or node selected by clicking on the map or via the sidebar menu inputs the plot s y axis displays the value of the swmm output parameter e g flow rate capacity water depth specified in the sidebar menu inputs and the plot s x axis displays the date time of the data points results are included for the swmm simulations using each individual precipitation forecast with one line for anc one line for hrrr one line for href and individual lines for each of the 26 sref ensemble members thus if all forecast products are displayed then the timeseries plot will include 29 lines these lines are color coded by forecast product with the anc simulation results symbolized in red hrrr results in yellow href results in green and the sref results symbolized in shades of blue fig 8 users can hover their cursor over the plot to display a tooltip that displays the forecast model date time and value of the nearest datapoint for any dataset in the plot see section 2 4 2 fig 8 2 2 1 4 forecast peaks boxplot the forecast peaks boxplot fig 9 is located adjacent to the forecast timeseries plot on puffin s forecast page fig 3 and provides a visual summary of the peak value from each swmm simulation included in the timeseries plot like the forecast timeseries plot the forecast peaks boxplot displays the data for the swmm object selected by clicking on the map or via the sidebar menu inputs the y axis of the boxplot corresponds to the y axis of the forecast timeseries plot and the peak values for the swmm simulations using the 26 sref ensemble members are summarized with the box and whisker plot that includes the lower whisker 25 quantile median 75 quantile upper whisker and outliers represented with blue circles peak values for the swmm simulations using the anc hrrr and href forecasts are displayed with red yellow and green x s respectively as with the forecast timeseries plot users can hover their cursor over the forecast peaks boxplot to display a tooltip that includes the name of the forecast product the name of the summary statistic and the value of the summary statistic for any dataset in the plot see section 2 4 2 2 2 1 5 object summary table like the forecast timeseries and forecast peaks plots the object summary table fig 10 is dynamically linked to the map and sidebar menu inputs and displays a summary of the swmm simulation results for the selected swmm object data is summarized in the table according to the green yellow and red ranges for the selected swmm output parameter as specified using the slider inputs in the sidebar menu section 2 2 1 1 fig 5 the table includes the minimum and maximum value for each color range as well as the percentage of the swmm simulation runs 29 total for each of the precipitation forecasts that had a peak value within the range for the selected subcatchment link or node see section 2 4 3 2 2 1 6 group analysis table the group analysis table see e g tables 2 and 3 can be viewed by selecting the group analysis view in the sidebar menu and was included in puffin to allow users to collectively analyze specific swmm objects objects can be included in the group analysis by either clicking a toggle button located below the object summary table or by selecting the object by its swmm object id using checkboxes in a set of tables located below the group analysis table the group analysis table includes columns which display the swmm object type as well as the number of selected objects and the selected swmm output parameter for the respective object type like the object summary table analysis in the group analysis table is based on the green yellow and red ranges for the selected swmm output parameter as specified using the slider inputs in the sidebar menu section 2 2 1 1 fig 5 the group analysis table includes the minimum and maximum values of each color range the percentage of the selected objects that had an overall peak value object s single maximum simulated parameter value from the swmm results for each precipitation forecast within the respective range and finally the percentage of the individual simulation timeseries for all of the selected objects that had an individual peak within the respective color range see section 2 4 3 2 2 2 settings page puffin s settings page fig 11 includes a variety of inputs and options to allow users to adjust the application settings first inputs are provided to allow users to adjust the reporting time step for the swmm simulation outputs as well as swmm s snow catch factor used to correct gauge readings for snowfall additional checkbox inputs allow users to specify which swmm output parameters are stored and available for display and analysis furthermore settings are included to specify the values of the coefficients in the nws relationship used to convert the anc forecast from radar reflectivity to precipitation intensity section 2 3 1 1 puffin allows users to manually specify these coefficient values or to automatically retrieve them from nws websites based on river forecast center and radar id section 2 3 1 1 puffin also includes two options to control how the spatial distribution of precipitation is represented in swmm users can choose to generate a unique precipitation timeseries for each swmm subcatchment through inverse distance weighting interpolation of the precipitation forecasts or they can choose to assign precipitation timeseries to each swmm subcatchment based on the nearest forecast grid cell section 2 3 3 in addition puffin allows users to specify a peak precipitation threshold value and if any precipitation forecast has a peak intensity greater than the threshold then puffin will display a warning model to alert the user finally in the desktop version of puffin inputs are provided to enable users to upload a new swmm model and a new subcatchments shapefile required for puffin to determine spatial location of the modeled watershed see section 2 1 1 2 3 data retrieval processing 2 3 1 precipitation forecast products puffin utilizes a set of four national centers for environmental prediction ncep precipitation products autonowcaster anc high resolution rapid refresh hrrr high resolution ensemble forecast href and short range ensemble forecast sref these grid based products were chosen to provide a mix of deterministic and probabilistic forecasts with a range of spatial and temporal resolutions a summary of the four precipitation products is included in table 1 and the individual products are described in greater detail in the following sections 2 3 1 1 autonowcaster anc developed by the national center for atmospheric research ncar the anc system mueller et al 2003 has been integrated into the multi radar multi sensor mrms system running at ncep anc produces two primary deterministic time and space specific 0 1 h nowcast products convective likelihood field and final thunderstorm nowcast the convective likelihood field nowcast indicate regions of probable storm initiation and or sustainment and are based on multiple data ingest products including weather surveillance radar 1988 doppler wsr 88d surface satellite sounding and numerical weather prediction nwp in contrast the final thunderstorm nowcast product depicts 1 h storm extrapolation including growth and decay as well as areas in which new storms are likely to initiate meteorological development laboratory n d nowcasts are produced approximately every 10 min at a 2 5 km grid resolution across the contiguous united states gridded binary grib files of the anc data produced over the past two days are archived in noaa s mrms server and there is an approximately 6 min lag between when nowcasts are produced and when they are available for download as the anc system does not directly forecast rainfall accumulation puffin must estimate rainfall from the final thunderstorm nowcast forecasted reflectivity each nowcast represents the storm extrapolation for only 1 h from the time the forecast was generated consequently each nowcast can only be used to estimate a single forecasted rainfall rate therefore to account for temporal variations in precipitation puffin joins multiple consecutive nowcasts in order to produce a precipitation timeseries for each anc grid cell to do this puffin first identifies all of the anc nowcasts generated for the next hour by scraping noaa s mrms server this typically consists of 5 6 forecasts because they are generated every 10 min i e puffin retrieves the final thunderstorm nowcasts generated for 10 20 30 40 50 and 60 min from the present time then each nowcast is downloaded as a grib file and imported into r using the rnomads r package bowman and lees 2015 while importing the grib files the nowcast data is subset to include only data from the anc grid cells that are located within or immediately surrounding the modeled once in r the final thunderstorm nowcast data is converted from decibels of reflectivity dbz to reflectivity z 1 d b z 10 l o g 10 z then rainfall rate can be estimated using the nws empirical relationship between reflectivity and rainfall 2 z a r b where z is the radar reflectivity mm6 m 3 and r is the rainfall rate mm h 1 the default values for a and b are 300 and 1 4 respectively fulton et al 1998 however the nws adjusts the a and b coefficient values for individual radars depending on location season and storm type puffin can automatically retrieve the current a and b values for nws radars based on their river forecast center and radar id section 2 2 2 however programmatically accessible coefficient values were only found for the nws s lower mississippi middle atlantic north central and ohio river forecast centers therefore puffin also allows users to manually specify the a and b coefficient values section 2 2 2 to use when estimating rainfall for watersheds radars located outside of these forecast center regions when estimating rainfall from radar reflectivity rain rate thresholds are typically used to correct for extreme measurements since the nws z r relationship can produce unreasonably high rainfall intensities in the hail cores of thunderstorms a hail cap is applied to limit the maximum estimated precipitation intensity the nws default threshold value is 53 dbz which corresponds to a rainfall intensity of 104 mm h 1 fulton et al 1998 conversely to account for clear air returns returns due to dust insects birds etc and not from precipitation a rain rate threshold is also applied to limit the minimum estimated precipitation intensity therefore when puffin estimates the anc rainfall all reflectivity values above 53 dbz are assumed to be the result of hail presence and are reset to 53 dbz and all reflectivity values below 25 dbz are assumed to be the result of clear air returns and are eliminated sharif et al 2006 2 3 1 2 high resolution rapid refresh hrrr hrrr is a real time hourly updated cloud resolving convection allowing atmospheric model developed by noaa s earth system research laboratory esrl to provide frequently updated high resolution short range weather forecasts designed to complement noaa s continental scale 13 km grid hourly updated assimilation modeling rapid refresh rap system hrrr provides deterministic forecasts at a higher 3 km grid resolution for the continental united states alaska hawaii and the caribbean noaa 2020 the rap and hrrr systems are used for the analysis and short range precipitation inputs to the national water model owp n d and an update of the systems from rapv4 hrrrv3 to rapv5 hrrrv4 is planned for june 2020 noaa 2020 an experimental hrrr ensemble is also being developed and tested but output data from the ensemble is not currently available for download dowell et al 2018 consisting of a numerical forecast model with an analysis assimilation system to initialize the model the hrrr system generates forecasts by using data from the rap system to perform a 1 h spin up that is followed by the assimilation of radar reflectivity data every 15 min ncep n d b hrrr forecasts 0 18 h in 1 h increments are produced for 81 parameters including accumulated precipitation and updated every hour noaa 2019a noaa archives the hrrr forecast data produced over the past two days in the grid analysis and display system distributed oceanographic data system grads dods data server wielgosz et al 2003 and data is available for download approximately 80 min after it is generated noaa 2020 puffin imports the hrrr forecast data directly into r from the grads dods server to do this puffin first identifies the latest hrrr model run using the getdodsdates function in the rnomads r package bowman and lees 2015 then puffin uses the spatial extent of the uploaded subcatchments shapefile section 2 1 1 to determine the indices of the hrrr grid cells located within or immediately surrounding the modeled watershed finally puffin retrieves the hrrr accumulated precipitation forecast timeseries for each of these hrrr grid cells from the grads dods server using the dodsgrab function in the rnomads r package bowman and lees 2015 2 3 1 3 high resolution ensemble forecast href the href system is a multi model multi physics multi initial condition convection allowing model cam ensemble developed by the ncep environmental modeling center emc as a resource for convective forecasting gallo et al 2018 spc 2020 in the current href v2 1 configuration implemented in 2019 the system produces forecasts based on the hrrr model the high resolution window hrw weather research and forecasting advanced research wrf arw model the national severe storms laboratory nssl configuration of the hrw wrf arw model the hrw nonhydrostatic multiscale model on b grid nmmb model and the north american model nam continental united states conus nest additional ensemble diversity is achieved by time lagging these members to create the hrrr 6 h hrw arw 12 h hrw nmmb 12 h hrw nssl 12 h and nam conus nest 12 h href ensemble members spc 2020 combined the five models and their five time lagged counterparts comprise the 10 member href ensemble for the continental united states the href system also produces forecasts for alaska guam hawaii and puerto rico however the forecasts for these domains are based on an 8 member ensemble because the nam model output is not available for these areas ncep n d a href forecasts are produced at a 3 km grid resolution four times per day for the continental united states 00 06 12 18 utc and two times per day for alaska 06 18 utc guam 00 12 utc hawaii 00 12 utc and puerto rico 06 18 utc ncep n d a noaa 2019a for each model cycle forecasts are output for 0 36 h in 1 h increments termed forecast hours in total forecasts are produced for 34 parameters including accumulated precipitation and reported in five different types of outputs mean of all members probability matched mean combines information from the ensemble mean with amplitude of individual members average of the mean and probability matched mean outputs spread of the ensemble and probabilistic output percentage of ensemble members meeting a specified parameter threshold ncep n d a the href forecast data generated over the past two days is archived in noaa s nomads server and there is an approximately 2 5 3 h lag time between when the forecasts are produced and when they can be downloaded as grib files puffin utilizes href s probability matched mean 1 h accumulated precipitation ensemble output to create the href precipitation timeseries used to run the swmm model section 2 3 3 the probability matched mean output was selected over the standard mean output because probability matched mean ensemble outputs are typically viewed as more representative of the ensemble members clark 2017 in order to import the href forecast data into r puffin first identifies the latest href model run by scraping noaa s nomads server then puffin downloads the individual grib files for the probability matched mean outputs for each of the 36 href forecast hours 1 36 these grib files are imported into r using the rnomads package bowman and lees 2015 and subset to include only data from the href grid cells that are located within or immediately surrounding the modeled watershed finally the forecasted precipitation accumulation from each forecast hour are joined to create the precipitation time series for each href grid cell 2 3 1 4 short range ensemble forecast sref ncep s sref system was developed as a multi regional prediction system to provide short range 0 3 days guidance on the probability distribution of weather elements and events goals of the system are to improve forecasts with ensemble averaging to provide information on each forecast s uncertainty confidence and to provide quantitative probabilistic forecasts mcqueen et al 2004 the sref system was first implemented for the north america and alaska regions in 2001 and was comprised of 10 ensemble members du et al 2006 in 2015 the system was upgraded and expanded to include 26 ensemble members based on the nmmb and wrf arw models both models are run with one control run ctl plus six initial condition breeding pairs n1 p1 n2 p2 n3 p3 n4 p4 n5 p5 n6 p6 to produce the 26 member product each of the breeding pairs is perturbed both positively and negatively using the ncep breeding technique from toth and kalnay 1997 sref is produced four times per day 03 09 15 21 utc at 40 km grid 212 32 km grid 221 and 16 km grid 132 resolutions for each of the four model cycles forecasts are output for 0 87 h in 3 h increments mcclung 2015 forecasts are produced for 63 parameters including accumulated precipitation noaa 2019a noaa archives the sref forecast data produced over the past six days in the grads dods data server and data is available for download approximately 3 4 h after it is generated to retrieve the sref precipitation forecasts for each of the 26 sref ensemble members puffin first identifies the latest sref model run using the getdodsdates function in the rnomads r package bowman and lees 2015 next puffin identifies the indices of the sref grid cells located within or immediately surrounding the modeled watershed based on the latitude and longitude of the center of each grid cell and the spatial extent of the subcatchments obtained from the uploaded subcatchments shapefile section 2 1 1 puffin then imports the sref accumulated precipitation forecast timeseries for each sref grid cell directly into r from the grads dods server wielgosz et al 2003 using the dodsgrab function in the rnomads package bowman and lees 2015 2 3 2 swmm model swmm is a semi distributed physically based hydrologic and hydraulic model developed by the united states environmental protection agency epa for urban watersheds the modeling routines used by swmm are well documented in rossman 2015 and rossman and huber 2016 swmm was chosen as the hydrology and hydraulics model for puffin because swmm is publicly available online and is among the most widely used models for urban water resource management dongquan et al 2009 tsihrintzis and hamid 1997 in addition swmm was chosen because the model can be updated and run in r using the swmmr package see section 2 3 4 leutnant et al 2019 and because the model s run times are short enough that running separate simulations for many different forecasts could be completed in a reasonable amount of time although puffin has no maximum allowable time limit for swmm simulations models with shorter simulation run times will reduce the lag time between when precipitation forecasts are generated and when puffin s results are available for viewing as implemented for the city of roanoke section 2 1 2 run times puffin s individual swmm simulations are in the order of 0 10 s 2 3 3 swmm precipitation timeseries puffin generates precipitation timeseries for the swmm model from the forecasted precipitation timeseries retrieved during preprocessing for the individual forecast grid cells section 2 3 1 to represent spatial variations in precipitation puffin creates an individual precipitation timeseries for each swmm subcatchment for each precipitation forecast i e anc hrrr href and each sref ensemble member two methods are included in puffin to accomplish this first users can interpolate a precipitation timeseries for each subcatchment from the precipitation timeseries for each forecast grid cell puffin uses the user uploaded subcatchments shapefile section 2 1 1 to determine the latitude and longitude of the center of each swmm subcatchment then puffin generates the precipitation timeseries for each subcatchment using an inverse distance weighting idw interpolation of the forecasted precipitation timeseries from each forecast grid cell based on the latitude longitude coordinates of the center of the subcatchment and the latitude longitude coordinates of the center of each grid cell these computations can be time consuming if the swmm model contains a large number of subcatchments therefore puffin also includes the option to assign precipitation timeseries to each swmm subcatchment using the forecasted precipitation timeseries from the nearest forecast grid cell 2 3 4 swmm simulations after generating precipitation timeseries for each swmm subcatchment section 2 3 3 puffin updates and runs the swmm model for each forecast to do this puffin uses r s swmmr package leutnant et al 2019 which provides an r interface to swmm first swmmr is used to read the swmm inp file to access the model structure and rain gauges are added to the model for each subcatchment then the forecasted precipitation timeseries generated for each subcatchment section 2 3 3 is assigned to the subcatchment s corresponding rain gauge after that the model s start and end dates times are updated according to the date time range included in the forecast and a new updated swmm inp file is exported finally swmmr is used to run the swmm model and retrieve the model outputs when retrieving the swmm simulation results swmmr reads the swmm out file and returns separate lists of extensible time series xts objects containing the simulation date time and parameter value for each individual swmm object subcatchment link and node and simulated parameter combination puffin reshapes this data into a data frame containing the date time of the puffin run the name of the input precipitation forecast product the swmm object id and the name date times and values of the simulated parameters this process is repeated for each precipitation forecast i e anc hrrr href and each sref ensemble member and the subcatchment link and node simulation results are joined and stored in data frames desktop version of puffin or the database cloud version of puffin puffin s analysis features section 2 4 subset and reshape the stored data as necessary 2 4 analysis features the puffin application includes a variety of visual and tabular analysis features puffin s visual analysis features include the interactive map and the interactive forecast timeseries and forecast peaks plots conversely puffin s tabular analysis features include the object summary and group analysis tables puffin s analysis features are dynamically linked and require inputs from users such as clicking a swmm object in the interactive map or adjusting the green yellow and red ranges in the slider inputs this information is passed to the application s shiny server to query the requisite data and perform any required computations before updating the user interface outputs due to puffin s reactive shiny framework the application s analysis outputs are updated immediately upon changes and selections made using the application s inputs and controls all of puffin s analysis features pull forecast data and simulation results from the data frames and or database as necessary thus puffin s implementation is generic so that further analysis features can be included by adding code to query and reshape data from the data frames database perform any requisite computations and display outputs in the user interface 2 4 1 interactive map an interactive map is included prominently in the center of puffin s forecast page this map is generated using r s leaflet package cheng et al 2019 and automatically populated with shapefiles representing the swmm model s subcatchments links and nodes the shapefiles for the map are generated directly from the swmm model using r s swmmr package leutnant et al 2019 users can zoom and pan around the map and the objects in the map can be symbolized according to the swmm output parameters and the green yellow and red ranges specified in the sidebar menu inputs section 2 2 1 1 the interactive map can also be used to select swmm objects for analysis by clicking the object on the map interactive mapping is a crucial element of any spatial application and provides spatial context to the precipitation forecasts and model results the interactive mapping analysis features were included in puffin to enable users to quickly detect spatial trends in the forecast and model results as well as to identify specific infrastructure components at risk of flooding in addition the interactive map provides users with an intuitive way to select objects for investigation using puffin s plot and table analysis features sections 2 4 2 2 4 3 in the city of roanoke puffin s interactive maps have enabled city staff to rapidly identify infrastructure components at risk of flooding 2 4 2 interactive plots puffin s forecast page includes two interactive plots the forecast timeseries plot fig 8 and the forecast peaks boxplot fig 9 these plots are generating using r s ggplot2 package wickham 2016 and allow users to view the swmm simulation results from each precipitation forecast for a selected swmm object users can select the swmm object by clicking it on the interactive map or by using inputs in the forecast page sidebar menu section 2 2 1 1 inputs are also located in the sidebar menu to specify the plotted swmm output parameters and forecast products individual datasets are included in the plots for each precipitation forecast product and the datasets are color coded by forecast product with the anc hrrr href and sref results symbolized in red yellow green and blue respectively the shiny framework provides built in support to interact click hover and brush with plots generated by r s ggplot2 package puffin utilizes these features to provide interactivity with the forecast timeseries and forecast peaks plots through the inclusion of tooltips that display the name of the forecast product the name of the swmm output parameter summary statistic and the date time and or value of the parameter summary statistic when users hover their cursor over the plots see e g fig 8 to create the plot hover interactions puffin first identifies the position of the cursor as well as the coordinates of the nearest data point then puffin retrieves the corresponding data values for nearest point and creates a floating html panel at the cursor s position to display the hover tooltip the two interactive plots have separate hover tooltips but users can hover over any data point on any dataset in either plot to view the corresponding tooltip unlike other flash flood prediction methods puffin s interactive plots allow users to evaluate the flash flood potential at specific infrastructure locations combined puffin s forecast timeseries plot and forecast peaks boxplot enable users to visually examine the temporal variations in the swmm simulation results for a selected swmm object in addition the interactive plots facilitate a rapid assessment of the agreement and spread in the results from the various forecasts and ensemble members 2 4 3 tables whereas puffin s interactive map and plots provide a qualitative analysis puffin s tabular analysis features provide a quantitative alternative for analysis and decision making two analysis tables are included in puffin the object summary table and the group analysis table both tables are dynamically linked to the forecast page s sidebar menu and inputs section 2 2 1 1 are provided for users to specify the swmm output parameters and the green yellow and red ranges used for analysis both tables subset and reshape data from the swmm result data frames or database as necessary section 2 1 1 the object summary table fig 10 was included in puffin to quantitatively evaluate the information presented in the interactive plots as with the interactive plots users can display data for a selected swmm object by clicking it on the interactive map or by identifying it using inputs in the sidebar menu unlike the interactive plots however the object summary table summarizes the swmm simulation results according to the green yellow and red ranges specified using the slider inputs section 2 2 1 1 columns in the table indicate the minimum and maximum value within each range as well as the percentage of the individual simulated timeseries for the object with a peak value in the respective range see fig 12 the group analysis table see e g tables 2 and 3 was included in puffin to provide system wide evaluations to support decision making whereas puffin s interactive plots and object summary table present data for only one swmm object the group analysis table presents data based on groups of selected objects users can add remove objects to from the analysis groups by clicking a toggle button located below the object summary table fig 3 alternatively users can add remove objects by their swmm object ids using checkboxes located in tables below the group analysis table three rows are included the group analysis table one for subcatchments one for links and one for nodes and columns in the table indicate the object type the selected swmm output parameter for each object type and the specified minimum and maximum values for the green yellow and red ranges for each object type output parameter two additional types of outputs are included in the group analysis table first the table includes columns that indicate the percentage of the selected objects with an overall peak object s single maximum simulated parameter value from the swmm results for each precipitation forecast see fig 12 within each respective color range this output was included in the table to allow users to make decisions based on the worst case scenario however because the overall peak output is based on the maximum values for each of the selected objects it can be biased by outliers and extremes in the precipitation forecasts therefore columns are also included in the group analysis table to indicate the percentage of the individual simulated timeseries for all the selected objects number of individual timeseries equals number of selected objects multiplied by number of precipitation forecasts swmm simulation runs with a peak value in the respective range see fig 12 overall puffin s group analysis table has provided an immediate benefit to users by allowing them to quickly evaluate the risk of flash flooding at specific infrastructure components section 3 3 case study city of roanoke puffin has been implemented for the city of roanoke using an existing swmm model since january 2020 although the initial use of puffin has been limited to the city of roanoke the authors reiterate that puffin was designed to be compatible with any swmm model for locations within the continental united states because puffin has been in operation for only a few months the purpose of this section is not to make claims about the overall accuracy of the flash flood predictions as this would require implementation in multiple localities and validation during flash flood events instead this section is included to demonstrate the immediate added value puffin has provided the city of roanoke as an additional tool to assess the probable watershed response section 3 1 and to explore how the application may have benefited the city during a historical rain event that resulted in flash flooding section 3 2 the city s implementation of puffin uses a highly calibrated and validated swmm model which is described in detail in brendel et al 2020 this model was developed to evaluate the risk of flash flooding and guide the implementation of flood mitigation infrastructure in the city s central business district where recurring flash flooding has occurred during brief intense rainfall calibration and validation of the model were performed on nine heavy rainfall events average annual recurrence intervals of 2 69 years and the model was evaluated on its ability to reproduce measured stream discharge and storm sewer flow depths in four locations overall the model closely replicated the observed discharge and flow depths for a collection of storm events with differing precipitation characteristics and hydrologic responses brendel et al 2020 the city is primarily interested in evaluating the probability of a flash flood event occurring in the central business district region thus analyses for the two event case studies sections 3 1 3 2 were based on puffin s group analysis output for 14 subcatchments 13 links and 9 nodes located in this area fig 13 selected analysis parameters were rainfall rate for subcatchments capacity for links and surface flooding flow for nodes the puffin application and the files required to run the analyses for the two case studies are publicly available online brendel et al 2020b for each case study the green yellow and red ranges for each parameter were specified using the slider inputs according to the acceptable risk tolerances decided by city staff it is worth noting that for each slider input the range of selectable values and the interval between selectable values is dependent upon the maximum forecasted value for the selected parameter section 2 2 1 1 consequently because the maximum forecasted rainfall rates differed greatly between the two case study events so did the values used to specify the rainfall rate color ranges in both case studies however the green yellow and red ranges for link capacity were set to 0 50 50 80 and 80 100 respectively finally for each case study the maximum of the green range for surface flooding flow was set to zero no flooding and the maximum of the yellow range was set to the first selectable slider value greater than zero 3 1 recent february 6th 2020 event since puffin was deployed for the city of roanoke in january 2020 the city s largest precipitation event occurred on february 6th puffin was run at approximately 5 30 p m local time on february 5th and the application s group analysis feature section 2 4 3 was used to evaluate the probability of flash flooding in the central business district based on subcatchment rainfall rate link capacity and node surface flooding flow city staff first evaluated the risk of flash flooding by comparing the forecasted peak rainfall rates to a rainfall intensity threshold established from previous observations during past events city staff have noted that flooding occurs in the central business district when rainfall exceeds an intensity of 6 35 cm h based on puffin s group analysis output table 2 all of the individual rainfall rate timeseries peaks for the 14 selected subcatchments were below the 6 35 cm h observed threshold thus city staff decided that there was a low probability of flash flooding however it is worth noting that the peak forecasted precipitation intensities are based on the forecasted precipitation accumulations over the forecast timesteps i e 10 min for anc 1 h for hrrr and href and 3 h for sref consequently the forecasted peak intensities may not capture brief periods of intense precipitation within the forecast timestep duration that could exceed the observed 6 35 cm h threshold therefore city staff also evaluated the risk of flash flooding based on link capacity and node surface flooding flow to provide additional confidence in their assessment of the flash flooding risk in the central business district city staff also used puffin s group analysis feature to determine if the 13 selected links were likely to be near at capacity and if any of the 9 selected nodes were likely to flood the swmm capacity parameter indicates the fraction of the full area full capacity 1 0 and the surface flooding parameter indicates the excess overflow from a node when the node is at full depth results from the analysis indicated that under the worst case scenario conditions overall peak simulated parameter values from all swmm forecast simulations 38 5 of the selected links would have a peak capacity within 0 8 1 0 but that none of the nodes would have any surface flooding table 2 further analysis of the individual link capacity timeseries indicated that the worst case scenario was supported by few of the timeseries with only 2 7 of the individual timeseries having a peak capacity 0 8 overall based on analysis of subcatchment rainfall rate link capacity and node surface flooding flow using the february 5th puffin run city staff determined that the approaching february 6th storm posed a low risk of flash flooding during the actual event the watershed received 5 23 5 84 cm of rain but no flash flooding was observed within the city s central business district thus for this event puffin provided information and analytics that led to the correct decision that no flash flooding early response action was required when no flash flooding actually occurred 3 2 historical october 11th 2018 hurricane michael event the city of roanoke has not experienced any flash flooding events since the puffin application was deployed therefore to explore how puffin may have benefited the city during a past flash flood event the application was used to analyze the october 11th 2018 event during the event the remains of hurricane michael dropped 6 71 8 53 cm of rain across the watershed over an approximately 6 h period as a result flash flooding occurred throughout the city including in the central business district in order to analyze the event puffin was run using archived forecast datasets the october 10th 2018 15 00 utc sref forecast run data was available from a personal archive adams 2020 and the october 10th 2018 15 00 utc hrrr forecast run was available from blaylock and horel 2015 however archived data for the anc and href forecasts was unavailable evaluation of the hurricane michael event was based on group analysis of the same 14 subcatchments 13 links and 9 nodes that were analyzed for the february 6th 2020 event fig 13 findings from the analysis of the hurricane michael event indicate mixed results table 3 under the worst case scenario conditions overall peak simulated parameter values from all swmm forecast simulations puffin predicted that 100 of the selected subcatchments would have a peak rainfall rate greater than 2 54 cm h 100 of the selected links would have a peak capacity greater than 0 8 and 88 9 of the selected nodes would experience some amount of surface flooding however the results also indicated that the worst case scenario was supported by few forecasts only 7 4 of the individual subcatchment rainfall rate timeseries had peaks greater than 2 54 cm h only 16 5 of the individual link capacity timeseries had peaks greater than 0 8 and only 8 6 of the individual node surface flooding timeseries had any surface flooding based on the results of puffin s group analysis it is likely that the city would have concluded that the risk of flash flooding was low however during the actual event measured peak rainfall rates exceed the worst case scenario with measured peak precipitation intensities based on 5 min accumulation ranging from 4 27 to 9 14 cm h across the city furthermore surface flooding occurred in and around the central business district using the event s measured rainfall data the swmm model predicted flooding throughout the central business district including at six of the nine nodes selected for puffin s group analysis therefore for this event puffin would have benefited from more accurate precipitation forecasts the forecasts used for this analysis were produced approximately 24 h before the storm reached the city thus it is possible that a greater percentage of puffin s simulation runs would have predicted flooding when using forecasts produced closer to the time when the storm reached the city it is also worth noting that the nws is continuously adjusting and improving how forecasts are made and thus puffin s performance using archived forecast data may not be reflective of its performance using newer forecast data overall puffin is one tool among many used by the city to make decisions regarding flash flooding and it is dependent on precipitation forecasts that are subject to a high level of uncertainty thus while puffin provides useful information to guide decision making the results from this analysis indicate that it should not be the single determining factor used to evaluate the risk of urban flash flooding 4 discussion the puffin app provides a new level of capability to flash flood guidance for urban areas by considering the impact individual infrastructure components may have on urban flooding and by utilizing a proactive probabilistic approach current flash flood guidance ffg methods are inadequate for use in urban areas because they do not consider the site specific factors and infrastructure components that contribute to urban flooding furthermore current ffg is based on a reactive approach in which flash flood warnings are issued based on deterministic precipitation estimates rfcdmt 2003 hydrologic responses in urban watersheds are typically flashier than the natural watersheds for which current ffg methods were designed ten veldhuis and schleiss 2017 thus current ffg may not provide enough time to respond to floodwaters puffin introduces a new proactive system for urban flash flood prediction that models the entire infrastructure system and is based on probabilistic forecasting by modeling each individual infrastructure component puffin not only allows users to identify components at risk of flooding but also considers how interactions between components may impact flooding in addition puffin s proactive approach allows users to assess the probability of flash flooding up to three days ahead of an approaching storm providing ample time for planning decision making and conveying requisite information to the necessary stakeholders finally puffin is adaptable for low cost implementation in other locations because it uses publicly available data and has low computational requirements puffin has added immediate value to the city of roanoke by providing a tool for proactive mitigation of flood damages in the cbd with its cloud based implementation puffin allows for multiple simultaneous users across pertinent city sections e g public works emergency management to view forecast information without the need to install any software on their devices furthermore puffin currently provides the city with a low cost alternative for flood damage mitigation in the cbd while traditional flood mitigation infrastructure is extremely capital intensive due to the dense and highly developed nature of the cbd until the city can develop and implement engineering solutions to the flooding issues the city can use puffin to respond proactively to potential flooding by alerting citizens moving cars and setting up floodproofing measures 5 summary overall the puffin application expands the capabilities of existing urban flash flood prediction tools by integrating high resolution quantitative precipitation forecasting ensemble forecasting and hydrologic and hydraulic modeling into a platform that can be adapted for implementation in other locations with minimal modification puffin s analysis features utilize a combination of deterministic and probabilistic precipitation products with a range of spatial and temporal resolutions to help users evaluate the probability of flash flooding and identify specific infrastructure components that are at risk no specialized coding or expertise are needed to utilize puffin as implemented for the city of roanoke the application has demonstrated its value as an additional tool to assess and warn against the risk of flash flooding in the city s central business district however several key limitations to the puffin application exist puffin s dependence on external software packages and data sources presents a risk to the data stream due to a lack of available precipitation forecast archives puffin cannot access historical data puffin does not provide validation features to compare its predictions with observed data after an event has occurred puffin does not account for antecedent soil moisture conditions puffin s performance accuracy is subject to the user s swmm model and their acceptable risk tolerance however puffin allows users to make these changes as needed long term evaluation of the accuracy of the puffin application will require implementation in other locations and evaluation of performance i e false positives and false negatives through additional storm events the developers plan to actively continue the development and maintenance of puffin and offer guidance to those who wish to implement puffin for their location potential updates to the app include the addition of an alternate colorblind accessible color coding scheme and the inclusion of a feature to automatically send alerts when certain precipitation or flooding thresholds are exceeded e g forecasted precipitation intensity presence of surface flooding flow pipe capacity etc in addition the developers plan to maintain an archive of predicted vs actual flooding to determine the long term accuracy of puffin s predictions software availability software name probabilistic urban flash flood information nexus puffin developers puffin team contact information cbrendel vt edu hardware required any computer running a windows operating system software required internet browser r version 3 6 2 https www r project org national centers for environmental prediction ncep wgrib2 program https www cpc ncep noaa gov products wesley wgrib2 environmental protection agency epa storm water management model software https www epa gov water research storm water management model swmm program languages r html css javascript availability the source code for the desktop based implementation of puffin may be obtained on github at https github com bigbadcrad puffin or on zenodo brendel et al 2020a and the cloud based implementation of puffin is available online at https bigbadcrad shinyapps io puffin code is released under the mit license dependencies r package version puffin use data table 1 12 8 data manipulation dplyr 0 8 3 data manipulation dt 0 11 ui data tables feather 0 3 5 exporting data geosphere 1 5 10 spatial manipulation ggplot2 3 2 1 ui plots jsonlite 1 6 read json data leaflet 2 0 3 ui maps lubridate 1 7 4 date time manipulation lutz 0 3 1 date time manipulation plyr 1 8 5 data manipulation purrr 0 3 3 data manipulation r utils 2 9 2 data extraction reshape2 1 4 3 data manipulation rgdal 1 4 8 spatial manipulation rgeos 0 5 2 spatial manipulation rnomads 2 4 1 data retrieval rvest 0 3 5 data retrieval sfsmisc 1 1 4 perform numerical integration of temporal datasets shiny 1 4 0 user interface shinyjs 1 ui javascript shinywidgets 0 5 0 ui inputs options sp 1 3 2 spatial manipulation stringr 1 4 0 string manipulation swmmr 0 9 0 interface with epa swmm xts 0 11 2 temporal manipulation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the city of roanoke virginia and the virginia tech via department of civil environmental engineering special thanks to tom adams mckenzie brocker and peter corrigan for their valuable insights and helpful contributions during the development and testing of this application abbreviations anc autonowcaster ffg flash flood guidance href high resolution ensemble forecast hrrr high resolution rapid refresh sref short range ensemble forecast puffin probabilistic urban flash flood information nexus appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104864 
25938,as the world becomes more urbanized and heavy precipitation events increase in frequency and intensity urban pluvial flooding is an emerging concern recent advances in hydrologic and hydraulic modeling high resolution quantitative precipitation forecasting and ensemble forecasting have improved the ability to evaluate flash flooding potential in urban areas the probabilistic urban flash flood information nexis puffin app integrates these components into a tool to evaluate the probability of an urban flash flood event and to identify specific infrastructure components at risk puffin uses a combination of deterministic and probabilistic precipitation forecasts to provide analyses over a range of spatial and temporal resolutions a case study for the city of roanoke virginia in the united states demonstrates how puffin can be used to assess the risk of urban flooding however with minimal modification the application can be adapted for implementation in other locations keywords quantitative precipitation forecasts ensemble forecasts hydrology hydraulics modeling urban flash flooding data analysis 1 introduction urban pluvial flooding has received considerable media attention in recent years due to its repetitive costly and systemic impacts in contrast to its fluvial and coastal counterparts urban flooding occurs more frequently and is caused when heavy precipitation collects on the land surface and exceeds the capacity of drainage systems to capture and convey stormwater galloway et al 2018 national academies of sciences and medicine 2019 furthermore the effects of urban flooding are not constrained to floodplains and areas near bodies of water galloway et al 2018 the severity extent and location of urban flooding are affected by many factors including land use land cover and drainage system design and condition national academies of sciences and medicine 2019 as urbanization continues globally and heavy precipitation events become more intense and frequent wuebbles et al 2017 urban flooding is projected to become an even more pervasive issue thus there is a need for improved tools for urban flash flood prediction predicting flash flooding in urban areas is challenging because of complex spatial variations in land use and land cover the lack of a well defined natural drainage network and interactions between man made structures including buildings channels culverts pipes roads tunnels and stormwater management infrastructure chahinian et al 2019 hapuarachchi et al 2011 javier et al 2007 rfcdmt 2003 the predominant method for flash flood prediction flash flood guidance ffg compares measured precipitation to established threshold values and was designed to predict flash flooding in small stream channels rfcdmt 2003 ffg threshold values are derived by running hydrological models to determine the precipitations and durations resulting in discharges that exceed bankfull conditions and flash flood watches and warnings are issued if measured precipitation exceeds the threshold historically the us national weather service nws implementation of ffg was based on lumped model parameters and basin averaged precipitation gourley et al 2014 however enhanced detection of the spatial variability of flash flooding has been achieved through the application of distributed hydrologic models for ffg in methods such as gridded flash flood guidance gffg schmidt et al 2007 distributed hydrological model threshold frequency dhm tf reed et al 2007 and the flooded locations and simulated hydrographs flash project gourley et al 2017 in addition the ability to evaluate flooding potential in urban areas has been improved due to advances in high spatial and temporal resolution quantitative precipitation estimation chen and chandrasekar 2015 cifelli et al 2018 javier et al 2007 rafieeinasab et al 2015 quantitative precipitation forecasts sharif et al 2006 yoon 2019 and ensemble forecasting lee et al 2013 schellart et al 2014 yang et al 2015 precipitation forecasts have also been used to estimate sewer flows and urban flood discharge and inundation using hydrology and hydraulics models lee et al 2013 schellart et al 2014 yoon 2019 despite the advances in high resolution quantitative precipitation forecasts ensemble forecasting and hydrology and hydraulics modeling there are few operational urban flash flood forecasting systems existing systems can be classified into two categories those based on pre simulated scenarios and those based on real time modeling henonin et al 2013 in systems utilizing pre simulated scenarios a combination of radar rain gauge and or water level measuring station data is used as input to a hydrologic model the outputs from the model are then used to select the most probable flooding scenario from a library of pre computed flood risk maps notable systems using this configuration include espada in nîmes france raymond et al 2006 and the texas medical center flood alert system fas in houston united states fang et al 2011 like those based on pre simulated scenarios systems utilizing real time modeling also use a combination of radar rain gauge and or water level measuring station data to run a hydrologic and hydraulic model in this method however the simulation results are used directly to evaluate the risk of flash flooding systems using this configuration include the dallas fort worth urban demonstration network in the united states habibi et al 2016 the asian institute of technology ait system in bangkok thailand mark and hosner 2004 and hidromet in barcelona spain montero et al 2010 it is worth noting that the drainage network is modeled in both the ait and hidromet systems existing urban flash flood forecasting systems have several important limitations first whereas probabilistic forecasts are routinely produced for use in evaluating the probability of fluvial flooding the existing urban flash flood forecasting systems are deterministic in nature thus they do not include any information about the uncertainty in their forecasts second real time modeling systems are computationally intensive and may require expensive high powered computing third systems that do not model the drainage network cannot identify which infrastructure components may be at risk for flooding finally the systems were developed for specific areas and may use local data streams thus they cannot be easily adapted and implemented in other locations to address the shortcomings in current urban flash flood prediction an application called the probabilistic urban flash flood information nexus puffin https bigbadcrad shinyapps io puffin was developed by integrating quantitative precipitation forecasts with real time modeling and data visualization and analysis tools puffin is tailored to stormwater managers emergency managers hydrologists and or meteorologists that need to evaluate the probability of an urban flash flood event in real time and to identify specific infrastructure components at risk the app incorporates data from multiple precipitation forecasts with varying spatial and temporal resolutions and provides immediately useful information for decision making the interface of the puffin application was designed with the objective of providing an intuitive platform for users with an understanding of basic hydrology and hydraulics although puffin was conceived and implemented for the city of roanoke virginia the application was developed to work for any location within the continental united states this paper describes the components of the puffin application and presents a case study demonstrating how puffin can be used to assess the risk of urban flash flooding 2 software implementation 2 1 architecture puffin was developed using the open source r language r was chosen over alternatives such as python because r is typically regarded as having superior data visualization features and because it includes packages that facilitate the retrieval and processing of meteorological data from the national oceanic and atmospheric administration noaa operational model archive and distribution system nomads server and interfacing with the environmental protection agency epa storm water management model swmm furthermore r facilitates the development of applications with its shiny application framework https shiny rstudio com which allows developers to create desktop and web based applications using entirely r code finally r and shiny are both free and open source so that others can update and customize puffin s publicly available source code brendel et al 2020a besides r puffin has two additional software dependencies first puffin requires the national centers for environmental prediction ncep wgrib2 program noaa 2019b to extract data from the quantitative precipitation forecast qpf datasets that cannot be downloaded directly into r from the noaa nomads server noaa 2019a second puffin requires the epa swmm software to perform the hydrology and hydraulics simulations both wgrib2 and swmm are freely available online and can be downloaded from noaa 2019b and epa 2020 respectively due to puffin s dependence on the wgrib2 and swmm programs extracting certain qpf datasets and performing the model simulations cannot be performed within a traditional cloud based shiny app instance thus puffin was originally conceived as a desktop based shiny app the desktop version of puffin can be launched in any r environment console r rgui rstudio etc and the application s source code is released online under the mit license brendel et al 2020a with this version puffin must be installed for each individual user and updates to the swmm model or puffin code must be repeated for each individual deployment in addition computations i e processing forecast data running swmm querying the results etc must be performed separately for each deployment consequently the desktop version of puffin is best suited for testing and small scale implementation for larger scale deployment puffin was reconfigured into a separate cloud based version overall the user experience in the cloud version of puffin is identical to that of the desktop version but the two versions differ in how they handle processing the qpf data and performing the model simulations fig 1 in the desktop configuration the shiny app performs all requisite computations i e processing forecast data running swmm and providing visualization and analysis features however in the cloud configuration the computational load is divided between a script hosted on a windows server and a web based shiny app the server script processes the forecast data performs the model simulations and stores the results in a mysql database whereas the shiny app queries the results and provides the visualization and analysis features mysql was chosen for data storage because it is free and open source it supports a wide variety of data types and it offers remote data access with high security features benefits of the cloud configuration of puffin include that it can be accessed online instead of requiring separate installations for each user and that it ensures that all users are using the same version of the app and swmm model in addition the response time of the shiny app interface is reduced because it is only required to retrieve the pre processed simulation results from the database however reliance of the cloud version on additional servers and databases makes it more complicated and expensive to set up than the desktop version 2 1 1 mechanics two components a user interface object and a reactive server function form puffin s shiny framework puffin s user interface was constructed using r code and customized with html css and javascript the application is controlled via input widgets e g checkboxes switches and dropdown menus that pass information to the shiny server function the shiny framework is based on a reactive programming model which updates outputs instantly as inputs are modified chang et al 2020 thus when users make selections or adjustments using puffin s input widgets the shiny server function immediately performs the requisite computations and updates the output objects e g maps tables and plots displayed in the user interface the mechanics of puffin are illustrated in fig 1 and described as follows puffin requires two files in order to retrieve the qpf data and perform the model simulations the swmm model file and a shapefile of the modeled subcatchments including the swmm id for each subcatchment puffin performs model simulations using whichever swmm model is saved in its model directory thus users can overwrite an existing model or replace an existing model with a model with a different name and puffin will use the new model when performing the simulations because swmm does not natively include a spatial reference the subcatchments shapefile is necessary for puffin to determine the location of the modeled watershed after identifying the location of the watershed puffin retrieves the autonowcaster anc high resolution rapid refresh hrrr high resolution ensemble forecast href and short range ensemble forecast sref forecasts section 2 3 1 for the area the data for each precipitation forecast is stored in a set of r data frames and puffin retrieves the precipitation accumulation data from these data frames as necessary to account for spatial variations in precipitation puffin creates an inverse distance weighted precipitation time series for each swmm subcatchment based on each precipitation forecast section 2 3 3 then puffin updates the swmm model settings and precipitation time series and runs the swmm model once for each for forecast i e anc hrrr href and each of the 26 sref ensemble members section 2 3 4 in the desktop version of puffin the simulation results from each swmm model run are joined and stored in independent r data frames for the subcatchment link and node results conversely in the cloud version of puffin the simulation results are stored in the mysql database then puffin queries and reshapes the data from the r data frames desktop version or mysql database cloud version as necessary for the various output and analysis features in the app s shiny user interface the settings page in puffin s shiny user interface provides users with various inputs to configure how the qpf data and swmm simulation results are processed see section 2 2 2 in the desktop version of puffin these inputs are used directly by the shiny app s reactive server function while retrieving the forecast data and running swmm however in the cloud version of puffin the script that retrieves the qpf data and runs the swmm model is hosted separately from the shiny app that provides the user interface see section 2 1 therefore to enable users to change puffin s settings from the web based shiny interface the settings are first saved to the mysql database and then retrieved by the windows server script before the script retrieves the qpf data and processes the model simulation results 2 1 2 implementation puffin has been deployed for the city of roanoke a medium sized urbanized area in southwest virginia the city has experienced recurring flash flooding in its central business district during brief intense rainfall see e g chittum 2017 this flooding has had detrimental economic effects for the city and has posed a hazard to health and safety thus puffin was conceived as an early warning system that the city stormwater managers and emergency managers could use to evaluate the probability of a flash flood occurring from an approaching storm and then communicate the information to the necessary stakeholders the cloud based configuration of puffin is currently deployed for the city online at https bigbadcrad shinyapps io puffin using the amazon web services aws https aws amazon com platform to host puffin s windows server and mysql database and the shinyapps io http www shinyapps io platform to host puffin s shiny app interface these platforms were chosen as low cost alternatives to configuring and maintaining city owned servers and databases for puffin in addition both platforms offer scalable hosting plans ranging from free options to professional options with increasing performance with higher tier plans under the current aws hosting plan puffin s windows server instance has one virtual cpu and 0 5 gb of ram and puffin s mysql database has two virtual cpus 1 gb of ram and 20 gb of available storage space with the current shinyapps io hosting plan each puffin instance has an available 8 gb of memory for computations to maximize the performance of puffin for concurrent users the shinyapps io server has been configured to rapidly trigger the addition of new worker processes and application instances to spread the computational load as usage of puffin increases and or more computational power or database storage is required e g the swmm model is updated to a larger more complex model or the time step for the model outputs is reduced then the aws and shinyapps io hosting plans can be upgraded as necessary 2 2 graphical user interface the puffin user interface consists of three pages home forecast and settings upon initialization of the application the puffin home screen fig 2 presents users with two buttons to access the forecast and settings pages the forecast page fig 3 includes a sidebar menu containing various inputs and controls as well as a main panel that contains puffin s map plot and table outputs the sidebar menu and main panel are dynamically linked and selections on the sidebar menu determine which swmm outputs are displayed summarized and mapped in the main panel the settings page section 2 2 2 includes inputs control how the qpf data is processed and to select which swmm parameters to simulate help buttons have been included throughout puffin s user interface and when clicked display modals with messages describing the various inputs outputs and options sections 2 2 1 2 2 2 describe the aforementioned components in greater detail 2 2 1 forecast page 2 2 1 1 sidebar menu the forecast page s sidebar menu contains inputs and options to control the interactive map object summary table forecast timeseries plot forecast peaks boxplot and group analysis table outputs sections 2 2 1 1 2 2 1 6 located in the page s main panel due to puffin s reactive shiny framework the outputs in the main panel are reactive to the sidebar menu and are immediately updated as selections and adjustments are made using the sidebar menu controls puffin includes two options to retrieve new precipitation forecasts section 2 3 1 first users can click the manual update button to manually have puffin check to see if any new forecasts have been released however users can also enable the auto update switch to have puffin automatically check for new forecasts every 5 min after puffin checks for new forecasts a modal is displayed to tell the user whether any new forecasts were released if new forecasts were released then puffin automatically retrieves the forecast data and a summary of the forecasts is displayed in a table in the sidebar menu fig 4 data in the summary table includes the name of the forecast product the time the forecast was released and the peak precipitation intensity in hr or mm hr depending on swmm model units users can set a peak precipitation intensity threshold on puffin s settings page section 2 2 2 and if any forecast s peak precipitation intensity exceeds that threshold value then a warning modal will be displayed to alert the user inputs fig 5 located in the sidebar menu allow users to customize which forecast products and swmm output parameters are displayed and summarized in the main panel outputs first a group of colored checkboxes allows users to select which forecast product results are used to symbolize the objects in the interactive map section 2 2 1 2 and are included in the main panel s plot and table outputs sections 2 2 1 3 2 2 1 6 below these inputs dropdown menus allow users to select the swmm output parameter e g flow rate capacity water depth summarized in the plot and table outputs for the model s subcatchments links and nodes finally slider inputs allow users to specify green yellow and red ranges for each parameter that control how the subcatchments links and nodes are symbolized in the interactive map and how the model results are analyzed for the table outputs sections 2 2 1 5 2 2 1 6 objects are assigned to the green yellow or red color group based on the object s overall peak simulated value maximum value from swmm simulations for each precipitation forecast for the selected output parameter the green range includes all objects with peak simulated values from zero to less than or equal to the first slider handle e g 0 2 for subcatchment rainfall rate in fig 5 the yellow range includes all objects with peak simulated values greater than the value of the first slider handle and less than or equal to the value of the second slider handle e g 0 5 for subcatchment rainfall rate in fig 5 and the red range includes all objects with peak simulated values greater than the value of the second slider handle the maximum value of the red range is the maximum simulated value for the parameter rounded up the interval between each selectable value on the slider is calculated based on the range between the minimum and maximum values on each slider two other inputs are included in the sidebar menu on the forecast page first a pair of dropdown menus allow users to locate a swmm object i e subcatchment link or node on the interactive map section 2 2 1 2 based on the object s swmm id after a user selects an object using the inputs puffin automatically zooms and centers the map to the object in addition puffin updates the main panel s object summary table forecast timeseries plot and forecast peaks boxplot outputs sections 2 2 1 3 2 2 1 5 to display the data for the selected object and parameter 2 2 1 2 interactive map puffin s forecast page fig 3 includes a large interactive map fig 6 in which users can zoom and pan around their modeled watershed to view the swmm model s subcatchments links and nodes a layer control menu is included in the map and allows users to toggle on off the swmm objects and their respective labels and switch the basemap between the default map a topographic map and satellite imagery by default subcatchments are symbolized in light blue links are symbolized in dark blue and nodes are symbolized in black however the objects can also be symbolized according to green yellow and red ranges for the selected swmm output parameter as specified using the dropdown menu and slider inputs in the sidebar menu section 2 2 1 1 fig 5 the map is also dynamically linked to the object summary table section 2 2 1 5 the forecast timeseries plot section 2 2 1 3 and the forecasts peaks boxplot section 2 2 1 4 thus if a user clicks on an object in the map then the table and plot outputs are updated to display the data for the selected object finally a toggle switch is included to display the spatial extent of the downloaded forecasts on the map fig 7 2 2 1 3 forecast timeseries plot a timeseries plot fig 8 is located prominently beneath the interactive map on puffin s forecast page fig 3 this plot is dynamically linked to the map and the sidebar menu inputs and displays the swmm simulation results for the swmm object i e subcatchment link or node selected by clicking on the map or via the sidebar menu inputs the plot s y axis displays the value of the swmm output parameter e g flow rate capacity water depth specified in the sidebar menu inputs and the plot s x axis displays the date time of the data points results are included for the swmm simulations using each individual precipitation forecast with one line for anc one line for hrrr one line for href and individual lines for each of the 26 sref ensemble members thus if all forecast products are displayed then the timeseries plot will include 29 lines these lines are color coded by forecast product with the anc simulation results symbolized in red hrrr results in yellow href results in green and the sref results symbolized in shades of blue fig 8 users can hover their cursor over the plot to display a tooltip that displays the forecast model date time and value of the nearest datapoint for any dataset in the plot see section 2 4 2 fig 8 2 2 1 4 forecast peaks boxplot the forecast peaks boxplot fig 9 is located adjacent to the forecast timeseries plot on puffin s forecast page fig 3 and provides a visual summary of the peak value from each swmm simulation included in the timeseries plot like the forecast timeseries plot the forecast peaks boxplot displays the data for the swmm object selected by clicking on the map or via the sidebar menu inputs the y axis of the boxplot corresponds to the y axis of the forecast timeseries plot and the peak values for the swmm simulations using the 26 sref ensemble members are summarized with the box and whisker plot that includes the lower whisker 25 quantile median 75 quantile upper whisker and outliers represented with blue circles peak values for the swmm simulations using the anc hrrr and href forecasts are displayed with red yellow and green x s respectively as with the forecast timeseries plot users can hover their cursor over the forecast peaks boxplot to display a tooltip that includes the name of the forecast product the name of the summary statistic and the value of the summary statistic for any dataset in the plot see section 2 4 2 2 2 1 5 object summary table like the forecast timeseries and forecast peaks plots the object summary table fig 10 is dynamically linked to the map and sidebar menu inputs and displays a summary of the swmm simulation results for the selected swmm object data is summarized in the table according to the green yellow and red ranges for the selected swmm output parameter as specified using the slider inputs in the sidebar menu section 2 2 1 1 fig 5 the table includes the minimum and maximum value for each color range as well as the percentage of the swmm simulation runs 29 total for each of the precipitation forecasts that had a peak value within the range for the selected subcatchment link or node see section 2 4 3 2 2 1 6 group analysis table the group analysis table see e g tables 2 and 3 can be viewed by selecting the group analysis view in the sidebar menu and was included in puffin to allow users to collectively analyze specific swmm objects objects can be included in the group analysis by either clicking a toggle button located below the object summary table or by selecting the object by its swmm object id using checkboxes in a set of tables located below the group analysis table the group analysis table includes columns which display the swmm object type as well as the number of selected objects and the selected swmm output parameter for the respective object type like the object summary table analysis in the group analysis table is based on the green yellow and red ranges for the selected swmm output parameter as specified using the slider inputs in the sidebar menu section 2 2 1 1 fig 5 the group analysis table includes the minimum and maximum values of each color range the percentage of the selected objects that had an overall peak value object s single maximum simulated parameter value from the swmm results for each precipitation forecast within the respective range and finally the percentage of the individual simulation timeseries for all of the selected objects that had an individual peak within the respective color range see section 2 4 3 2 2 2 settings page puffin s settings page fig 11 includes a variety of inputs and options to allow users to adjust the application settings first inputs are provided to allow users to adjust the reporting time step for the swmm simulation outputs as well as swmm s snow catch factor used to correct gauge readings for snowfall additional checkbox inputs allow users to specify which swmm output parameters are stored and available for display and analysis furthermore settings are included to specify the values of the coefficients in the nws relationship used to convert the anc forecast from radar reflectivity to precipitation intensity section 2 3 1 1 puffin allows users to manually specify these coefficient values or to automatically retrieve them from nws websites based on river forecast center and radar id section 2 3 1 1 puffin also includes two options to control how the spatial distribution of precipitation is represented in swmm users can choose to generate a unique precipitation timeseries for each swmm subcatchment through inverse distance weighting interpolation of the precipitation forecasts or they can choose to assign precipitation timeseries to each swmm subcatchment based on the nearest forecast grid cell section 2 3 3 in addition puffin allows users to specify a peak precipitation threshold value and if any precipitation forecast has a peak intensity greater than the threshold then puffin will display a warning model to alert the user finally in the desktop version of puffin inputs are provided to enable users to upload a new swmm model and a new subcatchments shapefile required for puffin to determine spatial location of the modeled watershed see section 2 1 1 2 3 data retrieval processing 2 3 1 precipitation forecast products puffin utilizes a set of four national centers for environmental prediction ncep precipitation products autonowcaster anc high resolution rapid refresh hrrr high resolution ensemble forecast href and short range ensemble forecast sref these grid based products were chosen to provide a mix of deterministic and probabilistic forecasts with a range of spatial and temporal resolutions a summary of the four precipitation products is included in table 1 and the individual products are described in greater detail in the following sections 2 3 1 1 autonowcaster anc developed by the national center for atmospheric research ncar the anc system mueller et al 2003 has been integrated into the multi radar multi sensor mrms system running at ncep anc produces two primary deterministic time and space specific 0 1 h nowcast products convective likelihood field and final thunderstorm nowcast the convective likelihood field nowcast indicate regions of probable storm initiation and or sustainment and are based on multiple data ingest products including weather surveillance radar 1988 doppler wsr 88d surface satellite sounding and numerical weather prediction nwp in contrast the final thunderstorm nowcast product depicts 1 h storm extrapolation including growth and decay as well as areas in which new storms are likely to initiate meteorological development laboratory n d nowcasts are produced approximately every 10 min at a 2 5 km grid resolution across the contiguous united states gridded binary grib files of the anc data produced over the past two days are archived in noaa s mrms server and there is an approximately 6 min lag between when nowcasts are produced and when they are available for download as the anc system does not directly forecast rainfall accumulation puffin must estimate rainfall from the final thunderstorm nowcast forecasted reflectivity each nowcast represents the storm extrapolation for only 1 h from the time the forecast was generated consequently each nowcast can only be used to estimate a single forecasted rainfall rate therefore to account for temporal variations in precipitation puffin joins multiple consecutive nowcasts in order to produce a precipitation timeseries for each anc grid cell to do this puffin first identifies all of the anc nowcasts generated for the next hour by scraping noaa s mrms server this typically consists of 5 6 forecasts because they are generated every 10 min i e puffin retrieves the final thunderstorm nowcasts generated for 10 20 30 40 50 and 60 min from the present time then each nowcast is downloaded as a grib file and imported into r using the rnomads r package bowman and lees 2015 while importing the grib files the nowcast data is subset to include only data from the anc grid cells that are located within or immediately surrounding the modeled once in r the final thunderstorm nowcast data is converted from decibels of reflectivity dbz to reflectivity z 1 d b z 10 l o g 10 z then rainfall rate can be estimated using the nws empirical relationship between reflectivity and rainfall 2 z a r b where z is the radar reflectivity mm6 m 3 and r is the rainfall rate mm h 1 the default values for a and b are 300 and 1 4 respectively fulton et al 1998 however the nws adjusts the a and b coefficient values for individual radars depending on location season and storm type puffin can automatically retrieve the current a and b values for nws radars based on their river forecast center and radar id section 2 2 2 however programmatically accessible coefficient values were only found for the nws s lower mississippi middle atlantic north central and ohio river forecast centers therefore puffin also allows users to manually specify the a and b coefficient values section 2 2 2 to use when estimating rainfall for watersheds radars located outside of these forecast center regions when estimating rainfall from radar reflectivity rain rate thresholds are typically used to correct for extreme measurements since the nws z r relationship can produce unreasonably high rainfall intensities in the hail cores of thunderstorms a hail cap is applied to limit the maximum estimated precipitation intensity the nws default threshold value is 53 dbz which corresponds to a rainfall intensity of 104 mm h 1 fulton et al 1998 conversely to account for clear air returns returns due to dust insects birds etc and not from precipitation a rain rate threshold is also applied to limit the minimum estimated precipitation intensity therefore when puffin estimates the anc rainfall all reflectivity values above 53 dbz are assumed to be the result of hail presence and are reset to 53 dbz and all reflectivity values below 25 dbz are assumed to be the result of clear air returns and are eliminated sharif et al 2006 2 3 1 2 high resolution rapid refresh hrrr hrrr is a real time hourly updated cloud resolving convection allowing atmospheric model developed by noaa s earth system research laboratory esrl to provide frequently updated high resolution short range weather forecasts designed to complement noaa s continental scale 13 km grid hourly updated assimilation modeling rapid refresh rap system hrrr provides deterministic forecasts at a higher 3 km grid resolution for the continental united states alaska hawaii and the caribbean noaa 2020 the rap and hrrr systems are used for the analysis and short range precipitation inputs to the national water model owp n d and an update of the systems from rapv4 hrrrv3 to rapv5 hrrrv4 is planned for june 2020 noaa 2020 an experimental hrrr ensemble is also being developed and tested but output data from the ensemble is not currently available for download dowell et al 2018 consisting of a numerical forecast model with an analysis assimilation system to initialize the model the hrrr system generates forecasts by using data from the rap system to perform a 1 h spin up that is followed by the assimilation of radar reflectivity data every 15 min ncep n d b hrrr forecasts 0 18 h in 1 h increments are produced for 81 parameters including accumulated precipitation and updated every hour noaa 2019a noaa archives the hrrr forecast data produced over the past two days in the grid analysis and display system distributed oceanographic data system grads dods data server wielgosz et al 2003 and data is available for download approximately 80 min after it is generated noaa 2020 puffin imports the hrrr forecast data directly into r from the grads dods server to do this puffin first identifies the latest hrrr model run using the getdodsdates function in the rnomads r package bowman and lees 2015 then puffin uses the spatial extent of the uploaded subcatchments shapefile section 2 1 1 to determine the indices of the hrrr grid cells located within or immediately surrounding the modeled watershed finally puffin retrieves the hrrr accumulated precipitation forecast timeseries for each of these hrrr grid cells from the grads dods server using the dodsgrab function in the rnomads r package bowman and lees 2015 2 3 1 3 high resolution ensemble forecast href the href system is a multi model multi physics multi initial condition convection allowing model cam ensemble developed by the ncep environmental modeling center emc as a resource for convective forecasting gallo et al 2018 spc 2020 in the current href v2 1 configuration implemented in 2019 the system produces forecasts based on the hrrr model the high resolution window hrw weather research and forecasting advanced research wrf arw model the national severe storms laboratory nssl configuration of the hrw wrf arw model the hrw nonhydrostatic multiscale model on b grid nmmb model and the north american model nam continental united states conus nest additional ensemble diversity is achieved by time lagging these members to create the hrrr 6 h hrw arw 12 h hrw nmmb 12 h hrw nssl 12 h and nam conus nest 12 h href ensemble members spc 2020 combined the five models and their five time lagged counterparts comprise the 10 member href ensemble for the continental united states the href system also produces forecasts for alaska guam hawaii and puerto rico however the forecasts for these domains are based on an 8 member ensemble because the nam model output is not available for these areas ncep n d a href forecasts are produced at a 3 km grid resolution four times per day for the continental united states 00 06 12 18 utc and two times per day for alaska 06 18 utc guam 00 12 utc hawaii 00 12 utc and puerto rico 06 18 utc ncep n d a noaa 2019a for each model cycle forecasts are output for 0 36 h in 1 h increments termed forecast hours in total forecasts are produced for 34 parameters including accumulated precipitation and reported in five different types of outputs mean of all members probability matched mean combines information from the ensemble mean with amplitude of individual members average of the mean and probability matched mean outputs spread of the ensemble and probabilistic output percentage of ensemble members meeting a specified parameter threshold ncep n d a the href forecast data generated over the past two days is archived in noaa s nomads server and there is an approximately 2 5 3 h lag time between when the forecasts are produced and when they can be downloaded as grib files puffin utilizes href s probability matched mean 1 h accumulated precipitation ensemble output to create the href precipitation timeseries used to run the swmm model section 2 3 3 the probability matched mean output was selected over the standard mean output because probability matched mean ensemble outputs are typically viewed as more representative of the ensemble members clark 2017 in order to import the href forecast data into r puffin first identifies the latest href model run by scraping noaa s nomads server then puffin downloads the individual grib files for the probability matched mean outputs for each of the 36 href forecast hours 1 36 these grib files are imported into r using the rnomads package bowman and lees 2015 and subset to include only data from the href grid cells that are located within or immediately surrounding the modeled watershed finally the forecasted precipitation accumulation from each forecast hour are joined to create the precipitation time series for each href grid cell 2 3 1 4 short range ensemble forecast sref ncep s sref system was developed as a multi regional prediction system to provide short range 0 3 days guidance on the probability distribution of weather elements and events goals of the system are to improve forecasts with ensemble averaging to provide information on each forecast s uncertainty confidence and to provide quantitative probabilistic forecasts mcqueen et al 2004 the sref system was first implemented for the north america and alaska regions in 2001 and was comprised of 10 ensemble members du et al 2006 in 2015 the system was upgraded and expanded to include 26 ensemble members based on the nmmb and wrf arw models both models are run with one control run ctl plus six initial condition breeding pairs n1 p1 n2 p2 n3 p3 n4 p4 n5 p5 n6 p6 to produce the 26 member product each of the breeding pairs is perturbed both positively and negatively using the ncep breeding technique from toth and kalnay 1997 sref is produced four times per day 03 09 15 21 utc at 40 km grid 212 32 km grid 221 and 16 km grid 132 resolutions for each of the four model cycles forecasts are output for 0 87 h in 3 h increments mcclung 2015 forecasts are produced for 63 parameters including accumulated precipitation noaa 2019a noaa archives the sref forecast data produced over the past six days in the grads dods data server and data is available for download approximately 3 4 h after it is generated to retrieve the sref precipitation forecasts for each of the 26 sref ensemble members puffin first identifies the latest sref model run using the getdodsdates function in the rnomads r package bowman and lees 2015 next puffin identifies the indices of the sref grid cells located within or immediately surrounding the modeled watershed based on the latitude and longitude of the center of each grid cell and the spatial extent of the subcatchments obtained from the uploaded subcatchments shapefile section 2 1 1 puffin then imports the sref accumulated precipitation forecast timeseries for each sref grid cell directly into r from the grads dods server wielgosz et al 2003 using the dodsgrab function in the rnomads package bowman and lees 2015 2 3 2 swmm model swmm is a semi distributed physically based hydrologic and hydraulic model developed by the united states environmental protection agency epa for urban watersheds the modeling routines used by swmm are well documented in rossman 2015 and rossman and huber 2016 swmm was chosen as the hydrology and hydraulics model for puffin because swmm is publicly available online and is among the most widely used models for urban water resource management dongquan et al 2009 tsihrintzis and hamid 1997 in addition swmm was chosen because the model can be updated and run in r using the swmmr package see section 2 3 4 leutnant et al 2019 and because the model s run times are short enough that running separate simulations for many different forecasts could be completed in a reasonable amount of time although puffin has no maximum allowable time limit for swmm simulations models with shorter simulation run times will reduce the lag time between when precipitation forecasts are generated and when puffin s results are available for viewing as implemented for the city of roanoke section 2 1 2 run times puffin s individual swmm simulations are in the order of 0 10 s 2 3 3 swmm precipitation timeseries puffin generates precipitation timeseries for the swmm model from the forecasted precipitation timeseries retrieved during preprocessing for the individual forecast grid cells section 2 3 1 to represent spatial variations in precipitation puffin creates an individual precipitation timeseries for each swmm subcatchment for each precipitation forecast i e anc hrrr href and each sref ensemble member two methods are included in puffin to accomplish this first users can interpolate a precipitation timeseries for each subcatchment from the precipitation timeseries for each forecast grid cell puffin uses the user uploaded subcatchments shapefile section 2 1 1 to determine the latitude and longitude of the center of each swmm subcatchment then puffin generates the precipitation timeseries for each subcatchment using an inverse distance weighting idw interpolation of the forecasted precipitation timeseries from each forecast grid cell based on the latitude longitude coordinates of the center of the subcatchment and the latitude longitude coordinates of the center of each grid cell these computations can be time consuming if the swmm model contains a large number of subcatchments therefore puffin also includes the option to assign precipitation timeseries to each swmm subcatchment using the forecasted precipitation timeseries from the nearest forecast grid cell 2 3 4 swmm simulations after generating precipitation timeseries for each swmm subcatchment section 2 3 3 puffin updates and runs the swmm model for each forecast to do this puffin uses r s swmmr package leutnant et al 2019 which provides an r interface to swmm first swmmr is used to read the swmm inp file to access the model structure and rain gauges are added to the model for each subcatchment then the forecasted precipitation timeseries generated for each subcatchment section 2 3 3 is assigned to the subcatchment s corresponding rain gauge after that the model s start and end dates times are updated according to the date time range included in the forecast and a new updated swmm inp file is exported finally swmmr is used to run the swmm model and retrieve the model outputs when retrieving the swmm simulation results swmmr reads the swmm out file and returns separate lists of extensible time series xts objects containing the simulation date time and parameter value for each individual swmm object subcatchment link and node and simulated parameter combination puffin reshapes this data into a data frame containing the date time of the puffin run the name of the input precipitation forecast product the swmm object id and the name date times and values of the simulated parameters this process is repeated for each precipitation forecast i e anc hrrr href and each sref ensemble member and the subcatchment link and node simulation results are joined and stored in data frames desktop version of puffin or the database cloud version of puffin puffin s analysis features section 2 4 subset and reshape the stored data as necessary 2 4 analysis features the puffin application includes a variety of visual and tabular analysis features puffin s visual analysis features include the interactive map and the interactive forecast timeseries and forecast peaks plots conversely puffin s tabular analysis features include the object summary and group analysis tables puffin s analysis features are dynamically linked and require inputs from users such as clicking a swmm object in the interactive map or adjusting the green yellow and red ranges in the slider inputs this information is passed to the application s shiny server to query the requisite data and perform any required computations before updating the user interface outputs due to puffin s reactive shiny framework the application s analysis outputs are updated immediately upon changes and selections made using the application s inputs and controls all of puffin s analysis features pull forecast data and simulation results from the data frames and or database as necessary thus puffin s implementation is generic so that further analysis features can be included by adding code to query and reshape data from the data frames database perform any requisite computations and display outputs in the user interface 2 4 1 interactive map an interactive map is included prominently in the center of puffin s forecast page this map is generated using r s leaflet package cheng et al 2019 and automatically populated with shapefiles representing the swmm model s subcatchments links and nodes the shapefiles for the map are generated directly from the swmm model using r s swmmr package leutnant et al 2019 users can zoom and pan around the map and the objects in the map can be symbolized according to the swmm output parameters and the green yellow and red ranges specified in the sidebar menu inputs section 2 2 1 1 the interactive map can also be used to select swmm objects for analysis by clicking the object on the map interactive mapping is a crucial element of any spatial application and provides spatial context to the precipitation forecasts and model results the interactive mapping analysis features were included in puffin to enable users to quickly detect spatial trends in the forecast and model results as well as to identify specific infrastructure components at risk of flooding in addition the interactive map provides users with an intuitive way to select objects for investigation using puffin s plot and table analysis features sections 2 4 2 2 4 3 in the city of roanoke puffin s interactive maps have enabled city staff to rapidly identify infrastructure components at risk of flooding 2 4 2 interactive plots puffin s forecast page includes two interactive plots the forecast timeseries plot fig 8 and the forecast peaks boxplot fig 9 these plots are generating using r s ggplot2 package wickham 2016 and allow users to view the swmm simulation results from each precipitation forecast for a selected swmm object users can select the swmm object by clicking it on the interactive map or by using inputs in the forecast page sidebar menu section 2 2 1 1 inputs are also located in the sidebar menu to specify the plotted swmm output parameters and forecast products individual datasets are included in the plots for each precipitation forecast product and the datasets are color coded by forecast product with the anc hrrr href and sref results symbolized in red yellow green and blue respectively the shiny framework provides built in support to interact click hover and brush with plots generated by r s ggplot2 package puffin utilizes these features to provide interactivity with the forecast timeseries and forecast peaks plots through the inclusion of tooltips that display the name of the forecast product the name of the swmm output parameter summary statistic and the date time and or value of the parameter summary statistic when users hover their cursor over the plots see e g fig 8 to create the plot hover interactions puffin first identifies the position of the cursor as well as the coordinates of the nearest data point then puffin retrieves the corresponding data values for nearest point and creates a floating html panel at the cursor s position to display the hover tooltip the two interactive plots have separate hover tooltips but users can hover over any data point on any dataset in either plot to view the corresponding tooltip unlike other flash flood prediction methods puffin s interactive plots allow users to evaluate the flash flood potential at specific infrastructure locations combined puffin s forecast timeseries plot and forecast peaks boxplot enable users to visually examine the temporal variations in the swmm simulation results for a selected swmm object in addition the interactive plots facilitate a rapid assessment of the agreement and spread in the results from the various forecasts and ensemble members 2 4 3 tables whereas puffin s interactive map and plots provide a qualitative analysis puffin s tabular analysis features provide a quantitative alternative for analysis and decision making two analysis tables are included in puffin the object summary table and the group analysis table both tables are dynamically linked to the forecast page s sidebar menu and inputs section 2 2 1 1 are provided for users to specify the swmm output parameters and the green yellow and red ranges used for analysis both tables subset and reshape data from the swmm result data frames or database as necessary section 2 1 1 the object summary table fig 10 was included in puffin to quantitatively evaluate the information presented in the interactive plots as with the interactive plots users can display data for a selected swmm object by clicking it on the interactive map or by identifying it using inputs in the sidebar menu unlike the interactive plots however the object summary table summarizes the swmm simulation results according to the green yellow and red ranges specified using the slider inputs section 2 2 1 1 columns in the table indicate the minimum and maximum value within each range as well as the percentage of the individual simulated timeseries for the object with a peak value in the respective range see fig 12 the group analysis table see e g tables 2 and 3 was included in puffin to provide system wide evaluations to support decision making whereas puffin s interactive plots and object summary table present data for only one swmm object the group analysis table presents data based on groups of selected objects users can add remove objects to from the analysis groups by clicking a toggle button located below the object summary table fig 3 alternatively users can add remove objects by their swmm object ids using checkboxes located in tables below the group analysis table three rows are included the group analysis table one for subcatchments one for links and one for nodes and columns in the table indicate the object type the selected swmm output parameter for each object type and the specified minimum and maximum values for the green yellow and red ranges for each object type output parameter two additional types of outputs are included in the group analysis table first the table includes columns that indicate the percentage of the selected objects with an overall peak object s single maximum simulated parameter value from the swmm results for each precipitation forecast see fig 12 within each respective color range this output was included in the table to allow users to make decisions based on the worst case scenario however because the overall peak output is based on the maximum values for each of the selected objects it can be biased by outliers and extremes in the precipitation forecasts therefore columns are also included in the group analysis table to indicate the percentage of the individual simulated timeseries for all the selected objects number of individual timeseries equals number of selected objects multiplied by number of precipitation forecasts swmm simulation runs with a peak value in the respective range see fig 12 overall puffin s group analysis table has provided an immediate benefit to users by allowing them to quickly evaluate the risk of flash flooding at specific infrastructure components section 3 3 case study city of roanoke puffin has been implemented for the city of roanoke using an existing swmm model since january 2020 although the initial use of puffin has been limited to the city of roanoke the authors reiterate that puffin was designed to be compatible with any swmm model for locations within the continental united states because puffin has been in operation for only a few months the purpose of this section is not to make claims about the overall accuracy of the flash flood predictions as this would require implementation in multiple localities and validation during flash flood events instead this section is included to demonstrate the immediate added value puffin has provided the city of roanoke as an additional tool to assess the probable watershed response section 3 1 and to explore how the application may have benefited the city during a historical rain event that resulted in flash flooding section 3 2 the city s implementation of puffin uses a highly calibrated and validated swmm model which is described in detail in brendel et al 2020 this model was developed to evaluate the risk of flash flooding and guide the implementation of flood mitigation infrastructure in the city s central business district where recurring flash flooding has occurred during brief intense rainfall calibration and validation of the model were performed on nine heavy rainfall events average annual recurrence intervals of 2 69 years and the model was evaluated on its ability to reproduce measured stream discharge and storm sewer flow depths in four locations overall the model closely replicated the observed discharge and flow depths for a collection of storm events with differing precipitation characteristics and hydrologic responses brendel et al 2020 the city is primarily interested in evaluating the probability of a flash flood event occurring in the central business district region thus analyses for the two event case studies sections 3 1 3 2 were based on puffin s group analysis output for 14 subcatchments 13 links and 9 nodes located in this area fig 13 selected analysis parameters were rainfall rate for subcatchments capacity for links and surface flooding flow for nodes the puffin application and the files required to run the analyses for the two case studies are publicly available online brendel et al 2020b for each case study the green yellow and red ranges for each parameter were specified using the slider inputs according to the acceptable risk tolerances decided by city staff it is worth noting that for each slider input the range of selectable values and the interval between selectable values is dependent upon the maximum forecasted value for the selected parameter section 2 2 1 1 consequently because the maximum forecasted rainfall rates differed greatly between the two case study events so did the values used to specify the rainfall rate color ranges in both case studies however the green yellow and red ranges for link capacity were set to 0 50 50 80 and 80 100 respectively finally for each case study the maximum of the green range for surface flooding flow was set to zero no flooding and the maximum of the yellow range was set to the first selectable slider value greater than zero 3 1 recent february 6th 2020 event since puffin was deployed for the city of roanoke in january 2020 the city s largest precipitation event occurred on february 6th puffin was run at approximately 5 30 p m local time on february 5th and the application s group analysis feature section 2 4 3 was used to evaluate the probability of flash flooding in the central business district based on subcatchment rainfall rate link capacity and node surface flooding flow city staff first evaluated the risk of flash flooding by comparing the forecasted peak rainfall rates to a rainfall intensity threshold established from previous observations during past events city staff have noted that flooding occurs in the central business district when rainfall exceeds an intensity of 6 35 cm h based on puffin s group analysis output table 2 all of the individual rainfall rate timeseries peaks for the 14 selected subcatchments were below the 6 35 cm h observed threshold thus city staff decided that there was a low probability of flash flooding however it is worth noting that the peak forecasted precipitation intensities are based on the forecasted precipitation accumulations over the forecast timesteps i e 10 min for anc 1 h for hrrr and href and 3 h for sref consequently the forecasted peak intensities may not capture brief periods of intense precipitation within the forecast timestep duration that could exceed the observed 6 35 cm h threshold therefore city staff also evaluated the risk of flash flooding based on link capacity and node surface flooding flow to provide additional confidence in their assessment of the flash flooding risk in the central business district city staff also used puffin s group analysis feature to determine if the 13 selected links were likely to be near at capacity and if any of the 9 selected nodes were likely to flood the swmm capacity parameter indicates the fraction of the full area full capacity 1 0 and the surface flooding parameter indicates the excess overflow from a node when the node is at full depth results from the analysis indicated that under the worst case scenario conditions overall peak simulated parameter values from all swmm forecast simulations 38 5 of the selected links would have a peak capacity within 0 8 1 0 but that none of the nodes would have any surface flooding table 2 further analysis of the individual link capacity timeseries indicated that the worst case scenario was supported by few of the timeseries with only 2 7 of the individual timeseries having a peak capacity 0 8 overall based on analysis of subcatchment rainfall rate link capacity and node surface flooding flow using the february 5th puffin run city staff determined that the approaching february 6th storm posed a low risk of flash flooding during the actual event the watershed received 5 23 5 84 cm of rain but no flash flooding was observed within the city s central business district thus for this event puffin provided information and analytics that led to the correct decision that no flash flooding early response action was required when no flash flooding actually occurred 3 2 historical october 11th 2018 hurricane michael event the city of roanoke has not experienced any flash flooding events since the puffin application was deployed therefore to explore how puffin may have benefited the city during a past flash flood event the application was used to analyze the october 11th 2018 event during the event the remains of hurricane michael dropped 6 71 8 53 cm of rain across the watershed over an approximately 6 h period as a result flash flooding occurred throughout the city including in the central business district in order to analyze the event puffin was run using archived forecast datasets the october 10th 2018 15 00 utc sref forecast run data was available from a personal archive adams 2020 and the october 10th 2018 15 00 utc hrrr forecast run was available from blaylock and horel 2015 however archived data for the anc and href forecasts was unavailable evaluation of the hurricane michael event was based on group analysis of the same 14 subcatchments 13 links and 9 nodes that were analyzed for the february 6th 2020 event fig 13 findings from the analysis of the hurricane michael event indicate mixed results table 3 under the worst case scenario conditions overall peak simulated parameter values from all swmm forecast simulations puffin predicted that 100 of the selected subcatchments would have a peak rainfall rate greater than 2 54 cm h 100 of the selected links would have a peak capacity greater than 0 8 and 88 9 of the selected nodes would experience some amount of surface flooding however the results also indicated that the worst case scenario was supported by few forecasts only 7 4 of the individual subcatchment rainfall rate timeseries had peaks greater than 2 54 cm h only 16 5 of the individual link capacity timeseries had peaks greater than 0 8 and only 8 6 of the individual node surface flooding timeseries had any surface flooding based on the results of puffin s group analysis it is likely that the city would have concluded that the risk of flash flooding was low however during the actual event measured peak rainfall rates exceed the worst case scenario with measured peak precipitation intensities based on 5 min accumulation ranging from 4 27 to 9 14 cm h across the city furthermore surface flooding occurred in and around the central business district using the event s measured rainfall data the swmm model predicted flooding throughout the central business district including at six of the nine nodes selected for puffin s group analysis therefore for this event puffin would have benefited from more accurate precipitation forecasts the forecasts used for this analysis were produced approximately 24 h before the storm reached the city thus it is possible that a greater percentage of puffin s simulation runs would have predicted flooding when using forecasts produced closer to the time when the storm reached the city it is also worth noting that the nws is continuously adjusting and improving how forecasts are made and thus puffin s performance using archived forecast data may not be reflective of its performance using newer forecast data overall puffin is one tool among many used by the city to make decisions regarding flash flooding and it is dependent on precipitation forecasts that are subject to a high level of uncertainty thus while puffin provides useful information to guide decision making the results from this analysis indicate that it should not be the single determining factor used to evaluate the risk of urban flash flooding 4 discussion the puffin app provides a new level of capability to flash flood guidance for urban areas by considering the impact individual infrastructure components may have on urban flooding and by utilizing a proactive probabilistic approach current flash flood guidance ffg methods are inadequate for use in urban areas because they do not consider the site specific factors and infrastructure components that contribute to urban flooding furthermore current ffg is based on a reactive approach in which flash flood warnings are issued based on deterministic precipitation estimates rfcdmt 2003 hydrologic responses in urban watersheds are typically flashier than the natural watersheds for which current ffg methods were designed ten veldhuis and schleiss 2017 thus current ffg may not provide enough time to respond to floodwaters puffin introduces a new proactive system for urban flash flood prediction that models the entire infrastructure system and is based on probabilistic forecasting by modeling each individual infrastructure component puffin not only allows users to identify components at risk of flooding but also considers how interactions between components may impact flooding in addition puffin s proactive approach allows users to assess the probability of flash flooding up to three days ahead of an approaching storm providing ample time for planning decision making and conveying requisite information to the necessary stakeholders finally puffin is adaptable for low cost implementation in other locations because it uses publicly available data and has low computational requirements puffin has added immediate value to the city of roanoke by providing a tool for proactive mitigation of flood damages in the cbd with its cloud based implementation puffin allows for multiple simultaneous users across pertinent city sections e g public works emergency management to view forecast information without the need to install any software on their devices furthermore puffin currently provides the city with a low cost alternative for flood damage mitigation in the cbd while traditional flood mitigation infrastructure is extremely capital intensive due to the dense and highly developed nature of the cbd until the city can develop and implement engineering solutions to the flooding issues the city can use puffin to respond proactively to potential flooding by alerting citizens moving cars and setting up floodproofing measures 5 summary overall the puffin application expands the capabilities of existing urban flash flood prediction tools by integrating high resolution quantitative precipitation forecasting ensemble forecasting and hydrologic and hydraulic modeling into a platform that can be adapted for implementation in other locations with minimal modification puffin s analysis features utilize a combination of deterministic and probabilistic precipitation products with a range of spatial and temporal resolutions to help users evaluate the probability of flash flooding and identify specific infrastructure components that are at risk no specialized coding or expertise are needed to utilize puffin as implemented for the city of roanoke the application has demonstrated its value as an additional tool to assess and warn against the risk of flash flooding in the city s central business district however several key limitations to the puffin application exist puffin s dependence on external software packages and data sources presents a risk to the data stream due to a lack of available precipitation forecast archives puffin cannot access historical data puffin does not provide validation features to compare its predictions with observed data after an event has occurred puffin does not account for antecedent soil moisture conditions puffin s performance accuracy is subject to the user s swmm model and their acceptable risk tolerance however puffin allows users to make these changes as needed long term evaluation of the accuracy of the puffin application will require implementation in other locations and evaluation of performance i e false positives and false negatives through additional storm events the developers plan to actively continue the development and maintenance of puffin and offer guidance to those who wish to implement puffin for their location potential updates to the app include the addition of an alternate colorblind accessible color coding scheme and the inclusion of a feature to automatically send alerts when certain precipitation or flooding thresholds are exceeded e g forecasted precipitation intensity presence of surface flooding flow pipe capacity etc in addition the developers plan to maintain an archive of predicted vs actual flooding to determine the long term accuracy of puffin s predictions software availability software name probabilistic urban flash flood information nexus puffin developers puffin team contact information cbrendel vt edu hardware required any computer running a windows operating system software required internet browser r version 3 6 2 https www r project org national centers for environmental prediction ncep wgrib2 program https www cpc ncep noaa gov products wesley wgrib2 environmental protection agency epa storm water management model software https www epa gov water research storm water management model swmm program languages r html css javascript availability the source code for the desktop based implementation of puffin may be obtained on github at https github com bigbadcrad puffin or on zenodo brendel et al 2020a and the cloud based implementation of puffin is available online at https bigbadcrad shinyapps io puffin code is released under the mit license dependencies r package version puffin use data table 1 12 8 data manipulation dplyr 0 8 3 data manipulation dt 0 11 ui data tables feather 0 3 5 exporting data geosphere 1 5 10 spatial manipulation ggplot2 3 2 1 ui plots jsonlite 1 6 read json data leaflet 2 0 3 ui maps lubridate 1 7 4 date time manipulation lutz 0 3 1 date time manipulation plyr 1 8 5 data manipulation purrr 0 3 3 data manipulation r utils 2 9 2 data extraction reshape2 1 4 3 data manipulation rgdal 1 4 8 spatial manipulation rgeos 0 5 2 spatial manipulation rnomads 2 4 1 data retrieval rvest 0 3 5 data retrieval sfsmisc 1 1 4 perform numerical integration of temporal datasets shiny 1 4 0 user interface shinyjs 1 ui javascript shinywidgets 0 5 0 ui inputs options sp 1 3 2 spatial manipulation stringr 1 4 0 string manipulation swmmr 0 9 0 interface with epa swmm xts 0 11 2 temporal manipulation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the city of roanoke virginia and the virginia tech via department of civil environmental engineering special thanks to tom adams mckenzie brocker and peter corrigan for their valuable insights and helpful contributions during the development and testing of this application abbreviations anc autonowcaster ffg flash flood guidance href high resolution ensemble forecast hrrr high resolution rapid refresh sref short range ensemble forecast puffin probabilistic urban flash flood information nexus appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104864 
25939,this paper presents a hybrid automatic calibration method for transition potential based cellular automata land use models by integrating two calibration methods process specific and optimisation based into a single hybrid approach combining the advantages of these two methods the hybrid approach features the detailed exploration of a large population of possible model parameterisations achieved using optimisation with valuable understanding of land use systems and their dynamics commonly utilised in process specific methods to better enhance the plausibility of the results obtained the utility of the proposed hybrid approach is tested through an application to madrid spain and outperforms the two other methods conventional multi objective optimisation and process specific in terms of objective performance quality of simulated output maps based on visual assessment and parameter estimates that are more consistent with process understanding keywords cellular automata automatic calibration multi objective optimisation genetic algorithms hybrid models 1 introduction land use models that use cellular automata ca have become increasingly prevalent in the analysis and prediction of land use change processes most notably urban growth and sustainable development santé et al 2010 tong and feng 2019 land use cellular automata luca models have been used to investigate a wide range of problems related to these processes including river basin management van delden et al 2007 development of sustainable agricultural practises wickramasuriya et al 2009 gomes et al 2019 ecological policy support van delden et al 2011 rutledge et al 2008 and forecasting of urban development and sprawl chaudhuri and clarke 2013 berberoğlu et al 2016 osman et al 2018 li et al 2014 the meaningful application of luca models relies on model calibration hewitt et al 2014 and is a major research focus due to the availability of common luca modelling frameworks most notably sleuth developed by clarke et al 1997 the development probability model developed by wu 2002 and the transition potential model developed by white and engelen 1993 which has become widely used with a number of implementations to global case studies guzman et al 2020 blecic et al 2015 hewitt et al 2013 roodposhti et al 2020 wickramasuriya et al 2009 calibration of a luca model involves the iterative adjustment of model parameters so that modelled predictions represent recorded data calibration is an inherently complex process stemming from four main sources 1 the metrics used to evaluate modelled outputs against known data brown et al 2005 2 the number of parameters that require calibration garcía et al 2013 3 the highly nonlinear and complex relationship between model parameters process representation and model fit van vliet et al 2013b and 4 the limited availability and reliability of land use data to guide model calibration and validation white 2006 to address the complexity of calibration there has been a focus on the development of automatic calibration methods particularly for transition potential luca models that have conventionally been calibrated manually white et al 1997 barredo et al 2003 one approach used to automate calibration of transition potential models is optimisation garcía et al 2013 blecic et al 2010 using a genetic algorithm ga this includes applying multi objective gas newland et al 2018a that allow for a detailed exploration of the trade off between different calibration objectives this is highly beneficial for luca model calibration given the two distinct types of performance objectives brown et al 2005 replicating data i e predictive accuracy and capturing the underlying processes driving land use changes in a particular region i e process accuracy using a multi objective ga ultimately generates a pareto front maier et al 2019 a set of non dominated solutions in the n dimensional objective space where improved performance in one objective cannot be achieved without reduced performance in another though gas provide an effective method for automating calibration their black box nature means the resultant outputs obtained may not be consistent with expectation gas also operate with a population of solutions e g hundreds of possible model parameter combinations that are simultaneously being improved meaning parallel computing resources are often required for practical implementation blecic et al 2015 newland et al 2018a to address this alternative approaches have been developed which for transition potential type models have built on the traditional method of manual calibration white et al 1997 barredo et al 2003 such approaches generally focus on efficiency and low resource requirements to generate a single calibrated model approaches include the random sampling method of roodposhti et al 2020 and process based methods that attempt to replicate manual calibration by exploiting process knowledge and mathematical properties of luca models for efficient calibration straatman et al 2004 engelen et al 2005 van vliet et al 2013a newland et al 2018b process knowledge the understanding of land use systems and their dynamics is highly valuable to calibration this knowledge can be provided in multiple ways including participatory stakeholder surveys hewitt et al 2014 assessment of land use structure and parameter realism by those with local knowledge hagen 2003 and as part of the automatic calibration methods mentioned previously knowledge of a local area and the problem domain can greatly enhance model validity by generating parameter values that match with expectation for a given region and enhance the likelihood of future use process knowledge has the potential to greatly enhance automatic calibration via multi objective gas throughout the various steps of such calibration processes with potential benefits including increased efficiency greater result validity and improved objective performance over conventional ga automatic calibration however it is not straightforward to combine optimisation and process focussed approaches as process based methods focus on the efficient generation of a single calibrated model whilst considering objective performance and subjective interpretation including parameter realism and assessment of land use dynamics whereas optimisation approaches focus on the iterative improvement of hundreds of simultaneous model parameterisations through objective evaluation hence the aim of this paper is to present a hybrid approach that combines the strengths of optimisation and process based calibration methods the objectives of this paper are i to introduce a hybrid method that integrates the results of process based calibration into an optimisation based approach and ii to evaluate the utility of the proposed hybrid approach comparing the results with implementations of existing optimisation and process focussed methods the remainder of this paper is organised as follows section 2 presents the methodology describing the case study and implementation of the proposed hybrid automatic calibration approach section 3 presents and discusses the results the conclusions and recommendations following this body of work are presented in section 4 2 methodology 2 1 proposed hybrid approach the proposed hybrid approach is shown in fig 1 and comprises a four stage methodology with process knowledge integrated across the different stages as shown first an initial solution or solutions is are generated using a calibration method with an emphasis on system understanding e g manual calibration random sampling process based next an initial population of solutions is generated this step is distinct from step one as it focusses on generating a population of solutions sufficient to initialise a ga for optimisation in the third step a multi objective optimisation algorithm such as nsga ii deb et al 2002 is used to perform automatic calibration finally an assessment step is used to determine the final calibrated model assessing land use location with metrics and land use structure dynamics and parameter realism with process knowledge 2 2 case study to demonstrate the proposed hybrid approach it is applied for calibration of the metronamica land use model van delden and hurkens 2011 with a study area of madrid spain metronamica riks 2015 van delden and vanhout 2018 is a generic luca model that uses transition potentials for allocating land use changes van delden and hurkens 2011 transition potential evaluated for every cell at every time step represents the relative potential for a given cell to be occupied by a given land use class in metronamica this is calculated as 1 t p c k γ a c k s c k n c k z c k where tp c k is the transition potential for land use class k to occupy cell c note that all subscripts have the same interpretation a c k is the accessibility factor for class k to occupy cell c the influence that proximity to infrastructure has on class k s c k is the suitability factor for class k to occupy cell c the influence that bio physical factors such as slope have on a given land use type n c k is the neighbourhood effect for class k to occupy cell c the spatial interactions between different land uses in the competition for space z c k is the zoning factor for class k to occupy cell c the influence of spatial planning and γ is a stochastic element included to capture the uncertainty of human decisions the madrid study area is shown in fig 2 it covers an area of 400 by 400 cells at a resolution of 250 m a total area of 10 000 km2 the calibration period is 1990 2000 and the validation period is 2000 2006 to ensure independent data are used to evaluate the proposed methodology contingency tables are given in appendix a corine land use data is used haines young and weber 2006 with the 48 corine level 3 land use classes reclassified into the 13 classes shown in metronamica land use classes are categorised as either active which are dynamically modelled such as active agricultural land or urban areas static which do not change during the simulation but influence land use dynamics such as water or airports and passive which have low transition costs and only change in extent based on the allocation of the other categories such as natural areas of the 13 classes eight are actively modelled four are static and one is passive the case study includes major road data for accessibility and slope and elevation data for suitability 2 2 1 calibration objectives metrics of locational agreement and landscape pattern structure are required to quantify the agreement between the simulated output and the data maps locational agreement is quantified using two metrics to account for agreement of the entire land use map versus transitions garcía álvarez et al 2019 fuzzy kappa hagen zanker 2009 and fuzzy kappa simulation van vliet et al 2013a to measure landscape pattern structure the clumpiness metric mcgarigal 2014 is used as clumpiness is measured at the class level and a single value is required as an objective to represent overall performance during the optimisation process an averaging process is applied to form this objective the error between the clumpiness metric value calculated for actively allocated land use classes for the simulated and data maps are aggregated to this single objective value by taking the weighted clumpiness error wce the wce is used to focus the automatic calibration process on capturing the most important dynamic behaviour that of the urban classes these are distinguished as priority classes and given double weighting in the calculation of wce please refer to appendix b for the explicit formulation of each metric 2 3 implementation this section details the implementation of the proposed approach shown in fig 1 to the case study described previously for a more detailed description of the technical implementation please refer to appendix c 2 3 1 generation of initial solution s to efficiently generate initial solutions the method of newland et al 2018b is used however before the method is applied the calibration problem is simplified by removing certain processes based on the attributes of the case study as the case study consists of a region of urban growth and the calibration period is relatively short 10 years socio economic land use classes such as residential and industry commerce are the most likely to increase in size the major processes driving changes for such land use classes are the self organising behaviour of the system leading to an expansion of existing urban cores couclelis 1989 batty and longley 1994 captured by the neighbourhood effect and accessibility processes verburg et al 2004 consequently these parameters are included for automatic calibration suitability parameters are set manually prior to calibration zoning was not included in this case study to simplify calibration emphasizing the calibration of the interaction rules and accessibility parameters most critical for simulating urban developments to reduce complexity neighbourhood rules are simplified defined by a locus point of inertia or conversion influence based on the current class at the location of interest and a tail that defines the attraction or repulsion influences for classes at different distances using a distance decay function that produces a tail shape consistent with process knowledge of declining impact with increasing distance rules are parameterised as 2 y x c f o r x 0 a e b x f o r 0 x x c 0 f o r x x c where a and b are the controlling parameters of the neighbourhood rule x is the distance to the cell of interest y x is the weight value expressed by the neighbourhood rule c is the locus point of inertia or conversion and x c is the critical distance where the neighbourhood influence is set to zero if a is negative a repulsive influence is generated the four step method of newland et al 2018b removes neighbourhood rules via statistical significance testing based on assessing the influence of certain classes on one another in the first step categorises the remaining neighbourhood rule parameters based on empirical analysis in step two and calibrates the parameters over the final two steps for the case study 176 of a possible 312 neighbourhood rule parameters are removed additionally accessibility parameters not capturing the influence of urban classes are removed manually as they are not considered meaningful for the case study six solutions are subsequently generated over the final two calibration steps by using different weightings of the calibration objectives combined via weighted sum into a single composite objective during calibration two are weighted towards fk and fks two are weighted towards wce and two have a balanced weighting between the objectives the final outputs of this phase are parameter sets with good objective performance outperforming performance benchmarks detailed in section 2 3 4 that are consistent with process understanding 2 3 2 generation of initial population the initial population step generates a quantity of solutions sufficient to initialise a ga given the limited number of initial solutions a novel approach for generating the initial population of solutions is introduced in this paper this approach is developed to increase the quantity of solutions with similar properties good objective performance high plausibility by sampling the parameter space within the vicinity of the process based solutions to achieve this a triangular distribution is applied for localised parameter sampling of a fixed range which assists in preserving the relative relationship between the different parameters the width of the triangular distribution is set relative to the parameter value being sampled for this implementation a value of 25 of the total parameter interval is used given the above the initial population consists of three types of solutions 1 process specific as detailed previously in section 2 3 1 2 sampled obtained by sampling the initial solutions and 3 randomly generated the conventional method for initialising an optimisation algorithm the proportion of the different types of solutions used is given in table 1 totalling 350 the population size as shown randomly generated solutions are the most common to ensure sufficient exploration of the solution space during optimisation the sampled solutions are obtained by generating 200 candidate solutions for each solution from step one and sampling each parameter of these solutions within the triangular distribution this results in 1200 possible parameter combinations with 40 extracted based on pareto dominance finally two replicates of each of the process specific solutions are used 12 total 2 3 3 optimisation the proposed optimisation approach is an adapted version of the multi objective optimisation framework of newland et al 2018a a recommended configuration of the nsga ii algorithm is used including a probability of crossover of 0 9 and a probability of mutation of 0 0029 the inverse of the population size to account for stochasticity in the metronamica model the performance metrics are averaged across 10 different simulations to determine the ultimate performance objective values five optimisation runs are performed with the stopping criterion used being negligible improvement less than 0 5 in the hyper volume metric zitzler 1999 for 50 generations after a minimum of 800 generations have been run this criterion was met for all optimisation runs 2 3 4 assessment the first step of the assessment is evaluating the objective performance of the pareto front solutions against benchmark metrics calculated via neutral models hagen zanker and lajoie 2008 solutions that do not outperform the performance benchmarks are not assessed further this process is repeated for the validation period to ensure that over calibration has not occurred the next stage of the assessment is evaluating the simulated output maps and parameter values for a selection of model parameterisations evaluation of simulated output maps is performed via visual inspection comparing the output maps with reference data a meaningful method of solution quality evaluation hagen 2003 pontius et al 2004 that can engage multiple model users with knowledge of the area hewitt et al 2014 and overcome certain limitations of purely objective evaluation garcía álvarez et al 2019 the parameter values are evaluated to identify solutions with realistic combinations relative to each other 2 4 evaluation of proposed hybrid approach the utility of the proposed method is evaluated by comparing the results from the hybrid approach with those obtained using the process specific method that generated the initial solutions section 2 3 1 and the conventional multi objective optimisation starting with a randomly generated population see newland et al 2018a the conventional multi objective optimisation is configured the same as the hybrid approach as described in section 2 3 3 probability of crossover probability of mutation population size and run for the same number of generations 800 and the same number of runs 5 the performance of the different approaches is compared by considering the objective performance the visual similarity of the simulated outputs and the obtained parameters 3 results and discussion the results are divided as follows first the utility of the proposed hybrid approach is evaluated by considering the objective performance metrics for the calibration and validation period a visual assessment of the simulated output maps for a subset of the solutions see table 3 and an evaluation of a key set of resultant parameters that differentiate solution quality following this the performance is compared with the two alternative calibration methods conventional multi objective optimisation and process specific semi automatic calibration considering objective performance simulated output parameter validity and efficiency of the optimisation process following the evaluation a final calibrated model is recommended 3 1 performance of proposed hybrid approach 3 1 1 objective performance the utility of the proposed hybrid method is evaluated by considering its objective performance the visual similarity of the simulated output maps compared to the data and the realism of the calibrated parameters obtained as part of the first step of this process the pareto front of solutions for the calibration and validation periods is evaluated against benchmark models from which reference metrics are calculated to determine if the output solutions have correctly captured modelled processes reference metrics are given in table 2 the three dimensional objective performance of the hybrid approach for the calibration period is shown in fig 3 two dimensional cross section plots of the objectives are shown in comparison plots with the other two methods in fig 11 in total 253 pareto optimal solutions were identified with the shading of the blue indicating the relative performance of wce lighter points correspond to lower better wce values each dot represents the average performance metric values for the 10 random seeds considered during the optimisation process the black lines define the acceptable region of the objective space for which modelled solutions outperform all the benchmark metric values with the solid and dashed lines showing the growing clusters and random constraint match benchmarks respectively these illustrate the region in the objective space which a solution must sit within to outperform all the performance benchmarks in figs 11 and 12 as the two dimensional objective space is shown the benchmarks are illustrated as lines in fig 3 the benchmark for wce for the random constraint match model is not shown due to the scale as the benchmark wce values are greater than the values obtained for the hybrid approach in the calibration period see table 2 as shown a meaningful trade off was found between the objectives particularly for fk vs wce and fks vs wce only two solutions did not outperform the benchmark metrics for the calibration period which are the two extreme points in the top right of the figure with high fk and fks values fig 4 shows the performance of the proposed hybrid approach for the validation period with a three dimensional plot of the objective performance metrics two dimensional cross section plots of the objectives are shown in fig 12 in fig 4 the benchmark plane for wce is shown for the growing clusters model as shown there is reasonable performance for the solutions obtained though fewer solutions 40 outperformed the performance benchmarks than for the calibration period which was due to having a higher wce than the growing clusters benchmark model 3 1 2 simulated output evaluation given the large number of solutions obtained five were selected for detailed analysis of the simulated output and parameter evaluation these were selected based on objective performance only taking solutions that outperformed the benchmarks for both the calibration and validation period and a visual evaluation picking solutions with high visual diversity relative to the other solutions these solutions are summarised in table 3 ordered by fks value as shown the visual diversity corresponds to a range of metric values the solutions analysed are shown by the black dots in figs 3 and 4 and henceforth are referred to as solutions h1 to h5 with solution h1 corresponding to the point shown in fig 3 with the best fks performance and solution h5 corresponding to the point shown in fig 3 with the lowest wce but relatively poor performance in terms of fks and fk these solutions were analysed in further detail by considering the simulated output maps generated for the calibration period these are shown in fig 5 with 5 a 1 showing the data map for the end of the calibration period 2000 and the remaining maps showing simulated output for each solution ordered by decreasing fks value also shown are agreement maps between the data and the simulated output for the classes residential red in the land use map and industry commerce purple in the land use map as these are the major urban classes and capturing their behavior accurately is a major focus of calibration as shown in fig 5 the simulated output was generally more clustered than the data however solution h5 corresponding to the best wce value appears to be the most consistent with the data as there is less over clustering than for the other solutions this is discussed further for the classes residential industry commerce and airports below as shown in fig 5 the simulated output across the solutions analysed exhibits a general tendency for the residential areas to be more clustered in the simulated output than in the data and tends to favour the existing larger residential clusters suggesting the solutions place an over emphasis on the attraction of residential to itself this is most noticeable in figs 5b 2 and 5c 2 interestingly these solutions also feature fewer transitions to the class residential which might explain why the fks value is slightly higher because the solutions with worse fks are capturing incorrect transitions to residential the appearance of industry commerce purple in the simulated output is also generally more clustered than in the data this again is likely due to an over emphasis on the attraction of industry commerce to itself in certain solutions h2 and h3 there also appears to be more interspersion of industry commerce in residential areas shown in figs 5c 1 and 5d 1 than expected given the data when analysing the historic data new areas of industry commerce tended to be located near major roads which only occurred in solution h5 though not necessarily the roads expected based on the data the latter issue could potentially be resolved by further differentiating the various road types as only one type of road was included finally behaviour for the airports grey in the eastern portion of the land use map is often not consistent with expectation airports tend to be near urban areas but are not expected to be attractors of residential or recreation areas or attractors of classes such as forest however for solutions h3 and h4 there appears to be an attractive influence between airports and forest as shown in figs 5 d 1 and 5 e 1 the obtained parameters in the discussed solutions confirmed the attraction of the different land uses to the airport so a solution to possibly eliminate this behaviour in future would be to exclude such unrealistic interactions from the parameter space during the optimisation preserving the parameter elimination from the process specific approach additionally introducing metrics such as the enrichment factor van vliet et al 2013a which focuses on neighbourhood composition as an optimisation objective could potentially improve the resultant output 3 1 3 parameter evaluation in is impractical to present an evaluation of all parameters obtained from the resultant hybrid approach hence an analysis of key parameters that differentiate the solutions based on which are more consistent with process knowledge including inertia points conversion point for conversions to the class residential conversion points for conversion to the class permanent crops influence tails for influence on the class residential and the accessibility parameters are presented below the inertia parameters for the different solutions obtained using the hybrid approach were generally consistent with expectation as shown in fig 6 the general trend is for the urban classes residential and industry commerce to exhibit the highest inertia and agricultural land use classes arable land pastures and other agriculture to exhibit the lowest inertia with the other classes exhibiting moderate inertia while this general trend is observed across the solutions it is most pronounced for solution h1 the conversion point parameters obtained for the hybrid approach were generally consistent with expectation as shown in fig 7 with high conversion values from agricultural land use classes most notably arable land however there are also examples of inconsistency for example solution h3 which has an almost zero conversion value to residential from arable land and h4 which has a high conversion value to residential from the class industry commerce that potentially explains the interspersion observed in the simulated output fig 5 e1 also the values are relatively low compared with the residential self influence which partially explains the larger than expected clusters of residential conversions to the class permanent crops are shown in fig 8 across the solutions analysed these parameters are generally positive which might be expected for conversions for the classes natural areas and arable land which are the dominant conversions across the solutions but not for urban classes residential industry commerce and recreation areas solution h5 is the only solution that does not include any conversion points to this effect which may explain why the behaviour in the simulated output is more consistent with expectation than that of the other solutions influence tails between the different land use classes were generally consistent with expectation though this varied across the solutions evaluated an example of this is shown for the cross influence tails to the class residential for the solutions analysed shown in fig 9 for the influence of urban classes residential industry commerce recreation areas as shown solution h5 exhibits much lower self influence of the class residential than the other solutions analysed the accessibility parameters for the influence of major roads are shown in fig 10 for the urban classes residential industry commerce and recreation areas with the weight and distance decay parameters being similar across the classes and solutions though the distance decay for recreation areas was generally lower than that for the other two though this is consistent with expectation the parameter values are fairly equal with the exception of solution h5 which has a lower distance decay for industry commerce implying this land use must occur closer to the accessibility layer in the model which is consistent with evaluation of the simulated output section 3 1 2 and process understanding as this land use class is often located near major roads 3 2 comparative assessment 3 2 1 objective performance a comparison of the objective performance of the different calibration methods is presented in fig 11 for the calibration period comparing the 253 pareto optimal points for the hybrid approach blue dots with the 392 pareto optimal points identified using the conventional multi objective optimisation method red dots and the six solutions obtained using the process specific method green crosses fig 11a shows the three dimensional objective space and indicates the benchmark metric values as planes in this space fig 11 b 11 d show a two dimensional comparison of the trade off between the different combinations of performance objectives fks against wce b fk against wce c and fk against fks d fig 11 b 11 d indicate the solutions that were evaluated with black dots for the hybrid approach and black diamonds for the conventional multi objective optimisation approach fig 11 shows that the proposed hybrid approach outperformed both the other calibration methods in the objective space for the calibration period the starting position of the process specific solutions has clearly been improved upon and the pareto front has better objective performance and more diversity than the pareto front of the conventional multi objective optimisation approach this is particularly observed in fig 11c and d where as seen in the bi objective comparison the pareto solutions from the proposed method completely dominate all pareto solutions from the conventional multi objective optimisation approach the improved objective performance of the solutions obtained using the hybrid approach over the other two methods was similar for the validation period shown in fig 12 as for the calibration period fig 11 the proposed hybrid method clearly outperformed the conventional multi objective optimisation method for fks vs wce and fk vs fks with similar performance for fk vs wce hence the hybrid approach is confirmed to have the better objective performance as opposed to over fitting the calibration period 3 2 2 simulated output evaluation given the large number of solutions obtained for the conventional multi objective implementation the same selection criteria were used for further evaluation selecting five solutions for detailed analysis of the simulated output and parameter evaluation these solutions are summarised in table 4 ordered by fks value as shown the visual diversity corresponds to a range of metric values evaluation of the simulated output maps via visual interpretation suggests that the solutions produced using the proposed hybrid approach were more consistent with the data than the solutions obtained using the conventional multi objective optimisation and process specific approaches the simulated output maps are presented in fig 13 for the conventional multi objective optimisation method and fig 14 for the process specific method for the conventional multi objective optimisation approach there are solutions that can be immediately eliminated from further consideration given the visual output because there is too much interspersion of the classes residential and industry commerce figs 13d 1 to figs 13d 3 or occurrences of residential in areas of the map that are completely unexpected and unrealistic figs 13f 1 and figs 13f 2 the other simulated maps generated by the conventional multi objective optimisation approach exhibit more clustering of the major urban classes residential red and industry commerce purple across the solutions analysed than the data and the corresponding seeded output for the process specific solutions the simulated output maps generated when there was a preference for locational agreement exhibited less clustering of the urban classes analysed though still more than the data but removed residential area in the south west region which is not consistent with expectation the simulated output maps generated when there was a preference for landscape pattern structure exhibited a similar issue and also resulted in large clusters of industry commerce in totally different areas than observed in the data though this solution did seem to include better attraction to the relevant roads in the allocation of industry commerce the limited ability of the solutions across the three calibration methods to capture sufficient attraction of the industry commerce land use class suggests there was insufficient detail in the road network classes used as mentioned previously 3 2 3 parameter comparison the calibrated parameters obtained using the proposed hybrid approach were more consistent with expectation than those obtained using the conventional multi objective optimisation approach comparison plots are presented in appendix d for the parameters previously analysed section 3 1 3 for the conventional multi objective optimisation approach and the hybrid approach the calibrated parameters obtained using the conventional multi objective optimisation approach were not as consistent with process understanding as those obtained using the hybrid approach this was generally due to there being less relative separation between certain influence values or the obtained parameters being unexpected given the case study an example of less relative separation of values was observed for the inertia points obtained using the conventional multi objective optimisation approach where the solutions had agricultural classes such as pastures and other agriculture exhibiting inertia values that were approximately equivalent to those of the urban classes residential industry commerce recreation areas which is inconsistent with expectation as agricultural land uses generally exhibit less inertia to facilitate allocation of new urban classes also examples of unexpected parameter values that were not as prevalent in the hybrid approach were a high conversion parameter for residential to permanent crops and vice versa and higher attraction between infrastructure classes and urban classes highlighted in appendix d for the influence of airports on residential and road rail on residential 3 2 4 optimisation efficiency optimisation is a computationally demanding process and hence efficiency obtaining useable results with a limited computational budget is important and is a well recognised issue when applying evolutionary algorithms to realistic problems gibbs et al 2008 2015 zheng et al 2017 hence the relative efficiency of the hybrid approach is compared with the conventional multi objective optimisation approach by comparing the objective performance metrics that exceed the performance benchmarks previously discussed to compare the efficiency of the two optimisation approaches the computational effort was judged in terms of model evaluations that is how many times the luca model was run over the calibration period to generate a simulated output map and calculating the metrics that measure the agreement between the simulated output and data maps as this is by far the most computationally expensive step in the optimisation procedure a process that takes several seconds other processes such as optimisation algorithm operations e g crossover and mutation and the generation of random solutions were considered negligible given the relatively small computational effort required with such processes generally taking milliseconds to performs to appropriately evaluate the computational efficiency of the hybrid approach the computational budget of generating the initial set of solutions was considered the starting population generated using the hybrid approach was equivalent to just over two optimisation generations as detailed below six solutions were obtained using the process specific calibration approach each generated over 1060 model evaluations including 60 model evaluations for the coarse calibration stage and 1000 for the fine calibration stage making a total of 6360 model evaluations for each of the six process specific solutions 200 sample solutions were generated requiring a total of 1200 model evaluations as the random parameter generation has negligible computational demand for the remaining members of the seed population a total of 7560 model evaluations were performed to generate the initial population for the hybrid approach as a population size of 350 is used for the optimisation and each member of the population is evaluated over 10 random seeds one generation of the optimisation process required 3500 model evaluations equivalent to just over two generations consider fig 15 which shows the percentage of the population exceeding both metric benchmarks for the proposed hybrid approach and the conventional multi objective optimisation approach as shown the hybrid approach generated plausible models with far less computational effort with 20 of solutions considered plausible after approximately 200 generations compared with approximately 600 generations for the same share of plausible solutions for the conventional multi objective optimisation approach and 40 considered plausible after 400 generations a level of plausibility that could not be achieved using the conventional multi objective optimisation approach for the number of generations considered in this study hence the hybrid approach has a major efficiency advantage over conventional multi objective optimisation if the optimisation process were run for much longer both approaches would potentially yield similar results in terms of objective performance outperforming the benchmarks however as discussed previously this would be infeasible from a practical perspective given the computational resource requirements 3 3 recommended parameter values of the semi automatic calibration methods compared in this analysis the hybrid approach performed better than the conventional multi objective optimisation and process specific methods the objective performance is superior to that of the other methods and the corresponding simulated output although the parameter values obtained using the process specific method were more consistent with process understanding of the hybrid approach solutions evaluated the recommended solution was judged to be solution h5 generating the simulated output shown in fig 5f this solution had parameter values that were the most consistent with those expected from knowledge of land use change processes was visually most consistent with the land use change data and was characterised by the best clumpiness metric value of the solutions evaluated the recommended parameterisation for the case study is given in appendix e 4 conclusions and recommendations to address issues with the computational efficiency and parameter validity of optimisation based approaches applied to automatically calibrate transition potential based luca models this research proposes a hybrid method to integrate process knowledge into optimisation based automatic calibration the proposed approach incorporates process knowledge throughout the different phases of an optimisation approach using system understanding in the generation of the initial solution using knowledge of parameter relationships to convert the limited number of solutions into a population sufficient for use during optimisation and assessment that incorporates evaluation of land use structure and parameter realism in the specific implementation shown the proposed hybrid method was used to generate a population of solutions in good regions of the parameter space based on the output of a process specific semi automatic calibration method to initialise a more powerful yet computationally demanding multi objective optimisation algorithm that otherwise does not take into account process specific information the presented method is generic allowing for the use of different optimisation algorithms land use models and process specific calibration methods the performance of the hybrid approach was demonstrated via application to a case study of madrid spain with its utility compared with a conventional multi objective optimisation based automatic calibration method and a process specific semi automatic calibration method used in isolation the results showed the following key advantages of the hybrid approach 1 avoidance of over calibration verified by the validation performance 2 superior objective performance compared to the process specific method 3 superior objective performance and greater plausibility for the simulated output and parameter values generated compared to a conventional multi objective optimisation approach with the same computational budget 4 more efficient identification of solutions that outperform performance benchmarks than the conventional multi objective optimisation approach with the same computational budget given the advantages achieved using the hybrid approach further avenues utilising such an approach should continue to be explored first additional detail could be incorporated into the input data for example a more detailed road network to improve calibration performance the method could also be used in the application of decision support systems to long term simulations which could include adaptation to other luca modelling frameworks such as sleuth to demonstrate how process knowledge can be utilised in a differently parameterised luca model to improve optimisation based calibration also greater process knowledge could be incorporated to further enhance efficiency and parameter validity by filtering unrealistic parameters or using a more detailed parameterisation method and combined with additional filtering so as to not compromise efficiency for neighbourhood rules to allow for more diverse and complex interactions to be generated to potentially further enhance the plausibility of the rules obtained which could further improve calibration performance software availability name of software land use model parameter sampler developer charles p newland contact address 39a claremont avenue netherby sa 5062 contact email charlesnewlandprofessional outlook com software requirements current implementation is linked with metronamica land use model http www metronamica nl use with metronamica is most straightforward but code can be adjusted for linking with other land use models first year available 2019 program language python 3 program size 60 mb availability and cost open source software downloadable from https github com ursidean lum sampler git declaration of competing interest i confirm that this work is original and has not been published elsewhere nor is it currently under consideration for other publications i have no conflicts of interest to disclose acknowledgements the authors wish to acknowledge the financial support from the bushfire and natural hazards cooperative research centre made available by the commonwealth of australia through the cooperative research centre program this work was supported with supercomputing resources provided by the phoenix high performance computing services at the university of adelaide we would also like to acknowledge the reviewers for their time and feedback which greatly assisted the quality and readability of this paper appendix a contingency tables this section contains contingency tables for the data maps for the madrid case study for the calibration 1990 2000 and validation 2000 2006 periods table a1 madrid contingency table 1990 2000 table a1 map 2000 luc nat arl per pas oag res i c rec for r r air m d fre tot map 1990 nat 46821 268 3 0 50 1051 316 164 357 31 12 161 20 49254 arl 1540 55265 30 0 383 1492 1059 121 32 70 92 351 26 60461 per 50 18 4278 0 0 29 3 0 0 0 0 18 5 4401 pas 13 0 0 624 0 23 5 0 0 0 0 0 0 665 oag 191 34 15 0 20761 207 45 22 6 5 0 101 0 21387 res 1 4 0 0 6 7653 85 31 0 1 0 0 0 7781 i c 3 8 0 0 0 15 1152 5 0 0 0 0 0 1183 rec 0 0 0 0 0 37 0 656 0 0 0 0 0 693 for 55 16 0 0 8 21 0 11 12164 0 0 0 7 12282 r r 0 0 0 0 0 0 0 0 0 145 0 0 0 145 air 0 0 0 0 0 0 4 0 0 0 494 0 0 498 m d 33 6 0 0 0 29 26 39 0 0 0 368 2 503 fre 24 4 0 0 0 0 0 0 0 0 0 0 719 747 tot 48731 55623 4326 624 21208 10557 2695 1049 12559 252 598 999 779 160000 table a2 madrid contingency table 2000 2006 table a2 map 2006 luc nat arl per pas oag res i c rec for r r air m d fre tot map 2000 nat 47105 386 17 68 152 451 90 34 166 107 6 68 81 48731 arl 191 52871 35 0 249 865 442 50 5 518 186 202 9 55623 per 17 25 4149 0 45 38 5 0 2 11 0 10 24 4326 pas 13 0 0 586 0 17 0 0 2 4 0 0 2 624 oag 27 58 39 336 20499 109 43 11 17 30 0 39 0 21208 res 116 55 0 0 55 9931 136 86 21 62 90 2 3 10557 i c 16 23 0 0 0 287 2300 30 0 28 11 0 0 2695 rec 19 4 0 0 4 47 13 953 1 1 0 7 0 1049 for 143 4 0 0 16 37 3 6 12324 9 15 2 0 12559 r r 0 0 0 0 0 148 9 0 0 95 0 0 0 252 air 6 44 0 0 0 6 4 0 0 0 538 0 0 598 m d 88 78 0 11 25 71 71 41 0 11 13 590 0 999 fre 0 1 0 4 0 0 0 0 0 0 0 10 764 779 tot 47741 53549 4240 1005 21045 12007 3116 1211 12538 876 859 930 883 160000 appendix b metric formulation this appendix contains specific details of the formulation of the metrics used as performance objectives for this research locational agreement locational agreement is quantified using two metrics fuzzy kappa hagen zanker 2009 and fuzzy kappa simulation van vliet et al 2013a these two metrics are used to balance two types of spatial land use agreement that of all cells in the land use map measured by fuzzy kappa fk and that for land use class cells that transition from one time point to another measured by fuzzy kappa simulation fks both metrics have the same functional form b1 f k s p o p e 1 p e where p o is the observed agreement the number of matches of land use class pixels between the data and simulated output map and p e is the expected agreement the number of matches expected given the data and simulated output map fk and fks vary by how observed and expected agreement are calculated with fks weighted heavily toward cells that transition between time periods both metrics range between 1 and 1 landscape pattern structure landscape pattern structure is measured using the clumpiness metric clumpiness is a measure of the proportional deviation of the proportion of like adjacencies i e whether the land uses adjacent to a cell of a particular land use class are the same class from what is expected under a spatially random distribution mcgarigal 2014 it is calculated first by determining the proportion of like adjacencies b2 g k g k k l 1 l g k l min e k where g k is the proportion of like adjacencies for land use class k g kl is the count of like adjacencies between patches of class k and l using the double count method mcgarigal 2014 l is the total number of land use classes and min e k is minimum perimeter of a maximally clumped patch of class k defined as b3 min e k 4 n 4 n 2 4 n 4 n 2 a k n 2 a k n 1 n a k n 1 n where a k is the area of class k in terms of number of cells and n is the length of a side of the largest integer square possible with a smaller area than a k with the proportion of like adjacencies calculated it follows that clumpiness is calculated by b4 c l u m p y k g k p k p k g k p k 1 p k for g k p k and p k 0 5 otherwise where p k is the proportion of the landscape occupied by patch type class k clumpiness is measured for each land use class in a specific land use map and ranges from 1 fully disaggregated to 1 fully clumped as clumpiness is measured at the class level and a single value is required as an objective to represent overall performance during the optimisation process the error between the metric value calculated for actively allocated land use classes for the simulated and data maps are aggregated to a single value by taking the weighted clumpiness error wce wce is used to focus the automatic calibration process on capturing the most important dynamic behaviour that of the urban classes these are distinguished as priority classes and given double weighting in the calculation of wce b5 w c e i n p a w i c e i i p a 2 w i c e i where n npa is the number of non priority active classes n pa is the number of priority active classes and ce i is the clumpiness error of active class i appendix c technical implementation this appendix provides a detailed explanation of the implementation of the proposed hybrid approach generation of initial solution the first step of the proposed implementation is to generate a calibrated model efficiently using system understanding as detailed in section 2 3 1 this is achieved by using the method of newland et al 2018b to calibrate the neighbourhood rules the steps of this method are as follows 1 parameter elimination where certain parameters within the different conceptual groups of neighbourhood rules inertia points conversion points self influence tails cross influence tails are eliminated based on statistical analysis and user discretion 2 parameter categorisation where the remaining parameters within each group are graded 3 coarse parameter adjustment tuning the parameters within their respective groups using meta parameters and 4 fine parameter adjustment tuning the parameters individually prior to implementing the method the neighbourhood rules are simplified the effect of attraction and repulsion captured by the tail of the neighbourhood rule generally has a diminishing effect with increasing distance hence to greatly reduce the complexity of the calibration problem neighbourhood rules are parameterised as shown in equation c1 c1 y x c f o r x 0 a e b x f o r 0 x x c 0 f o r x x c where a and b are the controlling parameters of the neighbourhood rule x is the distance to the cell of interest y x is the weight value expressed by the neighbourhood rule c is the locus point of inertia or conversion and x c is the critical distance where the neighbourhood influence is set to zero note that if a is negative a repulsive influence will be generated as there are 8 active and 13 total classes there are a total of 104 neighbourhood rules and hence 312 neighbourhood rule parameters for calibration in the parameter elimination step a statistical analysis is used to remove certain neighbourhood rule parameters that based on the representation of transition cells relative to the total land use map are not assessed as having a statistically significant representation for the impact to be considered meaningful for land use change when applied to the case study 176 conversion points and influence tails are eliminated all inertia points and self influence tails are included when reviewing the removed parameters they are consistent with expectation for example conversions from urban land use classes e g residential industry commerce recreation areas to agricultural land use classes e g arable land pastures no additional neighbourhood rule parameters are removed based on user discretion accessibility parameters are removed manually only including accessibility for the urban land use classes as these were considered meaningful for the case study following the parameter reduction categorisation of the remaining neighbourhood rule parameters within their respective groups inertia points conversion points self influence tails cross influence tails is performed by calculating the enrichment factor van vliet et al 2013b for each land use class and grading each parameter based on user defined enrichment factor levels this is performed to establish a hierarchy of parameter values prior to calibration as transition potential model parameters have no absolute interpretation i e the specific influence value has no physical meaning but can be interpreted relative to each other for example the inertia of urban land use classes e g residential should be higher than agricultural land use classes e g arable land due to the higher transition costs of such land use classes for this case parameters are graded as either high medium or low relative to the other values in a specific neighbourhood rule parameter group following parameter categorisation a coarse calibration step is performed to refine the neighbourhood rule values of the conversion points self influence tails and cross influence tails relative to the inertia points for example the best values of the calibration objectives may be found when the conversion points maintaining the high medium and low classification are scaled to be 10 of the inertia point values for the respective groups as opposed to 5 or 15 the final step is a fine tuning process where the neighbourhood rule parameters included for calibration are individually adjusted in this process the other neighbourhood rule parameters are held constant whilst a single parameter is tuned to optimise a weighted sum of the calibration objectives six solutions are generated by using different objective weightings two weighted towards fk and fks two weighted towards wce and two with a balanced weighting between the objectives the final outputs are parameter sets with good objective performance that are consistent with process understanding generation of initial population the objective of this step is to produce a population of solutions sufficiently large to initialise a ga as the process specific method only produces six solutions and the required population size is 350 this is not straightforward consequently a novel approach for generating the initial population of solutions is introduced in this paper when seeding the initial population with solutions based on process understanding a decision has to be made as to what proportion of the initial population should be based on process understanding and what proportion should be random if the proportion of solutions developed based on process understanding is too high it might be difficult for the optimisation process to explore potentially better regions of the solution space resulting in premature convergence to sub optimal solutions by contrast if this proportion is too low the influence of the process based solutions might be insufficient to guide the search into promising regions of the search space and to increase convergence speed in addition given the limited number of initial solutions an approach has to be developed to increase the quantity of such solutions with similar properties good objective performance high plausibility based on system understanding hence a method is developed to generate additional solutions by sampling in the vicinity of the process based solutions as has been done in other problem domains bi et al 2015 2016 a triangular distribution is used for this implementation because it has a fixed range which assists with preserving the relative relationship between the different parameters it should be noted that the sampling is done in a way that preserves the parameter reduction detailed previously an example of fitting a triangular distribution to a model parameter is shown in figure c1 where the mean of the distribution θ shown by the vertical grey line corresponds to a parameter value obtained in the initial solution generation stage and the upper and lower limits expressed as a percentage of the obtained parameter value influence the range of the potential sampled values as shown by increasing the limits the potential range of parameter values obtained is broader for example if an inertia parameter of 1000 was obtained during stage one during the sampling procedure it would be varied between 900 and 1100 if the narrow limits are used or 700 and 1300 if the wide limits are used fig c1 comparison of triangular distributions with varying limits for sampling of parameter values fig c1 the purpose of selecting limits is to ensure sufficient variation without compromising solution quality for example if the limits are too wide for inertia parameters the result may be that urban e g residential and agricultural e g arable land land use classes have relatively equal inertia which is inconsistent with expectation and would compromise the quality of the sampled solutions for this implementation a width of 25 of the total parameter interval works best to achieve this the random solutions are generated in the same way as a ga traditionally initialises the optimisation process by randomly generating parameter values within a certain range randomly generated solutions utilise the distance decay function to parameterise neighbourhood rule tails but do not include the parameter reduction for this implementation the proportions of process specific sampled and randomly generated solutions are given in table c1 and are determined from preliminary testing as shown most of the solutions are randomly generated to ensure sufficient diversity during the optimisation process most of the remaining solutions are generated via the sampling approach to ensure sufficient exploration of the promising regions of the parameter space that have both good objective performance and are consistent with system knowledge for this implementation 200 sampled solutions are generated from each process specific solution from the combined 1200 solutions a total of 40 are extracted based on pareto dominance i e extracting non dominated solutions from the available candidates to serve as the sampled input finally two replicates of each of the initial solutions are included in the initial population hence 12 in total table c1 proportion of solution types used for initial population table c1 solution type number of total population percentage of total population randomly generated 298 85 2 sampled 40 11 4 process specific 12 3 4 optimisation the hybrid approach uses an adapted version of the multi objective optimisation framework of newland et al 2018a for this implementation rather than starting the optimisation process with a random population of solutions the initial population is made up of the mixture of solutions detailed in table c1 as part of the optimisation process the optimisation algorithm takes the initial population of luca model parameter combinations and iteratively improves them by running each through the luca model to generate a simulated output map and calculating a set of land use metrics to quantify the different aspects of luca model performance i e locational agreement and landscape pattern structure for each solution the sets of parameter values contained within the population are then adjusted based on feedback of their relative performance using the operators of the ga e g selection cross over and mutation see maier et al 2019 this work uses the non dominated sorting genetic algorithm ii nsga ii proposed by deb et al 2002 because of its demonstrated ability to tune luca model parameters trunfio 2006 cao et al 2014 newland et al 2018a a recommended configuration of the nsga ii algorithm is used with a probability of crossover of 0 9 and a probability of mutation of 0 0029 the latter equalling the inverse of the population size 350 it is important that the randomness within the metronamica model is appropriately considered during the optimisation process hence the generation of simulated output maps and calculation of performance metrics is repeated for 10 different random number seeds for each solution during the optimisation process and the average for the performance metrics across the ten seeds used as the ultimate performance objective value the optimisation stage of automatic calibration is an iterative adjustment process that is performed until certain stopping criteria are met for this implementation the stopping criterion adopted is based on solution convergence after a set number of generations in this case 800 convergence implies that there is negligible improvement of the calibration objectives with continued optimisation which in this case is measured using the hyper volume metric zitzler 1999 the hypervolume summarises the convergence and diversity of the pareto front by measuring the n dimensional objective space covered by the pareto front relative to the nadir point in the parameter space where all objectives are minimised the solutions are considered to have converged when there is negligible improvement in the hyper volume for 50 generations defined as less than 0 5 improvement in the hyper volume metric when a minimum of 800 generations had been run this occurred for all optimisation runs performed however if convergence had not been achieved additional generations would have been run the multi objective optimisation stage is implemented using a parallelised version of nsga ii to ensure completion of the optimisation within a feasible 48 h amount of time this was performed on the phoenix high performance computing resources operated by research services at the university of adelaide the optimisation runs are evenly distributed across a number of cpu processing cores with a number of slave processes evaluating the objective functions of different members of the population and a master process co ordinating the search using the nsga ii genetic operators selection crossover mutation non dominated sorting and crowding distance to evolve the population of solutions assessment the final component of the proposed hybrid approach is an assessment step which is used to determine a final calibrated model the assessment step is required because of the multi objective nature of the optimisation process used resulting in a pareto front where all solutions are optimal based on the objective performance metrics hence subjective input based on process understanding is used to select which of these model parameterisations to adopt the first step of the assessment is evaluating the objective performance of the pareto front solutions this is conducted by evaluating the performance against neutral models which generate pseudo simulated output based on varying growth strategies with the same boundary conditions for which benchmark metrics are calculated these metrics provide context for calibration performance as objective performance can be attributed to either modelled processes or boundary conditions hagen zanker and lajoie 2008 as neutral models have the same boundary conditions the output metrics can be considered minimum objective performance targets which if outperformed by calibrated solutions suggest the correct capture of modelled processes benchmark metrics are calculated for fk and wce because fks has an implicit baseline at a value of zero solutions that do not outperform the performance benchmarks are not assessed further further evaluation is performed by repeating this process for the validation period to ensure that over calibration has not occurred the next stage of the assessment is conducted by evaluating a selection of model parameterisations on the pareto front for these solutions the simulated output maps and parameters values are evaluated based on process knowledge evaluation of simulated output maps is performed via visual inspection comparing the output maps with reference data which is a meaningful method of solution quality evaluation hagen 2003 pontius jr et al 2004 that can engage a wide number of model users with knowledge of the area hewitt et al 2014 and overcome certain limitations with only evaluating solution quality using metrics garcía álvarez et al 2019 the parameter values are evaluated to identify solutions with realistic parameter combinations relative to each other appendix d comparison plots of parameters analysed for seeded and unseeded calibration method this appendix contains a series of plots comparing the parameters obtained from the hybrid and conventional multi objective optimisation calibration methods the inertia point parameters the conversion point parameters for transitions to the class residential conversion point parameters for transitions to the class permanent crops influence tails for interactions with the class residential and the accessibility parameters generally the conventional multi objective optimisation calibration method resulted in higher parameter values the conventional multi objective optimisation method also resulted in more examples of parameters that were not consistent with expectation for example high inertia across a majority of classes figure d1 high conversion values for transitions from the classes forest pastures and permanent crops to residential figure d2 greater emphasis on transitions to permanent crops from the class residential figure d3 attractive influences on residential land exerted by classes such as airports and road rail figure d7 and no weighting given to residential or recreation areas for accessibility figure d8 fig d1 comparison of inertia influence per class for analysed solutions for hybrid approach and conventional multi objective optimisation methods fig d1 fig d2 comparison of conversion point influence values to class residential for each land use class for analysed solutions for hybrid approach and conventional multi objective optimisation methods fig d2 fig d3 comparison of conversion point influence values to class permanent crops for each land use class for analysed solutions for hybrid approach and conventional multi objective optimisation methods fig d3 fig d4 comparison of tail influences to the class residential for the natural land use classes for analysed solutions for hybrid approach and conventional multi objective optimisation methods fig d4 fig d5 comparison of tail influences to the class residential for the agricultural land use classes for analysed solutions for hybrid approach and conventional multi objective optimisation methods fig d5 fig d6 comparison of tail influences to the class residential for the urban land use classes for analysed solutions for hybrid approach and conventional multi objective optimisation methods fig d6 fig d7 comparison of tail influences to the class residential for the infrastructure land use classes for analysed solutions for hybrid approach and conventional multi objective optimisation methods fig d7 fig d8 comparison of accessibility parameters for the urban land use classes for analysed solutions for hybrid approach and conventional multi objective optimisation methods fig d8 appendix e final model parameterisation table e1 inertia conversion parameters table e1 from to arl per pas oag res i c rec for nat 13 11 31 90 0 00 16 07 0 91 0 02 45 43 0 00 arl 247 42 49 80 1 14 61 99 117 34 0 14 15 08 87 65 per 16 74 467 24 3 26 3 77 23 24 1 40 0 11 2 60 pas 4 00 32 95 594 33 0 02 0 68 27 18 4 62 22 25 oag 6 54 16 55 5 62 656 86 18 46 47 99 11 69 0 06 res 4 37 0 22 10 44 0 15 1111 86 18 10 4 60 0 17 i c 0 49 0 01 13 20 0 05 0 28 749 02 0 00 0 10 rec 0 02 0 01 0 02 4 04 3 80 1 87 650 87 0 02 for 0 94 3 21 0 22 1 43 1 64 0 01 21 72 1080 55 r r 3 27 0 04 7 74 2 79 0 29 0 02 0 73 0 14 air 14 04 1 01 2 74 2 57 0 06 0 32 4 43 2 39 m d 11 51 0 00 0 14 0 58 0 04 1 95 5 57 0 14 fre 0 03 28 43 46 40 0 30 0 58 4 36 2 27 0 01 table e2 neighbourhood rule a parameters table e2 from to arl per pas oag res i c rec for nat 95 99 2 67 26 73 35 26 196 98 245 31 11 46 18 57 arl 95 65 16 60 120 62 247 87 15 75 151 90 0 79 143 91 per 73 55 185 52 376 35 76 05 149 34 124 99 0 00 29 72 pas 72 20 56 79 53 03 3 00 162 01 0 06 38 57 49 21 oag 149 04 202 01 89 77 5 72 19 47 40 53 73 04 24 07 res 330 08 8 83 84 60 56 19 265 13 378 97 95 87 19 19 i c 43 87 194 36 299 39 155 99 246 42 989 89 416 06 30 89 rec 0 30 0 61 167 85 188 65 9 46 12 19 921 89 1 89 for 82 47 1 95 14 37 29 76 2 19 0 05 350 72 76 24 r r 0 23 0 87 108 12 200 15 31 86 538 68 78 14 262 63 air 105 76 77 21 160 91 17 81 40 17 712 40 176 30 37 74 m d 490 11 19 72 0 03 87 37 98 26 862 42 53 91 168 49 fre 47 12 3 77 74 31 146 16 68 52 8 80 122 35 5 09 table e3 neighbourhood rule b parameters table e3 from to arl per pas oag res i c rec for nat 2 78 2 30 2 15 2 73 1 48 2 59 1 66 2 31 arl 2 31 2 74 1 58 3 02 2 60 1 80 2 17 2 21 per 1 69 2 52 2 98 1 83 2 31 2 52 1 98 2 17 pas 2 39 2 38 2 52 2 11 2 97 2 54 2 77 1 07 oag 2 42 2 35 3 37 1 94 2 33 2 78 2 35 3 38 res 2 40 2 09 2 30 2 16 2 31 2 31 2 01 2 15 i c 2 61 3 11 3 27 2 34 2 32 2 57 2 85 2 31 rec 2 64 2 23 2 33 2 65 2 54 1 62 2 31 2 28 for 2 30 2 63 2 73 1 81 2 31 2 16 2 34 2 36 r r 2 30 3 17 2 27 2 34 2 31 1 94 2 99 2 86 air 2 21 2 34 1 59 2 32 2 73 2 65 2 92 2 32 m d 2 56 2 50 1 51 2 32 2 49 2 31 2 72 1 41 fre 2 31 2 12 2 75 2 32 1 99 1 54 3 14 2 03 table e4 accessibility parameters table e4 land use class major roads distance decay major roads weight arl 0 87 0 00 per 3 53 0 00 pas 2 51 0 00 oag 1 27 0 00 res 15 29 0 39 i c 10 76 0 54 rec 15 11 0 50 for 2 06 0 00 
25939,this paper presents a hybrid automatic calibration method for transition potential based cellular automata land use models by integrating two calibration methods process specific and optimisation based into a single hybrid approach combining the advantages of these two methods the hybrid approach features the detailed exploration of a large population of possible model parameterisations achieved using optimisation with valuable understanding of land use systems and their dynamics commonly utilised in process specific methods to better enhance the plausibility of the results obtained the utility of the proposed hybrid approach is tested through an application to madrid spain and outperforms the two other methods conventional multi objective optimisation and process specific in terms of objective performance quality of simulated output maps based on visual assessment and parameter estimates that are more consistent with process understanding keywords cellular automata automatic calibration multi objective optimisation genetic algorithms hybrid models 1 introduction land use models that use cellular automata ca have become increasingly prevalent in the analysis and prediction of land use change processes most notably urban growth and sustainable development santé et al 2010 tong and feng 2019 land use cellular automata luca models have been used to investigate a wide range of problems related to these processes including river basin management van delden et al 2007 development of sustainable agricultural practises wickramasuriya et al 2009 gomes et al 2019 ecological policy support van delden et al 2011 rutledge et al 2008 and forecasting of urban development and sprawl chaudhuri and clarke 2013 berberoğlu et al 2016 osman et al 2018 li et al 2014 the meaningful application of luca models relies on model calibration hewitt et al 2014 and is a major research focus due to the availability of common luca modelling frameworks most notably sleuth developed by clarke et al 1997 the development probability model developed by wu 2002 and the transition potential model developed by white and engelen 1993 which has become widely used with a number of implementations to global case studies guzman et al 2020 blecic et al 2015 hewitt et al 2013 roodposhti et al 2020 wickramasuriya et al 2009 calibration of a luca model involves the iterative adjustment of model parameters so that modelled predictions represent recorded data calibration is an inherently complex process stemming from four main sources 1 the metrics used to evaluate modelled outputs against known data brown et al 2005 2 the number of parameters that require calibration garcía et al 2013 3 the highly nonlinear and complex relationship between model parameters process representation and model fit van vliet et al 2013b and 4 the limited availability and reliability of land use data to guide model calibration and validation white 2006 to address the complexity of calibration there has been a focus on the development of automatic calibration methods particularly for transition potential luca models that have conventionally been calibrated manually white et al 1997 barredo et al 2003 one approach used to automate calibration of transition potential models is optimisation garcía et al 2013 blecic et al 2010 using a genetic algorithm ga this includes applying multi objective gas newland et al 2018a that allow for a detailed exploration of the trade off between different calibration objectives this is highly beneficial for luca model calibration given the two distinct types of performance objectives brown et al 2005 replicating data i e predictive accuracy and capturing the underlying processes driving land use changes in a particular region i e process accuracy using a multi objective ga ultimately generates a pareto front maier et al 2019 a set of non dominated solutions in the n dimensional objective space where improved performance in one objective cannot be achieved without reduced performance in another though gas provide an effective method for automating calibration their black box nature means the resultant outputs obtained may not be consistent with expectation gas also operate with a population of solutions e g hundreds of possible model parameter combinations that are simultaneously being improved meaning parallel computing resources are often required for practical implementation blecic et al 2015 newland et al 2018a to address this alternative approaches have been developed which for transition potential type models have built on the traditional method of manual calibration white et al 1997 barredo et al 2003 such approaches generally focus on efficiency and low resource requirements to generate a single calibrated model approaches include the random sampling method of roodposhti et al 2020 and process based methods that attempt to replicate manual calibration by exploiting process knowledge and mathematical properties of luca models for efficient calibration straatman et al 2004 engelen et al 2005 van vliet et al 2013a newland et al 2018b process knowledge the understanding of land use systems and their dynamics is highly valuable to calibration this knowledge can be provided in multiple ways including participatory stakeholder surveys hewitt et al 2014 assessment of land use structure and parameter realism by those with local knowledge hagen 2003 and as part of the automatic calibration methods mentioned previously knowledge of a local area and the problem domain can greatly enhance model validity by generating parameter values that match with expectation for a given region and enhance the likelihood of future use process knowledge has the potential to greatly enhance automatic calibration via multi objective gas throughout the various steps of such calibration processes with potential benefits including increased efficiency greater result validity and improved objective performance over conventional ga automatic calibration however it is not straightforward to combine optimisation and process focussed approaches as process based methods focus on the efficient generation of a single calibrated model whilst considering objective performance and subjective interpretation including parameter realism and assessment of land use dynamics whereas optimisation approaches focus on the iterative improvement of hundreds of simultaneous model parameterisations through objective evaluation hence the aim of this paper is to present a hybrid approach that combines the strengths of optimisation and process based calibration methods the objectives of this paper are i to introduce a hybrid method that integrates the results of process based calibration into an optimisation based approach and ii to evaluate the utility of the proposed hybrid approach comparing the results with implementations of existing optimisation and process focussed methods the remainder of this paper is organised as follows section 2 presents the methodology describing the case study and implementation of the proposed hybrid automatic calibration approach section 3 presents and discusses the results the conclusions and recommendations following this body of work are presented in section 4 2 methodology 2 1 proposed hybrid approach the proposed hybrid approach is shown in fig 1 and comprises a four stage methodology with process knowledge integrated across the different stages as shown first an initial solution or solutions is are generated using a calibration method with an emphasis on system understanding e g manual calibration random sampling process based next an initial population of solutions is generated this step is distinct from step one as it focusses on generating a population of solutions sufficient to initialise a ga for optimisation in the third step a multi objective optimisation algorithm such as nsga ii deb et al 2002 is used to perform automatic calibration finally an assessment step is used to determine the final calibrated model assessing land use location with metrics and land use structure dynamics and parameter realism with process knowledge 2 2 case study to demonstrate the proposed hybrid approach it is applied for calibration of the metronamica land use model van delden and hurkens 2011 with a study area of madrid spain metronamica riks 2015 van delden and vanhout 2018 is a generic luca model that uses transition potentials for allocating land use changes van delden and hurkens 2011 transition potential evaluated for every cell at every time step represents the relative potential for a given cell to be occupied by a given land use class in metronamica this is calculated as 1 t p c k γ a c k s c k n c k z c k where tp c k is the transition potential for land use class k to occupy cell c note that all subscripts have the same interpretation a c k is the accessibility factor for class k to occupy cell c the influence that proximity to infrastructure has on class k s c k is the suitability factor for class k to occupy cell c the influence that bio physical factors such as slope have on a given land use type n c k is the neighbourhood effect for class k to occupy cell c the spatial interactions between different land uses in the competition for space z c k is the zoning factor for class k to occupy cell c the influence of spatial planning and γ is a stochastic element included to capture the uncertainty of human decisions the madrid study area is shown in fig 2 it covers an area of 400 by 400 cells at a resolution of 250 m a total area of 10 000 km2 the calibration period is 1990 2000 and the validation period is 2000 2006 to ensure independent data are used to evaluate the proposed methodology contingency tables are given in appendix a corine land use data is used haines young and weber 2006 with the 48 corine level 3 land use classes reclassified into the 13 classes shown in metronamica land use classes are categorised as either active which are dynamically modelled such as active agricultural land or urban areas static which do not change during the simulation but influence land use dynamics such as water or airports and passive which have low transition costs and only change in extent based on the allocation of the other categories such as natural areas of the 13 classes eight are actively modelled four are static and one is passive the case study includes major road data for accessibility and slope and elevation data for suitability 2 2 1 calibration objectives metrics of locational agreement and landscape pattern structure are required to quantify the agreement between the simulated output and the data maps locational agreement is quantified using two metrics to account for agreement of the entire land use map versus transitions garcía álvarez et al 2019 fuzzy kappa hagen zanker 2009 and fuzzy kappa simulation van vliet et al 2013a to measure landscape pattern structure the clumpiness metric mcgarigal 2014 is used as clumpiness is measured at the class level and a single value is required as an objective to represent overall performance during the optimisation process an averaging process is applied to form this objective the error between the clumpiness metric value calculated for actively allocated land use classes for the simulated and data maps are aggregated to this single objective value by taking the weighted clumpiness error wce the wce is used to focus the automatic calibration process on capturing the most important dynamic behaviour that of the urban classes these are distinguished as priority classes and given double weighting in the calculation of wce please refer to appendix b for the explicit formulation of each metric 2 3 implementation this section details the implementation of the proposed approach shown in fig 1 to the case study described previously for a more detailed description of the technical implementation please refer to appendix c 2 3 1 generation of initial solution s to efficiently generate initial solutions the method of newland et al 2018b is used however before the method is applied the calibration problem is simplified by removing certain processes based on the attributes of the case study as the case study consists of a region of urban growth and the calibration period is relatively short 10 years socio economic land use classes such as residential and industry commerce are the most likely to increase in size the major processes driving changes for such land use classes are the self organising behaviour of the system leading to an expansion of existing urban cores couclelis 1989 batty and longley 1994 captured by the neighbourhood effect and accessibility processes verburg et al 2004 consequently these parameters are included for automatic calibration suitability parameters are set manually prior to calibration zoning was not included in this case study to simplify calibration emphasizing the calibration of the interaction rules and accessibility parameters most critical for simulating urban developments to reduce complexity neighbourhood rules are simplified defined by a locus point of inertia or conversion influence based on the current class at the location of interest and a tail that defines the attraction or repulsion influences for classes at different distances using a distance decay function that produces a tail shape consistent with process knowledge of declining impact with increasing distance rules are parameterised as 2 y x c f o r x 0 a e b x f o r 0 x x c 0 f o r x x c where a and b are the controlling parameters of the neighbourhood rule x is the distance to the cell of interest y x is the weight value expressed by the neighbourhood rule c is the locus point of inertia or conversion and x c is the critical distance where the neighbourhood influence is set to zero if a is negative a repulsive influence is generated the four step method of newland et al 2018b removes neighbourhood rules via statistical significance testing based on assessing the influence of certain classes on one another in the first step categorises the remaining neighbourhood rule parameters based on empirical analysis in step two and calibrates the parameters over the final two steps for the case study 176 of a possible 312 neighbourhood rule parameters are removed additionally accessibility parameters not capturing the influence of urban classes are removed manually as they are not considered meaningful for the case study six solutions are subsequently generated over the final two calibration steps by using different weightings of the calibration objectives combined via weighted sum into a single composite objective during calibration two are weighted towards fk and fks two are weighted towards wce and two have a balanced weighting between the objectives the final outputs of this phase are parameter sets with good objective performance outperforming performance benchmarks detailed in section 2 3 4 that are consistent with process understanding 2 3 2 generation of initial population the initial population step generates a quantity of solutions sufficient to initialise a ga given the limited number of initial solutions a novel approach for generating the initial population of solutions is introduced in this paper this approach is developed to increase the quantity of solutions with similar properties good objective performance high plausibility by sampling the parameter space within the vicinity of the process based solutions to achieve this a triangular distribution is applied for localised parameter sampling of a fixed range which assists in preserving the relative relationship between the different parameters the width of the triangular distribution is set relative to the parameter value being sampled for this implementation a value of 25 of the total parameter interval is used given the above the initial population consists of three types of solutions 1 process specific as detailed previously in section 2 3 1 2 sampled obtained by sampling the initial solutions and 3 randomly generated the conventional method for initialising an optimisation algorithm the proportion of the different types of solutions used is given in table 1 totalling 350 the population size as shown randomly generated solutions are the most common to ensure sufficient exploration of the solution space during optimisation the sampled solutions are obtained by generating 200 candidate solutions for each solution from step one and sampling each parameter of these solutions within the triangular distribution this results in 1200 possible parameter combinations with 40 extracted based on pareto dominance finally two replicates of each of the process specific solutions are used 12 total 2 3 3 optimisation the proposed optimisation approach is an adapted version of the multi objective optimisation framework of newland et al 2018a a recommended configuration of the nsga ii algorithm is used including a probability of crossover of 0 9 and a probability of mutation of 0 0029 the inverse of the population size to account for stochasticity in the metronamica model the performance metrics are averaged across 10 different simulations to determine the ultimate performance objective values five optimisation runs are performed with the stopping criterion used being negligible improvement less than 0 5 in the hyper volume metric zitzler 1999 for 50 generations after a minimum of 800 generations have been run this criterion was met for all optimisation runs 2 3 4 assessment the first step of the assessment is evaluating the objective performance of the pareto front solutions against benchmark metrics calculated via neutral models hagen zanker and lajoie 2008 solutions that do not outperform the performance benchmarks are not assessed further this process is repeated for the validation period to ensure that over calibration has not occurred the next stage of the assessment is evaluating the simulated output maps and parameter values for a selection of model parameterisations evaluation of simulated output maps is performed via visual inspection comparing the output maps with reference data a meaningful method of solution quality evaluation hagen 2003 pontius et al 2004 that can engage multiple model users with knowledge of the area hewitt et al 2014 and overcome certain limitations of purely objective evaluation garcía álvarez et al 2019 the parameter values are evaluated to identify solutions with realistic combinations relative to each other 2 4 evaluation of proposed hybrid approach the utility of the proposed method is evaluated by comparing the results from the hybrid approach with those obtained using the process specific method that generated the initial solutions section 2 3 1 and the conventional multi objective optimisation starting with a randomly generated population see newland et al 2018a the conventional multi objective optimisation is configured the same as the hybrid approach as described in section 2 3 3 probability of crossover probability of mutation population size and run for the same number of generations 800 and the same number of runs 5 the performance of the different approaches is compared by considering the objective performance the visual similarity of the simulated outputs and the obtained parameters 3 results and discussion the results are divided as follows first the utility of the proposed hybrid approach is evaluated by considering the objective performance metrics for the calibration and validation period a visual assessment of the simulated output maps for a subset of the solutions see table 3 and an evaluation of a key set of resultant parameters that differentiate solution quality following this the performance is compared with the two alternative calibration methods conventional multi objective optimisation and process specific semi automatic calibration considering objective performance simulated output parameter validity and efficiency of the optimisation process following the evaluation a final calibrated model is recommended 3 1 performance of proposed hybrid approach 3 1 1 objective performance the utility of the proposed hybrid method is evaluated by considering its objective performance the visual similarity of the simulated output maps compared to the data and the realism of the calibrated parameters obtained as part of the first step of this process the pareto front of solutions for the calibration and validation periods is evaluated against benchmark models from which reference metrics are calculated to determine if the output solutions have correctly captured modelled processes reference metrics are given in table 2 the three dimensional objective performance of the hybrid approach for the calibration period is shown in fig 3 two dimensional cross section plots of the objectives are shown in comparison plots with the other two methods in fig 11 in total 253 pareto optimal solutions were identified with the shading of the blue indicating the relative performance of wce lighter points correspond to lower better wce values each dot represents the average performance metric values for the 10 random seeds considered during the optimisation process the black lines define the acceptable region of the objective space for which modelled solutions outperform all the benchmark metric values with the solid and dashed lines showing the growing clusters and random constraint match benchmarks respectively these illustrate the region in the objective space which a solution must sit within to outperform all the performance benchmarks in figs 11 and 12 as the two dimensional objective space is shown the benchmarks are illustrated as lines in fig 3 the benchmark for wce for the random constraint match model is not shown due to the scale as the benchmark wce values are greater than the values obtained for the hybrid approach in the calibration period see table 2 as shown a meaningful trade off was found between the objectives particularly for fk vs wce and fks vs wce only two solutions did not outperform the benchmark metrics for the calibration period which are the two extreme points in the top right of the figure with high fk and fks values fig 4 shows the performance of the proposed hybrid approach for the validation period with a three dimensional plot of the objective performance metrics two dimensional cross section plots of the objectives are shown in fig 12 in fig 4 the benchmark plane for wce is shown for the growing clusters model as shown there is reasonable performance for the solutions obtained though fewer solutions 40 outperformed the performance benchmarks than for the calibration period which was due to having a higher wce than the growing clusters benchmark model 3 1 2 simulated output evaluation given the large number of solutions obtained five were selected for detailed analysis of the simulated output and parameter evaluation these were selected based on objective performance only taking solutions that outperformed the benchmarks for both the calibration and validation period and a visual evaluation picking solutions with high visual diversity relative to the other solutions these solutions are summarised in table 3 ordered by fks value as shown the visual diversity corresponds to a range of metric values the solutions analysed are shown by the black dots in figs 3 and 4 and henceforth are referred to as solutions h1 to h5 with solution h1 corresponding to the point shown in fig 3 with the best fks performance and solution h5 corresponding to the point shown in fig 3 with the lowest wce but relatively poor performance in terms of fks and fk these solutions were analysed in further detail by considering the simulated output maps generated for the calibration period these are shown in fig 5 with 5 a 1 showing the data map for the end of the calibration period 2000 and the remaining maps showing simulated output for each solution ordered by decreasing fks value also shown are agreement maps between the data and the simulated output for the classes residential red in the land use map and industry commerce purple in the land use map as these are the major urban classes and capturing their behavior accurately is a major focus of calibration as shown in fig 5 the simulated output was generally more clustered than the data however solution h5 corresponding to the best wce value appears to be the most consistent with the data as there is less over clustering than for the other solutions this is discussed further for the classes residential industry commerce and airports below as shown in fig 5 the simulated output across the solutions analysed exhibits a general tendency for the residential areas to be more clustered in the simulated output than in the data and tends to favour the existing larger residential clusters suggesting the solutions place an over emphasis on the attraction of residential to itself this is most noticeable in figs 5b 2 and 5c 2 interestingly these solutions also feature fewer transitions to the class residential which might explain why the fks value is slightly higher because the solutions with worse fks are capturing incorrect transitions to residential the appearance of industry commerce purple in the simulated output is also generally more clustered than in the data this again is likely due to an over emphasis on the attraction of industry commerce to itself in certain solutions h2 and h3 there also appears to be more interspersion of industry commerce in residential areas shown in figs 5c 1 and 5d 1 than expected given the data when analysing the historic data new areas of industry commerce tended to be located near major roads which only occurred in solution h5 though not necessarily the roads expected based on the data the latter issue could potentially be resolved by further differentiating the various road types as only one type of road was included finally behaviour for the airports grey in the eastern portion of the land use map is often not consistent with expectation airports tend to be near urban areas but are not expected to be attractors of residential or recreation areas or attractors of classes such as forest however for solutions h3 and h4 there appears to be an attractive influence between airports and forest as shown in figs 5 d 1 and 5 e 1 the obtained parameters in the discussed solutions confirmed the attraction of the different land uses to the airport so a solution to possibly eliminate this behaviour in future would be to exclude such unrealistic interactions from the parameter space during the optimisation preserving the parameter elimination from the process specific approach additionally introducing metrics such as the enrichment factor van vliet et al 2013a which focuses on neighbourhood composition as an optimisation objective could potentially improve the resultant output 3 1 3 parameter evaluation in is impractical to present an evaluation of all parameters obtained from the resultant hybrid approach hence an analysis of key parameters that differentiate the solutions based on which are more consistent with process knowledge including inertia points conversion point for conversions to the class residential conversion points for conversion to the class permanent crops influence tails for influence on the class residential and the accessibility parameters are presented below the inertia parameters for the different solutions obtained using the hybrid approach were generally consistent with expectation as shown in fig 6 the general trend is for the urban classes residential and industry commerce to exhibit the highest inertia and agricultural land use classes arable land pastures and other agriculture to exhibit the lowest inertia with the other classes exhibiting moderate inertia while this general trend is observed across the solutions it is most pronounced for solution h1 the conversion point parameters obtained for the hybrid approach were generally consistent with expectation as shown in fig 7 with high conversion values from agricultural land use classes most notably arable land however there are also examples of inconsistency for example solution h3 which has an almost zero conversion value to residential from arable land and h4 which has a high conversion value to residential from the class industry commerce that potentially explains the interspersion observed in the simulated output fig 5 e1 also the values are relatively low compared with the residential self influence which partially explains the larger than expected clusters of residential conversions to the class permanent crops are shown in fig 8 across the solutions analysed these parameters are generally positive which might be expected for conversions for the classes natural areas and arable land which are the dominant conversions across the solutions but not for urban classes residential industry commerce and recreation areas solution h5 is the only solution that does not include any conversion points to this effect which may explain why the behaviour in the simulated output is more consistent with expectation than that of the other solutions influence tails between the different land use classes were generally consistent with expectation though this varied across the solutions evaluated an example of this is shown for the cross influence tails to the class residential for the solutions analysed shown in fig 9 for the influence of urban classes residential industry commerce recreation areas as shown solution h5 exhibits much lower self influence of the class residential than the other solutions analysed the accessibility parameters for the influence of major roads are shown in fig 10 for the urban classes residential industry commerce and recreation areas with the weight and distance decay parameters being similar across the classes and solutions though the distance decay for recreation areas was generally lower than that for the other two though this is consistent with expectation the parameter values are fairly equal with the exception of solution h5 which has a lower distance decay for industry commerce implying this land use must occur closer to the accessibility layer in the model which is consistent with evaluation of the simulated output section 3 1 2 and process understanding as this land use class is often located near major roads 3 2 comparative assessment 3 2 1 objective performance a comparison of the objective performance of the different calibration methods is presented in fig 11 for the calibration period comparing the 253 pareto optimal points for the hybrid approach blue dots with the 392 pareto optimal points identified using the conventional multi objective optimisation method red dots and the six solutions obtained using the process specific method green crosses fig 11a shows the three dimensional objective space and indicates the benchmark metric values as planes in this space fig 11 b 11 d show a two dimensional comparison of the trade off between the different combinations of performance objectives fks against wce b fk against wce c and fk against fks d fig 11 b 11 d indicate the solutions that were evaluated with black dots for the hybrid approach and black diamonds for the conventional multi objective optimisation approach fig 11 shows that the proposed hybrid approach outperformed both the other calibration methods in the objective space for the calibration period the starting position of the process specific solutions has clearly been improved upon and the pareto front has better objective performance and more diversity than the pareto front of the conventional multi objective optimisation approach this is particularly observed in fig 11c and d where as seen in the bi objective comparison the pareto solutions from the proposed method completely dominate all pareto solutions from the conventional multi objective optimisation approach the improved objective performance of the solutions obtained using the hybrid approach over the other two methods was similar for the validation period shown in fig 12 as for the calibration period fig 11 the proposed hybrid method clearly outperformed the conventional multi objective optimisation method for fks vs wce and fk vs fks with similar performance for fk vs wce hence the hybrid approach is confirmed to have the better objective performance as opposed to over fitting the calibration period 3 2 2 simulated output evaluation given the large number of solutions obtained for the conventional multi objective implementation the same selection criteria were used for further evaluation selecting five solutions for detailed analysis of the simulated output and parameter evaluation these solutions are summarised in table 4 ordered by fks value as shown the visual diversity corresponds to a range of metric values evaluation of the simulated output maps via visual interpretation suggests that the solutions produced using the proposed hybrid approach were more consistent with the data than the solutions obtained using the conventional multi objective optimisation and process specific approaches the simulated output maps are presented in fig 13 for the conventional multi objective optimisation method and fig 14 for the process specific method for the conventional multi objective optimisation approach there are solutions that can be immediately eliminated from further consideration given the visual output because there is too much interspersion of the classes residential and industry commerce figs 13d 1 to figs 13d 3 or occurrences of residential in areas of the map that are completely unexpected and unrealistic figs 13f 1 and figs 13f 2 the other simulated maps generated by the conventional multi objective optimisation approach exhibit more clustering of the major urban classes residential red and industry commerce purple across the solutions analysed than the data and the corresponding seeded output for the process specific solutions the simulated output maps generated when there was a preference for locational agreement exhibited less clustering of the urban classes analysed though still more than the data but removed residential area in the south west region which is not consistent with expectation the simulated output maps generated when there was a preference for landscape pattern structure exhibited a similar issue and also resulted in large clusters of industry commerce in totally different areas than observed in the data though this solution did seem to include better attraction to the relevant roads in the allocation of industry commerce the limited ability of the solutions across the three calibration methods to capture sufficient attraction of the industry commerce land use class suggests there was insufficient detail in the road network classes used as mentioned previously 3 2 3 parameter comparison the calibrated parameters obtained using the proposed hybrid approach were more consistent with expectation than those obtained using the conventional multi objective optimisation approach comparison plots are presented in appendix d for the parameters previously analysed section 3 1 3 for the conventional multi objective optimisation approach and the hybrid approach the calibrated parameters obtained using the conventional multi objective optimisation approach were not as consistent with process understanding as those obtained using the hybrid approach this was generally due to there being less relative separation between certain influence values or the obtained parameters being unexpected given the case study an example of less relative separation of values was observed for the inertia points obtained using the conventional multi objective optimisation approach where the solutions had agricultural classes such as pastures and other agriculture exhibiting inertia values that were approximately equivalent to those of the urban classes residential industry commerce recreation areas which is inconsistent with expectation as agricultural land uses generally exhibit less inertia to facilitate allocation of new urban classes also examples of unexpected parameter values that were not as prevalent in the hybrid approach were a high conversion parameter for residential to permanent crops and vice versa and higher attraction between infrastructure classes and urban classes highlighted in appendix d for the influence of airports on residential and road rail on residential 3 2 4 optimisation efficiency optimisation is a computationally demanding process and hence efficiency obtaining useable results with a limited computational budget is important and is a well recognised issue when applying evolutionary algorithms to realistic problems gibbs et al 2008 2015 zheng et al 2017 hence the relative efficiency of the hybrid approach is compared with the conventional multi objective optimisation approach by comparing the objective performance metrics that exceed the performance benchmarks previously discussed to compare the efficiency of the two optimisation approaches the computational effort was judged in terms of model evaluations that is how many times the luca model was run over the calibration period to generate a simulated output map and calculating the metrics that measure the agreement between the simulated output and data maps as this is by far the most computationally expensive step in the optimisation procedure a process that takes several seconds other processes such as optimisation algorithm operations e g crossover and mutation and the generation of random solutions were considered negligible given the relatively small computational effort required with such processes generally taking milliseconds to performs to appropriately evaluate the computational efficiency of the hybrid approach the computational budget of generating the initial set of solutions was considered the starting population generated using the hybrid approach was equivalent to just over two optimisation generations as detailed below six solutions were obtained using the process specific calibration approach each generated over 1060 model evaluations including 60 model evaluations for the coarse calibration stage and 1000 for the fine calibration stage making a total of 6360 model evaluations for each of the six process specific solutions 200 sample solutions were generated requiring a total of 1200 model evaluations as the random parameter generation has negligible computational demand for the remaining members of the seed population a total of 7560 model evaluations were performed to generate the initial population for the hybrid approach as a population size of 350 is used for the optimisation and each member of the population is evaluated over 10 random seeds one generation of the optimisation process required 3500 model evaluations equivalent to just over two generations consider fig 15 which shows the percentage of the population exceeding both metric benchmarks for the proposed hybrid approach and the conventional multi objective optimisation approach as shown the hybrid approach generated plausible models with far less computational effort with 20 of solutions considered plausible after approximately 200 generations compared with approximately 600 generations for the same share of plausible solutions for the conventional multi objective optimisation approach and 40 considered plausible after 400 generations a level of plausibility that could not be achieved using the conventional multi objective optimisation approach for the number of generations considered in this study hence the hybrid approach has a major efficiency advantage over conventional multi objective optimisation if the optimisation process were run for much longer both approaches would potentially yield similar results in terms of objective performance outperforming the benchmarks however as discussed previously this would be infeasible from a practical perspective given the computational resource requirements 3 3 recommended parameter values of the semi automatic calibration methods compared in this analysis the hybrid approach performed better than the conventional multi objective optimisation and process specific methods the objective performance is superior to that of the other methods and the corresponding simulated output although the parameter values obtained using the process specific method were more consistent with process understanding of the hybrid approach solutions evaluated the recommended solution was judged to be solution h5 generating the simulated output shown in fig 5f this solution had parameter values that were the most consistent with those expected from knowledge of land use change processes was visually most consistent with the land use change data and was characterised by the best clumpiness metric value of the solutions evaluated the recommended parameterisation for the case study is given in appendix e 4 conclusions and recommendations to address issues with the computational efficiency and parameter validity of optimisation based approaches applied to automatically calibrate transition potential based luca models this research proposes a hybrid method to integrate process knowledge into optimisation based automatic calibration the proposed approach incorporates process knowledge throughout the different phases of an optimisation approach using system understanding in the generation of the initial solution using knowledge of parameter relationships to convert the limited number of solutions into a population sufficient for use during optimisation and assessment that incorporates evaluation of land use structure and parameter realism in the specific implementation shown the proposed hybrid method was used to generate a population of solutions in good regions of the parameter space based on the output of a process specific semi automatic calibration method to initialise a more powerful yet computationally demanding multi objective optimisation algorithm that otherwise does not take into account process specific information the presented method is generic allowing for the use of different optimisation algorithms land use models and process specific calibration methods the performance of the hybrid approach was demonstrated via application to a case study of madrid spain with its utility compared with a conventional multi objective optimisation based automatic calibration method and a process specific semi automatic calibration method used in isolation the results showed the following key advantages of the hybrid approach 1 avoidance of over calibration verified by the validation performance 2 superior objective performance compared to the process specific method 3 superior objective performance and greater plausibility for the simulated output and parameter values generated compared to a conventional multi objective optimisation approach with the same computational budget 4 more efficient identification of solutions that outperform performance benchmarks than the conventional multi objective optimisation approach with the same computational budget given the advantages achieved using the hybrid approach further avenues utilising such an approach should continue to be explored first additional detail could be incorporated into the input data for example a more detailed road network to improve calibration performance the method could also be used in the application of decision support systems to long term simulations which could include adaptation to other luca modelling frameworks such as sleuth to demonstrate how process knowledge can be utilised in a differently parameterised luca model to improve optimisation based calibration also greater process knowledge could be incorporated to further enhance efficiency and parameter validity by filtering unrealistic parameters or using a more detailed parameterisation method and combined with additional filtering so as to not compromise efficiency for neighbourhood rules to allow for more diverse and complex interactions to be generated to potentially further enhance the plausibility of the rules obtained which could further improve calibration performance software availability name of software land use model parameter sampler developer charles p newland contact address 39a claremont avenue netherby sa 5062 contact email charlesnewlandprofessional outlook com software requirements current implementation is linked with metronamica land use model http www metronamica nl use with metronamica is most straightforward but code can be adjusted for linking with other land use models first year available 2019 program language python 3 program size 60 mb availability and cost open source software downloadable from https github com ursidean lum sampler git declaration of competing interest i confirm that this work is original and has not been published elsewhere nor is it currently under consideration for other publications i have no conflicts of interest to disclose acknowledgements the authors wish to acknowledge the financial support from the bushfire and natural hazards cooperative research centre made available by the commonwealth of australia through the cooperative research centre program this work was supported with supercomputing resources provided by the phoenix high performance computing services at the university of adelaide we would also like to acknowledge the reviewers for their time and feedback which greatly assisted the quality and readability of this paper appendix a contingency tables this section contains contingency tables for the data maps for the madrid case study for the calibration 1990 2000 and validation 2000 2006 periods table a1 madrid contingency table 1990 2000 table a1 map 2000 luc nat arl per pas oag res i c rec for r r air m d fre tot map 1990 nat 46821 268 3 0 50 1051 316 164 357 31 12 161 20 49254 arl 1540 55265 30 0 383 1492 1059 121 32 70 92 351 26 60461 per 50 18 4278 0 0 29 3 0 0 0 0 18 5 4401 pas 13 0 0 624 0 23 5 0 0 0 0 0 0 665 oag 191 34 15 0 20761 207 45 22 6 5 0 101 0 21387 res 1 4 0 0 6 7653 85 31 0 1 0 0 0 7781 i c 3 8 0 0 0 15 1152 5 0 0 0 0 0 1183 rec 0 0 0 0 0 37 0 656 0 0 0 0 0 693 for 55 16 0 0 8 21 0 11 12164 0 0 0 7 12282 r r 0 0 0 0 0 0 0 0 0 145 0 0 0 145 air 0 0 0 0 0 0 4 0 0 0 494 0 0 498 m d 33 6 0 0 0 29 26 39 0 0 0 368 2 503 fre 24 4 0 0 0 0 0 0 0 0 0 0 719 747 tot 48731 55623 4326 624 21208 10557 2695 1049 12559 252 598 999 779 160000 table a2 madrid contingency table 2000 2006 table a2 map 2006 luc nat arl per pas oag res i c rec for r r air m d fre tot map 2000 nat 47105 386 17 68 152 451 90 34 166 107 6 68 81 48731 arl 191 52871 35 0 249 865 442 50 5 518 186 202 9 55623 per 17 25 4149 0 45 38 5 0 2 11 0 10 24 4326 pas 13 0 0 586 0 17 0 0 2 4 0 0 2 624 oag 27 58 39 336 20499 109 43 11 17 30 0 39 0 21208 res 116 55 0 0 55 9931 136 86 21 62 90 2 3 10557 i c 16 23 0 0 0 287 2300 30 0 28 11 0 0 2695 rec 19 4 0 0 4 47 13 953 1 1 0 7 0 1049 for 143 4 0 0 16 37 3 6 12324 9 15 2 0 12559 r r 0 0 0 0 0 148 9 0 0 95 0 0 0 252 air 6 44 0 0 0 6 4 0 0 0 538 0 0 598 m d 88 78 0 11 25 71 71 41 0 11 13 590 0 999 fre 0 1 0 4 0 0 0 0 0 0 0 10 764 779 tot 47741 53549 4240 1005 21045 12007 3116 1211 12538 876 859 930 883 160000 appendix b metric formulation this appendix contains specific details of the formulation of the metrics used as performance objectives for this research locational agreement locational agreement is quantified using two metrics fuzzy kappa hagen zanker 2009 and fuzzy kappa simulation van vliet et al 2013a these two metrics are used to balance two types of spatial land use agreement that of all cells in the land use map measured by fuzzy kappa fk and that for land use class cells that transition from one time point to another measured by fuzzy kappa simulation fks both metrics have the same functional form b1 f k s p o p e 1 p e where p o is the observed agreement the number of matches of land use class pixels between the data and simulated output map and p e is the expected agreement the number of matches expected given the data and simulated output map fk and fks vary by how observed and expected agreement are calculated with fks weighted heavily toward cells that transition between time periods both metrics range between 1 and 1 landscape pattern structure landscape pattern structure is measured using the clumpiness metric clumpiness is a measure of the proportional deviation of the proportion of like adjacencies i e whether the land uses adjacent to a cell of a particular land use class are the same class from what is expected under a spatially random distribution mcgarigal 2014 it is calculated first by determining the proportion of like adjacencies b2 g k g k k l 1 l g k l min e k where g k is the proportion of like adjacencies for land use class k g kl is the count of like adjacencies between patches of class k and l using the double count method mcgarigal 2014 l is the total number of land use classes and min e k is minimum perimeter of a maximally clumped patch of class k defined as b3 min e k 4 n 4 n 2 4 n 4 n 2 a k n 2 a k n 1 n a k n 1 n where a k is the area of class k in terms of number of cells and n is the length of a side of the largest integer square possible with a smaller area than a k with the proportion of like adjacencies calculated it follows that clumpiness is calculated by b4 c l u m p y k g k p k p k g k p k 1 p k for g k p k and p k 0 5 otherwise where p k is the proportion of the landscape occupied by patch type class k clumpiness is measured for each land use class in a specific land use map and ranges from 1 fully disaggregated to 1 fully clumped as clumpiness is measured at the class level and a single value is required as an objective to represent overall performance during the optimisation process the error between the metric value calculated for actively allocated land use classes for the simulated and data maps are aggregated to a single value by taking the weighted clumpiness error wce wce is used to focus the automatic calibration process on capturing the most important dynamic behaviour that of the urban classes these are distinguished as priority classes and given double weighting in the calculation of wce b5 w c e i n p a w i c e i i p a 2 w i c e i where n npa is the number of non priority active classes n pa is the number of priority active classes and ce i is the clumpiness error of active class i appendix c technical implementation this appendix provides a detailed explanation of the implementation of the proposed hybrid approach generation of initial solution the first step of the proposed implementation is to generate a calibrated model efficiently using system understanding as detailed in section 2 3 1 this is achieved by using the method of newland et al 2018b to calibrate the neighbourhood rules the steps of this method are as follows 1 parameter elimination where certain parameters within the different conceptual groups of neighbourhood rules inertia points conversion points self influence tails cross influence tails are eliminated based on statistical analysis and user discretion 2 parameter categorisation where the remaining parameters within each group are graded 3 coarse parameter adjustment tuning the parameters within their respective groups using meta parameters and 4 fine parameter adjustment tuning the parameters individually prior to implementing the method the neighbourhood rules are simplified the effect of attraction and repulsion captured by the tail of the neighbourhood rule generally has a diminishing effect with increasing distance hence to greatly reduce the complexity of the calibration problem neighbourhood rules are parameterised as shown in equation c1 c1 y x c f o r x 0 a e b x f o r 0 x x c 0 f o r x x c where a and b are the controlling parameters of the neighbourhood rule x is the distance to the cell of interest y x is the weight value expressed by the neighbourhood rule c is the locus point of inertia or conversion and x c is the critical distance where the neighbourhood influence is set to zero note that if a is negative a repulsive influence will be generated as there are 8 active and 13 total classes there are a total of 104 neighbourhood rules and hence 312 neighbourhood rule parameters for calibration in the parameter elimination step a statistical analysis is used to remove certain neighbourhood rule parameters that based on the representation of transition cells relative to the total land use map are not assessed as having a statistically significant representation for the impact to be considered meaningful for land use change when applied to the case study 176 conversion points and influence tails are eliminated all inertia points and self influence tails are included when reviewing the removed parameters they are consistent with expectation for example conversions from urban land use classes e g residential industry commerce recreation areas to agricultural land use classes e g arable land pastures no additional neighbourhood rule parameters are removed based on user discretion accessibility parameters are removed manually only including accessibility for the urban land use classes as these were considered meaningful for the case study following the parameter reduction categorisation of the remaining neighbourhood rule parameters within their respective groups inertia points conversion points self influence tails cross influence tails is performed by calculating the enrichment factor van vliet et al 2013b for each land use class and grading each parameter based on user defined enrichment factor levels this is performed to establish a hierarchy of parameter values prior to calibration as transition potential model parameters have no absolute interpretation i e the specific influence value has no physical meaning but can be interpreted relative to each other for example the inertia of urban land use classes e g residential should be higher than agricultural land use classes e g arable land due to the higher transition costs of such land use classes for this case parameters are graded as either high medium or low relative to the other values in a specific neighbourhood rule parameter group following parameter categorisation a coarse calibration step is performed to refine the neighbourhood rule values of the conversion points self influence tails and cross influence tails relative to the inertia points for example the best values of the calibration objectives may be found when the conversion points maintaining the high medium and low classification are scaled to be 10 of the inertia point values for the respective groups as opposed to 5 or 15 the final step is a fine tuning process where the neighbourhood rule parameters included for calibration are individually adjusted in this process the other neighbourhood rule parameters are held constant whilst a single parameter is tuned to optimise a weighted sum of the calibration objectives six solutions are generated by using different objective weightings two weighted towards fk and fks two weighted towards wce and two with a balanced weighting between the objectives the final outputs are parameter sets with good objective performance that are consistent with process understanding generation of initial population the objective of this step is to produce a population of solutions sufficiently large to initialise a ga as the process specific method only produces six solutions and the required population size is 350 this is not straightforward consequently a novel approach for generating the initial population of solutions is introduced in this paper when seeding the initial population with solutions based on process understanding a decision has to be made as to what proportion of the initial population should be based on process understanding and what proportion should be random if the proportion of solutions developed based on process understanding is too high it might be difficult for the optimisation process to explore potentially better regions of the solution space resulting in premature convergence to sub optimal solutions by contrast if this proportion is too low the influence of the process based solutions might be insufficient to guide the search into promising regions of the search space and to increase convergence speed in addition given the limited number of initial solutions an approach has to be developed to increase the quantity of such solutions with similar properties good objective performance high plausibility based on system understanding hence a method is developed to generate additional solutions by sampling in the vicinity of the process based solutions as has been done in other problem domains bi et al 2015 2016 a triangular distribution is used for this implementation because it has a fixed range which assists with preserving the relative relationship between the different parameters it should be noted that the sampling is done in a way that preserves the parameter reduction detailed previously an example of fitting a triangular distribution to a model parameter is shown in figure c1 where the mean of the distribution θ shown by the vertical grey line corresponds to a parameter value obtained in the initial solution generation stage and the upper and lower limits expressed as a percentage of the obtained parameter value influence the range of the potential sampled values as shown by increasing the limits the potential range of parameter values obtained is broader for example if an inertia parameter of 1000 was obtained during stage one during the sampling procedure it would be varied between 900 and 1100 if the narrow limits are used or 700 and 1300 if the wide limits are used fig c1 comparison of triangular distributions with varying limits for sampling of parameter values fig c1 the purpose of selecting limits is to ensure sufficient variation without compromising solution quality for example if the limits are too wide for inertia parameters the result may be that urban e g residential and agricultural e g arable land land use classes have relatively equal inertia which is inconsistent with expectation and would compromise the quality of the sampled solutions for this implementation a width of 25 of the total parameter interval works best to achieve this the random solutions are generated in the same way as a ga traditionally initialises the optimisation process by randomly generating parameter values within a certain range randomly generated solutions utilise the distance decay function to parameterise neighbourhood rule tails but do not include the parameter reduction for this implementation the proportions of process specific sampled and randomly generated solutions are given in table c1 and are determined from preliminary testing as shown most of the solutions are randomly generated to ensure sufficient diversity during the optimisation process most of the remaining solutions are generated via the sampling approach to ensure sufficient exploration of the promising regions of the parameter space that have both good objective performance and are consistent with system knowledge for this implementation 200 sampled solutions are generated from each process specific solution from the combined 1200 solutions a total of 40 are extracted based on pareto dominance i e extracting non dominated solutions from the available candidates to serve as the sampled input finally two replicates of each of the initial solutions are included in the initial population hence 12 in total table c1 proportion of solution types used for initial population table c1 solution type number of total population percentage of total population randomly generated 298 85 2 sampled 40 11 4 process specific 12 3 4 optimisation the hybrid approach uses an adapted version of the multi objective optimisation framework of newland et al 2018a for this implementation rather than starting the optimisation process with a random population of solutions the initial population is made up of the mixture of solutions detailed in table c1 as part of the optimisation process the optimisation algorithm takes the initial population of luca model parameter combinations and iteratively improves them by running each through the luca model to generate a simulated output map and calculating a set of land use metrics to quantify the different aspects of luca model performance i e locational agreement and landscape pattern structure for each solution the sets of parameter values contained within the population are then adjusted based on feedback of their relative performance using the operators of the ga e g selection cross over and mutation see maier et al 2019 this work uses the non dominated sorting genetic algorithm ii nsga ii proposed by deb et al 2002 because of its demonstrated ability to tune luca model parameters trunfio 2006 cao et al 2014 newland et al 2018a a recommended configuration of the nsga ii algorithm is used with a probability of crossover of 0 9 and a probability of mutation of 0 0029 the latter equalling the inverse of the population size 350 it is important that the randomness within the metronamica model is appropriately considered during the optimisation process hence the generation of simulated output maps and calculation of performance metrics is repeated for 10 different random number seeds for each solution during the optimisation process and the average for the performance metrics across the ten seeds used as the ultimate performance objective value the optimisation stage of automatic calibration is an iterative adjustment process that is performed until certain stopping criteria are met for this implementation the stopping criterion adopted is based on solution convergence after a set number of generations in this case 800 convergence implies that there is negligible improvement of the calibration objectives with continued optimisation which in this case is measured using the hyper volume metric zitzler 1999 the hypervolume summarises the convergence and diversity of the pareto front by measuring the n dimensional objective space covered by the pareto front relative to the nadir point in the parameter space where all objectives are minimised the solutions are considered to have converged when there is negligible improvement in the hyper volume for 50 generations defined as less than 0 5 improvement in the hyper volume metric when a minimum of 800 generations had been run this occurred for all optimisation runs performed however if convergence had not been achieved additional generations would have been run the multi objective optimisation stage is implemented using a parallelised version of nsga ii to ensure completion of the optimisation within a feasible 48 h amount of time this was performed on the phoenix high performance computing resources operated by research services at the university of adelaide the optimisation runs are evenly distributed across a number of cpu processing cores with a number of slave processes evaluating the objective functions of different members of the population and a master process co ordinating the search using the nsga ii genetic operators selection crossover mutation non dominated sorting and crowding distance to evolve the population of solutions assessment the final component of the proposed hybrid approach is an assessment step which is used to determine a final calibrated model the assessment step is required because of the multi objective nature of the optimisation process used resulting in a pareto front where all solutions are optimal based on the objective performance metrics hence subjective input based on process understanding is used to select which of these model parameterisations to adopt the first step of the assessment is evaluating the objective performance of the pareto front solutions this is conducted by evaluating the performance against neutral models which generate pseudo simulated output based on varying growth strategies with the same boundary conditions for which benchmark metrics are calculated these metrics provide context for calibration performance as objective performance can be attributed to either modelled processes or boundary conditions hagen zanker and lajoie 2008 as neutral models have the same boundary conditions the output metrics can be considered minimum objective performance targets which if outperformed by calibrated solutions suggest the correct capture of modelled processes benchmark metrics are calculated for fk and wce because fks has an implicit baseline at a value of zero solutions that do not outperform the performance benchmarks are not assessed further further evaluation is performed by repeating this process for the validation period to ensure that over calibration has not occurred the next stage of the assessment is conducted by evaluating a selection of model parameterisations on the pareto front for these solutions the simulated output maps and parameters values are evaluated based on process knowledge evaluation of simulated output maps is performed via visual inspection comparing the output maps with reference data which is a meaningful method of solution quality evaluation hagen 2003 pontius jr et al 2004 that can engage a wide number of model users with knowledge of the area hewitt et al 2014 and overcome certain limitations with only evaluating solution quality using metrics garcía álvarez et al 2019 the parameter values are evaluated to identify solutions with realistic parameter combinations relative to each other appendix d comparison plots of parameters analysed for seeded and unseeded calibration method this appendix contains a series of plots comparing the parameters obtained from the hybrid and conventional multi objective optimisation calibration methods the inertia point parameters the conversion point parameters for transitions to the class residential conversion point parameters for transitions to the class permanent crops influence tails for interactions with the class residential and the accessibility parameters generally the conventional multi objective optimisation calibration method resulted in higher parameter values the conventional multi objective optimisation method also resulted in more examples of parameters that were not consistent with expectation for example high inertia across a majority of classes figure d1 high conversion values for transitions from the classes forest pastures and permanent crops to residential figure d2 greater emphasis on transitions to permanent crops from the class residential figure d3 attractive influences on residential land exerted by classes such as airports and road rail figure d7 and no weighting given to residential or recreation areas for accessibility figure d8 fig d1 comparison of inertia influence per class for analysed solutions for hybrid approach and conventional multi objective optimisation methods fig d1 fig d2 comparison of conversion point influence values to class residential for each land use class for analysed solutions for hybrid approach and conventional multi objective optimisation methods fig d2 fig d3 comparison of conversion point influence values to class permanent crops for each land use class for analysed solutions for hybrid approach and conventional multi objective optimisation methods fig d3 fig d4 comparison of tail influences to the class residential for the natural land use classes for analysed solutions for hybrid approach and conventional multi objective optimisation methods fig d4 fig d5 comparison of tail influences to the class residential for the agricultural land use classes for analysed solutions for hybrid approach and conventional multi objective optimisation methods fig d5 fig d6 comparison of tail influences to the class residential for the urban land use classes for analysed solutions for hybrid approach and conventional multi objective optimisation methods fig d6 fig d7 comparison of tail influences to the class residential for the infrastructure land use classes for analysed solutions for hybrid approach and conventional multi objective optimisation methods fig d7 fig d8 comparison of accessibility parameters for the urban land use classes for analysed solutions for hybrid approach and conventional multi objective optimisation methods fig d8 appendix e final model parameterisation table e1 inertia conversion parameters table e1 from to arl per pas oag res i c rec for nat 13 11 31 90 0 00 16 07 0 91 0 02 45 43 0 00 arl 247 42 49 80 1 14 61 99 117 34 0 14 15 08 87 65 per 16 74 467 24 3 26 3 77 23 24 1 40 0 11 2 60 pas 4 00 32 95 594 33 0 02 0 68 27 18 4 62 22 25 oag 6 54 16 55 5 62 656 86 18 46 47 99 11 69 0 06 res 4 37 0 22 10 44 0 15 1111 86 18 10 4 60 0 17 i c 0 49 0 01 13 20 0 05 0 28 749 02 0 00 0 10 rec 0 02 0 01 0 02 4 04 3 80 1 87 650 87 0 02 for 0 94 3 21 0 22 1 43 1 64 0 01 21 72 1080 55 r r 3 27 0 04 7 74 2 79 0 29 0 02 0 73 0 14 air 14 04 1 01 2 74 2 57 0 06 0 32 4 43 2 39 m d 11 51 0 00 0 14 0 58 0 04 1 95 5 57 0 14 fre 0 03 28 43 46 40 0 30 0 58 4 36 2 27 0 01 table e2 neighbourhood rule a parameters table e2 from to arl per pas oag res i c rec for nat 95 99 2 67 26 73 35 26 196 98 245 31 11 46 18 57 arl 95 65 16 60 120 62 247 87 15 75 151 90 0 79 143 91 per 73 55 185 52 376 35 76 05 149 34 124 99 0 00 29 72 pas 72 20 56 79 53 03 3 00 162 01 0 06 38 57 49 21 oag 149 04 202 01 89 77 5 72 19 47 40 53 73 04 24 07 res 330 08 8 83 84 60 56 19 265 13 378 97 95 87 19 19 i c 43 87 194 36 299 39 155 99 246 42 989 89 416 06 30 89 rec 0 30 0 61 167 85 188 65 9 46 12 19 921 89 1 89 for 82 47 1 95 14 37 29 76 2 19 0 05 350 72 76 24 r r 0 23 0 87 108 12 200 15 31 86 538 68 78 14 262 63 air 105 76 77 21 160 91 17 81 40 17 712 40 176 30 37 74 m d 490 11 19 72 0 03 87 37 98 26 862 42 53 91 168 49 fre 47 12 3 77 74 31 146 16 68 52 8 80 122 35 5 09 table e3 neighbourhood rule b parameters table e3 from to arl per pas oag res i c rec for nat 2 78 2 30 2 15 2 73 1 48 2 59 1 66 2 31 arl 2 31 2 74 1 58 3 02 2 60 1 80 2 17 2 21 per 1 69 2 52 2 98 1 83 2 31 2 52 1 98 2 17 pas 2 39 2 38 2 52 2 11 2 97 2 54 2 77 1 07 oag 2 42 2 35 3 37 1 94 2 33 2 78 2 35 3 38 res 2 40 2 09 2 30 2 16 2 31 2 31 2 01 2 15 i c 2 61 3 11 3 27 2 34 2 32 2 57 2 85 2 31 rec 2 64 2 23 2 33 2 65 2 54 1 62 2 31 2 28 for 2 30 2 63 2 73 1 81 2 31 2 16 2 34 2 36 r r 2 30 3 17 2 27 2 34 2 31 1 94 2 99 2 86 air 2 21 2 34 1 59 2 32 2 73 2 65 2 92 2 32 m d 2 56 2 50 1 51 2 32 2 49 2 31 2 72 1 41 fre 2 31 2 12 2 75 2 32 1 99 1 54 3 14 2 03 table e4 accessibility parameters table e4 land use class major roads distance decay major roads weight arl 0 87 0 00 per 3 53 0 00 pas 2 51 0 00 oag 1 27 0 00 res 15 29 0 39 i c 10 76 0 54 rec 15 11 0 50 for 2 06 0 00 
