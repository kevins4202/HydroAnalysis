index,text
20840,the effect of extreme wind speeds in applications for design is of great interest in a variety of fields such as meteorology and coastal engineering in these fields a common problem is the scarcity of long datasets to overcome this limitation a common approach is to utilize the entire available dataset using the peak over threshold pot approach in small samples there may be a limited number of extremes and so re sampling is often beneficial however the re samples are often affected by dependency and the independence limitations are usually disregarded to alleviate this effect the deca uncorrelated decaun model is proposed taking into account the correlation effect when re sampling this model provides an improvement to the current physical de clustering algorithm deca by re sampling the samples of deca irregularly in time the methodology proposed in this assessment is illustrated using wind speed data from a high resolution database over the north sea the atlantic ocean and the mediterranean sea from this evaluation the decaun model is proposed as an alternative re sampling strategy for observations irregularly spaced in time keywords extreme value analysis deca model similarity de clustering pot threshold selection modelling irregular samples slotting autocorrelation non rectangular kernel irregular correlation estimator nomenclature abbreviation definition pot peaks over threshold approach gpd generalized pareto distribution gev generalized extreme value distribution i i d independent and identically distributed limitations acf sample autocorrelation function ssl standard storm length tawn 1988 ps parameter stability plot coles 2001 nc diagnostics threshold selection from the multiple threshold model proposed in northrop and coleman 2014 gxcf gaussian kernel smooth function for the slotting method bm annual block maximum sampling approach runs standard de clustering model smith and weissman 1994 deca the physical de clustering algorithm deca model decaun the proposed deca uncorrelated decaun model variable definition ν y the annual sample length of the hourly wind speed time series h standard width parameter of the gaussian weight function δ τ effective width of the gaussian kernel and set equal to the mean sampling time intervals c b d w h δ τ normalized bandwidth bdw of the gaussian kernel weight function lag k apart the required minimum number of k values between successive observations of the irregular deca sample u threshold for the modelling of the excesses ξ ˆ shape parameter estimates of the gev and gpd dep pre defined energy percentage level of energy reductions for cluster separation from deca resid ξ ˆ the residual metric resid ξ ˆ ξ ˆ r e f ξ ˆ m k test the non parametric rank based mann kendall test at a significance level of a 0 05 a 3 in appendix naoi the index of the north atlantic oscillation dv design values or return levels dv 50 d v estimates of the 50 years return period nrmse normalized root mean square error rp return period 1 introduction a crucial problem often arising in the assessment of climatological and hydrological extremes is the scarcity of long and complete time series regarding wind speed time records many authors have outlined a minimum annual length of 10 years to be regarded as the absolute minimum for a comprehensive extreme value analysis eva based on the annual block maximum bm sampling approach cook 1985 abild et al 1992 coles and walshaw 1994 additional studies on bm outline a minimum annual length of 13 years for an adequate fit to the generalized extreme value gev distribution brabson and palutikof 2000 however samples with a limited amount of extremes often fail to provide effective quantile estimates for large return periods when little information is available for analysis re sampling is often imperative the question arises as to how to successfully enlarge a sample of extremes when the independence assumption is violated one classical approach to this problem is to make use of more than one observation per year by using the r largest order statistics concept recent applications of this concept in wind speed time series are found in an and pandey 2007 other approaches use the entire time series and consider different ways to extract the values to be processed under the principles of the extreme value theory extremes in environmental series tend naturally to form clusters and consequently exhibit serial correlation the standard sampling approach under the independent and identically distributed i i d limitations should involve identifying independent clusters of extremes from which the cluster maxima is then typically extracted to give us a sample of approximately independent cluster maxima the latter strategy defines the de clustering concept in time series where applications can be found in ferro and segers 2003 however the asymptotic independence in cluster maxima is essential in eva but frequently disregarded the distribution and parameters of limited long range dependent extremes are strongly influenced by the dependence in the series leadbetter 1983 in reviewing the relevant literature the most popular de clustering strategies in stationary time series are block and runs method smith and weissman 1994 the improved method of independent storms mis harris 1999 the filtering de clustering approach fawcett and walshaw 2007 the standard storm length ssl tawn 1988 the separation time approach by walshaw 1994 morton et al 1997 and the physical de clustering algorithm deca model soukissian et al 2006 soukissian and kalantzi 2009 bernardara et al 2014 arns et al 2013 vanem 2011 oikonomou et al 2020 another sampling approach besides the classical bm is by modeling exceedances over a high enough threshold u at which the generalized pareto distribution gpd provides a valid approximation to the excess distribution the peaks over threshold pot approach davison and smith 1990 simiu and heckert 1996 naess 1998 harris 2005 caires and sterl 2005 is widely accepted as the standard approach for modeling cluster exceedances and the most suitable for small samples the major disadvantage of pot is related to the selection of the appropriate threshold value in a trade off between bias and variance traditionally the pearson correlation estimator is regarded as the standard approach for quantifying the dependence in de clustered observations however the validity of the pearson estimator may be questioned when the observations of these samples are irregularly spaced in time scargle 1989 zhang et al 2008 the accepted methods for processing irregular observations in terms of correlation analysis are sorted into three major classes i direct transform methods spectral analysis estimating the spectrum of the irregular sample by a generalization of the fourier transform lomp scargle periodogram he et al 2009 babu and stoica 2010 ii interpolation methods suggesting re sampling at uniform intervals schulz and stattegger 1997 kreindler and lumsden 2006 and finally iii slotting methods computing the correlation of irregularly sampled data to generalize the pearson correlation operator mayo 1993 this is achieved by calculating the products of observations according to their sampling time differences using the gaussian kernel as a weight function over the observations the technique was developed in fluid mechanics and applications can be found in tummers and passchier 1996 benedict et al 2000 and rehfeld et al 2011 the classical methods analyzing the correlation effect in samples irregularly spaced in time often yield poor performance in comparison to the non rectangular gaussian slotting method examples of other slotting approaches such as slot boundaries local normalization fuzzy slotting and variable windowing can be found in nobach 2002 or damaschke et al 2018 and references therein in this study a re sampling strategy is proposed for samples irregularly spaced in time the irregular samples are formed as the maxima of grouped observations the separation of groups under the cluster concept is derived from energy reductions between consecutive time steps in observations assigned hereafter as discrete energy reduction levels the proposed deca uncorrelated decaun model performs re sampling taking into account the correlation effect in the irregular samples of deca for a range of discrete energy reduction levels this effect is evaluated using the gaussian kernel weight function for the computation of the generalized pearson correlation operator from the slotting method the performance of the gaussian kernel used is assessed over a range of smoothing factors bandwidths enhancing the performance of the weight function over the irregular sample of observations the functionality of the proposed model requires two subjective admissions these admissions encountered the main objectives of our evaluation which include the favorably bound estimation of the energy reduction level the optimal bandwidth smoothing response of the weight function to the irregularly spaced observations in time taking advantage of these arguments a dependent sample of extremes is reconstructed successfully by decaun to an asymptotically independent re sample converging to the i i d limitations the performance of the decaun model in applications where there is a limited availability of data is one key motivation of this work therefore the assessment is carried out for relatively small datasets of annual wind speed time series corresponding to two sample periods of 10 and 15 years the quantile estimations of the proposed model are evaluated based on estimates from the reference bm approach bm ref with a block size of one year and assuming the available records are of at least 20 years palutikof et al 1999 the second key motivation of this analysis is the performance of the proposed model with regards to the return levels design values and return periods of wind speed for locations over the north sea the atlantic ocean and the mediterranean sea with respect to this evaluation decaun is demonstrated to be a viable alternative model for extreme wind speed projections considering relatively small sample periods of data however the proposed model was confounded by large variability following the reduction in sample size from the re sampling process the nomenclature denotes all symbols and abbreviations frequently used in this analysis this paper is organized as follows in section 2 we commence with the methodology and section 3 the diagnostics for the henceforth statistical analysis emphasizing to the irregular modeling of the proposed re sampling procedure in section 4 all models used in this setting and their characteristics are analytically presented we proceed in section 5 with the wind characteristics of the study area in short and the associated data used furthermore in section 6 all results in terms of the return levels confidence bounds and the shape parameter estimates of the model fit are illustrated and discussed finally in section 7 we summarize the basic findings of this assessment 2 methodology the present univariate analysis aims to assess the effect of the asymptotic distributional behavior of two types of extreme wind speed sampling data techniques the first type of sampling data will be the classical bm which is extracted from blocks of observations of annual length subsets and modeled by the gev family of distributions the second type models the peaks in samples of wind speed exceeding a high enough threshold u by the approximation to the gpd within the pot framework the latter two distributions in extremes are proven to be the limiting distribution of the annual maxima and the peak exceedances respectively for inference in environmental studies nikulin et al 2011 papalexiou and koutsoyiannis 2013 in time series under dependence the de clustering concept is regulated from excluding successive events in order to succeed asymptotic independence however the events that form the de clustered sample must be controlled in a way to avoid the statistical bias effect in the results the focus on developing approaches that aim to enlarge the sample of extremes beyond the standard annual maxima often disregards the effect of forming samples that are irregularly spaced in time the irregular sample of n observations considers the successive differences δ t t i 1 t i at the corresponding observation times t 1 t 2 t i t n in the sample not necessarily equal samples of irregular observations are proven to be sensitive to standard correlation analysis vio et al 2000 broersen 2009 mondal and percival 2010 to handle these important features of irregularity in the correlation analysis the slotting method is first employed to the samples of deca followed by a re sampling strategy from the decaun model to these samples in this way the samples of irregularly dependent observations in time are reconstructed efficiently under independence the latter reconstruction is the main goal of the present work all results are evaluated systematically alongside the standard runs method of de clustering where hereafter denoted as the runs model which is assigned as the standard comparable model in this assessment before proceeding thoroughly to the diagnostics of the methodology in this setting the following should be outlined the implementation of the runs model for identifying clusters uses a a threshold value from the multiple threshold model northrop and coleman 2014 commonly referred to as the nc diagnostics and b the extremal index estimator based on the intervals method proposed in ferro and segers 2003 the decaun model will be implemented using a a gaussian kernel smooth function gxcf rehfeld and kurths 2014 for the slotting method over the lagged products of the irregular deca observations and b a range of discrete energy reduction levels and bandwidths for the correlation analysis of the unequally spaced samples of deca in time the extreme value model parameter estimation of the gev and gpd distributions will be based on the maximum likelihood estimator mle the statistical software package extremes in r gilleland and katz 2016 is used for the estimation of the associated parameters 3 diagnostics in the following sub sections the standard correlation estimator which disregards the irregular sampling scheme effect is briefly presented before introducing the slotting method as the correct method 3 1 standard correlation estimator autocorrelation or lagged correlation refers to the correlation among members of a series of numbers arranged in time a random process is considered statistically stationary if the first and second moments are time invariant covariance stationary and if the sample moments converge in probability to the population moments ergodic under this assumption from a sample of n number of observations regularly spaced at times t 1 t 2 t i t n the correlation between observations that are separated by a lag number of k equally distant time steps δ t δ t t i 1 t i is defined as the sample autocorrelation function acf at each time lag k 1 ρ x k c o v k σ x 2 1 n k t 1 n k x t x x t k x 1 n t 1 n x t x 2 for any positive integer k n here c o v k is the sample autocovariance at time lag k for the regularly sampled process in time where x and σ x 2 is the sample mean and variance respectively the sample acf takes both positive and negative values 1 ρ x k 1 while the associated plot is referred to as correlogram the generalized bartlett s formula for acf bartlett 1946 yields the following standard error 2 s e ρ x 1 2 i 1 k 1 ρ x i 2 n for the associated 95 confidence interval the confidence bounds of the correlogram effectively test the null hypothesis of convergence to an uncorrelated approximation by that particular lag number of k time steps if ρ x k is located outside the bounds it means the preceding autocorrelations have not been successfully reduced to close to zero the desired lag number of k or lag k is set as the first time lag entering the confidence bounds for samples irregularly spaced in time the inter sampling times vary and the standard acf described in eq 1 cannot be directly applied chatfield 1996 an alternative to this restriction is the generalization of the regular correlation operator through a rectangular or non rectangular kernel function which this approach is commonly called the slotting or slotting autocorrelation method 3 2 slotting method to analyze the unevenly spaced time series edelson and krolik 1988 and mayo 1993 considered estimating the pair products of all available observations and discretizing them into bins according to their sampling time differences in this study the estimator of acf for the irregularly n number of observations is derived as the weighted mean over the available products according to their sampling time differences and desired time lag for which the correlation is estimated replacing the sample autocovariance divided by the sample variance of the irregular process for zero time lag and is presented as 3 ρ x k c o v k σ x 2 i 1 n j 1 n x i x j b h k t j t i h i 1 n j 1 n b h k t j t i h i 1 n j 1 n x i x j b h 0 t j t i h i 1 n j 1 n b h 0 t j t i h the observations x with indexes i j in eq 3 hereafter are considered as centralized and standardized values in addition the associated discrete time variables for these observations are re scaled as a transformation from the observed t o b s to the t t o b s δ t for each index i j respectively where δ t 1 n i 1 n 1 t i 1 t i is the mean sampling time interval accordingly the lag number of k is also dimensionless considering the re scaled k lag k δ t for which the acf is estimated the weighting function b h used in eq 3 is generally referred to as the kernel estimator for the product inter sampling time intervals t j t i the estimator b h was introduced by de oliveira 1963 and nadaraya 1964 and is stated 4 b h k t j t i h 1 h k k t j t i h where k is the kernel that determines the shape of the weighting function rectangular triangle gaussian etc which is symmetrically placed around t j t i and positive on the interval t j t i h t j t i h the parameter h is defined as the bandwidth or smoothing parameter which determines the width of the weighting function and adjusts the amount of smoothing applied over the inter sampling time intervals hence the kernel estimate of the unknown density f k is approximated by 5 f ˆ h k 1 n 2 i 1 n j 1 n b h k t j t i h where n 2 is the number of the weights used for the inter sampling time intervals moreover isolated peak estimates are derived for h 0 where h 0 the kronecker delta function denoted as δ is the approximation to the asymptotic limit lim h 0 k k t j t i h δ k t j t i the distribution of the kernel estimator and bandwidth selection are the subjective requirements to optimize the properties of the estimator wand and jones 1994 the most popular kernel distribution is in practice the gaussian due to its analytical properties however the optimum bandwidth is not guided by mathematical considerations babu and feigelson 1996 hall et al 2004 there is a vast amount of literature suggesting practical methods for optimal bandwidth estimation derived by the minimization of the distance between the unknown density function f k and the estimator f ˆ h k the major drawback of this measure of distance is over and under smooth density estimates jones et al 1996 simonoff 1996 therefore for the weighting function as defined in eq 4 we considered the usual gaussian distribution as the kernel function and for the bandwidth selection we did not restrict our analysis to a single optima in this setting the performance of the irregular acf estimator as stated previously in eq 3 is evaluated against a range of possible bandwidths recommended by sheather 2004 3 3 non rectangular kernel the slotting method from edelson and krolik 1988 used a rectangular kernel to select the products whose time lag is not further than half the bin width from the time lag k i e 6 b h k t j t i h 1 for t j t i k 1 2 0 otherwise where the lag bin width in their study is set equal to the mean sampling time interval in this study the slotting method uses a non rectangular kernel function gaussian kernel to weight the product inter sampling time intervals the advantages of this function include primarily the increase of the pair products to be averaged into the irregular acf estimator therefore a sudden cutoff in the time domain is prevented weighting the products smoothly according to the difference between the product inter sampling time interval t j t i and the considered time lag k the gaussian kernel density function tends to zero for time differences much larger or smaller than the considered time lag k hall et al 1994 bjornstad and falck 2001 and defined as follows 7 b h k t j t i h 1 h 2 π e x p k t j t i 2 2 h 2 and h c δ τ however there is no theoretical functional of the effective width of the weight function the gaussian kernel used in this study considers the standard width parameter h to be scaled to the mean sampling time intervals i e δ τ δ t 1 n i 1 n 1 t i 1 t i to adjust the effective width of the weight function satisfactorily to the mean width of the time intervals this way it is ensured that observations appearing at an almost constant frequency are rated higher than infrequent observations for brevity parameter c in eq 7 is defined as the normalized bandwidth to increase the precision of the analysis the degree of smoothing is of great importance the selection of a large bandwidth will result in an over smoothed performance of the density function while a small value will under smooth the kernel estimator examinations on asian monsoon records from rehfeld et al 2011 revealed empirical normalized bandwidth at c 0 25 for the analysis undertaken in this study we considered the range of c 0 125 1 with a 0 125 step to optimize the irregular acf estimator in eq 3 as defined in the slotting method in the following sub sections the deca and decaun models will be described in detail 4 models the present univariate analysis discusses the challenges in assessing the effects of the asymptotic distributional behavior of two types of wind speed samples in extremes the first type is the bm modeled by the gev family of distributions and the second sample forming the pot exceedances is modeled by the approximation to the gpd for extreme value analysis the asymptotic forms of the gev and gpd are the most widely used statistical distributions for describing extremes of wind speed beirlant et al 2004 and holmes 2015 under the knowledge of the possible intractable problems of inference related to small sample sizes the gpd for a fixed threshold u was chosen as the asymptotic form of the excesses in this setting specifically when the analysis is restricted to a set of cluster peak exceedances the use of a gpd distribution with an upper bound to the modeling of extremes of wind speed requires caution in applications fawcett and walshaw 2007 it is shown that the parameter inconsistency of the mle estimator to small samples affects the return level estimates of extremes however when wind speed is characterized of being heavy right tailed and extremes are typically modeled from short tailed distributions with finite right endpoint the asymptotic forms of gev and gpd can be a reasonable assumption for modelling recognizing the possible bias effect to the estimates fawcett and walshaw 2006a 2006b ashkar and tatsambon 2007 pinheiro and ferrari 2015 before proceeding to our analysis a summary table 1 is presented with the basic characteristics of the models used and a short introduction on the basic properties of the asymptotic forms in extremes 4 1 model properties 4 1 1 bm and pot exceedances the fisher tippett gnedenko theorem fisher and tippett 1928 and gnedenko 1943 implies convergence of appropriately normalized infinite number of block maxima to one of three types of extreme value distributions known as the fisher tippet type i ft i or gumbel class of distributions type ii ft ii or fréchet class and type iii ft iii or reverse weibull class these three classes of distribution can be combined into a single form for modeling the distribution of maxima given from the generalized extreme value gev distribution such that 8 g x μ σ ξ exp 1 ξ x μ σ 1 ξ ξ 0 exp exp x μ σ ξ 0 where the corresponding location μ scale σ and shape ξ parameters are defined for μ σ 0 ξ respectively specifically the support of extreme values from the gev is bounded from the right hand side when x μ σ ξ and ξ 0 otherwise unbounded on the right if μ σ ξ x and ξ 0 however when ξ 0 and x the subset of the gev leads to the gumbel class of distributions assuming that the conditions of the fisher tippett gnedenko theorem hold pickands 1975 and balkema and de haan 1974 showed that the distribution function of the excess y x u x u over a high enough threshold u can be approximated through the gpd that is defined as follows 9 g y σ ξ 1 1 ξ y σ 1 ξ ξ 0 1 exp y σ ξ 0 where the scale parameter is defined as σ ξ u μ and supported for σ 0 while the shape parameter is supported for ξ and is common to gpd and gev the gpd distribution is generalized because it assumes different distribution classes in the same sense as gev specifically when ξ 0 and 0 y σ ξ the gpd corresponds to the asymptote class of distributions with a finite right endpoint and is considered bounded on the contrary if ξ 0 and y 0 the gpd is considered unbounded or heavy tailed with an infinite right endpoint finally the case where ξ 0 and y 0 is interpreted as the limiting case for gpd converging to the exponential class of distributions summarizing the above if bm is approximated by the gev distribution then the threshold excesses have a corresponding approximate distribution within the gpd family and vice versa under the hypothesis that the gev distribution with estimated parameters μ ˆ σ ˆ ξ ˆ is a suitable model the level x p for p 0 1 satisfying g x p 1 p is defined by the quantile function related to the return levels or design values against the return periods and given by the following equation 10 x p μ ˆ σ ˆ ξ ˆ μ ˆ σ ˆ ξ ˆ 1 log 1 p ξ ˆ ξ ˆ 0 μ ˆ σ ˆ log log 1 p ξ ˆ 0 where the value x p is assigned as the return level or design value with return period r p 1 p similarly to the bm case the return level x p that is exceeded on average once every 1 p observations for the model of the pot exceedances assuming that x p u over a suitable high threshold u is given by 11 x p σ ˆ ξ ˆ ζ ˆ u u σ ˆ ξ ˆ ζ ˆ u p ξ ˆ 1 ξ ˆ 0 u σ ˆ log ζ ˆ u p ξ ˆ 0 where the estimated parameters σ ˆ ξ ˆ correspond to the gpd family moreover coles 2001 suggested that the probability of exceedance of u denoted as ζ u pr x u follows the binomial distribution bin n ζ u where n is the total number of observations from the time series hence a natural estimator of ζ u is ζ ˆ u n u n where n u denotes the number of exceedances 4 1 2 parameter estimation and uncertainty several numerical methods for the estimation of the associated parameters of the gev and gpd distributions have been proposed e g from naess and clausen 2001 bermudez and kotz 2010b outlining the pickands estimator probability weighted moments pwm ordinary moments mom and l moments lmom however only the mle method is theoretically effective and provides approximate normal distributions and approximate sample variances that can be used to formulate confidence bounds for inference for this analysis the mle method was implemented setting the discussion within the most popular framework for stationary bm and pot samples this way all results are easily comparable with those reported in the relevant literature the regularity conditions of the gev and gpd exist when the shape parameter ξ which is equal for the two distributions is restricted specifically the mle is valid when ξ 1 although the asymptotically normal properties of the parameters are valid for ξ 0 5 when ξ 1 the estimators generally do not exist davison and smith 1990 grimshaw 1993 tajvidi 2003 in practice for the modeling of extremes of wind speed it is likely to obtain more often zero and negative estimates i e indicating right tail distributions of an exponential type or short and light tailed respectively with an infinite or finite right endpoint rather than positive value estimates of the shape parameter jonathan and ewans 2013 brabson and palutikof 2000 with regards to the reference bm ref model fitting the gev using the sample of 20 years the estimated 95 confidence intervals of the associated extreme value model parameters are derived by the normal approximation to the distribution of the mle estimator coles 2001 however the interval estimates of the parameters from the bm model fitting the gev using samples of 10 and 15 years is supported by the standard non parametric bootstrap method percentile efron and tibshirani 1994 the preceding bootstrap method is also employed to runs deca and decaun for the confidence bound estimates of the parameters fitting the gpd the latter method is considered for the samples of 10 and 15 years respectively to these models it should be pointed out that the use of percentile bootstrap for deriving confidence bounds is arguably the simplest and most intuitive bootstrap interval approach this approach is found in various environmental applications davison and hinkley 1997 ferro et al 2005 kharin and zwiers 2005 the non parametric approach performs sample replications without any prior knowledge of the distribution of the sampling data the 95 confidence intervals of the examined statistics are estimated from a large number of repeated samples in the present bootstrap application 999 bootstrap replicates are generated from resampling with replacements from the given sample furthermore the so called alpha level was selected to a 0 05 providing the apparent proportion of samples for which the 95 confidence intervals would fail to cover the examined statistics 4 2 standard runs model a rational method for identifying clusters of extremes in time series was presented in smith and weissman 1994 and employed to the pot exceedances this was achieved by introducing the idea of the runs length of r observations that fall below a high enough threshold u the required r number of observations is closely related to the extremal index θ nandagopalan 1990 the latter index describes the clustering tendency of the data and is equivalent to the reciprocal of the limiting mean cluster size supported in the range 0 θ 1 before applying the runs model in this setting for the identification of clusters the extremal index should be estimated in order to specify the optimal runs length parameter of the given time series the runs model used relies on the intervals estimate ferro and segers 2003 for the associated θ u providing an optimal threshold u from the multiple threshold model by the nc diagnostics northrop and coleman 2014 the nc diagnostics fit piecewise equality of the shape parameter using score and likelihood ratio tests wadsworth and tawn 2012 ideally the p values as an output of the tests should increase as the threshold increases the threshold is set at the point where approximate stabilization is achieved the null hypothesis should be not only rejected at that point but also at all other higher thresholds however an informal approach is also considered by locating the threshold at the sharpest p value increase this threshold should impose a separation of at least 50 events in total jonathan and ewans 2013 and generate 5 to 10 events year as reported for example in mazas and hamm 2011 the multiple threshold model has an advantage over other threshold models as it requires less subjectivity and experience to detect an appropriate threshold scarrott and macdonald 2012 bommier 2014 4 3 deca model the cluster formation proposed by soukissian et al 2006 aims at separating approximately independent events considering physical limitations to identify the successive groups of observations the events are defined as a continuous physical phenomenon of the environmental variable the model separates all events by looking at energy reductions in consecutive time steps initially the available time series is filtered twice using a simple monotonicity criterion obtaining a series of local maxima successive points with the exact same value are removed under the monotonicity criterion from the latter series the local maxima and minima are identified considering hereafter the third power of their corresponding values therefore numerical differences of cubic local maxima to the next consecutive cubic local minima are identified selecting a pre defined energy percentage dep with respect to cubic local max values the method introduces wind energy percentage reductions wind climatology studies have established that the wind power potential available in a flow of air per unit cross sectioned area normal to the flow is proportional to the third power of wind speed reed 1974 12 p w i n d 1 2 ρ v 3 where ρ is denoted as the air density and v is the wind speed the time indexes of local minima that correspond to the identified energy reductions are referred to as transition points as a result the series of transition points samples the initial time series into successive and approximately independent wind state clusters of generally unequal length then by selecting the maximum value of each previous cluster a sample of approximately irregular maxima randomly spaced in the time axis is derived even if each event can be associated with a particular instant of occurrence on the time axis the derived sample does not depend on time anymore the optimum dep level should generally be high enough in order to safely separate the approximately independent events but it should not be extremely high in order to avoid unnecessary concatenation of clusters and thus loss of data information at this point it should be mentioned that the initial de clustering approach of soukissian et al 2006 avoided assumptions regarding the irregular effect of sampling at any dep level furthermore deca presumed that the statistical threshold to the gpd model fit should be provided hierarchically and set to the modal value without further investigation taking into account the re scaled series of local wind energy for this analysis we considered eight levels sampling the series of transition points the dep levels are set constant at values 60 65 70 75 80 85 90 and 95 with the time indexes of local minima initiating the successive irregular clusters to these levels 4 4 decaun model in this section the re sampling schemes are defined in addition the irregularly modeling procedure of decaun is presented in steps considering all dep and bandwidth values 4 4 1 re sampling schemes the samples of deca at the associated dep levels required further investigation in terms of correlation re sampling is advised when the condition of independence is violated miquel 1984 lang et al 1999 in this assessment two re sampling scheme strategies for the samples of deca are proposed and denoted as decaun 1 and decaun 2 respectively the re sampling scheme decaun 1 is formed as follows i selecting the maximum value of the corresponding deca sample ii identifying and selecting the following lag k apart values of the remaining data from both sides of the value chosen in i until all available values are considered the re sampled decaun 2 is closely related to the concept of the ssl soares and scotto 2004 the decaun 2 scheme consists of the following steps i identifying and selecting the largest value of the correspondent deca sample ii discarding values with a lag k apart from both sides of the value chosen in i iii selecting the next largest value of the remaining data and finally iv repeat steps ii and iii until all data are used at this point the lag k apart value for re sampling is difficult to estimate it is the required minimum value between successive irregular maxima of deca clusters which renders the maxima statistically independent the desired lag k is obtained from the estimator algorithm of acf for different time lags the latter algorithm is defined as similarity providing estimates for the irregular acf in eq 3 using the non rectangular gaussian kernel gxcf in eq 7 in this way the products x i x j of the irregular acf estimator are weighted according to their difference between the product inter sampling time interval t j t i and the associated time lag k for the samples of deca the desired lag k apart value is derived as a time lag transformation from the correlogram it is set as the value of k observations obtained from the transformation of the first time lag entering the confidence bounds described by the generalized bartlett s formula in sub section 3 1 the acf estimator algorithm similarity is utilized in sample periods of 10 and 15 years the available software analyzing irregular samples in time and the correlogram at given time lags can be found in the nestoolbox https tocsy pik potsdam de nest php cited in rehfeld and kurths 2014 an example of the re sampling scheme decaun 2 described in this section is illustrated in fig 1 4 4 2 modeling irregularly in time the irregularly modeling procedure was carried out for eight dep levels at values 60 65 70 75 80 85 90 and 95 deriving an equal number of deca samples furthermore each deca sample was modeled at eight normalized bandwidths c 0 125 1 with a 0 125 step deriving an equal number of similarity estimates this way all deca samples are re sampled to decaun 1 and decaun 2 as previously described the associated lag k apart value for re sampling will be derived from the correlogram of acf using the estimator algorithm similarity for all bandwidth considerations step 1 the re samples of decaun 1 and decaun 2 are effective only if the common assumption of stationarity is not violated for this reason the non parametric rank based mann kendall m k test at a significance level of a 0 05 was implemented see also a 3 the aim of the test was to ensure the absence of a monotonic upward or downward trend of the examined re samples the null hypothesis of the test is h 0 no monotonic trend is present against the alternative h there is a monotonic trend present the test is used and discussed in the context of eva in cheng et al 2014 it should be noted that the application of the m k tests to the re samples is a fundamental step as the presence of possible temporal dependence and monotonic trends can affect and bias the gpd model fit which relies on the hypothesis of independent observations step 2 the re samples of decaun at each dep level are subjected to a statistical gpd model fit however the statistical threshold considerations for the model fit will be set within a range of values u 0 first quartile mean mode and median obtained from the re samples respectively the optimum re samples are assigned in terms of the lowest aic and the lowest statistic mse under the statistical threshold considerations step 3 the decaun re samples derived from step 2 are considered optimum for the statistical gpd model fit all previous steps 1 and 2 are repeated over all normalized bandwidth considerations at this point we note that the quantitative comparison of the decaun re sampled model fit was not based on the standard aic and mse criteria thus neither common criteria for model selection nor goodness of fit tests are appropriate for evaluating the quality of the model fit therefore we considered a metric guide rule in the least square sense to measure the goodness of fit each optimum model fit will be counted upon a relative measure of performance based on the estimated design values d v per sample period n y denoted as d v o p t d e p u c n y the relativity of the metric is gauged by the associated estimates from the bm approach within the largest annual available time series d v max the quantitative measure is defined as a normalized root mean square error n r m s e based on the modal position of hyndman and fan 1996 13 n r m s e n y 1 n t r p t d v r p max d v o p t r p d e p u c n y d v r p max 2 where t 2 10 20 90 and 100 indicating the return periods r p of length n t 11 and ν y 10 15 years denoting the two sample periods of examination the precision of the n r m s e measure will be counted upon the d v max estimates of the reference model bm ref using the largest available sample of 20 years 1996 2015 in this implementation the optimum normalized bandwidth of c 0 125 1 with a 0 125 step is the value that minimizes the nrmse estimates of the re samples in step 2 at this optimum bandwidth selection the optimum dep level is also derived and the optimum re sampling scheme of decaun this empirical procedure applied for the selection of the optimum normalized bandwidth to the weighting function is a result of no theoretical rule of the effective bandwidth the aforementioned steps 1 2 and 3 are respectively applied to the two sample periods of examination for a comprehensive overview of the irregularly modeling process a flow diagram is illustrated in fig 2 summarizing in brief the key steps followed in this setting from the time series to the re sampling scheme 5 study area and dataset our examination is focused on different offshore regions in europe with a special focus on the north sea the european coastline that is exposed to the atlantic ocean and finally the mediterranean the considered locations in these regions are of high interest in terms of wind energy and offshore activities their characteristics vary and are highly affected by the different climatological patterns for each region specifically the wind conditions in the north sea are driven by the passage of cyclonic systems such as extra tropical cyclones influenced by the inflow of oceanic water from the atlantic ocean this combined with the north sea s shallow water basins result in a remarkable offshore wind profile sušelj et al 2010 the west european offshore locations exposed to the atlantic ocean are affected by the extra or post tropical cyclones that are generated along with the polar and the arctic front respectively dodet et al 2010 finally the mediterranean sea is a semi enclosed basin surrounded by complex mountainous terrain and is divided into several sub basins with contradistinctive characteristics in addition the mid latitude cyclone passage results in complicated wind patterns with extreme winds a detailed description of the main mediterranean winds is provided by zecchetto and cappa 2001 and the references therein to gain more insight about the behavior of the extreme fluctuation of wind speed at these regions a high resolution data product is considered for a reliable statistical analysis especially near the offshore regions of europe where the demonstration of decaun is challenged from the highly dependent regional effects surface roughness landmass etc the time series used are for a period of 20 years from 1996 to 2015 they have been extracted from the marina platform database created within the framework of the homonymous project 2014 the dataset was produced as an outcome of atmospheric modeling hindcast simulations providing information for the entire european coastline with an hourly time frequency and a spatial resolution of 5 km the atmospheric model used is skiron kallos et al 1997 the outcome has been evaluated within the framework of the marina platform project see http forecast uoa gr oldproj php in the present study the wind components of the model are obtained at 10m above sea level for the 30 in total selected locations denoted as l1 l2 l29 and l30 with their descriptive statistics presented in detail in appendix table a 1 1 and table a 1 2 specifically the extremity of wind speed is inferred from the systematically positive values of the skewness and kurtosis statistical estimate to all locations indicating heavy tailed distributions of elongated right tail for the wind speed see the discussion in kalogeri et al 2017 the ability of the high resolution database to reproduce more intense extremes particularly at the european offshore regions will strengthen the re sampling strategy of decaun to short samples at these locations all models however are applied to the wind speed datasets at locations respectively shown in fig 3 6 results and discussion in the following sub sections the decaun model estimates from the statistical analysis are presented to evaluate the model behavior for the selected 30 locations under asymptotic independence the proposed model reconstructed a dependent sample of observations that are irregularly spaced in time to motivate decaun from this study the effect of the model to the correlation analysis is presented and discussed to illustrate the implications of the re sampling procedure in addition the number of observations from the re samples in this setting is in agreement with the extreme wind variability at these sites of interest all re samples from the decaun model assume that an average of at least 1 65 peaks per year should be selected in a pot approach in order to gain an advantage over block maxima cunnane 1973 tanaka and takara 2002 serinaldi and kilsby 2014 the n r m s e measure of the re samples stated in equation 13 is evaluated for all dep levels normalized bandwidths c and statistical threshold considerations in addition the sensitivity of inference to the measure of performance is obtained using samples of 10 and 15 years regarding sample periods from 1996 to 2005 and 1996 2010 respectively furthermore the quantile estimates from decaun are compared to the estimates derived by the reference model bm ref employing to the latter the maximum available series extending from 1996 to 2015 20 years estimations in terms of the optimum dep levels of deca will be illustrated at each sample period for the 30 selected locations in sub section 6 1 for inference of the gaussian kernel estimator to the optimum re sampling scheme used at these locations the normalized bandwidth c and the desired lag k apart estimates are included in sub sections 6 2 6 3 moreover the associated n r m s e measure of performance is also presented in sub section 6 4 for the optimum re sample of the decaun model at each location furthermore one representative location from each region locations l2 l16 and l21 is selected for the illustration of decaun in terms of the return level estimates and variability of the proposed model to each sample period specifically the three representative locations of the strong wind conditions observed within each area are located in the southern part of the north sea l2 with highest wind speed 26 704 m s in table a 1 2 the northern part of the atlantic l16 with 28 671 m s the highest wind speed and the western part of the mediterranean l21 with 25 901 m s the highest wind speed respectively in addition the shape parameter estimates from all models are presented for these three locations the foregoing estimations are illustrated in sub sections 6 5 6 7 respectively 6 1 dep level estimates beginning with the estimated dep level of energy reductions for the physical de clustering approach from deca the results are presented in fig 4 by means of colored dots assigned to the eight levels 60 90 and 95 with a 5 step the dep level estimates per sample period resulted in a safe range for deca to cluster events without the loss of valuable information in general the increase in sample size leads to reductions in the dep level for the majority of the locations specifically as the sample period increased from 10 to 15 years the modal value estimate of dep was slightly reduced for the 10 locations l1 l2 l9 and l10 in the north sea additionally reductions of the mode estimate of the dep level is obtained for the 10 locations in the atlantic l11 l12 l19 and l20 and the 10 locations l21 l22 l29 and l30 in the mediterranean respectively the increase in sample size does not necessarily follow the large increase in the number of extreme events as expected the approximate stable or reduced dep level estimates as the sample size increased controlled the deca model to encompass the largest number of events as possible this effect is illustrated as follows the response of decaun to the modal value estimates of the dep levels is illustrated in fig 5 at the 30 regional locations of interest inference is made for the number of observations of the samples of deca that are irregularly spaced in time and most likely to be sampled from these locations the moderate to low mode value estimates of the dep levels for the 10 locations in the north sea 0 68 for the 10 years and 0 66 for the 15 years are in line with the variability of extreme winds over this region one key point of this agreement is the influence described by the large scale circulation patterns such as the north atlantic oscillation nao on the wind speed over the north sea the index of the north atlantic oscillation naoi is strongly correlated with the wind speed over northern europe having an impact on the cyclones generating in this area by shifting the westerly zonal flow however an increase in the naoi and the mean wind speeds from the 1960s to the mid 1990s does not necessarily increase the extreme wind profile sušelj et al 2010 thus the dep estimates for these locations are controlled by deca to a moderate to low level in order to enlarge as much as possible the number of extreme wind storm events considering the atlantic where strong extra tropical storms can cause massive storm surges affecting the 10 locations there the moderate to high mode value estimates of dep 0 83 for the 10 years and 0 74 for the 15 years is somewhat reasonable it is well known that the extra tropical cyclones travel eastward along the polar jet stream e g the icelandic low and azores high thus the prevailing westerlies affecting the locations l16 l19 l14 l17 and l20 in the north atlantic feser et al 2015 justify the moderate to high dep estimates from a large number of extreme wind storm events obtained over these sites on the other hand locations l15 l12 l11 and l18 in the central and location l13 in the south atlantic are also influenced by the strong pressure centers over the atlantic ocean specifically the number of atmospheric circulation patterns that govern the extreme wind speed variability at these locations are influenced by the atmospheric dynamics in the north atlantic as discussed in pascual et al 2013 justifying the dep estimates at this region for the 10 locations in the mediterranean the mode value estimates of the dep level is characterized as moderate to high 0 85 for the 10 years and low 0 63 for the 15 years the nature of the storms in this semi enclosed basin is subjected to many external factors like land sea contrasts near surface temperatures atmospheric waves and large scale weather patterns flaounas et al 2015b campins et al 2011 although the windiest areas of the mediterranean sea are located in the nw sw part e g locations l25 l26 l21 and l27 and the se part e g locations l22 and l23 the dep level estimations at these locations are characterized as moderate this effect is due to the large but constant wind conditions at these sites where deca is controlled by a moderate level of dep in order to encompass the largest number of extreme wind storm events as possible on the contrary locations l28 l29 l24 and l30 at the central and north mediterranean are characterized by a high dep level as expected where the largest number of explosive cyclogenesis is observed 6 2 bandwidth estimates in this section the results of the optimum normalized bandwidths for the 30 locations are presented from the empirical selection procedure as described previously in sub section 4 4 the most likely optimal normalized bandwidth estimates illustrated in fig 6 range in a bound from 0 126 to 0 267 avoiding over or under smoothing kernel adjustments it is apparent that as the sample period increased from 10 to 15 years the optimal normalized bandwidth was reduced for the majority of the locations given the relatively small sample period of data such as 10 years it is not surprising that we obtain a limited number of events from the deca model thus the irregular sample of deca will be characterized by a limited amount of values surrounding the mean width of the time intervals as a consequence the gaussian weight function adjusted accordingly to the largest bandwidth for the sample period of 10 years in this way for the estimator of the irregular acf in eq 3 the largest possible number of higher weights is assigned to the inter sampling time intervals closer to the given time lag k in other words the weight function stretched out to a wider bandwidth scale in sample periods where little information is available conversely as the sample period increased from 10 to 15 years the optimal normalized bandwidth reduced leading accordingly to a less wide bandwidth adjustment of the weight function however for locations characterized of strong but stable wind profile the increase of the sample period increased the bandwidth as a response of decaun to the inconsistent increase of extremes as the sample size increased for the most locations in general as the sample size increased the bandwidth estimates reduced as shown from the gaussian based correlation analysis of the acf estimator similarity in the irregular deca samples the reduced trend of the most likely optimal normalized bandwidth estimates to smaller values as the sample size increased is shown in fig 7 6 3 lag k estimates the required lag k apart value of observations is presented for the re sampling procedure at each location the desired lag value estimates of k will assign the statistically independent events from the optimum re sample of decaun at each sample period with an increasing sample period the lag k also increased for the majority of the examined locations illustrated in fig 8 in general for the sample period of 10 years the lower lag value estimate of k ensures a successful trade off between excluding events and loss of information thus in order to increase the number of events and consequently the associated inter sampling time intervals of the weights to be applied a lower lag k is selected conversely as the sample period increased from 10 to 15 years the irregular acf estimator was adjusted to a larger lag k as a consequence the acf estimator included a sufficient amount of independent events more efficiently at the larger sample period the response of decaun to the lag k estimates is also closely related to the dep estimates that form the samples of deca specifically for locations of strong winds and low volatility in extremes the demonstration of decaun in the increase of sample period resulted to higher dep and to relatively smaller lag k when little information of extremes is apparent at these locations the lower lag k is preferable to form clusters efficiently of larger length and consequently derive samples of higher dep to this effect an example is presented in the following table 2 for one case at the two sample periods of examination the effect of decaun to the mode estimates of the desired lag k is illustrated in fig 9 where generally an increment of the mode estimates is obtained as the sample size increases inference is also made illustrating decaun 2 to the most likely re sampling scheme from the demonstration of decaun at each sample period and region inference based on the lag value estimates of k at the associated dep level is also made for the number of the asymptotically independent events from decaun the number of events estimated respectively to the re sampling scheme at each sample period and region is illustrated in fig 10 under the decaun 2 scheme for the locations in the north sea the likely number of re sampled events is approximated at 3 8 year for the sample period of 10 years and at 3 9 year events for the 15 years respectively similarly for the locations in the atlantic the likely number of re sampled events under the decaun 2 scheme is approximated at 4 8 year for the 10 years and at 3 7 year for the 15 years finally for the locations in the mediterranean decaun 2 re sampled approximately a number of 5 4 year events for the 10 years and a number of 5 2 year for the 15 years respectively 6 4 the nrmse measure of decaun the estimation of the nrmse measure and its relation to the sample period resulted in something unsurprising for the majority of locations increase in the sample period increased the precision of nrmse to the quantile estimates of decaun to this effect the re sampled models and their fit to the gpd distribution are based on a larger amount of values containing more information about the high end tail distribution of the wind speed data therefore the quantile estimates of decaun to a gpd fit converged with the increase in sample period to the quantile estimates of the bm ref fit to the gev this is illustrated in fig 11 where the quantitative measure of performance of the optimum re sampled models from decaun generally reduced as the sample period increased the increase in the sample period does not reduce the nrmse measure for all locations this inverse proportional behavior of the measure is possibly explained by the weak performance of the reference model bm ref at the locations l1 l4 l13 l19 l22 and l24 the over under quantile estimates from the reference model using small samples such as the available sample of 20 years are also discussed in an and pandey 2007 ceppi et al 2008 and della marta et al 2009 however high wind speeds are frequently apparent at these locations as obtained in table a 1 2 in appendix to this effect the available samples will probably force the fit of the extremes to the gev to lie near the mode of the distribution and hence away from the tail area of interest hence the reference model described by the bm approach will probably not give the best fit to the tail of the distribution at these locations for samples of limited information in extremes the validity of the asymptotic forms is challenged to assess the fit of the reference model bm ref to the gev and the decaun model to the gpd the plots of q q p p and kernel density are presented in fig 12 for one case l13 if the empirical data align closely with the modeled estimates then it is likely that the chosen model for relatively small samples of wind speed is a good representation of the true extreme asymptotic form for these samples in addition the 95 confidence bands are also provided based on the kolmogorov smirnov statistic doksum and sievers 1976 6 5 design values design value dv estimations in m s for various return periods are presented for the two relatively small sample periods of examination 10 and 15 years henceforth all results will be referring to the selected locations l2 l16 and l21 alongside the d v estimations of the optimal re samples from the decaun model we include estimations from deca the performance of the runs is also presented as the standard comparable model at each sample period of examination moreover the bm model refers to sample periods of 10 and 15 years and the bm ref at the maximum available sample period 20 years regarding l2 d v estimates of the 50 years return period d v 50 for bm ref yielded 26 59 m s as the reference for both sample periods of examination fig 13 decaun provided d v 50 estimates at 26 84 m s with an nrmse measure at 0 013 with regards to the sample period of 10 years figs 13a and 26 67 m s followed by a reduction of the nrmse measure to 0 003 for the 15 years fig 13b the d v 50 for bm ref regarding l16 was estimated at 28 72 m s subsequently the estimations for l16 with regards to the d v 50 and n r m s e measure presented in fig 14 are outlined in the same context as previously decaun provided d v 50 estimates at 28 86 m s with an nrmse measure at 0 005 with regards to the sample period of 10 years figs 14a and 28 64 m s followed by a reduction of the nrmse measure to 0 003 for the 15 years fig 14b finally the d v 50 for bm ref regarding l21 was estimated at 25 79 m s in the same context as before we subsequently outlined the estimations for l21 with regards to the d v 50 and n r m s e measure decaun provided d v 50 estimates at 25 79 m s with an nrmse measure at 0 011 with regards to the sample period of 10 years fig 15 a and 25 86 m s followed by a reduction of the nrmse measure to 0 007 for the 15 years fig 15b as a general remark to the d v estimates from the decaun model the proposed model slightly overestimated the quantile prediction with regards to bm ref for small scale return periods of 50 years as obtained for all three locations this effect is more obvious considering the sample period of 10 years in figs 13a 14a and 15a on the contrary for the larger sample period of 15 years there is a remarkable convergence of decaun towards the bm ref model moreover runs failed to provide reliable d v estimates for the sample of 10 years yielding quantile overestimates in comparison to those made from the reference model finally we anticipated weak performance from the bm model for the two sample periods although the latter model was supplied as a weak comparable measure of prediction in line with the aforementioned remarks design value estimations d v 50 for the 30 locations are illustrated in fig 16 for inference of the wind speed projections of the decaun model additional diagnostics are presented in table a 2 3 and table a 2 4 in appendix 6 6 confidence bounds for the three locations l2 l16 and l21 and to each sample period of examination only the confidence bound of the estimated d v 50 of the 50 years return period for each model is illustrated the interval estimates are supported by two methods the first method is the normal approximation and is applied only to the bm ref model the second method is the standard non parametric bootstrap method percentile and is applied to the models bm runs deca and decaun the decaun model variability for l2 fig 17 a yielded a slight increment in the width of the 95 confidence interval width of ci by 0 044 m s with the increase of the sample period from 10 to 15 years on the contrary a considerable reduction in the width of ci by 5 509 m s and 1 451 m s is obtained for l16 fig 17b and l21 fig 17c respectively with regards to the decaun model the confidence bounds are considerably wider in comparison to deca for all three locations the estimations confirmed the wider bound effect that was expected despite the fact that a bootstrap approach was performed for a reliable inference in terms of variability it failed to overcome the weakness of small samples such as the irregular re samples from decaun the inevitable high variability of the proposed model is caused by the reduction in the sample size as a repercussion from re sampling at the associated dep levels moreover the increase of the sample period to 15 years positively affected deca decaun and runs yielding narrower confidence bounds finally bm confirmed the inability to provide useful bounds in comparison to the other models inference of the variability of design value estimates d v 50 of decaun with the increase of the sample period for the 30 locations is illustrated in fig 18 6 7 model parameters for easier representation of the modeling to the asymptotic distributions only the estimated shape parameter ξ ˆ is provided for each model at the three locations the 95 confidence intervals of the estimated parameters of the gev and gpd distributions at each sample period are derived by two methods as discussed previously in sub section 4 1 2 assuming that the shape parameter estimation with regards to the bm ref model ξ ˆ r e f has a reduced degree of uncertainty it will be considered as the reference estimation for the comparisons made in the parameter ξ ˆ for both sample periods of examination in general decaun estimates of ξ ˆ resulted a reduction of the 95 width of ci in the increase of the sample period from 10 to 15 years for the three locations l2 l16 and l21 illustrated in fig 19 the estimates of the shape parameter of decaun for the three locations ranged from 0 471 to 0 771 and even wider for the associated bounds however as the sample period increased we observed ξ ˆ estimates converging to the ξ ˆ r e f this convergence confirmed the unique relationship between the distributions of gev and gpd fitting successfully the samples of the reference and proposed model respectively in this implementation the residual resid ξ ˆ ξ ˆ r e f ξ ˆ is used as a visual metric estimate performance of decaun the ξ ˆ r e f parameter estimates to locations l2 l16 and l21 are at 0 168 0 473 and 0 170 respectively shown in fig 19a b and c at the latter three locations and for the sample period of 10 years the resid ξ ˆ of decaun is obtained at 0 303 0 017 and 0 601 with regards to the 15 years the resid estimate followed a reduction at 0 121 converging to ξ ˆ r e f increment at 0 242 diverging negatively from ξ ˆ r e f and reduction at 0 178 converging to ξ ˆ r e f for inference of the shape parameter estimates of decaun to the 30 locations fig 20 is given in general the resid ξ ˆ metric reduced to zero converging to ξ ˆ r e f as the sample period increased for the majority of the locations as a remark the parameter assessment from the proposed model derived almost weak asymptotically normal properties especially for the sample periods of 10 years i e l5 l7 l15 and l21 a possible explanation is given from the over underestimation of ξ ˆ under the standard mle method considering the relatively small number of observation at these locations in addition the inverse proportional behavior of the metric measure resid in the increase of sample period e g location l13 is also possible related to the intractable problems of inference for the ξ ˆ r e f parameter of gev under mle however for the relatively small samples using the lmom method according to hosking and wallis 1997 would probably model in a better way the statistical weakness of the small re samples obtained from decaun we close our results in the following sub section 6 8 with the threshold selection for the runs model at the three aforementioned locations 6 8 threshold diagnostics for the runs model the threshold selection for the runs model was derived from the nc diagnostics as discussed previously in sub section 4 2 the nc diagnostics are presented for the sample periods of 10 diagrams b f and j and 15 years diagrams d h and l with the standard parameter stability ps plot coles 2011 as a comparative measure diagrams a e and i for 10 years and diagrams c g and k for 15 years respectively in fig 21 for the diagnostics score test is performed for the shape parameter over multiple thresholds to the three locations the threshold range for the test was limited between the 60 and 99 5 sample quantile of the daily wind speed maxima with a step of 0 01 the empirical threshold selection is depicted as the value associated with the sharpest p value increase at the significance level of 0 05 for easier representation peak p values and threshold are located on the vertical dashed line on the diagram of the nc diagnostics furthermore threshold exceedances are also denoted on the top scale of the same diagram additional diagnostics of the runs model to the threshold from nc are summarized in table 3 considering the three locations the number of the cluster peak exceedances over threshold yielded in a range from 70 to 92 with regards to the sample period of 10 years and 90 to 121 for the 15 years respectively moreover the threshold depicted from the nc diagnostics is illustrated to the ps plot as a comparative measure in fig 21 the nc threshold obtained from the ps plot is located on the diagram as the solid dot line highlighting empirical estimation for the shape parameter with approximate pointwise wald 95 confidence intervals as vertical solid lines as a remark threshold inference of the multiple threshold model by the nc diagnostics confirmed the advantage against the standard ps plot the threshold from nc required less subjectivity and experience to detect 7 conclusions the present analysis assessed the effect of irregularity in samples to the design value estimates of wind speed under the asymptotic form of gpd summarizing the implementation of decaun we recall that the main idea was to re sample the irregular samples of deca to approximate i i d observations under the concept of de clustering the re sampling strategy used the slotting method employing a gaussian kernel weight function to the irregular acf estimator moreover this utilization accounted for eight dep reduction levels and the same number of bandwidths in this way the re samples encompassed as large a number as possible of discrete events balancing the over or under smooth effects on the weight function from this assessment decaun is regarded as an alternative de clustering approach for relatively small samples furthermore the high resolution database in this setting avoided the apparent underestimation of the extreme wind speed variability identifying the extreme characteristics primarily near the coasts and in narrow straits and basins the general findings are outlined as follows for small samples of wind speed e g 15 years the re samples of decaun modeled by the approximation to the gpd illustrated effective projections in terms of precision and variability specifically with regards to the 50 year design values and ξ ˆ estimates decaun yielded larger confidence bounds in comparison to the runs and deca models within the pot concept the samples of 10 and 15 years confirmed the inability of bm to provide useful bounds in comparison to the other models resulting in a weak comparable measure of prediction to these samples in addition decaun circumvented successfully the problems of irregular sampling modeling events that meet approximately the i i d requirements the most important findings considering the proposed model for the 30 locations can be summarized as follows the assessment yielded the most likely dep reduction level to range approximately from 0 68 to 0 85 with regards to the sample period of 10 years and from 0 63 to 0 74 for the sample of 15 years furthermore estimations in terms of the most likely optimal normalized bandwidths are summarized approximately to range from 0 203 to 0 267 for the sample of 10 years and from 0 126 to 0 25 for the sample of 15 years based on the most likely number of asymptotically independent events the approximated range is in line with the variability of extreme winds over the regional locations in this setting specifically over the locations in the north sea from 3 8 to 3 9 year the atlantic from 3 7 to 4 8 year and the mediterranean from 5 2 to 5 4 year additional issues related to this analysis still remain and require further assessment for example considering the relatively small sample periods of examination parameter estimations made by the lmom method is probably more suitable for these samples additionally the use of the parametric bootstrap approach davison and hinkley 1997 for the model validation of decaun will probably improve the uncertainty bound effect of the estimates this remark also follows the suggestion from kyselý 2008 for the inference based on the small to moderate sample sizes such as those from the decaun re samples however the main scope of this study is to assess the effect of the irregular deca samples into the irregular acf estimator and not the response of the proposed model to the parameter estimation methods a more comprehensive investigation with regards to the optimization of the parameter estimation and the sampling uncertainties must be undertaken before the proposed model is widely applied credit authorship contribution statement christos tsalis conceptualization formal analysis methodology software writing original draft platon patlakas data curation christos stathopoulos validation george kallos supervision resources declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to thank dr ioannis papastathopoulos and mr graeme auld from the department of mathematics at the university of edinburgh for their scientific advisory and proof reading and dr takvor soukissian from the hellenic centre for marine research for his thoughtful suggestions from discussions in addition mrs fani anagnostou for her assistance in computational issues and finally mr panagiotis triantafyllopoulos for his assistance in graphical issues the study was supported by the operational program human resources development education and lifelong learning and co financed by the european union european social fund and greek national funds mis 5007050 in http www edulll gr appendix a 1 locations and wind speed datasets used table a 1 1 locations of daily wind speed records used for this analysis table a 1 1 north sea atlantic ocean mediterranean sea location lat lon location lat lon location lat lon l1 55 7 n 7 4 e l11 50 45 n 1 55 w l21 40 8 n 5 5 e l2 52 05 n 2 15 e l12 50 05 n 4 25 w l22 35 5 n 26 4 e l3 51 65 n 3 45 e l13 42 85 n 9 95 w l23 33 9 n 29 9 e l4 54 8 n 1 35 e l14 53 25 n 10 25 w l24 43 4 n 15 4 e l5 57 15 n 3 1 e l15 49 65 n 6 55 w l25 36 5 n 3 5 w l6 58 4 n 10 3 e l16 57 25 n 7 65 w l26 37 9 n 3 1 e l7 51 65 n 1 35 e l17 52 75 n 9 75 w l27 43 3 n 7 5 e l8 57 95 n 3 1 e l18 50 65 n 0 95 e l28 42 2 n 11 4 e l9 56 2 n 4 15 e l19 54 85 n 8 95 w l29 40 5 n 12 2 e l10 55 75 n 2 25 e l20 53 35 n 4 85 w l30 42 2 n 17 9 e table a 1 2 descriptive statistics for the 30 locations of the datasets used from the marina platform database extending from 1996 to 2015 20 years table a 1 2 location min max median mean se mean ci mean0 95 var std dev coef var skewness kurtosis l1 0 25 965 7 691 8 058 0 009 0 017 12 992 3 604 0 447 0 454 3 113 l2 0 26 704 7 236 7 627 0 008 0 017 12 446 3 528 0 463 0 487 3 129 l3 0 26 436 6 925 7 327 0 008 0 017 12 579 3 547 0 484 0 570 3 210 l4 0 25 840 7 417 7 904 0 009 0 017 13 750 3 708 0 469 0 497 3 050 l5 0 28 389 7 831 8 300 0 009 0 018 14 535 3 812 0 459 0 499 3 110 l6 0 23 912 7 170 7 572 0 009 0 018 14 169 3 764 0 497 0 372 2 661 l7 0 24 480 6 810 7 173 0 008 0 016 11 068 3 327 0 464 0 527 3 119 l8 0 28 801 7 968 8 455 0 009 0 019 15 738 3 967 0 469 0 525 3 131 l9 0 26 495 7 714 8 153 0 009 0 017 13 583 3 685 0 452 0 489 3 111 l10 0 28 029 7 623 8 098 0 009 0 018 14 139 3 760 0 464 0 494 3 077 l11 0 24 330 6 912 7 303 0 009 0 017 13 128 3 623 0 496 0 546 3 162 l12 0 24 877 6 946 7 378 0 009 0 017 13 297 3 646 0 494 0 551 3 154 l13 0 25 098 7 556 7 903 0 009 0 018 14 682 3 832 0 485 0 304 2 513 l14 0 28 752 7 777 8 200 0 009 0 018 15 351 3 918 0 478 0 523 3 125 l15 0 27 063 7 544 8 015 0 009 0 017 13 568 3 684 0 460 0 539 3 183 l16 0 28 671 7 653 8 111 0 010 0 019 16 469 4 058 0 500 0 554 3 081 l17 0 26 142 6 508 7 025 0 009 0 017 13 602 3 688 0 525 0 706 3 400 l18 0 25 769 7 032 7 396 0 009 0 017 13 036 3 611 0 488 0 493 3 131 l19 0 28 851 7 427 7 898 0 010 0 019 15 906 3 988 0 505 0 543 3 041 l20 0 24 816 7 175 7 639 0 009 0 018 15 415 3 926 0 514 0 548 3 042 l21 0 25 901 5 950 6 582 0 009 0 019 15 637 3 954 0 601 0 911 3 642 l22 0 20 863 7 225 7 121 0 008 0 016 11 392 3 375 0 474 0 072 2 499 l23 0 20 874 5 758 5 822 0 006 0 012 7 112 2 667 0 458 0 584 3 733 l24 0 23 377 4 941 5 487 0 008 0 016 11 976 3 461 0 631 0 847 3 478 l25 0 20 752 4 191 5 194 0 009 0 018 15 145 3 892 0 749 0 855 2 951 l26 0 21 371 5 866 6 021 0 007 0 015 9 625 3 102 0 515 0 637 3 453 l27 0 23 511 4 054 5 323 0 010 0 019 16 441 4 055 0 762 1 131 3 665 l28 0 19 200 3 865 4 460 0 007 0 013 8 101 2 846 0 638 1 008 3 927 l29 0 21 085 4 382 5 040 0 008 0 015 9 881 3 143 0 624 0 976 3 800 l30 0 21 555 4 601 5 047 0 007 0 015 9 725 3 118 0 618 0 769 3 406 a 2 decaun model estimates additional characteristic estimates of decaun is presented for the 30 locations with regards to the sample period of 10 and 15 years specifically the lowest aic and mse statistic measures and the associated total number of peak exceedances fitting the gpd distribution table a 2 3 additional characteristic estimates of the decaun modelwith regards to the sample period of 10 years extending from 1996 to 2005 table a 2 3 location aic mse lowest no decaun cluster u m s no thres exceedances l1 aic 238 075 68 15 007 51 l2 aic 100 512 30 16 852 22 l3 mse 15 720 106 12 142 53 l4 aic 78 251 64 19 273 22 l5 mse 73 450 45 0 000 45 l6 aic 153 035 51 16 068 38 l7 mse 55 100 43 0 000 43 l8 mse 20 140 35 17 110 26 l9 mse 16 700 34 16 387 25 l10 mse 20 510 32 16 748 24 l11 mse 8 740 50 17 040 25 l12 aic 82 652 34 17 703 20 l13 aic 119 096 61 17 574 30 l14 aic 109 212 32 18 443 24 l15 aic 293 395 44 0 000 44 l16 mse 10 150 52 20 659 23 l17 mse 69 920 75 0 000 75 l18 aic 74 992 35 17 403 17 l19 mse 6 540 45 19 433 24 l20 mse 7 460 53 18 470 26 l21 mse 64 700 57 0 000 57 l22 mse 6 400 187 12 129 140 l23 aic 419 345 245 10 266 112 l24 aic 58 360 54 17 983 20 l25 mse 5 080 103 14 075 57 l26 aic 57 326 43 16 784 18 l27 aic 66 129 48 18 529 21 l28 aic 98 418 37 13 442 27 l29 mse 9 720 127 11 788 57 l30 mse 5 470 115 15 012 45 table a 2 4 additional characteristic estimates of the decaun model with regards to the sample period of 15 years extending from 1996 to 2010 table a 2 4 location aic mse lowest no decaun cluster u m s no thres exceedances l1 mse 10 510 71 18 202 34 l2 aic 129 915 66 18 164 33 l3 mse 13 990 227 13 678 103 l4 aic 87 682 104 20 024 29 l5 aic 111 829 51 19 113 25 l6 aic 243 521 130 16 252 62 l7 aic 102 150 52 17 252 26 l8 aic 114 597 53 19 787 27 l9 aic 124 461 57 18 653 30 l10 aic 185 238 56 16 411 39 l11 aic 86 283 50 19 114 25 l12 aic 105 115 77 19 207 30 l13 aic 251 810 130 17 057 65 l14 mse 18 980 180 14 318 90 l15 mse 10 580 50 18 029 37 l16 mse 17 910 47 18 325 35 l17 aic 94 653 51 19 767 25 l18 aic 120 108 60 17 974 30 l19 mse 5 890 54 19 811 27 l20 mse 7 540 140 17 681 64 l21 mse 9 420 200 16 997 90 l22 mse 7 010 127 12 928 95 l23 aic 120 056 61 14 013 33 l24 aic 88 248 77 17 816 30 l25 aic 77 223 76 16 975 29 l26 aic 63 457 67 17 625 25 l27 mse 3 380 62 18 703 27 l28 aic 160 306 91 13 474 45 l29 mse 11 410 303 10 902 148 l30 aic 1058 479 175 0 000 175 a 3 mann kendall test the mann kendall m k test is a non parametric test frequently used to detect the existence of monotonic upward or downward trends in samples of environmental data the test is based on the correlation between ranks of a sample and their order instead of the actual values of the sample the null hypothesis h 0 is that the data come from a population of independent and identically distributed variables an important advantage of the m k test is that it is distribution free in contrast for example to the regression slope test where the residuals are assumed to be normally distributed on the other hand the examined data should not be serially correlated in order for the estimated p values to be correct the m k test statistic is calculated as follows a 3 1 s k 1 n 1 j k 1 n s g n x j x k where a 3 2 s g n x 1 if x 0 0 if x 0 1 if x 0 the variance of s in the general case where ties are present is given as follows a 3 3 σ s 2 1 18 n n 1 2 n 5 q 1 q t q t q 1 2 t q 5 where n denotes the total number of observations from the sample q is the number of tied groups and t q is the number of observations in the q th tied group the final test statistic z results from the following transformation of s a 3 4 z s 1 σ s if s 0 0 if s 0 s 1 σ s if s 0 positive negative values of z suggest an upward downward trend as the sample size becomes larger the test statistic z follows approximately the gaussian distribution 
20840,the effect of extreme wind speeds in applications for design is of great interest in a variety of fields such as meteorology and coastal engineering in these fields a common problem is the scarcity of long datasets to overcome this limitation a common approach is to utilize the entire available dataset using the peak over threshold pot approach in small samples there may be a limited number of extremes and so re sampling is often beneficial however the re samples are often affected by dependency and the independence limitations are usually disregarded to alleviate this effect the deca uncorrelated decaun model is proposed taking into account the correlation effect when re sampling this model provides an improvement to the current physical de clustering algorithm deca by re sampling the samples of deca irregularly in time the methodology proposed in this assessment is illustrated using wind speed data from a high resolution database over the north sea the atlantic ocean and the mediterranean sea from this evaluation the decaun model is proposed as an alternative re sampling strategy for observations irregularly spaced in time keywords extreme value analysis deca model similarity de clustering pot threshold selection modelling irregular samples slotting autocorrelation non rectangular kernel irregular correlation estimator nomenclature abbreviation definition pot peaks over threshold approach gpd generalized pareto distribution gev generalized extreme value distribution i i d independent and identically distributed limitations acf sample autocorrelation function ssl standard storm length tawn 1988 ps parameter stability plot coles 2001 nc diagnostics threshold selection from the multiple threshold model proposed in northrop and coleman 2014 gxcf gaussian kernel smooth function for the slotting method bm annual block maximum sampling approach runs standard de clustering model smith and weissman 1994 deca the physical de clustering algorithm deca model decaun the proposed deca uncorrelated decaun model variable definition ν y the annual sample length of the hourly wind speed time series h standard width parameter of the gaussian weight function δ τ effective width of the gaussian kernel and set equal to the mean sampling time intervals c b d w h δ τ normalized bandwidth bdw of the gaussian kernel weight function lag k apart the required minimum number of k values between successive observations of the irregular deca sample u threshold for the modelling of the excesses ξ ˆ shape parameter estimates of the gev and gpd dep pre defined energy percentage level of energy reductions for cluster separation from deca resid ξ ˆ the residual metric resid ξ ˆ ξ ˆ r e f ξ ˆ m k test the non parametric rank based mann kendall test at a significance level of a 0 05 a 3 in appendix naoi the index of the north atlantic oscillation dv design values or return levels dv 50 d v estimates of the 50 years return period nrmse normalized root mean square error rp return period 1 introduction a crucial problem often arising in the assessment of climatological and hydrological extremes is the scarcity of long and complete time series regarding wind speed time records many authors have outlined a minimum annual length of 10 years to be regarded as the absolute minimum for a comprehensive extreme value analysis eva based on the annual block maximum bm sampling approach cook 1985 abild et al 1992 coles and walshaw 1994 additional studies on bm outline a minimum annual length of 13 years for an adequate fit to the generalized extreme value gev distribution brabson and palutikof 2000 however samples with a limited amount of extremes often fail to provide effective quantile estimates for large return periods when little information is available for analysis re sampling is often imperative the question arises as to how to successfully enlarge a sample of extremes when the independence assumption is violated one classical approach to this problem is to make use of more than one observation per year by using the r largest order statistics concept recent applications of this concept in wind speed time series are found in an and pandey 2007 other approaches use the entire time series and consider different ways to extract the values to be processed under the principles of the extreme value theory extremes in environmental series tend naturally to form clusters and consequently exhibit serial correlation the standard sampling approach under the independent and identically distributed i i d limitations should involve identifying independent clusters of extremes from which the cluster maxima is then typically extracted to give us a sample of approximately independent cluster maxima the latter strategy defines the de clustering concept in time series where applications can be found in ferro and segers 2003 however the asymptotic independence in cluster maxima is essential in eva but frequently disregarded the distribution and parameters of limited long range dependent extremes are strongly influenced by the dependence in the series leadbetter 1983 in reviewing the relevant literature the most popular de clustering strategies in stationary time series are block and runs method smith and weissman 1994 the improved method of independent storms mis harris 1999 the filtering de clustering approach fawcett and walshaw 2007 the standard storm length ssl tawn 1988 the separation time approach by walshaw 1994 morton et al 1997 and the physical de clustering algorithm deca model soukissian et al 2006 soukissian and kalantzi 2009 bernardara et al 2014 arns et al 2013 vanem 2011 oikonomou et al 2020 another sampling approach besides the classical bm is by modeling exceedances over a high enough threshold u at which the generalized pareto distribution gpd provides a valid approximation to the excess distribution the peaks over threshold pot approach davison and smith 1990 simiu and heckert 1996 naess 1998 harris 2005 caires and sterl 2005 is widely accepted as the standard approach for modeling cluster exceedances and the most suitable for small samples the major disadvantage of pot is related to the selection of the appropriate threshold value in a trade off between bias and variance traditionally the pearson correlation estimator is regarded as the standard approach for quantifying the dependence in de clustered observations however the validity of the pearson estimator may be questioned when the observations of these samples are irregularly spaced in time scargle 1989 zhang et al 2008 the accepted methods for processing irregular observations in terms of correlation analysis are sorted into three major classes i direct transform methods spectral analysis estimating the spectrum of the irregular sample by a generalization of the fourier transform lomp scargle periodogram he et al 2009 babu and stoica 2010 ii interpolation methods suggesting re sampling at uniform intervals schulz and stattegger 1997 kreindler and lumsden 2006 and finally iii slotting methods computing the correlation of irregularly sampled data to generalize the pearson correlation operator mayo 1993 this is achieved by calculating the products of observations according to their sampling time differences using the gaussian kernel as a weight function over the observations the technique was developed in fluid mechanics and applications can be found in tummers and passchier 1996 benedict et al 2000 and rehfeld et al 2011 the classical methods analyzing the correlation effect in samples irregularly spaced in time often yield poor performance in comparison to the non rectangular gaussian slotting method examples of other slotting approaches such as slot boundaries local normalization fuzzy slotting and variable windowing can be found in nobach 2002 or damaschke et al 2018 and references therein in this study a re sampling strategy is proposed for samples irregularly spaced in time the irregular samples are formed as the maxima of grouped observations the separation of groups under the cluster concept is derived from energy reductions between consecutive time steps in observations assigned hereafter as discrete energy reduction levels the proposed deca uncorrelated decaun model performs re sampling taking into account the correlation effect in the irregular samples of deca for a range of discrete energy reduction levels this effect is evaluated using the gaussian kernel weight function for the computation of the generalized pearson correlation operator from the slotting method the performance of the gaussian kernel used is assessed over a range of smoothing factors bandwidths enhancing the performance of the weight function over the irregular sample of observations the functionality of the proposed model requires two subjective admissions these admissions encountered the main objectives of our evaluation which include the favorably bound estimation of the energy reduction level the optimal bandwidth smoothing response of the weight function to the irregularly spaced observations in time taking advantage of these arguments a dependent sample of extremes is reconstructed successfully by decaun to an asymptotically independent re sample converging to the i i d limitations the performance of the decaun model in applications where there is a limited availability of data is one key motivation of this work therefore the assessment is carried out for relatively small datasets of annual wind speed time series corresponding to two sample periods of 10 and 15 years the quantile estimations of the proposed model are evaluated based on estimates from the reference bm approach bm ref with a block size of one year and assuming the available records are of at least 20 years palutikof et al 1999 the second key motivation of this analysis is the performance of the proposed model with regards to the return levels design values and return periods of wind speed for locations over the north sea the atlantic ocean and the mediterranean sea with respect to this evaluation decaun is demonstrated to be a viable alternative model for extreme wind speed projections considering relatively small sample periods of data however the proposed model was confounded by large variability following the reduction in sample size from the re sampling process the nomenclature denotes all symbols and abbreviations frequently used in this analysis this paper is organized as follows in section 2 we commence with the methodology and section 3 the diagnostics for the henceforth statistical analysis emphasizing to the irregular modeling of the proposed re sampling procedure in section 4 all models used in this setting and their characteristics are analytically presented we proceed in section 5 with the wind characteristics of the study area in short and the associated data used furthermore in section 6 all results in terms of the return levels confidence bounds and the shape parameter estimates of the model fit are illustrated and discussed finally in section 7 we summarize the basic findings of this assessment 2 methodology the present univariate analysis aims to assess the effect of the asymptotic distributional behavior of two types of extreme wind speed sampling data techniques the first type of sampling data will be the classical bm which is extracted from blocks of observations of annual length subsets and modeled by the gev family of distributions the second type models the peaks in samples of wind speed exceeding a high enough threshold u by the approximation to the gpd within the pot framework the latter two distributions in extremes are proven to be the limiting distribution of the annual maxima and the peak exceedances respectively for inference in environmental studies nikulin et al 2011 papalexiou and koutsoyiannis 2013 in time series under dependence the de clustering concept is regulated from excluding successive events in order to succeed asymptotic independence however the events that form the de clustered sample must be controlled in a way to avoid the statistical bias effect in the results the focus on developing approaches that aim to enlarge the sample of extremes beyond the standard annual maxima often disregards the effect of forming samples that are irregularly spaced in time the irregular sample of n observations considers the successive differences δ t t i 1 t i at the corresponding observation times t 1 t 2 t i t n in the sample not necessarily equal samples of irregular observations are proven to be sensitive to standard correlation analysis vio et al 2000 broersen 2009 mondal and percival 2010 to handle these important features of irregularity in the correlation analysis the slotting method is first employed to the samples of deca followed by a re sampling strategy from the decaun model to these samples in this way the samples of irregularly dependent observations in time are reconstructed efficiently under independence the latter reconstruction is the main goal of the present work all results are evaluated systematically alongside the standard runs method of de clustering where hereafter denoted as the runs model which is assigned as the standard comparable model in this assessment before proceeding thoroughly to the diagnostics of the methodology in this setting the following should be outlined the implementation of the runs model for identifying clusters uses a a threshold value from the multiple threshold model northrop and coleman 2014 commonly referred to as the nc diagnostics and b the extremal index estimator based on the intervals method proposed in ferro and segers 2003 the decaun model will be implemented using a a gaussian kernel smooth function gxcf rehfeld and kurths 2014 for the slotting method over the lagged products of the irregular deca observations and b a range of discrete energy reduction levels and bandwidths for the correlation analysis of the unequally spaced samples of deca in time the extreme value model parameter estimation of the gev and gpd distributions will be based on the maximum likelihood estimator mle the statistical software package extremes in r gilleland and katz 2016 is used for the estimation of the associated parameters 3 diagnostics in the following sub sections the standard correlation estimator which disregards the irregular sampling scheme effect is briefly presented before introducing the slotting method as the correct method 3 1 standard correlation estimator autocorrelation or lagged correlation refers to the correlation among members of a series of numbers arranged in time a random process is considered statistically stationary if the first and second moments are time invariant covariance stationary and if the sample moments converge in probability to the population moments ergodic under this assumption from a sample of n number of observations regularly spaced at times t 1 t 2 t i t n the correlation between observations that are separated by a lag number of k equally distant time steps δ t δ t t i 1 t i is defined as the sample autocorrelation function acf at each time lag k 1 ρ x k c o v k σ x 2 1 n k t 1 n k x t x x t k x 1 n t 1 n x t x 2 for any positive integer k n here c o v k is the sample autocovariance at time lag k for the regularly sampled process in time where x and σ x 2 is the sample mean and variance respectively the sample acf takes both positive and negative values 1 ρ x k 1 while the associated plot is referred to as correlogram the generalized bartlett s formula for acf bartlett 1946 yields the following standard error 2 s e ρ x 1 2 i 1 k 1 ρ x i 2 n for the associated 95 confidence interval the confidence bounds of the correlogram effectively test the null hypothesis of convergence to an uncorrelated approximation by that particular lag number of k time steps if ρ x k is located outside the bounds it means the preceding autocorrelations have not been successfully reduced to close to zero the desired lag number of k or lag k is set as the first time lag entering the confidence bounds for samples irregularly spaced in time the inter sampling times vary and the standard acf described in eq 1 cannot be directly applied chatfield 1996 an alternative to this restriction is the generalization of the regular correlation operator through a rectangular or non rectangular kernel function which this approach is commonly called the slotting or slotting autocorrelation method 3 2 slotting method to analyze the unevenly spaced time series edelson and krolik 1988 and mayo 1993 considered estimating the pair products of all available observations and discretizing them into bins according to their sampling time differences in this study the estimator of acf for the irregularly n number of observations is derived as the weighted mean over the available products according to their sampling time differences and desired time lag for which the correlation is estimated replacing the sample autocovariance divided by the sample variance of the irregular process for zero time lag and is presented as 3 ρ x k c o v k σ x 2 i 1 n j 1 n x i x j b h k t j t i h i 1 n j 1 n b h k t j t i h i 1 n j 1 n x i x j b h 0 t j t i h i 1 n j 1 n b h 0 t j t i h the observations x with indexes i j in eq 3 hereafter are considered as centralized and standardized values in addition the associated discrete time variables for these observations are re scaled as a transformation from the observed t o b s to the t t o b s δ t for each index i j respectively where δ t 1 n i 1 n 1 t i 1 t i is the mean sampling time interval accordingly the lag number of k is also dimensionless considering the re scaled k lag k δ t for which the acf is estimated the weighting function b h used in eq 3 is generally referred to as the kernel estimator for the product inter sampling time intervals t j t i the estimator b h was introduced by de oliveira 1963 and nadaraya 1964 and is stated 4 b h k t j t i h 1 h k k t j t i h where k is the kernel that determines the shape of the weighting function rectangular triangle gaussian etc which is symmetrically placed around t j t i and positive on the interval t j t i h t j t i h the parameter h is defined as the bandwidth or smoothing parameter which determines the width of the weighting function and adjusts the amount of smoothing applied over the inter sampling time intervals hence the kernel estimate of the unknown density f k is approximated by 5 f ˆ h k 1 n 2 i 1 n j 1 n b h k t j t i h where n 2 is the number of the weights used for the inter sampling time intervals moreover isolated peak estimates are derived for h 0 where h 0 the kronecker delta function denoted as δ is the approximation to the asymptotic limit lim h 0 k k t j t i h δ k t j t i the distribution of the kernel estimator and bandwidth selection are the subjective requirements to optimize the properties of the estimator wand and jones 1994 the most popular kernel distribution is in practice the gaussian due to its analytical properties however the optimum bandwidth is not guided by mathematical considerations babu and feigelson 1996 hall et al 2004 there is a vast amount of literature suggesting practical methods for optimal bandwidth estimation derived by the minimization of the distance between the unknown density function f k and the estimator f ˆ h k the major drawback of this measure of distance is over and under smooth density estimates jones et al 1996 simonoff 1996 therefore for the weighting function as defined in eq 4 we considered the usual gaussian distribution as the kernel function and for the bandwidth selection we did not restrict our analysis to a single optima in this setting the performance of the irregular acf estimator as stated previously in eq 3 is evaluated against a range of possible bandwidths recommended by sheather 2004 3 3 non rectangular kernel the slotting method from edelson and krolik 1988 used a rectangular kernel to select the products whose time lag is not further than half the bin width from the time lag k i e 6 b h k t j t i h 1 for t j t i k 1 2 0 otherwise where the lag bin width in their study is set equal to the mean sampling time interval in this study the slotting method uses a non rectangular kernel function gaussian kernel to weight the product inter sampling time intervals the advantages of this function include primarily the increase of the pair products to be averaged into the irregular acf estimator therefore a sudden cutoff in the time domain is prevented weighting the products smoothly according to the difference between the product inter sampling time interval t j t i and the considered time lag k the gaussian kernel density function tends to zero for time differences much larger or smaller than the considered time lag k hall et al 1994 bjornstad and falck 2001 and defined as follows 7 b h k t j t i h 1 h 2 π e x p k t j t i 2 2 h 2 and h c δ τ however there is no theoretical functional of the effective width of the weight function the gaussian kernel used in this study considers the standard width parameter h to be scaled to the mean sampling time intervals i e δ τ δ t 1 n i 1 n 1 t i 1 t i to adjust the effective width of the weight function satisfactorily to the mean width of the time intervals this way it is ensured that observations appearing at an almost constant frequency are rated higher than infrequent observations for brevity parameter c in eq 7 is defined as the normalized bandwidth to increase the precision of the analysis the degree of smoothing is of great importance the selection of a large bandwidth will result in an over smoothed performance of the density function while a small value will under smooth the kernel estimator examinations on asian monsoon records from rehfeld et al 2011 revealed empirical normalized bandwidth at c 0 25 for the analysis undertaken in this study we considered the range of c 0 125 1 with a 0 125 step to optimize the irregular acf estimator in eq 3 as defined in the slotting method in the following sub sections the deca and decaun models will be described in detail 4 models the present univariate analysis discusses the challenges in assessing the effects of the asymptotic distributional behavior of two types of wind speed samples in extremes the first type is the bm modeled by the gev family of distributions and the second sample forming the pot exceedances is modeled by the approximation to the gpd for extreme value analysis the asymptotic forms of the gev and gpd are the most widely used statistical distributions for describing extremes of wind speed beirlant et al 2004 and holmes 2015 under the knowledge of the possible intractable problems of inference related to small sample sizes the gpd for a fixed threshold u was chosen as the asymptotic form of the excesses in this setting specifically when the analysis is restricted to a set of cluster peak exceedances the use of a gpd distribution with an upper bound to the modeling of extremes of wind speed requires caution in applications fawcett and walshaw 2007 it is shown that the parameter inconsistency of the mle estimator to small samples affects the return level estimates of extremes however when wind speed is characterized of being heavy right tailed and extremes are typically modeled from short tailed distributions with finite right endpoint the asymptotic forms of gev and gpd can be a reasonable assumption for modelling recognizing the possible bias effect to the estimates fawcett and walshaw 2006a 2006b ashkar and tatsambon 2007 pinheiro and ferrari 2015 before proceeding to our analysis a summary table 1 is presented with the basic characteristics of the models used and a short introduction on the basic properties of the asymptotic forms in extremes 4 1 model properties 4 1 1 bm and pot exceedances the fisher tippett gnedenko theorem fisher and tippett 1928 and gnedenko 1943 implies convergence of appropriately normalized infinite number of block maxima to one of three types of extreme value distributions known as the fisher tippet type i ft i or gumbel class of distributions type ii ft ii or fréchet class and type iii ft iii or reverse weibull class these three classes of distribution can be combined into a single form for modeling the distribution of maxima given from the generalized extreme value gev distribution such that 8 g x μ σ ξ exp 1 ξ x μ σ 1 ξ ξ 0 exp exp x μ σ ξ 0 where the corresponding location μ scale σ and shape ξ parameters are defined for μ σ 0 ξ respectively specifically the support of extreme values from the gev is bounded from the right hand side when x μ σ ξ and ξ 0 otherwise unbounded on the right if μ σ ξ x and ξ 0 however when ξ 0 and x the subset of the gev leads to the gumbel class of distributions assuming that the conditions of the fisher tippett gnedenko theorem hold pickands 1975 and balkema and de haan 1974 showed that the distribution function of the excess y x u x u over a high enough threshold u can be approximated through the gpd that is defined as follows 9 g y σ ξ 1 1 ξ y σ 1 ξ ξ 0 1 exp y σ ξ 0 where the scale parameter is defined as σ ξ u μ and supported for σ 0 while the shape parameter is supported for ξ and is common to gpd and gev the gpd distribution is generalized because it assumes different distribution classes in the same sense as gev specifically when ξ 0 and 0 y σ ξ the gpd corresponds to the asymptote class of distributions with a finite right endpoint and is considered bounded on the contrary if ξ 0 and y 0 the gpd is considered unbounded or heavy tailed with an infinite right endpoint finally the case where ξ 0 and y 0 is interpreted as the limiting case for gpd converging to the exponential class of distributions summarizing the above if bm is approximated by the gev distribution then the threshold excesses have a corresponding approximate distribution within the gpd family and vice versa under the hypothesis that the gev distribution with estimated parameters μ ˆ σ ˆ ξ ˆ is a suitable model the level x p for p 0 1 satisfying g x p 1 p is defined by the quantile function related to the return levels or design values against the return periods and given by the following equation 10 x p μ ˆ σ ˆ ξ ˆ μ ˆ σ ˆ ξ ˆ 1 log 1 p ξ ˆ ξ ˆ 0 μ ˆ σ ˆ log log 1 p ξ ˆ 0 where the value x p is assigned as the return level or design value with return period r p 1 p similarly to the bm case the return level x p that is exceeded on average once every 1 p observations for the model of the pot exceedances assuming that x p u over a suitable high threshold u is given by 11 x p σ ˆ ξ ˆ ζ ˆ u u σ ˆ ξ ˆ ζ ˆ u p ξ ˆ 1 ξ ˆ 0 u σ ˆ log ζ ˆ u p ξ ˆ 0 where the estimated parameters σ ˆ ξ ˆ correspond to the gpd family moreover coles 2001 suggested that the probability of exceedance of u denoted as ζ u pr x u follows the binomial distribution bin n ζ u where n is the total number of observations from the time series hence a natural estimator of ζ u is ζ ˆ u n u n where n u denotes the number of exceedances 4 1 2 parameter estimation and uncertainty several numerical methods for the estimation of the associated parameters of the gev and gpd distributions have been proposed e g from naess and clausen 2001 bermudez and kotz 2010b outlining the pickands estimator probability weighted moments pwm ordinary moments mom and l moments lmom however only the mle method is theoretically effective and provides approximate normal distributions and approximate sample variances that can be used to formulate confidence bounds for inference for this analysis the mle method was implemented setting the discussion within the most popular framework for stationary bm and pot samples this way all results are easily comparable with those reported in the relevant literature the regularity conditions of the gev and gpd exist when the shape parameter ξ which is equal for the two distributions is restricted specifically the mle is valid when ξ 1 although the asymptotically normal properties of the parameters are valid for ξ 0 5 when ξ 1 the estimators generally do not exist davison and smith 1990 grimshaw 1993 tajvidi 2003 in practice for the modeling of extremes of wind speed it is likely to obtain more often zero and negative estimates i e indicating right tail distributions of an exponential type or short and light tailed respectively with an infinite or finite right endpoint rather than positive value estimates of the shape parameter jonathan and ewans 2013 brabson and palutikof 2000 with regards to the reference bm ref model fitting the gev using the sample of 20 years the estimated 95 confidence intervals of the associated extreme value model parameters are derived by the normal approximation to the distribution of the mle estimator coles 2001 however the interval estimates of the parameters from the bm model fitting the gev using samples of 10 and 15 years is supported by the standard non parametric bootstrap method percentile efron and tibshirani 1994 the preceding bootstrap method is also employed to runs deca and decaun for the confidence bound estimates of the parameters fitting the gpd the latter method is considered for the samples of 10 and 15 years respectively to these models it should be pointed out that the use of percentile bootstrap for deriving confidence bounds is arguably the simplest and most intuitive bootstrap interval approach this approach is found in various environmental applications davison and hinkley 1997 ferro et al 2005 kharin and zwiers 2005 the non parametric approach performs sample replications without any prior knowledge of the distribution of the sampling data the 95 confidence intervals of the examined statistics are estimated from a large number of repeated samples in the present bootstrap application 999 bootstrap replicates are generated from resampling with replacements from the given sample furthermore the so called alpha level was selected to a 0 05 providing the apparent proportion of samples for which the 95 confidence intervals would fail to cover the examined statistics 4 2 standard runs model a rational method for identifying clusters of extremes in time series was presented in smith and weissman 1994 and employed to the pot exceedances this was achieved by introducing the idea of the runs length of r observations that fall below a high enough threshold u the required r number of observations is closely related to the extremal index θ nandagopalan 1990 the latter index describes the clustering tendency of the data and is equivalent to the reciprocal of the limiting mean cluster size supported in the range 0 θ 1 before applying the runs model in this setting for the identification of clusters the extremal index should be estimated in order to specify the optimal runs length parameter of the given time series the runs model used relies on the intervals estimate ferro and segers 2003 for the associated θ u providing an optimal threshold u from the multiple threshold model by the nc diagnostics northrop and coleman 2014 the nc diagnostics fit piecewise equality of the shape parameter using score and likelihood ratio tests wadsworth and tawn 2012 ideally the p values as an output of the tests should increase as the threshold increases the threshold is set at the point where approximate stabilization is achieved the null hypothesis should be not only rejected at that point but also at all other higher thresholds however an informal approach is also considered by locating the threshold at the sharpest p value increase this threshold should impose a separation of at least 50 events in total jonathan and ewans 2013 and generate 5 to 10 events year as reported for example in mazas and hamm 2011 the multiple threshold model has an advantage over other threshold models as it requires less subjectivity and experience to detect an appropriate threshold scarrott and macdonald 2012 bommier 2014 4 3 deca model the cluster formation proposed by soukissian et al 2006 aims at separating approximately independent events considering physical limitations to identify the successive groups of observations the events are defined as a continuous physical phenomenon of the environmental variable the model separates all events by looking at energy reductions in consecutive time steps initially the available time series is filtered twice using a simple monotonicity criterion obtaining a series of local maxima successive points with the exact same value are removed under the monotonicity criterion from the latter series the local maxima and minima are identified considering hereafter the third power of their corresponding values therefore numerical differences of cubic local maxima to the next consecutive cubic local minima are identified selecting a pre defined energy percentage dep with respect to cubic local max values the method introduces wind energy percentage reductions wind climatology studies have established that the wind power potential available in a flow of air per unit cross sectioned area normal to the flow is proportional to the third power of wind speed reed 1974 12 p w i n d 1 2 ρ v 3 where ρ is denoted as the air density and v is the wind speed the time indexes of local minima that correspond to the identified energy reductions are referred to as transition points as a result the series of transition points samples the initial time series into successive and approximately independent wind state clusters of generally unequal length then by selecting the maximum value of each previous cluster a sample of approximately irregular maxima randomly spaced in the time axis is derived even if each event can be associated with a particular instant of occurrence on the time axis the derived sample does not depend on time anymore the optimum dep level should generally be high enough in order to safely separate the approximately independent events but it should not be extremely high in order to avoid unnecessary concatenation of clusters and thus loss of data information at this point it should be mentioned that the initial de clustering approach of soukissian et al 2006 avoided assumptions regarding the irregular effect of sampling at any dep level furthermore deca presumed that the statistical threshold to the gpd model fit should be provided hierarchically and set to the modal value without further investigation taking into account the re scaled series of local wind energy for this analysis we considered eight levels sampling the series of transition points the dep levels are set constant at values 60 65 70 75 80 85 90 and 95 with the time indexes of local minima initiating the successive irregular clusters to these levels 4 4 decaun model in this section the re sampling schemes are defined in addition the irregularly modeling procedure of decaun is presented in steps considering all dep and bandwidth values 4 4 1 re sampling schemes the samples of deca at the associated dep levels required further investigation in terms of correlation re sampling is advised when the condition of independence is violated miquel 1984 lang et al 1999 in this assessment two re sampling scheme strategies for the samples of deca are proposed and denoted as decaun 1 and decaun 2 respectively the re sampling scheme decaun 1 is formed as follows i selecting the maximum value of the corresponding deca sample ii identifying and selecting the following lag k apart values of the remaining data from both sides of the value chosen in i until all available values are considered the re sampled decaun 2 is closely related to the concept of the ssl soares and scotto 2004 the decaun 2 scheme consists of the following steps i identifying and selecting the largest value of the correspondent deca sample ii discarding values with a lag k apart from both sides of the value chosen in i iii selecting the next largest value of the remaining data and finally iv repeat steps ii and iii until all data are used at this point the lag k apart value for re sampling is difficult to estimate it is the required minimum value between successive irregular maxima of deca clusters which renders the maxima statistically independent the desired lag k is obtained from the estimator algorithm of acf for different time lags the latter algorithm is defined as similarity providing estimates for the irregular acf in eq 3 using the non rectangular gaussian kernel gxcf in eq 7 in this way the products x i x j of the irregular acf estimator are weighted according to their difference between the product inter sampling time interval t j t i and the associated time lag k for the samples of deca the desired lag k apart value is derived as a time lag transformation from the correlogram it is set as the value of k observations obtained from the transformation of the first time lag entering the confidence bounds described by the generalized bartlett s formula in sub section 3 1 the acf estimator algorithm similarity is utilized in sample periods of 10 and 15 years the available software analyzing irregular samples in time and the correlogram at given time lags can be found in the nestoolbox https tocsy pik potsdam de nest php cited in rehfeld and kurths 2014 an example of the re sampling scheme decaun 2 described in this section is illustrated in fig 1 4 4 2 modeling irregularly in time the irregularly modeling procedure was carried out for eight dep levels at values 60 65 70 75 80 85 90 and 95 deriving an equal number of deca samples furthermore each deca sample was modeled at eight normalized bandwidths c 0 125 1 with a 0 125 step deriving an equal number of similarity estimates this way all deca samples are re sampled to decaun 1 and decaun 2 as previously described the associated lag k apart value for re sampling will be derived from the correlogram of acf using the estimator algorithm similarity for all bandwidth considerations step 1 the re samples of decaun 1 and decaun 2 are effective only if the common assumption of stationarity is not violated for this reason the non parametric rank based mann kendall m k test at a significance level of a 0 05 was implemented see also a 3 the aim of the test was to ensure the absence of a monotonic upward or downward trend of the examined re samples the null hypothesis of the test is h 0 no monotonic trend is present against the alternative h there is a monotonic trend present the test is used and discussed in the context of eva in cheng et al 2014 it should be noted that the application of the m k tests to the re samples is a fundamental step as the presence of possible temporal dependence and monotonic trends can affect and bias the gpd model fit which relies on the hypothesis of independent observations step 2 the re samples of decaun at each dep level are subjected to a statistical gpd model fit however the statistical threshold considerations for the model fit will be set within a range of values u 0 first quartile mean mode and median obtained from the re samples respectively the optimum re samples are assigned in terms of the lowest aic and the lowest statistic mse under the statistical threshold considerations step 3 the decaun re samples derived from step 2 are considered optimum for the statistical gpd model fit all previous steps 1 and 2 are repeated over all normalized bandwidth considerations at this point we note that the quantitative comparison of the decaun re sampled model fit was not based on the standard aic and mse criteria thus neither common criteria for model selection nor goodness of fit tests are appropriate for evaluating the quality of the model fit therefore we considered a metric guide rule in the least square sense to measure the goodness of fit each optimum model fit will be counted upon a relative measure of performance based on the estimated design values d v per sample period n y denoted as d v o p t d e p u c n y the relativity of the metric is gauged by the associated estimates from the bm approach within the largest annual available time series d v max the quantitative measure is defined as a normalized root mean square error n r m s e based on the modal position of hyndman and fan 1996 13 n r m s e n y 1 n t r p t d v r p max d v o p t r p d e p u c n y d v r p max 2 where t 2 10 20 90 and 100 indicating the return periods r p of length n t 11 and ν y 10 15 years denoting the two sample periods of examination the precision of the n r m s e measure will be counted upon the d v max estimates of the reference model bm ref using the largest available sample of 20 years 1996 2015 in this implementation the optimum normalized bandwidth of c 0 125 1 with a 0 125 step is the value that minimizes the nrmse estimates of the re samples in step 2 at this optimum bandwidth selection the optimum dep level is also derived and the optimum re sampling scheme of decaun this empirical procedure applied for the selection of the optimum normalized bandwidth to the weighting function is a result of no theoretical rule of the effective bandwidth the aforementioned steps 1 2 and 3 are respectively applied to the two sample periods of examination for a comprehensive overview of the irregularly modeling process a flow diagram is illustrated in fig 2 summarizing in brief the key steps followed in this setting from the time series to the re sampling scheme 5 study area and dataset our examination is focused on different offshore regions in europe with a special focus on the north sea the european coastline that is exposed to the atlantic ocean and finally the mediterranean the considered locations in these regions are of high interest in terms of wind energy and offshore activities their characteristics vary and are highly affected by the different climatological patterns for each region specifically the wind conditions in the north sea are driven by the passage of cyclonic systems such as extra tropical cyclones influenced by the inflow of oceanic water from the atlantic ocean this combined with the north sea s shallow water basins result in a remarkable offshore wind profile sušelj et al 2010 the west european offshore locations exposed to the atlantic ocean are affected by the extra or post tropical cyclones that are generated along with the polar and the arctic front respectively dodet et al 2010 finally the mediterranean sea is a semi enclosed basin surrounded by complex mountainous terrain and is divided into several sub basins with contradistinctive characteristics in addition the mid latitude cyclone passage results in complicated wind patterns with extreme winds a detailed description of the main mediterranean winds is provided by zecchetto and cappa 2001 and the references therein to gain more insight about the behavior of the extreme fluctuation of wind speed at these regions a high resolution data product is considered for a reliable statistical analysis especially near the offshore regions of europe where the demonstration of decaun is challenged from the highly dependent regional effects surface roughness landmass etc the time series used are for a period of 20 years from 1996 to 2015 they have been extracted from the marina platform database created within the framework of the homonymous project 2014 the dataset was produced as an outcome of atmospheric modeling hindcast simulations providing information for the entire european coastline with an hourly time frequency and a spatial resolution of 5 km the atmospheric model used is skiron kallos et al 1997 the outcome has been evaluated within the framework of the marina platform project see http forecast uoa gr oldproj php in the present study the wind components of the model are obtained at 10m above sea level for the 30 in total selected locations denoted as l1 l2 l29 and l30 with their descriptive statistics presented in detail in appendix table a 1 1 and table a 1 2 specifically the extremity of wind speed is inferred from the systematically positive values of the skewness and kurtosis statistical estimate to all locations indicating heavy tailed distributions of elongated right tail for the wind speed see the discussion in kalogeri et al 2017 the ability of the high resolution database to reproduce more intense extremes particularly at the european offshore regions will strengthen the re sampling strategy of decaun to short samples at these locations all models however are applied to the wind speed datasets at locations respectively shown in fig 3 6 results and discussion in the following sub sections the decaun model estimates from the statistical analysis are presented to evaluate the model behavior for the selected 30 locations under asymptotic independence the proposed model reconstructed a dependent sample of observations that are irregularly spaced in time to motivate decaun from this study the effect of the model to the correlation analysis is presented and discussed to illustrate the implications of the re sampling procedure in addition the number of observations from the re samples in this setting is in agreement with the extreme wind variability at these sites of interest all re samples from the decaun model assume that an average of at least 1 65 peaks per year should be selected in a pot approach in order to gain an advantage over block maxima cunnane 1973 tanaka and takara 2002 serinaldi and kilsby 2014 the n r m s e measure of the re samples stated in equation 13 is evaluated for all dep levels normalized bandwidths c and statistical threshold considerations in addition the sensitivity of inference to the measure of performance is obtained using samples of 10 and 15 years regarding sample periods from 1996 to 2005 and 1996 2010 respectively furthermore the quantile estimates from decaun are compared to the estimates derived by the reference model bm ref employing to the latter the maximum available series extending from 1996 to 2015 20 years estimations in terms of the optimum dep levels of deca will be illustrated at each sample period for the 30 selected locations in sub section 6 1 for inference of the gaussian kernel estimator to the optimum re sampling scheme used at these locations the normalized bandwidth c and the desired lag k apart estimates are included in sub sections 6 2 6 3 moreover the associated n r m s e measure of performance is also presented in sub section 6 4 for the optimum re sample of the decaun model at each location furthermore one representative location from each region locations l2 l16 and l21 is selected for the illustration of decaun in terms of the return level estimates and variability of the proposed model to each sample period specifically the three representative locations of the strong wind conditions observed within each area are located in the southern part of the north sea l2 with highest wind speed 26 704 m s in table a 1 2 the northern part of the atlantic l16 with 28 671 m s the highest wind speed and the western part of the mediterranean l21 with 25 901 m s the highest wind speed respectively in addition the shape parameter estimates from all models are presented for these three locations the foregoing estimations are illustrated in sub sections 6 5 6 7 respectively 6 1 dep level estimates beginning with the estimated dep level of energy reductions for the physical de clustering approach from deca the results are presented in fig 4 by means of colored dots assigned to the eight levels 60 90 and 95 with a 5 step the dep level estimates per sample period resulted in a safe range for deca to cluster events without the loss of valuable information in general the increase in sample size leads to reductions in the dep level for the majority of the locations specifically as the sample period increased from 10 to 15 years the modal value estimate of dep was slightly reduced for the 10 locations l1 l2 l9 and l10 in the north sea additionally reductions of the mode estimate of the dep level is obtained for the 10 locations in the atlantic l11 l12 l19 and l20 and the 10 locations l21 l22 l29 and l30 in the mediterranean respectively the increase in sample size does not necessarily follow the large increase in the number of extreme events as expected the approximate stable or reduced dep level estimates as the sample size increased controlled the deca model to encompass the largest number of events as possible this effect is illustrated as follows the response of decaun to the modal value estimates of the dep levels is illustrated in fig 5 at the 30 regional locations of interest inference is made for the number of observations of the samples of deca that are irregularly spaced in time and most likely to be sampled from these locations the moderate to low mode value estimates of the dep levels for the 10 locations in the north sea 0 68 for the 10 years and 0 66 for the 15 years are in line with the variability of extreme winds over this region one key point of this agreement is the influence described by the large scale circulation patterns such as the north atlantic oscillation nao on the wind speed over the north sea the index of the north atlantic oscillation naoi is strongly correlated with the wind speed over northern europe having an impact on the cyclones generating in this area by shifting the westerly zonal flow however an increase in the naoi and the mean wind speeds from the 1960s to the mid 1990s does not necessarily increase the extreme wind profile sušelj et al 2010 thus the dep estimates for these locations are controlled by deca to a moderate to low level in order to enlarge as much as possible the number of extreme wind storm events considering the atlantic where strong extra tropical storms can cause massive storm surges affecting the 10 locations there the moderate to high mode value estimates of dep 0 83 for the 10 years and 0 74 for the 15 years is somewhat reasonable it is well known that the extra tropical cyclones travel eastward along the polar jet stream e g the icelandic low and azores high thus the prevailing westerlies affecting the locations l16 l19 l14 l17 and l20 in the north atlantic feser et al 2015 justify the moderate to high dep estimates from a large number of extreme wind storm events obtained over these sites on the other hand locations l15 l12 l11 and l18 in the central and location l13 in the south atlantic are also influenced by the strong pressure centers over the atlantic ocean specifically the number of atmospheric circulation patterns that govern the extreme wind speed variability at these locations are influenced by the atmospheric dynamics in the north atlantic as discussed in pascual et al 2013 justifying the dep estimates at this region for the 10 locations in the mediterranean the mode value estimates of the dep level is characterized as moderate to high 0 85 for the 10 years and low 0 63 for the 15 years the nature of the storms in this semi enclosed basin is subjected to many external factors like land sea contrasts near surface temperatures atmospheric waves and large scale weather patterns flaounas et al 2015b campins et al 2011 although the windiest areas of the mediterranean sea are located in the nw sw part e g locations l25 l26 l21 and l27 and the se part e g locations l22 and l23 the dep level estimations at these locations are characterized as moderate this effect is due to the large but constant wind conditions at these sites where deca is controlled by a moderate level of dep in order to encompass the largest number of extreme wind storm events as possible on the contrary locations l28 l29 l24 and l30 at the central and north mediterranean are characterized by a high dep level as expected where the largest number of explosive cyclogenesis is observed 6 2 bandwidth estimates in this section the results of the optimum normalized bandwidths for the 30 locations are presented from the empirical selection procedure as described previously in sub section 4 4 the most likely optimal normalized bandwidth estimates illustrated in fig 6 range in a bound from 0 126 to 0 267 avoiding over or under smoothing kernel adjustments it is apparent that as the sample period increased from 10 to 15 years the optimal normalized bandwidth was reduced for the majority of the locations given the relatively small sample period of data such as 10 years it is not surprising that we obtain a limited number of events from the deca model thus the irregular sample of deca will be characterized by a limited amount of values surrounding the mean width of the time intervals as a consequence the gaussian weight function adjusted accordingly to the largest bandwidth for the sample period of 10 years in this way for the estimator of the irregular acf in eq 3 the largest possible number of higher weights is assigned to the inter sampling time intervals closer to the given time lag k in other words the weight function stretched out to a wider bandwidth scale in sample periods where little information is available conversely as the sample period increased from 10 to 15 years the optimal normalized bandwidth reduced leading accordingly to a less wide bandwidth adjustment of the weight function however for locations characterized of strong but stable wind profile the increase of the sample period increased the bandwidth as a response of decaun to the inconsistent increase of extremes as the sample size increased for the most locations in general as the sample size increased the bandwidth estimates reduced as shown from the gaussian based correlation analysis of the acf estimator similarity in the irregular deca samples the reduced trend of the most likely optimal normalized bandwidth estimates to smaller values as the sample size increased is shown in fig 7 6 3 lag k estimates the required lag k apart value of observations is presented for the re sampling procedure at each location the desired lag value estimates of k will assign the statistically independent events from the optimum re sample of decaun at each sample period with an increasing sample period the lag k also increased for the majority of the examined locations illustrated in fig 8 in general for the sample period of 10 years the lower lag value estimate of k ensures a successful trade off between excluding events and loss of information thus in order to increase the number of events and consequently the associated inter sampling time intervals of the weights to be applied a lower lag k is selected conversely as the sample period increased from 10 to 15 years the irregular acf estimator was adjusted to a larger lag k as a consequence the acf estimator included a sufficient amount of independent events more efficiently at the larger sample period the response of decaun to the lag k estimates is also closely related to the dep estimates that form the samples of deca specifically for locations of strong winds and low volatility in extremes the demonstration of decaun in the increase of sample period resulted to higher dep and to relatively smaller lag k when little information of extremes is apparent at these locations the lower lag k is preferable to form clusters efficiently of larger length and consequently derive samples of higher dep to this effect an example is presented in the following table 2 for one case at the two sample periods of examination the effect of decaun to the mode estimates of the desired lag k is illustrated in fig 9 where generally an increment of the mode estimates is obtained as the sample size increases inference is also made illustrating decaun 2 to the most likely re sampling scheme from the demonstration of decaun at each sample period and region inference based on the lag value estimates of k at the associated dep level is also made for the number of the asymptotically independent events from decaun the number of events estimated respectively to the re sampling scheme at each sample period and region is illustrated in fig 10 under the decaun 2 scheme for the locations in the north sea the likely number of re sampled events is approximated at 3 8 year for the sample period of 10 years and at 3 9 year events for the 15 years respectively similarly for the locations in the atlantic the likely number of re sampled events under the decaun 2 scheme is approximated at 4 8 year for the 10 years and at 3 7 year for the 15 years finally for the locations in the mediterranean decaun 2 re sampled approximately a number of 5 4 year events for the 10 years and a number of 5 2 year for the 15 years respectively 6 4 the nrmse measure of decaun the estimation of the nrmse measure and its relation to the sample period resulted in something unsurprising for the majority of locations increase in the sample period increased the precision of nrmse to the quantile estimates of decaun to this effect the re sampled models and their fit to the gpd distribution are based on a larger amount of values containing more information about the high end tail distribution of the wind speed data therefore the quantile estimates of decaun to a gpd fit converged with the increase in sample period to the quantile estimates of the bm ref fit to the gev this is illustrated in fig 11 where the quantitative measure of performance of the optimum re sampled models from decaun generally reduced as the sample period increased the increase in the sample period does not reduce the nrmse measure for all locations this inverse proportional behavior of the measure is possibly explained by the weak performance of the reference model bm ref at the locations l1 l4 l13 l19 l22 and l24 the over under quantile estimates from the reference model using small samples such as the available sample of 20 years are also discussed in an and pandey 2007 ceppi et al 2008 and della marta et al 2009 however high wind speeds are frequently apparent at these locations as obtained in table a 1 2 in appendix to this effect the available samples will probably force the fit of the extremes to the gev to lie near the mode of the distribution and hence away from the tail area of interest hence the reference model described by the bm approach will probably not give the best fit to the tail of the distribution at these locations for samples of limited information in extremes the validity of the asymptotic forms is challenged to assess the fit of the reference model bm ref to the gev and the decaun model to the gpd the plots of q q p p and kernel density are presented in fig 12 for one case l13 if the empirical data align closely with the modeled estimates then it is likely that the chosen model for relatively small samples of wind speed is a good representation of the true extreme asymptotic form for these samples in addition the 95 confidence bands are also provided based on the kolmogorov smirnov statistic doksum and sievers 1976 6 5 design values design value dv estimations in m s for various return periods are presented for the two relatively small sample periods of examination 10 and 15 years henceforth all results will be referring to the selected locations l2 l16 and l21 alongside the d v estimations of the optimal re samples from the decaun model we include estimations from deca the performance of the runs is also presented as the standard comparable model at each sample period of examination moreover the bm model refers to sample periods of 10 and 15 years and the bm ref at the maximum available sample period 20 years regarding l2 d v estimates of the 50 years return period d v 50 for bm ref yielded 26 59 m s as the reference for both sample periods of examination fig 13 decaun provided d v 50 estimates at 26 84 m s with an nrmse measure at 0 013 with regards to the sample period of 10 years figs 13a and 26 67 m s followed by a reduction of the nrmse measure to 0 003 for the 15 years fig 13b the d v 50 for bm ref regarding l16 was estimated at 28 72 m s subsequently the estimations for l16 with regards to the d v 50 and n r m s e measure presented in fig 14 are outlined in the same context as previously decaun provided d v 50 estimates at 28 86 m s with an nrmse measure at 0 005 with regards to the sample period of 10 years figs 14a and 28 64 m s followed by a reduction of the nrmse measure to 0 003 for the 15 years fig 14b finally the d v 50 for bm ref regarding l21 was estimated at 25 79 m s in the same context as before we subsequently outlined the estimations for l21 with regards to the d v 50 and n r m s e measure decaun provided d v 50 estimates at 25 79 m s with an nrmse measure at 0 011 with regards to the sample period of 10 years fig 15 a and 25 86 m s followed by a reduction of the nrmse measure to 0 007 for the 15 years fig 15b as a general remark to the d v estimates from the decaun model the proposed model slightly overestimated the quantile prediction with regards to bm ref for small scale return periods of 50 years as obtained for all three locations this effect is more obvious considering the sample period of 10 years in figs 13a 14a and 15a on the contrary for the larger sample period of 15 years there is a remarkable convergence of decaun towards the bm ref model moreover runs failed to provide reliable d v estimates for the sample of 10 years yielding quantile overestimates in comparison to those made from the reference model finally we anticipated weak performance from the bm model for the two sample periods although the latter model was supplied as a weak comparable measure of prediction in line with the aforementioned remarks design value estimations d v 50 for the 30 locations are illustrated in fig 16 for inference of the wind speed projections of the decaun model additional diagnostics are presented in table a 2 3 and table a 2 4 in appendix 6 6 confidence bounds for the three locations l2 l16 and l21 and to each sample period of examination only the confidence bound of the estimated d v 50 of the 50 years return period for each model is illustrated the interval estimates are supported by two methods the first method is the normal approximation and is applied only to the bm ref model the second method is the standard non parametric bootstrap method percentile and is applied to the models bm runs deca and decaun the decaun model variability for l2 fig 17 a yielded a slight increment in the width of the 95 confidence interval width of ci by 0 044 m s with the increase of the sample period from 10 to 15 years on the contrary a considerable reduction in the width of ci by 5 509 m s and 1 451 m s is obtained for l16 fig 17b and l21 fig 17c respectively with regards to the decaun model the confidence bounds are considerably wider in comparison to deca for all three locations the estimations confirmed the wider bound effect that was expected despite the fact that a bootstrap approach was performed for a reliable inference in terms of variability it failed to overcome the weakness of small samples such as the irregular re samples from decaun the inevitable high variability of the proposed model is caused by the reduction in the sample size as a repercussion from re sampling at the associated dep levels moreover the increase of the sample period to 15 years positively affected deca decaun and runs yielding narrower confidence bounds finally bm confirmed the inability to provide useful bounds in comparison to the other models inference of the variability of design value estimates d v 50 of decaun with the increase of the sample period for the 30 locations is illustrated in fig 18 6 7 model parameters for easier representation of the modeling to the asymptotic distributions only the estimated shape parameter ξ ˆ is provided for each model at the three locations the 95 confidence intervals of the estimated parameters of the gev and gpd distributions at each sample period are derived by two methods as discussed previously in sub section 4 1 2 assuming that the shape parameter estimation with regards to the bm ref model ξ ˆ r e f has a reduced degree of uncertainty it will be considered as the reference estimation for the comparisons made in the parameter ξ ˆ for both sample periods of examination in general decaun estimates of ξ ˆ resulted a reduction of the 95 width of ci in the increase of the sample period from 10 to 15 years for the three locations l2 l16 and l21 illustrated in fig 19 the estimates of the shape parameter of decaun for the three locations ranged from 0 471 to 0 771 and even wider for the associated bounds however as the sample period increased we observed ξ ˆ estimates converging to the ξ ˆ r e f this convergence confirmed the unique relationship between the distributions of gev and gpd fitting successfully the samples of the reference and proposed model respectively in this implementation the residual resid ξ ˆ ξ ˆ r e f ξ ˆ is used as a visual metric estimate performance of decaun the ξ ˆ r e f parameter estimates to locations l2 l16 and l21 are at 0 168 0 473 and 0 170 respectively shown in fig 19a b and c at the latter three locations and for the sample period of 10 years the resid ξ ˆ of decaun is obtained at 0 303 0 017 and 0 601 with regards to the 15 years the resid estimate followed a reduction at 0 121 converging to ξ ˆ r e f increment at 0 242 diverging negatively from ξ ˆ r e f and reduction at 0 178 converging to ξ ˆ r e f for inference of the shape parameter estimates of decaun to the 30 locations fig 20 is given in general the resid ξ ˆ metric reduced to zero converging to ξ ˆ r e f as the sample period increased for the majority of the locations as a remark the parameter assessment from the proposed model derived almost weak asymptotically normal properties especially for the sample periods of 10 years i e l5 l7 l15 and l21 a possible explanation is given from the over underestimation of ξ ˆ under the standard mle method considering the relatively small number of observation at these locations in addition the inverse proportional behavior of the metric measure resid in the increase of sample period e g location l13 is also possible related to the intractable problems of inference for the ξ ˆ r e f parameter of gev under mle however for the relatively small samples using the lmom method according to hosking and wallis 1997 would probably model in a better way the statistical weakness of the small re samples obtained from decaun we close our results in the following sub section 6 8 with the threshold selection for the runs model at the three aforementioned locations 6 8 threshold diagnostics for the runs model the threshold selection for the runs model was derived from the nc diagnostics as discussed previously in sub section 4 2 the nc diagnostics are presented for the sample periods of 10 diagrams b f and j and 15 years diagrams d h and l with the standard parameter stability ps plot coles 2011 as a comparative measure diagrams a e and i for 10 years and diagrams c g and k for 15 years respectively in fig 21 for the diagnostics score test is performed for the shape parameter over multiple thresholds to the three locations the threshold range for the test was limited between the 60 and 99 5 sample quantile of the daily wind speed maxima with a step of 0 01 the empirical threshold selection is depicted as the value associated with the sharpest p value increase at the significance level of 0 05 for easier representation peak p values and threshold are located on the vertical dashed line on the diagram of the nc diagnostics furthermore threshold exceedances are also denoted on the top scale of the same diagram additional diagnostics of the runs model to the threshold from nc are summarized in table 3 considering the three locations the number of the cluster peak exceedances over threshold yielded in a range from 70 to 92 with regards to the sample period of 10 years and 90 to 121 for the 15 years respectively moreover the threshold depicted from the nc diagnostics is illustrated to the ps plot as a comparative measure in fig 21 the nc threshold obtained from the ps plot is located on the diagram as the solid dot line highlighting empirical estimation for the shape parameter with approximate pointwise wald 95 confidence intervals as vertical solid lines as a remark threshold inference of the multiple threshold model by the nc diagnostics confirmed the advantage against the standard ps plot the threshold from nc required less subjectivity and experience to detect 7 conclusions the present analysis assessed the effect of irregularity in samples to the design value estimates of wind speed under the asymptotic form of gpd summarizing the implementation of decaun we recall that the main idea was to re sample the irregular samples of deca to approximate i i d observations under the concept of de clustering the re sampling strategy used the slotting method employing a gaussian kernel weight function to the irregular acf estimator moreover this utilization accounted for eight dep reduction levels and the same number of bandwidths in this way the re samples encompassed as large a number as possible of discrete events balancing the over or under smooth effects on the weight function from this assessment decaun is regarded as an alternative de clustering approach for relatively small samples furthermore the high resolution database in this setting avoided the apparent underestimation of the extreme wind speed variability identifying the extreme characteristics primarily near the coasts and in narrow straits and basins the general findings are outlined as follows for small samples of wind speed e g 15 years the re samples of decaun modeled by the approximation to the gpd illustrated effective projections in terms of precision and variability specifically with regards to the 50 year design values and ξ ˆ estimates decaun yielded larger confidence bounds in comparison to the runs and deca models within the pot concept the samples of 10 and 15 years confirmed the inability of bm to provide useful bounds in comparison to the other models resulting in a weak comparable measure of prediction to these samples in addition decaun circumvented successfully the problems of irregular sampling modeling events that meet approximately the i i d requirements the most important findings considering the proposed model for the 30 locations can be summarized as follows the assessment yielded the most likely dep reduction level to range approximately from 0 68 to 0 85 with regards to the sample period of 10 years and from 0 63 to 0 74 for the sample of 15 years furthermore estimations in terms of the most likely optimal normalized bandwidths are summarized approximately to range from 0 203 to 0 267 for the sample of 10 years and from 0 126 to 0 25 for the sample of 15 years based on the most likely number of asymptotically independent events the approximated range is in line with the variability of extreme winds over the regional locations in this setting specifically over the locations in the north sea from 3 8 to 3 9 year the atlantic from 3 7 to 4 8 year and the mediterranean from 5 2 to 5 4 year additional issues related to this analysis still remain and require further assessment for example considering the relatively small sample periods of examination parameter estimations made by the lmom method is probably more suitable for these samples additionally the use of the parametric bootstrap approach davison and hinkley 1997 for the model validation of decaun will probably improve the uncertainty bound effect of the estimates this remark also follows the suggestion from kyselý 2008 for the inference based on the small to moderate sample sizes such as those from the decaun re samples however the main scope of this study is to assess the effect of the irregular deca samples into the irregular acf estimator and not the response of the proposed model to the parameter estimation methods a more comprehensive investigation with regards to the optimization of the parameter estimation and the sampling uncertainties must be undertaken before the proposed model is widely applied credit authorship contribution statement christos tsalis conceptualization formal analysis methodology software writing original draft platon patlakas data curation christos stathopoulos validation george kallos supervision resources declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to thank dr ioannis papastathopoulos and mr graeme auld from the department of mathematics at the university of edinburgh for their scientific advisory and proof reading and dr takvor soukissian from the hellenic centre for marine research for his thoughtful suggestions from discussions in addition mrs fani anagnostou for her assistance in computational issues and finally mr panagiotis triantafyllopoulos for his assistance in graphical issues the study was supported by the operational program human resources development education and lifelong learning and co financed by the european union european social fund and greek national funds mis 5007050 in http www edulll gr appendix a 1 locations and wind speed datasets used table a 1 1 locations of daily wind speed records used for this analysis table a 1 1 north sea atlantic ocean mediterranean sea location lat lon location lat lon location lat lon l1 55 7 n 7 4 e l11 50 45 n 1 55 w l21 40 8 n 5 5 e l2 52 05 n 2 15 e l12 50 05 n 4 25 w l22 35 5 n 26 4 e l3 51 65 n 3 45 e l13 42 85 n 9 95 w l23 33 9 n 29 9 e l4 54 8 n 1 35 e l14 53 25 n 10 25 w l24 43 4 n 15 4 e l5 57 15 n 3 1 e l15 49 65 n 6 55 w l25 36 5 n 3 5 w l6 58 4 n 10 3 e l16 57 25 n 7 65 w l26 37 9 n 3 1 e l7 51 65 n 1 35 e l17 52 75 n 9 75 w l27 43 3 n 7 5 e l8 57 95 n 3 1 e l18 50 65 n 0 95 e l28 42 2 n 11 4 e l9 56 2 n 4 15 e l19 54 85 n 8 95 w l29 40 5 n 12 2 e l10 55 75 n 2 25 e l20 53 35 n 4 85 w l30 42 2 n 17 9 e table a 1 2 descriptive statistics for the 30 locations of the datasets used from the marina platform database extending from 1996 to 2015 20 years table a 1 2 location min max median mean se mean ci mean0 95 var std dev coef var skewness kurtosis l1 0 25 965 7 691 8 058 0 009 0 017 12 992 3 604 0 447 0 454 3 113 l2 0 26 704 7 236 7 627 0 008 0 017 12 446 3 528 0 463 0 487 3 129 l3 0 26 436 6 925 7 327 0 008 0 017 12 579 3 547 0 484 0 570 3 210 l4 0 25 840 7 417 7 904 0 009 0 017 13 750 3 708 0 469 0 497 3 050 l5 0 28 389 7 831 8 300 0 009 0 018 14 535 3 812 0 459 0 499 3 110 l6 0 23 912 7 170 7 572 0 009 0 018 14 169 3 764 0 497 0 372 2 661 l7 0 24 480 6 810 7 173 0 008 0 016 11 068 3 327 0 464 0 527 3 119 l8 0 28 801 7 968 8 455 0 009 0 019 15 738 3 967 0 469 0 525 3 131 l9 0 26 495 7 714 8 153 0 009 0 017 13 583 3 685 0 452 0 489 3 111 l10 0 28 029 7 623 8 098 0 009 0 018 14 139 3 760 0 464 0 494 3 077 l11 0 24 330 6 912 7 303 0 009 0 017 13 128 3 623 0 496 0 546 3 162 l12 0 24 877 6 946 7 378 0 009 0 017 13 297 3 646 0 494 0 551 3 154 l13 0 25 098 7 556 7 903 0 009 0 018 14 682 3 832 0 485 0 304 2 513 l14 0 28 752 7 777 8 200 0 009 0 018 15 351 3 918 0 478 0 523 3 125 l15 0 27 063 7 544 8 015 0 009 0 017 13 568 3 684 0 460 0 539 3 183 l16 0 28 671 7 653 8 111 0 010 0 019 16 469 4 058 0 500 0 554 3 081 l17 0 26 142 6 508 7 025 0 009 0 017 13 602 3 688 0 525 0 706 3 400 l18 0 25 769 7 032 7 396 0 009 0 017 13 036 3 611 0 488 0 493 3 131 l19 0 28 851 7 427 7 898 0 010 0 019 15 906 3 988 0 505 0 543 3 041 l20 0 24 816 7 175 7 639 0 009 0 018 15 415 3 926 0 514 0 548 3 042 l21 0 25 901 5 950 6 582 0 009 0 019 15 637 3 954 0 601 0 911 3 642 l22 0 20 863 7 225 7 121 0 008 0 016 11 392 3 375 0 474 0 072 2 499 l23 0 20 874 5 758 5 822 0 006 0 012 7 112 2 667 0 458 0 584 3 733 l24 0 23 377 4 941 5 487 0 008 0 016 11 976 3 461 0 631 0 847 3 478 l25 0 20 752 4 191 5 194 0 009 0 018 15 145 3 892 0 749 0 855 2 951 l26 0 21 371 5 866 6 021 0 007 0 015 9 625 3 102 0 515 0 637 3 453 l27 0 23 511 4 054 5 323 0 010 0 019 16 441 4 055 0 762 1 131 3 665 l28 0 19 200 3 865 4 460 0 007 0 013 8 101 2 846 0 638 1 008 3 927 l29 0 21 085 4 382 5 040 0 008 0 015 9 881 3 143 0 624 0 976 3 800 l30 0 21 555 4 601 5 047 0 007 0 015 9 725 3 118 0 618 0 769 3 406 a 2 decaun model estimates additional characteristic estimates of decaun is presented for the 30 locations with regards to the sample period of 10 and 15 years specifically the lowest aic and mse statistic measures and the associated total number of peak exceedances fitting the gpd distribution table a 2 3 additional characteristic estimates of the decaun modelwith regards to the sample period of 10 years extending from 1996 to 2005 table a 2 3 location aic mse lowest no decaun cluster u m s no thres exceedances l1 aic 238 075 68 15 007 51 l2 aic 100 512 30 16 852 22 l3 mse 15 720 106 12 142 53 l4 aic 78 251 64 19 273 22 l5 mse 73 450 45 0 000 45 l6 aic 153 035 51 16 068 38 l7 mse 55 100 43 0 000 43 l8 mse 20 140 35 17 110 26 l9 mse 16 700 34 16 387 25 l10 mse 20 510 32 16 748 24 l11 mse 8 740 50 17 040 25 l12 aic 82 652 34 17 703 20 l13 aic 119 096 61 17 574 30 l14 aic 109 212 32 18 443 24 l15 aic 293 395 44 0 000 44 l16 mse 10 150 52 20 659 23 l17 mse 69 920 75 0 000 75 l18 aic 74 992 35 17 403 17 l19 mse 6 540 45 19 433 24 l20 mse 7 460 53 18 470 26 l21 mse 64 700 57 0 000 57 l22 mse 6 400 187 12 129 140 l23 aic 419 345 245 10 266 112 l24 aic 58 360 54 17 983 20 l25 mse 5 080 103 14 075 57 l26 aic 57 326 43 16 784 18 l27 aic 66 129 48 18 529 21 l28 aic 98 418 37 13 442 27 l29 mse 9 720 127 11 788 57 l30 mse 5 470 115 15 012 45 table a 2 4 additional characteristic estimates of the decaun model with regards to the sample period of 15 years extending from 1996 to 2010 table a 2 4 location aic mse lowest no decaun cluster u m s no thres exceedances l1 mse 10 510 71 18 202 34 l2 aic 129 915 66 18 164 33 l3 mse 13 990 227 13 678 103 l4 aic 87 682 104 20 024 29 l5 aic 111 829 51 19 113 25 l6 aic 243 521 130 16 252 62 l7 aic 102 150 52 17 252 26 l8 aic 114 597 53 19 787 27 l9 aic 124 461 57 18 653 30 l10 aic 185 238 56 16 411 39 l11 aic 86 283 50 19 114 25 l12 aic 105 115 77 19 207 30 l13 aic 251 810 130 17 057 65 l14 mse 18 980 180 14 318 90 l15 mse 10 580 50 18 029 37 l16 mse 17 910 47 18 325 35 l17 aic 94 653 51 19 767 25 l18 aic 120 108 60 17 974 30 l19 mse 5 890 54 19 811 27 l20 mse 7 540 140 17 681 64 l21 mse 9 420 200 16 997 90 l22 mse 7 010 127 12 928 95 l23 aic 120 056 61 14 013 33 l24 aic 88 248 77 17 816 30 l25 aic 77 223 76 16 975 29 l26 aic 63 457 67 17 625 25 l27 mse 3 380 62 18 703 27 l28 aic 160 306 91 13 474 45 l29 mse 11 410 303 10 902 148 l30 aic 1058 479 175 0 000 175 a 3 mann kendall test the mann kendall m k test is a non parametric test frequently used to detect the existence of monotonic upward or downward trends in samples of environmental data the test is based on the correlation between ranks of a sample and their order instead of the actual values of the sample the null hypothesis h 0 is that the data come from a population of independent and identically distributed variables an important advantage of the m k test is that it is distribution free in contrast for example to the regression slope test where the residuals are assumed to be normally distributed on the other hand the examined data should not be serially correlated in order for the estimated p values to be correct the m k test statistic is calculated as follows a 3 1 s k 1 n 1 j k 1 n s g n x j x k where a 3 2 s g n x 1 if x 0 0 if x 0 1 if x 0 the variance of s in the general case where ties are present is given as follows a 3 3 σ s 2 1 18 n n 1 2 n 5 q 1 q t q t q 1 2 t q 5 where n denotes the total number of observations from the sample q is the number of tied groups and t q is the number of observations in the q th tied group the final test statistic z results from the following transformation of s a 3 4 z s 1 σ s if s 0 0 if s 0 s 1 σ s if s 0 positive negative values of z suggest an upward downward trend as the sample size becomes larger the test statistic z follows approximately the gaussian distribution 
20841,a new control strategy based on adaptive second order sliding mode approach is applied to a floating wind turbine system in the above rated region this adaptive controller is well adapted to a highly nonlinear system as floating wind turbine and can be easily implemented with very reduced knowledge of modeling the proposed controller partially based on multi blade coordinates transformation combines collective and individual collective blade pitch control for power regulation platform pitch motion reduction and reduction of blades fatigue load the proposed controller is implemented on fast simulator and shows high level of performances keywords second order sliding mode adaptive control floating wind turbine blade pitch control 1 introduction floating wind turbines fwts allow the use of the huge wind resource in ocean area and are considered as a promising solution of renewable energy however some issues arise first of all unlike the onshore wind turbine the floating platform introduces additional degrees of freedoms dofs that have negative impacts especially on the platform pitch motion they also induce the issue of negative damping nielsen et al 2007 that leads to system instability and degrades the power production furthermore with the increasing capacity and flexibility of wind turbines fatigue loads of the structure became more and more important especially for floating systems and affect the service life therefore reducing the fatigue loads is a key point bossanyi 2003 menezes et al 2018 for large scale wind turbines the control strategy must provide an efficient solution for such problems and appears crucial for wind turbine systems the main control objectives of fwt in the above rated region consist in maintaining the power output at rated value meanwhile avoiding the negative damping i e reducing the platform pitch motion jonkman et al 2009 many works have been made based on the collective blade pitch cbp control strategy in this case all the blades of the wind turbine are controlled by a similar way among existing results in jonkman 2008 the famous baseline gain scheduling proportional integral gspi control is proposed platform pitch motion is successfully reduced but with a large power fluctuation in wakui et al 2017 a novel gain scheduling control strategy is developed improving the power regulation while keeping the same platform pitch motion as gspi linear quadratic regulator control model predictive control and feed forward control namik et al 2008 schlipf et al 2012 2015 have been also applied to fwt systems however fatigue load reduction is not considered in those works in terms of the fatigue load reduction individual blade pitch ibp control bossanyi 2003 lio et al 2018 ossmann et al 2017 selvam et al 2009 has been introduced in this case the blades are independently controlled this approach has also been extended to floating ones cunha et al 2014 suemoto et al 2017 namik and stol 2014 nonetheless in all these works the control design is based on linearized models around an equilibrium point obtained by the fast fatigue aerodynamics structures and turbulence software jonkman et al 2005 these approaches based on linearization around an operating point are in opposition with the use of fwts in a large operating domain hence in order to avoid this drawback different models for different equilibrium points are required that induces design of different controllers producing a large effort of parameter tuning an other solution could be the use of nonlinear control strategies based on nonlinear models however given that the nonlinear models of fwt homer and nagamune 2018 jonkman et al 2005 sandner et al 2012 are not well adapted to the control design there are few studies on the nonlinear control of fwt as detailed just below a solution consists in designing nonlinear control laws that are efficient on a large operating domain but without precise models in order to reduce as much as possible the modeling effort sliding mode control smc utkin 1977 is a well adapted solution given its robustness versus uncertainties and perturbations then in the current work smc is applied for the floating wind turbines control smc requires very limited knowledge of system model especially in its adaptive version while keeping robust versus uncertainties and perturbations such control algorithms ensure that the sliding variable that is defined from the control objectives converges to a vicinity of the origin in a finite time however due to the discontinuous feature of standard smc control chattering phenomenon i e high frequency oscillations of control input appears in order to reduce the chattering effect super twisting stw shtessel et al 2014 control combined with gain adaptation plestan et al 2010 shtessel et al 2012 is applied in this study the stw is one of most famous second order sliding mode control that generates continuous control input and thereby reduce the chattering furthermore stw only requires the knowledge of sliding variable then it can be viewed as an output feedback control and is very simple to implement furthermore adaptive gain allows to keep control accuracy versus perturbations and uncertainties even in the case that the information of system model is very reduced in fact only the relative degree isidori 1999 of the sliding variable is required hence such algorithm is very well adapted to the fwt control problem notice that in authors previous works zhang et al 2019a b zhang and plestan 2020 super twisting control with gain adaptation based on cbp control technology has been successfully applied to fwt system here the first novelty is based on the fact that ibp and cbp control structures are combined to control the power and to reduce the fatigue load of the wind turbine especially the blade load reduction among the structure loads the blade root reduction is the most important being the source of the loads for the rest of the structures jelavić et al 2010 an other novelty is the use of an adaptive second order sliding mode controller in the frame of ibp cbp control strategy section 2 introduces the model of the fwt section 3 states the control problem section 4 describes the stw control laws with adaptation laws based on both cbp and ibp control approaches and their application to the fwt section 5 displays the results obtained by fast simulink co simulations and their analysis 2 system modeling the national renewable energy laboratory nrel 5mw oc3 hywind floating wind turbine see fig 1 is selected in this study this wind turbine is simulated by the well known wind turbine simulation software fast jonkman et al 2005 the detailed parameters and the properties being given in jonkman et al 2009 jonkman 2010 however the wind turbine model used in fast is composed by a large number of complex nonlinear functions and cannot be adopted for control design fast software can provide a linear state model of the fwt around an operating point that depends on the wind speed and the rotor speed in this case for a given operating point one can obtain the following state space model 1 x a x b u b d δ with x the system state vector depending on the dofs enabled during the linearization process u β 1 β 2 β 3 t the control input vector i e the pitch angle of each of the three blades and δ the wind perturbation input matrices a b and b d depend on the considered dofs and are obtained from fast depending on the operating conditions considering that the above rated wind speed of nrel 5mw wind turbine is varying between 11 4 m s and 25 m s in such large operating domain different linear models i e different matrices a b and b d must be carried out depending on the variation of the wind speed remark 1 as detailed in the sequel only 2 dofs are considered for the control design in this case the state vector is composed by the platform pitch angle its velocity and the rotor speed the matrices a b and b d are provided by fast software for each operating point as an example considering a wind speed equal to 16 m s and a rotor speed equal to 12 1 rpm they are 2 a 0 1 0 0 0141 0 0402 0 0003 0 0525 2 0615 0 1624 b 0 0 0033 0 9479 b d 0 0 0001 0 0253 thus the fwt modeling on the whole above rated region reads as 3 x a t x b t u b d t δ by a more general point of view system 3 can be represented as a class of nonlinear system 4 x f x t g x t u with f x t including the term a t x and the associated perturbations b d t δ and uncertainties as g x t including the term b t it is important to notice that in the sequel f and g are viewed as unknown but bounded functions the main purposes of the controllers designed in this paper are the limitation of the power at its rated value the reduction of the platform pitch motion and the attenuation the blade flap wise root moment the two first objectives can be achieved by the cbp control while the third one is fulfilled by ibp control since the cbp and ibp control can be separately designed as two independent control loops see details in the sequel two models are carried out notice that fast provides dozens of degree of freedoms dofs including the motions of tower blades and platform the rotation of rotor the yaw motion these models are too complex and large for control design then in order to simplify the control design reduced models are considered however in the sequel notice that all the simulations will be made by applying the controllers designed on reduce models to the full dofs model running in fast 2 1 reduced cbp control model this model is focused on the platform pitch and the rotor concerning the control objectives of cbp controller only two dofs the rotor rotation and the platform pitch are considered based on a similar writing as 4 the model for cbp control reads as 5 x c f c x c t g c x c t u c with x c φ φ ω t φ being the platform pitch angle φ the platform pitch velocity and ω the rotor speed the cbp control adjusts each blade pitch angle by the same amount simultaneously hence the control input is defined as u c β c o l this angle being applied to each blade 2 2 reduced ibp control model this model is focused on the blade behavior in this case 3 dofs are enabled i e the first flap wise bending mode of each blade hence the state vector includes the flap wise bending deflection of each blade q 1 2 3 see q 1 for blade 1 fig 2 right and its corresponding velocity q 1 2 3 i e x i q 1 q 2 q 3 q 1 q 2 q 3 t the control input vector u i β 1 β 2 β 3 t is such that each blade pitch angle has its own value as previously the system model can be written as 4 6 x i f i x i t g i x i t u i however the dynamics of each blade depends on the azimuth angle ψ i e the angle between a vertical axis and the current position of the blade symmetrical axis see fig 2 left which induces a periodic system therefore analysis and control design could be not straightforward in order to avoid the periodic dynamics the most conventional method in ibp control is the application of the multi blade coordinate mbc transformation bir 2008 also known as coleman transformation mbc transformation allows to write the dynamics into a fixed non rotating frame by this way the controller is designed without considering the periodic property such coordinates transformation also allows to decouple the ibp control that is focused on load reduction from the cbp control stol et al 2009 notice that the control based on mbc transformation has almost same results as the directly periodic control stol et al 2009 but without complexity consider the following state coordinates transformation called mbc transformation bir 2008 7 x i n r t x i with 8 t 1 3 2 cos ψ 2 cos ψ 2 π 3 2 cos ψ 4 π 3 2 sin ψ 2 sin ψ 2 π 3 2 sin ψ 4 π 3 then applying this transformation to 5 gives the following system that will be called in the sequel mbc system 9 x i n r f i n r x i n r t g i n r x i n r t u i n r with x i n r q t i l t q y a w q t i l t q y a w t and u i n r β t i l t β y a w t the state and input vectors in the non rotating frame q t i l t and q y a w the fictitious tilt and yaw component of blade flap wise deflections respectively β y a w and β t i l t the yaw and tilt component of blade pitch angles 3 problem statement recall that the control objectives of the current study are to ensure the power output at rated meanwhile reducing the platform pitch motion and reducing the flap wise load of blades in authors previous works zhang et al 2019a b both the first control objectives power platform pitch motion are achieved by collective blade pitch control here the blade load especially the blade flap wise load alleviation is also considered and can be ensured by separately adjusting the pitch angle of each blade namely by using the individual blade pitch control bossanyi 2003 selvam et al 2009 van engelen 2006 the overall control scheme is shown in fig 3 the ibp angle adjustment is added to the cbp control input but has a limited effect on the global behavior of the power and platform pitch motion in other words there is a very reduced coupling between the cbp and ibp control bossanyi 2003 jelavić et al 2010 hence these latter can be separately designed as two independent control loops while achieving their own control objectives 3 1 formalization of the collective blade pitch control problem the task of cbp control loop is to regulate power at rated p 0 meanwhile reducing the platform pitch motion usually 1 1 in the paper for a sake of simplicity the generator torque is supposed to be constant the objective being to focus the attention on the control of the hydrodynamic part of the wind turbine however the authors are fully aware that it is necessary to also consider the control by generator point of view that will be the object of future works the generator torque is supposed to be fixed at its rated t g 0 in the above rated region then the power regulation turns into regulate the rotor speed at its rated value ω r ω r p 0 n g t g 0 with n g the gear box ratio between high speed shaft and low speed shaft however there are two control objectives with a single control input that is the collective blade pitch angle β c o l a solution to such problem regarding to the fwt control is to modify the rotor speed reference by including the platform pitch rate φ that gives a new rotor speed reference ω defined as 10 ω ω r k φ with k a positive constant such solution takes advantage of the physical features of the rotor rotation and platform pitching in response to aerodynamic torque and thrust consider that forward pitching is appearing and suppose that the control is efficient in this case φ 0 in order to fulfill 10 thank to the action on the blade pitch angle that increases the aerodynamic torque of blades the rotor speed increases and is greater than ω r at the same time the aerodynamic thrust increases preventing the platform forward pitching therefore the platform pitch velocity φ will be reduced and thereby the rotor speed will converge to the rated lackner 2009 cunha et al 2014 as conclusion the controlled output associated to the system 5 is defined as 11 y c ω ω remark 2 from remark 1 and zhang et al 2019b zhang and plestan 2020 it can be verified that in the considered operating domain the relative degree isidori 1999 of system 5 with the previous output y c is equal to 1 i e the first time derivative of y c explicitly depends on the control input u c 3 2 formalization of the individual blade pitch control problem the rotor of wind turbine transforms the wind power into aerodynamic torque that drives the generator at the same time partial wind energy is transformed into thrust on the rotor that induces load due to the wind shear tower shadow and turbulence the wind speed and direction are varying across the rotor plane these factors cause additional loads on the blades these loads are related with the frequency of the rotor speed and can be decomposed along different modes the main one being at the rotor speed frequency this mode is denoted the 1p mode once per revolution see fig 9 other modes are existing at multiples of rotor speed and are denoted 2p 3p bossanyi 2003 the reduction of the 1p mode for each blade appears being a main objective of ibp control in this regard the flap wise bending moment of each blade are considered as the outputs of ibp control loop consider the mbc system 9 and denote the control output y i n r m t i l t m y a w t with m t i l t and m y a w respectively the tilt and yaw component of blade root flap wise moment see fig 4 as shown in wang et al 2016 xiao et al 2013 this output can be written as 12 y i n r h i n r x i n r t l i n r x i n r t u i n r remark 3 notice that the output y i n r depends on the control input vector u i n r in this case the relative degree of system 9 with output y i n r is equal to 0 the main idea of ibp control is to force the magnitudes of m y a w and m t i l t close to zero that reduces the blade flap wise load mbc approach allows the decoupling between the ibp control that is responsible for load reduction and the cbp control furthermore it has been shown bossanyi 2003 that m y a w and m t i l t can be treated independently by β y a w and β t i l t respectively i e it is possible to use two single input single output controllers for the m y a w and m t i l t alleviation hence the control objectives of ibp control are to ensure the m t i l t and m y a w close to zero by the control β t i l t and β y a w respectively notice that the following inverse mbc transformation should be applied after the controllers design in order to generate ibp control command β 1 β 2 and β 3 see fig 5 13 β 1 β 2 β 3 t t 1 β y a w β t i l t t t 1 c o s ψ s i n ψ c o s ψ 2 π 3 s i n ψ 2 π 3 c o s ψ 4 π 3 s i n ψ 4 π 3 3 3 overall control scheme by a structural point of view the overall control scheme is the combination of cbp control and ibp control then the overall control system design process can be summarized as follows design the cbp control β c o l for regulation of the power and reduction of the platform pitch motion transform the three flap wise blade flap wise bending moments m y 1 m y 2 and m y 3 into the fictitious ones m y a w and m t i l t and design the control loop that provides β y a w and β t i l t respectively and obtain the components β 1 β 2 and β 3 thanks to the inverse mbc transformation the real blade pitch angles β 1 β 2 and β 3 that are the control inputs equal to the sum of β c o l with β 1 β 2 and β 3 4 control design as previously detailed floating wind turbine is a class of nonlinear system with model uncertainties and perturbations that introduced from the flexible structures wind and waves furthermore the traditional controllers of fwt based on the linearized model such as lqr mpc and gspi need great effort of tuning due to the large set of operating points in order to keep high performances all over the operating domain then there is a real interest to design a robust controller with a reduced tuning effort and a single set of parameter tuning meanwhile keeping the control efficiency among the large operating range despite of the uncertainties and perturbations the robust nonlinear strategy selected in this work is based on sliding mode control smc theory utkin 1977 utkin et al 2009 a well known nonlinear control strategy with properties of robustness accuracy and finite time convergence in fact the standard first order smc can be easily implemented however the control of standard smc is discontinuous due to the discontinuous term of the control input chattering is introduced and can damage the physical components such as blade pitch actuator in order to reduce chattering and keep robustness super twisting stw levant 1993 control is introduced furthermore given the unknown terms of system dynamics the uncertainties on turbine and the perturbations the gains of the controller must be chosen sufficiently large to accommodate those effects this fact gives a high gain control that is not good for chattering reduction therefore adaptive version of stw shtessel et al 2012 is well adapted it allows to dynamically adapt the gain versus uncertainties and perturbations while keeping high level of performances even in case of very reduced knowledge of the system 4 1 recalls consider the nonlinear system 14 z f z t g z t υ y h z t with z z r n the state vector and υ u r the control input f z t and g z t the bounded unknown uncertain functions y the control output i e the control objective is to force y to 0 define the sliding variable σ σ z t such that control objective is achieved when σ 0 the objective of the control design is to define a control input υ that drives the sliding variable σ to the sliding surface σ z t 0 in a finite time despite the uncertainties and perturbations note that sliding variable is defined according to control objective y and its relative degree isidori 1999 assumption 1 the relative degree of 14 is equal to 1 define σ y its dynamic reads as 15 σ σ t σ z f z t a z t σ z g z t b z t υ assumption 2 a z t and b z t are unknown but bounded functions such that a a m 0 b m b b m for z z and t 0 a m b m and b m being the positive constant as mentioned above standard smc with sufficient large controller gains can establish a sliding mode namely drive σ to zero in a finite time but with important chattering the super twisting algorithm stw levant 1993 given by 16 υ k 1 σ 1 2 sign σ w w k 2 sign σ with k 1 and k 2 the controller gains satisfying 17 k 1 a m b m k 2 2 4 a m b m 2 b m b m k 1 a m k 1 a m ensures the establishment of a second order sliding mode i e σ σ 0 in a finite time in practice especially due to sampling period only real second order sliding mode is established that is defined as levant 1993 18 σ μ 1 t e 2 σ μ 2 t e with t e the sampling time of controller μ 1 and μ 2 positive constants thanks to the continuous nature of stw algorithm the chattering is greatly reduced while the robustness is kept however due to the uncertainties and perturbations of the real systems sufficiently large gains are necessary such gains are always overestimated which limits the interest of this control strategy to this end gain adaptation is adopted in order to further increase the control performance the adaptation law dynamically adapts the controller that avoids the gain overestimation and strongly reduces the effort of uncertainties perturbations evaluation adaptive super twisting astw controller proposed in shtessel et al 2012 is used here the adaptive law being defined as 19 k 1 ω χ 2 sign σ μ if k 1 k 1 m k if k 1 k 1 m k 2 ϵ k 1 with k 1 m ϵ ω χ μ k positive constants and k 1 0 k 1 m from 19 one can find that σ μ the control accuracy is high so the controller gains can be reduced because they are certainly sufficient k 1 is negative σ μ the control accuracy is low the gains could be too small it is necessary to increase the gains in order to improve the accuracy k 1 is positive the parameter k 1 m is a very small positive constant that ensures the positiveness of controller gains remark that ω and χ determine k 1 dynamics and also k 2 dynamics given that k 1 and k 2 are proportional if they are stated large k 1 will strongly increases resp decreases when real second order sliding mode is lost resp established large dynamics of k 1 and k 2 could induce large variations of the control input it could be damageable for the actuator ϵ defines the ratio between k 1 and k 2 that depends on the system μ acts on the accuracy of the close loop system see the items following 19 a smaller μ means a higher accuracy but induces a more intensive control given that it gives a larger gain k 1 on the other hand a larger value for μ means a lower accuracy in this case the control gain is smaller that gives a less intensive control 4 2 application to the floating wind turbine system as detailed in section 3 the control scheme includes two control loops the first one is the cbp control loop focusing on the control of rotor speed and platform pitch motion the second one is the ibp control loop producing an additional term to each blade pitch angle in order to reduce the variation of blade root flap wise bending moments as previously recalled these two control loops can be independently designed bossanyi 2003 jelavić et al 2010 cbp loop as claimed in remark 2 the relative degree 5 with y c 11 is equal to 1 therefore according to assumption 1 the sliding variable of cbp control can be defined as 20 σ 1 y c ω ω ω ω r k φ ibp loop as recalled in remark 3 the relative degree of system 9 with output y i n r is equal to 0 given that astw algorithm must be applied to system with relative degree equal to 1 consider again the system 9 21 x i n r f i n r x i n r t g i n r x i n r t u i n r with 22 y i n r m t i l t m y a w h i n r x i n r t l i n r x i n r t u i n r a solution consists in defining a dynamic control input that increases the relative degree of the system defining 23 x i n r u i n r and v i n r u i n r the new control input system 9 can be reformulated as 24 x i n r f i n r x i n r t g i n r x i n r t x i n r x i n r v i n r y i n r h i n r x i n r t l i n r x i n r t x i n r with v i n r β t i l t β y a w t the new control input then the relative degree of 24 with respect to β t i l t β y a w t is equal to 1 therefore astw algorithm can be applied the sliding variables of ibp loop are defined as σ 2 σ 3 t m t i l t m y a w t fig 5 depicts the ibp control scheme then one has the sliding variable vector 25 σ σ 1 σ 2 σ 3 ω ω r k φ m t i l t m y a w and its dynamics reads as 26 σ a b v with a and b unknown but bounded functions obtained from 5 24 the control input v is defined as 27 υ β c o l β t i l t β y a w t υ 1 υ 2 υ 3 t with 28 υ 1 υ 2 υ 3 k 11 σ 1 1 2 sign σ 1 0 t k 12 sign σ 1 d τ k 21 σ 2 1 2 sign σ 2 0 t k 22 sign σ 2 d τ k 31 σ 3 1 2 sign σ 3 0 t k 32 sign σ 3 d τ the gains k 1 and k 2 1 2 3 are evolving according to adaptation law 19 5 simulation results the nonlinear oc3 hywind 5mw floating wind turbine model from nrel especially controlled by the previously detailed controller is simulated in this section the used model is built in the fast software and is regarded as a benchmark in many wind turbines studies the parameters of this wind turbine are shown in table 1 the control is developed in the simulink environment and link with the fast model by an s function finally the co simulations between fast and simulink are made on the full dofs fast nonlinear model while the control has been designed based on the reduced dofs model as detailed previously three controllers are used in the following simulations gspi cbp the baseline gspi controller with collective blade pitch control jonkman 2008 astw cbp the adaptive super twisting controller with collective blade pitch control in this case only υ 1 28 is considered zhang et al 2019b astw cibp the adaptive super twisting controller 28 that combines collective with individual blade pitch control the use of these controllers has several objectives comparison between standard gspi and advanced controllers stw and comparison between cbp and cbp ibp control structures in addition two cases of wind and wave conditions are simulated in the sequel case 1 18 m s constant wind velocity with still water i e no wave case 2 18 m s wind velocity with 15 turbulence intensity irregular wave with significant height of 3 25 m and peak spectral period of 9 7 s see fig 6 note that the wind speed of both cases are in above rated region all the simulations are made in 10 min and euler integration is used with sample time fixed at 0 0125 s moreover the blade pitch angle and its rated are saturated as shown in table 1 5 1 case 1 constant wind velocity with still water in this case the astw cbp and astw cibp control strategies are compared the objective being to check the interest to include a ibp control loop firstly fig 7 displays that both the cbp and ibp controllers ensure the rotor speed around its rated value 12 1 rpm and reduced the platform pitch motion i e reduced the platform pitch angle variation furthermore fig 8 shows that the tilt and yaw moment are forced around zero with the cibp controller as a consequence the blade root flap wise moment are strongly reduced comparing with the cbp controller see fig 8 right specifically from the power spectral density psd of blade 1 root flap wise moment displayed by fig 9 one can find that the load reduction of ibp control is acting on the 1p component of the blade load meantime the rotor speed and platform pitch motion are not affected as shown in fig 7 the trajectories of cbp and cibp control are highly coincidence as mentioned in previous section the collective pitch control and the individual pitch control are decoupled fig 10 shows the blade 1 pitch angle obtained with the two controllers 5 2 case 2 turbulent wind and irregular wave previous case shows that both the astw controllers allow to achieve all the control objectives in ideal conditions and proves the necessity to introduce a ibp control loop case 2 now allows to evaluate the controllers gspi cbp astw cbp astw cibp in a more realistic situation the performance of the 3 controllers are compared by using the following indicators root mean square rms of rotor speed error power error platform angular rates these values evaluate the accuracy of the closed loop system for the 3 controllers variation var wang et al 2014 of the blade pitch angle this value evaluates the activity of the actuator damage equivalent load del hayman 2012 of the tower base fore aft side to side and torsional moment and del of averaged blade root flap wise and edge wise bending moment of the three blades these values evaluate the level of mechanical constraints acting on the blades fig 11 shows that for two of the main control objectives rotor speed power regulation and platform pitch motion reduction astw cbp and astw cibp controllers have similar performances allowing reduction of rotor speed error by 8 9 and platform pitch rate by 22 23 versus gspi cbp as previously mentioned there is no coupling between cbp and ibp control loops hence astw cbp and astw cibp have similar performances on rotor speed and platform pitch rate as shown by fig 12 the time series of astw cbp and astw cibp in terms of rotor speed power and platform pitch angle are almost identical furthermore astw cbp controller has also reduced the platform roll and yaw rate on the contrary astw cibp controller induces more important platform roll and yaw rate due to the greatly increased blade pitch actuation namik and stol 2014 however given that the magnitude of platform roll and yaw are relatively small see fig 12 they have a very limited influence on the stability of the whole system fig 13 shows the del results it is clear that astw cbp control law reduces the platform base loads while increasing the blade root flap wise load for the astw cibp the tower base side to side and fore aft loads have similar reductions than astw cbp while the torsional load increases by 3 nonetheless fig 12 shows the torsional load is very reduced comparing to the side to side and fore aft loads of tower base then an increasing of 3 is meaningless for the load of tower furthermore astw cibp reduces the blade root flap wise load 1p load see fig 14 generally astw cibp control strategy has not only better performance on the rotor speed power regulation and platform pitch motion reduction than gspi cbp as astw cbp but also can reduce the fatigue load of blade all of which being crucial problems of the floating wind turbine control moreover this controller requires very few knowledge of system model and the controller gains can be dynamically adapted with the uncertainties and perturbations see fig 15 that largely reduced the parameters tuning effort however such improvement has a cost that is a more aggressive actuator use as shown by fig 11 the variation of astw cbp increases by 82 versus cbp gspi whereas it is worst with astw cibp controller notice that given that the dynamics of blade pitch actuators is taken into account in the simulations such a use of these actuators is practically acceptable see fig 16 6 conclusion super twisting algorithms with gain adaptation algorithm based on collective individual blade pitch control are applied to the floating wind turbines control problem in above rated region such control algorithms strongly reduce the workload of parameters tuning only few knowledge of system model is required that makes such control strategy well adapted to the floating wind turbine systems the control goals are the regulation of the rotor speed the reduction of the platform pitch motion and the reduction of the fatigue load of the blades the simulations made on fast software show that the collective control loop and individual blade pitch control loop are well decoupled by the mbc transformation then the cibp based astw algorithm gives not only better performances on the power regulation and platform pitch motion reduction than cbp controllers but also provides better performances on the blade load reduction future works will be focused on the application of the proposed control solution on experimental set up and on the use of simpler adaptive super twisting algorithm zhang et al 2021 credit authorship contribution statement cheng zhang conceptualization simulation draft writing franck plestan supervision conceptualization reading writing reviewing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work is a part of the ph d thesis of cheng zhang who has been supported by chinese scholarship council csc furthermore this work has been supported by weamec program from région pays de la loire france thanks to o2grace grant appendix the controller gains of gspi cbp controller can be found in jonkman et al 2009 jonkman 2008 whereas the gains of the astw cbp controller can be found in authors previous work zhang et al 2019b see table 2 
20841,a new control strategy based on adaptive second order sliding mode approach is applied to a floating wind turbine system in the above rated region this adaptive controller is well adapted to a highly nonlinear system as floating wind turbine and can be easily implemented with very reduced knowledge of modeling the proposed controller partially based on multi blade coordinates transformation combines collective and individual collective blade pitch control for power regulation platform pitch motion reduction and reduction of blades fatigue load the proposed controller is implemented on fast simulator and shows high level of performances keywords second order sliding mode adaptive control floating wind turbine blade pitch control 1 introduction floating wind turbines fwts allow the use of the huge wind resource in ocean area and are considered as a promising solution of renewable energy however some issues arise first of all unlike the onshore wind turbine the floating platform introduces additional degrees of freedoms dofs that have negative impacts especially on the platform pitch motion they also induce the issue of negative damping nielsen et al 2007 that leads to system instability and degrades the power production furthermore with the increasing capacity and flexibility of wind turbines fatigue loads of the structure became more and more important especially for floating systems and affect the service life therefore reducing the fatigue loads is a key point bossanyi 2003 menezes et al 2018 for large scale wind turbines the control strategy must provide an efficient solution for such problems and appears crucial for wind turbine systems the main control objectives of fwt in the above rated region consist in maintaining the power output at rated value meanwhile avoiding the negative damping i e reducing the platform pitch motion jonkman et al 2009 many works have been made based on the collective blade pitch cbp control strategy in this case all the blades of the wind turbine are controlled by a similar way among existing results in jonkman 2008 the famous baseline gain scheduling proportional integral gspi control is proposed platform pitch motion is successfully reduced but with a large power fluctuation in wakui et al 2017 a novel gain scheduling control strategy is developed improving the power regulation while keeping the same platform pitch motion as gspi linear quadratic regulator control model predictive control and feed forward control namik et al 2008 schlipf et al 2012 2015 have been also applied to fwt systems however fatigue load reduction is not considered in those works in terms of the fatigue load reduction individual blade pitch ibp control bossanyi 2003 lio et al 2018 ossmann et al 2017 selvam et al 2009 has been introduced in this case the blades are independently controlled this approach has also been extended to floating ones cunha et al 2014 suemoto et al 2017 namik and stol 2014 nonetheless in all these works the control design is based on linearized models around an equilibrium point obtained by the fast fatigue aerodynamics structures and turbulence software jonkman et al 2005 these approaches based on linearization around an operating point are in opposition with the use of fwts in a large operating domain hence in order to avoid this drawback different models for different equilibrium points are required that induces design of different controllers producing a large effort of parameter tuning an other solution could be the use of nonlinear control strategies based on nonlinear models however given that the nonlinear models of fwt homer and nagamune 2018 jonkman et al 2005 sandner et al 2012 are not well adapted to the control design there are few studies on the nonlinear control of fwt as detailed just below a solution consists in designing nonlinear control laws that are efficient on a large operating domain but without precise models in order to reduce as much as possible the modeling effort sliding mode control smc utkin 1977 is a well adapted solution given its robustness versus uncertainties and perturbations then in the current work smc is applied for the floating wind turbines control smc requires very limited knowledge of system model especially in its adaptive version while keeping robust versus uncertainties and perturbations such control algorithms ensure that the sliding variable that is defined from the control objectives converges to a vicinity of the origin in a finite time however due to the discontinuous feature of standard smc control chattering phenomenon i e high frequency oscillations of control input appears in order to reduce the chattering effect super twisting stw shtessel et al 2014 control combined with gain adaptation plestan et al 2010 shtessel et al 2012 is applied in this study the stw is one of most famous second order sliding mode control that generates continuous control input and thereby reduce the chattering furthermore stw only requires the knowledge of sliding variable then it can be viewed as an output feedback control and is very simple to implement furthermore adaptive gain allows to keep control accuracy versus perturbations and uncertainties even in the case that the information of system model is very reduced in fact only the relative degree isidori 1999 of the sliding variable is required hence such algorithm is very well adapted to the fwt control problem notice that in authors previous works zhang et al 2019a b zhang and plestan 2020 super twisting control with gain adaptation based on cbp control technology has been successfully applied to fwt system here the first novelty is based on the fact that ibp and cbp control structures are combined to control the power and to reduce the fatigue load of the wind turbine especially the blade load reduction among the structure loads the blade root reduction is the most important being the source of the loads for the rest of the structures jelavić et al 2010 an other novelty is the use of an adaptive second order sliding mode controller in the frame of ibp cbp control strategy section 2 introduces the model of the fwt section 3 states the control problem section 4 describes the stw control laws with adaptation laws based on both cbp and ibp control approaches and their application to the fwt section 5 displays the results obtained by fast simulink co simulations and their analysis 2 system modeling the national renewable energy laboratory nrel 5mw oc3 hywind floating wind turbine see fig 1 is selected in this study this wind turbine is simulated by the well known wind turbine simulation software fast jonkman et al 2005 the detailed parameters and the properties being given in jonkman et al 2009 jonkman 2010 however the wind turbine model used in fast is composed by a large number of complex nonlinear functions and cannot be adopted for control design fast software can provide a linear state model of the fwt around an operating point that depends on the wind speed and the rotor speed in this case for a given operating point one can obtain the following state space model 1 x a x b u b d δ with x the system state vector depending on the dofs enabled during the linearization process u β 1 β 2 β 3 t the control input vector i e the pitch angle of each of the three blades and δ the wind perturbation input matrices a b and b d depend on the considered dofs and are obtained from fast depending on the operating conditions considering that the above rated wind speed of nrel 5mw wind turbine is varying between 11 4 m s and 25 m s in such large operating domain different linear models i e different matrices a b and b d must be carried out depending on the variation of the wind speed remark 1 as detailed in the sequel only 2 dofs are considered for the control design in this case the state vector is composed by the platform pitch angle its velocity and the rotor speed the matrices a b and b d are provided by fast software for each operating point as an example considering a wind speed equal to 16 m s and a rotor speed equal to 12 1 rpm they are 2 a 0 1 0 0 0141 0 0402 0 0003 0 0525 2 0615 0 1624 b 0 0 0033 0 9479 b d 0 0 0001 0 0253 thus the fwt modeling on the whole above rated region reads as 3 x a t x b t u b d t δ by a more general point of view system 3 can be represented as a class of nonlinear system 4 x f x t g x t u with f x t including the term a t x and the associated perturbations b d t δ and uncertainties as g x t including the term b t it is important to notice that in the sequel f and g are viewed as unknown but bounded functions the main purposes of the controllers designed in this paper are the limitation of the power at its rated value the reduction of the platform pitch motion and the attenuation the blade flap wise root moment the two first objectives can be achieved by the cbp control while the third one is fulfilled by ibp control since the cbp and ibp control can be separately designed as two independent control loops see details in the sequel two models are carried out notice that fast provides dozens of degree of freedoms dofs including the motions of tower blades and platform the rotation of rotor the yaw motion these models are too complex and large for control design then in order to simplify the control design reduced models are considered however in the sequel notice that all the simulations will be made by applying the controllers designed on reduce models to the full dofs model running in fast 2 1 reduced cbp control model this model is focused on the platform pitch and the rotor concerning the control objectives of cbp controller only two dofs the rotor rotation and the platform pitch are considered based on a similar writing as 4 the model for cbp control reads as 5 x c f c x c t g c x c t u c with x c φ φ ω t φ being the platform pitch angle φ the platform pitch velocity and ω the rotor speed the cbp control adjusts each blade pitch angle by the same amount simultaneously hence the control input is defined as u c β c o l this angle being applied to each blade 2 2 reduced ibp control model this model is focused on the blade behavior in this case 3 dofs are enabled i e the first flap wise bending mode of each blade hence the state vector includes the flap wise bending deflection of each blade q 1 2 3 see q 1 for blade 1 fig 2 right and its corresponding velocity q 1 2 3 i e x i q 1 q 2 q 3 q 1 q 2 q 3 t the control input vector u i β 1 β 2 β 3 t is such that each blade pitch angle has its own value as previously the system model can be written as 4 6 x i f i x i t g i x i t u i however the dynamics of each blade depends on the azimuth angle ψ i e the angle between a vertical axis and the current position of the blade symmetrical axis see fig 2 left which induces a periodic system therefore analysis and control design could be not straightforward in order to avoid the periodic dynamics the most conventional method in ibp control is the application of the multi blade coordinate mbc transformation bir 2008 also known as coleman transformation mbc transformation allows to write the dynamics into a fixed non rotating frame by this way the controller is designed without considering the periodic property such coordinates transformation also allows to decouple the ibp control that is focused on load reduction from the cbp control stol et al 2009 notice that the control based on mbc transformation has almost same results as the directly periodic control stol et al 2009 but without complexity consider the following state coordinates transformation called mbc transformation bir 2008 7 x i n r t x i with 8 t 1 3 2 cos ψ 2 cos ψ 2 π 3 2 cos ψ 4 π 3 2 sin ψ 2 sin ψ 2 π 3 2 sin ψ 4 π 3 then applying this transformation to 5 gives the following system that will be called in the sequel mbc system 9 x i n r f i n r x i n r t g i n r x i n r t u i n r with x i n r q t i l t q y a w q t i l t q y a w t and u i n r β t i l t β y a w t the state and input vectors in the non rotating frame q t i l t and q y a w the fictitious tilt and yaw component of blade flap wise deflections respectively β y a w and β t i l t the yaw and tilt component of blade pitch angles 3 problem statement recall that the control objectives of the current study are to ensure the power output at rated meanwhile reducing the platform pitch motion and reducing the flap wise load of blades in authors previous works zhang et al 2019a b both the first control objectives power platform pitch motion are achieved by collective blade pitch control here the blade load especially the blade flap wise load alleviation is also considered and can be ensured by separately adjusting the pitch angle of each blade namely by using the individual blade pitch control bossanyi 2003 selvam et al 2009 van engelen 2006 the overall control scheme is shown in fig 3 the ibp angle adjustment is added to the cbp control input but has a limited effect on the global behavior of the power and platform pitch motion in other words there is a very reduced coupling between the cbp and ibp control bossanyi 2003 jelavić et al 2010 hence these latter can be separately designed as two independent control loops while achieving their own control objectives 3 1 formalization of the collective blade pitch control problem the task of cbp control loop is to regulate power at rated p 0 meanwhile reducing the platform pitch motion usually 1 1 in the paper for a sake of simplicity the generator torque is supposed to be constant the objective being to focus the attention on the control of the hydrodynamic part of the wind turbine however the authors are fully aware that it is necessary to also consider the control by generator point of view that will be the object of future works the generator torque is supposed to be fixed at its rated t g 0 in the above rated region then the power regulation turns into regulate the rotor speed at its rated value ω r ω r p 0 n g t g 0 with n g the gear box ratio between high speed shaft and low speed shaft however there are two control objectives with a single control input that is the collective blade pitch angle β c o l a solution to such problem regarding to the fwt control is to modify the rotor speed reference by including the platform pitch rate φ that gives a new rotor speed reference ω defined as 10 ω ω r k φ with k a positive constant such solution takes advantage of the physical features of the rotor rotation and platform pitching in response to aerodynamic torque and thrust consider that forward pitching is appearing and suppose that the control is efficient in this case φ 0 in order to fulfill 10 thank to the action on the blade pitch angle that increases the aerodynamic torque of blades the rotor speed increases and is greater than ω r at the same time the aerodynamic thrust increases preventing the platform forward pitching therefore the platform pitch velocity φ will be reduced and thereby the rotor speed will converge to the rated lackner 2009 cunha et al 2014 as conclusion the controlled output associated to the system 5 is defined as 11 y c ω ω remark 2 from remark 1 and zhang et al 2019b zhang and plestan 2020 it can be verified that in the considered operating domain the relative degree isidori 1999 of system 5 with the previous output y c is equal to 1 i e the first time derivative of y c explicitly depends on the control input u c 3 2 formalization of the individual blade pitch control problem the rotor of wind turbine transforms the wind power into aerodynamic torque that drives the generator at the same time partial wind energy is transformed into thrust on the rotor that induces load due to the wind shear tower shadow and turbulence the wind speed and direction are varying across the rotor plane these factors cause additional loads on the blades these loads are related with the frequency of the rotor speed and can be decomposed along different modes the main one being at the rotor speed frequency this mode is denoted the 1p mode once per revolution see fig 9 other modes are existing at multiples of rotor speed and are denoted 2p 3p bossanyi 2003 the reduction of the 1p mode for each blade appears being a main objective of ibp control in this regard the flap wise bending moment of each blade are considered as the outputs of ibp control loop consider the mbc system 9 and denote the control output y i n r m t i l t m y a w t with m t i l t and m y a w respectively the tilt and yaw component of blade root flap wise moment see fig 4 as shown in wang et al 2016 xiao et al 2013 this output can be written as 12 y i n r h i n r x i n r t l i n r x i n r t u i n r remark 3 notice that the output y i n r depends on the control input vector u i n r in this case the relative degree of system 9 with output y i n r is equal to 0 the main idea of ibp control is to force the magnitudes of m y a w and m t i l t close to zero that reduces the blade flap wise load mbc approach allows the decoupling between the ibp control that is responsible for load reduction and the cbp control furthermore it has been shown bossanyi 2003 that m y a w and m t i l t can be treated independently by β y a w and β t i l t respectively i e it is possible to use two single input single output controllers for the m y a w and m t i l t alleviation hence the control objectives of ibp control are to ensure the m t i l t and m y a w close to zero by the control β t i l t and β y a w respectively notice that the following inverse mbc transformation should be applied after the controllers design in order to generate ibp control command β 1 β 2 and β 3 see fig 5 13 β 1 β 2 β 3 t t 1 β y a w β t i l t t t 1 c o s ψ s i n ψ c o s ψ 2 π 3 s i n ψ 2 π 3 c o s ψ 4 π 3 s i n ψ 4 π 3 3 3 overall control scheme by a structural point of view the overall control scheme is the combination of cbp control and ibp control then the overall control system design process can be summarized as follows design the cbp control β c o l for regulation of the power and reduction of the platform pitch motion transform the three flap wise blade flap wise bending moments m y 1 m y 2 and m y 3 into the fictitious ones m y a w and m t i l t and design the control loop that provides β y a w and β t i l t respectively and obtain the components β 1 β 2 and β 3 thanks to the inverse mbc transformation the real blade pitch angles β 1 β 2 and β 3 that are the control inputs equal to the sum of β c o l with β 1 β 2 and β 3 4 control design as previously detailed floating wind turbine is a class of nonlinear system with model uncertainties and perturbations that introduced from the flexible structures wind and waves furthermore the traditional controllers of fwt based on the linearized model such as lqr mpc and gspi need great effort of tuning due to the large set of operating points in order to keep high performances all over the operating domain then there is a real interest to design a robust controller with a reduced tuning effort and a single set of parameter tuning meanwhile keeping the control efficiency among the large operating range despite of the uncertainties and perturbations the robust nonlinear strategy selected in this work is based on sliding mode control smc theory utkin 1977 utkin et al 2009 a well known nonlinear control strategy with properties of robustness accuracy and finite time convergence in fact the standard first order smc can be easily implemented however the control of standard smc is discontinuous due to the discontinuous term of the control input chattering is introduced and can damage the physical components such as blade pitch actuator in order to reduce chattering and keep robustness super twisting stw levant 1993 control is introduced furthermore given the unknown terms of system dynamics the uncertainties on turbine and the perturbations the gains of the controller must be chosen sufficiently large to accommodate those effects this fact gives a high gain control that is not good for chattering reduction therefore adaptive version of stw shtessel et al 2012 is well adapted it allows to dynamically adapt the gain versus uncertainties and perturbations while keeping high level of performances even in case of very reduced knowledge of the system 4 1 recalls consider the nonlinear system 14 z f z t g z t υ y h z t with z z r n the state vector and υ u r the control input f z t and g z t the bounded unknown uncertain functions y the control output i e the control objective is to force y to 0 define the sliding variable σ σ z t such that control objective is achieved when σ 0 the objective of the control design is to define a control input υ that drives the sliding variable σ to the sliding surface σ z t 0 in a finite time despite the uncertainties and perturbations note that sliding variable is defined according to control objective y and its relative degree isidori 1999 assumption 1 the relative degree of 14 is equal to 1 define σ y its dynamic reads as 15 σ σ t σ z f z t a z t σ z g z t b z t υ assumption 2 a z t and b z t are unknown but bounded functions such that a a m 0 b m b b m for z z and t 0 a m b m and b m being the positive constant as mentioned above standard smc with sufficient large controller gains can establish a sliding mode namely drive σ to zero in a finite time but with important chattering the super twisting algorithm stw levant 1993 given by 16 υ k 1 σ 1 2 sign σ w w k 2 sign σ with k 1 and k 2 the controller gains satisfying 17 k 1 a m b m k 2 2 4 a m b m 2 b m b m k 1 a m k 1 a m ensures the establishment of a second order sliding mode i e σ σ 0 in a finite time in practice especially due to sampling period only real second order sliding mode is established that is defined as levant 1993 18 σ μ 1 t e 2 σ μ 2 t e with t e the sampling time of controller μ 1 and μ 2 positive constants thanks to the continuous nature of stw algorithm the chattering is greatly reduced while the robustness is kept however due to the uncertainties and perturbations of the real systems sufficiently large gains are necessary such gains are always overestimated which limits the interest of this control strategy to this end gain adaptation is adopted in order to further increase the control performance the adaptation law dynamically adapts the controller that avoids the gain overestimation and strongly reduces the effort of uncertainties perturbations evaluation adaptive super twisting astw controller proposed in shtessel et al 2012 is used here the adaptive law being defined as 19 k 1 ω χ 2 sign σ μ if k 1 k 1 m k if k 1 k 1 m k 2 ϵ k 1 with k 1 m ϵ ω χ μ k positive constants and k 1 0 k 1 m from 19 one can find that σ μ the control accuracy is high so the controller gains can be reduced because they are certainly sufficient k 1 is negative σ μ the control accuracy is low the gains could be too small it is necessary to increase the gains in order to improve the accuracy k 1 is positive the parameter k 1 m is a very small positive constant that ensures the positiveness of controller gains remark that ω and χ determine k 1 dynamics and also k 2 dynamics given that k 1 and k 2 are proportional if they are stated large k 1 will strongly increases resp decreases when real second order sliding mode is lost resp established large dynamics of k 1 and k 2 could induce large variations of the control input it could be damageable for the actuator ϵ defines the ratio between k 1 and k 2 that depends on the system μ acts on the accuracy of the close loop system see the items following 19 a smaller μ means a higher accuracy but induces a more intensive control given that it gives a larger gain k 1 on the other hand a larger value for μ means a lower accuracy in this case the control gain is smaller that gives a less intensive control 4 2 application to the floating wind turbine system as detailed in section 3 the control scheme includes two control loops the first one is the cbp control loop focusing on the control of rotor speed and platform pitch motion the second one is the ibp control loop producing an additional term to each blade pitch angle in order to reduce the variation of blade root flap wise bending moments as previously recalled these two control loops can be independently designed bossanyi 2003 jelavić et al 2010 cbp loop as claimed in remark 2 the relative degree 5 with y c 11 is equal to 1 therefore according to assumption 1 the sliding variable of cbp control can be defined as 20 σ 1 y c ω ω ω ω r k φ ibp loop as recalled in remark 3 the relative degree of system 9 with output y i n r is equal to 0 given that astw algorithm must be applied to system with relative degree equal to 1 consider again the system 9 21 x i n r f i n r x i n r t g i n r x i n r t u i n r with 22 y i n r m t i l t m y a w h i n r x i n r t l i n r x i n r t u i n r a solution consists in defining a dynamic control input that increases the relative degree of the system defining 23 x i n r u i n r and v i n r u i n r the new control input system 9 can be reformulated as 24 x i n r f i n r x i n r t g i n r x i n r t x i n r x i n r v i n r y i n r h i n r x i n r t l i n r x i n r t x i n r with v i n r β t i l t β y a w t the new control input then the relative degree of 24 with respect to β t i l t β y a w t is equal to 1 therefore astw algorithm can be applied the sliding variables of ibp loop are defined as σ 2 σ 3 t m t i l t m y a w t fig 5 depicts the ibp control scheme then one has the sliding variable vector 25 σ σ 1 σ 2 σ 3 ω ω r k φ m t i l t m y a w and its dynamics reads as 26 σ a b v with a and b unknown but bounded functions obtained from 5 24 the control input v is defined as 27 υ β c o l β t i l t β y a w t υ 1 υ 2 υ 3 t with 28 υ 1 υ 2 υ 3 k 11 σ 1 1 2 sign σ 1 0 t k 12 sign σ 1 d τ k 21 σ 2 1 2 sign σ 2 0 t k 22 sign σ 2 d τ k 31 σ 3 1 2 sign σ 3 0 t k 32 sign σ 3 d τ the gains k 1 and k 2 1 2 3 are evolving according to adaptation law 19 5 simulation results the nonlinear oc3 hywind 5mw floating wind turbine model from nrel especially controlled by the previously detailed controller is simulated in this section the used model is built in the fast software and is regarded as a benchmark in many wind turbines studies the parameters of this wind turbine are shown in table 1 the control is developed in the simulink environment and link with the fast model by an s function finally the co simulations between fast and simulink are made on the full dofs fast nonlinear model while the control has been designed based on the reduced dofs model as detailed previously three controllers are used in the following simulations gspi cbp the baseline gspi controller with collective blade pitch control jonkman 2008 astw cbp the adaptive super twisting controller with collective blade pitch control in this case only υ 1 28 is considered zhang et al 2019b astw cibp the adaptive super twisting controller 28 that combines collective with individual blade pitch control the use of these controllers has several objectives comparison between standard gspi and advanced controllers stw and comparison between cbp and cbp ibp control structures in addition two cases of wind and wave conditions are simulated in the sequel case 1 18 m s constant wind velocity with still water i e no wave case 2 18 m s wind velocity with 15 turbulence intensity irregular wave with significant height of 3 25 m and peak spectral period of 9 7 s see fig 6 note that the wind speed of both cases are in above rated region all the simulations are made in 10 min and euler integration is used with sample time fixed at 0 0125 s moreover the blade pitch angle and its rated are saturated as shown in table 1 5 1 case 1 constant wind velocity with still water in this case the astw cbp and astw cibp control strategies are compared the objective being to check the interest to include a ibp control loop firstly fig 7 displays that both the cbp and ibp controllers ensure the rotor speed around its rated value 12 1 rpm and reduced the platform pitch motion i e reduced the platform pitch angle variation furthermore fig 8 shows that the tilt and yaw moment are forced around zero with the cibp controller as a consequence the blade root flap wise moment are strongly reduced comparing with the cbp controller see fig 8 right specifically from the power spectral density psd of blade 1 root flap wise moment displayed by fig 9 one can find that the load reduction of ibp control is acting on the 1p component of the blade load meantime the rotor speed and platform pitch motion are not affected as shown in fig 7 the trajectories of cbp and cibp control are highly coincidence as mentioned in previous section the collective pitch control and the individual pitch control are decoupled fig 10 shows the blade 1 pitch angle obtained with the two controllers 5 2 case 2 turbulent wind and irregular wave previous case shows that both the astw controllers allow to achieve all the control objectives in ideal conditions and proves the necessity to introduce a ibp control loop case 2 now allows to evaluate the controllers gspi cbp astw cbp astw cibp in a more realistic situation the performance of the 3 controllers are compared by using the following indicators root mean square rms of rotor speed error power error platform angular rates these values evaluate the accuracy of the closed loop system for the 3 controllers variation var wang et al 2014 of the blade pitch angle this value evaluates the activity of the actuator damage equivalent load del hayman 2012 of the tower base fore aft side to side and torsional moment and del of averaged blade root flap wise and edge wise bending moment of the three blades these values evaluate the level of mechanical constraints acting on the blades fig 11 shows that for two of the main control objectives rotor speed power regulation and platform pitch motion reduction astw cbp and astw cibp controllers have similar performances allowing reduction of rotor speed error by 8 9 and platform pitch rate by 22 23 versus gspi cbp as previously mentioned there is no coupling between cbp and ibp control loops hence astw cbp and astw cibp have similar performances on rotor speed and platform pitch rate as shown by fig 12 the time series of astw cbp and astw cibp in terms of rotor speed power and platform pitch angle are almost identical furthermore astw cbp controller has also reduced the platform roll and yaw rate on the contrary astw cibp controller induces more important platform roll and yaw rate due to the greatly increased blade pitch actuation namik and stol 2014 however given that the magnitude of platform roll and yaw are relatively small see fig 12 they have a very limited influence on the stability of the whole system fig 13 shows the del results it is clear that astw cbp control law reduces the platform base loads while increasing the blade root flap wise load for the astw cibp the tower base side to side and fore aft loads have similar reductions than astw cbp while the torsional load increases by 3 nonetheless fig 12 shows the torsional load is very reduced comparing to the side to side and fore aft loads of tower base then an increasing of 3 is meaningless for the load of tower furthermore astw cibp reduces the blade root flap wise load 1p load see fig 14 generally astw cibp control strategy has not only better performance on the rotor speed power regulation and platform pitch motion reduction than gspi cbp as astw cbp but also can reduce the fatigue load of blade all of which being crucial problems of the floating wind turbine control moreover this controller requires very few knowledge of system model and the controller gains can be dynamically adapted with the uncertainties and perturbations see fig 15 that largely reduced the parameters tuning effort however such improvement has a cost that is a more aggressive actuator use as shown by fig 11 the variation of astw cbp increases by 82 versus cbp gspi whereas it is worst with astw cibp controller notice that given that the dynamics of blade pitch actuators is taken into account in the simulations such a use of these actuators is practically acceptable see fig 16 6 conclusion super twisting algorithms with gain adaptation algorithm based on collective individual blade pitch control are applied to the floating wind turbines control problem in above rated region such control algorithms strongly reduce the workload of parameters tuning only few knowledge of system model is required that makes such control strategy well adapted to the floating wind turbine systems the control goals are the regulation of the rotor speed the reduction of the platform pitch motion and the reduction of the fatigue load of the blades the simulations made on fast software show that the collective control loop and individual blade pitch control loop are well decoupled by the mbc transformation then the cibp based astw algorithm gives not only better performances on the power regulation and platform pitch motion reduction than cbp controllers but also provides better performances on the blade load reduction future works will be focused on the application of the proposed control solution on experimental set up and on the use of simpler adaptive super twisting algorithm zhang et al 2021 credit authorship contribution statement cheng zhang conceptualization simulation draft writing franck plestan supervision conceptualization reading writing reviewing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work is a part of the ph d thesis of cheng zhang who has been supported by chinese scholarship council csc furthermore this work has been supported by weamec program from région pays de la loire france thanks to o2grace grant appendix the controller gains of gspi cbp controller can be found in jonkman et al 2009 jonkman 2008 whereas the gains of the astw cbp controller can be found in authors previous work zhang et al 2019b see table 2 
20842,the effects of a high speed frigate model passing close to a moored container model alongside a closed quay are experimentally and numerically investigated with a focus on ship generated waves transient ship motions and hydrodynamic forces acting on the moored ship scaled model experiments are conducted to investigate the hydrodynamic interactions between the passing ship generated waves and the moored ship a rans based cfd method is applied to solve the hydrodynamic problem including the effect of the free surface the overset mesh technique is adopted to simulate the relative movements between the two vessels the numerical method is compared with experimental results for the ship generated waves and total resistance in open water then validated for the passing ship problem in restricted water numerical simulations are carried out to study the effect of different parameters on the passing ship effects results show that water depth passing speed and separation distance are important parameters that influence the transient interaction the shape of hydrodynamic interaction forces and moments on the moored ship at relatively high speeds is different from the well known behaviour at slow speeds in practice the most effective measure to reduce the effects of passing ship is to control the speed of the passing ship keywords ship generated wave ship ship interaction passing ship moored ship overset mesh rans 1 introduction the development of maritime transportation and activities results in large ship traffic in harbours which increases the probability of moving ships to pass by moored ships also the introduction of fast ferries increased the concerns regarding the impact of the ship generated waves on the moored ships therefore the passing ships and their generated waves may introduce significant hydrodynamic forces on the moored ships that end up with large motions which may cause accidents for crew or passengers and damage to the infrastructures swiegers 2011 in 1990 the suction effect from a passing vessel caused the total loss of oil tanker jupiter due to fire during unloading cargo at a dock in michigan seelig 2001 pinkster and keuning 2013a showed that 40 of the damages in the mooring equipment of inland vessels were due to the passing ship interactions small ships and barges can also be in danger of capsizing due to large shoaling waves generated by a passing ship also the undesirable large motions would result in significant mooring forces and breakdown of the cargo handling operation between the ships and quay inside ports and channels for the above reasons the passing ship problem should be one of the parameters to be considered in the design of ships channels and ports predicting the passing ship effect on a moored correctly in the ship design phase is an attractive choice to assure the safety of operations and efficiency of activities in ports the passing ship effect on a moored ship is classified in the literature into two different hydrodynamic problems according to the passing speed the first problem is at low speeds which is attributed to the low frequency hydrodynamic pressure variation force due to the presence of the passing ship which is the dominant part at low speeds it is called as low speed near field effect which is the case for most of the large merchant displacement ships where the problem can be formulated using double body flow assumption to avoid the complexity of mesh due to free surface modelling korsmeyer et al 1993 the key elements for this problem are the displacement of the passing ships passing distance speed and the flow interferences created from bottom and quay effects double body flow can indeed form a good assumption for this problem but there are also some cases where the low frequency hydrodynamics are not well represented by double body flow such as the standing wave generation seiche or solitary wave of ships entering canals pinkster 2004 the second problem is at high speeds although it still includes the near field contribution it is mainly focusing on the interaction with the high frequency waves that are generated by passing ships at high froude number it is called the high speed far field effect which must account for the free surface and can be described by the forces on the moored ship due to the passing ship generated waves taking into account the interactions of these waves with the moored ship and its surrounding geometry nam and park 2018 the scale model experiment is one of the most suitable methods to study the passing ship effects remery 1974 presented a scaled model passing ship test with a captive moored model at an open jetty in shallow water potential flow based methods have been applied to study the passing ship effects on moored ships wang 1975 applied the slender body theory to solve the problem of ship ship interaction with a two dimensional flow solution in deep water by simplifying the ship hull as a slender body with parabolic cross sections and assumed the free surface as a rigid wall yeung 1978 applied the same theory to investigate the unsteady hydrodynamic interaction of two slender ships in shallow water gourlay 2009 applied yeung s method to predict the sinkage and trim of two moving ships passing each other in open water at a shallow constant depth flory 2002 vantorre et al 2002 and kriebel 2007 developed empirical formulas for the interaction forces and moments including the shallow water effects based on model tests swiegers 2011 conducted several scaled model tests for a passing ship effect on a captive moored model including open water case quay wall and channel effects comparisons of results for surge and sway forces as well as yaw moments with several existing empirical formulas show that model tests are still needed few experimental studies focused on the high speed problem such as pinkster 2009 and pinkster and keuning 2013b where the moored ship motions due to wash waves are obtained based on the experimental data of the so called wave cut of the wash wake pattern recorded at a short distance from the centreline of a passing fast ferry most of the numerical studies on passing ship effects on moored ships assumed that the two ships are close to each other and the ship is passing with a speed corresponding to low froude numbers pinkster 2004 focused on the low frequency pressure wave forces on a captive moored ship induced by a passing ship at relatively low speeds two potential flow numerical models were presented based on the three dimensional boundary element method double body numerical model and another free surface included model were adopted to predict the pressure wave forces due to a passing ship where the potentials describing the flow are based on rankine source formulation both methods are subsequently applied to different interaction cases results show that the mooring forces obtained in open water simulations should not be used as an approximation for the cases that include quay walls channels or complex harbour geometries a similar numerical model was proposed by sutulo and guedes soares 2008 2012 and validated by sutulo et al 2012 for the interaction between ships in real time where the free surface is treated as a rigid wall in the three dimensional potential flow method the model was further extended by zhou et al 2012 zhou et al 2014 and zhou et al 2016 to simulate shallow water case with both flat and arbitrary seabed topology furthermore ren et al 2020 extended the numerical model to account for sinkage and trim pinkster and pinkster 2014 introduced the three dimensional double body numerical model ropes that neglects free surface to predict the hydrodynamic forces on moored ships comparison with results of captive moored models presented by talstra and bliek 2014 showed that the free surface effects cannot be neglected at high speed froude number greater than 0 3 at high speeds the passing vessel generates waves that are capable of surviving and travelling for long distances with little dissipation in energy and excite the moored ship with high interaction transient forces van der molen and wenneker 2008 pinkster 2009 studied the harbour oscillation effects on a captive moored ship due to the interaction of either the suction wave created by a slow passing ship or ship generated waves created by a fast passing ship including the port geometry dong et al 2009 presented a numerical model that studies the effect of the ship generated waves on a floating hemispherical body taking into account the free surface effect at relatively high speeds the numerical model coupled the thin ship theory with the three dimensional boundary element method to evaluate the effect of the ship generated waves on a moored body in the time domain the model assumed that the effect of the ship generated wave will not change due to the interaction with a moored body apart from captive moored ship studies some numerical models studied the full dynamics of the moored vessel including lines and fenders li et al 2018 proposed a hybrid numerical model that combined the three dimensional boundary element method and impulse response theory to estimate the transient response of a moored ship exposed to wash waves and sea waves nam and park 2018 presented a three dimensional potential flow numerical model in the time domain based on the classical finite element method it solves the hydrodynamic problem of a passing ship with a barge moored alongside a quay using the multi zone mesh approach proposed by nam et al 2016 results show that the ship generated waves strongly interact with the barge and quay which increases the hydrodynamic forces acting on the barge at froude numbers beyond 0 3 yuan 2019 presented a three dimensional potential flow solver based on the rankine type green function it evaluates the passing ship generated waves and hydrodynamic interaction forces for ships sailing in confined waterways with various restriction cases the limitations of the potential flow methods are determined by non viscous and irrotational assumptions few studies are focusing on the unsteady reynolds averaged navier stokes rans approach to find out the hydrodynamic interaction on a moored ship due to passing ship chen et al 2002 was the first author to develop a numerical model that uses the rans method to calculate the excitation forces on a moored ship at quay due to the parallel interaction of a passing ship in a canal huang and chen 2007 extended chen s code to simulate the ship motions in the rans solver and investigate the dynamic performance of a ship moored at a pier perpendicular to the navigation channel of the passing ship bunnik and toxopeus 2011 adopted a cfd based model using the rans method to study the effect of the drift angle of the passing ship on the hydrodynamic interaction forces on a moored ship results show that the viscous effect should be considered for angles higher than 7 5 wang and zou 2014 applied the commercial software ansys fluent to evaluate the hydrodynamic interaction forces generated by a ship entering or leaving a lock on another nearby berthed ship the dynamic mesh technique is applied and the free surface effect is neglected based on slow speed assumption lee 2015 presented rans calculations using openfoam to estimate the hydrodynamic forces generated by a passing ship on a moored ship the open source toolkit of the arbitrary coupled mesh interface acmi technique and volume of fluid vof method were applied to simulate the passing ship and capture the free surface pawar et al 2019 calculated the hydrodynamic forces on a captive moored ship due to a passing ship by using the commercial software star ccm where free surface modelling is neglected based on double body approximation fonfach et al 2011 and wnęk et al 2018 used different commercial cfd models to study the interaction between tug and tanker by applying four different flow models which include inviscid or viscous fluid assumption and with or without free surface effect kok et al 2018 presented both model scale and full scale study for the passing ship effect using the star ccm software results show that the differences between model scale and full scale interaction forces and moment predictions are not significant nandhini and nallayarasu 2020 presented a similar study using star ccm and the results show that the longitudinal and transverse forces on the moored vessel become negligible for separation distance more than three times the width of the vessel however most of these models neglected the free surface effect which is one of the key advantages of applying cfd method in passing ship problems at high speeds this paper presents the experimental program for scaled model tests to investigate the interactions between passing ships and moored ships at relatively high speeds subsequently numerical simulations are conducted using star ccm based on the unsteady reynolds averaged navier stokes solver it simulates the waves generated by a passing ship and calculates the hydrodynamic interaction between the passing ship and moored ship at high speeds considering the free surface effect and modelling of the full mooring system the overset mesh technique is adopted to simulate the relative movements between the two vessels the numerical method is verified and subsequently compared with experimental results for the ship generated wave and total resistance problem in open water then validated for the passing ship problem in restricted water the influences of different parameters as the transverse passing distance ship speed and water depth on the passing ship moored ship interaction are discussed 2 experimental work program a series of scaled model experiments are conducted in a towing tank 132m in length 10 8m in width and the depth can be adjustable at wuhan university of technology wut to investigate the interaction between a passing ship and a moored ship in calm water two standard ships are used for the tests the moeri container ship kcs as a moored ship and the standard us navy frigate dtmb5415 hull form as a passing ship the kcs moored to a frame that is fixed at one of the banks of the towing tank this frame has 6 m in length and used to simulate the quay wall effect the dtmb5415 model is attached to a towing carriage passing on a parallel course past the moored kcs model as shown in fig 1 it is accelerated to a steady speed and then meets with the moored ship the fluid is quiescent in the experiments the passing ship model is free to heave pitch and roll while the moored ship can move in 6 degrees of freedom the froude scaling is used for both models with a scale of 1 58 fig 2 shows the body plan of the kcs and dtmb5415 models the models are fabricated from wood and calibrated before tests to satisfy the characteristics of the loading condition shown in table 1 fig 3 shows the layout of the test configuration of the two ships in the towing tank the wave surface elevation is recorded at different fixed positions relative to the moored ship using a set of four wave probes as shown in fig 3 two wave probes are installed alongside the mid ship of the moored ship to evaluate the transverse propagation of the wake waves generated by the passing ship the other two wave probes are installed at the bow and stern of the moored ship the data obtained from the wave probes are recorded at a sample rate of 200 hz this paper presents the results for the test performed at a water depth of h 2 m and a passing distance of sd 3 m the dtmb5415 passing model moves with a constant speed along a straight path at a relatively high passing speed v 2 36 m s that corresponds to depth froude number frh 0 53 where the depth froude number f r h v g h is the ratio between the vessel speed and the maximum wave celerity fig 4 shows the layout of the kcs model with a generic mooring system attached to a fixed frame that is used to simulate quay wall effects eight mooring lines are arranged with names of 1 8 from the stern to the bow of the moored ship the scaled model diameter of each mooring line is 2 mm 116 mm in the prototype scale the mooring lines are defined according to ocimf guidelines to simulate synthetic nylon ropes for open terminals six super cell fenders suc2250h are simulated in the model test the fenders are fixed on the frame alongside the kcs model as shown in fig 4 fig 5 shows the force transducers attached to the mooring lines and fenders to measure the loads on mooring lines and fender reaction forces respectively the xsens inertial navigation system which is used to measure the moored ship motions has an accelerometer that gives the linear acceleration in three degrees of freedom as surge sway and heave also it has a gyroscope that gives the angular velocity of the roll pitch and yaw measurements are recorded at a sample rate of 200 hz the xsens system uses the measurements to compute accurately the 3d orientation in terms of roll pitch and yaw angles at a sample rate of 10 hz table 2 shows the scaled model length of each mooring line the physical models for mooring lines and fenders have geometrically similar properties to the prototypes table 3 shows the load deformation relation for the mooring lines and fenders in the prototype scale for example 50 deformation means that the mooring line elongates by 50 of its original length in table 2 at an applied load of 2490 kn the testing procedure starts with accelerating the passing ship to a pre determined constant speed before reaching the region that affects the moored ship the passing ship is tested at a relatively high speed 2 36 m s corresponds to frh 0 53 at a water depth of h 2 m and with transverse passing distance sd 3 m the constant passing ship speed is maintained until the passing ship reaches the end of the towing tank at least 45 min is determined between each test run to make sure that the water surface in the testing tank is calm enough to start the following test with similar laboratory conditions unfortunately the calculation for the experimental uncertainty is not carried out because the repetitions for the test at the presented testing parameters are not available 3 numerical method computational fluid dynamics cfd calculations can be a valuable solution method to study ship ship interaction effects when the hydrodynamic problem includes a significant viscous effect the cfd approach is important to be applied in this paper the numerical simulations are carried out with the commercial software star ccm that is based on the unsteady reynolds averaged navier stokes solver the following sections explain the formulation of the hydrodynamic problem and all necessary settings required for simulations 3 1 problem formulation and governing equations as the relative positions of the passing ship and the moored ship are continuously changing the hydrodynamic problem can be formulated concerning the earth fixed coordinate system the passing ship with a length lp is advancing at a constant forward speed v in the parallel course and passing by a moored ship of a length lm then the characteristic length lc can be defined as the average length of the two ships the right hand coordinate system is used in the presented model with the x axis is positive to the bow the y axis is positive pointing toward the port side of the ship and the z axis is positive upward and positioned at the still water surface above the centre of gravity of the ship as shown in fig 6 a the separation distance sd can be defined as the transverse distance between the sides of both passing and moored ships the instantaneous distance id is the longitudinal distance between the centre of gravity of the two ships at each time step the gap distance g is the transverse distance between the moored ship and the quay wall as shown in fig 6 b the distances between the two ships are represented in non dimensional form as a ratio of the characteristic length lc to evaluate the passing ship effects on a moored ship at various parameters the longitudinal relative position is expressed as p id lc while the separation ratio is defined as sr sd lc the direct simulation of the turbulence around the complex engineering structures is a demanding problem the flow filed of the turbulent flows is random in the function of time and space so the velocity fields will be expressed as the sum of mean and fluctuating parts to simplify the instantaneous incompressible navier stokes equations and obtain the reynolds averaged navier stokes rans equations the numerical simulations in the presented work are based on the following rans equations 1 ρ t ρ u i x i 0 2 ρ u i t ρ u i u j x i p x i ρ x j μ u i x j u j x i x j ρ u i u j where u i and u j i j 1 2 3 are the time averaged velocity components x i and x j i j 1 2 3 are coordinated in longitudinal transverse and vertical directions respectively ρ is fluid density p is the time averaged pressure μ is water dynamic viscosity and the last term in equation 2 represents the reynolds stress term 3 2 turbulence modelling an additional turbulent model is applied to represent the diffusion and mixing of turbulence in the flow field the realizable κ ε model is used for turbulence modelling which employs the solution of the following transport equations for κ and ε 3 ρ κ t ρ κ u i x i x j μ μ t σ κ κ x j g κ ρ 4 ρ ε t ρ ε u i x i x j μ μ t σ ε ε x j ρ c 1 e ε ρ c 2 ε 2 κ ν ε where μ t is the turbulent viscosity and σ κ 1 0 σ ε 1 2 c2 1 9 c 1 max 0 43 η η 5 the boundary conditions mentioned in section 4 1 are applied in the fluid domain then the generated equations are used together with continuity equations and rans equations to compute the total pressure and fluid velocity components in the three dimensional domain around the target body the integration of pressure and viscous stresses result in the hydrodynamic interaction forces and moments induced by the presence of the hull such as surge force sway force and yaw moment the hydrodynamic interaction forces and moments produced by the passing ship on the moored ship are presented in non dimensional form according to the following equations the sign convention for the forces and moments follows fig 6 b 5 c f f o r c e 0 5 ρ b m t m v 2 6 c m m o m e n t 0 5 ρ l m b m t m v 2 3 3 free surface zone the simulations of the passing ship effect at a relatively high speed require the incorporation of the free surface region in the simulations accordingly the volume of fluid vof method is applied to simulate the free surface region by describing two phases of fluid the evolution of the free surface in time is described by an additional transport equation for the volume fraction of the background fluid the convection term in the transport equation of the volume fraction is discretized by the high resolution interface capturing scheme hric the vof transport equation is formulated as the following equation 7 α t u u g α u r 1 α 0 where α distinguishes the fluid of two phases it represents the volume fraction as the relative proportion of the two phase fluid u stands for the velocity field while u g means the velocity of mesh points the value of the volume fraction indicates the presence or absence of the background fluid in the control volume when α 0 the fluid in the grid is air when α 1 the fluid is water the free surface is defined when the water and air are mixed at 0 α 1 3 4 dynamic fluid body interaction the six degrees of freedom solver dynamic fluid body interaction dfbi is used to simulate the responses of the moored ship due to waves generated by the passing ship dfbi is used to obtain the motion response of a ship under the impact of pressure and shear force the motions are solved by the solution of the governing equation for ship motions under different forces the equation for the translation of the centre of mass of the body is defined in the global inertial coordinate system by the following equation 8 m d v d t f where m represents the mass of the body f is the resultant force acting on the body and v is the velocity of the centre of mass the rotation of the body is formulated in the body local coordinate system with the origin in the centre of mass of the body the following equation 9 m d ω d t ω m ω n where m is the tensor of the moments of inertia ω is the angular velocity of the rigid body and n is the resultant moment acting on the body 3 5 mooring system model the catenary coupling model is adopted to simulate the mooring lines which is an elastic quasi stationary model that represents a line hanging between two endpoints being subject to its weight in the gravity field cd adapco 2016 the configuration of the mooring coupling model between the ship and the quay is shown in fig 6 a p 1 and p 2 are the two endpoints of the mooring line the direction of forces f 1 and f 2 are tangential to the line the horizontal components of forces are equal in magnitude and opposite direction in the local cartesian coordinate the shape of the mooring line can be obtained by the following equations 10 x a u b sin h u α 11 y a cosh u b 2 sin h 2 u β where u 1 u u 2 a c λ 0 g b c a d l e q c λ 0 l e q g sinh u 0 sinh u 1 g is the gravitational acceleration λ 0 and l e q are the quality of line and the relaxation length of line respectively d is the stiffness of the line α and β are two constants that depend on the location of the two endpoints and the total quality of the line and u 1 and u 2 are the locations of the two endpoints of the line and u can be obtained by the following equation 12 tan φ sinh u the forces and the effects on the line are shown in fig 6 a the tangential components of the forces f 1 and f 2 at the two endpoints can be calculated from 13 f 1 x c f 1 y c sinh u 1 f 2 x c f 2 y c sinh u 2 the fender simulation is not to be included in the presented model 4 numerical model setup 4 1 fluid domain and boundary conditions the description of the physical problem in the numerical model starts with the definition of the fluid domain and its boundary conditions as shown in fig 7 a the global computational domain is a large rectangular parallelepiped that is designed to comply with the experimental setup presented in the current paper the length of the computational domain is 12 times the length of mooed ship lm the width of the domain is 1 5 times the length of the moored ship plus the simulated transverse passing distance sd the breadth of both moored and passing ships and the gap distance g between moored ship and the quay as shown in fig 7 b the whole depth of the domain is 2 m as the air zone plus h where h is the simulated water depth the upstream face of the global domain is set as a velocity inlet while the downstream face is set as a pressure outlet standard atmospheric pressure the quay side wall is treated in the numerical model as one of the sides of the computational domain which is assigned with the no slip wall boundary condition the same no slip wall condition is applied on the bottom of the domain the other bank boundary and the hull of the passing and moored ships this boundary condition assumes zero relative velocity between the surface and the fluid flow at the surface however the simulations performed for the open water case defines both sides of the fluid domain with a velocity inlet boundary condition the presented numerical model studies the passing ship effects at relatively high speeds accordingly the contribution from the free surface effect is significant on the interaction forces and moments so the free surface is modelled with the vof method that represents a two phase fluid to track and locate the free surface other environmental loads due to wind sea waves and currents are not considered in the presented simulations 4 2 mesh technique the passing ship problem requires a huge computational domain with the movement of two floating bodies this domain can be divided into two regions one is around the passing ship which is defined as refined overset mesh another region includes the moored ship for which a coarser grid will be applied the grid of the computational domain in the presented numerical model is defined based on cd adapco 2016 the trimmed hexahedral mesh and prism layer mesh techniques are employed in the computational region this strategy provides a balanced solution for complex mesh generation problems they are relatively easy and efficient to build requiring no more surface preparation than the equivalent tetrahedral meshes trimmed meshes are composed of hexahedral mesh mostly the mesh density can be locally increased or decreased by using the volume mesh density factors to define the volumetric controls so that volumetric controls are applied to have better mesh refinement around the critical computational regions for example around the bow stern quay gap bottom gap and wake regions prism layer mesh is the mesh generated from the surface mesh to the fluid which makes the mesh at the wall more refinement and fulfils the requirements of y required for the definition of thickness of the boundary layer to apply the wall function method fig 8 shows close up views for the mesh size around the hull at critical regions the mesh is refined at the bow stern the free surface quay wall and at the bottom for the simulated shallow water depth the presented numerical model adapted the overset mesh technique to simulate the relative movements between the two vessels at each time step the computational domain is discretized into several meshes that overlap each other the generation of overset grids contains two parts as shown in fig 9 a one is the background global region that includes the entire solution domain and the second is the smaller local regions that contain the bodies within the domain as shown in fig 9 b the regions overlap each other and simultaneously pass the information of the flow between them interfaces between the overset domain and background domain are set as interface overset to exchange data between two domains the moving speed of the overset domain is the velocity of the passing ship v in the overset mesh cells are grouped into active inactive or acceptor cells the discretized governing equations are solved for the active cells on the other side no equations are solved for inactive cells these cells could convert to active cells if the overset region is moving acceptor cells separate the active cells from the inactive cells in the background region they are attached to the overset boundary in the overset region the main function of the acceptor cells is to couple the solutions on the two overlapping grids the overset mesh interface is used to couple the overset regions with the background region as shown in fig 9 b fig 10 illustrates the grid distribution of the overall computational domain including both passing and moored ships the main parameters applied in the presented numerical simulations are mentioned in table 4 which are applied to minimize the required computational power the passing ship is advancing with a constant forward speed and restricted in the remaining modes of motion while the moored ship incorporated in the hydrodynamic problem is free to move in the 6 degrees of freedom 5 verification and validation the accuracy of the numerical cfd results depends mainly on the assigned mesh and time step for the simulations therefore mesh and time step convergence analyses are carried out to optimize the settings required to obtain accurate numerical results the three solution method proposed by stern et al 2001 is applied to calculate the errors and numerical uncertainties to verify the numerical model then the numerical cfd results are compared with experimental results for two different hydrodynamic problems first the ship generated wave problem in open water where the numerical results for the free surface elevation of the ship generated waves in open water are compared with experimental results for the npl model presented by zhou and gao 2013 at different speeds the numerical simulations at different water depths and speeds are qualitatively investigated through the presentation of the well known kelvin wave pattern the output cfd numerical results for the total resistance coefficient are also compared with experimental results for the npl model in open water at different combinations of speed and water depth second the passing ship problem in restricted water where the passing ship effect on a moored ship at closed quay is numerically simulated the general characteristics of the hydrodynamic forces on a moored ship due to the passing ship are investigated then the ship generated waves moored ship motions and mooring forces are compared and validated with the experimental results obtained from the presented experiments at relatively high speed v 2 36 m s water depth h 2 m and passing distance of sd 3 m all numerical simulations are carried out with a workstation of intel xeon processor of 16 cores and 32 gb ram 5 1 convergence and uncertainty analyses the convergence analysis is applied for the passing ship problem in restricted water where the dtmb5415 passing model moves at a relatively high speed v 2 36 m s at a water depth of h 2 m and passing distance of sd 3 m the numerical simulations are carried out to obtain three solutions for both the grid size and time step to calculate the simulation numerical uncertainty usn as shown in the following equation 14 u s n u g 2 u t 2 where ug is the uncorrected mesh uncertainty and ut is the uncorrected time step uncertainty the grid convergence analysis is carried out with a refinement ratio of mesh size rg 1 2 at the smallest time step t 0 01s on the other hand the time step convergence analysis is carried out with a refinement ratio of time step rt 2 at the medium mesh g2 the analyzed mesh configurations are shown in table 5 fig 11 shows the achieved y value and courant number for the free surface applying the medium mesh at a passing speed v 2 36 m s h 2 m and sd 3 m as the main exciting source for the moored ship is the high frequency ship generated waves motions that are less influenced by these short waves are not presented in the current study the characteristics of both roll and pitch motions of the moored ship are investigated against the selected mesh and time step configurations three obtained solutions are used to calculate the convergence ratio ri as shown in the following equation 15 r i s 2 s 1 s 3 s 2 where s1 s2 and s3 are the solutions obtained from fine medium and coarse mesh or time step respectively the convergence conditions can be classified as monotonic convergence mc for 0 r i 1 oscillatory convergence for r i 0 r i 1 and divergence for r i 1 the obtained results from the cfd simulations at three meshes and three time steps are used to calculate the errors and uncertainties as shown in table 6 and table 7 the results obtained from the mesh convergence analysis show that monotonic convergence is achieved for the characteristics of roll and pitch motions of the moored ship excited by the passing ship the corrected errors and corrected uncertainties are calculated based on the correction factor method following the ittc 2008 guidelines the corrected numerical error due to grid size δg is less than 8 2 of the medium grid solution while the corrected grid spacing uncertainty ug is around 4 of the medium grid solution for the time step uncertainty analysis oscillatory convergence is obtained for the roll and pitch period while monotonic convergence is achieved for the maximum roll and pitch amplitudes the corrected numerical error due to time step δt is around 6 of the smallest time step solution while the corrected time step uncertainty ut is less than 1 of the smallest time step solution so that the obtained results for mesh and time step uncertainty analyses show that the presented numerical model is verified as it gives solutions with reasonable minor uncertainties for the dtmb5415 passing ship effect on the moored kcs model 5 2 ship generated wave problem in open water the experimental data obtained at the wuhan university of technology for a high speed rounded bilge model npl model 100a in calm water is used to validate the presented numerical model for the ship generated wave problem in open water case the hull form is extrapolated from the original offsets of the npl model 100a the main dimensions of the tested model are illustrated in table 8 the size of the computational domain for the current problem is defined as 10lpp 10 8m h where h is the tested water depth the boundary conditions defined for the computation domain are illustrated above however the boundary conditions for both sides of the fluid domain for this problem are velocity inlet also there is no moored body included in these simulations the numerical approach is like the case mentioned in table 4 the overset meshes are adapted as shown in fig 12 and the total number of cells is 2 13 million the earth fixed coordinate system is adopted and the npl model is moving from left to right fig 13 shows the achieved y value on the passing model and the courant number for the free surface at a passing speed v 3 0 m s and a depth to draft ratio h t 7 75 the numerical simulations are carried out for the npl model at various parameters as shown in table 8 in this manner the cfd numerical model for ship generated waves in open water is evaluated not only at different speeds but also at different water depths as the ship sails in open water with a constant speed the kelvin wave pattern starts to be created on the free surface in the form of transverse and divergent waves in deep water the ship generated wave is classified according to froude number length f r v g l p p fig 14 shows the results for the free surface contours that describe the ship generated wave around the npl model at four different instants of the simulation time the npl model is sailing at a constant forward speed that is corresponding to froude number length fr 0 4 at a depth to draft ratio h t 7 75 the presented results show that the divergent and transverse waves can be captured by the presented numerical model the ship generated waves reached the stationary wave pattern around the passing model after about 10 s fig 15 shows the free surface contours of the ship generated wave around the npl model at three different speeds corresponding to fr 0 24 0 4 and 0 47 and with depth to draft ratio h t 7 75 at low speeds several crests of the dominant transverse wave exist along the ship s length with a small wavelength as the speed increase the ship generated waves start to be remarkable the length of the transverse waves increase and the divergent waves become more significant and represent the dominant effect fig 16 shows a comparison between the experimental results and simulated cfd results for the wave surface elevation at three different speeds and depth to draft ratio h t 7 75 the results are presented at two different collection points located at y lpp where y is the transverse distance between the collection point and the centreline of the npl model as shown in fig 17 again results show that the wave amplitude of the ship generated waves is getting higher as the forward speed increase in general results show a good agreement between the presented numerical model and the experimental data for the wave surface elevation at different collection points the wave period and wave amplitude for the ship generated waves are correlated with the experimental data especially at high speeds small differences are obtained for the wave amplitudes at the lower speed however the obtained amplitudes at low speeds are very small and the differences could be minimized by applying finer mesh distribution around the npl model in general the comparison shows that the numerical model simulates the free surface elevation of the ship generated waves in open water with reasonable accuracy especially at high speeds different water depths can be simulated by defining the height of the lower part of the fluid domain as shown in fig 7 a as the ship generated wave propagates in water with a depth of less than half of the wavelength the wave particles interact with the bottom and the water depth becomes effective on wave characteristics so that the wave pattern generated by the sailing vessel in shallow water is influenced by the interaction with the bottom then it is often better to present the results concerning depth froude number frh fig 18 shows the free surface contours of the ship generated waves at two speeds in the shallow water of a depth to draft ratio h t 3 88 compared to deeper water results for the same two speeds in fig 15 b and 15 c results show that the angle of the generated wave system increases with the decrease of the water depth fig 18 b shows that as the speed reaches the hydrodynamic barrier v g h in shallow water the wave system mainly consists of dominant divergent waves the obtained wave patterns from the numerical simulations at different water depths shows consistency with the well known changes in kelvin wave pattern and angle fig 19 compares the time series of the free surface elevation of the ship generated waves at a relatively high passing speed of v 2 5 m s and three different water depths that correspond to frh 0 56 0 8 and 1 11 the results are obtained at two fixed collection points in the computational domain that are located around the centreline of the passing npl model at distance y lpp 0 2 and 0 4 respectively results show that the ship generated wave system significantly changes with the decrease in water depth the interactions with the bottom are pronounced and wave elevation increases at h t 2 0 m that satisfy the super critical depth froude number of frh 1 11 table 9 shows a comparison of cfd results with experimental data for the total resistance coefficients on the npl 100a at different combinations of testing parameters the obtained numerical results at constant water depths and different advancing speeds froude number length show a good agreement with the experimental data results show that the total resistance coefficient increase with the forward speed similar consistency is obtained between the numerical and experimental data at different water depths but the same depth froude number results show that the resistance coefficient increases rapidly with the decrease of the water depth especially at the super critical depth froude number frh 1 11 the comparison between experimental c td and numerical c tc total resistance coefficients at different parameters shows that the maximum obtained error is less than 5 so that the numerical model calculates the ship resistance in open water accurately and can be feasible for further simulations 5 3 passing ship problem in restricted water to validate the numerical model for the passing ship problem at high speeds in restricted water the model test results from the above mentioned experiments are selected to study the effect of a passing dtmb5415 model on a kcs model that is moored parallel to a closed quay in calm water the numerical simulation is carried out at a water depth of h 2m a passing distance of 3 m and with a separation ratio of sr 0 94 as described above the fluid domain is defined to simulate the same conditions of the experimental setup the dtmb5415 passing model is starting from the left end of the computational domain and moves with a constant speed along a straight path at a relatively high passing speed v 2 36 m s that corresponds to depth froude number frh 0 53 the simulations are carried out with the smallest time step of 0 01s and with the medium mesh configuration g2 the numerical model includes the representation of eight mooring lines and neglects the fender simulation the analysis of the passing ship problem in restricted water is decoupled into three stages it starts with the comparison of the ship generated waves with experimental data then the description of pressure and forces on the moored body and finally the validation of motions and mooring forces with experimental data fig 20 shows the free surface contours of the ship generated waves at various longitudinal relative positions p id lc between the two ships the wave contours are used to investigate the effect of the ship generated wave on the moored model at p 1 the ship generated waves are fully developed around the passing model it can propagate in the direction of the advancing passing model which is located behind the moored model at p 0 the passing ship is just passing by the moored ship and the two ships are exactly alongside each other the generated wave system is capable to maintain the steady state with a defined inclined angle around the passing ship model so that the propagated wave still cannot reach the bow of the moored ship yet at that time the moored model is mainly affected by the pressure change created around the passing model however this effect might not be significant due to the high separation ratio fig 20 shows that at p 0 5 1 0 the propagated waves reached the bow of the moored model and started to be the main source of excitation for the moored model also these waves are interacting with the quay wall and reflect on the port side of the moored model although the simulated passing distance is 3 m with a separation ratio near to one the ship generated waves at high speed are not dissipated by the time they reached the moored vessel at p 3 3 the generated waves are still interacting with moored ship and quay wall and last until it passes away from the zone of the moored body and decay fig 21 shows a comparison between the numerical and experimental results for the generated waves by the passing dtmb5145 at different locations around the kcs moored ship the numerical simulations account for the quay wall effect without including the complete representation of the fixed mooring frame inside the domain the numerical results agree with the model test results especially at the beginning of the formation of the waves fig 21 a shows that when the waves propagate over time some differences are obtained at the peak of the generated waves at probe 1 this probe is located near the bow of the moored ship and might be affected by the lacking of the fixed mooring frame in the numerical simulations better numerical results are obtained for wave probes 2 and 3 which are located between the two ships and far from the fixed frame in general the comparison between experimental and numerical results shows a good prediction for the peak and period of the generated waves so that the applied mesh might be suitable to solve the ship generated wave problem in restricted water fig 22 shows the numerical results of direct interaction forces and moments on the hull of the moored ship due to the passing ship effect the non dimensional surge force cfx sway force cfy and yaw moment cmz are plotted at various relative positions between the two ships results show that the hydrodynamic interaction forces and moments on the moored ship at relatively high speed are different from the well known behaviour at slow speeds this could be explained as the moored ship is mainly excited by the generated wave of the high speed passing ship and its interactions with the nearby wall for a better explanation fig 23 investigates the pressure distribution on the hull of the moored kcs ship the pressure distribution on port and starboard sides and around the bow and stern of the berthed ship changes rapidly when the relative longitudinal distance between the passing and moored ships at p 2 5 and 4 0 at p 2 5 fig 23 a shows that the pressure distribution on the bow is lower than the pressure at the stern which results in a total negative surge force on the moored ship as shown in fig 22 a furthermore the starboard pressure is lower than the port side pressure around the mid ship which makes the moored ship subjected to a total negative sway force that matches the negative peak value in the cfy curve in fig 22 b moreover the differences between port and starboard pressures around the bow and stern result in a positive yaw moment as in fig 22 c at p 3 0 the starboard pressure around the moored ship is almost balanced by the port pressure at mid ship bow and stern so that the sway force and yaw moment on the moored ships vanishes to zero as shown in fig 22 b and c at the same time the stern pressure is higher than bow pressure therefore a positive peak surge act on the moored ship again at p 3 5 the negative pressure on the bow of the moored ship overcomes the positive stern pressure which results in a total negative surge force at p 4 0 the forces and moment continue to change to positive surge force negative sway force and positive yaw moment the shape of the forces and moment on the moored body is mainly attributed to interactions with generated waves of the high speed passing ship at a different longitudinal relative position between the two ships which matches the above clarified results for the generated wave contours the validation of the numerical model for the passing ship problem in restricted water is mainly based on the comparison of the cfd results with the experimental data for the motions of the moored ship to evaluate the comparison error e to validate the numerical model the comparison error must be less than the validation uncertainty uv which is calculated based on the combination of the simulation numerical uncertainty usn and the experimental uncertainty ud as shown in the equation below as the experimental uncertainty is not available it is assumed to be around 7 5 based on similar studies kok et al 2018 16 u v u s n 2 u d 2 fig 24 shows the comparison between numerical and experimental results for roll and pitch motions table 10 shows the validation uncertainty uv against the comparison error in general the numerical cfd results agree with the experimental results however the numerical results give lower peak values than experimental data with around 22 error in maximum roll and pitch motions these discrepancies might have resulted from the far field interaction of the generated waves with the fixed mooring frame that is not included in the numerical simulations the peak period of the pitch motion agrees with the experimental results with a comparison error of 3 8 while the roll peak period has a larger difference this might be due to the representation of the mooring system without simulating fenders in the numerical model accurate representation of the full mooring system with nonlinear stiffness might be important for better motion results fig 24 shows that the maximum amplitude of the motions of the moored ship occurs at 12 5s 13 5s that corresponds to the relative longitudinal position of p 3 3 where the passing ship generated waves strongly interact with the moored model as shown in fig 20 the behaviour of the roll motion obtained from the numerical simulation is different from the model test result at 12 6 s as shown in fig 24 b this difference might be due to the combined action of fenders and mooring lines during the model test while the numerical simulation is performed without fenders in general two variables are validated which implies that the presented cfd numerical method might be feasible for further simulations fig 25 shows the mooring loads on the moored ship in line 4 and line 5 during the passing ship interaction results show that the numerical simulations underestimate the loads on the mooring lines again this might be due to the differences in the simulation of the full mooring system 6 computational results as the applied numerical model is verified and validated it is used to study the effect of the lateral separation distance and speed of the passing dtmb5145 model on the transient motions forces and moments of the moored kcs model in calm water several simulations are carried out as shown in table 11 all simulated cases apply the settings of the above mentioned numerical set up to solve the hydrodynamic problem the froude number range is carefully selected to simulate relatively high speed cases 6 1 effects of lateral passing distance although the high speed problem still includes the near field contribution the ship generated wave is the dominant source that influences the transient interaction between the passing and moored ships the lateral separation between the two ships affects the propagation distance of these transient generated waves fig 26 shows the non dimensional forces and moments on the moored model due to the passing ship at different separation ratios passing ship fig 27 shows the transient response of the moored ship in three degrees of freedom at different separation ratios all results are presented against the non dimensional longitudinal relative position between the two ships results show that forces and motions of the moored model increase with the decrease of the separation ratio however the shape characteristics are identical at different separation ratios the fully developed waves around the passing ship can interact with the moored kcs model earlier under a small separation ratio consequently the peak value of either forces or motions appears earlier for a large separation ratio compared to the peak at a lower separation ratio however the period is identical at different separation ratios the forces and motions of the moored kcs model change significantly between p 1 and 4 0 and decay gradually when p 4 0 the absolute maximum amplitude of forces and motions at a different transverse distance is illustrated in fig 28 results show that the interaction forces moments and transient motions decrease linearly with the increase of separation ratio when the separation ratio decreases by 50 the maximum surge force increases by 80 maximum sway force and yaw moment increase by 50 and the maximum of either roll pitch or yaw motions increase by about 50 6 2 effects of passing ship speed the ship generated wave by a passing ship is the dominant exciting source that affects the transient interaction on a moored ship at high speeds as described above when the passing speed increases the ship generated waves start to be remarkable and the divergent wave will be more significant fig 29 and fig 30 illustrates the non dimensional forces and motions on the moored kcs model at different passing speed respectively results show that the forces and motions change significantly between p 1 and p 5 0 forces and moments are mainly depending on the produced waves at high speed that reach the moored body later unlike the slow speed case where the hydrodynamic interaction forces and moments mainly depend on the instant pressure variation due to the presence of passing ship and are effective between ve and ve relative position p fig 31 shows the effect of passing ship speed on the absolute maximum amplitude of forces and motions on the moored body results show that the interaction forces moments and transient motions highly increase with the increase of passing ship speed when the passing speed has an increase of 30 the maximum sway force and yaw moment reach about 4 5 times their value at initial speed and surge force is tripled also the maximum amplitude of roll and pitch and yaw motions reach 15 8 and 10 times its value at the initial speed respectively in general the speed of passing ships plays a significant role in the effects of the passing ship on the moored ship than the transverse distance between passing ships and mooring ships in practice the most effective measure to reduce the effects of passing ship is to control the speed of the passing ship 7 conclusion in this paper the passing ship effect due to the dtmb5145 frigate model on the moored kcs container model alongside a closed quay is experimentally and numerically investigated with a focus on ship generated waves transient ship motions and hydrodynamic forces acting on the moored ship at relatively high speed a scaled model experiment is presented to investigate the hydrodynamic interactions between the waves generated by a high speed passing ship and a moored ship a rans based cfd numerical method is applied using the star ccm software to solve the hydrodynamic problem including the significant effect of the free surface the overset mesh technique is adopted to simulate the relative movements between the two vessels the numerical method is verified and subsequently compared with the experimental results for the ship generated wave and total resistance problem in open water and passing ship problem in restricted water water depth passing speed and separation distance are important parameters that affect the main source of excitation and consequently influences the transient interaction between the passing ship and moored ship the wave pattern generated by the sailing vessel in shallow water is influenced by the interaction with the bottom the angle of the generated wave system increases with the decrease of the water depth when the speed reaches the hydrodynamic barrier in shallow water the wave system is mainly composed of a dominant divergent wave the ship resistance increases rapidly with the decrease of the water depth especially at the super critical depth froude number the shape of hydrodynamic interaction forces and moments on the moored ship at relatively high speeds is different from the well known behaviour at slow speeds it is mainly depending on the produced waves at high speed that interact with the moored body and the nearby quay wall unlike the slow speed case where the hydrodynamic forces and moments change according to the instant pressure variation due to the existence of a passing ship the lateral separation between the two ships affects the propagation distance of the transient generated waves the interaction forces moments and motions decrease linearly with the increase of separation ratio and they highly increase with the increase of passing ship speed the passing ship speed plays a significant role in the effects of the passing ship on the moored ship than the lateral separation distance between the two ships in practice the most effective measure to reduce the effects of passing ship is to control the speed of the passing ship credit authorship contribution statement lilan zhou methodology formal analysis h s abdelwahab formal analysis writing original draft c guedes soares writing review editing supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this experimental work is supported by the national natural science foundation of china no 51409201 and the fundamental research funds for the central universities wut 2017ii19xz and the national key technologies research development program 2017yfe0118000 the second author was funded by the project reset reliability and safety engineering and technology for large maritime engineering systems which is partially financed by the european union horizon 2020 research and innovation program under the marie skodowska curie grant agreement no 73088 reset which allowed his visit to the wuhan university of technology this work contributes to the strategic research plan of the centre for marine technology and ocean engineering centec which is financed by the portuguese foundation for science and technology fundação para a ciência e tecnologia fct under contract uidb uidp 00134 2020 
20842,the effects of a high speed frigate model passing close to a moored container model alongside a closed quay are experimentally and numerically investigated with a focus on ship generated waves transient ship motions and hydrodynamic forces acting on the moored ship scaled model experiments are conducted to investigate the hydrodynamic interactions between the passing ship generated waves and the moored ship a rans based cfd method is applied to solve the hydrodynamic problem including the effect of the free surface the overset mesh technique is adopted to simulate the relative movements between the two vessels the numerical method is compared with experimental results for the ship generated waves and total resistance in open water then validated for the passing ship problem in restricted water numerical simulations are carried out to study the effect of different parameters on the passing ship effects results show that water depth passing speed and separation distance are important parameters that influence the transient interaction the shape of hydrodynamic interaction forces and moments on the moored ship at relatively high speeds is different from the well known behaviour at slow speeds in practice the most effective measure to reduce the effects of passing ship is to control the speed of the passing ship keywords ship generated wave ship ship interaction passing ship moored ship overset mesh rans 1 introduction the development of maritime transportation and activities results in large ship traffic in harbours which increases the probability of moving ships to pass by moored ships also the introduction of fast ferries increased the concerns regarding the impact of the ship generated waves on the moored ships therefore the passing ships and their generated waves may introduce significant hydrodynamic forces on the moored ships that end up with large motions which may cause accidents for crew or passengers and damage to the infrastructures swiegers 2011 in 1990 the suction effect from a passing vessel caused the total loss of oil tanker jupiter due to fire during unloading cargo at a dock in michigan seelig 2001 pinkster and keuning 2013a showed that 40 of the damages in the mooring equipment of inland vessels were due to the passing ship interactions small ships and barges can also be in danger of capsizing due to large shoaling waves generated by a passing ship also the undesirable large motions would result in significant mooring forces and breakdown of the cargo handling operation between the ships and quay inside ports and channels for the above reasons the passing ship problem should be one of the parameters to be considered in the design of ships channels and ports predicting the passing ship effect on a moored correctly in the ship design phase is an attractive choice to assure the safety of operations and efficiency of activities in ports the passing ship effect on a moored ship is classified in the literature into two different hydrodynamic problems according to the passing speed the first problem is at low speeds which is attributed to the low frequency hydrodynamic pressure variation force due to the presence of the passing ship which is the dominant part at low speeds it is called as low speed near field effect which is the case for most of the large merchant displacement ships where the problem can be formulated using double body flow assumption to avoid the complexity of mesh due to free surface modelling korsmeyer et al 1993 the key elements for this problem are the displacement of the passing ships passing distance speed and the flow interferences created from bottom and quay effects double body flow can indeed form a good assumption for this problem but there are also some cases where the low frequency hydrodynamics are not well represented by double body flow such as the standing wave generation seiche or solitary wave of ships entering canals pinkster 2004 the second problem is at high speeds although it still includes the near field contribution it is mainly focusing on the interaction with the high frequency waves that are generated by passing ships at high froude number it is called the high speed far field effect which must account for the free surface and can be described by the forces on the moored ship due to the passing ship generated waves taking into account the interactions of these waves with the moored ship and its surrounding geometry nam and park 2018 the scale model experiment is one of the most suitable methods to study the passing ship effects remery 1974 presented a scaled model passing ship test with a captive moored model at an open jetty in shallow water potential flow based methods have been applied to study the passing ship effects on moored ships wang 1975 applied the slender body theory to solve the problem of ship ship interaction with a two dimensional flow solution in deep water by simplifying the ship hull as a slender body with parabolic cross sections and assumed the free surface as a rigid wall yeung 1978 applied the same theory to investigate the unsteady hydrodynamic interaction of two slender ships in shallow water gourlay 2009 applied yeung s method to predict the sinkage and trim of two moving ships passing each other in open water at a shallow constant depth flory 2002 vantorre et al 2002 and kriebel 2007 developed empirical formulas for the interaction forces and moments including the shallow water effects based on model tests swiegers 2011 conducted several scaled model tests for a passing ship effect on a captive moored model including open water case quay wall and channel effects comparisons of results for surge and sway forces as well as yaw moments with several existing empirical formulas show that model tests are still needed few experimental studies focused on the high speed problem such as pinkster 2009 and pinkster and keuning 2013b where the moored ship motions due to wash waves are obtained based on the experimental data of the so called wave cut of the wash wake pattern recorded at a short distance from the centreline of a passing fast ferry most of the numerical studies on passing ship effects on moored ships assumed that the two ships are close to each other and the ship is passing with a speed corresponding to low froude numbers pinkster 2004 focused on the low frequency pressure wave forces on a captive moored ship induced by a passing ship at relatively low speeds two potential flow numerical models were presented based on the three dimensional boundary element method double body numerical model and another free surface included model were adopted to predict the pressure wave forces due to a passing ship where the potentials describing the flow are based on rankine source formulation both methods are subsequently applied to different interaction cases results show that the mooring forces obtained in open water simulations should not be used as an approximation for the cases that include quay walls channels or complex harbour geometries a similar numerical model was proposed by sutulo and guedes soares 2008 2012 and validated by sutulo et al 2012 for the interaction between ships in real time where the free surface is treated as a rigid wall in the three dimensional potential flow method the model was further extended by zhou et al 2012 zhou et al 2014 and zhou et al 2016 to simulate shallow water case with both flat and arbitrary seabed topology furthermore ren et al 2020 extended the numerical model to account for sinkage and trim pinkster and pinkster 2014 introduced the three dimensional double body numerical model ropes that neglects free surface to predict the hydrodynamic forces on moored ships comparison with results of captive moored models presented by talstra and bliek 2014 showed that the free surface effects cannot be neglected at high speed froude number greater than 0 3 at high speeds the passing vessel generates waves that are capable of surviving and travelling for long distances with little dissipation in energy and excite the moored ship with high interaction transient forces van der molen and wenneker 2008 pinkster 2009 studied the harbour oscillation effects on a captive moored ship due to the interaction of either the suction wave created by a slow passing ship or ship generated waves created by a fast passing ship including the port geometry dong et al 2009 presented a numerical model that studies the effect of the ship generated waves on a floating hemispherical body taking into account the free surface effect at relatively high speeds the numerical model coupled the thin ship theory with the three dimensional boundary element method to evaluate the effect of the ship generated waves on a moored body in the time domain the model assumed that the effect of the ship generated wave will not change due to the interaction with a moored body apart from captive moored ship studies some numerical models studied the full dynamics of the moored vessel including lines and fenders li et al 2018 proposed a hybrid numerical model that combined the three dimensional boundary element method and impulse response theory to estimate the transient response of a moored ship exposed to wash waves and sea waves nam and park 2018 presented a three dimensional potential flow numerical model in the time domain based on the classical finite element method it solves the hydrodynamic problem of a passing ship with a barge moored alongside a quay using the multi zone mesh approach proposed by nam et al 2016 results show that the ship generated waves strongly interact with the barge and quay which increases the hydrodynamic forces acting on the barge at froude numbers beyond 0 3 yuan 2019 presented a three dimensional potential flow solver based on the rankine type green function it evaluates the passing ship generated waves and hydrodynamic interaction forces for ships sailing in confined waterways with various restriction cases the limitations of the potential flow methods are determined by non viscous and irrotational assumptions few studies are focusing on the unsteady reynolds averaged navier stokes rans approach to find out the hydrodynamic interaction on a moored ship due to passing ship chen et al 2002 was the first author to develop a numerical model that uses the rans method to calculate the excitation forces on a moored ship at quay due to the parallel interaction of a passing ship in a canal huang and chen 2007 extended chen s code to simulate the ship motions in the rans solver and investigate the dynamic performance of a ship moored at a pier perpendicular to the navigation channel of the passing ship bunnik and toxopeus 2011 adopted a cfd based model using the rans method to study the effect of the drift angle of the passing ship on the hydrodynamic interaction forces on a moored ship results show that the viscous effect should be considered for angles higher than 7 5 wang and zou 2014 applied the commercial software ansys fluent to evaluate the hydrodynamic interaction forces generated by a ship entering or leaving a lock on another nearby berthed ship the dynamic mesh technique is applied and the free surface effect is neglected based on slow speed assumption lee 2015 presented rans calculations using openfoam to estimate the hydrodynamic forces generated by a passing ship on a moored ship the open source toolkit of the arbitrary coupled mesh interface acmi technique and volume of fluid vof method were applied to simulate the passing ship and capture the free surface pawar et al 2019 calculated the hydrodynamic forces on a captive moored ship due to a passing ship by using the commercial software star ccm where free surface modelling is neglected based on double body approximation fonfach et al 2011 and wnęk et al 2018 used different commercial cfd models to study the interaction between tug and tanker by applying four different flow models which include inviscid or viscous fluid assumption and with or without free surface effect kok et al 2018 presented both model scale and full scale study for the passing ship effect using the star ccm software results show that the differences between model scale and full scale interaction forces and moment predictions are not significant nandhini and nallayarasu 2020 presented a similar study using star ccm and the results show that the longitudinal and transverse forces on the moored vessel become negligible for separation distance more than three times the width of the vessel however most of these models neglected the free surface effect which is one of the key advantages of applying cfd method in passing ship problems at high speeds this paper presents the experimental program for scaled model tests to investigate the interactions between passing ships and moored ships at relatively high speeds subsequently numerical simulations are conducted using star ccm based on the unsteady reynolds averaged navier stokes solver it simulates the waves generated by a passing ship and calculates the hydrodynamic interaction between the passing ship and moored ship at high speeds considering the free surface effect and modelling of the full mooring system the overset mesh technique is adopted to simulate the relative movements between the two vessels the numerical method is verified and subsequently compared with experimental results for the ship generated wave and total resistance problem in open water then validated for the passing ship problem in restricted water the influences of different parameters as the transverse passing distance ship speed and water depth on the passing ship moored ship interaction are discussed 2 experimental work program a series of scaled model experiments are conducted in a towing tank 132m in length 10 8m in width and the depth can be adjustable at wuhan university of technology wut to investigate the interaction between a passing ship and a moored ship in calm water two standard ships are used for the tests the moeri container ship kcs as a moored ship and the standard us navy frigate dtmb5415 hull form as a passing ship the kcs moored to a frame that is fixed at one of the banks of the towing tank this frame has 6 m in length and used to simulate the quay wall effect the dtmb5415 model is attached to a towing carriage passing on a parallel course past the moored kcs model as shown in fig 1 it is accelerated to a steady speed and then meets with the moored ship the fluid is quiescent in the experiments the passing ship model is free to heave pitch and roll while the moored ship can move in 6 degrees of freedom the froude scaling is used for both models with a scale of 1 58 fig 2 shows the body plan of the kcs and dtmb5415 models the models are fabricated from wood and calibrated before tests to satisfy the characteristics of the loading condition shown in table 1 fig 3 shows the layout of the test configuration of the two ships in the towing tank the wave surface elevation is recorded at different fixed positions relative to the moored ship using a set of four wave probes as shown in fig 3 two wave probes are installed alongside the mid ship of the moored ship to evaluate the transverse propagation of the wake waves generated by the passing ship the other two wave probes are installed at the bow and stern of the moored ship the data obtained from the wave probes are recorded at a sample rate of 200 hz this paper presents the results for the test performed at a water depth of h 2 m and a passing distance of sd 3 m the dtmb5415 passing model moves with a constant speed along a straight path at a relatively high passing speed v 2 36 m s that corresponds to depth froude number frh 0 53 where the depth froude number f r h v g h is the ratio between the vessel speed and the maximum wave celerity fig 4 shows the layout of the kcs model with a generic mooring system attached to a fixed frame that is used to simulate quay wall effects eight mooring lines are arranged with names of 1 8 from the stern to the bow of the moored ship the scaled model diameter of each mooring line is 2 mm 116 mm in the prototype scale the mooring lines are defined according to ocimf guidelines to simulate synthetic nylon ropes for open terminals six super cell fenders suc2250h are simulated in the model test the fenders are fixed on the frame alongside the kcs model as shown in fig 4 fig 5 shows the force transducers attached to the mooring lines and fenders to measure the loads on mooring lines and fender reaction forces respectively the xsens inertial navigation system which is used to measure the moored ship motions has an accelerometer that gives the linear acceleration in three degrees of freedom as surge sway and heave also it has a gyroscope that gives the angular velocity of the roll pitch and yaw measurements are recorded at a sample rate of 200 hz the xsens system uses the measurements to compute accurately the 3d orientation in terms of roll pitch and yaw angles at a sample rate of 10 hz table 2 shows the scaled model length of each mooring line the physical models for mooring lines and fenders have geometrically similar properties to the prototypes table 3 shows the load deformation relation for the mooring lines and fenders in the prototype scale for example 50 deformation means that the mooring line elongates by 50 of its original length in table 2 at an applied load of 2490 kn the testing procedure starts with accelerating the passing ship to a pre determined constant speed before reaching the region that affects the moored ship the passing ship is tested at a relatively high speed 2 36 m s corresponds to frh 0 53 at a water depth of h 2 m and with transverse passing distance sd 3 m the constant passing ship speed is maintained until the passing ship reaches the end of the towing tank at least 45 min is determined between each test run to make sure that the water surface in the testing tank is calm enough to start the following test with similar laboratory conditions unfortunately the calculation for the experimental uncertainty is not carried out because the repetitions for the test at the presented testing parameters are not available 3 numerical method computational fluid dynamics cfd calculations can be a valuable solution method to study ship ship interaction effects when the hydrodynamic problem includes a significant viscous effect the cfd approach is important to be applied in this paper the numerical simulations are carried out with the commercial software star ccm that is based on the unsteady reynolds averaged navier stokes solver the following sections explain the formulation of the hydrodynamic problem and all necessary settings required for simulations 3 1 problem formulation and governing equations as the relative positions of the passing ship and the moored ship are continuously changing the hydrodynamic problem can be formulated concerning the earth fixed coordinate system the passing ship with a length lp is advancing at a constant forward speed v in the parallel course and passing by a moored ship of a length lm then the characteristic length lc can be defined as the average length of the two ships the right hand coordinate system is used in the presented model with the x axis is positive to the bow the y axis is positive pointing toward the port side of the ship and the z axis is positive upward and positioned at the still water surface above the centre of gravity of the ship as shown in fig 6 a the separation distance sd can be defined as the transverse distance between the sides of both passing and moored ships the instantaneous distance id is the longitudinal distance between the centre of gravity of the two ships at each time step the gap distance g is the transverse distance between the moored ship and the quay wall as shown in fig 6 b the distances between the two ships are represented in non dimensional form as a ratio of the characteristic length lc to evaluate the passing ship effects on a moored ship at various parameters the longitudinal relative position is expressed as p id lc while the separation ratio is defined as sr sd lc the direct simulation of the turbulence around the complex engineering structures is a demanding problem the flow filed of the turbulent flows is random in the function of time and space so the velocity fields will be expressed as the sum of mean and fluctuating parts to simplify the instantaneous incompressible navier stokes equations and obtain the reynolds averaged navier stokes rans equations the numerical simulations in the presented work are based on the following rans equations 1 ρ t ρ u i x i 0 2 ρ u i t ρ u i u j x i p x i ρ x j μ u i x j u j x i x j ρ u i u j where u i and u j i j 1 2 3 are the time averaged velocity components x i and x j i j 1 2 3 are coordinated in longitudinal transverse and vertical directions respectively ρ is fluid density p is the time averaged pressure μ is water dynamic viscosity and the last term in equation 2 represents the reynolds stress term 3 2 turbulence modelling an additional turbulent model is applied to represent the diffusion and mixing of turbulence in the flow field the realizable κ ε model is used for turbulence modelling which employs the solution of the following transport equations for κ and ε 3 ρ κ t ρ κ u i x i x j μ μ t σ κ κ x j g κ ρ 4 ρ ε t ρ ε u i x i x j μ μ t σ ε ε x j ρ c 1 e ε ρ c 2 ε 2 κ ν ε where μ t is the turbulent viscosity and σ κ 1 0 σ ε 1 2 c2 1 9 c 1 max 0 43 η η 5 the boundary conditions mentioned in section 4 1 are applied in the fluid domain then the generated equations are used together with continuity equations and rans equations to compute the total pressure and fluid velocity components in the three dimensional domain around the target body the integration of pressure and viscous stresses result in the hydrodynamic interaction forces and moments induced by the presence of the hull such as surge force sway force and yaw moment the hydrodynamic interaction forces and moments produced by the passing ship on the moored ship are presented in non dimensional form according to the following equations the sign convention for the forces and moments follows fig 6 b 5 c f f o r c e 0 5 ρ b m t m v 2 6 c m m o m e n t 0 5 ρ l m b m t m v 2 3 3 free surface zone the simulations of the passing ship effect at a relatively high speed require the incorporation of the free surface region in the simulations accordingly the volume of fluid vof method is applied to simulate the free surface region by describing two phases of fluid the evolution of the free surface in time is described by an additional transport equation for the volume fraction of the background fluid the convection term in the transport equation of the volume fraction is discretized by the high resolution interface capturing scheme hric the vof transport equation is formulated as the following equation 7 α t u u g α u r 1 α 0 where α distinguishes the fluid of two phases it represents the volume fraction as the relative proportion of the two phase fluid u stands for the velocity field while u g means the velocity of mesh points the value of the volume fraction indicates the presence or absence of the background fluid in the control volume when α 0 the fluid in the grid is air when α 1 the fluid is water the free surface is defined when the water and air are mixed at 0 α 1 3 4 dynamic fluid body interaction the six degrees of freedom solver dynamic fluid body interaction dfbi is used to simulate the responses of the moored ship due to waves generated by the passing ship dfbi is used to obtain the motion response of a ship under the impact of pressure and shear force the motions are solved by the solution of the governing equation for ship motions under different forces the equation for the translation of the centre of mass of the body is defined in the global inertial coordinate system by the following equation 8 m d v d t f where m represents the mass of the body f is the resultant force acting on the body and v is the velocity of the centre of mass the rotation of the body is formulated in the body local coordinate system with the origin in the centre of mass of the body the following equation 9 m d ω d t ω m ω n where m is the tensor of the moments of inertia ω is the angular velocity of the rigid body and n is the resultant moment acting on the body 3 5 mooring system model the catenary coupling model is adopted to simulate the mooring lines which is an elastic quasi stationary model that represents a line hanging between two endpoints being subject to its weight in the gravity field cd adapco 2016 the configuration of the mooring coupling model between the ship and the quay is shown in fig 6 a p 1 and p 2 are the two endpoints of the mooring line the direction of forces f 1 and f 2 are tangential to the line the horizontal components of forces are equal in magnitude and opposite direction in the local cartesian coordinate the shape of the mooring line can be obtained by the following equations 10 x a u b sin h u α 11 y a cosh u b 2 sin h 2 u β where u 1 u u 2 a c λ 0 g b c a d l e q c λ 0 l e q g sinh u 0 sinh u 1 g is the gravitational acceleration λ 0 and l e q are the quality of line and the relaxation length of line respectively d is the stiffness of the line α and β are two constants that depend on the location of the two endpoints and the total quality of the line and u 1 and u 2 are the locations of the two endpoints of the line and u can be obtained by the following equation 12 tan φ sinh u the forces and the effects on the line are shown in fig 6 a the tangential components of the forces f 1 and f 2 at the two endpoints can be calculated from 13 f 1 x c f 1 y c sinh u 1 f 2 x c f 2 y c sinh u 2 the fender simulation is not to be included in the presented model 4 numerical model setup 4 1 fluid domain and boundary conditions the description of the physical problem in the numerical model starts with the definition of the fluid domain and its boundary conditions as shown in fig 7 a the global computational domain is a large rectangular parallelepiped that is designed to comply with the experimental setup presented in the current paper the length of the computational domain is 12 times the length of mooed ship lm the width of the domain is 1 5 times the length of the moored ship plus the simulated transverse passing distance sd the breadth of both moored and passing ships and the gap distance g between moored ship and the quay as shown in fig 7 b the whole depth of the domain is 2 m as the air zone plus h where h is the simulated water depth the upstream face of the global domain is set as a velocity inlet while the downstream face is set as a pressure outlet standard atmospheric pressure the quay side wall is treated in the numerical model as one of the sides of the computational domain which is assigned with the no slip wall boundary condition the same no slip wall condition is applied on the bottom of the domain the other bank boundary and the hull of the passing and moored ships this boundary condition assumes zero relative velocity between the surface and the fluid flow at the surface however the simulations performed for the open water case defines both sides of the fluid domain with a velocity inlet boundary condition the presented numerical model studies the passing ship effects at relatively high speeds accordingly the contribution from the free surface effect is significant on the interaction forces and moments so the free surface is modelled with the vof method that represents a two phase fluid to track and locate the free surface other environmental loads due to wind sea waves and currents are not considered in the presented simulations 4 2 mesh technique the passing ship problem requires a huge computational domain with the movement of two floating bodies this domain can be divided into two regions one is around the passing ship which is defined as refined overset mesh another region includes the moored ship for which a coarser grid will be applied the grid of the computational domain in the presented numerical model is defined based on cd adapco 2016 the trimmed hexahedral mesh and prism layer mesh techniques are employed in the computational region this strategy provides a balanced solution for complex mesh generation problems they are relatively easy and efficient to build requiring no more surface preparation than the equivalent tetrahedral meshes trimmed meshes are composed of hexahedral mesh mostly the mesh density can be locally increased or decreased by using the volume mesh density factors to define the volumetric controls so that volumetric controls are applied to have better mesh refinement around the critical computational regions for example around the bow stern quay gap bottom gap and wake regions prism layer mesh is the mesh generated from the surface mesh to the fluid which makes the mesh at the wall more refinement and fulfils the requirements of y required for the definition of thickness of the boundary layer to apply the wall function method fig 8 shows close up views for the mesh size around the hull at critical regions the mesh is refined at the bow stern the free surface quay wall and at the bottom for the simulated shallow water depth the presented numerical model adapted the overset mesh technique to simulate the relative movements between the two vessels at each time step the computational domain is discretized into several meshes that overlap each other the generation of overset grids contains two parts as shown in fig 9 a one is the background global region that includes the entire solution domain and the second is the smaller local regions that contain the bodies within the domain as shown in fig 9 b the regions overlap each other and simultaneously pass the information of the flow between them interfaces between the overset domain and background domain are set as interface overset to exchange data between two domains the moving speed of the overset domain is the velocity of the passing ship v in the overset mesh cells are grouped into active inactive or acceptor cells the discretized governing equations are solved for the active cells on the other side no equations are solved for inactive cells these cells could convert to active cells if the overset region is moving acceptor cells separate the active cells from the inactive cells in the background region they are attached to the overset boundary in the overset region the main function of the acceptor cells is to couple the solutions on the two overlapping grids the overset mesh interface is used to couple the overset regions with the background region as shown in fig 9 b fig 10 illustrates the grid distribution of the overall computational domain including both passing and moored ships the main parameters applied in the presented numerical simulations are mentioned in table 4 which are applied to minimize the required computational power the passing ship is advancing with a constant forward speed and restricted in the remaining modes of motion while the moored ship incorporated in the hydrodynamic problem is free to move in the 6 degrees of freedom 5 verification and validation the accuracy of the numerical cfd results depends mainly on the assigned mesh and time step for the simulations therefore mesh and time step convergence analyses are carried out to optimize the settings required to obtain accurate numerical results the three solution method proposed by stern et al 2001 is applied to calculate the errors and numerical uncertainties to verify the numerical model then the numerical cfd results are compared with experimental results for two different hydrodynamic problems first the ship generated wave problem in open water where the numerical results for the free surface elevation of the ship generated waves in open water are compared with experimental results for the npl model presented by zhou and gao 2013 at different speeds the numerical simulations at different water depths and speeds are qualitatively investigated through the presentation of the well known kelvin wave pattern the output cfd numerical results for the total resistance coefficient are also compared with experimental results for the npl model in open water at different combinations of speed and water depth second the passing ship problem in restricted water where the passing ship effect on a moored ship at closed quay is numerically simulated the general characteristics of the hydrodynamic forces on a moored ship due to the passing ship are investigated then the ship generated waves moored ship motions and mooring forces are compared and validated with the experimental results obtained from the presented experiments at relatively high speed v 2 36 m s water depth h 2 m and passing distance of sd 3 m all numerical simulations are carried out with a workstation of intel xeon processor of 16 cores and 32 gb ram 5 1 convergence and uncertainty analyses the convergence analysis is applied for the passing ship problem in restricted water where the dtmb5415 passing model moves at a relatively high speed v 2 36 m s at a water depth of h 2 m and passing distance of sd 3 m the numerical simulations are carried out to obtain three solutions for both the grid size and time step to calculate the simulation numerical uncertainty usn as shown in the following equation 14 u s n u g 2 u t 2 where ug is the uncorrected mesh uncertainty and ut is the uncorrected time step uncertainty the grid convergence analysis is carried out with a refinement ratio of mesh size rg 1 2 at the smallest time step t 0 01s on the other hand the time step convergence analysis is carried out with a refinement ratio of time step rt 2 at the medium mesh g2 the analyzed mesh configurations are shown in table 5 fig 11 shows the achieved y value and courant number for the free surface applying the medium mesh at a passing speed v 2 36 m s h 2 m and sd 3 m as the main exciting source for the moored ship is the high frequency ship generated waves motions that are less influenced by these short waves are not presented in the current study the characteristics of both roll and pitch motions of the moored ship are investigated against the selected mesh and time step configurations three obtained solutions are used to calculate the convergence ratio ri as shown in the following equation 15 r i s 2 s 1 s 3 s 2 where s1 s2 and s3 are the solutions obtained from fine medium and coarse mesh or time step respectively the convergence conditions can be classified as monotonic convergence mc for 0 r i 1 oscillatory convergence for r i 0 r i 1 and divergence for r i 1 the obtained results from the cfd simulations at three meshes and three time steps are used to calculate the errors and uncertainties as shown in table 6 and table 7 the results obtained from the mesh convergence analysis show that monotonic convergence is achieved for the characteristics of roll and pitch motions of the moored ship excited by the passing ship the corrected errors and corrected uncertainties are calculated based on the correction factor method following the ittc 2008 guidelines the corrected numerical error due to grid size δg is less than 8 2 of the medium grid solution while the corrected grid spacing uncertainty ug is around 4 of the medium grid solution for the time step uncertainty analysis oscillatory convergence is obtained for the roll and pitch period while monotonic convergence is achieved for the maximum roll and pitch amplitudes the corrected numerical error due to time step δt is around 6 of the smallest time step solution while the corrected time step uncertainty ut is less than 1 of the smallest time step solution so that the obtained results for mesh and time step uncertainty analyses show that the presented numerical model is verified as it gives solutions with reasonable minor uncertainties for the dtmb5415 passing ship effect on the moored kcs model 5 2 ship generated wave problem in open water the experimental data obtained at the wuhan university of technology for a high speed rounded bilge model npl model 100a in calm water is used to validate the presented numerical model for the ship generated wave problem in open water case the hull form is extrapolated from the original offsets of the npl model 100a the main dimensions of the tested model are illustrated in table 8 the size of the computational domain for the current problem is defined as 10lpp 10 8m h where h is the tested water depth the boundary conditions defined for the computation domain are illustrated above however the boundary conditions for both sides of the fluid domain for this problem are velocity inlet also there is no moored body included in these simulations the numerical approach is like the case mentioned in table 4 the overset meshes are adapted as shown in fig 12 and the total number of cells is 2 13 million the earth fixed coordinate system is adopted and the npl model is moving from left to right fig 13 shows the achieved y value on the passing model and the courant number for the free surface at a passing speed v 3 0 m s and a depth to draft ratio h t 7 75 the numerical simulations are carried out for the npl model at various parameters as shown in table 8 in this manner the cfd numerical model for ship generated waves in open water is evaluated not only at different speeds but also at different water depths as the ship sails in open water with a constant speed the kelvin wave pattern starts to be created on the free surface in the form of transverse and divergent waves in deep water the ship generated wave is classified according to froude number length f r v g l p p fig 14 shows the results for the free surface contours that describe the ship generated wave around the npl model at four different instants of the simulation time the npl model is sailing at a constant forward speed that is corresponding to froude number length fr 0 4 at a depth to draft ratio h t 7 75 the presented results show that the divergent and transverse waves can be captured by the presented numerical model the ship generated waves reached the stationary wave pattern around the passing model after about 10 s fig 15 shows the free surface contours of the ship generated wave around the npl model at three different speeds corresponding to fr 0 24 0 4 and 0 47 and with depth to draft ratio h t 7 75 at low speeds several crests of the dominant transverse wave exist along the ship s length with a small wavelength as the speed increase the ship generated waves start to be remarkable the length of the transverse waves increase and the divergent waves become more significant and represent the dominant effect fig 16 shows a comparison between the experimental results and simulated cfd results for the wave surface elevation at three different speeds and depth to draft ratio h t 7 75 the results are presented at two different collection points located at y lpp where y is the transverse distance between the collection point and the centreline of the npl model as shown in fig 17 again results show that the wave amplitude of the ship generated waves is getting higher as the forward speed increase in general results show a good agreement between the presented numerical model and the experimental data for the wave surface elevation at different collection points the wave period and wave amplitude for the ship generated waves are correlated with the experimental data especially at high speeds small differences are obtained for the wave amplitudes at the lower speed however the obtained amplitudes at low speeds are very small and the differences could be minimized by applying finer mesh distribution around the npl model in general the comparison shows that the numerical model simulates the free surface elevation of the ship generated waves in open water with reasonable accuracy especially at high speeds different water depths can be simulated by defining the height of the lower part of the fluid domain as shown in fig 7 a as the ship generated wave propagates in water with a depth of less than half of the wavelength the wave particles interact with the bottom and the water depth becomes effective on wave characteristics so that the wave pattern generated by the sailing vessel in shallow water is influenced by the interaction with the bottom then it is often better to present the results concerning depth froude number frh fig 18 shows the free surface contours of the ship generated waves at two speeds in the shallow water of a depth to draft ratio h t 3 88 compared to deeper water results for the same two speeds in fig 15 b and 15 c results show that the angle of the generated wave system increases with the decrease of the water depth fig 18 b shows that as the speed reaches the hydrodynamic barrier v g h in shallow water the wave system mainly consists of dominant divergent waves the obtained wave patterns from the numerical simulations at different water depths shows consistency with the well known changes in kelvin wave pattern and angle fig 19 compares the time series of the free surface elevation of the ship generated waves at a relatively high passing speed of v 2 5 m s and three different water depths that correspond to frh 0 56 0 8 and 1 11 the results are obtained at two fixed collection points in the computational domain that are located around the centreline of the passing npl model at distance y lpp 0 2 and 0 4 respectively results show that the ship generated wave system significantly changes with the decrease in water depth the interactions with the bottom are pronounced and wave elevation increases at h t 2 0 m that satisfy the super critical depth froude number of frh 1 11 table 9 shows a comparison of cfd results with experimental data for the total resistance coefficients on the npl 100a at different combinations of testing parameters the obtained numerical results at constant water depths and different advancing speeds froude number length show a good agreement with the experimental data results show that the total resistance coefficient increase with the forward speed similar consistency is obtained between the numerical and experimental data at different water depths but the same depth froude number results show that the resistance coefficient increases rapidly with the decrease of the water depth especially at the super critical depth froude number frh 1 11 the comparison between experimental c td and numerical c tc total resistance coefficients at different parameters shows that the maximum obtained error is less than 5 so that the numerical model calculates the ship resistance in open water accurately and can be feasible for further simulations 5 3 passing ship problem in restricted water to validate the numerical model for the passing ship problem at high speeds in restricted water the model test results from the above mentioned experiments are selected to study the effect of a passing dtmb5415 model on a kcs model that is moored parallel to a closed quay in calm water the numerical simulation is carried out at a water depth of h 2m a passing distance of 3 m and with a separation ratio of sr 0 94 as described above the fluid domain is defined to simulate the same conditions of the experimental setup the dtmb5415 passing model is starting from the left end of the computational domain and moves with a constant speed along a straight path at a relatively high passing speed v 2 36 m s that corresponds to depth froude number frh 0 53 the simulations are carried out with the smallest time step of 0 01s and with the medium mesh configuration g2 the numerical model includes the representation of eight mooring lines and neglects the fender simulation the analysis of the passing ship problem in restricted water is decoupled into three stages it starts with the comparison of the ship generated waves with experimental data then the description of pressure and forces on the moored body and finally the validation of motions and mooring forces with experimental data fig 20 shows the free surface contours of the ship generated waves at various longitudinal relative positions p id lc between the two ships the wave contours are used to investigate the effect of the ship generated wave on the moored model at p 1 the ship generated waves are fully developed around the passing model it can propagate in the direction of the advancing passing model which is located behind the moored model at p 0 the passing ship is just passing by the moored ship and the two ships are exactly alongside each other the generated wave system is capable to maintain the steady state with a defined inclined angle around the passing ship model so that the propagated wave still cannot reach the bow of the moored ship yet at that time the moored model is mainly affected by the pressure change created around the passing model however this effect might not be significant due to the high separation ratio fig 20 shows that at p 0 5 1 0 the propagated waves reached the bow of the moored model and started to be the main source of excitation for the moored model also these waves are interacting with the quay wall and reflect on the port side of the moored model although the simulated passing distance is 3 m with a separation ratio near to one the ship generated waves at high speed are not dissipated by the time they reached the moored vessel at p 3 3 the generated waves are still interacting with moored ship and quay wall and last until it passes away from the zone of the moored body and decay fig 21 shows a comparison between the numerical and experimental results for the generated waves by the passing dtmb5145 at different locations around the kcs moored ship the numerical simulations account for the quay wall effect without including the complete representation of the fixed mooring frame inside the domain the numerical results agree with the model test results especially at the beginning of the formation of the waves fig 21 a shows that when the waves propagate over time some differences are obtained at the peak of the generated waves at probe 1 this probe is located near the bow of the moored ship and might be affected by the lacking of the fixed mooring frame in the numerical simulations better numerical results are obtained for wave probes 2 and 3 which are located between the two ships and far from the fixed frame in general the comparison between experimental and numerical results shows a good prediction for the peak and period of the generated waves so that the applied mesh might be suitable to solve the ship generated wave problem in restricted water fig 22 shows the numerical results of direct interaction forces and moments on the hull of the moored ship due to the passing ship effect the non dimensional surge force cfx sway force cfy and yaw moment cmz are plotted at various relative positions between the two ships results show that the hydrodynamic interaction forces and moments on the moored ship at relatively high speed are different from the well known behaviour at slow speeds this could be explained as the moored ship is mainly excited by the generated wave of the high speed passing ship and its interactions with the nearby wall for a better explanation fig 23 investigates the pressure distribution on the hull of the moored kcs ship the pressure distribution on port and starboard sides and around the bow and stern of the berthed ship changes rapidly when the relative longitudinal distance between the passing and moored ships at p 2 5 and 4 0 at p 2 5 fig 23 a shows that the pressure distribution on the bow is lower than the pressure at the stern which results in a total negative surge force on the moored ship as shown in fig 22 a furthermore the starboard pressure is lower than the port side pressure around the mid ship which makes the moored ship subjected to a total negative sway force that matches the negative peak value in the cfy curve in fig 22 b moreover the differences between port and starboard pressures around the bow and stern result in a positive yaw moment as in fig 22 c at p 3 0 the starboard pressure around the moored ship is almost balanced by the port pressure at mid ship bow and stern so that the sway force and yaw moment on the moored ships vanishes to zero as shown in fig 22 b and c at the same time the stern pressure is higher than bow pressure therefore a positive peak surge act on the moored ship again at p 3 5 the negative pressure on the bow of the moored ship overcomes the positive stern pressure which results in a total negative surge force at p 4 0 the forces and moment continue to change to positive surge force negative sway force and positive yaw moment the shape of the forces and moment on the moored body is mainly attributed to interactions with generated waves of the high speed passing ship at a different longitudinal relative position between the two ships which matches the above clarified results for the generated wave contours the validation of the numerical model for the passing ship problem in restricted water is mainly based on the comparison of the cfd results with the experimental data for the motions of the moored ship to evaluate the comparison error e to validate the numerical model the comparison error must be less than the validation uncertainty uv which is calculated based on the combination of the simulation numerical uncertainty usn and the experimental uncertainty ud as shown in the equation below as the experimental uncertainty is not available it is assumed to be around 7 5 based on similar studies kok et al 2018 16 u v u s n 2 u d 2 fig 24 shows the comparison between numerical and experimental results for roll and pitch motions table 10 shows the validation uncertainty uv against the comparison error in general the numerical cfd results agree with the experimental results however the numerical results give lower peak values than experimental data with around 22 error in maximum roll and pitch motions these discrepancies might have resulted from the far field interaction of the generated waves with the fixed mooring frame that is not included in the numerical simulations the peak period of the pitch motion agrees with the experimental results with a comparison error of 3 8 while the roll peak period has a larger difference this might be due to the representation of the mooring system without simulating fenders in the numerical model accurate representation of the full mooring system with nonlinear stiffness might be important for better motion results fig 24 shows that the maximum amplitude of the motions of the moored ship occurs at 12 5s 13 5s that corresponds to the relative longitudinal position of p 3 3 where the passing ship generated waves strongly interact with the moored model as shown in fig 20 the behaviour of the roll motion obtained from the numerical simulation is different from the model test result at 12 6 s as shown in fig 24 b this difference might be due to the combined action of fenders and mooring lines during the model test while the numerical simulation is performed without fenders in general two variables are validated which implies that the presented cfd numerical method might be feasible for further simulations fig 25 shows the mooring loads on the moored ship in line 4 and line 5 during the passing ship interaction results show that the numerical simulations underestimate the loads on the mooring lines again this might be due to the differences in the simulation of the full mooring system 6 computational results as the applied numerical model is verified and validated it is used to study the effect of the lateral separation distance and speed of the passing dtmb5145 model on the transient motions forces and moments of the moored kcs model in calm water several simulations are carried out as shown in table 11 all simulated cases apply the settings of the above mentioned numerical set up to solve the hydrodynamic problem the froude number range is carefully selected to simulate relatively high speed cases 6 1 effects of lateral passing distance although the high speed problem still includes the near field contribution the ship generated wave is the dominant source that influences the transient interaction between the passing and moored ships the lateral separation between the two ships affects the propagation distance of these transient generated waves fig 26 shows the non dimensional forces and moments on the moored model due to the passing ship at different separation ratios passing ship fig 27 shows the transient response of the moored ship in three degrees of freedom at different separation ratios all results are presented against the non dimensional longitudinal relative position between the two ships results show that forces and motions of the moored model increase with the decrease of the separation ratio however the shape characteristics are identical at different separation ratios the fully developed waves around the passing ship can interact with the moored kcs model earlier under a small separation ratio consequently the peak value of either forces or motions appears earlier for a large separation ratio compared to the peak at a lower separation ratio however the period is identical at different separation ratios the forces and motions of the moored kcs model change significantly between p 1 and 4 0 and decay gradually when p 4 0 the absolute maximum amplitude of forces and motions at a different transverse distance is illustrated in fig 28 results show that the interaction forces moments and transient motions decrease linearly with the increase of separation ratio when the separation ratio decreases by 50 the maximum surge force increases by 80 maximum sway force and yaw moment increase by 50 and the maximum of either roll pitch or yaw motions increase by about 50 6 2 effects of passing ship speed the ship generated wave by a passing ship is the dominant exciting source that affects the transient interaction on a moored ship at high speeds as described above when the passing speed increases the ship generated waves start to be remarkable and the divergent wave will be more significant fig 29 and fig 30 illustrates the non dimensional forces and motions on the moored kcs model at different passing speed respectively results show that the forces and motions change significantly between p 1 and p 5 0 forces and moments are mainly depending on the produced waves at high speed that reach the moored body later unlike the slow speed case where the hydrodynamic interaction forces and moments mainly depend on the instant pressure variation due to the presence of passing ship and are effective between ve and ve relative position p fig 31 shows the effect of passing ship speed on the absolute maximum amplitude of forces and motions on the moored body results show that the interaction forces moments and transient motions highly increase with the increase of passing ship speed when the passing speed has an increase of 30 the maximum sway force and yaw moment reach about 4 5 times their value at initial speed and surge force is tripled also the maximum amplitude of roll and pitch and yaw motions reach 15 8 and 10 times its value at the initial speed respectively in general the speed of passing ships plays a significant role in the effects of the passing ship on the moored ship than the transverse distance between passing ships and mooring ships in practice the most effective measure to reduce the effects of passing ship is to control the speed of the passing ship 7 conclusion in this paper the passing ship effect due to the dtmb5145 frigate model on the moored kcs container model alongside a closed quay is experimentally and numerically investigated with a focus on ship generated waves transient ship motions and hydrodynamic forces acting on the moored ship at relatively high speed a scaled model experiment is presented to investigate the hydrodynamic interactions between the waves generated by a high speed passing ship and a moored ship a rans based cfd numerical method is applied using the star ccm software to solve the hydrodynamic problem including the significant effect of the free surface the overset mesh technique is adopted to simulate the relative movements between the two vessels the numerical method is verified and subsequently compared with the experimental results for the ship generated wave and total resistance problem in open water and passing ship problem in restricted water water depth passing speed and separation distance are important parameters that affect the main source of excitation and consequently influences the transient interaction between the passing ship and moored ship the wave pattern generated by the sailing vessel in shallow water is influenced by the interaction with the bottom the angle of the generated wave system increases with the decrease of the water depth when the speed reaches the hydrodynamic barrier in shallow water the wave system is mainly composed of a dominant divergent wave the ship resistance increases rapidly with the decrease of the water depth especially at the super critical depth froude number the shape of hydrodynamic interaction forces and moments on the moored ship at relatively high speeds is different from the well known behaviour at slow speeds it is mainly depending on the produced waves at high speed that interact with the moored body and the nearby quay wall unlike the slow speed case where the hydrodynamic forces and moments change according to the instant pressure variation due to the existence of a passing ship the lateral separation between the two ships affects the propagation distance of the transient generated waves the interaction forces moments and motions decrease linearly with the increase of separation ratio and they highly increase with the increase of passing ship speed the passing ship speed plays a significant role in the effects of the passing ship on the moored ship than the lateral separation distance between the two ships in practice the most effective measure to reduce the effects of passing ship is to control the speed of the passing ship credit authorship contribution statement lilan zhou methodology formal analysis h s abdelwahab formal analysis writing original draft c guedes soares writing review editing supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this experimental work is supported by the national natural science foundation of china no 51409201 and the fundamental research funds for the central universities wut 2017ii19xz and the national key technologies research development program 2017yfe0118000 the second author was funded by the project reset reliability and safety engineering and technology for large maritime engineering systems which is partially financed by the european union horizon 2020 research and innovation program under the marie skodowska curie grant agreement no 73088 reset which allowed his visit to the wuhan university of technology this work contributes to the strategic research plan of the centre for marine technology and ocean engineering centec which is financed by the portuguese foundation for science and technology fundação para a ciência e tecnologia fct under contract uidb uidp 00134 2020 
20843,environmental contours of significant wave height and wave period are established for 10 25 and 100 year return periods by different approaches the environmental contours are first derived by direct sampling and by the inverse first order reliability method iform using the rosenblatt transformation then the importance sampling technique is applied at different locations along the environmental contours to reduce the variance of the estimates and consequently to increase the accuracy and efficiency of the prediction method the different environmental contours are derived from a large collection of hindcast wave data from offshore of canary islands which is used as a case study to compare the different approaches keywords environmental contours iform direct sampling importance sampling 1 introduction adequate knowledge of wave parameters is essential in any field related to the behaviour of natural or man made structures exposed to load actions such as coastal and marine structures for designing structures in the marine environment extreme conditions and extreme responses corresponding to specific return periods should be considered for coastal structures and fixed offshore structures typically long term distributions and extreme distributions of significant wave height h s are used to derive reference values for the design ferreira and guedes soares 2000 scotto and guedes soares 2007 guedes soares and scotto 2011 while those structures have a predominantly static behaviour floating structures have dynamic responses that depend on the predominant frequency of the sea states and of the natural frequencies of the structure therefore it is not enough to determine the extreme value of h s but it is necessary to determine the characteristic period of the sea state associated with the extreme h s and for this purpose joint distributions of h s and characteristic sea state period t ferreira and guedes soares 2002 repko et al 2004 jonathan et al 2010 dong et al 2013 are required and typically long term models are derived in alternative to full long term response analysis the environmental contour concept has been used to define contours of the environmental parameters along which extreme loads and responses with a given return period should lie this approach is already included in marine standards and guidelines e g dnvgl 2019 comparative studies have shown that for some wave climates the environmental contours estimated from different methods can be very different vanem and bitner gregersen 2015 therefore the scientific community continues to investigate and to carry out rigorous comparisons of the different environmental contour techniques suggested in the literature for the joint model of met ocean data perspective the development of reliability methods in the 1980s see madsen et al 1986 melchers and beck 2018 was vital as the methods require robust joint estimates of the met ocean variables for a consistent treatment of load events teixeira and guedes soares 2009 gaspar et al 2016 the applications of the reliability methods have highlighted the importance of various environmental variables during extreme load and response conditions gaspar et al 2014 2017 as well as failures due to fatigue dong et al 2020 2021 measured met ocean data have had limited spatial coverage as buoys are predominantly in coastal areas however theoretical studies and computational advances over the years changed the scenario nowadays hindcast data are available at different locations including environmental variables such as wind waves and currents in addition the data present excellent time resolution for a wide range of engineering applications such as for marine structural design were the joint extremes of environmental variables like wave height and wind speed are vital to define the load cases this way it is possible to establish a distribution model of the structural response depending on the parameters of the sea state different studies have shown that several of these databases have consistent results up to moderately high sea states but they tend to show some discrepancies for extreme sea states campos and guedes soares 2016b 2017 vettor and guedes soares 2016 which needs to be kept in mind the approach consists of integrating the product of the probability of occurrence of each sea state by the cumulative short term probability distribution of the extreme response guedes soares and schellin 1998 bagbanci et al 2015 however this method is considered not efficient due to the large number of simulations required while a huge percentage from these simulations are not contributing to the extreme response alternatively extreme environmental conditions can be derived independently of a particular structure using environmental contour methods these methods allow to derive long term predictions based on a limited number of response calculations over a set of design sea states the concept of environmental contour is defined in various ways in the literature see haver and nyhus 1986 winterstein et al 1993 leira 2008 haver and winterstein 2009 huseby et al 2013 karmakar et al 2016 montes iturrizaga and heredia zavoni 2017 haselsteiner et al 2017 manuel et al 2018 raed et al 2020 it is considered a rational way of defining extreme values of environmental variables for engineering design in general the approach compromises three main steps i statistical modelling ii contour construction and iii design condition design engineers typically use these steps to obtain a set of reliable extreme conditions for several types of projects traditionally the environmental contours have been obtained by the inverse first order reliability method iform winterstein et al 1993 1998 baarholm et al 2010 vanem and bitner gregersen 2012 the iform approach to derive the contour consists of establishing a circle in a normal space u space with radius equal to a reliability index corresponding to a particular probability of failure and then transforming this contour to the physical space x space of the environmental variables using the rosenblatt transformation rosenblatt 1952 the iform method typically assumes a hierarchical conditional model for the joint distribution of the observations later huseby et al 2013 2014 2015 suggested an alternative approach to construct environmental contours directly in the physical space of the environmental variables by direct sampling the overall process is performed using crude monte carlo simulation of the joint observations the approach used to derive these environmental contours is referred to as direct monte carlo dmc simulation method the initial inaccuracies due to insufficient number of monte carlo samples are considered by huseby et al 2014 in which several sample sizes are tested this approach was later extended to 3 dimensions by vanem 2018 montes iturrizaga and heredia zavoni 2015 suggested a procedure based on different classes applying copula theory to construct the contours later manuel et al 2018 compared the copula theory with transformation approaches such as rosenblatt and nataf to derive the environmental contours and illustrated the usefulness of the method in wave energy converter design applications haselsteiner et al 2017 suggested to derive environmental contours from high density regions discretised by a numerical grid in the variable space chai and leira 2018 described the principle of environmental contours based on the inverse second order reliability methods isorm the contours obtained by isorm for a given return period are shown to be conservative compared to the traditional environmental contours generated by iform like iform determining the contour by the isorm is also an inverse reliability problem firstly an n dimensional sphere with radius β is created corresponding to particular level of safety reliability index then the approximation of these levels is performed through second order reliability considerations for a specific structural system a crucial topic on environmental contour methods is related to the uncertainty quantification some relevant discussions can be found silva gonzález et al 2015 studied the effect of uncertainties associated to the environmental contour on the extreme response of a fpso under hurricane conditions the results showed the importance of the joint probabilistic model to compute the sea states as well as the effect from extreme conditions on response of the fpso vanem and bitner gregersen 2015 compared alternative ec approaches based on direct samples generated by monte carlo simulation with the traditional iform method for establishing marine structural design criteria the methods are then compared in application cases particular attention is given to mixed sea systems guedes soares 1984 vettor and guedes soares 2020 for these situations both approaches to construct environmental contours may be very different while for other wave conditions as swell or wind sea dominant environmental contour lines present similar shape additionally the practical consequences of the adopted approach are discussed vanem 2017 compared the extreme structural responses from different environmental contour methods the results clearly demonstrate the influence of the environmental contour method on the estimated extreme structural response despite these contours address the same problem there were significant differences between contours for some cases in particular the alternative direct monte carlo environmental contour method always produce convex contours whereas the traditional iform contours may be either convex or non convex montes iturrizaga and heredia zavoni 2017 assessed the uncertainty in environmental contours due to parametric dependence on the structural system and met ocean variables considered a point estimation approach is adopted to estimate the statistics of the environmental contour coordinates as well the related uncertainties the results indicated how parametric uncertainty induces larger variability in the environmental contours manuel et al 2018 discussed in detail how the uncertainty on the parametric distributions and fitting process influences the accuracy of the environmental contours the evaluation considered met ocean data to assess site specific conditions for the design and installation of wave energy converters vanem et al 2019 performed a detailed study on the uncertainties of the process of deriving environmental contours due to the variability of the samples the environmental contours were derived from hindcast data sets of different sizes the uncertainty of the contours is quantified and the potential bias resulting from assuming different estimation methods iform and direct sampling are discussed moreover the uncertainty associated to the joint distribution fitting process is assessed haselsteiner et al 2019 proposed a benchmarking exercise focused on estimating extreme environmental conditions visual comparisons and uncertainty quantification of environmental contours derived by various approaches were performed the exercise provided the participants with a robust wave buoys dataset to derive sea states contours and hindcast models for wind sea states contours recently ross et al 2019 reviewed the main guidance concerning the application of environmental contour methods recommending procedures for practice in the ocean engineering industry they concluded that environmental contours for marine and coastal design lack clarity regarding i the differences between the environmental contour approaches reported in the literature ii the relationship between the environmental contours corresponding to given return period and their structural response for the same period in the light of indecision of how and when environmental contours should be used they provided some assistance in understanding the fundamental questions regarding environmental contours for structural reliability analysis the statistical model to estimate the joint distribution of variables as well as to construct the environmental contours based on that distribution are described with detail the direct monte carlo simulation method provides a good approximation to the iform approach however the efficiency of direct sampling depends on the magnitude of the exceedance probability i e smaller exceedance probabilities require larger numbers of samples when the exceedance probability is small most of sample values are wasted i e do not contribute to reduce the variance of the probability estimates and therefore the direct monte carlo sampling method becomes inefficient this drawback of the direct monte carlo sampling method can be overcome by generating samples closer to the environmental contour line using a different probability density function this approach known as monte carlo importance sampling mcis technique e g bucher 2009 melchers and beck 2018 has been successfully implemented to assess rare event probabilities this paper proposes a monte carlo importance sampling mcis approach to improve the efficiency and accuracy of the environmental contours a weighted probability density function is used for sampling at locations along the environmental contour line previously derived by iform the objective is to reduce the uncertainty of the location of specific points of the environmental contour the approach is applied to the construction of environmental contours that represent sea states with different return periods the paper is organized as follows section 2 introduces the reliability techniques employed to estimate the probability of failure section 3 presents the methods applied to estimate the environmental contour lines section 4 presents the joint environmental model of wave height and period adopted the results of the environmental contours derived by iform direct monte carlo simulation and monte carlo importance sampling at specific locations of the contour line are presented in section 5 environmental contours for 10 25 and 100 year return periods derived by different approaches are compared finally conclusions are drawn in section 6 2 probability of failure estimation methods for time independent reliability assessment the probability of failure of a structural system can be written as e g melchers and beck 2018 1 p f p g x 0 g x 0 f x x d x x i g x e i g x 0 where g x is the limit state function f x x is the joint probability density function for the n dimensional vector x of the basic variables i is an indicator function that equals 1 if g x 0 is true and 0 otherwise and e is the mathematical expectation the domain of integration g x 0 in eq 1 denotes the space of limit state violation and is directly analogous to the failure domain d in some special cases this integration over the failure domain g x 0 cannot be performed analytically due to the nature or complexity of the function the direct monte carlo unbiased estimator of eq 1 is 2 p f p ˆ f 1 n i 1 n i g x i 0 where x i represents the ith vector of random values sampled from f x x and n is the number of samples it is evident that for low values of p f few samples vectors x i will contribute to eq 2 and consequently very large numbers of samples are required gaspar and guedes soares 2013 2 1 first order reliability method the first order reliability method form approximates the probability integral in eq 1 by linearizing the limit state function in a standard normal space at an optimal point firstly all variables are transformed into the standard normal space where all variables follow normal distributions with zero mean and unit variance the mapping transformation operation is denoted by t such that u τ χ ditlevsen 1981a and der kiureghian and liu 1986 the form approximation is then obtained by solving a constrained optimization problem this corresponds to finding the optimal point u located on the limit state surface that has minimum distance from the origin in the standard normal space the shortest distance between the origin and this particular point on the limit state surface is denoted by reliability index termed the reliability index β hasofer and lind 1974 3 u min u g u 0 a n d β u since equal probability density contours in the standard normal space are concentric circles centred at the origin u has the highest probability density among all realizations in the failure domain g u 0 it follows that the neighbourhood of this point makes a dominant contribution to the integral in eq 1 in this sense u is an optimal point for the linearization of the limit state function this point is commonly known as the design point but other names such as most probable point mpp and beta point are also used having calculated the reliability index β the first order approximation of the failure probability can be calculated from 4 p f p f f o r m φ β φ u where φ is the standard normal cumulative probability distribution 2 2 monte carlo importance sampling concepts the direct monte carlo simulation method involves sampling from the original joint probability density function of a n dimensional vector of random variables for low probabilities of failure only a small number of samples fall in the failure domain and therefore a large number of samples n are required to compute accurate estimates for the failure probability the variance of the failure probability estimates can be reduced by increasing the number of samples n or by making use of other sampling schemes such as the importance sampling technique with this technique the uncertainty or variance of the failure probability estimator is reduced by concentrating the simulated samples n s at the region of the basic random variables space that most contribute to the failure probability i e around the form design point instead of simulating them from the original joint probability density function around the mean value of the random variables as in direct monte carlo simulation method bucher 2009 melchers and beck 2018 this approach known as monte carlo importance sampling mcis method is adopted in this paper to calculated new environmental contour points from previously derived iform contours the monte carlo importance sampling method see hohenbichler and rackwitz 1988 engelund and rackwitz 1993 au and beck 2003 consists basically of selecting an importance sampling joint probability density function to generate samples that lead to failure more frequently using some prior information on the failure domain the low efficiency of the direct monte carlo simulation method is improved by concentrating the sampling at these locations the ratio of theses density functions original f x and importance sampling h v density functions is known as the importance weight the monte carlo importance sampling method is considered a discrete method for approximating an expected value i g x 0 and can only be achieved by using additional information about the problem to be solved it is important to mention that the samples are independent in the standard normal space but not in the real or physical space of the variables h s t p the integral of eq 1 can be re written as 5 p f x i g x 0 f x x h v x h v x dx the monte carlo importance sampling unbiased estimator of the failure probability is then given by 6 p f p ˆ f 1 n s i 1 n s i g x i 0 f x x i h v x i where n s is the number of mcis samples defining the importance weight w i as 7 w i f x x i h v x i equation 6 can be re written as 8 p f p ˆ f 1 n s i 1 n s w i i g x i 0 the h v is the importance sampling probability density function evidently it is desirable to have h v with same form of the integral of equation 1 obviously the derivation of an optimal h v function is not a trivial task an appropriate function is often selected to cover a region of most interest or more precisely a location on the limit state function g x i 0 having the largest probability density f x x known as the most probable failure point mpp once the mpp is identified one approach for choosing h v is simply to use the distribution f x x shifted to this point the low efficiency of the direct monte carlo simulation method is improved by concentrating the sampling at these points this kind of sampling function has shown to be very effective and robust for a wide range of different shapes of limit state functions nevertheless there are some limitations associated to the use of this sampling function see melchers 1991 engelund and rackwitz 1993 suggestions for practical applications include h v being selected as gaussian distributed centred in the point of maximum failure likelihood or if available the β points of iform harbitz 1986 the ratio of these density functions eq 7 is known as the importance weight w a particular case is when f h then w 1 the ratio estimator reduces to the usual direct monte carlo simulation estimator in short the basic concept of this technique is to sample from an analogous distribution other than f x x say h v x and then modify equation 2 to correct the bias introduced by the different importance sampling probability density function equations 7 and 8 demonstrate that the bias correction can be assessed using the importance weight w when using the monte carlo importance sampling method clearly the variance of failure probability estimator p ˆ f decreases as the number of samples increases this is quantified by the variance σ 2 p ˆ f 9 σ 2 p ˆ f var p ˆ f 1 n s 1 i 1 n s i g x i 0 f x x i h v x i p ˆ f 2 the effective sample size ess recommended by kong et al 1994 calculated from the variance of the weights is adopted to evaluate the impact on the simulation variance of increasing the sample size and to know how many samples are enough 10 e s s n s 1 var w i 3 environmental contours 3 1 inverse first order reliability method iform the environmental contour ec derived from the inverse first order reliability method iform has been widely used in the early design stage due to its high efficiency and acceptable accuracy in this method a limited set of design conditions is selected on the environmental contour the inverse form approach consists of defining a n dimensional sphere with radius β corresponding to the n year return period in the u space where all the variables are independent and standard normally distributed with zero mean and a unit standard deviation the n dimensional sphere in the u space is then transformed into the physical parameter space by the rosenblatt transformation rosenblatt 1952 to obtain β the probability of failure p f is first estimated by 11 p f t s s 365 24 t r where t ss is the sea state duration in hours t r is the return period in years and the 360 24 factor converts the number of years to hours commonly the relation between β and p f is given by 12 1 p f φ β subsequently the circle in the u space is transformed to the physical space x of h s t p using the rosenblatt transformation as illustrated in fig 1 the values of variables u 1 u 2 in the normal space are given by 13 u 1 β cos θ u 2 β sin θ where β is the radius of the circle and θ is the angle between the radius of the circle and u 1 axis varying between π and π according to the rosenblatt transformation the relation between the physical space x and the normal space u is given by 14 φ u 1 f h s h s φ u 2 u 1 f t p h s t p h s accordingly for a given marginal distribution for the variable h s and a conditional distribution for t z the rosenblatt transformation is given by 15 h s f h s 1 φ u 1 t p h s f t p h s 1 φ u 2 u 1 3 2 direct monte carlo dmc simulation method an alternative approach to derive the ec has been proposed that does not require the rosenblatt transformation the ec is directly generated in the physical space of the environmental variables by adopting the direct monte carlo simulation method the procedure of generating the environmental contour is summarized by the following main steps huseby et al 2013 1 generate a suitable sample of the joint environmental distribution using monte carlo simulations 2 assume an angle θ situated between π and π and for each sample generated in 1 estimate its projection that takes the form of a straight line 16 x i θ t i cos θ h i sin θ 3 sort these projections in ascending order x 1 x 2 x n and let t n h n denotes the corresponding samples 4 estimate the number of samples that will be kept within the desired probability of failure p f in a half space so called π θ such that it only contains k numbers of samples 17 p f n k n k n 1 p f 5 assume another angle and repeat the steps excluding step 1 18 t sin θ δ θ c θ sin θ c θ δ θ sin θ δ θ cos θ sin θ cos θ δ θ 19 h cos θ δ θ c θ cos θ c θ δ θ sin θ δ θ cos θ sin θ cos θ δ θ the entire process is performed directly in the physical space of the variables including the linearization of the limit state function as discussed before the direct monte carlo simulation method requires a substantial number n of limit state evaluations to accurately estimate small failure probabilities 3 3 monte carlo importance sampling mcis the inverse form estimates of individual failure probabilities can provide significant information to improve the ec using the monte carlo importance sampling technique as described in section 2 2 the approach consists basically of the following two main steps i apply the iform approach to derive the ec that provides pre information to improve the sampling scheme in step ii ii generate n s samples around the importance locations pre computed in the previous step i e along the iform environmental contour line using a new importance sampling joint probability density function and compute more accurate predictions of the failure probability using eqs 7 and 8 the prediction variance reduction of this technique is achieved by sampling at the iform design points the result is an increase of the number of samples in the failure domain which allows to compute accurate estimates for the failure probability with a reduced number n s of simulation cycles as compared to the direct monte carlo simulation method for the same level of accuracy as sampling is conducted from a new distribution located on the ec line the occurrence of failures has to be corrected using eqs 7 and 8 4 joint environmental distributions of heights and periods of sea waves in this section the wave statistics described by the significant wave height h s and the wave period t p are selected to construction of environmental contour the contours do not estimate a p f instead they are generated for a given p f or better for a given exceedance probability the joint distribution of h s and t p is obtained by the conditional modelling approach cma this probabilistic model consists of a marginal pdf for the significant wave height and a conditional pdf for the wave period naturally numerous met ocean parameters influence the response of a structure and when carrying out load and response calculations of marine structures a joint environmental model can be employed details on the use of the concept of conditional distributions have been provided by haver 1987 guedes soares et al 1988 bitner gregersen and haver 1991 lucas and guedes soares 2015 these authors adopted the cma to model wave heights and periods horn et al 2018 presented a novel combination for conditional distributions extended the approach to all relevant environmental parameters used in design of offshore structures including directional components however the focus is restricted to joint environmental models for significant wave height and wave period the conditional joint model for significant wave height and wave period applied in this case is based on the marginal distribution of significant wave height h s assumed to follow a 3 parameters weibull distribution while a conditional log normal distribution is used for the wave period t p 20 f h s t z h t f h s h f t z h s t h an implicit assumption in many joint environmental models is that the environmental random variables are independent in time it is assumed that the return periods of environmental random variables can be found as the reciprocal of the corresponding failure probability the observations may be construed as a binomial experiment where the return period of a certain rare event is represented by a geometric random variable with the corresponding failure probability as parameter the following sessions briefly discuss the distributions adopted in the cma 4 1 weibull distribution the weibull distribution to describe met ocean phenomena is widely applied in reliability data analysis due to its versatility and it is highly useful for random environmental extreme applications depending on the parameter values the weibull distribution can take several forms the three parameter h s marginal weibull probability distribution is adopted and fitted to the sea wave observations 21 f h s h β α h γ α β 1 e h γ α β were γ α and β are parameters of location scale and shape of the probability function respectively 4 2 log normal distribution the wave period follows a log normal probability distribution with parameters conditional on h s given by 22 f t p h s t h 1 σ h t 2 π exp ln t μ h 2 2 σ h 2 the parameters of the log normal distribution for wave period are dependent on h s as shown in the following equations 23 μ t h e ln t p h s h a 1 a 2 h a 3 24 σ t h s t d ln t p h s h b 1 b 2 e b 3 h where the coefficients a i and b i with i 1 2 are estimated from actual data 4 3 study case computational example the present work constructs and compares extreme ecs for a specific location derived by iform and direct and importance sampling techniques the study zone is located at northeast of the island of gran canaria see fig 2 the hindcast data has been produced by pilar et al 2008 and is managed by the spanish port and harbor administration puertos del estado from spain national state that were also members of the hipocas project guedes soares 2008 a comparison between this database and others can be found in campos and guedes soares 2016a the frequency of the wave data records is 1 h and the zone is chosen because is one of the most energetic and presented important extreme conditions over the years see rodriguez et al 2002 chiri et al 2013 gonçalves et al 2014 2020 the two dimensional bivariate histogram is presented in fig 2 bottom left panel to illustrate the joint occurrence of the variables considered the dataset includes 173781 sea states over a period of 51 years table 1 this dataset is fitted to a joint bivariate probability distribution that is then used to generate a new joint dataset of significant wave height and wave period fig 2 bottom right panel shows in a compact robust and informative way the joint bivariate probability distribution and the empirical scatter diagram based on the wave data recorded the fitted parameters for weibull significant wave height and conditional log normal wave period distributions are shown in table 2 the general nonlinear fits of the log normal parameters of t p conditional on h s are illustrated in fig 3 a nonlinear least squares method has been adopted to select the best fitting distribution among the set 5 results the ecs presented in this section are first obtained using the direct monte carlo dmc simulation method which are then compared with the traditional method based on iform as previously mentioned when comparing these methods one may expect results somewhat different as they are linearized in different spaces except for the primary variable h s however for practical cases the secondary variable t p is also critical and should be evaluated jointly in addition to conditioned by the primary in short the estimation of the environmental contours generally requires two distinct steps firstly the joint distribution of variables is estimated then contours are constructed from the joint distribution using the iform direct sampling and importance sampling techniques it is known that the contours constructed using different methods for the same joint distribution can differ significantly to provide a fair comparison dmc simulations are performed with an acceptable and consistent angular resolution a 10 angular interval is adopted that results in 36 angles or coordinates of the environmental variables furthermore to make use of the advantages of the approach the number of samples n simulated by the direct monte carlo technique has been increased to 1 million which is much larger than the number of observations in the original dataset a preliminary comparison of the environmental contour lines generated by both iform and direct sampling is conducted assuming the joint environmental model defined previously the ec lines for 10 25 and 100 year return periods derived by the direct monte carlo dmc simulation method and iform are shown in fig 4 overlaid with the original dataset of sea states observations it is important to note that the ec lines plotted overlaid with the scatter data allows to understand whether the fitting procedure is adequate some inconsistencies between the environmental contours and the series of sea states can be observed at a particular location when the data are scattered beyond the contour lines from this one can intuitively see how different contours lines behave and assess if there is over or under estimation along the environmental contour however fig 4 shows that in general the scattered data present a good agreement with the contour lines computed by both methods this indicates a good predictive capability of the approaches based on these fitted parameters it is known that the mathematical spaces where the probabilities are linearized are different so the construction of the contours takes different paths however fig 4 clearly shows that the shapes of the contours are similar after considering the shape of each ec the 100 year hs tp pairs are considered and the extreme points for each variable are extracted along the contours these combinations of extreme values known as design points can be applied for several marine engineering purposes the criteria for selecting critical locations along the contour are based on several joint monte carlo simulations in this sense one million samples are generated artificially based on the fitted joint environmental model described previously this procedure allows to identify the locations along the contour that contain exceedances of sea state events table 3 presents the hs tp design points and fig 5 shows the contour lines computed for a 100 year return period using both methods as well as the sea state data artificially generated by monte carlo simulation from the joint environmental model 5 1 monte carlo importance sampling mcis the monte carlo importance sampling variance reduction technique retains most of the advantages of the direct monte carlo simulation method such as the sampling in the physical space of the environmental variables however it is considerably more efficient than the direct monte carlo sampling method as more accurate predictions of small failure probabilities are obtained with a reduced number of limit state evaluations the approach takes advantage of the previously derived iform environmental contour to better locate new samples at some locations along the contour line the samples are generated by means of a new multivariate standard probability density function centred at a iform design point corresponding to a particular location angle or coordinates of the circle in the u space related to a reliability index iform βr that is mapped to the physical space h s t p equation 8 is then used to calculate the improved unbiased estimator of the failure probability mcis pf and the corresponding reliability index mcis βr for demonstration purposes calculations are conducted for the 100 year contours at some particular angles θ 45 90 and 135 along the iform contour fig 6 and table 4 after transformation new n samples are generated at the physical space around these design points which are highly concentrated at the failure domain fig 6 presents the case where the design values for this contour region were computed the red marker indicates the average value of the importance samples is grey point cloud the figure also illustrates the data of the original sea states as well as the number of sea states generated artificially by monte carlo mc simulation the coordinates red points generated as final results can be considered robust as they are the average of several events most probable events in that region the new samples overlap the safe failure regions as shown in fig 6 these bivariate samples or coordinates of variables can be used to better define design points for marine structure design purposes these design points are estimated using a robust sampling scheme for various angles along the contour if required employing this sampling scheme one can compute more precisely failure or exceedance probabilities of design points in various locations along the contour with less variance as shown in table 4 fig 7 shows the sets of samples simulated for three specific locations along the environmental contours corresponding to all return periods considered table 5 represents the design points for these sets of simulations fig 8 shows in more detail the mcis simulations along the 100 25 and 19 year environmental contours and the corresponding results are presented in table 6 as shown in fig 8 the samples generated by is grey cloud of points and the corresponding design points red points are close to the boundary of the contours therefore they have much contribution to the probability estimates requiring a smaller number of simulations consequently reducing the variance of the predictions in addition these samples contribute significantly to the probability estimation these three angular examples demonstrate the benefit of mcis technique to computed design points automatically based on reliable sampling schemes moreover works well for low failure or exceedance probabilities one can see that the mcis technique models very well these critical locations all the most probable failure points are computed close the contour line a closer look at these locations shows that the is points grey points are predominantly at the safety and failure domain table 6 presents the probability of failures mcis pf and the corresponding reliability indices mcis β predicted by mcis and iform at the 3 locations of the 100 25 and 10 year environmental contours it can be clearly seen that in this case the importance sampling technique provides predictions close to the ones from iform but the advantage of the approach is that the estimates have less variance which may be relevant for some particular joint environmental models and high return periods fig 9 shows the 100 year environmental contour derived by the direct monte carlo dmc simulation method and the importance sampling at an extreme design sea state corresponding to the maximum significant wave height along the contour h s 5 87 m and associated t p 12 71s the figure illustrates that a better prediction of the failure probability or of the probability of exceeding this important design sea sate can obtained by the monte carlo importance sampling technique mcis which results in a lower failure probability and lower associated variance mcis pf 4 18 10 7 and variance 0 000023 when compared to the direct monte carlo dmc simulation method dmc pf 1 14 10 6 and variance 0 001 6 conclusions the paper explores the use of the monte carlo importance sampling technique to reduce the variance on the probability of failure estimates based on prior information on the contour provided by the classical iform approach the joint environmental model of the sea waves is described by the marginal 3 parameter weibull distribution of significant wave height h s and by the conditional log normal distribution of the wave period fitted to a large collection of simulated wave data the environmental contours obtained by iform and direct monte carlo dmc simulation are first compared it is shown that the iform approach gives quite smoother contour lines than the dmc approach although these two approaches are suggested based on different concepts related to the idealization of the state limit function and ways of transforming variables between spaces the environmental contours derived from these procedures are quite similar in this case the accuracy of the dmc technique has been improved by using some prior knowledge about the problem since the joint environmental model of the sea waves is described by known probability distributions the iform environmental contour line can be easily obtained using the rosenblatt transformation this provides the information used to improve the performance of the dmc method by sampling around the importance locations provided by the iform computed in the previous step as expected the monte carlo importance sampling technique reduces the variance of the failure probability estimator which is always smaller than that of the conventional estimator this is due to the sampling close to the failure surface corresponding to a specific location along the contour the monte carlo importance sampling mcis technique is applied at three relevant locations along the environmental contours of different return periods e g 10 25 and 100 year return periods a closer look at these locations shows that some design points computed by mcis are predominantly at the failure domain θ 45 ec for 100 year return period while others are predominantly at the safe domain i e bounded by the environmental contour θ 90 and 135 for the 25 and 10 year return period respectively this further shows the advantages of the mcis approach that provides more accurate and robust predictions of the return period corresponding to a particular design point the mcis technique has been demonstrated to be very effective in general structural reliability problems since the number of samples required for a given confidence level is reduced this is because the number of samples needed for the structural analysis can be reduced significantly when compared to direct monte carlo without any improvements information on the location of the design point provided by iform has been successfully used to construct very good approximations of sampling densities the lack of the above information however can have a significant effect on the estimator thus in situations where the important failure domains are not known a priori constructing the sampling density becomes even more difficult without knowledge of the role of the random environmental variables finally the mcis technique needs to be applied for different wave environments and return periods to verify the general applicability of the procedure credit authorship contribution statement g clarindo methodology formal analysis writing original draft a p teixeira writing review editing supervision c guedes soares writing review editing supervision declaration of competing interest there are no conflicts of interest acknowledgements this study has been performed in the project exwav extreme wind and wave modelling and statistics in the atlantic ocean which is funded by the portuguese foundation for science and technology fundação para a ciência e tecnologia fct under contract ptdc eam oce 31325 2017 this work contributes to the strategic research plan of the centre for marine technology and ocean engineering which is financed by portuguese foundation for science and technology fundação para a ciência e tecnologia fct under contract uidb uidp 00134 2020 
20843,environmental contours of significant wave height and wave period are established for 10 25 and 100 year return periods by different approaches the environmental contours are first derived by direct sampling and by the inverse first order reliability method iform using the rosenblatt transformation then the importance sampling technique is applied at different locations along the environmental contours to reduce the variance of the estimates and consequently to increase the accuracy and efficiency of the prediction method the different environmental contours are derived from a large collection of hindcast wave data from offshore of canary islands which is used as a case study to compare the different approaches keywords environmental contours iform direct sampling importance sampling 1 introduction adequate knowledge of wave parameters is essential in any field related to the behaviour of natural or man made structures exposed to load actions such as coastal and marine structures for designing structures in the marine environment extreme conditions and extreme responses corresponding to specific return periods should be considered for coastal structures and fixed offshore structures typically long term distributions and extreme distributions of significant wave height h s are used to derive reference values for the design ferreira and guedes soares 2000 scotto and guedes soares 2007 guedes soares and scotto 2011 while those structures have a predominantly static behaviour floating structures have dynamic responses that depend on the predominant frequency of the sea states and of the natural frequencies of the structure therefore it is not enough to determine the extreme value of h s but it is necessary to determine the characteristic period of the sea state associated with the extreme h s and for this purpose joint distributions of h s and characteristic sea state period t ferreira and guedes soares 2002 repko et al 2004 jonathan et al 2010 dong et al 2013 are required and typically long term models are derived in alternative to full long term response analysis the environmental contour concept has been used to define contours of the environmental parameters along which extreme loads and responses with a given return period should lie this approach is already included in marine standards and guidelines e g dnvgl 2019 comparative studies have shown that for some wave climates the environmental contours estimated from different methods can be very different vanem and bitner gregersen 2015 therefore the scientific community continues to investigate and to carry out rigorous comparisons of the different environmental contour techniques suggested in the literature for the joint model of met ocean data perspective the development of reliability methods in the 1980s see madsen et al 1986 melchers and beck 2018 was vital as the methods require robust joint estimates of the met ocean variables for a consistent treatment of load events teixeira and guedes soares 2009 gaspar et al 2016 the applications of the reliability methods have highlighted the importance of various environmental variables during extreme load and response conditions gaspar et al 2014 2017 as well as failures due to fatigue dong et al 2020 2021 measured met ocean data have had limited spatial coverage as buoys are predominantly in coastal areas however theoretical studies and computational advances over the years changed the scenario nowadays hindcast data are available at different locations including environmental variables such as wind waves and currents in addition the data present excellent time resolution for a wide range of engineering applications such as for marine structural design were the joint extremes of environmental variables like wave height and wind speed are vital to define the load cases this way it is possible to establish a distribution model of the structural response depending on the parameters of the sea state different studies have shown that several of these databases have consistent results up to moderately high sea states but they tend to show some discrepancies for extreme sea states campos and guedes soares 2016b 2017 vettor and guedes soares 2016 which needs to be kept in mind the approach consists of integrating the product of the probability of occurrence of each sea state by the cumulative short term probability distribution of the extreme response guedes soares and schellin 1998 bagbanci et al 2015 however this method is considered not efficient due to the large number of simulations required while a huge percentage from these simulations are not contributing to the extreme response alternatively extreme environmental conditions can be derived independently of a particular structure using environmental contour methods these methods allow to derive long term predictions based on a limited number of response calculations over a set of design sea states the concept of environmental contour is defined in various ways in the literature see haver and nyhus 1986 winterstein et al 1993 leira 2008 haver and winterstein 2009 huseby et al 2013 karmakar et al 2016 montes iturrizaga and heredia zavoni 2017 haselsteiner et al 2017 manuel et al 2018 raed et al 2020 it is considered a rational way of defining extreme values of environmental variables for engineering design in general the approach compromises three main steps i statistical modelling ii contour construction and iii design condition design engineers typically use these steps to obtain a set of reliable extreme conditions for several types of projects traditionally the environmental contours have been obtained by the inverse first order reliability method iform winterstein et al 1993 1998 baarholm et al 2010 vanem and bitner gregersen 2012 the iform approach to derive the contour consists of establishing a circle in a normal space u space with radius equal to a reliability index corresponding to a particular probability of failure and then transforming this contour to the physical space x space of the environmental variables using the rosenblatt transformation rosenblatt 1952 the iform method typically assumes a hierarchical conditional model for the joint distribution of the observations later huseby et al 2013 2014 2015 suggested an alternative approach to construct environmental contours directly in the physical space of the environmental variables by direct sampling the overall process is performed using crude monte carlo simulation of the joint observations the approach used to derive these environmental contours is referred to as direct monte carlo dmc simulation method the initial inaccuracies due to insufficient number of monte carlo samples are considered by huseby et al 2014 in which several sample sizes are tested this approach was later extended to 3 dimensions by vanem 2018 montes iturrizaga and heredia zavoni 2015 suggested a procedure based on different classes applying copula theory to construct the contours later manuel et al 2018 compared the copula theory with transformation approaches such as rosenblatt and nataf to derive the environmental contours and illustrated the usefulness of the method in wave energy converter design applications haselsteiner et al 2017 suggested to derive environmental contours from high density regions discretised by a numerical grid in the variable space chai and leira 2018 described the principle of environmental contours based on the inverse second order reliability methods isorm the contours obtained by isorm for a given return period are shown to be conservative compared to the traditional environmental contours generated by iform like iform determining the contour by the isorm is also an inverse reliability problem firstly an n dimensional sphere with radius β is created corresponding to particular level of safety reliability index then the approximation of these levels is performed through second order reliability considerations for a specific structural system a crucial topic on environmental contour methods is related to the uncertainty quantification some relevant discussions can be found silva gonzález et al 2015 studied the effect of uncertainties associated to the environmental contour on the extreme response of a fpso under hurricane conditions the results showed the importance of the joint probabilistic model to compute the sea states as well as the effect from extreme conditions on response of the fpso vanem and bitner gregersen 2015 compared alternative ec approaches based on direct samples generated by monte carlo simulation with the traditional iform method for establishing marine structural design criteria the methods are then compared in application cases particular attention is given to mixed sea systems guedes soares 1984 vettor and guedes soares 2020 for these situations both approaches to construct environmental contours may be very different while for other wave conditions as swell or wind sea dominant environmental contour lines present similar shape additionally the practical consequences of the adopted approach are discussed vanem 2017 compared the extreme structural responses from different environmental contour methods the results clearly demonstrate the influence of the environmental contour method on the estimated extreme structural response despite these contours address the same problem there were significant differences between contours for some cases in particular the alternative direct monte carlo environmental contour method always produce convex contours whereas the traditional iform contours may be either convex or non convex montes iturrizaga and heredia zavoni 2017 assessed the uncertainty in environmental contours due to parametric dependence on the structural system and met ocean variables considered a point estimation approach is adopted to estimate the statistics of the environmental contour coordinates as well the related uncertainties the results indicated how parametric uncertainty induces larger variability in the environmental contours manuel et al 2018 discussed in detail how the uncertainty on the parametric distributions and fitting process influences the accuracy of the environmental contours the evaluation considered met ocean data to assess site specific conditions for the design and installation of wave energy converters vanem et al 2019 performed a detailed study on the uncertainties of the process of deriving environmental contours due to the variability of the samples the environmental contours were derived from hindcast data sets of different sizes the uncertainty of the contours is quantified and the potential bias resulting from assuming different estimation methods iform and direct sampling are discussed moreover the uncertainty associated to the joint distribution fitting process is assessed haselsteiner et al 2019 proposed a benchmarking exercise focused on estimating extreme environmental conditions visual comparisons and uncertainty quantification of environmental contours derived by various approaches were performed the exercise provided the participants with a robust wave buoys dataset to derive sea states contours and hindcast models for wind sea states contours recently ross et al 2019 reviewed the main guidance concerning the application of environmental contour methods recommending procedures for practice in the ocean engineering industry they concluded that environmental contours for marine and coastal design lack clarity regarding i the differences between the environmental contour approaches reported in the literature ii the relationship between the environmental contours corresponding to given return period and their structural response for the same period in the light of indecision of how and when environmental contours should be used they provided some assistance in understanding the fundamental questions regarding environmental contours for structural reliability analysis the statistical model to estimate the joint distribution of variables as well as to construct the environmental contours based on that distribution are described with detail the direct monte carlo simulation method provides a good approximation to the iform approach however the efficiency of direct sampling depends on the magnitude of the exceedance probability i e smaller exceedance probabilities require larger numbers of samples when the exceedance probability is small most of sample values are wasted i e do not contribute to reduce the variance of the probability estimates and therefore the direct monte carlo sampling method becomes inefficient this drawback of the direct monte carlo sampling method can be overcome by generating samples closer to the environmental contour line using a different probability density function this approach known as monte carlo importance sampling mcis technique e g bucher 2009 melchers and beck 2018 has been successfully implemented to assess rare event probabilities this paper proposes a monte carlo importance sampling mcis approach to improve the efficiency and accuracy of the environmental contours a weighted probability density function is used for sampling at locations along the environmental contour line previously derived by iform the objective is to reduce the uncertainty of the location of specific points of the environmental contour the approach is applied to the construction of environmental contours that represent sea states with different return periods the paper is organized as follows section 2 introduces the reliability techniques employed to estimate the probability of failure section 3 presents the methods applied to estimate the environmental contour lines section 4 presents the joint environmental model of wave height and period adopted the results of the environmental contours derived by iform direct monte carlo simulation and monte carlo importance sampling at specific locations of the contour line are presented in section 5 environmental contours for 10 25 and 100 year return periods derived by different approaches are compared finally conclusions are drawn in section 6 2 probability of failure estimation methods for time independent reliability assessment the probability of failure of a structural system can be written as e g melchers and beck 2018 1 p f p g x 0 g x 0 f x x d x x i g x e i g x 0 where g x is the limit state function f x x is the joint probability density function for the n dimensional vector x of the basic variables i is an indicator function that equals 1 if g x 0 is true and 0 otherwise and e is the mathematical expectation the domain of integration g x 0 in eq 1 denotes the space of limit state violation and is directly analogous to the failure domain d in some special cases this integration over the failure domain g x 0 cannot be performed analytically due to the nature or complexity of the function the direct monte carlo unbiased estimator of eq 1 is 2 p f p ˆ f 1 n i 1 n i g x i 0 where x i represents the ith vector of random values sampled from f x x and n is the number of samples it is evident that for low values of p f few samples vectors x i will contribute to eq 2 and consequently very large numbers of samples are required gaspar and guedes soares 2013 2 1 first order reliability method the first order reliability method form approximates the probability integral in eq 1 by linearizing the limit state function in a standard normal space at an optimal point firstly all variables are transformed into the standard normal space where all variables follow normal distributions with zero mean and unit variance the mapping transformation operation is denoted by t such that u τ χ ditlevsen 1981a and der kiureghian and liu 1986 the form approximation is then obtained by solving a constrained optimization problem this corresponds to finding the optimal point u located on the limit state surface that has minimum distance from the origin in the standard normal space the shortest distance between the origin and this particular point on the limit state surface is denoted by reliability index termed the reliability index β hasofer and lind 1974 3 u min u g u 0 a n d β u since equal probability density contours in the standard normal space are concentric circles centred at the origin u has the highest probability density among all realizations in the failure domain g u 0 it follows that the neighbourhood of this point makes a dominant contribution to the integral in eq 1 in this sense u is an optimal point for the linearization of the limit state function this point is commonly known as the design point but other names such as most probable point mpp and beta point are also used having calculated the reliability index β the first order approximation of the failure probability can be calculated from 4 p f p f f o r m φ β φ u where φ is the standard normal cumulative probability distribution 2 2 monte carlo importance sampling concepts the direct monte carlo simulation method involves sampling from the original joint probability density function of a n dimensional vector of random variables for low probabilities of failure only a small number of samples fall in the failure domain and therefore a large number of samples n are required to compute accurate estimates for the failure probability the variance of the failure probability estimates can be reduced by increasing the number of samples n or by making use of other sampling schemes such as the importance sampling technique with this technique the uncertainty or variance of the failure probability estimator is reduced by concentrating the simulated samples n s at the region of the basic random variables space that most contribute to the failure probability i e around the form design point instead of simulating them from the original joint probability density function around the mean value of the random variables as in direct monte carlo simulation method bucher 2009 melchers and beck 2018 this approach known as monte carlo importance sampling mcis method is adopted in this paper to calculated new environmental contour points from previously derived iform contours the monte carlo importance sampling method see hohenbichler and rackwitz 1988 engelund and rackwitz 1993 au and beck 2003 consists basically of selecting an importance sampling joint probability density function to generate samples that lead to failure more frequently using some prior information on the failure domain the low efficiency of the direct monte carlo simulation method is improved by concentrating the sampling at these locations the ratio of theses density functions original f x and importance sampling h v density functions is known as the importance weight the monte carlo importance sampling method is considered a discrete method for approximating an expected value i g x 0 and can only be achieved by using additional information about the problem to be solved it is important to mention that the samples are independent in the standard normal space but not in the real or physical space of the variables h s t p the integral of eq 1 can be re written as 5 p f x i g x 0 f x x h v x h v x dx the monte carlo importance sampling unbiased estimator of the failure probability is then given by 6 p f p ˆ f 1 n s i 1 n s i g x i 0 f x x i h v x i where n s is the number of mcis samples defining the importance weight w i as 7 w i f x x i h v x i equation 6 can be re written as 8 p f p ˆ f 1 n s i 1 n s w i i g x i 0 the h v is the importance sampling probability density function evidently it is desirable to have h v with same form of the integral of equation 1 obviously the derivation of an optimal h v function is not a trivial task an appropriate function is often selected to cover a region of most interest or more precisely a location on the limit state function g x i 0 having the largest probability density f x x known as the most probable failure point mpp once the mpp is identified one approach for choosing h v is simply to use the distribution f x x shifted to this point the low efficiency of the direct monte carlo simulation method is improved by concentrating the sampling at these points this kind of sampling function has shown to be very effective and robust for a wide range of different shapes of limit state functions nevertheless there are some limitations associated to the use of this sampling function see melchers 1991 engelund and rackwitz 1993 suggestions for practical applications include h v being selected as gaussian distributed centred in the point of maximum failure likelihood or if available the β points of iform harbitz 1986 the ratio of these density functions eq 7 is known as the importance weight w a particular case is when f h then w 1 the ratio estimator reduces to the usual direct monte carlo simulation estimator in short the basic concept of this technique is to sample from an analogous distribution other than f x x say h v x and then modify equation 2 to correct the bias introduced by the different importance sampling probability density function equations 7 and 8 demonstrate that the bias correction can be assessed using the importance weight w when using the monte carlo importance sampling method clearly the variance of failure probability estimator p ˆ f decreases as the number of samples increases this is quantified by the variance σ 2 p ˆ f 9 σ 2 p ˆ f var p ˆ f 1 n s 1 i 1 n s i g x i 0 f x x i h v x i p ˆ f 2 the effective sample size ess recommended by kong et al 1994 calculated from the variance of the weights is adopted to evaluate the impact on the simulation variance of increasing the sample size and to know how many samples are enough 10 e s s n s 1 var w i 3 environmental contours 3 1 inverse first order reliability method iform the environmental contour ec derived from the inverse first order reliability method iform has been widely used in the early design stage due to its high efficiency and acceptable accuracy in this method a limited set of design conditions is selected on the environmental contour the inverse form approach consists of defining a n dimensional sphere with radius β corresponding to the n year return period in the u space where all the variables are independent and standard normally distributed with zero mean and a unit standard deviation the n dimensional sphere in the u space is then transformed into the physical parameter space by the rosenblatt transformation rosenblatt 1952 to obtain β the probability of failure p f is first estimated by 11 p f t s s 365 24 t r where t ss is the sea state duration in hours t r is the return period in years and the 360 24 factor converts the number of years to hours commonly the relation between β and p f is given by 12 1 p f φ β subsequently the circle in the u space is transformed to the physical space x of h s t p using the rosenblatt transformation as illustrated in fig 1 the values of variables u 1 u 2 in the normal space are given by 13 u 1 β cos θ u 2 β sin θ where β is the radius of the circle and θ is the angle between the radius of the circle and u 1 axis varying between π and π according to the rosenblatt transformation the relation between the physical space x and the normal space u is given by 14 φ u 1 f h s h s φ u 2 u 1 f t p h s t p h s accordingly for a given marginal distribution for the variable h s and a conditional distribution for t z the rosenblatt transformation is given by 15 h s f h s 1 φ u 1 t p h s f t p h s 1 φ u 2 u 1 3 2 direct monte carlo dmc simulation method an alternative approach to derive the ec has been proposed that does not require the rosenblatt transformation the ec is directly generated in the physical space of the environmental variables by adopting the direct monte carlo simulation method the procedure of generating the environmental contour is summarized by the following main steps huseby et al 2013 1 generate a suitable sample of the joint environmental distribution using monte carlo simulations 2 assume an angle θ situated between π and π and for each sample generated in 1 estimate its projection that takes the form of a straight line 16 x i θ t i cos θ h i sin θ 3 sort these projections in ascending order x 1 x 2 x n and let t n h n denotes the corresponding samples 4 estimate the number of samples that will be kept within the desired probability of failure p f in a half space so called π θ such that it only contains k numbers of samples 17 p f n k n k n 1 p f 5 assume another angle and repeat the steps excluding step 1 18 t sin θ δ θ c θ sin θ c θ δ θ sin θ δ θ cos θ sin θ cos θ δ θ 19 h cos θ δ θ c θ cos θ c θ δ θ sin θ δ θ cos θ sin θ cos θ δ θ the entire process is performed directly in the physical space of the variables including the linearization of the limit state function as discussed before the direct monte carlo simulation method requires a substantial number n of limit state evaluations to accurately estimate small failure probabilities 3 3 monte carlo importance sampling mcis the inverse form estimates of individual failure probabilities can provide significant information to improve the ec using the monte carlo importance sampling technique as described in section 2 2 the approach consists basically of the following two main steps i apply the iform approach to derive the ec that provides pre information to improve the sampling scheme in step ii ii generate n s samples around the importance locations pre computed in the previous step i e along the iform environmental contour line using a new importance sampling joint probability density function and compute more accurate predictions of the failure probability using eqs 7 and 8 the prediction variance reduction of this technique is achieved by sampling at the iform design points the result is an increase of the number of samples in the failure domain which allows to compute accurate estimates for the failure probability with a reduced number n s of simulation cycles as compared to the direct monte carlo simulation method for the same level of accuracy as sampling is conducted from a new distribution located on the ec line the occurrence of failures has to be corrected using eqs 7 and 8 4 joint environmental distributions of heights and periods of sea waves in this section the wave statistics described by the significant wave height h s and the wave period t p are selected to construction of environmental contour the contours do not estimate a p f instead they are generated for a given p f or better for a given exceedance probability the joint distribution of h s and t p is obtained by the conditional modelling approach cma this probabilistic model consists of a marginal pdf for the significant wave height and a conditional pdf for the wave period naturally numerous met ocean parameters influence the response of a structure and when carrying out load and response calculations of marine structures a joint environmental model can be employed details on the use of the concept of conditional distributions have been provided by haver 1987 guedes soares et al 1988 bitner gregersen and haver 1991 lucas and guedes soares 2015 these authors adopted the cma to model wave heights and periods horn et al 2018 presented a novel combination for conditional distributions extended the approach to all relevant environmental parameters used in design of offshore structures including directional components however the focus is restricted to joint environmental models for significant wave height and wave period the conditional joint model for significant wave height and wave period applied in this case is based on the marginal distribution of significant wave height h s assumed to follow a 3 parameters weibull distribution while a conditional log normal distribution is used for the wave period t p 20 f h s t z h t f h s h f t z h s t h an implicit assumption in many joint environmental models is that the environmental random variables are independent in time it is assumed that the return periods of environmental random variables can be found as the reciprocal of the corresponding failure probability the observations may be construed as a binomial experiment where the return period of a certain rare event is represented by a geometric random variable with the corresponding failure probability as parameter the following sessions briefly discuss the distributions adopted in the cma 4 1 weibull distribution the weibull distribution to describe met ocean phenomena is widely applied in reliability data analysis due to its versatility and it is highly useful for random environmental extreme applications depending on the parameter values the weibull distribution can take several forms the three parameter h s marginal weibull probability distribution is adopted and fitted to the sea wave observations 21 f h s h β α h γ α β 1 e h γ α β were γ α and β are parameters of location scale and shape of the probability function respectively 4 2 log normal distribution the wave period follows a log normal probability distribution with parameters conditional on h s given by 22 f t p h s t h 1 σ h t 2 π exp ln t μ h 2 2 σ h 2 the parameters of the log normal distribution for wave period are dependent on h s as shown in the following equations 23 μ t h e ln t p h s h a 1 a 2 h a 3 24 σ t h s t d ln t p h s h b 1 b 2 e b 3 h where the coefficients a i and b i with i 1 2 are estimated from actual data 4 3 study case computational example the present work constructs and compares extreme ecs for a specific location derived by iform and direct and importance sampling techniques the study zone is located at northeast of the island of gran canaria see fig 2 the hindcast data has been produced by pilar et al 2008 and is managed by the spanish port and harbor administration puertos del estado from spain national state that were also members of the hipocas project guedes soares 2008 a comparison between this database and others can be found in campos and guedes soares 2016a the frequency of the wave data records is 1 h and the zone is chosen because is one of the most energetic and presented important extreme conditions over the years see rodriguez et al 2002 chiri et al 2013 gonçalves et al 2014 2020 the two dimensional bivariate histogram is presented in fig 2 bottom left panel to illustrate the joint occurrence of the variables considered the dataset includes 173781 sea states over a period of 51 years table 1 this dataset is fitted to a joint bivariate probability distribution that is then used to generate a new joint dataset of significant wave height and wave period fig 2 bottom right panel shows in a compact robust and informative way the joint bivariate probability distribution and the empirical scatter diagram based on the wave data recorded the fitted parameters for weibull significant wave height and conditional log normal wave period distributions are shown in table 2 the general nonlinear fits of the log normal parameters of t p conditional on h s are illustrated in fig 3 a nonlinear least squares method has been adopted to select the best fitting distribution among the set 5 results the ecs presented in this section are first obtained using the direct monte carlo dmc simulation method which are then compared with the traditional method based on iform as previously mentioned when comparing these methods one may expect results somewhat different as they are linearized in different spaces except for the primary variable h s however for practical cases the secondary variable t p is also critical and should be evaluated jointly in addition to conditioned by the primary in short the estimation of the environmental contours generally requires two distinct steps firstly the joint distribution of variables is estimated then contours are constructed from the joint distribution using the iform direct sampling and importance sampling techniques it is known that the contours constructed using different methods for the same joint distribution can differ significantly to provide a fair comparison dmc simulations are performed with an acceptable and consistent angular resolution a 10 angular interval is adopted that results in 36 angles or coordinates of the environmental variables furthermore to make use of the advantages of the approach the number of samples n simulated by the direct monte carlo technique has been increased to 1 million which is much larger than the number of observations in the original dataset a preliminary comparison of the environmental contour lines generated by both iform and direct sampling is conducted assuming the joint environmental model defined previously the ec lines for 10 25 and 100 year return periods derived by the direct monte carlo dmc simulation method and iform are shown in fig 4 overlaid with the original dataset of sea states observations it is important to note that the ec lines plotted overlaid with the scatter data allows to understand whether the fitting procedure is adequate some inconsistencies between the environmental contours and the series of sea states can be observed at a particular location when the data are scattered beyond the contour lines from this one can intuitively see how different contours lines behave and assess if there is over or under estimation along the environmental contour however fig 4 shows that in general the scattered data present a good agreement with the contour lines computed by both methods this indicates a good predictive capability of the approaches based on these fitted parameters it is known that the mathematical spaces where the probabilities are linearized are different so the construction of the contours takes different paths however fig 4 clearly shows that the shapes of the contours are similar after considering the shape of each ec the 100 year hs tp pairs are considered and the extreme points for each variable are extracted along the contours these combinations of extreme values known as design points can be applied for several marine engineering purposes the criteria for selecting critical locations along the contour are based on several joint monte carlo simulations in this sense one million samples are generated artificially based on the fitted joint environmental model described previously this procedure allows to identify the locations along the contour that contain exceedances of sea state events table 3 presents the hs tp design points and fig 5 shows the contour lines computed for a 100 year return period using both methods as well as the sea state data artificially generated by monte carlo simulation from the joint environmental model 5 1 monte carlo importance sampling mcis the monte carlo importance sampling variance reduction technique retains most of the advantages of the direct monte carlo simulation method such as the sampling in the physical space of the environmental variables however it is considerably more efficient than the direct monte carlo sampling method as more accurate predictions of small failure probabilities are obtained with a reduced number of limit state evaluations the approach takes advantage of the previously derived iform environmental contour to better locate new samples at some locations along the contour line the samples are generated by means of a new multivariate standard probability density function centred at a iform design point corresponding to a particular location angle or coordinates of the circle in the u space related to a reliability index iform βr that is mapped to the physical space h s t p equation 8 is then used to calculate the improved unbiased estimator of the failure probability mcis pf and the corresponding reliability index mcis βr for demonstration purposes calculations are conducted for the 100 year contours at some particular angles θ 45 90 and 135 along the iform contour fig 6 and table 4 after transformation new n samples are generated at the physical space around these design points which are highly concentrated at the failure domain fig 6 presents the case where the design values for this contour region were computed the red marker indicates the average value of the importance samples is grey point cloud the figure also illustrates the data of the original sea states as well as the number of sea states generated artificially by monte carlo mc simulation the coordinates red points generated as final results can be considered robust as they are the average of several events most probable events in that region the new samples overlap the safe failure regions as shown in fig 6 these bivariate samples or coordinates of variables can be used to better define design points for marine structure design purposes these design points are estimated using a robust sampling scheme for various angles along the contour if required employing this sampling scheme one can compute more precisely failure or exceedance probabilities of design points in various locations along the contour with less variance as shown in table 4 fig 7 shows the sets of samples simulated for three specific locations along the environmental contours corresponding to all return periods considered table 5 represents the design points for these sets of simulations fig 8 shows in more detail the mcis simulations along the 100 25 and 19 year environmental contours and the corresponding results are presented in table 6 as shown in fig 8 the samples generated by is grey cloud of points and the corresponding design points red points are close to the boundary of the contours therefore they have much contribution to the probability estimates requiring a smaller number of simulations consequently reducing the variance of the predictions in addition these samples contribute significantly to the probability estimation these three angular examples demonstrate the benefit of mcis technique to computed design points automatically based on reliable sampling schemes moreover works well for low failure or exceedance probabilities one can see that the mcis technique models very well these critical locations all the most probable failure points are computed close the contour line a closer look at these locations shows that the is points grey points are predominantly at the safety and failure domain table 6 presents the probability of failures mcis pf and the corresponding reliability indices mcis β predicted by mcis and iform at the 3 locations of the 100 25 and 10 year environmental contours it can be clearly seen that in this case the importance sampling technique provides predictions close to the ones from iform but the advantage of the approach is that the estimates have less variance which may be relevant for some particular joint environmental models and high return periods fig 9 shows the 100 year environmental contour derived by the direct monte carlo dmc simulation method and the importance sampling at an extreme design sea state corresponding to the maximum significant wave height along the contour h s 5 87 m and associated t p 12 71s the figure illustrates that a better prediction of the failure probability or of the probability of exceeding this important design sea sate can obtained by the monte carlo importance sampling technique mcis which results in a lower failure probability and lower associated variance mcis pf 4 18 10 7 and variance 0 000023 when compared to the direct monte carlo dmc simulation method dmc pf 1 14 10 6 and variance 0 001 6 conclusions the paper explores the use of the monte carlo importance sampling technique to reduce the variance on the probability of failure estimates based on prior information on the contour provided by the classical iform approach the joint environmental model of the sea waves is described by the marginal 3 parameter weibull distribution of significant wave height h s and by the conditional log normal distribution of the wave period fitted to a large collection of simulated wave data the environmental contours obtained by iform and direct monte carlo dmc simulation are first compared it is shown that the iform approach gives quite smoother contour lines than the dmc approach although these two approaches are suggested based on different concepts related to the idealization of the state limit function and ways of transforming variables between spaces the environmental contours derived from these procedures are quite similar in this case the accuracy of the dmc technique has been improved by using some prior knowledge about the problem since the joint environmental model of the sea waves is described by known probability distributions the iform environmental contour line can be easily obtained using the rosenblatt transformation this provides the information used to improve the performance of the dmc method by sampling around the importance locations provided by the iform computed in the previous step as expected the monte carlo importance sampling technique reduces the variance of the failure probability estimator which is always smaller than that of the conventional estimator this is due to the sampling close to the failure surface corresponding to a specific location along the contour the monte carlo importance sampling mcis technique is applied at three relevant locations along the environmental contours of different return periods e g 10 25 and 100 year return periods a closer look at these locations shows that some design points computed by mcis are predominantly at the failure domain θ 45 ec for 100 year return period while others are predominantly at the safe domain i e bounded by the environmental contour θ 90 and 135 for the 25 and 10 year return period respectively this further shows the advantages of the mcis approach that provides more accurate and robust predictions of the return period corresponding to a particular design point the mcis technique has been demonstrated to be very effective in general structural reliability problems since the number of samples required for a given confidence level is reduced this is because the number of samples needed for the structural analysis can be reduced significantly when compared to direct monte carlo without any improvements information on the location of the design point provided by iform has been successfully used to construct very good approximations of sampling densities the lack of the above information however can have a significant effect on the estimator thus in situations where the important failure domains are not known a priori constructing the sampling density becomes even more difficult without knowledge of the role of the random environmental variables finally the mcis technique needs to be applied for different wave environments and return periods to verify the general applicability of the procedure credit authorship contribution statement g clarindo methodology formal analysis writing original draft a p teixeira writing review editing supervision c guedes soares writing review editing supervision declaration of competing interest there are no conflicts of interest acknowledgements this study has been performed in the project exwav extreme wind and wave modelling and statistics in the atlantic ocean which is funded by the portuguese foundation for science and technology fundação para a ciência e tecnologia fct under contract ptdc eam oce 31325 2017 this work contributes to the strategic research plan of the centre for marine technology and ocean engineering which is financed by portuguese foundation for science and technology fundação para a ciência e tecnologia fct under contract uidb uidp 00134 2020 
20844,the bucket foundation is considered as a promising alternative to conventional foundations for offshore wind turbines owts due to its cost effectiveness and high reliability the behavior of bucket foundation under static loading has been extensively investigated while its long term performance induced by the cyclic loading under the surrounding soil influence lack of study this study aims to investigate the influence of one way cyclic horizontal loading on the long term performance of the wide shallow bucket foundation wsbf model for offshore wind turbine in saturated sand by using single gravity 1 g model tests specifically a series of cyclic experiments with varied loading conditions including load magnitude p load frequency f excitation height h and cycle number n were conducted to reveal the influence of loading conditions on dynamic characteristics and accumulated rotation of the wsbf model furthermore the regression analysis method based on artificial neural network ann model is proposed to determine the relationship between loading conditions and the long term performance of the wsbf model it is shown that the natural frequency rises and damping ratio decreases with the increase of cyclic loading number during the early cyclic stage afterwards there is a declining or stable trend for the natural frequency and a rising trend for the damping ratio of the wsbf model as for the accumulated rotation more than 80 foundation rotation occurs in the first hundred cycles whereas accumulated rotation for rest cycles tends to be small finally multiple regression analysis and sensitivity analysis based on the ann model are demonstrated to evaluate and predict the changes in long term performance of the wsbf model with high accuracy in this study keywords offshore wind turbine owt wide shallow bucket foundation wsbf 1 g model tests long term performance artificial neural network ann 1 introduction recently wind energy has been playing an increasingly important role in successful transitions from fossil fuels to renewable energy compared with onshore wind energy offshore wind energy holds merits such as better quality of the wind resources larger suitable free area to install and less influence on the environment de azevedo et al 2016 oh et al 2018 the last few decades have witnessed the rapid development of offshore wind power industry throughout the world michalak and zimny 2011 keivanpour et al 2017 sahu 2018 due to the harsh ocean environment offshore wind turbine owt structures are subjected to various cyclic excitations such as wind wave current and 1p 3p rotor imbalance excitations which both pose great threatens to the owt structures and bring challenges to designers and manufacturers unobe and sorensen 2015 according to standards such as dnv gl three design approaches including soft soft soft stiff and stiff stiff are available for owt structures based on both natural frequency and 1p 3p rotor frequencies from one 3 mw owt with the rotation speed ranging from 8 5 rpm to 13 5 rpm as described in fig 1 a laszlo arany et al 2017 the soft soft design means that the structural modal frequency is less than 1p rotor frequency on the contrary the first natural frequency in the stiff stiff design method always exceeds the upper limit of the 3p band and the owt needs a very stiff support structure however the soft stiff design approach is commonly used for sake of economic efficiency and structural safety considering the defect of two above design methods jalbi s et al 2018 a campbell diagram of the 3 mw owt was used for the structural resonance check by illustrating the relationship between the natural frequencies of the owt structure at 0 33hz and harmonic excitation of 1p 3p frequencies as shown in fig 1 b when the soft stiff design method is adopted the natural frequencies of owt structures represented by red line should lie in the gray area and be also influenced by the change of structural stiffness damgaard et al 2014b bucket foundation known as a new type for owt wang et al 2018 includes different types such as mono bucket foundation liu et al 2017 mono bucket foundation with compartments ding et al 2015 zhang et al 2016 inter outer bucket foundation and hybrid bucket with monopile foundation as li et al 2015 wang et al 2017 shown in fig 2 have been put forward to be applicable to specific ocean conditions and soil types the most outstanding advantage of this new foundation is high lateral and moment resistance provided by the extended cylindrical skirt or compartments wang et al 2018 due to its cost effectiveness environment friendliness and high reliability bucket foundation is considered as a promising alternative to other foundations for owt chen et al 2016 lian et al 2012 in 2017 one type of wide shallow bucket foundation wsbf with pre stressed concrete transition lian et al 2011 2014 dong et al 2018 was applied in an offshore wind farm of china as shown in fig 3 furthermore the corresponding integrated installation technology including the onshore prefabrication of foundation onshore installation and debugging of wind turbine offshore integrated transportation and installation of the whole wind turbine was innovatively proposed in order to greatly save high transportation and installation costs as well as construction time this set of technology provides a novel and effective way for owt to achieve high efficiency low cost and large scale construction as recommended in standards like iec 2005 dnv 2010 and gl 2005 serviceability limit state sls ultimate limit state uls and fatigue limit state fls are main design considerations for owt structures laszlo arany et al 2017 in general when owt structures are designed only to meet these three states based on the initial design condition the long term performance especially dynamic characteristics and accumulated rotation after thousands or millions cyclic loading which may lead to potential risk and damage are easily neglected normally the service life of owt ranges from 25 years to 30 years during which the structures may undergo more than 107 to 108 cyclic loading lombardi et al 2013 yu et al 2015 so far though there have been a lot of long term monitoring records of owt structures the dynamic characteristics after cyclic loading haven t yet been investigated clearly the influence of long term cyclic loading on owt structures are mainly manifested in the following two aspects 1 the changes in stiffness and damping of foundation which would have an effect on dynamic characteristics of the structures 2 accumulated rotation of bucket foundation which tends to result in a large inclination then amplify the horizontal displacement at the top of tower lombardi et al 2013 these two impacts pose great threats to the structural safety of the owts on one hand the soft stiff approach is commonly adopted in the design of the owt structures of which the allowable frequency range is limited according to the dnv gl standard thus the stiffness changes of the owt caused by the cyclic loading may result in the resonance risk and fatigue damage on the other hand both the poor operating efficiency risk and overturning risk of wind turbine may be attributed to the obvious accumulated inclination of the foundations in previous studies numerical simulation and laboratory experiment are two generally employed approaches to investigate the long term performance of owt foundations achmus achmus et al 2009 introduced degradation stiffness approach to reveal the influence of cyclic loading on sandy soil and made use of numerical simulation to describe factors that impact the behaviour of monopile under long term cyclic lateral loading depina depina et al 2015 simulated the response of monopile foundation by three dimensional finite element method in which probabilistic assessment of monopile foundations subjected to long term lateral cyclic loading were conducted to indicate the variability of sand stiffness on the sls of the foundations giannakos giannakos et al 2012 also developed three dimensional finite element analysis to describe the plastic shakedown response of pile soil system in sand subjected to cyclic lateral loading and presented two mechanisms soil densification and system densification to explain the dynamic response damgaard damgaard et al 2014a analyzed lateral response of monopile foundation using beam theory on a nonlinear winkler model and utilized numerical approaches to investigate stiffness changes of foundation and subsoil due to cyclic load additionally the single gravity 1 g or centrifuge experiments were adopted by many researchers to investigate the long term performance of pile or bucket foundation in soil under cyclic loading barari and ibsen 2017 developed dimensionless frameworks to obtain the impedance functions accounting for soil structure interaction for suction caissons under cyclic lateral loading and carried out experiments to gather statistics of long term response of suction foundations bhattacharya bhattacharya et al 2011 2013 proposed similitude relationships between physical model and owt structures supported by multi pod foundations and conducted a series of scale tests to explain the influence of long term cyclic loading on natural frequency of the model doherty and gavin 2012 considered the factors influencing the response of offshore piles installed in soft clay by static and rapid cyclic loading experiments and performed stable and unstable analysis to reveal the increments of plastic displacement during excitation process lombardi lombardi et al 2013 conducted laboratory tests with a scaled model of owt structures supported by a monopile in kaolin clay subjected to cyclic horizontal loading and found that the shear strain level in soil had great influence on the natural frequency of the model by monitoring the changes in dynamic characteristics of the model zhu zhu et al 2013 assessed cyclic response of suction caisson by 1 g tests and explored settlement accumulated rotation and unloading stiffness response of the foundation by applying one way or two way cyclic loading with different characteristics hung hung et al 2018 studied accumulated rotation and unloading stiffness of two bucket foundation models with different embedment ratios by 1 g laboratory tests in soft clay under 104 cyclic horizontal loading and put forward empirical equations to evaluate the long term performance of the foundations zhang zhang et al 2007 carried out centrifuge modeling to investigate the behavior of suction bucket foundations under cyclic lateral loading induced by ice sheet and showed that the pore pressures and deformation increased with the load amplitude kim kim et al 2014 performed centrifuge tests to investigate monotonic and cyclic behavior of a tripod bucket foundation and compared the results with those of a monopod bucket foundation nielsen nielsen et al 2017 carried out experiments on a scaled bucket foundation in saturated dense sand subjected to cyclic loading at the excitation frequency of 1 hz with one way and two way direction and demonstrated that the accumulated rotation of the foundations was impacted by loading frequency and preloading kuo y s kuo y s et al 2018 also completed the numerical study on the deformation response of the monopod bucket foundation under lateral loads different from the previous studies the relationship between multiple influencing factors and the long term performance of the wsbf model has firstly been analyzed and discussed in this study based on the experiment results obtained from 1 g model tests the change of all dynamic features including natural frequency damping ratio and accumulated rotation due to the cyclic load has been considered in order to establish the relationship among the loading conditions and influencing factors which is cost effective and reasonable moreover the comparison between the model results with previous studies and the regression analysis were conducted to indicate that both the soil parameters and loading conditions have a significant effect on the dynamic characteristics of the wsbf model which provides a new insight into the long term performance of the owt foundation the framework of article is explained as follows firstly the experiment setup and procedure are introduced in section 2 next in section 3 the data processing and regression methods including fft based welch method and artificial neural network model ann are briefly presented then the experiment results and discussions are illustrated in section 4 finally section 5 presents a conclusion for the whole work which puts forward limitations and suggestions for further studies this study is looking forward to provide guidance and prediction for the long term performance of the owt structures supported by wsbf in saturated sand under the influence of cyclic loading 2 experiment setup and procedure 2 1 experiment setup in order to investigate the influence of cyclic loading on the long term performance of the wsbf model a series of tests with the detailed system layout and test equipment of the 1 g cyclic experiment displayed in fig 4 were conducted the entire experiment systems included model system cyclic loading system and signal acquisition system to be specific the model systems were established based on a bucket foundation model soil box consisting of steel frame plexiglass plates and drainage pipes crushed stone and water saturated sand then the cyclic loading systems were comprised of hev 200 exciter hea 200c power amplifier and dds digital synthesis function instrument the signal acquisition systems included vibration sensor load sensor data collector and signal processing software one way cyclic loading was provided by actuator equipped with power amplifier and signal generation the actuator was fixed on the channel steel which was connected with the angle steel at the both ends openings were provided on the channel steel and the angle steel to adjust the height of the actuator in fig 4 a a b and c were respectively denoted as a low frequency vibration sensor a flat diaphragm load cell and a dynamic inclinometer the intelligent data acquisition and signal analysis system was installed to record and collect data the geometrical parameters of the bucket foundation model which were determined for considering a length scale ratio of 1 100 were listed in table 1 the skirt thickness was set to 2 mm and the top lid with the thickness of 5 mm was bolted to the steel plate which was tied to the tower to prevent deformation at the connection between the two parts hung et al 2018 the bucket foundation model and tower were made of q235 steel with young s modulus of 210 gpa and poisson s ratio of 0 3 the tower structure was a cylinder with a length of 150 cm a diameter of 8 cm and a wall thickness of 2 mm and was welded with 2 mm thick steel plates on the top and bottom further the length width and height of the soil box are 150 cm 150 cm and 100 cm as shown in fig 4 b respectively as described in fig 5 there were 70 cm thickness of sand layer and 20 cm thickness of crushed stone layer in the soil box separated by geotextile and steel mesh and two rows of drainage pipe which played a role in rapid drainage based on the reference bhattacharya s et al 2013 any wall boundary effects can get negligible at about 5 times the foundation radius in order to further minimize the boundary effect of soil box on the model during the cyclic loading the soil box was also surrounded by 2 cm thick polystyrene foam panels to dissipate the energy the water level in the soil box was kept consistent with the top of the model throughout the experiment it has been demonstrated that the parameters of saturated sand had a great influence on soil structure interaction achmus et al 2009 depina et al 2015 bhattacharya et al 2013 hence it is necessary to further determine the grain composition and shear strength parameters of the sand based on the screening tests which were conducted to determine the grain size distribution to assess the diameter of the sand and direct shear strength experiment as described in fig 6 the sand used in this test is mainly composed of medium sized sand grain size ranging from 0 25 to 0 5 mm accounting for about 70 of the total sand the coefficients of uniformity c u and curvature c c were determined to be 3 1 and 1 3 respectively then it can be obtained from shear strength tests that the cohesive strength and internal friction angle of the sand was c 23 7 kpa and ϕ 14 5 respectively the saturated sand density used in this study was 1935 kg m3 and compressive modulus was 5 6 mpa 2 2 experiment procedure in order to study the effects of loading conditions including load magnitude p load frequency f excitation height h and cycle number n on the long term behaviors of the wsbf model 12 experiments were performed with a varying number of factors listed in table 2 each factor mentioned above has different values to reveal its influences on the changes in dynamic characteristics and accumulated rotation of the model the one way cyclic horizontal loading was generated by the actuator and the load magnitudes and frequencies were controlled by a power amplifier and a dds digital synthesis function instrument the waveform of cyclic loading was a half sine wave with the load peaks respectively chosen as 20 n and 40 n and the excitation frequencies were set to be 3 hz and 5 hz respectively in addition the ratio of length to diameter l d of bucket foundation model was selected as 1 3 the flowchart of experimental procedure is described in fig 7 firstly the wsbf model was located in center of the soil box and pushed into the sand until the top lid contacted with the sand secondly the vibration sensor load cell and dynamic inclination were installed and the initial rotation and test condition of the wsbf model were recorded thirdly hammering on the middle of the tower was conducted to achieve free decay tests and the judgment of termination condition was performed based on scheduled cycles afterwards the actuator and data collector were started simultaneously after the loading frequency and time step input into the dds digital synthesis function instrument during the cycle loading process finally free attenuation tests were carried out three times after desired numbers of cyclic loading in this study it was essential to assume that the model was kept the same and no fatigue damage of the structure occurred during all the tests to ensure that the changes in the dynamic characteristics and accumulated rotation were completely attributed to the changes in the foundation stiffness and damping caused by long term cyclic loading 3 methods for data processing and regression 3 1 data processing method in order to obtain the changes in natural frequency and damping ratio of the wsbf model after desired numbers of cyclic loading the data obtained from free attenuation tests was analyzed in both time and frequency domains the assessment of natural frequency was carried out through the fast fourier transform fft based welch method which was used to estimate the power spectral density psd of the vibration response at the top of tower the first natural frequency of the wsbf model was supposed to be the peak frequency with the highest psd value while the damping ratio was assessed in the time domain via logarithmic decrement method liang 2005 the typical result of free attenuation in time history represented by black line and its corresponding psd were shown in fig 8 it can be observed from the figure that the damping ratio ξ 0 could be calculated as 1 58 by the equation shown in fig 8 a and natural frequency f 0 of 5 hz was determined in red dash line intersected with x axis in fig 8 b it should be mentioned that the interaction characteristics between the bucket foundation model and sand was so complicated that it was almost impossible to measure directly however the changes in dynamic characteristics and accumulated rotation of the model caused by cyclic loading to some extent could reflect the variations in the interaction based on the assumption that the tower the model and connecting part remain the same during the experiments to illustrate changes in the long term performance of the wsbf model the evolutions of natural frequency damping ratio and accumulated rotation can be evaluated in terms of the normalized groups η ζ φ expressed in the following equations bhattacharya et al 2011 zhu et al 2013 yu et al 2015 1 η f n f i n i ζ ξ n ξ i n i φ θ n θ i n i θ i n i where f n ξ n and θ n are respectively natural frequency damping ratio and accumulated rotation after n cycle loading while f i n i ξ i n i and θ i n i are the corresponding initial values respectively however it is worth noting that θ i n i is set to be accumulated ration in the first 100 cyclic loading considering significant plastic deformation in early stage zhu et al 2013 3 2 artificial neural network model ann artificial neural network ann which has been widely used in engineering problem to find the mapping patterns between input data and output results naderpour et al 2018 was employed to obtain the relationship between loading conditions and the long term performance of the wsbf model in general the fundamental architecture of an ann composes of three different layers including an input layer a hidden layer and an output layer and the numbers of the input and output nodes are entirely problem dependent hertz 2018 after trial and error analysis the architecture of the ann model used in the study is shown in fig 9 a four layer feedback ann model is constructed with one input layer one output layer and two hidden layers included and implemented by means of the ann program code in the model loading conditions i e p f h and n are chosen as input variables while normalized natural frequency η normalized damping ratio ζ and normalized accumulated rotation φ are set to be output variables moreover the levenberg marquardt l m algorithm is selected as learning algorithm to alter the individual neural weights between nodes aiming at training networks and then transfer functions tansig and purelin are selected for hidden and output layers as activation functions respectively the performance of the applied ann can be assessed by statistical error analysis the mean square error mse and regression coefficient r which are chosen as indicators for the performance of the ann are calculated as the following equations hertz 2018 2 m s e 1 n i 1 n y i pred y i exp 2 r i 1 n y i pred y a v g pred y i exp y a v g exp i 1 n y i pred y a v g pred 2 i 1 n y i exp y a v g exp 2 where y i pred is the predicted output value from ann model y i exp is the experimental value while y a v g pred and y a v g exp are the average predicted and experimental values and n is the number of experimental data points 4 results and discussions 4 1 changes in natural frequency and damping ratio of the wsbf model experiment results for the changes in natural frequency and damping ratio of the wsbf model are presented in fig 10 and fig 11 which could be divided into 3 groups according to actuator heights blue circles black star pink squares and red triangles are used to represent the results of four tests in each group respectively for the sake of reflecting the trends of discrete points cubic spline interpolation is adopted to perform curve fitting the normalized natural frequency calculated by eq 1 is used to characterize the variation of the natural frequency of the wsbf model with increasing cycle number it can be observed that the normalized natural frequencies varies in the range from 1 0 to 1 45 the overall trends of these experiment results experienced a sharp increase in the early stage and then decreased slightly or remained stable when the bucket foundation model was pressed into the sand it had a certain degree of disturbance influence on the surrounding sand further the main reasons for the trend which could reflect the changes of foundation stiffness bhattacharya et al 2011 might be that the interior sand of the model became gradually densified with the increasing number of cyclic loading and the top lid of the model was in full contact with the sand though there could be some space between the model top lid and the sand in the initial phase thus the interior sand and the model gradually became an integral whole with the increasing loading number in the early stage which meant constraint enhancement and additional stiffness effect may occur during the tests and had some obvious influence on the dynamic characteristics of the wsbf model however when a slightly tilt of the model produced by the continuous cyclic loadings occurred the surrounding sand around the model would decentralize due to long term vibration which might bring out weaken constraints to the model hence the natural frequency of the wsbf model might tend to show some decline furthermore different variations in natural frequencies are described in fig 10 when the loading conditions as load magnitude p load frequency f and excitation height h change for instance when heights of the actuator are 90 cm and 70 cm results from the test2 test4 test6 test8 with the higher load magnitude were ranged above over those of other tests including test1 test3 test5 test7 while there is no great difference among these four tests in the 50 cm height in addition the load frequency also has a significant influence on the dynamic behaviors of the model based on the results from test4 test8 test12 compared with the ones obtained from test2 test6 test10 it can be summarized that the closer the load frequency to the natural frequency is the higher the load amplitude is more obvious change in the natural frequency of the wsbf model which is dependent on these loading conditions will occur because of the possibility of resonance effect however the excitation height make a weak influence on the change of natural frequency generally speaking the material damping sand damping and radiant damping are three important damping ratios of the owt as mentioned above structural material damping is assumed to remain the same during these tests hence the changes in the damping ratio of the wsbf model come from the sand damping and radiant damping that may be attributed to changes in surrounding sand in which energy is absorbed into the soil as radiation damping as the sand is subjected to cyclic loading the energy dissipates into the sand by means of viscosity and plastic deformations bisoi and haldar 2014 due to densification or decentralization of the interior and exterior sand changes in sand damping and radiant damping may occur under the cyclic loadings in all these experiments the normalized damping ratios of the wsbf model which varied in the range from 0 39 to 1 0 are shown in fig 11 it can be observed that the trends of normalized damping ratio undergo a significant decline firstly and then steadily recovered to a stable level in all the tests but there is noticeable variation among these tests with different loading conditions for example in the height of 90 cm and 70 cm differences of these results are obvious with a large range of variation while smaller ones are observed with the height of 50 cm furthermore both the influences of load magnitude p and load frequency f on the variations in damping ratio of the wsbf model are insignificant therefore it is necessary to perform a detailed analysis to reveal the influence of different loading conditions on the damping ratio of the wsbf model further it can be seen from the phenomenon of experiments a slight amount of settlement will occur in the wsbf model under the effect of vertical load and moment due to the rotation of the foundation top the soil on the front side of the foundation slightly uplifts and a local depression may occur in the back side of the foundation with the change of accumulated deformation as shown in fig 12 a with the continuous action of unidirectional cyclic excitation the sand inside the bucket gradually becomes denser on the contrary the sand outside the foundation will be disturbed because of the cyclic excitation and locally weaken to some extent as described in fig 12 b as view of the complex effect of compaction and weakening of non linear sand the change of the restraining effect on the bucket foundation and the interaction mechanism between soil and foundation will result in the obvious change in the natural frequency of the wsbf model system simultaneously the effect of compaction and weakening of sand change the soil damping characteristics which dissipates energy in the form of waves and it lead to the change of the entire system s damping 4 2 variation in accumulated rotation of the bucket foundation model the normalized accumulated rotation of the model with the increase of cyclic loading number is described in fig 13 it can be seen that a considerable increase appears from initialization to around the first hundred cycles and then the normalized accumulated rotation maintained at the constant level hence more than 80 accumulated rotation of the sand occurs during the first hundreds of cycles after which the rate of the increase decreases with the cycle number the main reason can be attributed to most of plastic deformation of the sand adjacent to the model occurs in the beginning stage of cyclic loading and then subsequent plastic deformation tends to be much smaller it is also noticed that different tests present the similar shape of curves but the magnitudes of the normalized accumulated rotation increase with the increase of the loading frequencies and magnitudes different loading conditions could lead to evident variations in accumulated rotation of the model therefore the results show that the variations in accumulated rotation are affected by the loading conditions as well 4 3 comparison with previous studies in previous studies the long term performance of the monopile or bucket foundation supported structures in clay or sand were investigated as mentioned in section i compared with these results the changes in natural frequency damping ratio and accumulated rotation can be observed in both similar and dissimilar trends of experimental results in this study the normalized cycle number is defined as the value of the cycle number divided by the maximum cycle number and used as x axis parameter in fig 14 the overall change trend of normalized natural frequency in the saturated sand follows nonlinear relationship showing first increase with a reduced rate stabilize and later decrease which is consistent with the observations reported by yu 2015 as shown in fig 14 a however due to cohesive clay adopted in the experiments a decline trend in natural frequency was presented by lombardi 2013 and bhattacharya 2011 for monopile foundation with the increase of cycle number two reasons can be found to explain the above difference between this study and previous studies on one hand the foundation stiffness around the sand will be strengthened at the initial stage of the cyclic loading while the stiffness around the clay will be weakened under the same load conditions which will cause the modal frequency of the model to decrease on the other hand the inclination of wsbf foundation is smaller than that of the monopile foundation under the same excitation condition which may also lead to the reduction of the modal frequency of the test model with regarding to changes in damping ratio a linear decreasing and nonlinear increasing relationship were observed by barari and ibsen 2017 with bucket foundation in sand and bhattacharya 2011 with monopile foundation in cohesive clay respectively which were out of trend lines with the data obtained by the 1 g cyclic experiment in this study as shown in fig 14 b this phenomenon can be explained that the increase in the overall foundation stiffness around the sand may make the surrounding sand damping ratio and radiation damping ratio both decrease however the opposite change trend of damping ratio in the clay test under the cyclic loading condition occurs because of the reduced foundation stiffness when it comes to the variation in accumulated rotation studies completed by zhu 2013 foglia 2015 nielsen 2017 and hung 2018 presented that the most deformation occurred within the first hundred cycles and the subsequent rotation became insignificant as loading continued which corresponds very well to the finding of this study based on the discussions mentioned above it can be concluded that the long term performance is strongly dependent on soil parameters as well as foundation form meanwhile the loading conditions are also one of important influencing aspects that should not be neglected 4 4 regression analysis by ann in order to evaluate and predict the changes in long term performance of the wsbf model with high accuracy the best performance for networks are nn 4 5 5 3 structure as mentioned above which means there was one input layer with four nodes two hidden layers with five nodes each and one output layer with four nodes in the ann model as shown in fig 9 a summary of the training results for ann model used in this study is presented in fig 15 and fig 16 it can be observed that the mse values depict a declining trend for the constructed ann model with training epoch s increase which shows a good indication of the learning process for networks afterwards regression analysis between ann outputs and experimental data are described in fig 16 which exhibites up to correlation coefficient of 0 991 0 988 0 987 and 0 990 for ann outputs and experiment data for training validation test and all of them respectively furthermore the comparison between experimental data and simulated results were are collected in fig 17 which demonstrates that the ann results match very well with experimental data hence it is indicated that the ann model is well trained and can be used to conduct regression analysis with an acceptable degree of accuracy in this regard a comprehensive analysis with multiple factors is constructed to link loading conditions to the long term performance of the wsbf model 4 5 sensitivity analysis sensitivity analysis aimed to explore the proportion contribution of each input parameter to the output within the constructed ann several methods have been put forward for the ann such as perturb method and profile method li et al 2012 in this study the method proposed by milne 1995 is used 3 r i o j 1 l w j i w o j l 1 n w j l k 1 n j 1 l w j k w o j l 1 n w j l where r i o is the importance of input factors representing the relative contribution of the input node i to the output node o n l and m are the number of the nodes in input layer hidden and output layer respectively then w j i and w o j are weights connecting input layer with hidden layer hidden layer with output layer respectively however it should be mentioned that there are two hidden layers in the constructed ann models that need a slight modification in the eq 3 the importance of loading factors p f h n to reveal the contribution to the long term performance of the wsbf model is described in fig 18 it can be observed that the load frequency f is the most influenced factor followed by the load magnitude p and cycle number n while the excitation height h has the comparatively least influence on the long term performance 5 conclusion and outlook the long term performance including changes in natural frequency damping ratio and accumulated rotation plays an important role in the safety of the owt which would be easily neglected in the design and analysis hence the long term performance of the wsbf model under cyclic loadings is investigated based on a series of 1 g experiments in this study the main conclusions can be summarized as follows 1 based on the experiment results the normalized natural frequency experience first increase with reduced rate stabilize and later decrease while there is a trend of first decline and later steadily recover to stabilization for the normalized damping ratio regarding the normalized accumulated rotation of the model it increases with the increasing cycle number in which more than 80 rotation of the model occurs in the first hundred cycles the long term performance of the wsbf model may be attributed to the changes in the sand adjacent to the bucket foundation model in the experiment 2 compared with the results of previous studies there are some similarities in the trend of variations in normalized natural frequency and accumulated rotation of the wsbf model in this experiment however different trend of the changes in the normalized damping ratio can be observed between this and previous studies due to the differences in soil parameters as well as foundation form 3 the ann model is used to conduct regression analysis to establish the relationship between loading conditions and long term performance of the wsbf model based on the experiment data which will provide a guidance and prediction for experiment with high accuracy furthermore sensitivity analysis reveals that loading frequency can impact most significantly on the long term performance of the wsbf model followed by the loading magnitude and cycle number however the limitations of this study are also obvious on one hand this study is limited to one embedded ratio of the bucket foundation model and one single type of sand and one way loading on the other hand the loading condition is chosen to be limited number of cycles and one way direction hence more cycle numbers multi direction loading different embedded ratio and soil parameters are looking forward to be investigated to perform more comprehensive study for the long term performance of bucket foundation in the further study credit authorship contribution statement jijian lian conceptualization funding acquisition resources supervision yue zhao data curation formal analysis writing original draft xiaofeng dong funding acquisition methodology validation writing review editing chong lian investigation software haijun wang project administration visualization declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this project was supported by the fund for national natural science foundation of china 51709202 innovation method fund of china 2016im030100 and the fund for key research area innovation groups of china 2014ra4031 support from the funding agency is sincerely acknowledged the writers acknowledge the assistance of anonymous reviewers as well 
20844,the bucket foundation is considered as a promising alternative to conventional foundations for offshore wind turbines owts due to its cost effectiveness and high reliability the behavior of bucket foundation under static loading has been extensively investigated while its long term performance induced by the cyclic loading under the surrounding soil influence lack of study this study aims to investigate the influence of one way cyclic horizontal loading on the long term performance of the wide shallow bucket foundation wsbf model for offshore wind turbine in saturated sand by using single gravity 1 g model tests specifically a series of cyclic experiments with varied loading conditions including load magnitude p load frequency f excitation height h and cycle number n were conducted to reveal the influence of loading conditions on dynamic characteristics and accumulated rotation of the wsbf model furthermore the regression analysis method based on artificial neural network ann model is proposed to determine the relationship between loading conditions and the long term performance of the wsbf model it is shown that the natural frequency rises and damping ratio decreases with the increase of cyclic loading number during the early cyclic stage afterwards there is a declining or stable trend for the natural frequency and a rising trend for the damping ratio of the wsbf model as for the accumulated rotation more than 80 foundation rotation occurs in the first hundred cycles whereas accumulated rotation for rest cycles tends to be small finally multiple regression analysis and sensitivity analysis based on the ann model are demonstrated to evaluate and predict the changes in long term performance of the wsbf model with high accuracy in this study keywords offshore wind turbine owt wide shallow bucket foundation wsbf 1 g model tests long term performance artificial neural network ann 1 introduction recently wind energy has been playing an increasingly important role in successful transitions from fossil fuels to renewable energy compared with onshore wind energy offshore wind energy holds merits such as better quality of the wind resources larger suitable free area to install and less influence on the environment de azevedo et al 2016 oh et al 2018 the last few decades have witnessed the rapid development of offshore wind power industry throughout the world michalak and zimny 2011 keivanpour et al 2017 sahu 2018 due to the harsh ocean environment offshore wind turbine owt structures are subjected to various cyclic excitations such as wind wave current and 1p 3p rotor imbalance excitations which both pose great threatens to the owt structures and bring challenges to designers and manufacturers unobe and sorensen 2015 according to standards such as dnv gl three design approaches including soft soft soft stiff and stiff stiff are available for owt structures based on both natural frequency and 1p 3p rotor frequencies from one 3 mw owt with the rotation speed ranging from 8 5 rpm to 13 5 rpm as described in fig 1 a laszlo arany et al 2017 the soft soft design means that the structural modal frequency is less than 1p rotor frequency on the contrary the first natural frequency in the stiff stiff design method always exceeds the upper limit of the 3p band and the owt needs a very stiff support structure however the soft stiff design approach is commonly used for sake of economic efficiency and structural safety considering the defect of two above design methods jalbi s et al 2018 a campbell diagram of the 3 mw owt was used for the structural resonance check by illustrating the relationship between the natural frequencies of the owt structure at 0 33hz and harmonic excitation of 1p 3p frequencies as shown in fig 1 b when the soft stiff design method is adopted the natural frequencies of owt structures represented by red line should lie in the gray area and be also influenced by the change of structural stiffness damgaard et al 2014b bucket foundation known as a new type for owt wang et al 2018 includes different types such as mono bucket foundation liu et al 2017 mono bucket foundation with compartments ding et al 2015 zhang et al 2016 inter outer bucket foundation and hybrid bucket with monopile foundation as li et al 2015 wang et al 2017 shown in fig 2 have been put forward to be applicable to specific ocean conditions and soil types the most outstanding advantage of this new foundation is high lateral and moment resistance provided by the extended cylindrical skirt or compartments wang et al 2018 due to its cost effectiveness environment friendliness and high reliability bucket foundation is considered as a promising alternative to other foundations for owt chen et al 2016 lian et al 2012 in 2017 one type of wide shallow bucket foundation wsbf with pre stressed concrete transition lian et al 2011 2014 dong et al 2018 was applied in an offshore wind farm of china as shown in fig 3 furthermore the corresponding integrated installation technology including the onshore prefabrication of foundation onshore installation and debugging of wind turbine offshore integrated transportation and installation of the whole wind turbine was innovatively proposed in order to greatly save high transportation and installation costs as well as construction time this set of technology provides a novel and effective way for owt to achieve high efficiency low cost and large scale construction as recommended in standards like iec 2005 dnv 2010 and gl 2005 serviceability limit state sls ultimate limit state uls and fatigue limit state fls are main design considerations for owt structures laszlo arany et al 2017 in general when owt structures are designed only to meet these three states based on the initial design condition the long term performance especially dynamic characteristics and accumulated rotation after thousands or millions cyclic loading which may lead to potential risk and damage are easily neglected normally the service life of owt ranges from 25 years to 30 years during which the structures may undergo more than 107 to 108 cyclic loading lombardi et al 2013 yu et al 2015 so far though there have been a lot of long term monitoring records of owt structures the dynamic characteristics after cyclic loading haven t yet been investigated clearly the influence of long term cyclic loading on owt structures are mainly manifested in the following two aspects 1 the changes in stiffness and damping of foundation which would have an effect on dynamic characteristics of the structures 2 accumulated rotation of bucket foundation which tends to result in a large inclination then amplify the horizontal displacement at the top of tower lombardi et al 2013 these two impacts pose great threats to the structural safety of the owts on one hand the soft stiff approach is commonly adopted in the design of the owt structures of which the allowable frequency range is limited according to the dnv gl standard thus the stiffness changes of the owt caused by the cyclic loading may result in the resonance risk and fatigue damage on the other hand both the poor operating efficiency risk and overturning risk of wind turbine may be attributed to the obvious accumulated inclination of the foundations in previous studies numerical simulation and laboratory experiment are two generally employed approaches to investigate the long term performance of owt foundations achmus achmus et al 2009 introduced degradation stiffness approach to reveal the influence of cyclic loading on sandy soil and made use of numerical simulation to describe factors that impact the behaviour of monopile under long term cyclic lateral loading depina depina et al 2015 simulated the response of monopile foundation by three dimensional finite element method in which probabilistic assessment of monopile foundations subjected to long term lateral cyclic loading were conducted to indicate the variability of sand stiffness on the sls of the foundations giannakos giannakos et al 2012 also developed three dimensional finite element analysis to describe the plastic shakedown response of pile soil system in sand subjected to cyclic lateral loading and presented two mechanisms soil densification and system densification to explain the dynamic response damgaard damgaard et al 2014a analyzed lateral response of monopile foundation using beam theory on a nonlinear winkler model and utilized numerical approaches to investigate stiffness changes of foundation and subsoil due to cyclic load additionally the single gravity 1 g or centrifuge experiments were adopted by many researchers to investigate the long term performance of pile or bucket foundation in soil under cyclic loading barari and ibsen 2017 developed dimensionless frameworks to obtain the impedance functions accounting for soil structure interaction for suction caissons under cyclic lateral loading and carried out experiments to gather statistics of long term response of suction foundations bhattacharya bhattacharya et al 2011 2013 proposed similitude relationships between physical model and owt structures supported by multi pod foundations and conducted a series of scale tests to explain the influence of long term cyclic loading on natural frequency of the model doherty and gavin 2012 considered the factors influencing the response of offshore piles installed in soft clay by static and rapid cyclic loading experiments and performed stable and unstable analysis to reveal the increments of plastic displacement during excitation process lombardi lombardi et al 2013 conducted laboratory tests with a scaled model of owt structures supported by a monopile in kaolin clay subjected to cyclic horizontal loading and found that the shear strain level in soil had great influence on the natural frequency of the model by monitoring the changes in dynamic characteristics of the model zhu zhu et al 2013 assessed cyclic response of suction caisson by 1 g tests and explored settlement accumulated rotation and unloading stiffness response of the foundation by applying one way or two way cyclic loading with different characteristics hung hung et al 2018 studied accumulated rotation and unloading stiffness of two bucket foundation models with different embedment ratios by 1 g laboratory tests in soft clay under 104 cyclic horizontal loading and put forward empirical equations to evaluate the long term performance of the foundations zhang zhang et al 2007 carried out centrifuge modeling to investigate the behavior of suction bucket foundations under cyclic lateral loading induced by ice sheet and showed that the pore pressures and deformation increased with the load amplitude kim kim et al 2014 performed centrifuge tests to investigate monotonic and cyclic behavior of a tripod bucket foundation and compared the results with those of a monopod bucket foundation nielsen nielsen et al 2017 carried out experiments on a scaled bucket foundation in saturated dense sand subjected to cyclic loading at the excitation frequency of 1 hz with one way and two way direction and demonstrated that the accumulated rotation of the foundations was impacted by loading frequency and preloading kuo y s kuo y s et al 2018 also completed the numerical study on the deformation response of the monopod bucket foundation under lateral loads different from the previous studies the relationship between multiple influencing factors and the long term performance of the wsbf model has firstly been analyzed and discussed in this study based on the experiment results obtained from 1 g model tests the change of all dynamic features including natural frequency damping ratio and accumulated rotation due to the cyclic load has been considered in order to establish the relationship among the loading conditions and influencing factors which is cost effective and reasonable moreover the comparison between the model results with previous studies and the regression analysis were conducted to indicate that both the soil parameters and loading conditions have a significant effect on the dynamic characteristics of the wsbf model which provides a new insight into the long term performance of the owt foundation the framework of article is explained as follows firstly the experiment setup and procedure are introduced in section 2 next in section 3 the data processing and regression methods including fft based welch method and artificial neural network model ann are briefly presented then the experiment results and discussions are illustrated in section 4 finally section 5 presents a conclusion for the whole work which puts forward limitations and suggestions for further studies this study is looking forward to provide guidance and prediction for the long term performance of the owt structures supported by wsbf in saturated sand under the influence of cyclic loading 2 experiment setup and procedure 2 1 experiment setup in order to investigate the influence of cyclic loading on the long term performance of the wsbf model a series of tests with the detailed system layout and test equipment of the 1 g cyclic experiment displayed in fig 4 were conducted the entire experiment systems included model system cyclic loading system and signal acquisition system to be specific the model systems were established based on a bucket foundation model soil box consisting of steel frame plexiglass plates and drainage pipes crushed stone and water saturated sand then the cyclic loading systems were comprised of hev 200 exciter hea 200c power amplifier and dds digital synthesis function instrument the signal acquisition systems included vibration sensor load sensor data collector and signal processing software one way cyclic loading was provided by actuator equipped with power amplifier and signal generation the actuator was fixed on the channel steel which was connected with the angle steel at the both ends openings were provided on the channel steel and the angle steel to adjust the height of the actuator in fig 4 a a b and c were respectively denoted as a low frequency vibration sensor a flat diaphragm load cell and a dynamic inclinometer the intelligent data acquisition and signal analysis system was installed to record and collect data the geometrical parameters of the bucket foundation model which were determined for considering a length scale ratio of 1 100 were listed in table 1 the skirt thickness was set to 2 mm and the top lid with the thickness of 5 mm was bolted to the steel plate which was tied to the tower to prevent deformation at the connection between the two parts hung et al 2018 the bucket foundation model and tower were made of q235 steel with young s modulus of 210 gpa and poisson s ratio of 0 3 the tower structure was a cylinder with a length of 150 cm a diameter of 8 cm and a wall thickness of 2 mm and was welded with 2 mm thick steel plates on the top and bottom further the length width and height of the soil box are 150 cm 150 cm and 100 cm as shown in fig 4 b respectively as described in fig 5 there were 70 cm thickness of sand layer and 20 cm thickness of crushed stone layer in the soil box separated by geotextile and steel mesh and two rows of drainage pipe which played a role in rapid drainage based on the reference bhattacharya s et al 2013 any wall boundary effects can get negligible at about 5 times the foundation radius in order to further minimize the boundary effect of soil box on the model during the cyclic loading the soil box was also surrounded by 2 cm thick polystyrene foam panels to dissipate the energy the water level in the soil box was kept consistent with the top of the model throughout the experiment it has been demonstrated that the parameters of saturated sand had a great influence on soil structure interaction achmus et al 2009 depina et al 2015 bhattacharya et al 2013 hence it is necessary to further determine the grain composition and shear strength parameters of the sand based on the screening tests which were conducted to determine the grain size distribution to assess the diameter of the sand and direct shear strength experiment as described in fig 6 the sand used in this test is mainly composed of medium sized sand grain size ranging from 0 25 to 0 5 mm accounting for about 70 of the total sand the coefficients of uniformity c u and curvature c c were determined to be 3 1 and 1 3 respectively then it can be obtained from shear strength tests that the cohesive strength and internal friction angle of the sand was c 23 7 kpa and ϕ 14 5 respectively the saturated sand density used in this study was 1935 kg m3 and compressive modulus was 5 6 mpa 2 2 experiment procedure in order to study the effects of loading conditions including load magnitude p load frequency f excitation height h and cycle number n on the long term behaviors of the wsbf model 12 experiments were performed with a varying number of factors listed in table 2 each factor mentioned above has different values to reveal its influences on the changes in dynamic characteristics and accumulated rotation of the model the one way cyclic horizontal loading was generated by the actuator and the load magnitudes and frequencies were controlled by a power amplifier and a dds digital synthesis function instrument the waveform of cyclic loading was a half sine wave with the load peaks respectively chosen as 20 n and 40 n and the excitation frequencies were set to be 3 hz and 5 hz respectively in addition the ratio of length to diameter l d of bucket foundation model was selected as 1 3 the flowchart of experimental procedure is described in fig 7 firstly the wsbf model was located in center of the soil box and pushed into the sand until the top lid contacted with the sand secondly the vibration sensor load cell and dynamic inclination were installed and the initial rotation and test condition of the wsbf model were recorded thirdly hammering on the middle of the tower was conducted to achieve free decay tests and the judgment of termination condition was performed based on scheduled cycles afterwards the actuator and data collector were started simultaneously after the loading frequency and time step input into the dds digital synthesis function instrument during the cycle loading process finally free attenuation tests were carried out three times after desired numbers of cyclic loading in this study it was essential to assume that the model was kept the same and no fatigue damage of the structure occurred during all the tests to ensure that the changes in the dynamic characteristics and accumulated rotation were completely attributed to the changes in the foundation stiffness and damping caused by long term cyclic loading 3 methods for data processing and regression 3 1 data processing method in order to obtain the changes in natural frequency and damping ratio of the wsbf model after desired numbers of cyclic loading the data obtained from free attenuation tests was analyzed in both time and frequency domains the assessment of natural frequency was carried out through the fast fourier transform fft based welch method which was used to estimate the power spectral density psd of the vibration response at the top of tower the first natural frequency of the wsbf model was supposed to be the peak frequency with the highest psd value while the damping ratio was assessed in the time domain via logarithmic decrement method liang 2005 the typical result of free attenuation in time history represented by black line and its corresponding psd were shown in fig 8 it can be observed from the figure that the damping ratio ξ 0 could be calculated as 1 58 by the equation shown in fig 8 a and natural frequency f 0 of 5 hz was determined in red dash line intersected with x axis in fig 8 b it should be mentioned that the interaction characteristics between the bucket foundation model and sand was so complicated that it was almost impossible to measure directly however the changes in dynamic characteristics and accumulated rotation of the model caused by cyclic loading to some extent could reflect the variations in the interaction based on the assumption that the tower the model and connecting part remain the same during the experiments to illustrate changes in the long term performance of the wsbf model the evolutions of natural frequency damping ratio and accumulated rotation can be evaluated in terms of the normalized groups η ζ φ expressed in the following equations bhattacharya et al 2011 zhu et al 2013 yu et al 2015 1 η f n f i n i ζ ξ n ξ i n i φ θ n θ i n i θ i n i where f n ξ n and θ n are respectively natural frequency damping ratio and accumulated rotation after n cycle loading while f i n i ξ i n i and θ i n i are the corresponding initial values respectively however it is worth noting that θ i n i is set to be accumulated ration in the first 100 cyclic loading considering significant plastic deformation in early stage zhu et al 2013 3 2 artificial neural network model ann artificial neural network ann which has been widely used in engineering problem to find the mapping patterns between input data and output results naderpour et al 2018 was employed to obtain the relationship between loading conditions and the long term performance of the wsbf model in general the fundamental architecture of an ann composes of three different layers including an input layer a hidden layer and an output layer and the numbers of the input and output nodes are entirely problem dependent hertz 2018 after trial and error analysis the architecture of the ann model used in the study is shown in fig 9 a four layer feedback ann model is constructed with one input layer one output layer and two hidden layers included and implemented by means of the ann program code in the model loading conditions i e p f h and n are chosen as input variables while normalized natural frequency η normalized damping ratio ζ and normalized accumulated rotation φ are set to be output variables moreover the levenberg marquardt l m algorithm is selected as learning algorithm to alter the individual neural weights between nodes aiming at training networks and then transfer functions tansig and purelin are selected for hidden and output layers as activation functions respectively the performance of the applied ann can be assessed by statistical error analysis the mean square error mse and regression coefficient r which are chosen as indicators for the performance of the ann are calculated as the following equations hertz 2018 2 m s e 1 n i 1 n y i pred y i exp 2 r i 1 n y i pred y a v g pred y i exp y a v g exp i 1 n y i pred y a v g pred 2 i 1 n y i exp y a v g exp 2 where y i pred is the predicted output value from ann model y i exp is the experimental value while y a v g pred and y a v g exp are the average predicted and experimental values and n is the number of experimental data points 4 results and discussions 4 1 changes in natural frequency and damping ratio of the wsbf model experiment results for the changes in natural frequency and damping ratio of the wsbf model are presented in fig 10 and fig 11 which could be divided into 3 groups according to actuator heights blue circles black star pink squares and red triangles are used to represent the results of four tests in each group respectively for the sake of reflecting the trends of discrete points cubic spline interpolation is adopted to perform curve fitting the normalized natural frequency calculated by eq 1 is used to characterize the variation of the natural frequency of the wsbf model with increasing cycle number it can be observed that the normalized natural frequencies varies in the range from 1 0 to 1 45 the overall trends of these experiment results experienced a sharp increase in the early stage and then decreased slightly or remained stable when the bucket foundation model was pressed into the sand it had a certain degree of disturbance influence on the surrounding sand further the main reasons for the trend which could reflect the changes of foundation stiffness bhattacharya et al 2011 might be that the interior sand of the model became gradually densified with the increasing number of cyclic loading and the top lid of the model was in full contact with the sand though there could be some space between the model top lid and the sand in the initial phase thus the interior sand and the model gradually became an integral whole with the increasing loading number in the early stage which meant constraint enhancement and additional stiffness effect may occur during the tests and had some obvious influence on the dynamic characteristics of the wsbf model however when a slightly tilt of the model produced by the continuous cyclic loadings occurred the surrounding sand around the model would decentralize due to long term vibration which might bring out weaken constraints to the model hence the natural frequency of the wsbf model might tend to show some decline furthermore different variations in natural frequencies are described in fig 10 when the loading conditions as load magnitude p load frequency f and excitation height h change for instance when heights of the actuator are 90 cm and 70 cm results from the test2 test4 test6 test8 with the higher load magnitude were ranged above over those of other tests including test1 test3 test5 test7 while there is no great difference among these four tests in the 50 cm height in addition the load frequency also has a significant influence on the dynamic behaviors of the model based on the results from test4 test8 test12 compared with the ones obtained from test2 test6 test10 it can be summarized that the closer the load frequency to the natural frequency is the higher the load amplitude is more obvious change in the natural frequency of the wsbf model which is dependent on these loading conditions will occur because of the possibility of resonance effect however the excitation height make a weak influence on the change of natural frequency generally speaking the material damping sand damping and radiant damping are three important damping ratios of the owt as mentioned above structural material damping is assumed to remain the same during these tests hence the changes in the damping ratio of the wsbf model come from the sand damping and radiant damping that may be attributed to changes in surrounding sand in which energy is absorbed into the soil as radiation damping as the sand is subjected to cyclic loading the energy dissipates into the sand by means of viscosity and plastic deformations bisoi and haldar 2014 due to densification or decentralization of the interior and exterior sand changes in sand damping and radiant damping may occur under the cyclic loadings in all these experiments the normalized damping ratios of the wsbf model which varied in the range from 0 39 to 1 0 are shown in fig 11 it can be observed that the trends of normalized damping ratio undergo a significant decline firstly and then steadily recovered to a stable level in all the tests but there is noticeable variation among these tests with different loading conditions for example in the height of 90 cm and 70 cm differences of these results are obvious with a large range of variation while smaller ones are observed with the height of 50 cm furthermore both the influences of load magnitude p and load frequency f on the variations in damping ratio of the wsbf model are insignificant therefore it is necessary to perform a detailed analysis to reveal the influence of different loading conditions on the damping ratio of the wsbf model further it can be seen from the phenomenon of experiments a slight amount of settlement will occur in the wsbf model under the effect of vertical load and moment due to the rotation of the foundation top the soil on the front side of the foundation slightly uplifts and a local depression may occur in the back side of the foundation with the change of accumulated deformation as shown in fig 12 a with the continuous action of unidirectional cyclic excitation the sand inside the bucket gradually becomes denser on the contrary the sand outside the foundation will be disturbed because of the cyclic excitation and locally weaken to some extent as described in fig 12 b as view of the complex effect of compaction and weakening of non linear sand the change of the restraining effect on the bucket foundation and the interaction mechanism between soil and foundation will result in the obvious change in the natural frequency of the wsbf model system simultaneously the effect of compaction and weakening of sand change the soil damping characteristics which dissipates energy in the form of waves and it lead to the change of the entire system s damping 4 2 variation in accumulated rotation of the bucket foundation model the normalized accumulated rotation of the model with the increase of cyclic loading number is described in fig 13 it can be seen that a considerable increase appears from initialization to around the first hundred cycles and then the normalized accumulated rotation maintained at the constant level hence more than 80 accumulated rotation of the sand occurs during the first hundreds of cycles after which the rate of the increase decreases with the cycle number the main reason can be attributed to most of plastic deformation of the sand adjacent to the model occurs in the beginning stage of cyclic loading and then subsequent plastic deformation tends to be much smaller it is also noticed that different tests present the similar shape of curves but the magnitudes of the normalized accumulated rotation increase with the increase of the loading frequencies and magnitudes different loading conditions could lead to evident variations in accumulated rotation of the model therefore the results show that the variations in accumulated rotation are affected by the loading conditions as well 4 3 comparison with previous studies in previous studies the long term performance of the monopile or bucket foundation supported structures in clay or sand were investigated as mentioned in section i compared with these results the changes in natural frequency damping ratio and accumulated rotation can be observed in both similar and dissimilar trends of experimental results in this study the normalized cycle number is defined as the value of the cycle number divided by the maximum cycle number and used as x axis parameter in fig 14 the overall change trend of normalized natural frequency in the saturated sand follows nonlinear relationship showing first increase with a reduced rate stabilize and later decrease which is consistent with the observations reported by yu 2015 as shown in fig 14 a however due to cohesive clay adopted in the experiments a decline trend in natural frequency was presented by lombardi 2013 and bhattacharya 2011 for monopile foundation with the increase of cycle number two reasons can be found to explain the above difference between this study and previous studies on one hand the foundation stiffness around the sand will be strengthened at the initial stage of the cyclic loading while the stiffness around the clay will be weakened under the same load conditions which will cause the modal frequency of the model to decrease on the other hand the inclination of wsbf foundation is smaller than that of the monopile foundation under the same excitation condition which may also lead to the reduction of the modal frequency of the test model with regarding to changes in damping ratio a linear decreasing and nonlinear increasing relationship were observed by barari and ibsen 2017 with bucket foundation in sand and bhattacharya 2011 with monopile foundation in cohesive clay respectively which were out of trend lines with the data obtained by the 1 g cyclic experiment in this study as shown in fig 14 b this phenomenon can be explained that the increase in the overall foundation stiffness around the sand may make the surrounding sand damping ratio and radiation damping ratio both decrease however the opposite change trend of damping ratio in the clay test under the cyclic loading condition occurs because of the reduced foundation stiffness when it comes to the variation in accumulated rotation studies completed by zhu 2013 foglia 2015 nielsen 2017 and hung 2018 presented that the most deformation occurred within the first hundred cycles and the subsequent rotation became insignificant as loading continued which corresponds very well to the finding of this study based on the discussions mentioned above it can be concluded that the long term performance is strongly dependent on soil parameters as well as foundation form meanwhile the loading conditions are also one of important influencing aspects that should not be neglected 4 4 regression analysis by ann in order to evaluate and predict the changes in long term performance of the wsbf model with high accuracy the best performance for networks are nn 4 5 5 3 structure as mentioned above which means there was one input layer with four nodes two hidden layers with five nodes each and one output layer with four nodes in the ann model as shown in fig 9 a summary of the training results for ann model used in this study is presented in fig 15 and fig 16 it can be observed that the mse values depict a declining trend for the constructed ann model with training epoch s increase which shows a good indication of the learning process for networks afterwards regression analysis between ann outputs and experimental data are described in fig 16 which exhibites up to correlation coefficient of 0 991 0 988 0 987 and 0 990 for ann outputs and experiment data for training validation test and all of them respectively furthermore the comparison between experimental data and simulated results were are collected in fig 17 which demonstrates that the ann results match very well with experimental data hence it is indicated that the ann model is well trained and can be used to conduct regression analysis with an acceptable degree of accuracy in this regard a comprehensive analysis with multiple factors is constructed to link loading conditions to the long term performance of the wsbf model 4 5 sensitivity analysis sensitivity analysis aimed to explore the proportion contribution of each input parameter to the output within the constructed ann several methods have been put forward for the ann such as perturb method and profile method li et al 2012 in this study the method proposed by milne 1995 is used 3 r i o j 1 l w j i w o j l 1 n w j l k 1 n j 1 l w j k w o j l 1 n w j l where r i o is the importance of input factors representing the relative contribution of the input node i to the output node o n l and m are the number of the nodes in input layer hidden and output layer respectively then w j i and w o j are weights connecting input layer with hidden layer hidden layer with output layer respectively however it should be mentioned that there are two hidden layers in the constructed ann models that need a slight modification in the eq 3 the importance of loading factors p f h n to reveal the contribution to the long term performance of the wsbf model is described in fig 18 it can be observed that the load frequency f is the most influenced factor followed by the load magnitude p and cycle number n while the excitation height h has the comparatively least influence on the long term performance 5 conclusion and outlook the long term performance including changes in natural frequency damping ratio and accumulated rotation plays an important role in the safety of the owt which would be easily neglected in the design and analysis hence the long term performance of the wsbf model under cyclic loadings is investigated based on a series of 1 g experiments in this study the main conclusions can be summarized as follows 1 based on the experiment results the normalized natural frequency experience first increase with reduced rate stabilize and later decrease while there is a trend of first decline and later steadily recover to stabilization for the normalized damping ratio regarding the normalized accumulated rotation of the model it increases with the increasing cycle number in which more than 80 rotation of the model occurs in the first hundred cycles the long term performance of the wsbf model may be attributed to the changes in the sand adjacent to the bucket foundation model in the experiment 2 compared with the results of previous studies there are some similarities in the trend of variations in normalized natural frequency and accumulated rotation of the wsbf model in this experiment however different trend of the changes in the normalized damping ratio can be observed between this and previous studies due to the differences in soil parameters as well as foundation form 3 the ann model is used to conduct regression analysis to establish the relationship between loading conditions and long term performance of the wsbf model based on the experiment data which will provide a guidance and prediction for experiment with high accuracy furthermore sensitivity analysis reveals that loading frequency can impact most significantly on the long term performance of the wsbf model followed by the loading magnitude and cycle number however the limitations of this study are also obvious on one hand this study is limited to one embedded ratio of the bucket foundation model and one single type of sand and one way loading on the other hand the loading condition is chosen to be limited number of cycles and one way direction hence more cycle numbers multi direction loading different embedded ratio and soil parameters are looking forward to be investigated to perform more comprehensive study for the long term performance of bucket foundation in the further study credit authorship contribution statement jijian lian conceptualization funding acquisition resources supervision yue zhao data curation formal analysis writing original draft xiaofeng dong funding acquisition methodology validation writing review editing chong lian investigation software haijun wang project administration visualization declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this project was supported by the fund for national natural science foundation of china 51709202 innovation method fund of china 2016im030100 and the fund for key research area innovation groups of china 2014ra4031 support from the funding agency is sincerely acknowledged the writers acknowledge the assistance of anonymous reviewers as well 
