index,text
24040,in global ocean simulations forward non data assimilative tide models generally feature large sea surface height errors near hudson strait in the north atlantic ocean with respect to altimetry constrained tidal solutions these errors may be associated with tidal resonances that are not well resolved by the complex coastal shelf bathymetry in low resolution simulations an online two way nesting framework has been implemented to improve global surface tides in the hybrid coordinate ocean model hycom in this framework a high resolution child domain covering hudson strait is coupled with a relatively low resolution parent domain for computational efficiency data such as barotropic pressure and velocity are exchanged between the child and parent domains with the external coupler oasis3 mct the developed nesting framework is validated with semi idealized basin scale model simulations the m2 sea surface heights show very good accuracy in the one way and two way nesting simulations in hudson strait where large tidal elevations are observed in addition the mass and tidal energy flux are not adversely impacted at the nesting boundaries in the semi idealized simulations in a next step the nesting framework is applied to a realistic global tide simulation in this simulation the resolution of the child domain 1 75 is three times as fine as that of the parent domain 1 25 the m2 sea surface height root mean square errors with tide gauge data and the altimetry constrained global fes2014 and tpxo9 atlas tidal solutions are evaluated for the nesting and no nesting solutions the better resolved coastal bathymetry and the finer grid in the child domain improve the local tides in hudson strait and bay and the back effect of the coastal tides induces an improvement of the barotropic tides in the open ocean of the atlantic keywords two way nesting hycom barotropic tides oasis3 mct fes2014 tpxo9 atlas 1 introduction global surface tides have been studied in barotropic two dimensional and baroclinic three dimensional forward and data assimilative simulations arbic et al 2004 arbic et al 2009 buijsman et al 2015 egbert and ray 2001 egbert et al 2004 green and nycander 2013 ngodock et al 2016 shriver et al 2012 stammer et al 2014 despite recent progress in reducing tidal sea surface height root mean square errors rmse in the global hybrid coordinate ocean model hycom the model used in this paper the errors in the north atlantic near hudson strait and near coastal shelf areas in general are still relatively large buijsman et al 2015 ngodock et al 2016 shriver et al 2012 coastal shelf bathymetry is imperfectly known and represented in models and may be a likely cause of tidal model errors egbert et al 2004 moreover the large errors in the north atlantic may be attributed to the inability of the model to correctly simulate the known semidiurnal tidal resonances in the north atlantic wünsch 1972 both analytical models of coastal tides e g the classic quarter wavelength resonance model see for instance defant 1961 and more realistic numerical models of coastal tides have long treated the open ocean as representing an unchanging boundary condition acting on a coastal region that may or may not achieve resonance depending on its geometry however in analytical and numerical studies arbic et al 2009 arbic et al 2007 and arbic and garrett 2010 have shown that there is a substantial back effect of coastal tides upon the open ocean tides they showed that if both the shelf sea and ocean basin are near resonance small geometry changes to the shelf sea can have a large impact on the open ocean tides this suggests that if we better resolve the coastal geometry by increasing the model resolution we may not only improve the coastal tides but also the open ocean tides through the back effect in particular if the shelf and ocean basins are near resonance as is the case in the north atlantic near hudson strait in general higher resolution yields more accurate tides in forward models egbert et al 2004 however high resolution model simulations are computationally expensive and require large data storage to achieve higher resolution in regions of interest without dramatically increasing the cost of a basin or global scale model one can use two way nesting as introduced in spall and holland 1991 the basic idea of two way nesting is to apply a high resolution fine grid only to a small area of interest i e the child domain and a low resolution coarse grid to the entire parent domain in two way nesting information is allowed to propagate from the coarse grid to the fine grid and vice versa in this way the improved solution on the child grid may also improve the solution on the parent grid in this paper we implement a two way nesting framework for barotropic hycom simulations to better simulate the local surface tides e g on hudson shelf as well as the remote tides in the open ocean in the two way nesting framework the data exchange between the child and parent domains occurs in two opposing directions in the first coarse to fine direction the information of the parent domain is transferred to the child domain via the lateral boundaries of the child domain data exchange in this direction is also referred to as one way nesting in one way nesting the parent domain is not updated with the child grid solution one way nesting has been extensively used for basin scale modeling chassignet et al 2007 hogan and hurlburt 2006 mason et al 2010 penven et al 2006 in the second fine to coarse direction the coupling variables in the high resolution child domain are transferred to the relatively low resolution parent grid the two way nesting approach has been widely applied in atmospheric modeling bowden et al 2012 lorenz and jacob 2005 and in coupling of multiple climate models lam et al 2009 ličer et al 2016 wahle et al 2017 as well as ocean modeling barth et al 2005 debreu et al 2012 ginis et al 1998 to make the updated variables continuous across the nesting boundaries of the parent and child domains in the second direction smoothing filters were sometimes applied to physical variables and or the bottom topography of the child domain was relaxed with that of the parent domain debreu et al 2012 fox and maskell 1995 ginis et al 1998 mason et al 2010 the artifacts at the nesting boundary are amplified when the grid resolution ratio of the child and parent domains increases spall and holland 1991 in this study barotropic pressure velocity and bottom topography are linearly relaxed near the boundary of the child domain to allow for a smooth transition of these data between the child and parent domains which the grid resolution ratio is three the two way nesting approaches can be classified into two categories i internal and ii external coupling in internal coupling a nesting framework is included in the source code of ocean models barth et al 2005 debreu and blayo 2008 haley jr and lermusiaux 2010 herrnstein et al 2005 santilli and scotti 2015 tang et al 2014 this approach requires more complicated nesting algorithms than a single resolution no nesting case in external coupling a separate coupler facilitates the communication between numerical models the main advantage of using such a coupler is that the modification of the source code of these models is minimal it is only necessary that the application programming interface api of the coupler be built into the existing source codes in addition external couplers provide a wide variety of interpolation schemes and efficient search algorithms that can be used to calculate interpolation remapping coefficients the data that are transferred between coupled models need to be interpolated for use in the destination domain because the grid cells in the destination domain may not be located at the same coordinates as the grid cells in the source domain several external couplers have been developed and applied valcke et al 2012 the earth system modeling framework esmf can be used to couple multiple domains with different resolutions qi et al 2018 in recent research the agrif and oasis3 mct coupling toolkits have been widely applied the agrif adaptive grid refinement in fortran coupler has been used to couple ocean circulation models debreu et al 2012 penven et al 2006 urrego blanco et al 2016 the oasis3 mct coupler valcke et al 2015 has been used to couple climate models juricke et al 2014 ličer et al 2016 wahle et al 2017 will et al 2017 oasis3 mct is an integrated version of oasis3 ocean atmosphere sea ice and soil by cerfacs in france and mct model coupling toolkit by the argonne national laboratory in usa we utilize oasis3 mct as a coupler for the hycom to hycom two way nesting because its partition option grid and mask and parallel computing by message passing interface mpi are more compatible with hycom as compared to agrif scientists at the service hydrographique et océanographique de la marine shom brest france applied agrif to facilitate the hycom to hycom coupling but found it hard to maintain as the hycom code was updated personal communication with remy baraille 2015 hycom provides reliable results as a multi layer global ocean circulation model and contributes to the united states navy s operation arbic et al 2010 chassignet et al 2007 metzger et al 2010 ngodock et al 2016 shriver et al 2012 the forward barotropic tide simulations single layer used in this paper employ astronomical tidal forcing a spatially varying self attraction and loading sal term a linear topographic wave drag buijsman et al 2015 and an atmospheric mean sea level pressure forcing which modulates the s2 tide and is obtained from the atmospheric navgem navy global environmental model hogan et al 2014 hycom is also capable of improving basin scale modeling with offline one way nesting chassignet et al 2007 hogan and hurlburt 2006 however the offline one way nesting in hycom has some drawbacks first it requires additional data storage for nesting data whose size increases proportionally to the resolution of the computational domain second global scale modeling on the parent grid must be performed in advance of regional basin scale modeling on the child grid to provide boundary conditions for the child domain third offline one way nesting is useful only for improving the solution in the child domain while the parent domain does not receive any feedback from the child domain an online two way nesting framework is able to resolve these issues and improve the accuracy in hycom for both regional basin and global scale modeling the most complicated issue in applying oasis3 mct to hycom is how to deal with barotropic time stepping in conjunction with the baroclinic time steps that are based on the second order leapfrog scheme this study synchronizes nesting variables at the barotropic time steps of the parent domain without any other treatments santilli and scotti 2015 urrego blanco et al 2016 and stores them so that they can be used at the next baroclinic time level in this study the improvement through two way nesting is evaluated and validated by comparing the nesting results to tide gauge data and the tpxo9 atlas and fes2014 global tidal solutions tpxo9 atlas volkov oce orst edu tides tpxo9 atlas html is an altimetry constrained tidal solution egbert and erofeeva 2002 fes2014 https www aviso altimetry fr is the latest global solution of the fes finite element solutions model which is constrained with altimetry and tide gauge data carrère et al 2015 stammer et al 2014 compared the numerical results of forward and data assimilative tide models to the tide gauge data separately in the deep ocean shelf sea and coastal regions according to stammer et al 2014 tpxo8 atlas the previous version of tpxo9 atlas is more accurate in the deep ocean than fes2012 the previous version of fes2014 whereas tpxo8 atlas has larger root mean square errors with the tide gauge data in the shelf seas than fes2012 the fes2014 tidal solution has improved its tidal accuracy compared to its previous versions cancet et al 2018 and so has tpxo9 atlas hence we compare the two way nesting results to both the tpxo9 atlas and fes2014 solutions as well as to the tide gauge data of stammer et al 2014 this paper is organized as follows the online two way nesting framework in hycom is described in section 2 the developed nesting framework is validated with semi idealized basin scale simulations in section 3 the framework is then applied to a realistic global scale simulation and validated against tide gauge data fes2014 and tpxo9 atlas in section 4 finally we end with a discussion and conclusions in section 5 2 methodology oasis3 mct in hycom 2 1 data exchange in hycom in solving the layer integrated nonlinear momentum equations hycom uses a split explicit time stepping scheme that separates the fast barotropic mode single layer from the slow baroclinic modes multiple layers for numerical efficiency bleck and smith 1990 in barotropic hycom simulations as in this paper this split explicit time stepping scheme is still used in this case hycom performs the calculations over two layers a surface layer with a varying thickness and a bottom layer with a zero thickness hence we need to prescribe a barotropic time step δt t and a baroclinic time step δt c since barotropic waves have larger phase speeds than baroclinic waves the barotropic time step is much smaller than the baroclinic time step the split explicit scheme is computationally efficient because the two dimensional calculations occur on the fast time step and the more elaborate three dimensional calculations occur on the longer time step an example of the barotropic and baroclinic time stepping is shown in fig 1 for both the parent and child domains time is integrated with the second order leapfrog scheme for the baroclinic mode and with a combination of the first order forward and backward euler schemes for the barotropic mode bleck and smith 1990 the leapfrog scheme is a two step method that uses two baroclinic time steps of t δt c and t to evaluate the values at t δt c where t denotes a certain simulation time this means that one of two baroclinic time steps is overlapped every baroclinic calculation fig 1 in this study the data of the second baroclinic time step of the previous simulation time t are stored in memory and the saved data are used in the first baroclinic time step of the next simulation time t δt c this method in which the stored data for one baroclinc time step is used is more efficient than exchanging data from t δt c and t δt c every simulation time in general the baroclinic and the barotropic time steps of a parent domain δt c 1 and δt t 1 are different from those of a child domain δt c 2 and δt t 2 because the stable time step for a fine grid is smaller than that for a coarse grid based on the cfl courant friedrichs lewy condition figs 1 and 2 as shown in fig 2 the barotropic longitudinal and latitudinal velocities and the barotropic pressure are transferred in the first coarse to fine direction before solving the continuity equation and in the second fine to coarse direction after computing the momentum equations in hycom two barotropic time steps are calculated in a pair to improve the numerical stability in calculating the momentum equations with the coriolis force bleck and smith 1990 the child domain can receive data every barotropic time step from the parent domain δt t 2 but no data are sent from the parent domain in the case that the barotropic time steps of the parent and the child domains do not coincide in figs 1 and 2 for example the barotropic δt t 1 and the baroclinic δt c 1 time steps of the parent domain are twice as large as those δt c 2 and δt t 2 of the child domain in this case data are exchanged in both directions every other barotropic time step of the child domain as the nesting time step δt nt this means that the nesting variables are synchronized at the end of the barotropic time step of the parent domain δt nt δt t 1 2δt t 2 the data from the parent domain are used as a lateral boundary condition for the child domain in the first direction however the difference in grid resolution between the two domains may create discontinuities in the topography along the domain boundaries to prevent this it is necessary to match the topography of the parent domain with that of the child domain along the boundary of the child domain as shown in fig 3a the bottom topography of the child domain is divided into three zones the outermost zone width n b uses the low resolution bottom topography that is the same as that of the parent domain while the innermost zone uses the high resolution bottom topography the smoothing zone in the middle merges the low and high resolution topographies as 1 ϕ γ r ϕ child 1 0 γ r ϕ parent where ϕ is the bottom topography and γ r is the relaxation factor that is linearly evaluated by 2 γ r min x r l n r l l n s e or w where n r l is the number of cells in the width of the smoothing zone and x r l is the cell number as counted from the outer north south east and west boundaries other physical variables such as barotropic pressure and velocity are also relaxed in the same manner in addition to avoid instabilities at the boundary of the child domain in the second direction a specified number of boundary cells in the child domain n e n w n n and n s are not updated along the eastern western northern and southern boundaries as shown in fig 3b moreover cutting off a parent domain sometimes results in isolated ocean cells near the boundary of a child domain as hycom requires that all ocean cells should be connected as one sea these isolated ocean cells are modified into land cells this means that oasis3 mct does not have any remapping coefficients and data are not transferred to these ocean cells therefore the area updated in the second direction should exclude these isolated ocean areas ez the data in the blue zone in fig 3b are only exchanged between the parent and child domains 2 2 remapping files of oasis3 mct in transferring data with oasis3 mct remapping files for each direction and variable are required the remapping files for oasis3 mct contain information on the coupling direction the grids involved in the exchange direction the interpolation coefficients and the address information of a target point on the destination grid and its four neighboring points on the source grid hycom is based on the staggered arakawa c grid bleck and smith 1990 the scalar variables such as pressure temperature and salinity are stored at the center of a computational cell p grid while the longitudinal and latitudinal velocities are stored at the center of the western side u grid and the southern side v grid of a computational cell respectively table 1 shows the nesting variables and their base grids for each direction where p u and v denote p u and v grids respectively in the first direction the lateral boundary condition of browning and kreiss 1982 is applied on the p grid for the barotropic pressure and velocity therefore the barotropic pressure on the p grid and the barotropic velocity fields on the u and v grids of the parent domain are interpolated onto the p grid along the boundary of the child domain in the second direction the barotropic variables of pressure and velocity in the child domain are interpolated onto the same types of grids in the parent domain in computing the remapping coefficients oasis3 mct searches the entire parent grid which may take a long time will et al 2017 suggested that the masks of the ocean cells located outside the child domain be modified into land in the second direction if the target point is a land cell this calculation is skipped in oasis3 mct reducing the computing time for the remapping coefficients in this study the approach by will et al 2017 is applied to both the first and the second directions in the first direction we not only modify most of the child domain into land cells except for the boundary cells but also set limits to the search algorithm since the data coming from the parent domain are used as the boundary condition for the child domain only boundary ocean cells in the child domain require data in the first direction as shown in fig 4a a specific number of boundary cells green 5 cells from the boundary are active while the other inner ocean cells in the child domain white are changed to land cells in order to calculate remapping coefficients for a target point in the destination child domain oasis3 mct searches for four surrounding points of each target point in the source parent domain instead of including all the parent grid points in the search we limit the search to a small area light blue box around the ocean cell in the parent domain corresponding to the target point in the child domain fig 4a in this study the search box has 20 grid points in the parent domain in both longitudinal and latitudinal directions this box follows the target point in the child domain as it moves eastward or northward the efficiency of this approach is shown in table 2 three regions arc hud and glb which are used in sections 3 and 4 are involved in the calculation the modified approach significantly reduces the computing time by at least an order of magnitude as compared to the original method in the second direction only ocean cells that are located inside the child region are updated in the parent domain as illustrated in fig 4b the remapping coefficients for the target points that are located outside the child region are not necessary in two way nesting by modifying the masks of the ocean cells outside the child area into land cells the computing time of creating the remapping files for the second direction significantly decreases by several orders of magnitude as shown in table 3 3 validation of the nesting framework the developed nesting framework is validated with six semi idealized model simulations experiments 1 2 concern one way nesting and experiments 3 6 concern two way nesting the parent and child grid resolutions are the same in experiments 1 4 while they differ in experiments 5 6 experiment pairs 1 2 3 4 and 5 6 concern the sensitivity to equal and different time steps on the parent and child grids the grid resolutions and time steps of the parent and child domains in experiment 6 are the same as those of the global experiment discussed in the next section the specifications of experiments 1 6 are presented in tables 4 and 5 the hud child domain covering hudson strait and the arc parent domain are shown in fig 5 the parent domain covers the arctic and the north atlantic oceans rather than the entire global ocean for computational efficiency for experiments 1 4 the grid resolution of both the child and parent domains is 1 25 to ensure the child grid is identical to the parent grid in experiments 1 4 the child domain is extracted from the parent domain fig 6a for experiments 5 and 6 the child grid has a resolution of 1 75 which is three times as fine as the parent grid fig 6b in case that the refinement ratio between parent and child domains is odd the interpolation error is reduced because the grid points on the p u and v grids of the parent domain are located at the same grid points of the child domain in experiments 1 4 the child domain has 658 cells in the longitudinal direction and 1015 cells in the latitudinal direction while in experiments 5 and 6 the child domain has 1975 cells in the longitudinal direction and 3046 cells in the latitudinal direction table 4 the number of computational cells in the parent domain is 3200 5040 for all the experiments the child domain runs on 128 mpi processes and the parent domain is partitioned in 320 mpi processes the barotropic and baroclinic time steps of the parent domain are δt t 3 0 s and δt c 60 0 s in all experiments except in experiment 5 which features δt t 1 0 s and δt c 20 0 s table 5 the nesting time step δt nt is the same as the barotropic time step of the parent domain in all experiments in experiments 1 3 and 5 the barotropic and baroclinic time steps of the child domain are the same as those of the parent domain in experiments 2 4 and 6 the barotropic and baroclinic time steps of the child domain are a third of those of the parent domain i e δt t 1 0 s and δt c 20 0 s in experiments 1 6 the number of cells excluded in the second direction n n s e w is 5 to 10 cells to prevent errors at the boundary of the child domain from propagating into the parent domain in all two way nesting experiments the width of the zone with the low resolution parent bathymetry is n b 10 cells and the width of the smoothing relaxation zone is n r 10 cells the simulations start from rest in all the experiments the m2 barotropic tide is forced at the southern boundary of the parent domain the only forcing of the child domain comes from the parent domain through the lateral boundary no atmospheric forcing is applied in all the simulations the boundary forcing and the identical grids in experiments 1 4 provide two advantages as a validation case first the result of the parent domain without any nesting can be used as the true reference solution in the experiments with identical grid resolution and two way nesting experiments 3 and 4 the two way nesting result computed in the parent domain should be the same as the reference solution because the parent domain does not obtain any improvement from the same resolution child domain this means that the difference between the nesting and the no nesting solutions represents the numerical error caused by the nesting communication the resolution of the child domain is not the same as that of the parent domain in experiments 5 and 6 since the higher resolution child domain generally returns more accurate results to the parent domain the no nesting solution is not valid as the true solution hence we focus on discontinuities and errors near the updated boundaries in the parent domain in experiments 5 and 6 second the performance of the lateral boundary condition of the child domain can be validated with one way nesting experiments 1 and 2 the child domain receives forcing only through the lateral boundary in the first coarse to fine direction but does not send back any data to the parent domain in the second fine to coarse direction the difference between the numerical results in the child domain and the no nesting solution informs us about the accuracy of the lateral boundary conditions in hycom in experiments 1 4 the accuracy over the parent domain is evaluated with the sea surface height root mean square error between the nesting result and the no nesting reference solution at each computational cell 3 rms e i j n 1 n η i j n η i j n 2 n where η is the sea surface height ssh of the nesting solution η is the ssh of the reference solution n is the temporal order of the hourly saved data n is the total number of output data and i and j are the computational cell indexes along the longitudinal and latitudinal directions respectively to exclude a spin up error only data between days 3 and 5 are used in the calculation eq 3 represents the time averaged absolute error with the reference solution the relative error can be estimated with the normalized root mean square error 4 nrm s i j r m s e i j σ i j where 5 σ i j n 1 n η i j n 2 n is the standard deviation of the sea surface heights the sea surface height root mean square errors rmse and the normalized errors nrms with the reference solution for experiments 1 4 are shown in fig 7 in these experiments the resolutions of the parent and child grids are identical the standard deviation of the sea surface heights in the parent domain for the no nesting reference solution is illustrated in fig 8 the nrms values in fig 7 are plotted only in the hudson strait region where relatively large sea surface heights are observed in fig 7a and b the rmse values for experiment 1 are mostly 2 0 mm over the child domain hud and the nrms values are 0 1 in the hudson strait region this result indicates that the lateral boundary condition of the child domain works accurately fig 7c and d shows rmse and nrms for experiment 2 in which the parent and child grid time steps differ even though these rmse and nrms values are larger than those of experiment 1 the accuracy is still quite good with errors 0 5 in the hudson strait region the errors of experiment 2 are larger than those of experiment 1 because the data are exchanged between the parent and child domains only at the nesting time steps δt nt 3 0 s and errors are accumulated at the other two barotropic time steps δt t 2 in the child domain the numerical errors of the two way nesting experiments are expected to be larger than those of the one way nesting experiments because the two way nesting performs one more communication for the second direction and there should be no accuracy improvement in the child domain with identical grid resolution the rmse and nrms values for experiment 3 in fig 7e and f are 5 0 mm and 0 2 in the hudson strait region respectively as expected these errors are slightly larger than those of the one way nesting experiment 1 however for experiment 4 with different parent and child grid time steps fig 7g and h the errors are reduced compared to experiment 2 7 0 mm and 0 35 due to unknown causes we do not observe significant error accumulations along the boundaries of the child domain in experiments 3 and 4 fig 7e and g to study how the two way nesting affects the propagation of physical variables in particular near the nesting boundaries and smoothing zones we calculate for experiments 3 6 the mass flow rate 6 m ρ η h u the residual of the continuity 7 r η t η h u and the barotropic energy fluxes egbert and ray 2001 8 f ρghη u where ρ is the water density 1036 31 kg m3 u is the barotropic velocity vector η is the sea surface height h is the water depth f f x f y is the energy flux vector f x and f y are the longitudinal and latitudinal components and g is the gravitational coefficient 9 81 m s2 the magnitude of the time mean and the standard deviation of the mass flow rate vectors and energy flux vectors and the time mean and the standard deviation of the continuity residual for experiment 6 are presented in fig 9 the results for experiments 3 6 are nearly identical and similar figures for experiments 3 5 are not shown the difference in time steps between experiments 3 and 4 and experiments 5 and 6 causes relative differences in the mass flow rate continuity and energy fluxes in the parent solution updated with the child solution that are generally smaller than 1 the mean and the standard deviation of the residual in fig 9c and d have larger values near topographic gradients as opposed to the nesting boundaries in all two way nesting experiments mass is conserved across the south west north and east boundaries there are no obvious indications that the nesting boundaries trap reflect and diffract the tidal waves this is further illustrated in fig 10 which shows the time averaged tidal energy flux in the parent domain integrated along the cells parallel to the boundaries for experiments 3 6 compared to the reference no nesting solution the tidal energy flux is conserved in all experiments since the comparisons for the mass flow rate and the continuity residual show the same smooth transition as that of the tidal energy flux they are not shown here in this section we have tested the performance of the developed two way nesting framework with six semi idealized experiments for various time steps and grid resolutions we conclude that the two way nesting framework works with a high accuracy and the tidal energy and mass are conserved quite well 4 global barotropic tides to improve the tidal accuracy in the global hycom simulations we apply the two way nesting framework to a high resolution child domain 1 75 and a relatively low resolution global parent domain 1 25 the child domain referred to as hbf and its surroundings are shown in fig 11 since the southeast entrance of fury and hecla strait is blocked as land in the relatively low resolution 1 25 domain and hudson bay plays an important role in the resonance of the m2 barotropic tide in hudson strait arbic et al 2007 the child domain in this realistic experiment is moved westward and extended to hudson bay rose welcome sound and fury and hecla strait moreover the child domain is expanded southward to include the bay of fundy and gulf of st lawrence regions that feature large tides and sea surface height root mean square errors the southward expansion ensures that additional tide gauge stations used for validation are included in the child domain the bathymetries of the parent and child domains are based on the 1 120 gebco general bathymetric chart of the oceans bathymetry it is interpolated onto the 1 75 and 1 25 hycom grids for the child and parent domains both the child and parent domains are forced by the five leading tidal constituents m2 s2 n2 k1 and o1 and a mean sea level pressure mslp the simulation period is 29 days between november 27 and december 26 in 2013 and the numerical results of the sea surface heights are stored every hour the child domain receives additional forcing from the parent domain through its lateral boundaries the simulation specifications of the realistic global scale case are summarized in table 6 to balance the number of computational cells for each process of the child and parent domains the child domain is parallelized with 1408 mpi processes and the parent domain is parallelized with 1296 mpi processes the simulation results for 29 days are decomposed into five tidal constituents with a least squares harmonic analysis the amplitudes and phases of the m2 barotropic tide are compared to the global fes2014 and tpxo9 atlas solutions in addition to the tide gauge data fig 12 displays the locations of the tide gauge stations in a the deep oceans and b shallow water coastal shelf regions for the deep oceans the tidal data from 132 stations are used for comparison and 44 out of 132 stations are located in the north atlantic ocean cyan triangles for the shallow water regions 75 stations of the total 128 sites are located in the north atlantic ocean to check the effect of the two way nesting on local and remote tides these stations are divided into two groups 38 tide gauges along the east coast of north america blue squares and 37 gauges along the european shelf cyan triangles the root mean square error between the tide gauge measurements and the hycom tpxo9 atlas and fes2014 tidal solutions averaged over all gauges is evaluated as 9 rm s tg 1 n k 1 n 1 2 a k e i ϕ k a k e i ϕ k 2 where i 1 k is the tide gauge station number n is the number of the tide gauge stations a and ϕ are the amplitude and phase of the tide gauge data a and ϕ are the amplitude and phase of the tidal solutions of hycom tpxo9 atlas or fes2014 respectively the tidal solutions are interpolated onto the locations of the tide gauge stations with a bi cubic interpolation method table 7 shows the gauge averaged root mean square error of the tidal solutions from the tide gauge data in the north atlantic ocean natl and the global ocean glb for the tpxo9 atlas and fes2014 tidal solutions and for the stand alone 1 25 hycom glb25 and the two way nesting results the overall surface tidal accuracy is improved through two way nesting in both the deep oceans and the shallow shelf seas the improvement generally decreases farther away from the child domain the reduction in rms tg of 3 cm is the largest for the north american east coast tide gauges which are located closest to the child domain for these gauges the relative improvement of tidal predictability rms tg glb25 rms tg two way rms tg glb25 15 89 however the two way nesting reduces the tidal accuracy for the gauges on the european shelf by 2 22 the relative improvement of predictability is 8 94 for all the gauges in the deep north atlantic ocean which is larger than the improvement of 6 24 for all the gauges in the deep global ocean the fes2014 and the tpxo9 atlas solutions have a better tidal accuracy than the hycom results small differences exist between the fes and tpxo solutions although fes2014 is generally more accurate than tpxo9 atlas in both the deep and shallow oceans except on the european shelf where the tpxo solution is more accurate the tide gauge data set is limited in spatial coverage hence we also validate the global hycom simulations with the fes2014 and tpxo9 atlas tidal solutions at each computational cell i j of hycom we calculate the m2 sea surface height root mean square error between the hycom simulations and the fes tpxo solutions as shriver et al 2012 10 rms e i j 1 2 a i j e i ϕ i j a i j e i ϕ i j 2 where a and ϕ are the amplitude and phase of the m2 barotropic tide in hycom and a and ϕ are the amplitude and phase of the global tidal solutions of tpxo9 atlas or fes2014 respectively since the hycom grid differs from the fes and tpxo grids the fes2014 and tpxo9 atlas solutions are interpolated onto the hycom grid with a bi cubic interpolation fig 13a shows the amplitudes and phases of the m2 barotropic tide for the two way nesting simulation the rmse between fes2014 and the two way nesting simulation is depicted in fig 13b fig 13c shows the improvement of predictability of the global m2 barotropic tide through two way nesting δrmse rmseglb25 rmsetwo way an increase in δrmse through two way nesting corresponds to a reduction in rmse relative to the reference solution without nesting the rmse values of the two way nesting experiment and δrmse in the region of the child domain are also plotted in fig 14 in figs 13c and 14b the warm cool colors indicate an improvement reduction of the predictability through two way nesting figs 13c and 14b show that the predictability is not only improved in the child domain but also in the parent domain outside the child area as far as the south atlantic ocean this clearly illustrates that the improved accuracy in the coastal shelf region of the child domain is allowed to propagate to the open ocean through the two way nesting in addition the spatially averaged root mean square error over several ocean domains rms avg is evaluated as arbic et al 2004 11 rm s avg j 1 n j i 1 n i rms e i j 2 a i j j 1 n j i 1 n i a i j where a is the area of the computational cell i j and n i and n j are the number of ocean cells in the longitudinal and latitudinal directions respectively table 8 represents the root mean square error averaged over the child area in the parent domain hbf the north atlantic ocean natl and the global parent domain glb respectively for water depths 5 0 m in addition rms avg is computed over water depths 1000 m and latitudes equatorward of 66 glb66 for comparison with previous studies the differences in rms avg computed with either fes2014 or tpxo9 atlas are small the improvement of predictability is largest in the child domain 7 26 when tpxo is used this relative improvement is smaller than the 15 89 improvement computed for the tide gauges on the shelf of the north american east coast table 7 as can be seen in figs 12b and 14b these tide gauges are located in areas where δrmse is both large and positive the relative improvement of 7 26 averaged over the child domain is lower due to localized reductions in the predictability in hudson bay and foxe basin blue areas in fig 14b the improvement in predictability decreases as the size of the averaging area increases however this reduction in predictability is minimal for the north atlantic natl where the relative improvement in predictability is 7 03 when fes is used this relatively large value can be attributed to δrmse being positive over a large area fig 13c however beyond the atlantic ocean the improvement in tidal accuracy is minimal as a consequence the reduction in rms avg for the glb66 case is small i e only 2 18 while the reduction in rmse may not seem very large the surface area where the predictability is improved is generally much larger than the area where it is reduced table 9 in the hbf domain δrmse 5 0 mm occurs in 64 44 of the domain while δrmse 5 0 mm occurs only in 5 84 of the domain the largest reduction in predictability increase in rmse is observed in foxe basin fig 14b in about 1 10 of the north atlantic ocean natl the predictability in the two way nesting solution is reduced this reduction mostly occurs in other shelf regions such as the english channel the patagonian shelf and the amazon shelf as shown in fig 13c only 9 69 of the global ocean glb features an improved predictability because the pacific and indian oceans are minimally affected by the two way nesting in the north atlantic ocean 5 discussion and conclusions we have implemented an online two way nesting framework in a global ocean circulation model hycom to improve global surface tides the barotropic pressure and velocity are exchanged between a parent and a child domain with an external coupler oasis3 mct we modified the masks in both the first coarse to fine and the second fine to coarse directions and set the limits of the search area in the first direction to reduce the computing time of the remapping coefficients in oasis3 mct in the global hycom simulations hudson strait and the adjacent north atlantic ocean feature relatively large m2 sea surface height root mean square errors with the tide gauges and the altimetry constrained tide models these errors may be associated with the inability of low resolution simulations to resolve the semi diurnal tidal resonances on and off the shelf in the north atlantic ocean these errors can be reduced by two way nesting a high resolution child domain in a low resolution parent domain hence we selected the hudson strait region as the child domain in our nesting experiments the developed two way nesting framework is validated with six semi idealized one and two way nesting experiments in these experiments the child domain covers the hudson strait region while the parent domain covers the arctic north pacific and north atlantic oceans the m2 barotropic tide is forced at the southern boundary of the parent domain the accuracy of the nesting framework is tested for time steps and grid resolutions that are the same and or differ between the parent and child domains the parent and child grids have the same resolution in experiments 1 4 the tidal solution on the parent grid without nesting is used as the true reference solution the semi idealized experiments have a good agreement with the reference solution the errors of the sea surface height in the four experiments are generally much 1 in addition two experiments are performed in which the grid resolution of the child domain is three times as fine 1 75 as that of the parent domain 1 25 to check for discontinuities along the nesting boundaries it is shown that mass and energy are conserved across the boundaries through all the experiments next the developed two way nesting framework is applied to a realistic global scale experiment the resolution of the child domain 1 75 is three times as high as that of the parent domain 1 25 realistic tidal forcing m2 s2 n2 k1 and o1 and atmospheric pressure mslp are applied over the child domain hbf as well as the parent domain glb compared to the child domain of the semi idealized experiments the hbf domain is extended to hudson bay and the bay of fundy to improve the tidal accuracy in these areas the no nesting and two way nesting results are compared to tide gauge data and the fes2014 and tpxo9 atlas tidal solutions the application of the two way nesting reduces the m2 sea surface height root mean square errors with the tide gauge data and the tidal solutions in particular in the northwest atlantic ocean the largest improvement of predictability i e reduction of the root mean square error of about 16 occurs at the tide gauges located along the east coast of north america the m2 root mean square errors between the two way nesting simulation and the tpxo fes tidal solutions are not only reduced in the vicinity of the child domain but they are reduced as far as the south atlantic ocean red areas in figs 13c and 14b the improvement of the tidal predictability in the north atlantic ocean averages about 6 28 the two way nesting simulation shows that a higher horizontal grid resolution and a better resolved coastal bathymetry in the regions of hudson strait and the bay of fundy not only improves the tidal prediction in the child domain but also in the coastal and abyssal ocean away from the nesting boundaries through the back effect arbic and garrett 2010 arbic et al 2009 arbic et al 2007 in recent years the global mean m2 sea surface height root mean square error between 1 12 5 hycom and tpxo tidal solutions has been reduced from 7 48 cm in shriver et al 2012 to 4 43 cm in buijsman et al 2015 to 2 60 cm in ngodock et al 2016 the reduction from 7 48 cm to 4 43 cm is due to the inclusion of bathymetry under the ice shelves the application of a spatially varying and iterated sal and the tuning of the linear wave drag the reduction from 4 43 cm to 2 60 cm is due to the application of an augmented state ensemble kalman filter asenkf to correct for errors in the tide model associated with imperfectly known topography and damping terms in this paper we do not apply the asenkf technique the reduction in rmse from 4 43 cm to 4 13 cm for the no nesting solution is due to an increase in model resolution from 1 12 5 to 1 25 the application of the two way nesting framework to one area i e the hudson shelf reduces this global mean error to 4 03 cm which is a modest improvement this paper mainly describes the development of a two way nesting framework in a non data assimilative version of hycom and its validation with the tide gauge data and the global fes2014 and tpxo9 atlas tidal solutions to reduce the large m2 surface tide errors in the north atlantic a high resolution child grid is applied to the hudson shelf this improves the accuracy of the tides not only in the hudson shelf region but also in the atlantic ocean as far south as south africa however the european shelf the patagonian shelf and the northwest australian shelf among others still feature large sea surface height root mean square errors in hycom fig 13b to further reduce these errors we plan to apply the two way nesting technique to multiple shelf areas in different ocean basins and find the break even point where two way nesting is computationally efficient compared to the stand alone high resolution global simulation in a subsequent paper acknowledgment c h j m c b b k a and j g r gratefully acknowledge funding from the project improving global surface and internal tides through two way coupling with high resolution coastal models as part of the office of naval research onr grant n00014 15 1 2288 a j w and j f s acknowledge support from the naval research laboratory nrl contract n00014 15 wx 01744 finally p j h acknowledges financial support from the project arctic shelf and large rivers seamless nesting in global hycom as part of onr grant n00014 15 1 2594 
24040,in global ocean simulations forward non data assimilative tide models generally feature large sea surface height errors near hudson strait in the north atlantic ocean with respect to altimetry constrained tidal solutions these errors may be associated with tidal resonances that are not well resolved by the complex coastal shelf bathymetry in low resolution simulations an online two way nesting framework has been implemented to improve global surface tides in the hybrid coordinate ocean model hycom in this framework a high resolution child domain covering hudson strait is coupled with a relatively low resolution parent domain for computational efficiency data such as barotropic pressure and velocity are exchanged between the child and parent domains with the external coupler oasis3 mct the developed nesting framework is validated with semi idealized basin scale model simulations the m2 sea surface heights show very good accuracy in the one way and two way nesting simulations in hudson strait where large tidal elevations are observed in addition the mass and tidal energy flux are not adversely impacted at the nesting boundaries in the semi idealized simulations in a next step the nesting framework is applied to a realistic global tide simulation in this simulation the resolution of the child domain 1 75 is three times as fine as that of the parent domain 1 25 the m2 sea surface height root mean square errors with tide gauge data and the altimetry constrained global fes2014 and tpxo9 atlas tidal solutions are evaluated for the nesting and no nesting solutions the better resolved coastal bathymetry and the finer grid in the child domain improve the local tides in hudson strait and bay and the back effect of the coastal tides induces an improvement of the barotropic tides in the open ocean of the atlantic keywords two way nesting hycom barotropic tides oasis3 mct fes2014 tpxo9 atlas 1 introduction global surface tides have been studied in barotropic two dimensional and baroclinic three dimensional forward and data assimilative simulations arbic et al 2004 arbic et al 2009 buijsman et al 2015 egbert and ray 2001 egbert et al 2004 green and nycander 2013 ngodock et al 2016 shriver et al 2012 stammer et al 2014 despite recent progress in reducing tidal sea surface height root mean square errors rmse in the global hybrid coordinate ocean model hycom the model used in this paper the errors in the north atlantic near hudson strait and near coastal shelf areas in general are still relatively large buijsman et al 2015 ngodock et al 2016 shriver et al 2012 coastal shelf bathymetry is imperfectly known and represented in models and may be a likely cause of tidal model errors egbert et al 2004 moreover the large errors in the north atlantic may be attributed to the inability of the model to correctly simulate the known semidiurnal tidal resonances in the north atlantic wünsch 1972 both analytical models of coastal tides e g the classic quarter wavelength resonance model see for instance defant 1961 and more realistic numerical models of coastal tides have long treated the open ocean as representing an unchanging boundary condition acting on a coastal region that may or may not achieve resonance depending on its geometry however in analytical and numerical studies arbic et al 2009 arbic et al 2007 and arbic and garrett 2010 have shown that there is a substantial back effect of coastal tides upon the open ocean tides they showed that if both the shelf sea and ocean basin are near resonance small geometry changes to the shelf sea can have a large impact on the open ocean tides this suggests that if we better resolve the coastal geometry by increasing the model resolution we may not only improve the coastal tides but also the open ocean tides through the back effect in particular if the shelf and ocean basins are near resonance as is the case in the north atlantic near hudson strait in general higher resolution yields more accurate tides in forward models egbert et al 2004 however high resolution model simulations are computationally expensive and require large data storage to achieve higher resolution in regions of interest without dramatically increasing the cost of a basin or global scale model one can use two way nesting as introduced in spall and holland 1991 the basic idea of two way nesting is to apply a high resolution fine grid only to a small area of interest i e the child domain and a low resolution coarse grid to the entire parent domain in two way nesting information is allowed to propagate from the coarse grid to the fine grid and vice versa in this way the improved solution on the child grid may also improve the solution on the parent grid in this paper we implement a two way nesting framework for barotropic hycom simulations to better simulate the local surface tides e g on hudson shelf as well as the remote tides in the open ocean in the two way nesting framework the data exchange between the child and parent domains occurs in two opposing directions in the first coarse to fine direction the information of the parent domain is transferred to the child domain via the lateral boundaries of the child domain data exchange in this direction is also referred to as one way nesting in one way nesting the parent domain is not updated with the child grid solution one way nesting has been extensively used for basin scale modeling chassignet et al 2007 hogan and hurlburt 2006 mason et al 2010 penven et al 2006 in the second fine to coarse direction the coupling variables in the high resolution child domain are transferred to the relatively low resolution parent grid the two way nesting approach has been widely applied in atmospheric modeling bowden et al 2012 lorenz and jacob 2005 and in coupling of multiple climate models lam et al 2009 ličer et al 2016 wahle et al 2017 as well as ocean modeling barth et al 2005 debreu et al 2012 ginis et al 1998 to make the updated variables continuous across the nesting boundaries of the parent and child domains in the second direction smoothing filters were sometimes applied to physical variables and or the bottom topography of the child domain was relaxed with that of the parent domain debreu et al 2012 fox and maskell 1995 ginis et al 1998 mason et al 2010 the artifacts at the nesting boundary are amplified when the grid resolution ratio of the child and parent domains increases spall and holland 1991 in this study barotropic pressure velocity and bottom topography are linearly relaxed near the boundary of the child domain to allow for a smooth transition of these data between the child and parent domains which the grid resolution ratio is three the two way nesting approaches can be classified into two categories i internal and ii external coupling in internal coupling a nesting framework is included in the source code of ocean models barth et al 2005 debreu and blayo 2008 haley jr and lermusiaux 2010 herrnstein et al 2005 santilli and scotti 2015 tang et al 2014 this approach requires more complicated nesting algorithms than a single resolution no nesting case in external coupling a separate coupler facilitates the communication between numerical models the main advantage of using such a coupler is that the modification of the source code of these models is minimal it is only necessary that the application programming interface api of the coupler be built into the existing source codes in addition external couplers provide a wide variety of interpolation schemes and efficient search algorithms that can be used to calculate interpolation remapping coefficients the data that are transferred between coupled models need to be interpolated for use in the destination domain because the grid cells in the destination domain may not be located at the same coordinates as the grid cells in the source domain several external couplers have been developed and applied valcke et al 2012 the earth system modeling framework esmf can be used to couple multiple domains with different resolutions qi et al 2018 in recent research the agrif and oasis3 mct coupling toolkits have been widely applied the agrif adaptive grid refinement in fortran coupler has been used to couple ocean circulation models debreu et al 2012 penven et al 2006 urrego blanco et al 2016 the oasis3 mct coupler valcke et al 2015 has been used to couple climate models juricke et al 2014 ličer et al 2016 wahle et al 2017 will et al 2017 oasis3 mct is an integrated version of oasis3 ocean atmosphere sea ice and soil by cerfacs in france and mct model coupling toolkit by the argonne national laboratory in usa we utilize oasis3 mct as a coupler for the hycom to hycom two way nesting because its partition option grid and mask and parallel computing by message passing interface mpi are more compatible with hycom as compared to agrif scientists at the service hydrographique et océanographique de la marine shom brest france applied agrif to facilitate the hycom to hycom coupling but found it hard to maintain as the hycom code was updated personal communication with remy baraille 2015 hycom provides reliable results as a multi layer global ocean circulation model and contributes to the united states navy s operation arbic et al 2010 chassignet et al 2007 metzger et al 2010 ngodock et al 2016 shriver et al 2012 the forward barotropic tide simulations single layer used in this paper employ astronomical tidal forcing a spatially varying self attraction and loading sal term a linear topographic wave drag buijsman et al 2015 and an atmospheric mean sea level pressure forcing which modulates the s2 tide and is obtained from the atmospheric navgem navy global environmental model hogan et al 2014 hycom is also capable of improving basin scale modeling with offline one way nesting chassignet et al 2007 hogan and hurlburt 2006 however the offline one way nesting in hycom has some drawbacks first it requires additional data storage for nesting data whose size increases proportionally to the resolution of the computational domain second global scale modeling on the parent grid must be performed in advance of regional basin scale modeling on the child grid to provide boundary conditions for the child domain third offline one way nesting is useful only for improving the solution in the child domain while the parent domain does not receive any feedback from the child domain an online two way nesting framework is able to resolve these issues and improve the accuracy in hycom for both regional basin and global scale modeling the most complicated issue in applying oasis3 mct to hycom is how to deal with barotropic time stepping in conjunction with the baroclinic time steps that are based on the second order leapfrog scheme this study synchronizes nesting variables at the barotropic time steps of the parent domain without any other treatments santilli and scotti 2015 urrego blanco et al 2016 and stores them so that they can be used at the next baroclinic time level in this study the improvement through two way nesting is evaluated and validated by comparing the nesting results to tide gauge data and the tpxo9 atlas and fes2014 global tidal solutions tpxo9 atlas volkov oce orst edu tides tpxo9 atlas html is an altimetry constrained tidal solution egbert and erofeeva 2002 fes2014 https www aviso altimetry fr is the latest global solution of the fes finite element solutions model which is constrained with altimetry and tide gauge data carrère et al 2015 stammer et al 2014 compared the numerical results of forward and data assimilative tide models to the tide gauge data separately in the deep ocean shelf sea and coastal regions according to stammer et al 2014 tpxo8 atlas the previous version of tpxo9 atlas is more accurate in the deep ocean than fes2012 the previous version of fes2014 whereas tpxo8 atlas has larger root mean square errors with the tide gauge data in the shelf seas than fes2012 the fes2014 tidal solution has improved its tidal accuracy compared to its previous versions cancet et al 2018 and so has tpxo9 atlas hence we compare the two way nesting results to both the tpxo9 atlas and fes2014 solutions as well as to the tide gauge data of stammer et al 2014 this paper is organized as follows the online two way nesting framework in hycom is described in section 2 the developed nesting framework is validated with semi idealized basin scale simulations in section 3 the framework is then applied to a realistic global scale simulation and validated against tide gauge data fes2014 and tpxo9 atlas in section 4 finally we end with a discussion and conclusions in section 5 2 methodology oasis3 mct in hycom 2 1 data exchange in hycom in solving the layer integrated nonlinear momentum equations hycom uses a split explicit time stepping scheme that separates the fast barotropic mode single layer from the slow baroclinic modes multiple layers for numerical efficiency bleck and smith 1990 in barotropic hycom simulations as in this paper this split explicit time stepping scheme is still used in this case hycom performs the calculations over two layers a surface layer with a varying thickness and a bottom layer with a zero thickness hence we need to prescribe a barotropic time step δt t and a baroclinic time step δt c since barotropic waves have larger phase speeds than baroclinic waves the barotropic time step is much smaller than the baroclinic time step the split explicit scheme is computationally efficient because the two dimensional calculations occur on the fast time step and the more elaborate three dimensional calculations occur on the longer time step an example of the barotropic and baroclinic time stepping is shown in fig 1 for both the parent and child domains time is integrated with the second order leapfrog scheme for the baroclinic mode and with a combination of the first order forward and backward euler schemes for the barotropic mode bleck and smith 1990 the leapfrog scheme is a two step method that uses two baroclinic time steps of t δt c and t to evaluate the values at t δt c where t denotes a certain simulation time this means that one of two baroclinic time steps is overlapped every baroclinic calculation fig 1 in this study the data of the second baroclinic time step of the previous simulation time t are stored in memory and the saved data are used in the first baroclinic time step of the next simulation time t δt c this method in which the stored data for one baroclinc time step is used is more efficient than exchanging data from t δt c and t δt c every simulation time in general the baroclinic and the barotropic time steps of a parent domain δt c 1 and δt t 1 are different from those of a child domain δt c 2 and δt t 2 because the stable time step for a fine grid is smaller than that for a coarse grid based on the cfl courant friedrichs lewy condition figs 1 and 2 as shown in fig 2 the barotropic longitudinal and latitudinal velocities and the barotropic pressure are transferred in the first coarse to fine direction before solving the continuity equation and in the second fine to coarse direction after computing the momentum equations in hycom two barotropic time steps are calculated in a pair to improve the numerical stability in calculating the momentum equations with the coriolis force bleck and smith 1990 the child domain can receive data every barotropic time step from the parent domain δt t 2 but no data are sent from the parent domain in the case that the barotropic time steps of the parent and the child domains do not coincide in figs 1 and 2 for example the barotropic δt t 1 and the baroclinic δt c 1 time steps of the parent domain are twice as large as those δt c 2 and δt t 2 of the child domain in this case data are exchanged in both directions every other barotropic time step of the child domain as the nesting time step δt nt this means that the nesting variables are synchronized at the end of the barotropic time step of the parent domain δt nt δt t 1 2δt t 2 the data from the parent domain are used as a lateral boundary condition for the child domain in the first direction however the difference in grid resolution between the two domains may create discontinuities in the topography along the domain boundaries to prevent this it is necessary to match the topography of the parent domain with that of the child domain along the boundary of the child domain as shown in fig 3a the bottom topography of the child domain is divided into three zones the outermost zone width n b uses the low resolution bottom topography that is the same as that of the parent domain while the innermost zone uses the high resolution bottom topography the smoothing zone in the middle merges the low and high resolution topographies as 1 ϕ γ r ϕ child 1 0 γ r ϕ parent where ϕ is the bottom topography and γ r is the relaxation factor that is linearly evaluated by 2 γ r min x r l n r l l n s e or w where n r l is the number of cells in the width of the smoothing zone and x r l is the cell number as counted from the outer north south east and west boundaries other physical variables such as barotropic pressure and velocity are also relaxed in the same manner in addition to avoid instabilities at the boundary of the child domain in the second direction a specified number of boundary cells in the child domain n e n w n n and n s are not updated along the eastern western northern and southern boundaries as shown in fig 3b moreover cutting off a parent domain sometimes results in isolated ocean cells near the boundary of a child domain as hycom requires that all ocean cells should be connected as one sea these isolated ocean cells are modified into land cells this means that oasis3 mct does not have any remapping coefficients and data are not transferred to these ocean cells therefore the area updated in the second direction should exclude these isolated ocean areas ez the data in the blue zone in fig 3b are only exchanged between the parent and child domains 2 2 remapping files of oasis3 mct in transferring data with oasis3 mct remapping files for each direction and variable are required the remapping files for oasis3 mct contain information on the coupling direction the grids involved in the exchange direction the interpolation coefficients and the address information of a target point on the destination grid and its four neighboring points on the source grid hycom is based on the staggered arakawa c grid bleck and smith 1990 the scalar variables such as pressure temperature and salinity are stored at the center of a computational cell p grid while the longitudinal and latitudinal velocities are stored at the center of the western side u grid and the southern side v grid of a computational cell respectively table 1 shows the nesting variables and their base grids for each direction where p u and v denote p u and v grids respectively in the first direction the lateral boundary condition of browning and kreiss 1982 is applied on the p grid for the barotropic pressure and velocity therefore the barotropic pressure on the p grid and the barotropic velocity fields on the u and v grids of the parent domain are interpolated onto the p grid along the boundary of the child domain in the second direction the barotropic variables of pressure and velocity in the child domain are interpolated onto the same types of grids in the parent domain in computing the remapping coefficients oasis3 mct searches the entire parent grid which may take a long time will et al 2017 suggested that the masks of the ocean cells located outside the child domain be modified into land in the second direction if the target point is a land cell this calculation is skipped in oasis3 mct reducing the computing time for the remapping coefficients in this study the approach by will et al 2017 is applied to both the first and the second directions in the first direction we not only modify most of the child domain into land cells except for the boundary cells but also set limits to the search algorithm since the data coming from the parent domain are used as the boundary condition for the child domain only boundary ocean cells in the child domain require data in the first direction as shown in fig 4a a specific number of boundary cells green 5 cells from the boundary are active while the other inner ocean cells in the child domain white are changed to land cells in order to calculate remapping coefficients for a target point in the destination child domain oasis3 mct searches for four surrounding points of each target point in the source parent domain instead of including all the parent grid points in the search we limit the search to a small area light blue box around the ocean cell in the parent domain corresponding to the target point in the child domain fig 4a in this study the search box has 20 grid points in the parent domain in both longitudinal and latitudinal directions this box follows the target point in the child domain as it moves eastward or northward the efficiency of this approach is shown in table 2 three regions arc hud and glb which are used in sections 3 and 4 are involved in the calculation the modified approach significantly reduces the computing time by at least an order of magnitude as compared to the original method in the second direction only ocean cells that are located inside the child region are updated in the parent domain as illustrated in fig 4b the remapping coefficients for the target points that are located outside the child region are not necessary in two way nesting by modifying the masks of the ocean cells outside the child area into land cells the computing time of creating the remapping files for the second direction significantly decreases by several orders of magnitude as shown in table 3 3 validation of the nesting framework the developed nesting framework is validated with six semi idealized model simulations experiments 1 2 concern one way nesting and experiments 3 6 concern two way nesting the parent and child grid resolutions are the same in experiments 1 4 while they differ in experiments 5 6 experiment pairs 1 2 3 4 and 5 6 concern the sensitivity to equal and different time steps on the parent and child grids the grid resolutions and time steps of the parent and child domains in experiment 6 are the same as those of the global experiment discussed in the next section the specifications of experiments 1 6 are presented in tables 4 and 5 the hud child domain covering hudson strait and the arc parent domain are shown in fig 5 the parent domain covers the arctic and the north atlantic oceans rather than the entire global ocean for computational efficiency for experiments 1 4 the grid resolution of both the child and parent domains is 1 25 to ensure the child grid is identical to the parent grid in experiments 1 4 the child domain is extracted from the parent domain fig 6a for experiments 5 and 6 the child grid has a resolution of 1 75 which is three times as fine as the parent grid fig 6b in case that the refinement ratio between parent and child domains is odd the interpolation error is reduced because the grid points on the p u and v grids of the parent domain are located at the same grid points of the child domain in experiments 1 4 the child domain has 658 cells in the longitudinal direction and 1015 cells in the latitudinal direction while in experiments 5 and 6 the child domain has 1975 cells in the longitudinal direction and 3046 cells in the latitudinal direction table 4 the number of computational cells in the parent domain is 3200 5040 for all the experiments the child domain runs on 128 mpi processes and the parent domain is partitioned in 320 mpi processes the barotropic and baroclinic time steps of the parent domain are δt t 3 0 s and δt c 60 0 s in all experiments except in experiment 5 which features δt t 1 0 s and δt c 20 0 s table 5 the nesting time step δt nt is the same as the barotropic time step of the parent domain in all experiments in experiments 1 3 and 5 the barotropic and baroclinic time steps of the child domain are the same as those of the parent domain in experiments 2 4 and 6 the barotropic and baroclinic time steps of the child domain are a third of those of the parent domain i e δt t 1 0 s and δt c 20 0 s in experiments 1 6 the number of cells excluded in the second direction n n s e w is 5 to 10 cells to prevent errors at the boundary of the child domain from propagating into the parent domain in all two way nesting experiments the width of the zone with the low resolution parent bathymetry is n b 10 cells and the width of the smoothing relaxation zone is n r 10 cells the simulations start from rest in all the experiments the m2 barotropic tide is forced at the southern boundary of the parent domain the only forcing of the child domain comes from the parent domain through the lateral boundary no atmospheric forcing is applied in all the simulations the boundary forcing and the identical grids in experiments 1 4 provide two advantages as a validation case first the result of the parent domain without any nesting can be used as the true reference solution in the experiments with identical grid resolution and two way nesting experiments 3 and 4 the two way nesting result computed in the parent domain should be the same as the reference solution because the parent domain does not obtain any improvement from the same resolution child domain this means that the difference between the nesting and the no nesting solutions represents the numerical error caused by the nesting communication the resolution of the child domain is not the same as that of the parent domain in experiments 5 and 6 since the higher resolution child domain generally returns more accurate results to the parent domain the no nesting solution is not valid as the true solution hence we focus on discontinuities and errors near the updated boundaries in the parent domain in experiments 5 and 6 second the performance of the lateral boundary condition of the child domain can be validated with one way nesting experiments 1 and 2 the child domain receives forcing only through the lateral boundary in the first coarse to fine direction but does not send back any data to the parent domain in the second fine to coarse direction the difference between the numerical results in the child domain and the no nesting solution informs us about the accuracy of the lateral boundary conditions in hycom in experiments 1 4 the accuracy over the parent domain is evaluated with the sea surface height root mean square error between the nesting result and the no nesting reference solution at each computational cell 3 rms e i j n 1 n η i j n η i j n 2 n where η is the sea surface height ssh of the nesting solution η is the ssh of the reference solution n is the temporal order of the hourly saved data n is the total number of output data and i and j are the computational cell indexes along the longitudinal and latitudinal directions respectively to exclude a spin up error only data between days 3 and 5 are used in the calculation eq 3 represents the time averaged absolute error with the reference solution the relative error can be estimated with the normalized root mean square error 4 nrm s i j r m s e i j σ i j where 5 σ i j n 1 n η i j n 2 n is the standard deviation of the sea surface heights the sea surface height root mean square errors rmse and the normalized errors nrms with the reference solution for experiments 1 4 are shown in fig 7 in these experiments the resolutions of the parent and child grids are identical the standard deviation of the sea surface heights in the parent domain for the no nesting reference solution is illustrated in fig 8 the nrms values in fig 7 are plotted only in the hudson strait region where relatively large sea surface heights are observed in fig 7a and b the rmse values for experiment 1 are mostly 2 0 mm over the child domain hud and the nrms values are 0 1 in the hudson strait region this result indicates that the lateral boundary condition of the child domain works accurately fig 7c and d shows rmse and nrms for experiment 2 in which the parent and child grid time steps differ even though these rmse and nrms values are larger than those of experiment 1 the accuracy is still quite good with errors 0 5 in the hudson strait region the errors of experiment 2 are larger than those of experiment 1 because the data are exchanged between the parent and child domains only at the nesting time steps δt nt 3 0 s and errors are accumulated at the other two barotropic time steps δt t 2 in the child domain the numerical errors of the two way nesting experiments are expected to be larger than those of the one way nesting experiments because the two way nesting performs one more communication for the second direction and there should be no accuracy improvement in the child domain with identical grid resolution the rmse and nrms values for experiment 3 in fig 7e and f are 5 0 mm and 0 2 in the hudson strait region respectively as expected these errors are slightly larger than those of the one way nesting experiment 1 however for experiment 4 with different parent and child grid time steps fig 7g and h the errors are reduced compared to experiment 2 7 0 mm and 0 35 due to unknown causes we do not observe significant error accumulations along the boundaries of the child domain in experiments 3 and 4 fig 7e and g to study how the two way nesting affects the propagation of physical variables in particular near the nesting boundaries and smoothing zones we calculate for experiments 3 6 the mass flow rate 6 m ρ η h u the residual of the continuity 7 r η t η h u and the barotropic energy fluxes egbert and ray 2001 8 f ρghη u where ρ is the water density 1036 31 kg m3 u is the barotropic velocity vector η is the sea surface height h is the water depth f f x f y is the energy flux vector f x and f y are the longitudinal and latitudinal components and g is the gravitational coefficient 9 81 m s2 the magnitude of the time mean and the standard deviation of the mass flow rate vectors and energy flux vectors and the time mean and the standard deviation of the continuity residual for experiment 6 are presented in fig 9 the results for experiments 3 6 are nearly identical and similar figures for experiments 3 5 are not shown the difference in time steps between experiments 3 and 4 and experiments 5 and 6 causes relative differences in the mass flow rate continuity and energy fluxes in the parent solution updated with the child solution that are generally smaller than 1 the mean and the standard deviation of the residual in fig 9c and d have larger values near topographic gradients as opposed to the nesting boundaries in all two way nesting experiments mass is conserved across the south west north and east boundaries there are no obvious indications that the nesting boundaries trap reflect and diffract the tidal waves this is further illustrated in fig 10 which shows the time averaged tidal energy flux in the parent domain integrated along the cells parallel to the boundaries for experiments 3 6 compared to the reference no nesting solution the tidal energy flux is conserved in all experiments since the comparisons for the mass flow rate and the continuity residual show the same smooth transition as that of the tidal energy flux they are not shown here in this section we have tested the performance of the developed two way nesting framework with six semi idealized experiments for various time steps and grid resolutions we conclude that the two way nesting framework works with a high accuracy and the tidal energy and mass are conserved quite well 4 global barotropic tides to improve the tidal accuracy in the global hycom simulations we apply the two way nesting framework to a high resolution child domain 1 75 and a relatively low resolution global parent domain 1 25 the child domain referred to as hbf and its surroundings are shown in fig 11 since the southeast entrance of fury and hecla strait is blocked as land in the relatively low resolution 1 25 domain and hudson bay plays an important role in the resonance of the m2 barotropic tide in hudson strait arbic et al 2007 the child domain in this realistic experiment is moved westward and extended to hudson bay rose welcome sound and fury and hecla strait moreover the child domain is expanded southward to include the bay of fundy and gulf of st lawrence regions that feature large tides and sea surface height root mean square errors the southward expansion ensures that additional tide gauge stations used for validation are included in the child domain the bathymetries of the parent and child domains are based on the 1 120 gebco general bathymetric chart of the oceans bathymetry it is interpolated onto the 1 75 and 1 25 hycom grids for the child and parent domains both the child and parent domains are forced by the five leading tidal constituents m2 s2 n2 k1 and o1 and a mean sea level pressure mslp the simulation period is 29 days between november 27 and december 26 in 2013 and the numerical results of the sea surface heights are stored every hour the child domain receives additional forcing from the parent domain through its lateral boundaries the simulation specifications of the realistic global scale case are summarized in table 6 to balance the number of computational cells for each process of the child and parent domains the child domain is parallelized with 1408 mpi processes and the parent domain is parallelized with 1296 mpi processes the simulation results for 29 days are decomposed into five tidal constituents with a least squares harmonic analysis the amplitudes and phases of the m2 barotropic tide are compared to the global fes2014 and tpxo9 atlas solutions in addition to the tide gauge data fig 12 displays the locations of the tide gauge stations in a the deep oceans and b shallow water coastal shelf regions for the deep oceans the tidal data from 132 stations are used for comparison and 44 out of 132 stations are located in the north atlantic ocean cyan triangles for the shallow water regions 75 stations of the total 128 sites are located in the north atlantic ocean to check the effect of the two way nesting on local and remote tides these stations are divided into two groups 38 tide gauges along the east coast of north america blue squares and 37 gauges along the european shelf cyan triangles the root mean square error between the tide gauge measurements and the hycom tpxo9 atlas and fes2014 tidal solutions averaged over all gauges is evaluated as 9 rm s tg 1 n k 1 n 1 2 a k e i ϕ k a k e i ϕ k 2 where i 1 k is the tide gauge station number n is the number of the tide gauge stations a and ϕ are the amplitude and phase of the tide gauge data a and ϕ are the amplitude and phase of the tidal solutions of hycom tpxo9 atlas or fes2014 respectively the tidal solutions are interpolated onto the locations of the tide gauge stations with a bi cubic interpolation method table 7 shows the gauge averaged root mean square error of the tidal solutions from the tide gauge data in the north atlantic ocean natl and the global ocean glb for the tpxo9 atlas and fes2014 tidal solutions and for the stand alone 1 25 hycom glb25 and the two way nesting results the overall surface tidal accuracy is improved through two way nesting in both the deep oceans and the shallow shelf seas the improvement generally decreases farther away from the child domain the reduction in rms tg of 3 cm is the largest for the north american east coast tide gauges which are located closest to the child domain for these gauges the relative improvement of tidal predictability rms tg glb25 rms tg two way rms tg glb25 15 89 however the two way nesting reduces the tidal accuracy for the gauges on the european shelf by 2 22 the relative improvement of predictability is 8 94 for all the gauges in the deep north atlantic ocean which is larger than the improvement of 6 24 for all the gauges in the deep global ocean the fes2014 and the tpxo9 atlas solutions have a better tidal accuracy than the hycom results small differences exist between the fes and tpxo solutions although fes2014 is generally more accurate than tpxo9 atlas in both the deep and shallow oceans except on the european shelf where the tpxo solution is more accurate the tide gauge data set is limited in spatial coverage hence we also validate the global hycom simulations with the fes2014 and tpxo9 atlas tidal solutions at each computational cell i j of hycom we calculate the m2 sea surface height root mean square error between the hycom simulations and the fes tpxo solutions as shriver et al 2012 10 rms e i j 1 2 a i j e i ϕ i j a i j e i ϕ i j 2 where a and ϕ are the amplitude and phase of the m2 barotropic tide in hycom and a and ϕ are the amplitude and phase of the global tidal solutions of tpxo9 atlas or fes2014 respectively since the hycom grid differs from the fes and tpxo grids the fes2014 and tpxo9 atlas solutions are interpolated onto the hycom grid with a bi cubic interpolation fig 13a shows the amplitudes and phases of the m2 barotropic tide for the two way nesting simulation the rmse between fes2014 and the two way nesting simulation is depicted in fig 13b fig 13c shows the improvement of predictability of the global m2 barotropic tide through two way nesting δrmse rmseglb25 rmsetwo way an increase in δrmse through two way nesting corresponds to a reduction in rmse relative to the reference solution without nesting the rmse values of the two way nesting experiment and δrmse in the region of the child domain are also plotted in fig 14 in figs 13c and 14b the warm cool colors indicate an improvement reduction of the predictability through two way nesting figs 13c and 14b show that the predictability is not only improved in the child domain but also in the parent domain outside the child area as far as the south atlantic ocean this clearly illustrates that the improved accuracy in the coastal shelf region of the child domain is allowed to propagate to the open ocean through the two way nesting in addition the spatially averaged root mean square error over several ocean domains rms avg is evaluated as arbic et al 2004 11 rm s avg j 1 n j i 1 n i rms e i j 2 a i j j 1 n j i 1 n i a i j where a is the area of the computational cell i j and n i and n j are the number of ocean cells in the longitudinal and latitudinal directions respectively table 8 represents the root mean square error averaged over the child area in the parent domain hbf the north atlantic ocean natl and the global parent domain glb respectively for water depths 5 0 m in addition rms avg is computed over water depths 1000 m and latitudes equatorward of 66 glb66 for comparison with previous studies the differences in rms avg computed with either fes2014 or tpxo9 atlas are small the improvement of predictability is largest in the child domain 7 26 when tpxo is used this relative improvement is smaller than the 15 89 improvement computed for the tide gauges on the shelf of the north american east coast table 7 as can be seen in figs 12b and 14b these tide gauges are located in areas where δrmse is both large and positive the relative improvement of 7 26 averaged over the child domain is lower due to localized reductions in the predictability in hudson bay and foxe basin blue areas in fig 14b the improvement in predictability decreases as the size of the averaging area increases however this reduction in predictability is minimal for the north atlantic natl where the relative improvement in predictability is 7 03 when fes is used this relatively large value can be attributed to δrmse being positive over a large area fig 13c however beyond the atlantic ocean the improvement in tidal accuracy is minimal as a consequence the reduction in rms avg for the glb66 case is small i e only 2 18 while the reduction in rmse may not seem very large the surface area where the predictability is improved is generally much larger than the area where it is reduced table 9 in the hbf domain δrmse 5 0 mm occurs in 64 44 of the domain while δrmse 5 0 mm occurs only in 5 84 of the domain the largest reduction in predictability increase in rmse is observed in foxe basin fig 14b in about 1 10 of the north atlantic ocean natl the predictability in the two way nesting solution is reduced this reduction mostly occurs in other shelf regions such as the english channel the patagonian shelf and the amazon shelf as shown in fig 13c only 9 69 of the global ocean glb features an improved predictability because the pacific and indian oceans are minimally affected by the two way nesting in the north atlantic ocean 5 discussion and conclusions we have implemented an online two way nesting framework in a global ocean circulation model hycom to improve global surface tides the barotropic pressure and velocity are exchanged between a parent and a child domain with an external coupler oasis3 mct we modified the masks in both the first coarse to fine and the second fine to coarse directions and set the limits of the search area in the first direction to reduce the computing time of the remapping coefficients in oasis3 mct in the global hycom simulations hudson strait and the adjacent north atlantic ocean feature relatively large m2 sea surface height root mean square errors with the tide gauges and the altimetry constrained tide models these errors may be associated with the inability of low resolution simulations to resolve the semi diurnal tidal resonances on and off the shelf in the north atlantic ocean these errors can be reduced by two way nesting a high resolution child domain in a low resolution parent domain hence we selected the hudson strait region as the child domain in our nesting experiments the developed two way nesting framework is validated with six semi idealized one and two way nesting experiments in these experiments the child domain covers the hudson strait region while the parent domain covers the arctic north pacific and north atlantic oceans the m2 barotropic tide is forced at the southern boundary of the parent domain the accuracy of the nesting framework is tested for time steps and grid resolutions that are the same and or differ between the parent and child domains the parent and child grids have the same resolution in experiments 1 4 the tidal solution on the parent grid without nesting is used as the true reference solution the semi idealized experiments have a good agreement with the reference solution the errors of the sea surface height in the four experiments are generally much 1 in addition two experiments are performed in which the grid resolution of the child domain is three times as fine 1 75 as that of the parent domain 1 25 to check for discontinuities along the nesting boundaries it is shown that mass and energy are conserved across the boundaries through all the experiments next the developed two way nesting framework is applied to a realistic global scale experiment the resolution of the child domain 1 75 is three times as high as that of the parent domain 1 25 realistic tidal forcing m2 s2 n2 k1 and o1 and atmospheric pressure mslp are applied over the child domain hbf as well as the parent domain glb compared to the child domain of the semi idealized experiments the hbf domain is extended to hudson bay and the bay of fundy to improve the tidal accuracy in these areas the no nesting and two way nesting results are compared to tide gauge data and the fes2014 and tpxo9 atlas tidal solutions the application of the two way nesting reduces the m2 sea surface height root mean square errors with the tide gauge data and the tidal solutions in particular in the northwest atlantic ocean the largest improvement of predictability i e reduction of the root mean square error of about 16 occurs at the tide gauges located along the east coast of north america the m2 root mean square errors between the two way nesting simulation and the tpxo fes tidal solutions are not only reduced in the vicinity of the child domain but they are reduced as far as the south atlantic ocean red areas in figs 13c and 14b the improvement of the tidal predictability in the north atlantic ocean averages about 6 28 the two way nesting simulation shows that a higher horizontal grid resolution and a better resolved coastal bathymetry in the regions of hudson strait and the bay of fundy not only improves the tidal prediction in the child domain but also in the coastal and abyssal ocean away from the nesting boundaries through the back effect arbic and garrett 2010 arbic et al 2009 arbic et al 2007 in recent years the global mean m2 sea surface height root mean square error between 1 12 5 hycom and tpxo tidal solutions has been reduced from 7 48 cm in shriver et al 2012 to 4 43 cm in buijsman et al 2015 to 2 60 cm in ngodock et al 2016 the reduction from 7 48 cm to 4 43 cm is due to the inclusion of bathymetry under the ice shelves the application of a spatially varying and iterated sal and the tuning of the linear wave drag the reduction from 4 43 cm to 2 60 cm is due to the application of an augmented state ensemble kalman filter asenkf to correct for errors in the tide model associated with imperfectly known topography and damping terms in this paper we do not apply the asenkf technique the reduction in rmse from 4 43 cm to 4 13 cm for the no nesting solution is due to an increase in model resolution from 1 12 5 to 1 25 the application of the two way nesting framework to one area i e the hudson shelf reduces this global mean error to 4 03 cm which is a modest improvement this paper mainly describes the development of a two way nesting framework in a non data assimilative version of hycom and its validation with the tide gauge data and the global fes2014 and tpxo9 atlas tidal solutions to reduce the large m2 surface tide errors in the north atlantic a high resolution child grid is applied to the hudson shelf this improves the accuracy of the tides not only in the hudson shelf region but also in the atlantic ocean as far south as south africa however the european shelf the patagonian shelf and the northwest australian shelf among others still feature large sea surface height root mean square errors in hycom fig 13b to further reduce these errors we plan to apply the two way nesting technique to multiple shelf areas in different ocean basins and find the break even point where two way nesting is computationally efficient compared to the stand alone high resolution global simulation in a subsequent paper acknowledgment c h j m c b b k a and j g r gratefully acknowledge funding from the project improving global surface and internal tides through two way coupling with high resolution coastal models as part of the office of naval research onr grant n00014 15 1 2288 a j w and j f s acknowledge support from the naval research laboratory nrl contract n00014 15 wx 01744 finally p j h acknowledges financial support from the project arctic shelf and large rivers seamless nesting in global hycom as part of onr grant n00014 15 1 2594 
24041,a new technique is proposed for estimating turbulent diffusion coefficients in the present paper turbulence is assumed to be homogeneous which is based upon wavelet decomposition which separates the mean and oscillatory random parts of lagrangian trajectories a one dimensional discrete daubechies wavelet transform is applied to decompose lagrangian trajectories into components each of which corresponds to a specific time scale τ diagonal diffusion coefficients are calculated from equations obtained from a combination of classical mixing length theory and general ideas from a theory of turbulent diffusion non diagonal diffusion coefficients are found using the classical theory of the first passage boundary the technique is illustrated through the analysis of twelve trajectories of rafos floats along the california oregon coast twenty surface drifters deployed in the california current system and forty five surface drifters deployed in the black sea in 2000 2002 the approach is compared with the well known davis 1991 approach in applications to the black sea drifters and single float trajectories along the california oregon coast keywords turbulent diffusion first passage boundary rafos drifter trajectories diffusion coefficients 1 introduction lately estimations of turbulent diffusion coefficients from data have been obtained for different geographic areas of the world ocean the statistical properties of lagrangian trajectories are crucial for studying problems such as plankton spreading in the ocean transport of pollutants in the atmosphere mixing of fluids or rain initiation in clouds different numerical methods have been used to estimate turbulent diffusion coefficients from the analysis of lagrangian trajectories starting from the pioneering work of davis 1991 which is reviewed below modifications of the techniques proposed by davis have been made by zhurbas and oh 2003 lilly and gascard 2006 lilly and olhede 2009 and lilly et al 2011 developed a method which identifies and extracts time varying oscillatory features of unknown frequency such as the signature of a particle trapped in a vortex or advected by a wave rypina et al 2012 quantified lagrangian particle dispersion a technique which is widely used to characterize eddy induced tracer fluxes by constructing spreading ellipses griffa et al 1995 has suggested a parametric estimation of turbulent transport characteristics from a simple single particle model bauer et al 1998 introduced a stochastic model for the analysis of motions of drifters etc these methods have a common weakness different approaches applied to separate the real lagrangian trajectories into mean and diffusion components give in general different results for discussion see lacasce 2008 de dominicis et al 2012 compare for example techniques from fratantoni 2001 who fitted binned velocities with cubic splines bauer et al 1998 and used a gauss markov estimator qian et al 2014 as suggested by wunsch 1996 some of these methods require additional knowledge of the correlation between lagrangian and eulerian representations see for example lacasce 2008 other methods use datasets which are too small for averaging so checking their convergence is not possible see for example fratantoni 2001 here an alternative approach is developed real lagrangian trajectories are divided into mean and oscillatory random parts using discrete wavelets which are then used to estimate turbulent diffusion coefficients from a simple equation okubo and ebbesmeyer 1976 the main advantages of the approach are simplicity in calculation and universality for the analysis of lagrangian trajectories turbulent diffusion coefficients from lagrangian data are calculated in several steps first separate mean x mean and oscillatory x oscillatory components of two dimensional lagrangian trajectories are determined in order to determine mean drift and diffusion coefficients 1 x x o t x mean x o t x oscillatory x o t where x mean is the mean part of a lagrangian trajectory x oscillatory is the oscillatory part of the lagrangian trajectory x o is an initial position of the lagrangian particle note that decomposition 1 does not require that x mean x the generalized lagrangian mean glm andrews and mcintyre 1978a 1978b in fluid mechanics a motion can be divided into the glm and oscillatory parts while the motion is described as a mixed eulerian lagrangian flow it still uses an earth fixed eulerian coordinate system therefore it is incorrect to simply writev v v where v is the lagrangian mean velocity v is its oscillatory component however the approach here introduces x mean as the mean component of motion in an approximate sense and estimates a degree of its determinism see section 3 for details methods to determine x mean have been discussed by lacasce 2008 and different methods are used e g davis 1991 bauer et al 1998 rypina et al 2012 and others davis 1991 suggested estimating the mean eulerian velocity from different lagrangian particles by averaging over defined geographic regions or bins another approach is to fit the binned velocities with cubic splines bauer et al 1998 in this approach a roughness parameter which determines the spatial resolution of the mean is chosen to minimize the low frequency energy in the ocean eddy field the technique yields smoother means and this in turn affects the residual velocities after x mean has been found x oscillatory x x mean can be estimated and it is assumed that x oscillatory is the result of turbulent diffusion more complex lagrangian motions with spatial dimensionality larger than two will be discussed in a separate paper the approach is presented below as follows first a lagrangian trajectory is divided into a set of lagrangian sub trajectories each of which is characterized by a specific time scale τm m 1 2 m where m is a finite number second each sub trajectory is identified as mean or random the mean motions are typically the low frequency lagrangian drift and identified by the low frequency part of the lagrangian turbulent spectrum this is demonstrated in section 2 the random motions simulate the diffusion like behavior or some other types of lagrangian trajectories the technique for the separation is based on a discrete daubechies wavelet which has been applied to the analysis of sea surface height ssh data for the california current system ivanov et al 2009 ivanov et al 2010 this technique is applied to analysis of rafos float drift along the california oregon coast surface drifter motions for the california current system and for the analysis of surface drifters in the black sea results demonstrate that wavelets of 5 6th order allow for close approximation of drifter trajectories and selection of the lagrangian mean trajectories which have a complex shape and are functions of time note that coefficients of turbulent diffusion estimated by this approach are usually smaller than those found by the traditional methods such as davis 1991 lacasce 2008 and others this is because mean flows reconstructed for lagrangian flow by this technique are often stronger than mean flows predicted by traditional techniques the basic technique for dividing lagrangian trajectories into a set of sub trajectories sub domains is discussed in section 2 this section also demonstrates how to distinguish mean and random trajectories in order to understand what causes the diffusion like behavior of floats section 3 explains how to calculate turbulent diffusion coefficients using the general theory of turbulent diffusion and classical mixture length theory here a method based on the probability weighted moments is used to estimate the minimal number of observations necessary for objective estimates section 4 contains information about drifter observations in the large scale black sea circulation collected jointly by the naval postgraduate school monterey and the marine hydrophysical institute sevastopol results of the lagrangian analysis in the california current system and the black sea are discussed in section 5 section 6 contains the conclusions appendices a b c and d discuss decomposition turbulent diffusion coefficients for the first passage theory probability weighted moments for estimations of some model parameters and the davis 1991 method for estimates of diffusion coefficients 2 theory and methods 2 1 wavelet technique for the analysis of lagrangian trajectories a lagrangian trajectory x t y t addison 2005 can be deterministic purely random or a combination of deterministic and random application of the grid wavelet to the trajectory yields 2 t mn x t ϕ mn dt 3 f mn y t ϕ mn dt where t mn and f mn are wavelet or detail coefficients at scale m ϕ mn is a scaling function the integers m and n control the wavelet dilation and translation respectively a detailed discussion is given in torrence and compo 1998 the signal detail at scale m is defined as 4 d m t σ n t mm ϕ mm m 1 m 5 h m t σ n f mm ϕ mm m 1 m details of the mathematical calculations for the above equations are given in appendix a if xm t and ym t are both smoothed functions representing x t and y t at scale index m see eqs a4 and a5 from appendix a for calculation of xm t and ym t then 6 d m t x m 1 t x m t m 1 m 7 h m t y m 1 t y m t m 1 m this is called a multiresolution representation mallat 1989 where each subdomain is determined by index m a value of six was used for m here when m exceeds six then the mean flow is distributed between several subdomains e g for m 8 the mean flow is observed in three subdomains m 1 2 3 x1 x2 x3 y1 y2 y3 fig 1 shows observed and reconstructed lagrangian trajectories the latter from dm and hm m 6 the difference between the trajectories was quite small typically less than the line width 2 4 km fig 1a b c in fig 1a the trajectories coincide in part because the observed daily rafos positions were smoothed with a cubic spline margolina et al 2006 the order of wavelet m 6 chosen for approximation of the observed trajectory allowed the width of subdomains to vary and different scales of the observed trajectory to be studied the mean parts of the reconstructed trajectories fig 1 are shown in fig 2 the low frequency portions of the trajectories have a complex form and should be examined for randomness see section 3 sudden changes of the trajectory direction shown in fig 2 b c and noted as 1 2 3 4 are probably due to wind direction changes and others factors why not use a 2d wavelet to reconstruct trajectories the answer is very simple different components of a drifter trajectory often require wavelets of a different order for their description for example sometimes the x component of x is decomposed by a wavelet of the 6th order but another component y requires only the 5th order therefore it was simpler to use one dimensional wavelets but of different orders the selection of different time scales is demonstrated using the rafos trajectory in the california current system shown in fig 1a as an example the daubechies wavelet transform decomposed this trajectory into 6 parts from to τ1 from τ1 to τ2 τ5 to τ6 where τ1 τ2 τ5 and τ6 are boundaries between time scales the time scales were determined by using a morlet wavelet transform addison 2005 an example of how time scales were selected from a wavelet is given in fig 3 the trajectory decomposed into six sub trajectories but only three are shown time scales within each of these intervals are determined as 3 days 4 days 14 days 20 days 106 days and 184 days fig 3a shows the mean trajectory fig 3b c give two other sub trajectories at times several scales for example 3 and 4 days could be within the same interval for example from to τ1 in this case m should be increased so that only one scale is within one interval a second way to estimate the time scale is to calculate its values for the behavior of dm hm for example take dm m 1 6 from fig 7 the time scales can be determined from a global energy spectrum as defined by astaf eva 1996 of dm the width of the subdomains can be changed by varying m because τ1 τ2 τ3 τ4 τ5 and τ6 change too note too that a number of different decompositions can be used to determine the time scales of the trajectory for example huang et al 1998 developed empirical mode decomposition for analysis of complex signals to determine if a part of an observed lagrangian trajectory is the mean trajectory a criterion needs to be specified for the mean trajectory portions of the lagrangian trajectories determined by dm hm m 2 m were concentrated in limited domains of the phase space centered at the point 0 0 here it was assumed that this was diffusion first approximation and determined soley by coefficients of turbulent diffusion however this simple decomposition of lagrangian motions could be wrong in which case another model of turbulent diffusion see for example bakunin 2008 should be used for description of the turbulent transport 2 2 selection of mean trajectories consider a lagrangian trajectory which is a combination of mean and oscillatory random components which component of the drifter trajectory should be considered the mean this depends on numerous geometric and physical factors such as the length of the trajectory its behavior and others here a simple criteria based on the results obtained by rios and de mello 2016 is used the fourier series coefficients akm and bkm k 1 k for each function dm m 1 m and ckm and gkm k 1 k for each function hm m 1 m are determined and a set of functions fxm and fym is defined as 8 f xm arctan a km b km 9 f ym arctan c km g km these functions were called a phase spectrum by rios and de mello 2016 they found that a phase spectrum of a mean trajectory had a form similar to that shown in fig 4b and an oscillatory trajectory with amplitude variability at intermediate frequencies as shown in fig 4c hence a degree of randomness for any component of the lagrangian trajectory can be defined from the structure of the following functions 10 η x m l xm l x 0 11 η y m l ym l y 0 where lxm and lym are sizes of the filled domains within the mth subdomain and lx0 and ly0 are the sizes of the mth subdomain from the physical point of view for mean and random motions ηxm 0 and ηxm 1 respectively fig 4b c show fxk1 and fxk4 which were used to determine whether the observed component of the lagrangian trajectory was deterministic or not it is obvious that ηxm ω and ηym ω where ω is a phenomenological parameter which defines a mean component of the observed trajectory from the theoretical point of view ω should be very small but not 0 and should depend on m when m increases ω decreases because the size of the filled domain reduces as well ω has been determined from numerical experiments with varying m and ω 0 05 when the 10 th order daubechies wavelet transform was used varying m up to 10 an m was found for which variations of ω with the transition to m 1 were smallest ω did not depend on individual trajectories in the numerical experiments discussed above it was equal to 0 05 for the opposite case when ηxm ω and ηym ω a random component exists sometimes a lagrangian trajectory is decomposed so that ηxm ω but ηym ω in this case the trajectory dm hm of a lagrangian particle is random therefore the mean trajectory is excluded and the focus is on oscillatory trajectories which are a response to turbulent diffusion also note that at times a component of a lagrangian trajectory which is purely deterministic cannot be found in this case contributions of noise must be removed this can be done using the method of rios and de mello 2016 or others next the choice of the order of wavelet which accurately approximates the lagrangian motion is discussed first m is increased until the wavelet energy spectrum has only one global maximum within each subdomain then criterion based on the phase spectrum from rios and de mello 2016 was applied to select the mean and oscillatory motions 3 diffusion coefficients and their estimates for single trajectories 3 1 diffusion coefficients diffusion coefficients can be determined from an original approach suggested by okubo and ebbesmeyer 1976 to estimate the two diagonal diffusion coefficients k11 k22 it is easily to be applied to either an ensemble of lagrangian trajectories as well as a single trajectory see appendix d other methods can also be used to calculate the diffusion coefficients see for example bakunin 2008 the eddy diffusivities k11 and k22 are obtained from a combination of the mixing length λ and the intensity of turbulent velocity ελ 1 2 monin and yaglom 2007 i e the diffusivity is proportional to the product of a mixing length and intensity of turbulent velocities where ε is the rate of energy dissipation the turbulent intensity is assumed to equal σ1 σ2 and the mixing length is the standard deviation of the lagrangian displacement δ1 δ2 so okubo and ebbesmeyer 1976 12 k 11 cσ 1 δ 1 13 k 22 cσ 2 δ 2 where c 0 1 ozmidov 1960 δ 1 1 n 1 n 1 n x n 2 σ 1 1 n 1 n 1 n u n 2 δ 2 1 n 1 n 1 n y n 2 σ 2 1 n 1 n 1 n v n 2 n is a number of observations xn yn is the oscillatory lagrangian trajectory at time tn and un vn is the oscillatory velocity along the lagrangian trajectory at time tn also here m 1 corresponds to a mean motion determined by d1 and h1 but all other motions m 2 6 are random with intensity determined by the diffusion coefficients sometimes mean motion was observed to include two subdomains m 1 and m 2 but these cases are not discussed here it was is not always possible to conclude that the trajectories exhibited diffusive behavior if only homogeneous turbulence is considered the mean square displacement will increase linearly with time when displacement behaved differently another mechanism of turbulent diffusion is responsible here it is assumed that x dl m 2 6 d m and y hl m 2 6 h m therefore k11 m 2 6 k 11 m and k22 m 2 6 k 22 m numerical experiments with variations of m demonstrated that the coefficients of turbulent diffusion estimated by 12 and 13 from lagrangian observations tend to zero when m 6 i e k11 m 2 k11 m 3 k11 m 6 and k22 m 2 k22 m 3 k22 m 6 see also fig 7 lagrangian integral times t1 and t2 and spatial scales l1 and l2 are usually determined using k11 k22 and σ1 σ2 as follows monin and yaglom 2007 14 t 1 k 11 σ 1 2 t 2 k 22 σ 2 2 15 l 1 σ 1 t 1 l 2 σ 2 t 2 to estimate other coefficients of the diffusion tensor k which characterizes the anisotropy of the ocean flow assume that k xy kyx symmetric tensor and use the length of time of first passage e g how long it takes a float to pass from a domain δ limited by ellipse 16 x 2 α k 11 y 2 β k 22 xy γ k 12 ε o 2 where ɑ β γ and ε o are coefficients determined numerically as described in appendix b note that the turbulent environment within the ellipse is assumed to be homogeneous therefore the contribution of asymmetric diffusion to the float transport because equals to zero and k12 k21 see also collins et al 2004 where it was shown that k12 k21 the mean first passage time of this area boundary see appendix b is 17 τ x y a 1 b αx 2 k 11 βy 2 k 22 γxy k 12 18 τ δ 0 when x 2 α k 11 y 2 β k 22 xy γ k 12 ε o 2 where a and b are determined as described in appendix b eq 17 derives from the analysis of results obtained by collins et al 2004 the mean first passage time mfpt defines an average timescale for a stochastic event to first occur stratonovich 1967 to estimate the mean first passage time divide a part of the float trajectory into no portions each starting from a new point this allows construction of the probability density function for τ p τ and an estimate of the mean first passage time τ the probability density function can be reconstructed when the number of ensemble realizations no is 8 10 see for ivanov and tokmakian 2011 for details the following example illustrates the construction of an appropriate distribution function from small size sampling ivanov and tokmakian require the reconstructed diffusion coefficient k11 0 to satisfy a two parameter weibull distribution p k11 johnston et al 1994 k11 is estimated through the probability weighted moments pwms of k11 calculated from the oscillatory part of lagrangian trajectories since the first four moments are not zero k11 1 is reconstructed from knowledge of the first three moments and k11 2 is calculated from knowing that the three other moments are non zero then the least square difference between both these estimates should be minimized as 19 k 11 1 k 11 2 2 min the same method can be used to estimate k22 after reconstruction of τ k12 from eqs 17 18 can be easily determined this is a new technique for reconstruction of diffusion coefficients using the mean first passage time and the probability weighted moments for estimation of probability density functions 3 2 analysis of single trajectories note that in the case of a single trajectory so called optimal diffusion coefficients k opt are introduced see for example metzler et al 2009 vestergaard and flyrbierg 2016 the optimal coefficients are more stable to errors in calculation of kij from a single lagrangian trajectory and satisfy 20 k opt ij k ij ß ij ω 2 where ω 2 is the squared intensity of noise distorting the real process ßij such that ßij 0 if ω 2 0 to explain how this diffusion is introduced an example of turbulent coefficients k11 and k22 and a simple diffusion model is used eremeev and ivanov 1987 if mean velocities equal zero then the following equations are true eremeev and ivanov 1987 21 dx dt 2k 11 1 2 f x t dy dt 2k 22 1 2 f y t where both fx and fy are stochastic delta correlated functions rewrite eq 21 in another form and introduce the optimal diffusion coefficients as 22 k 11 opt t s x 2 t s 2 s k 11 opt t s y 2 t s 2 s where x 2 t s 1 t s 0 t s x t s x t 2 dt y 2 t s 1 t s 0 t s y t s y t 2 dt in eq 2 t is the measurement time i e the length of the observation period s is the so called lag time and is the width of the time window sliding across the time series in an ergodic system the long time average 22 will provide the same information as the ensemble average note that metzler et al 2009 discussed a case of the diffusion coefficients for the mean square displacement t α where α 1 here we limit our discussion to the case of α 1 because t t 4 data description three datasets are used in this paper subsurface trajectories of twelve isobaric rafos floats along the oregon california coast twenty surface trajectories from noaa s global drifter program in the california current system and fourty five surface trajectories of marine hydrophysical institute mhi naval postgraduate school nps drifters in the black sea collins et al 2004 applied davis 1991 s technique to rafos floats deployed between 1992 and 2002 for statistical analysis of the large scale circulation in the california current system the symmetric part k s of the turbulent diffusivity tensor was estimated as 23 k s k 11 s k 12 s k 21 s k 22 s 0 92 0 22 0 22 1 07 10 7 cm 2 s k s 11 k s 22 k s 12 k s 21 0 the turbulent diffusion coefficients estimated here are usually smaller than those obtained using davis s 1991 approach note that the diffusivity tensor 23 was averaged over all rafos floats see poulain and niiler 1989 for details the diffusion coefficients were deterimed for 20 surface drifter trajectories from the noaa global drifter program http www aoml noaa gov phod dac gdp html results indicated that initial smoothing of the drifter trajectories was unsatisfactory when compared to the original data as well as trajectories reconstructed from the 6th order daubechies wavelet decomposition fig 10a when m 10 the reconstructed trajectories were closer to the observed trajectory but the difference between the observed and reconstructed trajectories was non zero because the drifter trajectories contained observational errors diffusion coefficients were also estimated for forty five surface drifters float numbers 16 330 to 17 491 from the nps mhi drifters deployed in the black sea by dr s motizhev mhi between 2001 and 2002 these floats have been used by poulain et al 2005 and ivanov et al 2007 to study the of large scale circulation of the black sea 5 results the lagrangian mean motion is estimated by applying the method discussed above to rafos floats deployed by the naval postgraduate school in the california current system surface drifters from noaa s global drifter program and the mhi nps surface drifters in the black sea in 2000 2002 an example is shown in fig 1a for rafos float n090 which moved northward along the california coast this is a complex trajectory for which the minimum of diffusion and thus a weaker turbulent field was observed near 44 n 126 w this can be demonstrated by estimating σy and σx 5 1 rafos floats deployed into the california current system as the first example of estimates for diffusion coefficients rafos floats n035 the trajectory is 455 days long and n115 the trajectory is 786 days long are used t for these floats is of order of several dozen of days the observation time t is about 1000 days the observational and mean trajectories for the floats are shown in figs 5a and 11a b respectively the estimates of k11 and k22 as a function of time were calculated for tn to nδt where to 100 days n 0 20 δt 50 days t20 t results are shown in fig 5b and 6a respectively a simple analysis of behavior of the diffusion coefficients indicates that they varied by an order of magnitude increasing with time for n035 decreasing with time for n115 k22 it is difficult to select a single value for the coefficients although they tend to asymptotes at the longest times the optimal diffusion coefficients estimated by eqs 22 are shown in figs 5c and 6b respectively they also appear to have explicit asymptotes for longest times and converge much faster than regular diffusion coefficients perturbations of kopt 11 and kopt 22 for long large times are a consequence of rossby waves and other processes our estimates give kopt 11 5 7 106 cm2 s and kopt 22 6 5 105 cm2 s for float n035 and kopt 11 2 3 106 cm2 s and kopt 22 2 2 106 cm2 s for float n115 the diffusion coefficients k11 and k22 calculated using methods suggested by davis 1991 are shown in figs 5d and 6c although the orders of k11 and k22 seem similar to those of kopt 11 and kopt 22 respectively the explicit convergence of k11 with time is abscent in both cases therefore based upon the calculations shown in figs 5d and 6c it isn t clear which method yields correct turbulent diffusion coefficients to reduce uncertainties which result from calculations of diffusion coefficients by davis 1991 an ensemble of realizations was used the n115 float trajectory was divided into several parts for example into 16 parts 50 days each diffusion coefficients calculated for each part and then averaged over all these parts results after the averaging are shown in fig 6c the number of used parts cannot be 8 10 because in opposite case errors of calculated diffusion coefficients will be large for details see ivanov and tokmakian 2011 there are two ways to use the results obtained by the technique described here first the optimal diffusion coefficients along each lagrangian trajectory can be calculated using eqs 21 22 the result is a field of diffusion coefficients then interpolation is used to re calculate diffusion coefficients onto nodes of a regular grid and to fill gaps diffusion coefficients are then reconstructed as a function of spatial variables the second method would be to use an ensemble of lagrangian trajectories calculate turbulent diffusion coefficients along the trajectories and then average them over the ensemble the results can be compared to results obtained by other techniques at the end of this section results are compared with those computed using by davis 1991 method without any physical interpretation of fig 1 apply a discrete daubechies wavelet of the sixth order to the trajectory results of the decomposition of the rafos trajectory are given in fig 7 a m appropriate subdomains can be chosen as follows the first subdomain larger than 49 days the second subdomain from 14 days to 40 days and etc estimates of τm are obtained easily from the analysis of the wavelet spectrum using the morlet wavelet addison 2005 it is clear that the mean trajectory this is a deterministic trajectory corresponding to the first selected subdomain see below is not a simple trajectory and cannot be approximated as a motion with velocity which is constant over time components of the lagrangian mean trajectory are given in fig 7 d1 and h1 all other components hi and di can be found in the same figure note that the mean lagrangian trajectory is differentiable see fig 8 a function f is said to be continuously differentiable if the derivative f x exists and is itself a continuous function i e it has no gaps banach 1931 the mean trajectory changes with time more slowly than the observed trajectory i e faster temporal oscillations have been filtered out of the observations compare red and blue trajectories in fig 9 before calculating diffusion coefficients from the decomposed data it is important to understand which trajectories represent a mean i e do not contribute to a diffusion process and which trajectories simulate diffusive behavior of floats and should be considered random for rafos floats ω 0 05 lx1 equals to 0 03 for the first subdomain 0 17 for the second subdomain 0 5 for the third subdomain 0 55 for the forth subdomain 0 7 for the fifth subdomain and 0 7 for the last sixth subdomain these values can be easily calculated from fig 10 a simple visual analysis of all figures confirms that the trajectory in the first subdomain should be considered deterministic all other trajectories are random and they simulate only the turbulent diffusion we assume that a trajectory corresponding to the first subdomain is entirely mean flow then xr x d1 and yr y h1 and the diffusion can be studied as the field of xr yr as an example see fig 11 analyzing behavior of d1 h1 with time note that the difference between the observed n115 trajectory and mean trajectory varies with time the optimal diffusion coefficients for twelve rafos floats deployed along the california oregon coast but at different times and at different depths margolina et al 2006 have be calculated results for rafos float n115 are 24 k opt 11 2 3 10 6 cm 2 s k opt 22 2 2 10 6 cm 2 s k opt 12 0 17 10 6 cm 2 s t1 7 8 days t2 13 6 days l1 8 1 km l2 8 7 km next diffusion coefficients kopt 11 τ kopt 22 τ and time scales τ are compared table 1 comparing results obtained by eqs 21 and 22 with previous results 23 the diffusion coefficients obtained by the new technique were somewhat smaller than previous results 23 additionally calculation of diffusion coefficients does not require a trajectory ensemble this is the principle difference between the new and old approaches and methods developed the optimal diffusion coefficients calculated for floats n108 n102 n090 n089 n088 n085 n071 n064 n050 and n039 are shown in table 2 table 2 shows that the optimal coefficients strongly varied from float to float but were of the same order the mean coefficients of turbulent diffusion averaged over all rafos trajectories were kopt 11 0 88 106 cm2 s kopt 22 0 86 106 cm2 s kopt 12 0 22 106 cm2 s in general good agreement exists between these estimates and those obtained by davis 1991 a visual analysis of the surface drifter trajectories demonstrate that these data are not as smooth and contain more noise as compared to the subsurface rafos float data the error of interpolation for the subsurface float data is 1 5 2 drifters deployed into the california current system the diffusion coefficients calculated from drifter data controlled interpolated and distributed by noaa s global drifter program are smaller than those obtained for the rafos floats for example for drifter n4438 see fig 12 a b the optimal diffusion coefficients was estimated as kopt 11 0 48 107 cm2 s kopt 22 0 26 107 cm2 s kopt 12 0 05 107 cm2 s t1 1 days t2 1 6 days l1 2 0 km l2 2 4 km and for drifter n9771 kopt 11 0 48 107 cm2 s kopt 22 0 37 107 cm2 s kopt 12 0 07 107 cm2 s t1 2 3 days t2 3 0 days l1 4 0 km l2 3 8 km 5 3 black sea surface drifters comparison to davis 1991 approach the usual not optimal diffusion coefficients were calculated for surface drifters deployed in the black sea during 2001 2002 an example trajectory is shown in fig 13a b and results are summarized in table 3 and fig 14 a b c here averaging is over drifters 16 330 16 337 17 430 17 491 28 377 28 379 33 347 33 352 34 829 34 834 35 499 35 502 40 419 40 428 poulain et al 2005 obtained smaller diffusion coefficients than those shown table 3 and fig 14 the technique developed here is compared with davis s method for instances where different mean velocities are subtracted from the black sea drifter data mean velocity obtained by wavelet decomposition the pseudo eulerian mean velocities calculated as in poulain et al 2005 and the mean eulerian velocities calculated by the black sea numerical model of dr s demishev mhi see also chu et al 2003 a geographic description of the black sea circulation can be found in korotaev 2003 diffusion estimates for trajectories have been found to be very sensitive to the kind of the mean flow removed from the drifter trajectories since strong shears especially those associated with the rim current contribute strongly to float dispersion following poulain et al 2005 a 50 km averaging scale to defines the pseudo eulerian statistics the diffusion coefficients calculated with subtraction of mean which is a function of t are shown in fig 14a they have explicit asymptotes for large times fig 14a which are k11 7 5 107 cm2 s k22 2 6 107 cm2 s t1 1 45 days t2 1 42 days l1 10 4 km l2 6 2 km for the same data but with the subtraction of the pseudo eulerian mean velocity calculated as it suggested by poulain et al 2005 the zonal k11 and meridional k22 components of diffusion coefficients are k11 4 5 107 cm2 s k22 1 4 107 cm2 s t1 3 0 days t2 1 2 days l1 34 km l2 12 2 km these are about half of those obtained above but the convergence of k11 and k22 is considerbly worse see fig 14b note that if the mean eulerian velocity obtained by a numerical model of the black sea circulation developed by dr s demishev mhi knysh et al 2001 is used then the difference between the diffusion coefficients obtained above note that poulain et al 2005 practically used davis 1991 approach techniques gets much larger if the mean velocity as a function of t is used then davis 1991 approach for this black sea data gives k11 6 9 107 cm2 s k22 2 4 107 cm2 s t1 1 5 days t2 1 42 days l1 11 l2 7 1 km fig 14c shows similar asymptotes for t 13 days for this estimate and the wavelet decomposition we see from the the previous analysis that for pure diffusion problems without any mean drift davis 1991 s approach gives results at least in values of diffusion coefficents in the asymptotic case when t if convergence is similar to those obtained by our approach based on eqs 12 and 13 if the mean trajectory calculated by the wavelet decomposition technique has been subtracted in both cases if we subtract a eulerian mean from the lagrangin motion based on the results of the black sea numerical model of dr s demishev mhi knysh et al 2001 see also for example chu et al 1994 or pseudo eulerian mean as it was done by poulain et al 2005 then the difference between the results from davis 1991 s and our approaches can be significant also it is not clear why in some cases for the davis 1991 approach kij does not converge to a constant for quite large times see fig 6 for a single diffusion trajectory see discussion in section 5 of the present paper 6 conclusions ocean currents are characterized by various eulerian and lagrangian time scales in the this paper a simple version of flow decomposition with only two scales is used one scale for the mean flow and another scale for turbulent diffusion the generalization of the approach for the case of multiple scales and including nonlinear exchange of energy between different scales can be made as suggested by moffatt 1983 here only the first step is developed a method to estimate the coefficients of turbulent diffusion from lagrangian trajectories it is based upon the decomposition of a lagrangian trajectory into several parts m 1 m each of which corresponds to a specific time scale τm these time scales are easily calculated from a wavelet decomposition fig 3 the general advantages of this approach are as follows first it is simple from the computational point of view matlab contains all necessary programs for the decomposition of lagrangian trajectories they work very quickly second the approach seems to work for almost all float drifter trajectories however it can be applied to the case of multi dimensional diffusivity too for example three dimensional and eight dimensional atmospheric models lorenz 1963 1996 have been examined these models were developed to study fundamental issues regarding the forecasting of spatially extended chaotic systems such as the atmospheric diffusion third the decomposition divides lagrangian trajectories into mean and oscillatory random parts based upon a wavelet decomposition see for example figs 1 and 2 mean trajectories generate a complex lagrangian field which has number of degrees of freedom explicitly larger than one or two obviously diffusion coefficients cannot be calculated arbitrarily they should satisfy limits as a consequence from general turbulence theory see for example vlasov and kelley 2014 2015 this suggests several ways to understand how well the turbulent diffusion coefficients are calculated first physical parameters such as a baroclinic deformation radius ri time scales and others upon which the coefficients depend are needed for example zhurbas and oh 2003 assumed that keff ri where keff is an effective diffusion coefficient naturally an eddy diffusivity tensor artale et al 1997 described as 25 k ij lim t 1 2 t x i t x i x j t x j i j 1 2 is the general way to describe the long time large distance behavior of the diffusion process but it gives little information on the physical parameters of the diffusion coefficients second a new method based on probabilistic representation of diffusion coefficients needs to be developed comparison of results is also very useful the results obtained in the present paper can be compared with paduan and niiler 1993 swenson and niiler 1996 who used other methods it is important to understand whether there is an important difference between the new technique and methods used earlier diffusion coefficients were determined along individual drifter trajectories but not for an ensemble of drifter trajectories as was done for example by zhurbas and oh 2003 their calculations gave k11 k22 4 6 107cm2 s t 3 4 days see table 1 lateral diffusivity and lagrangian scales in the pacific ocean for comparison with zhurbas and oh 2003 the new method proposed here gives lower values k11 k22 1 2x107cm2 s t 7 days for rafos floats compare to collins et al 2004 which used the davis 1991 technique and obtained k11 k22 1x107cm2 s and considerably less k11 k22 0 4 107cm2 s t 1 5 days for surface floats deployed in the california current system k11 k22 t are also functions of horizontal scale note also tha choice of c 0 1 in eqs 12 and 13 needs an additional study diffusion coefficients in the upper ocean layer being less than the same parameters estimated for subsurface rafos floats can be easily explained because t therafos floats sampled in the region of higher eddy activity the surface drifters give some averaged presentation of eddy activity in the upper ocean layer a simple example demonstrates that we have reconstructed k11 k22 and k12 quite accurately because the shape of the reconstructed distribution function weibull distribution is well known different approaches can potentially be used here such as a bayesian technique maximum likelihood method and others comparison of our reconstruction technique with bayesian see ivanov and tokmakian 2011 and likelihood techniques hosking 1990 2006 delicado and goria 2008 not shown here demonstrates that our reconstruction technique gives more accurate results when small samples are used presently there are many studies which obtain diffusion coefficients analytically these results can be used to find asymptotic equations for coefficients for example see dolginov and silan ev 1987 they found a general equation for calculation of the diffusion coefficients and demonstrated that if a gaussian ensemble of lagrangian trajectories keff ξ 2 ξ where keff is an effective diffusion coefficient ξ u 0 τ 0 3 r 0 is the strouhal number u0 τ0 and r0 are characteristic large scale velocity lifetime and correlation length of turbulent pulsations respectively for non gaussian ensembles we have keff ξ 1 if ξ obviously correct estimations for turbulent diffusion coefficients from lagrangian data should also have a correct asymptotics for ξ acknowledgements leonid ivanov thanks prof curt collins of the naval postgraduate school for constructive criticism and help with the manuscript appendix a decomposition of a lagrangian trajectory in the present paper we use a mathematical approach for the analysis of float drifter trajectories only but in principle the approach can be expanded onto a multi dimensional diffusion let us apply the grid wavelet to a lagrangian trajectory x t y t addison 2005 a1 t mn x t ϕ mn dt a2 f mn y t ϕ mn dt where tmn and fmn are the wavelet or detail coefficients at scale m φm n is the scaling function which represents the smoothed signal and is given by a3 φ m n 2 m 2 ϕ 2 m t n where ϕ is sometimes referred to as the father scaling function or father wavelet estimations of the scaling function for discrete daubechies wavelet transform are for example given in ruch and van fleet 2011 a continuous approximation of the signal can be generated by summing up a sequence of scaling functions at this scale factored by the approximation coefficients as follows a4 x m t σ n n s mn φ mn a5 y m t σ n n c mn φ mn where xm t and ym t are a smooth scaling function dependent version of the signal x t at scale index m a signal can be represented through a combined series expansion using both the approximation coefficients and detail coefficients as follows a6 x t σ n n s m 0 n φ m 0 n σ m m 0 σ n t mn ϕ mn a7 y t σ n n c m 0 n φ m 0 n σ m m 0 σ n f mn ϕ mn where xm ym are smooth scaling functions of the signal x t y t at scale m smn and cmn are approximate coefficients mo is an index which will be determined later the signal detail at scale m is defined as a8 d m t σ n t mn ϕ mn a9 h m t σ n f mn ϕ mn when a discrete input signal of finite length is considered the series of coefficients s 1 n has the length n 2m since 1 m m and the finite range of n halves at each scale addison 2005 we can re write equations a6 and a7 for the input signal m 1 as a sum of the smoothed signal at scale m and a combination of signal details at scales m a10 x 1 t x m t σ m 1 m m d m a11 y 1 t y m t σ m 1 m m h m we can re write eqs a10 and a11 as a11 d m t x m 1 t x m t a12 h m t y m 1 t y m t so if we can calculate dm t hm t from a12 and a13 we are able to estimate a signal within scale m eqs a12 and a13 tells us that if we add the signal detail at an arbitrary scale index m to the approximation at that scale we get the signal approximation at an increased resolution i e at a smaller scale index m 1 this is called a multiresolution representation mallat 1989 an error of approximation for eqs a12 and a13 can be calculated as a13 j t i σ i 1 i x obs t i σ m 1 m d m 2 x obs t i 2 y obs t i σ m 1 m d m 2 y obs t i 2 where eq a13 gives the root mean square error at time moments ti i is the number of observations along the lagrangian trajectory appendix b first passage time let us estimate the mean first passage time τ for a domain bounded by an ellipse δ and floats producing only diffusion transport i e under the assumption of a zero mean float velocity the mean first passage time should satisfy the following equation stratonovich 1967 b1 k 11 2 x 2 k 22 2 y 2 k 12 2 x y τ 1 where the ellipsoidal boundary δ equals to b2 x 2 α k 11 y 2 β k 22 xy γ k 12 ε o 2 where ε 0 is tolerance ɑ β and γ are some coefficients the boundary condition on δ is b3 τ 0 we try to find a formal solution in the form b4 τ a 1 b x 2 α k 11 y 2 β k 22 xy γ k 12 by substituting eq b4 into eqs b1 and b3 we find that b εo 1 and a b 1 α k 11 β k 22 γ k 12 1 k12 is a solution of eq b5 b5 τ exact τ obs 2 min here we assume that diffusion coefficients k11 and k22 were obtained from eqs 12 and 13 from the practical point of view we should solve an equation b6 n 1 n τ x n y n obs a k 11 k 22 k 12 1 b x 2 α k 11 y 2 β k 22 xy γ k 12 2 min by an iterative method to find k12 when coefficients k11 and k22 are known appendix c reconstruction of mean first passage time probability weighted moments defined by greenwood et al 1979 are precursors of l moments sample probability weighted moments computed from data values x1 x2 xn arranged in an increasing order are given by b0 n 1 j 1 n x j br n 1 j 1 n j 1 j r n 1 n r x j l moments are certain linear combinations of probability weighted moments that have simple interpretations as measures of the location dispersion and shape of the data sample the first few l moments are defined by l1 b1 l2 2b1 b0 l3 6b2 6b1 b0 l4 20b3 30b2 12b1 b0 the coefficients are those of the shifted legendre polynomials the first l moment is the sample mean a measure of location the second l moment is a multiple of gini s mean difference statistic a measure of the dispersion of the data values about their mean let us calculate the first four moments using some assumption about the shape of the probability density function for simplicity we take the cumulative distribution functions to get a more stable estimate for example let it be a two dimensional weibull distribution w which requires estimation of only two unknown parameters ko the shape parameter and so the scale parameter from the analysis of four nonzero pwms a mathematical procedure from ivanov and tokmakian 2011 allows to find one of parameters in two different ways because the number of unknown coefficients is less than the number of nonzero pwms i e we are ready for example to estimate ko 1 and ko 2 then the best estimate ko will be the one minimizing the value of ko 1 ko 2 2 when the probability density function p τ has been determined we can easily find the mean first passage time τ 0 τp τ dτ appendix d davis 1991 method for estimating diffusion coefficients following davis 1991 the single particle diffusivity tensor is defined as d1 k jk x t v j t o x t o d k t o t x t o where v and d are the departures from the lagrangian mean velocity and displacement respectively to is the label for the time origin and angle brackets indicate averaging over the ensemble of particles in practice the diffusivity tensor is calculated as follows every point in the area of interest is considered as the initial point x o t of a pseudo track with both positive and negative values of time yielding an individual displacement d to t x to as a function of time as well as an individual value of velocity v to x to at the origin averaging over the ensemble we obtain the mean value of displacement d t and velocity v to then we calculate the departures d to t x to d to t x to d t and v to x to v to x to v to finally the diffusivity tensor k is calculated as the ensemble mean product v j to x to d k to t x to or d2 k 11 t 0 t p ij t d t where pij is a lagrangian velocity covariance matrix lagrangian time and space scales are defined by scaling the diagonal elements of the diffusivity matrix by the velocity variance and the r m s velocity respectively they represent memory scales following the particles specifically we can define the following scales lagrangian integral time scale d3 t ij t k ij t p i j 0 lagrangian integral space scale d4 l ii t k ij t p ij 0 1 2 
24041,a new technique is proposed for estimating turbulent diffusion coefficients in the present paper turbulence is assumed to be homogeneous which is based upon wavelet decomposition which separates the mean and oscillatory random parts of lagrangian trajectories a one dimensional discrete daubechies wavelet transform is applied to decompose lagrangian trajectories into components each of which corresponds to a specific time scale τ diagonal diffusion coefficients are calculated from equations obtained from a combination of classical mixing length theory and general ideas from a theory of turbulent diffusion non diagonal diffusion coefficients are found using the classical theory of the first passage boundary the technique is illustrated through the analysis of twelve trajectories of rafos floats along the california oregon coast twenty surface drifters deployed in the california current system and forty five surface drifters deployed in the black sea in 2000 2002 the approach is compared with the well known davis 1991 approach in applications to the black sea drifters and single float trajectories along the california oregon coast keywords turbulent diffusion first passage boundary rafos drifter trajectories diffusion coefficients 1 introduction lately estimations of turbulent diffusion coefficients from data have been obtained for different geographic areas of the world ocean the statistical properties of lagrangian trajectories are crucial for studying problems such as plankton spreading in the ocean transport of pollutants in the atmosphere mixing of fluids or rain initiation in clouds different numerical methods have been used to estimate turbulent diffusion coefficients from the analysis of lagrangian trajectories starting from the pioneering work of davis 1991 which is reviewed below modifications of the techniques proposed by davis have been made by zhurbas and oh 2003 lilly and gascard 2006 lilly and olhede 2009 and lilly et al 2011 developed a method which identifies and extracts time varying oscillatory features of unknown frequency such as the signature of a particle trapped in a vortex or advected by a wave rypina et al 2012 quantified lagrangian particle dispersion a technique which is widely used to characterize eddy induced tracer fluxes by constructing spreading ellipses griffa et al 1995 has suggested a parametric estimation of turbulent transport characteristics from a simple single particle model bauer et al 1998 introduced a stochastic model for the analysis of motions of drifters etc these methods have a common weakness different approaches applied to separate the real lagrangian trajectories into mean and diffusion components give in general different results for discussion see lacasce 2008 de dominicis et al 2012 compare for example techniques from fratantoni 2001 who fitted binned velocities with cubic splines bauer et al 1998 and used a gauss markov estimator qian et al 2014 as suggested by wunsch 1996 some of these methods require additional knowledge of the correlation between lagrangian and eulerian representations see for example lacasce 2008 other methods use datasets which are too small for averaging so checking their convergence is not possible see for example fratantoni 2001 here an alternative approach is developed real lagrangian trajectories are divided into mean and oscillatory random parts using discrete wavelets which are then used to estimate turbulent diffusion coefficients from a simple equation okubo and ebbesmeyer 1976 the main advantages of the approach are simplicity in calculation and universality for the analysis of lagrangian trajectories turbulent diffusion coefficients from lagrangian data are calculated in several steps first separate mean x mean and oscillatory x oscillatory components of two dimensional lagrangian trajectories are determined in order to determine mean drift and diffusion coefficients 1 x x o t x mean x o t x oscillatory x o t where x mean is the mean part of a lagrangian trajectory x oscillatory is the oscillatory part of the lagrangian trajectory x o is an initial position of the lagrangian particle note that decomposition 1 does not require that x mean x the generalized lagrangian mean glm andrews and mcintyre 1978a 1978b in fluid mechanics a motion can be divided into the glm and oscillatory parts while the motion is described as a mixed eulerian lagrangian flow it still uses an earth fixed eulerian coordinate system therefore it is incorrect to simply writev v v where v is the lagrangian mean velocity v is its oscillatory component however the approach here introduces x mean as the mean component of motion in an approximate sense and estimates a degree of its determinism see section 3 for details methods to determine x mean have been discussed by lacasce 2008 and different methods are used e g davis 1991 bauer et al 1998 rypina et al 2012 and others davis 1991 suggested estimating the mean eulerian velocity from different lagrangian particles by averaging over defined geographic regions or bins another approach is to fit the binned velocities with cubic splines bauer et al 1998 in this approach a roughness parameter which determines the spatial resolution of the mean is chosen to minimize the low frequency energy in the ocean eddy field the technique yields smoother means and this in turn affects the residual velocities after x mean has been found x oscillatory x x mean can be estimated and it is assumed that x oscillatory is the result of turbulent diffusion more complex lagrangian motions with spatial dimensionality larger than two will be discussed in a separate paper the approach is presented below as follows first a lagrangian trajectory is divided into a set of lagrangian sub trajectories each of which is characterized by a specific time scale τm m 1 2 m where m is a finite number second each sub trajectory is identified as mean or random the mean motions are typically the low frequency lagrangian drift and identified by the low frequency part of the lagrangian turbulent spectrum this is demonstrated in section 2 the random motions simulate the diffusion like behavior or some other types of lagrangian trajectories the technique for the separation is based on a discrete daubechies wavelet which has been applied to the analysis of sea surface height ssh data for the california current system ivanov et al 2009 ivanov et al 2010 this technique is applied to analysis of rafos float drift along the california oregon coast surface drifter motions for the california current system and for the analysis of surface drifters in the black sea results demonstrate that wavelets of 5 6th order allow for close approximation of drifter trajectories and selection of the lagrangian mean trajectories which have a complex shape and are functions of time note that coefficients of turbulent diffusion estimated by this approach are usually smaller than those found by the traditional methods such as davis 1991 lacasce 2008 and others this is because mean flows reconstructed for lagrangian flow by this technique are often stronger than mean flows predicted by traditional techniques the basic technique for dividing lagrangian trajectories into a set of sub trajectories sub domains is discussed in section 2 this section also demonstrates how to distinguish mean and random trajectories in order to understand what causes the diffusion like behavior of floats section 3 explains how to calculate turbulent diffusion coefficients using the general theory of turbulent diffusion and classical mixture length theory here a method based on the probability weighted moments is used to estimate the minimal number of observations necessary for objective estimates section 4 contains information about drifter observations in the large scale black sea circulation collected jointly by the naval postgraduate school monterey and the marine hydrophysical institute sevastopol results of the lagrangian analysis in the california current system and the black sea are discussed in section 5 section 6 contains the conclusions appendices a b c and d discuss decomposition turbulent diffusion coefficients for the first passage theory probability weighted moments for estimations of some model parameters and the davis 1991 method for estimates of diffusion coefficients 2 theory and methods 2 1 wavelet technique for the analysis of lagrangian trajectories a lagrangian trajectory x t y t addison 2005 can be deterministic purely random or a combination of deterministic and random application of the grid wavelet to the trajectory yields 2 t mn x t ϕ mn dt 3 f mn y t ϕ mn dt where t mn and f mn are wavelet or detail coefficients at scale m ϕ mn is a scaling function the integers m and n control the wavelet dilation and translation respectively a detailed discussion is given in torrence and compo 1998 the signal detail at scale m is defined as 4 d m t σ n t mm ϕ mm m 1 m 5 h m t σ n f mm ϕ mm m 1 m details of the mathematical calculations for the above equations are given in appendix a if xm t and ym t are both smoothed functions representing x t and y t at scale index m see eqs a4 and a5 from appendix a for calculation of xm t and ym t then 6 d m t x m 1 t x m t m 1 m 7 h m t y m 1 t y m t m 1 m this is called a multiresolution representation mallat 1989 where each subdomain is determined by index m a value of six was used for m here when m exceeds six then the mean flow is distributed between several subdomains e g for m 8 the mean flow is observed in three subdomains m 1 2 3 x1 x2 x3 y1 y2 y3 fig 1 shows observed and reconstructed lagrangian trajectories the latter from dm and hm m 6 the difference between the trajectories was quite small typically less than the line width 2 4 km fig 1a b c in fig 1a the trajectories coincide in part because the observed daily rafos positions were smoothed with a cubic spline margolina et al 2006 the order of wavelet m 6 chosen for approximation of the observed trajectory allowed the width of subdomains to vary and different scales of the observed trajectory to be studied the mean parts of the reconstructed trajectories fig 1 are shown in fig 2 the low frequency portions of the trajectories have a complex form and should be examined for randomness see section 3 sudden changes of the trajectory direction shown in fig 2 b c and noted as 1 2 3 4 are probably due to wind direction changes and others factors why not use a 2d wavelet to reconstruct trajectories the answer is very simple different components of a drifter trajectory often require wavelets of a different order for their description for example sometimes the x component of x is decomposed by a wavelet of the 6th order but another component y requires only the 5th order therefore it was simpler to use one dimensional wavelets but of different orders the selection of different time scales is demonstrated using the rafos trajectory in the california current system shown in fig 1a as an example the daubechies wavelet transform decomposed this trajectory into 6 parts from to τ1 from τ1 to τ2 τ5 to τ6 where τ1 τ2 τ5 and τ6 are boundaries between time scales the time scales were determined by using a morlet wavelet transform addison 2005 an example of how time scales were selected from a wavelet is given in fig 3 the trajectory decomposed into six sub trajectories but only three are shown time scales within each of these intervals are determined as 3 days 4 days 14 days 20 days 106 days and 184 days fig 3a shows the mean trajectory fig 3b c give two other sub trajectories at times several scales for example 3 and 4 days could be within the same interval for example from to τ1 in this case m should be increased so that only one scale is within one interval a second way to estimate the time scale is to calculate its values for the behavior of dm hm for example take dm m 1 6 from fig 7 the time scales can be determined from a global energy spectrum as defined by astaf eva 1996 of dm the width of the subdomains can be changed by varying m because τ1 τ2 τ3 τ4 τ5 and τ6 change too note too that a number of different decompositions can be used to determine the time scales of the trajectory for example huang et al 1998 developed empirical mode decomposition for analysis of complex signals to determine if a part of an observed lagrangian trajectory is the mean trajectory a criterion needs to be specified for the mean trajectory portions of the lagrangian trajectories determined by dm hm m 2 m were concentrated in limited domains of the phase space centered at the point 0 0 here it was assumed that this was diffusion first approximation and determined soley by coefficients of turbulent diffusion however this simple decomposition of lagrangian motions could be wrong in which case another model of turbulent diffusion see for example bakunin 2008 should be used for description of the turbulent transport 2 2 selection of mean trajectories consider a lagrangian trajectory which is a combination of mean and oscillatory random components which component of the drifter trajectory should be considered the mean this depends on numerous geometric and physical factors such as the length of the trajectory its behavior and others here a simple criteria based on the results obtained by rios and de mello 2016 is used the fourier series coefficients akm and bkm k 1 k for each function dm m 1 m and ckm and gkm k 1 k for each function hm m 1 m are determined and a set of functions fxm and fym is defined as 8 f xm arctan a km b km 9 f ym arctan c km g km these functions were called a phase spectrum by rios and de mello 2016 they found that a phase spectrum of a mean trajectory had a form similar to that shown in fig 4b and an oscillatory trajectory with amplitude variability at intermediate frequencies as shown in fig 4c hence a degree of randomness for any component of the lagrangian trajectory can be defined from the structure of the following functions 10 η x m l xm l x 0 11 η y m l ym l y 0 where lxm and lym are sizes of the filled domains within the mth subdomain and lx0 and ly0 are the sizes of the mth subdomain from the physical point of view for mean and random motions ηxm 0 and ηxm 1 respectively fig 4b c show fxk1 and fxk4 which were used to determine whether the observed component of the lagrangian trajectory was deterministic or not it is obvious that ηxm ω and ηym ω where ω is a phenomenological parameter which defines a mean component of the observed trajectory from the theoretical point of view ω should be very small but not 0 and should depend on m when m increases ω decreases because the size of the filled domain reduces as well ω has been determined from numerical experiments with varying m and ω 0 05 when the 10 th order daubechies wavelet transform was used varying m up to 10 an m was found for which variations of ω with the transition to m 1 were smallest ω did not depend on individual trajectories in the numerical experiments discussed above it was equal to 0 05 for the opposite case when ηxm ω and ηym ω a random component exists sometimes a lagrangian trajectory is decomposed so that ηxm ω but ηym ω in this case the trajectory dm hm of a lagrangian particle is random therefore the mean trajectory is excluded and the focus is on oscillatory trajectories which are a response to turbulent diffusion also note that at times a component of a lagrangian trajectory which is purely deterministic cannot be found in this case contributions of noise must be removed this can be done using the method of rios and de mello 2016 or others next the choice of the order of wavelet which accurately approximates the lagrangian motion is discussed first m is increased until the wavelet energy spectrum has only one global maximum within each subdomain then criterion based on the phase spectrum from rios and de mello 2016 was applied to select the mean and oscillatory motions 3 diffusion coefficients and their estimates for single trajectories 3 1 diffusion coefficients diffusion coefficients can be determined from an original approach suggested by okubo and ebbesmeyer 1976 to estimate the two diagonal diffusion coefficients k11 k22 it is easily to be applied to either an ensemble of lagrangian trajectories as well as a single trajectory see appendix d other methods can also be used to calculate the diffusion coefficients see for example bakunin 2008 the eddy diffusivities k11 and k22 are obtained from a combination of the mixing length λ and the intensity of turbulent velocity ελ 1 2 monin and yaglom 2007 i e the diffusivity is proportional to the product of a mixing length and intensity of turbulent velocities where ε is the rate of energy dissipation the turbulent intensity is assumed to equal σ1 σ2 and the mixing length is the standard deviation of the lagrangian displacement δ1 δ2 so okubo and ebbesmeyer 1976 12 k 11 cσ 1 δ 1 13 k 22 cσ 2 δ 2 where c 0 1 ozmidov 1960 δ 1 1 n 1 n 1 n x n 2 σ 1 1 n 1 n 1 n u n 2 δ 2 1 n 1 n 1 n y n 2 σ 2 1 n 1 n 1 n v n 2 n is a number of observations xn yn is the oscillatory lagrangian trajectory at time tn and un vn is the oscillatory velocity along the lagrangian trajectory at time tn also here m 1 corresponds to a mean motion determined by d1 and h1 but all other motions m 2 6 are random with intensity determined by the diffusion coefficients sometimes mean motion was observed to include two subdomains m 1 and m 2 but these cases are not discussed here it was is not always possible to conclude that the trajectories exhibited diffusive behavior if only homogeneous turbulence is considered the mean square displacement will increase linearly with time when displacement behaved differently another mechanism of turbulent diffusion is responsible here it is assumed that x dl m 2 6 d m and y hl m 2 6 h m therefore k11 m 2 6 k 11 m and k22 m 2 6 k 22 m numerical experiments with variations of m demonstrated that the coefficients of turbulent diffusion estimated by 12 and 13 from lagrangian observations tend to zero when m 6 i e k11 m 2 k11 m 3 k11 m 6 and k22 m 2 k22 m 3 k22 m 6 see also fig 7 lagrangian integral times t1 and t2 and spatial scales l1 and l2 are usually determined using k11 k22 and σ1 σ2 as follows monin and yaglom 2007 14 t 1 k 11 σ 1 2 t 2 k 22 σ 2 2 15 l 1 σ 1 t 1 l 2 σ 2 t 2 to estimate other coefficients of the diffusion tensor k which characterizes the anisotropy of the ocean flow assume that k xy kyx symmetric tensor and use the length of time of first passage e g how long it takes a float to pass from a domain δ limited by ellipse 16 x 2 α k 11 y 2 β k 22 xy γ k 12 ε o 2 where ɑ β γ and ε o are coefficients determined numerically as described in appendix b note that the turbulent environment within the ellipse is assumed to be homogeneous therefore the contribution of asymmetric diffusion to the float transport because equals to zero and k12 k21 see also collins et al 2004 where it was shown that k12 k21 the mean first passage time of this area boundary see appendix b is 17 τ x y a 1 b αx 2 k 11 βy 2 k 22 γxy k 12 18 τ δ 0 when x 2 α k 11 y 2 β k 22 xy γ k 12 ε o 2 where a and b are determined as described in appendix b eq 17 derives from the analysis of results obtained by collins et al 2004 the mean first passage time mfpt defines an average timescale for a stochastic event to first occur stratonovich 1967 to estimate the mean first passage time divide a part of the float trajectory into no portions each starting from a new point this allows construction of the probability density function for τ p τ and an estimate of the mean first passage time τ the probability density function can be reconstructed when the number of ensemble realizations no is 8 10 see for ivanov and tokmakian 2011 for details the following example illustrates the construction of an appropriate distribution function from small size sampling ivanov and tokmakian require the reconstructed diffusion coefficient k11 0 to satisfy a two parameter weibull distribution p k11 johnston et al 1994 k11 is estimated through the probability weighted moments pwms of k11 calculated from the oscillatory part of lagrangian trajectories since the first four moments are not zero k11 1 is reconstructed from knowledge of the first three moments and k11 2 is calculated from knowing that the three other moments are non zero then the least square difference between both these estimates should be minimized as 19 k 11 1 k 11 2 2 min the same method can be used to estimate k22 after reconstruction of τ k12 from eqs 17 18 can be easily determined this is a new technique for reconstruction of diffusion coefficients using the mean first passage time and the probability weighted moments for estimation of probability density functions 3 2 analysis of single trajectories note that in the case of a single trajectory so called optimal diffusion coefficients k opt are introduced see for example metzler et al 2009 vestergaard and flyrbierg 2016 the optimal coefficients are more stable to errors in calculation of kij from a single lagrangian trajectory and satisfy 20 k opt ij k ij ß ij ω 2 where ω 2 is the squared intensity of noise distorting the real process ßij such that ßij 0 if ω 2 0 to explain how this diffusion is introduced an example of turbulent coefficients k11 and k22 and a simple diffusion model is used eremeev and ivanov 1987 if mean velocities equal zero then the following equations are true eremeev and ivanov 1987 21 dx dt 2k 11 1 2 f x t dy dt 2k 22 1 2 f y t where both fx and fy are stochastic delta correlated functions rewrite eq 21 in another form and introduce the optimal diffusion coefficients as 22 k 11 opt t s x 2 t s 2 s k 11 opt t s y 2 t s 2 s where x 2 t s 1 t s 0 t s x t s x t 2 dt y 2 t s 1 t s 0 t s y t s y t 2 dt in eq 2 t is the measurement time i e the length of the observation period s is the so called lag time and is the width of the time window sliding across the time series in an ergodic system the long time average 22 will provide the same information as the ensemble average note that metzler et al 2009 discussed a case of the diffusion coefficients for the mean square displacement t α where α 1 here we limit our discussion to the case of α 1 because t t 4 data description three datasets are used in this paper subsurface trajectories of twelve isobaric rafos floats along the oregon california coast twenty surface trajectories from noaa s global drifter program in the california current system and fourty five surface trajectories of marine hydrophysical institute mhi naval postgraduate school nps drifters in the black sea collins et al 2004 applied davis 1991 s technique to rafos floats deployed between 1992 and 2002 for statistical analysis of the large scale circulation in the california current system the symmetric part k s of the turbulent diffusivity tensor was estimated as 23 k s k 11 s k 12 s k 21 s k 22 s 0 92 0 22 0 22 1 07 10 7 cm 2 s k s 11 k s 22 k s 12 k s 21 0 the turbulent diffusion coefficients estimated here are usually smaller than those obtained using davis s 1991 approach note that the diffusivity tensor 23 was averaged over all rafos floats see poulain and niiler 1989 for details the diffusion coefficients were deterimed for 20 surface drifter trajectories from the noaa global drifter program http www aoml noaa gov phod dac gdp html results indicated that initial smoothing of the drifter trajectories was unsatisfactory when compared to the original data as well as trajectories reconstructed from the 6th order daubechies wavelet decomposition fig 10a when m 10 the reconstructed trajectories were closer to the observed trajectory but the difference between the observed and reconstructed trajectories was non zero because the drifter trajectories contained observational errors diffusion coefficients were also estimated for forty five surface drifters float numbers 16 330 to 17 491 from the nps mhi drifters deployed in the black sea by dr s motizhev mhi between 2001 and 2002 these floats have been used by poulain et al 2005 and ivanov et al 2007 to study the of large scale circulation of the black sea 5 results the lagrangian mean motion is estimated by applying the method discussed above to rafos floats deployed by the naval postgraduate school in the california current system surface drifters from noaa s global drifter program and the mhi nps surface drifters in the black sea in 2000 2002 an example is shown in fig 1a for rafos float n090 which moved northward along the california coast this is a complex trajectory for which the minimum of diffusion and thus a weaker turbulent field was observed near 44 n 126 w this can be demonstrated by estimating σy and σx 5 1 rafos floats deployed into the california current system as the first example of estimates for diffusion coefficients rafos floats n035 the trajectory is 455 days long and n115 the trajectory is 786 days long are used t for these floats is of order of several dozen of days the observation time t is about 1000 days the observational and mean trajectories for the floats are shown in figs 5a and 11a b respectively the estimates of k11 and k22 as a function of time were calculated for tn to nδt where to 100 days n 0 20 δt 50 days t20 t results are shown in fig 5b and 6a respectively a simple analysis of behavior of the diffusion coefficients indicates that they varied by an order of magnitude increasing with time for n035 decreasing with time for n115 k22 it is difficult to select a single value for the coefficients although they tend to asymptotes at the longest times the optimal diffusion coefficients estimated by eqs 22 are shown in figs 5c and 6b respectively they also appear to have explicit asymptotes for longest times and converge much faster than regular diffusion coefficients perturbations of kopt 11 and kopt 22 for long large times are a consequence of rossby waves and other processes our estimates give kopt 11 5 7 106 cm2 s and kopt 22 6 5 105 cm2 s for float n035 and kopt 11 2 3 106 cm2 s and kopt 22 2 2 106 cm2 s for float n115 the diffusion coefficients k11 and k22 calculated using methods suggested by davis 1991 are shown in figs 5d and 6c although the orders of k11 and k22 seem similar to those of kopt 11 and kopt 22 respectively the explicit convergence of k11 with time is abscent in both cases therefore based upon the calculations shown in figs 5d and 6c it isn t clear which method yields correct turbulent diffusion coefficients to reduce uncertainties which result from calculations of diffusion coefficients by davis 1991 an ensemble of realizations was used the n115 float trajectory was divided into several parts for example into 16 parts 50 days each diffusion coefficients calculated for each part and then averaged over all these parts results after the averaging are shown in fig 6c the number of used parts cannot be 8 10 because in opposite case errors of calculated diffusion coefficients will be large for details see ivanov and tokmakian 2011 there are two ways to use the results obtained by the technique described here first the optimal diffusion coefficients along each lagrangian trajectory can be calculated using eqs 21 22 the result is a field of diffusion coefficients then interpolation is used to re calculate diffusion coefficients onto nodes of a regular grid and to fill gaps diffusion coefficients are then reconstructed as a function of spatial variables the second method would be to use an ensemble of lagrangian trajectories calculate turbulent diffusion coefficients along the trajectories and then average them over the ensemble the results can be compared to results obtained by other techniques at the end of this section results are compared with those computed using by davis 1991 method without any physical interpretation of fig 1 apply a discrete daubechies wavelet of the sixth order to the trajectory results of the decomposition of the rafos trajectory are given in fig 7 a m appropriate subdomains can be chosen as follows the first subdomain larger than 49 days the second subdomain from 14 days to 40 days and etc estimates of τm are obtained easily from the analysis of the wavelet spectrum using the morlet wavelet addison 2005 it is clear that the mean trajectory this is a deterministic trajectory corresponding to the first selected subdomain see below is not a simple trajectory and cannot be approximated as a motion with velocity which is constant over time components of the lagrangian mean trajectory are given in fig 7 d1 and h1 all other components hi and di can be found in the same figure note that the mean lagrangian trajectory is differentiable see fig 8 a function f is said to be continuously differentiable if the derivative f x exists and is itself a continuous function i e it has no gaps banach 1931 the mean trajectory changes with time more slowly than the observed trajectory i e faster temporal oscillations have been filtered out of the observations compare red and blue trajectories in fig 9 before calculating diffusion coefficients from the decomposed data it is important to understand which trajectories represent a mean i e do not contribute to a diffusion process and which trajectories simulate diffusive behavior of floats and should be considered random for rafos floats ω 0 05 lx1 equals to 0 03 for the first subdomain 0 17 for the second subdomain 0 5 for the third subdomain 0 55 for the forth subdomain 0 7 for the fifth subdomain and 0 7 for the last sixth subdomain these values can be easily calculated from fig 10 a simple visual analysis of all figures confirms that the trajectory in the first subdomain should be considered deterministic all other trajectories are random and they simulate only the turbulent diffusion we assume that a trajectory corresponding to the first subdomain is entirely mean flow then xr x d1 and yr y h1 and the diffusion can be studied as the field of xr yr as an example see fig 11 analyzing behavior of d1 h1 with time note that the difference between the observed n115 trajectory and mean trajectory varies with time the optimal diffusion coefficients for twelve rafos floats deployed along the california oregon coast but at different times and at different depths margolina et al 2006 have be calculated results for rafos float n115 are 24 k opt 11 2 3 10 6 cm 2 s k opt 22 2 2 10 6 cm 2 s k opt 12 0 17 10 6 cm 2 s t1 7 8 days t2 13 6 days l1 8 1 km l2 8 7 km next diffusion coefficients kopt 11 τ kopt 22 τ and time scales τ are compared table 1 comparing results obtained by eqs 21 and 22 with previous results 23 the diffusion coefficients obtained by the new technique were somewhat smaller than previous results 23 additionally calculation of diffusion coefficients does not require a trajectory ensemble this is the principle difference between the new and old approaches and methods developed the optimal diffusion coefficients calculated for floats n108 n102 n090 n089 n088 n085 n071 n064 n050 and n039 are shown in table 2 table 2 shows that the optimal coefficients strongly varied from float to float but were of the same order the mean coefficients of turbulent diffusion averaged over all rafos trajectories were kopt 11 0 88 106 cm2 s kopt 22 0 86 106 cm2 s kopt 12 0 22 106 cm2 s in general good agreement exists between these estimates and those obtained by davis 1991 a visual analysis of the surface drifter trajectories demonstrate that these data are not as smooth and contain more noise as compared to the subsurface rafos float data the error of interpolation for the subsurface float data is 1 5 2 drifters deployed into the california current system the diffusion coefficients calculated from drifter data controlled interpolated and distributed by noaa s global drifter program are smaller than those obtained for the rafos floats for example for drifter n4438 see fig 12 a b the optimal diffusion coefficients was estimated as kopt 11 0 48 107 cm2 s kopt 22 0 26 107 cm2 s kopt 12 0 05 107 cm2 s t1 1 days t2 1 6 days l1 2 0 km l2 2 4 km and for drifter n9771 kopt 11 0 48 107 cm2 s kopt 22 0 37 107 cm2 s kopt 12 0 07 107 cm2 s t1 2 3 days t2 3 0 days l1 4 0 km l2 3 8 km 5 3 black sea surface drifters comparison to davis 1991 approach the usual not optimal diffusion coefficients were calculated for surface drifters deployed in the black sea during 2001 2002 an example trajectory is shown in fig 13a b and results are summarized in table 3 and fig 14 a b c here averaging is over drifters 16 330 16 337 17 430 17 491 28 377 28 379 33 347 33 352 34 829 34 834 35 499 35 502 40 419 40 428 poulain et al 2005 obtained smaller diffusion coefficients than those shown table 3 and fig 14 the technique developed here is compared with davis s method for instances where different mean velocities are subtracted from the black sea drifter data mean velocity obtained by wavelet decomposition the pseudo eulerian mean velocities calculated as in poulain et al 2005 and the mean eulerian velocities calculated by the black sea numerical model of dr s demishev mhi see also chu et al 2003 a geographic description of the black sea circulation can be found in korotaev 2003 diffusion estimates for trajectories have been found to be very sensitive to the kind of the mean flow removed from the drifter trajectories since strong shears especially those associated with the rim current contribute strongly to float dispersion following poulain et al 2005 a 50 km averaging scale to defines the pseudo eulerian statistics the diffusion coefficients calculated with subtraction of mean which is a function of t are shown in fig 14a they have explicit asymptotes for large times fig 14a which are k11 7 5 107 cm2 s k22 2 6 107 cm2 s t1 1 45 days t2 1 42 days l1 10 4 km l2 6 2 km for the same data but with the subtraction of the pseudo eulerian mean velocity calculated as it suggested by poulain et al 2005 the zonal k11 and meridional k22 components of diffusion coefficients are k11 4 5 107 cm2 s k22 1 4 107 cm2 s t1 3 0 days t2 1 2 days l1 34 km l2 12 2 km these are about half of those obtained above but the convergence of k11 and k22 is considerbly worse see fig 14b note that if the mean eulerian velocity obtained by a numerical model of the black sea circulation developed by dr s demishev mhi knysh et al 2001 is used then the difference between the diffusion coefficients obtained above note that poulain et al 2005 practically used davis 1991 approach techniques gets much larger if the mean velocity as a function of t is used then davis 1991 approach for this black sea data gives k11 6 9 107 cm2 s k22 2 4 107 cm2 s t1 1 5 days t2 1 42 days l1 11 l2 7 1 km fig 14c shows similar asymptotes for t 13 days for this estimate and the wavelet decomposition we see from the the previous analysis that for pure diffusion problems without any mean drift davis 1991 s approach gives results at least in values of diffusion coefficents in the asymptotic case when t if convergence is similar to those obtained by our approach based on eqs 12 and 13 if the mean trajectory calculated by the wavelet decomposition technique has been subtracted in both cases if we subtract a eulerian mean from the lagrangin motion based on the results of the black sea numerical model of dr s demishev mhi knysh et al 2001 see also for example chu et al 1994 or pseudo eulerian mean as it was done by poulain et al 2005 then the difference between the results from davis 1991 s and our approaches can be significant also it is not clear why in some cases for the davis 1991 approach kij does not converge to a constant for quite large times see fig 6 for a single diffusion trajectory see discussion in section 5 of the present paper 6 conclusions ocean currents are characterized by various eulerian and lagrangian time scales in the this paper a simple version of flow decomposition with only two scales is used one scale for the mean flow and another scale for turbulent diffusion the generalization of the approach for the case of multiple scales and including nonlinear exchange of energy between different scales can be made as suggested by moffatt 1983 here only the first step is developed a method to estimate the coefficients of turbulent diffusion from lagrangian trajectories it is based upon the decomposition of a lagrangian trajectory into several parts m 1 m each of which corresponds to a specific time scale τm these time scales are easily calculated from a wavelet decomposition fig 3 the general advantages of this approach are as follows first it is simple from the computational point of view matlab contains all necessary programs for the decomposition of lagrangian trajectories they work very quickly second the approach seems to work for almost all float drifter trajectories however it can be applied to the case of multi dimensional diffusivity too for example three dimensional and eight dimensional atmospheric models lorenz 1963 1996 have been examined these models were developed to study fundamental issues regarding the forecasting of spatially extended chaotic systems such as the atmospheric diffusion third the decomposition divides lagrangian trajectories into mean and oscillatory random parts based upon a wavelet decomposition see for example figs 1 and 2 mean trajectories generate a complex lagrangian field which has number of degrees of freedom explicitly larger than one or two obviously diffusion coefficients cannot be calculated arbitrarily they should satisfy limits as a consequence from general turbulence theory see for example vlasov and kelley 2014 2015 this suggests several ways to understand how well the turbulent diffusion coefficients are calculated first physical parameters such as a baroclinic deformation radius ri time scales and others upon which the coefficients depend are needed for example zhurbas and oh 2003 assumed that keff ri where keff is an effective diffusion coefficient naturally an eddy diffusivity tensor artale et al 1997 described as 25 k ij lim t 1 2 t x i t x i x j t x j i j 1 2 is the general way to describe the long time large distance behavior of the diffusion process but it gives little information on the physical parameters of the diffusion coefficients second a new method based on probabilistic representation of diffusion coefficients needs to be developed comparison of results is also very useful the results obtained in the present paper can be compared with paduan and niiler 1993 swenson and niiler 1996 who used other methods it is important to understand whether there is an important difference between the new technique and methods used earlier diffusion coefficients were determined along individual drifter trajectories but not for an ensemble of drifter trajectories as was done for example by zhurbas and oh 2003 their calculations gave k11 k22 4 6 107cm2 s t 3 4 days see table 1 lateral diffusivity and lagrangian scales in the pacific ocean for comparison with zhurbas and oh 2003 the new method proposed here gives lower values k11 k22 1 2x107cm2 s t 7 days for rafos floats compare to collins et al 2004 which used the davis 1991 technique and obtained k11 k22 1x107cm2 s and considerably less k11 k22 0 4 107cm2 s t 1 5 days for surface floats deployed in the california current system k11 k22 t are also functions of horizontal scale note also tha choice of c 0 1 in eqs 12 and 13 needs an additional study diffusion coefficients in the upper ocean layer being less than the same parameters estimated for subsurface rafos floats can be easily explained because t therafos floats sampled in the region of higher eddy activity the surface drifters give some averaged presentation of eddy activity in the upper ocean layer a simple example demonstrates that we have reconstructed k11 k22 and k12 quite accurately because the shape of the reconstructed distribution function weibull distribution is well known different approaches can potentially be used here such as a bayesian technique maximum likelihood method and others comparison of our reconstruction technique with bayesian see ivanov and tokmakian 2011 and likelihood techniques hosking 1990 2006 delicado and goria 2008 not shown here demonstrates that our reconstruction technique gives more accurate results when small samples are used presently there are many studies which obtain diffusion coefficients analytically these results can be used to find asymptotic equations for coefficients for example see dolginov and silan ev 1987 they found a general equation for calculation of the diffusion coefficients and demonstrated that if a gaussian ensemble of lagrangian trajectories keff ξ 2 ξ where keff is an effective diffusion coefficient ξ u 0 τ 0 3 r 0 is the strouhal number u0 τ0 and r0 are characteristic large scale velocity lifetime and correlation length of turbulent pulsations respectively for non gaussian ensembles we have keff ξ 1 if ξ obviously correct estimations for turbulent diffusion coefficients from lagrangian data should also have a correct asymptotics for ξ acknowledgements leonid ivanov thanks prof curt collins of the naval postgraduate school for constructive criticism and help with the manuscript appendix a decomposition of a lagrangian trajectory in the present paper we use a mathematical approach for the analysis of float drifter trajectories only but in principle the approach can be expanded onto a multi dimensional diffusion let us apply the grid wavelet to a lagrangian trajectory x t y t addison 2005 a1 t mn x t ϕ mn dt a2 f mn y t ϕ mn dt where tmn and fmn are the wavelet or detail coefficients at scale m φm n is the scaling function which represents the smoothed signal and is given by a3 φ m n 2 m 2 ϕ 2 m t n where ϕ is sometimes referred to as the father scaling function or father wavelet estimations of the scaling function for discrete daubechies wavelet transform are for example given in ruch and van fleet 2011 a continuous approximation of the signal can be generated by summing up a sequence of scaling functions at this scale factored by the approximation coefficients as follows a4 x m t σ n n s mn φ mn a5 y m t σ n n c mn φ mn where xm t and ym t are a smooth scaling function dependent version of the signal x t at scale index m a signal can be represented through a combined series expansion using both the approximation coefficients and detail coefficients as follows a6 x t σ n n s m 0 n φ m 0 n σ m m 0 σ n t mn ϕ mn a7 y t σ n n c m 0 n φ m 0 n σ m m 0 σ n f mn ϕ mn where xm ym are smooth scaling functions of the signal x t y t at scale m smn and cmn are approximate coefficients mo is an index which will be determined later the signal detail at scale m is defined as a8 d m t σ n t mn ϕ mn a9 h m t σ n f mn ϕ mn when a discrete input signal of finite length is considered the series of coefficients s 1 n has the length n 2m since 1 m m and the finite range of n halves at each scale addison 2005 we can re write equations a6 and a7 for the input signal m 1 as a sum of the smoothed signal at scale m and a combination of signal details at scales m a10 x 1 t x m t σ m 1 m m d m a11 y 1 t y m t σ m 1 m m h m we can re write eqs a10 and a11 as a11 d m t x m 1 t x m t a12 h m t y m 1 t y m t so if we can calculate dm t hm t from a12 and a13 we are able to estimate a signal within scale m eqs a12 and a13 tells us that if we add the signal detail at an arbitrary scale index m to the approximation at that scale we get the signal approximation at an increased resolution i e at a smaller scale index m 1 this is called a multiresolution representation mallat 1989 an error of approximation for eqs a12 and a13 can be calculated as a13 j t i σ i 1 i x obs t i σ m 1 m d m 2 x obs t i 2 y obs t i σ m 1 m d m 2 y obs t i 2 where eq a13 gives the root mean square error at time moments ti i is the number of observations along the lagrangian trajectory appendix b first passage time let us estimate the mean first passage time τ for a domain bounded by an ellipse δ and floats producing only diffusion transport i e under the assumption of a zero mean float velocity the mean first passage time should satisfy the following equation stratonovich 1967 b1 k 11 2 x 2 k 22 2 y 2 k 12 2 x y τ 1 where the ellipsoidal boundary δ equals to b2 x 2 α k 11 y 2 β k 22 xy γ k 12 ε o 2 where ε 0 is tolerance ɑ β and γ are some coefficients the boundary condition on δ is b3 τ 0 we try to find a formal solution in the form b4 τ a 1 b x 2 α k 11 y 2 β k 22 xy γ k 12 by substituting eq b4 into eqs b1 and b3 we find that b εo 1 and a b 1 α k 11 β k 22 γ k 12 1 k12 is a solution of eq b5 b5 τ exact τ obs 2 min here we assume that diffusion coefficients k11 and k22 were obtained from eqs 12 and 13 from the practical point of view we should solve an equation b6 n 1 n τ x n y n obs a k 11 k 22 k 12 1 b x 2 α k 11 y 2 β k 22 xy γ k 12 2 min by an iterative method to find k12 when coefficients k11 and k22 are known appendix c reconstruction of mean first passage time probability weighted moments defined by greenwood et al 1979 are precursors of l moments sample probability weighted moments computed from data values x1 x2 xn arranged in an increasing order are given by b0 n 1 j 1 n x j br n 1 j 1 n j 1 j r n 1 n r x j l moments are certain linear combinations of probability weighted moments that have simple interpretations as measures of the location dispersion and shape of the data sample the first few l moments are defined by l1 b1 l2 2b1 b0 l3 6b2 6b1 b0 l4 20b3 30b2 12b1 b0 the coefficients are those of the shifted legendre polynomials the first l moment is the sample mean a measure of location the second l moment is a multiple of gini s mean difference statistic a measure of the dispersion of the data values about their mean let us calculate the first four moments using some assumption about the shape of the probability density function for simplicity we take the cumulative distribution functions to get a more stable estimate for example let it be a two dimensional weibull distribution w which requires estimation of only two unknown parameters ko the shape parameter and so the scale parameter from the analysis of four nonzero pwms a mathematical procedure from ivanov and tokmakian 2011 allows to find one of parameters in two different ways because the number of unknown coefficients is less than the number of nonzero pwms i e we are ready for example to estimate ko 1 and ko 2 then the best estimate ko will be the one minimizing the value of ko 1 ko 2 2 when the probability density function p τ has been determined we can easily find the mean first passage time τ 0 τp τ dτ appendix d davis 1991 method for estimating diffusion coefficients following davis 1991 the single particle diffusivity tensor is defined as d1 k jk x t v j t o x t o d k t o t x t o where v and d are the departures from the lagrangian mean velocity and displacement respectively to is the label for the time origin and angle brackets indicate averaging over the ensemble of particles in practice the diffusivity tensor is calculated as follows every point in the area of interest is considered as the initial point x o t of a pseudo track with both positive and negative values of time yielding an individual displacement d to t x to as a function of time as well as an individual value of velocity v to x to at the origin averaging over the ensemble we obtain the mean value of displacement d t and velocity v to then we calculate the departures d to t x to d to t x to d t and v to x to v to x to v to finally the diffusivity tensor k is calculated as the ensemble mean product v j to x to d k to t x to or d2 k 11 t 0 t p ij t d t where pij is a lagrangian velocity covariance matrix lagrangian time and space scales are defined by scaling the diagonal elements of the diffusivity matrix by the velocity variance and the r m s velocity respectively they represent memory scales following the particles specifically we can define the following scales lagrangian integral time scale d3 t ij t k ij t p i j 0 lagrangian integral space scale d4 l ii t k ij t p ij 0 1 2 
24042,autoregressive logistic regression models have been demonstrated to be a powerful tool for statistical simulation of spatial patterns in climate and meteorology fields in this paper we introduce a statistical framework for the simulation of ocean current patterns based on the autoregressive logistic regression models and apply it to the gulf of mexico loop current the statistical model is forced by three autoregressive terms the wind stress curl in the gulf of mexico and in the caribbean sea and the sea level pressure anomalies over the north atlantic it is used to replicate the bi weekly historical sequence of 8 loop current patterns obtained from a 24 year altimetry derived dataset the model reproduces the inter annual and intra annual variability of the original time series showing notable fitting capacity a point by point comparison between the actual and simulated pattern series confirms the capability of the model in analysing the evolution of ocean current patterns the predictive skill of the model is also explored and the preliminary forecast up to 3 months results are encouraging the presented statistical framework may find more practical applications in the future such as the generation of statistically sound climate based oceanographic scenarios for risk analyses and the mid term probabilistic prediction of ocean current patterns keywords statistical modelling autoregressive logistic regression ocean current patterns gulf of mexico loop current 1 introduction ocean currents have a key influence in many different fields like navigation e g lo and mccord 1998 fishery management e g roberts and van den berg 2002 oil and gas industry e g abascal et al 2017 transport of floating objects in the ocean e g mínguez et al 2012 climate studies e g gent 2017 and ecological analyses e g rogers et al 2017 among others nevertheless analysing and understanding the mechanism and behaviour of the currents in a given area of interest is generally not an easy task e g alves et al 2018 indeed ocean circulation is one of the most complex features of the earth science being the response either globally or locally to the interaction of several phenomena like tides winds water density gradients earth rotation and being influenced also by bathymetric features and coast orientation e g kamenkovich 1977 available from around the early 2000 s several ocean current databases with broad spatial coverage and long progressively increasing temporal extension established the basis for an in depth and comprehensive analysis of ocean currents in different areas these current databases can result from numerical oceanographic reanalysis e g saha et al 2010 chassignet et al 2009 or from satellite altimetry campaigns e g sudre et al 2013 in situ deployed instruments like adcp e g liu and weisberg 2005 or hf radar e g solabarrieta et al 2015 liu et al 2007b can also be sources of long time series of ocean current data however it is challenging to analyse the data and extract valuable information from a large dataset in order to address this issue a common practice is to summarize the variability of the current in a given study area using a limited number of characteristic spatial patterns or groups this can be achieved through different techniques e g the unsupervised learning algorithms of self organizing map som kohonen 1982 kohonen et al 2001 e g liu and weisberg 2005 iskandar et al 2008 and k means lloyd 1957 mcqueen 1967 e g hsieh et al 2004 solabarrieta et al 2014 once representative spatial patterns are identified the original time series can be expressed in terms of these patterns associating each time stamp to the most similar pattern the best matching units bmus this approximation provides a simplified and useful way to analyse the currents in a specific region it allows for example estimating the probability of occurrence of the different patterns investigating the presence of any recurrent temporal sequence of current states or checking if some specific situation tends to persist more than others e g jin et al 2010 liu et al 2016b weisberg and liu 2017 in some studies the analysis of pattern evolution is used to detect seasonal cycles through the data series e g liu et al 2008 solabarrieta et al 2015 recently some authors have tried to link the inter annual variability of some currents to well known climate indexes and atmospheric conditions e g tsui and wu 2012 zeng et al 2015b however there is still a lack of methodologies to deal simultaneously with the memory of a system and its connection with external parameters or forcings in order to address this issue we propose to use autoregressive logistic regression alr models as a new statistical framework for investigating ocean currents by taking into account the memory of the system and the effect of local or external factors at the same time the approach of studying environmental conditions through the analysis of representative pattern series using statistical methods derives from climatology and meteorology in these fields logistic regression models are proving to be a particularly powerful tool capable of modelling the dynamics by considering simultaneously the influence of factors of different spatial and temporal scales these factors affecting the investigated dynamic represent the predictors or covariates of the model prasad et al 2010 apply logistic regression to precipitation anomalies for all india and for two homogeneous meteorological subdivision on both sides of the country considering 36 covariates guanche et al 2014 analyse daily atmospheric circulation patterns over the northeastern atlantic by fitting an alr model considering as predictors monthly sea level pressure anomalies over the study domain long term trend effects and including a markov chain in the model to consider the autoregressive component of the analysed system an alr model with a similar structure is applied by antolínez et al 2016 to european atlantic wave climate with a complementary model to better estimate the persistence probability of the patterns cárdenas et al 2017 are able to simulate daily wind states for northeastern atlantic by applying an alr model that considers seasonality pressure anomalies over the north atlantic and a second order markov chain these studies demonstrated alr as a powerful tool capable of uncovering the most important features of the analysed variable such as its dependence on different scale covariates or the limits of its memory despite those proved capabilities logistic regression has never been explored in the field of ocean currents in this paper we introduce the use of alr models as a tool for statistical simulation of ocean current patterns considering the influence of different temporal and spatial scale predictors to show the capability of alr application in ocean currents we set up and fit a model to emulate the dynamics of the gulf of mexico gom loop current lc this current presents a very particular spatial variability and multiple oceanographic studies exist in the literature analysing how its dynamics are affected by several factors of different spatial and temporal scales these two aspects make of the loop current a perfect example for a logistic model application the rest of this paper is organized as follows in section 2 we provide a description of the study area and a brief summary of the results of previous studies analysing the effect of different scale factors over the loop current variability in section 3 we present the data used in the study the alr methodology and its application to the loop current are described in section 4 section 5 shows the analysis of the results and section 6 summarizes the main conclusions of this paper 2 study site ocean circulation in the gom is dominated by the lc system the lc is a part of the north atlantic ocean s western boundary current between the yucatan current and the florida current leading to the gulf stream after entering in the gom through the yucatan strait the lc flows following an anticyclonic looping path before exiting the gom through the strait of florida the main feature of the lc is the position of this looping path which can alternatively penetrate northwestward toward the mississippi river delta or retract toward cuba e g sturges and lugo fernández 2005 additionally it has been observed that large intrusions generally produce the shedding of anticyclonic eddies that start travelling westward in the gom at the same time that the lc began to retract back to the south toward a more direct entrance exit path e g lugo fernández 2016 multiple studies based on different kinds of analysis share a similar conclusion that determined lc events tend to be affected by analogous previous lc states alvera azcárate et al 2009 analyse altimetry derived lc geostrophic component over a period of approximately 13 years they observe that large eddy shedding tend to produce large lc retraction followed by a long period before the lc penetrates enough for a new shedding and vice versa the shedding of a small eddy results in a limited lc retreat and in a shorter time until the next intrusion and eddy shedding lugo fernández 2007 applies different statistical analyses on observational databases from published sources in his aim to understand if the lc dynamics represents a chaotic system or not he obtains that the system have a memory of about 10 18 months the time it takes for the lc to complete an entire cycle consisting of inrushing shedding and returning to its initial position liu et al 2016b analysing the temporal evolution of 16 lc patterns extracted from a 23 years altimetry derived database identify some characteristic pattern sequences recurrent over the analysed historical period in order to analyse inter annual variability of the lc some recent studies tried to link the lc variability with large scale climate indices lugo fernández 2007 argues that lc presents oscillations in time scale of 3 5 years which agrees with the time scale of north atlantic oscillation nao and of el niño southern oscillation enso muller karger et al 2015 analyse monthly atlantic multidecadal oscillation amo index enfield et al 2001 and the monthly multivariate enso index mei wolter and timlin 2011 with several oceanographic variables in the gom but do not find significant correlation between the indices and any of the considered variables zeng et al 2015b after extracting three lc pattern through a som application compare the frequency of occurrence of a specific pattern considered as an eddy shedding situation with various climate indices including oceanic niño index oni nao southern oscillation index soi and pacific decadal oscillation pdo these authors also analyse the influence on the frequency of occurrence of the selected pattern of the wind stress curl in the gom caribbean sea and bahamas area they find that the correlation of the frequency of occurrence of the considered lc pattern is 0 6 with the six month moving averaged oni considering a 90 day lag and 0 83 with the wind stress curl in the caribbean sea oey et al 2003 also investigate the affection on the lc of the upstream conditions in the caribbean sea and even further in the north atlantic through numerical experiments they find that the wind stress curl in the caribbean sea spin up eddies that drifting later toward the gom highly affect the lc eddy shedding capacity their results also reveal a significant influence on the lc eddy shedding frequency of the interactions between the north atlantic and the caribbean sea through the greater and lesser antilles passages in the present study taking advantage of this previous knowledge of the lc and its variability we will consider as possible covariates of the alr model for the lc the memory of the system the impact of the mentioned climate indices nao amo enso mei oni soi and pdo considering a temporal lag up to 1 year the local effect of the wind stress curl in the gom and the possible upstream affection of the wind stress curl in the caribbean sea in order to consider a possible connection to the north atlantic variability we will also include in the analysis the sea level pressure anomalies over the north atlantic this variable is often used when dealing with the variability of the north atlantic e g guanche et al 2014 klotzbach and gray 2008 3 data description current data in the gom are a derived product of altimetry data from the archiving validation and interpretation of satellite oceanographic data aviso several recent works have shown that this represents a good product to study the geostrophic component of the lc liu et al 2011 2016a 2016b weisberg and liu 2017 aviso is a multimission gridded sea level anomaly data set produced by the ssalto duacs with support from the cnes http www aviso altimetry fr duacs and distributed by the copernicus marine and environment monitoring service cmems http www marine copernicus eu it is a global product with horizontal resolution of 1 4 and daily temporal resolution the delayed time data are used for the period between 1 january 1993 through 6 may 2016 and the near real time version are used for the remaining time 7 may 2016 to 31 december 2016 the anomaly fields are combined with the mean dynamic topography mdt cnes cls13 rio et al 2014 produced by cls space oceanography division and distributed by aviso with support from cnes surface geostrophic currents were then calculated as the gradient of the absolute sea level similar to that in the work of liu et al 2008 the geostrophic current data used in this work are limited to the gom region within latitude and longitude ranges of 22 8 31 8 north and 98 8 81 8 west respectively and with the omission of coastal ocean regions with water depth 100 m where conventional altimetry data are not as reliable as in the open oceans due to a number of factors explained by vignudelli et al 2011 see fig 2 the climate indices data were obtained from climate explorer website of the royal netherlands meteorological institute http climexp knmi nl as mentioned in section 2 based on previous studies of the lc variability in this paper we consider the following climate indices nao amo enso mei oni soi and pdo wind stress curl was calculated data from the national center for environmental prediction ncep north american regional reanalysis narr 2005 mesinger et al 2006 wind data narr database includes several meteorological variables from 1979 to present with a horizontal resolution of approximatively 32 km 45 vertical layers and 3 hourly frequency covering the whole north american region narr is a result of the ncep eta model with the regional data assimilation system rdas and significantly improves the accuracy of temperature winds and precipitation compared to the ncep doe global reanalysis 2 in this work we analyse the local effect of the wind in the gom and the upstream effect in the caribbean sea as shown by other authors from the point of view of the hydrodynamic flow two different sub areas of the caribbean sea can be identified the venezuela colombia basin and the cayman basin separated by a shallow ridge extending from honduras and nicaragua to hispaniola called the nicaraguan rise e g alvera azcárate et al 2009 following zeng et al 2015b with the aim of considering only the most direct effect of the upstream atmospheric situation the wind over the cayman basin is considered in this study hence for the gom region wind stress curl was calculated within latitude and longitude ranges of 22 8 31 8 north and 98 8 81 8 west respectively and for the caribbean sea within latitude and longitude ranges of 17 0 21 4 north and 89 0 78 3 west respectively in order to reduce the dimensionality of the data we apply principal component analysis pca to the data and keep the principal components pcs retaining the 95 of the total variance resulting in 68 and 41 pcs for the gom and the caribbean sea domains respectively sea level pressure data are provided by the ncep climate forecast system reanalysis cfsr saha et al 2010 in this study we use cfsr monthly mean data with 1 spatial resolution within latitude and longitude ranges of 0 65 north and 65 15 east respectively in this domain we calculated the monthly anomalies with respect to the average months similarly to that for the wind stress curl data we reduce the dimensionality of the data by keeping the first 10 pcs of the obtained anomaly series which account for 95 of the total variance 4 methods 4 1 logistic regression general framework logistic regression is a special case of generalized linear model glm glms are statistical models widely used to investigate effects of explanatory variables on a response or dependent variable in practice these models link a linear combination of a group of predictor variables to a function of the response variable logistic regression models are widely used when the response variable to be modelled is categorical i e it can take exclusively one from a set of possible values in the case of logistic regression the function of the response variable linked to the covariates is the logit function this function represents the natural logarithm of the odds of each possible outcome with respect to an arbitrary baseline category based on this definition if y ϵ 1 n is the set of n ocean current patterns and the last category j is the baseline a generic multinomial logistic regression model for the j th pattern can be expressed as follows 1 log p y t j x t p y t j x t α j β j x t j 1 n 1 where x t is the vector of covariates at time t β j refers to the vector of coefficients for each of the values of the covariates at time t and α j represents the intercept of the model the left hand side of eq 1 is the logit of a generic category of the response variable the right hand side is the linear combination of the predictors as mentioned in section 2 the influences of the lc variability have been studied in previous studies including the memory of the lc system possible relationship with some climate index phases and the response to atmospheric variations here we set up an alr model taking all these features into account see fig 1 the influence of the memory of the system is explored by adding to the model autoregressive terms as a markov chain of an order equal to the number of previous states to be considered the implementation in an alr model is achieved by using a contrast matrix such as the helmert matrix de vries et al 1998 a markov chain of order d can be implemented as follow 2 x t ar d i 1 d k 1 n 1 z k t i γ ik where z k t i is the dummy variable obtained through the helmert matrix transformation and represents the previous loop current pattern lcp at time t i γ ik is the coefficient associated with lcp k at time t i note that the autoregressive component adds a set of n 1 d new parameters for each category of the model the influence of climate indices or atmospheric conditions on the logit of the lcps can be included in the model as follows 3 x t c i 1 n c c i β i c where c i is the vector of the values of the n c climate indices considered or of the n c principal components of the atmospheric field considered at time t and β i c is the vector of associated coefficients eqs 2 and 3 show how to introduce the effect of the considered variables in a logistic regression model however the actual significance of these variables must be carefully assessed after defining the response variable of our model in section 4 3 a statistical based method is described to assess the significance of each of these components an outline of the methodology followed to obtain the lcps and to define the components of the alr model is presented in fig 1 4 2 response variable characterization the response variable in an alr model is categorical i e it is made up of different categories that the dependent variable can exclusively assume in our case the lc is the response variable and the categories are represented by a set of its typical spatial patterns lcps in order to select these representing lcps we follow a three step clustering methodology which aims to extract patterns that homogenously span the space of the original data considering the most significant temporal scale for the pattern selection the three phases of this clustering methodology are presented next see fig 1 4 2 1 principal component analysis in order to avoid spatially correlated variables that may disturb the lc data clustering the pca is applied to the altimetry derived daily data of geostrophic current components through this analysis original data are redefined with respect to new orthogonal dimensions or empirical orthogonal functions eof the eigenvectors of the covariance matrix of that dataset pcs are the time evolution of the weights of the eofs the obtained eofs and the associated pcs are ordered based on the variance of the data explained by each component the first 84 pcs accounting for 95 of the total variance are then considered as examples the first two and last two eofs and the associated pcs obtained are shown in fig 2 the percentage of the total variance accounted for by each component is shown in the individual eof plot as expected the first two eofs show patterns coherent with the lc system while the last modes represent smaller scale spatial variations and higher frequencies affecting quite homogeneously the whole basin 4 2 2 wavelet spectrum analysis as a second step in the clustering process a joint pc wavelet analysis similar to that in liu et al 2016b is performed over the selected leading pcs the aim of this step of the clustering methodology consisting in a rectified wavelet spectral analysis liu et al 2007a on the obtained pcs is to identify the temporal scale under which no major variability of the gom geostrophic currents occurs in fig 2 we show as examples the results of this analysis for the first two and last two considered pcs the rectified wavelet spectra for the first two pcs accounting for 15 9 and 12 3 of the total variance respectively show that variations of these gom current components are mainly in the seasonal to annual time scales 2 months to 1 year the variations of the last two pcs are generally faster being the most energetic between 14 days red horizontal line in the wavelet spectral analysis panels of fig 2 and 1 year we have performed this analysis for the total of the 84 considered pcs and found that in all cases the major variability for each pc occurs at a time scale greater than or equal to 14 days therefore in order to reduce the source of unexpected noise that could negatively affect the statistical model the 84 daily pcs of the lc are rescaled to bi weekly frequency by calculating the average values from the daily series please note that in this work the application of pca has a dual purpose first to allow the joint pc wavelet analysis with the aim of exploring the temporal scales of the gom geostrophic current variability and eventually choosing a consistent time scale for the pattern extraction and second to reduce the current data dimensionality and simplify the clustering process 4 2 3 k means as a last step of the pattern extraction the k means algorithm technique is applied to the bi weekly lc pcs the maximum dissimilarity algorithm was used to pre select initial centroids ensuring k means technique to correctly describe the diversity of the data camus et al 2011 antolínez et al 2018 several tests were performed in order to find an equilibrium between the spatial variability of the current and the number of groups to be considered as a result of the clustering 8 centroids are selected representing 8 lcps fig 3 these typical lc conditions extracted from the 24 year geostrophic current dataset will represent the 8 categories in which the alr model will classify the lc the order of the current patterns have been arranged following a similarity criteria but only for an easier interpretation of the analysis of the results from the perspective of the alr model the order of the categories of the response variable does not affect the results the patterns show a broad variability of the lc and cover well different situations between the maximum retraction lcp 1 and the deepest intrusion lcp 10 it is interesting to see how for lcp 1 lcp 2 and lcp 3 representing retraction or mid intrusion stages the lc path stays close to the southwestern part of the west florida shelf while the loop tends to curve westward as its path advances deeper in the gom lcp 4 lcp 5 and lcp 6 in accordance with the findings of weisberg and liu 2017 the obtained spatial lcps also agree well with the results of liu et al 2016b that extract 16 spatial pattern with a 4 4 som application for sea level and current data over the eastern part of the gom alvera azcárate et al 2009 describe extreme lc intrusion as an intrusion that extends up to 92 w or 27 n lcp 7 and lcp 8 show these situations highlighting that the used clustering algorithm spans the space of the original data enough to represent these extreme conditions once the characteristic patterns are obtained the original 24 year series of gom geostrophic current is expressed in a sequence of bmus in which each time step of the gom geostrophic current is expressed by the most similar lcp extracted the frequencies of occurrence of the lcps in the historical series is in order from lcp 1 to lcp 8 the following 6 5 12 6 13 6 14 1 15 7 14 5 11 8 and 11 2 4 3 model set up building an alr model is the hardest part of applying logistic regression although in some applications the inclusion of certain predictors can be easily guessed i e the wind effect in shallow water areas or tidal harmonic components in narrow channel flows in general it may be challenging to select the set of explanatory variables that actually influence the current of interest moreover it must be considered that even if a model with several well chosen features is likely to fit the data better as the number of feature increases possible effects and interactions among some covariates increase as well agresti 2007 generally it is preferable to keep the model as simple as possible for a better interpretation of the results statistical inference based algorithms can be used to build the model one particularly useful when studying ocean currents is the forward stepwise method it consists of starting from the simplest model possible i e considering just the constant value and then adding sequentially new explanatory variables by testing if they produce a significant improvement in the model fitting quality or not several significance tests can be used like wald s test or aikake s information criteria here following guanche et al 2014 we propose the use of the likelihood ratio statistic this method compares the deviance ratio δdev the change of fitting quality for two nested models and the chi square distribution with δdf δnp n 1 degree of freedom δnp is the number of parameters added in the more complex model assuming a confidence level α 95 when δdev x 0 95 δdf 2 the increasing of fitting quality of the model including the new covariates is statistically significant as mentioned earlier in section 4 1 the aim of this analysis is to assess the influence of autoregressive terms and inter annual variability on the lcp evolution the results of the forward stepwise method based on likelihood ratio statistic are shown in table 1 model 0 the null model only considers the intercept value nesting to model 0 model i includes the effect of an autoregressive term i e it checks if the manifestation of a current pattern is influenced by the previous current spatial situation or not the increment in deviance obtained by adding this effect δdevi 1781 7 x 95 8 2 allows to accept the hypothesis that the first autoregressive term is significant in the 24 year series of lcps the autocorrelation of the series is further explored by progressively adding a second third and fourth autoregressive term in model ii model iii and model iv respectively as shown in table 1 the second and third term are significant as well even if the improvement achieved in the model fit decreases as the order of the autoregressive terms increases based on the statistical test the effect of including a fourth autoregressive term is not significant being the improvement in the deviance between model iv and model iii δdeviv 25 7 x 95 29 2 this finding that three previous steps at a bi weekly interval are significant to determine the next lc status is in concordance with the results of the study by zeng et al 2015a in which the authors develop a model based on an autoregressive artificial neural network to predict the gom sea surface height and find that six steps at weekly scale is the optimal delay for their model once the autoregressive component of the model has been assessed we test if any of the atmospheric variables and climate indices investigated in the previous studies on the lc variability can improve the goodness of fit of the alr model since the lc patterns are calculated based on bi weekly averaged current data all the predictors are first resampled at the same time scale for a concordance among lcp series and predictor time series in model v we add to model iii the wind stress curl in the gom and obtain δdevv 652 6 x 95 90 2 hence the contribution of the new predictor is significant in the next model we include the effect of the wind stress curl in the caribbean sea which results not significant δdevvi 330 0 x 95 131 2 however considering a delay between the lcp series and the series of wind stress curl in the caribbean we find that the effect of the wind in the caribbean sea with a 6 week delay significantly improves the goodness of fit of the alr model as shown by model vii in table 1 in the same way spanning different time lag of the north atlantic mslpas with respect to the lcp series we find that model viii including north atlantic mslpas with a 4 week time lag significantly improves the model δdevviii 128 3 x 95 141 2 to further improve the capability of the model we consider the previously introduced climate indices each one with a possible time lag up to 1 year however none of these indices with none of the considered lags is found to be statistically significant at all not shown here as an example in table 1 we show model ix which includes oni with 6 week lag as a covariate following the test the increment in the deviance is not significant δdevix 9 8 x 95 142 2 based on these results we choose model viii as the optimal alr model for the statistical simulation of the lcp series hence considering as covariates three autoregressive terms the pcs of the wind stress curl in the gom the pcs of the wind stress curl in the caribbean sea with a 6 week lag and the pcs of the north atlantic mslpas based on eqs 1 2 and 3 the final equation for our alr model can be written as follows 5 p y t j y t 1 y t 2 y t 3 x t wg x t wc x t slpana exp α j i 1 3 k 1 7 z k t i γ ik i 1 68 c i wg β i c wg i 1 41 c i wc β i c wc i 1 10 c i slpana β i c slpana k 1 7 exp α j i 1 3 k 1 7 z k t i γ ik i 1 68 c i wg β i c wg i 1 41 c i wc β i c wc i 1 10 c i slpana β i c slpana where y t represent the lcp representing the lc state at the time t x t represents the conditions of a covariate at the time t c i specifically refers to the i th pc of the considered covariate superscript acronyms specify the covariate wind stress curl in the gom wg wind stress curl in the caribbean sea wc and mslpas over the north atlantic slpana the proposed model has a total of 141 coefficients one for the intercept 7 for each autoregressive component 68 corresponding to the number of pcs of the wind stress curl in the gom 41 for the pcs of the wind stress curl in the caribbean sea and 10 corresponding to the pcs of the north atlantic mslpas these coefficients are adjusted to maximize the fit of the model to the historical data several available statistical software allow for an easy implementation and fit of a logistic regression model in this work scikit learn library pedregosa et al 2011 of python is used we apply the one vs rest scheme l2 penalty option a tolerance default value of 1e 4 and the liblinear solver algorithm once the predictors have been selected and the corresponding coefficients have been adjusted we use the fitted model to simulate the sequence of lcps for the whole 24 year period considering the probabilistic nature of the statistical model we run the model 100 times and obtain the average values of the probabilities 5 results 5 1 model fit diagnosis in order to check the capabilities of the selected model and also to show the improvement brought by the different predictors we present in fig 4 the comparison among the historical series of lcp annual probability of occurrence panel a and the ones obtained by the simulations provided by model v in panel b model vii in panel c and by the finally selected model viii panel d in each bar of the figure the probabilities of occurrence of the 8 lcps are stacked for every year so the yearly sum of probabilities is one as can be seen the results of the selected model panel d compare favorably with the historical series of lcps panel a to check how the model capabilities improve by progressively including these covariates in panel b we show the results of model v considering the three autoregressive terms and only the wind stress curl in the gom the model is capable of reproducing some of the variability found in the historical series but in general a lot of inconsistencies exist with respect to the historical series model vii considering also the effect of the wind stress curl in the caribbean sea with a 6 week lag has greatly improved skills to reproduce the lcp series variability this is clear for example during the period 1998 2001 that the model v is unable to replicate and that is instead simulated quite favorably by the model vii clear improvements can be seen also for the periods 2004 2005 2009 2010 and for the year 2014 by also including the variability provided by the mslpas over the north atlantic with a 4 week lag model viii is able to further expand the capability of the statistical model to reconstruct the lcps original series as shown in panel d to explore the intra annual variability of the lcp evolution and to check for the skills of the selected alr model in fig 5 we show the comparison between historical left panel and modelled right panel annual average probability of occurrence of lcps at monthly scale the main features in the intra annual scale are replicated quite well by the model historical frequency of retraction stage patterns like lcp 1 and lcp 2 are higher in summer and autumn with the maximum retraction lcp 1 more likely between june and november this feature is well reflected by the model results both historical and modelled lcp probabilities show that weak intrusion stages lcp 3 and lcp 4 are common situations from autumn to early winter and in early spring with lcp 3 more likely in autumn and lcp 4 in early spring the historical frequency of occurrence of the intermediate stages lcp 5 and lcp 6 is higher in autumn and winter with a minimum in late summer again the model is shown to be capable of reproducing such variations as represented by both panels the deeper intrusions lcp 7 and lcp 8 have a bimodal behaviour they are more likely to occur in winter and in summer and less so in spring and autumn the periodicity of these lcp agrees with the frequencies found by other authors for eddy shedding events often associated with deep intrusion states chang and oey 2012 hall and leben 2016 the comparison with the seasonality of the lcps extracted by the historical data highlights the capability of the model in properly simulating the intra annual variability as well the previous analysis of the model performance suggests that the proposed arl model is capable of reproducing the probability of occurrence of the extracted lcps next in order to quantify this capability a series of indices generally used when assessing a statistical classifier are considered sensitivity precision f score and accuracy sensitivity or true positive rate tpr is the proportion of real positive cases that are correctly predicted as positive true positives precision or positive predictive value ppv is defined as the proportion of predicted positive cases correctly true positives or incorrectly false positives that are actually true positives f score can be interpreted as a weighted average of the precision and sensitivity accuracy acc is the percentage of real positive and real negative correctly predicted the values obtained for these parameters are shown in table 2 and confirm the general good capabilities of the model average values 0 75 at the same time these parameters allow for a detailed assessment of the skills of the model in reproducing each singular lcp the model performs better in simulating lcp 1 lcp 2 and lcp 3 corresponding with retraction and weak intrusion lc stages see f score for example in particular tpr values equal to 1 for lcp 1 and lcp 2 remark that the model is able to find all the samples from the historical series corresponding to these two patterns on the other side the model weakest performance occurs with lcp 7 however with the values of the parameters 0 5 in any case acc values are higher than 0 90 for all categories these results confirm that the proposed alr model is capable of simulating the lcp sequence particularly for the patterns of lc retraction phase these four examined parameters are obtained from the confusion matrix this matrix breaks down the overall model results compared to actual samples for each category so it is useful to analyse it in detail the confusion matrix for our case is shown in fig 6 left panel as absolute counts and right panel as percentage the confusion matrix relates the categories predicted by the models to the actual ones take lp3 third row for an example the matrix indicates that the model correctly predicts the pattern true positives 68 times but it misses the correct prediction 17 times false negatives assigning 7 times to lcp 1 once to lcp 2 and so on an interesting feature extractable from fig 6 is that when the model get confused i e it does not predict the true pattern it tends to pick up some other spatial pattern not very different to the actual one an exception to this is given by lcp 3 that when missed by the model is in most cases replaced by lcp 1 or lcp 8 actually quite different to lcp 3 see fig 2 5 2 exploratory prediction exercises although it is not the main goal of this paper we perform here a set of prediction exercises for a first exploration of the model skills in predicting future stages of the lc efforts have been made in the prediction of the lc system evolution using statistical data analyses lugo fernández and leben 2010 presented a linear model for the prediction of the time between two consecutive lc eddy shedding events based on the retreat latitude of the lc at the end of the first event zeng et al 2015a developed an autoregressive artificial neural network for lc evolution forecast that is demonstrated to be reliable up to 4 to 6 weeks ahead in order to test the forecasting skills of the proposed alr methodology we recalibrate the model using the historical lcp series from 1993 to 2015 and we simulate the year 2016 we execute two simulations of the alr model to predict 6 month ahead for each run in the first simulation we try to predict from january 2016 to june 2016 and in the other from july 2016 to december 2016 we then use the historical lcp series of 2016 to evaluate the prediction results similar to what described in section 5 1 we run the model 100 times for each test and the results refer to the average of the probabilities obtained with the 100 simulations first of all we describe the actual historical evolution of lcps for the year 2016 as obtained by the historical sequence of bmus see tables 3 and 4 the year begins with the lc in a mid intrusion phase lcp 5 this configuration persists until approximately mid march i e during 5 steps in the series then the situation starts to evolve represented by lcp 6 for the next two steps 4 weeks which still shows a mid intrusion phase but is characterized as a tightened bottom of the loop next starting from around mid april the lc experiences a strong retraction represented by lcp 1 for about 6 months until mid october during the rest of the year the lc is represented by lcp 2 corresponding to a first step toward a new retraction intrusion retraction cycle since the alr model considers the effect of three autoregressive terms the prediction model is fed besides the values of the predictors for the whole simulation period by the three lcps prior to the first time stamp of simulation for the first exercise corresponding to the prediction of the first 6 months of 2016 the three lcps prior to the first simulation step are lcp 6 lcp 4 and lcp 5 in this temporal sequence the results of the first prediction exercise are shown in table 3 it is encouraging that the model predicts well the persistence of lcp 5 during the first 5 steps of simulation 10 weeks with lcp 5 the most likely predicted pattern during this period during the next step the transition from lcp 5 to lcp 6 occurs the model is capable to predict both this change and the persistence of the new pattern as shown in table 3 starting from april the forecast of the model becomes less accurate instead of lcp 1 the model wrongly predicts two weeks of further intrusion as expressed by lcp 8 followed by 6 weeks of lcp 4 eventually switching to lcp 5 although missing the retraction from lcp 6 to lcp 1 between march and april it is interesting to see how the model still forecasts a retraction of the lc but from lcp 8 to lcp 4 however neither the intensity nor the timing of the retraction is correctly predicted for the second prediction exercise the three steps prior to the first simulated time are a sequence of three lcp 1 as shown in table 4 the model correctly predicts lcp 1 as the most likely pattern to occur the model predicts that this pattern would persist for the next 6 months actually the pattern only persisted from july to september and changed to lcp 2 starting from october even if the model misses to predict lcp 2 it must be observed that both lcp 1 and lcp 2 refers to retraction lc stages see fig 2 so the predicted evolution of the lc for the last 3 months of 2016 does not diverge excessively from the actual one both forecast exercises show that the model has been capable to predict the true pattern up to 3 months ahead however these are exploratory experiments a deeper investigation should be carried on to better assess its predictive skills based on a longer validation period and a more detailed analysis moreover due to the limited length of the available historical database of the currents in the gom especially in light of the complicated variability of the studied phenomenon we suggest that longer historical databases should be used for a proper prediction assessment study 6 conclusions in this paper we introduce the autoregressive logistic regression alr models as the core of a statistical framework for ocean current pattern simulation an alr model allows statistically simulating current patterns considering simultaneously the influence of different spatial and temporal scale factors predictors covariates in order to check the capability of these models in ocean current simulation we set up run and validate an alr model for the gulf of mexico loop current as a case study firstly we extract 8 representative patterns at bi weekly interval from a 24 year 1993 to 2016 altimetry derived database of the geostrophic current in the gulf of mexico using a combination of pca wavelet spectral analysis and k means techniques in order to define a model capable of reproducing the historical evolution of these patterns we investigate the capacity of different factors as predictors of the lc variability based on previous studies we consider the following parameters autoregressive terms of different order wind stress curl over the gom and the caribben sea the north atlantic mslpas and various climate indices with different time lag we find that autoregressive terms of order 1 2 and 3 i e the patterns occurring on the six previous weeks the wind stress curl over the gom with no lag and over the caribbean sea with 6 week lag and the north atlantic mslpas with 4 week lag are statistically significant for our alr model none of the climate indices is significant as predictors for the lcp sequence the final model combines 141 coefficients one for the intercept 7 for each autoregressive component 68 for the pcs of the wind stress curl in the gom 41 for the pcs of the wind stress curl in the caribbean sea and 10 corresponding to the pcs of the north atlantic mslpas these coefficients are optimized to achieve the best fit between the model and the historical pattern series in order to check for the goodness of fit of the model we compare the simulated and historical loop current pattern series at both intra annual and inter annual scales despites the strong variability of the selected current we find that the model is capable of reproducing its behaviour at both scales a point by point comparison between actual and simulated pattern series is analysed through the confusion matrix and four derived indexes sensitivity precision f score and accuracy we find that the overall performance of the model is good average values of the three coefficients 0 75 and that it is particularly efficient in simulating the patterns representative of retraction stages lcp1 and lcp2 we also find that in the fitting process when the model does not select the true pattern it tends to pick up some other spatial pattern close to the actual one we also perform two forecast exercises for a preliminary assessment of the predictive skills of the model and found that in both cases the model has been capable of correctly predicting the actual lcp during the first 3 months of forecast for the rest of the simulated period although the predicted patterns do not match the historical lcps the model looks to some extent capable of predicting the general future behaviour of the lc even if the model forecast capabilities look promising at least a few months ahead a more complete study is required to better analyse this interesting application of the alr model the results of this study show the effectiveness of the proposed statistical framework in analysing the evolution of ocean current patterns this methodology could be implemented in practical applications like for example to emulate statistically sound oceanographic scenarios through climate based monte carlo simulations and even for probabilistic mid term forecasts of ocean current patterns acknowledgments the authors acknowledge the support of the spanish ministry of economy industry and competitiveness mineco under plvma3d tra2014 59570 r and oilhazard3d tra2017 89164 r national research projects j a a antolínez was funded by the spanish ministry of education culture and sport mecd fpu formación del profesorado universitario studentship boe a 2013 12235 y liu and r h weisberg are partially supported by the gulf of mexico research initiative gomri grant g 231804 the authors would like to thank prof fernando j mendez from the university of cantabria for fruitful discussions about logistic regression and clustering techniques and three anonymous reviewers for key suggestions 
24042,autoregressive logistic regression models have been demonstrated to be a powerful tool for statistical simulation of spatial patterns in climate and meteorology fields in this paper we introduce a statistical framework for the simulation of ocean current patterns based on the autoregressive logistic regression models and apply it to the gulf of mexico loop current the statistical model is forced by three autoregressive terms the wind stress curl in the gulf of mexico and in the caribbean sea and the sea level pressure anomalies over the north atlantic it is used to replicate the bi weekly historical sequence of 8 loop current patterns obtained from a 24 year altimetry derived dataset the model reproduces the inter annual and intra annual variability of the original time series showing notable fitting capacity a point by point comparison between the actual and simulated pattern series confirms the capability of the model in analysing the evolution of ocean current patterns the predictive skill of the model is also explored and the preliminary forecast up to 3 months results are encouraging the presented statistical framework may find more practical applications in the future such as the generation of statistically sound climate based oceanographic scenarios for risk analyses and the mid term probabilistic prediction of ocean current patterns keywords statistical modelling autoregressive logistic regression ocean current patterns gulf of mexico loop current 1 introduction ocean currents have a key influence in many different fields like navigation e g lo and mccord 1998 fishery management e g roberts and van den berg 2002 oil and gas industry e g abascal et al 2017 transport of floating objects in the ocean e g mínguez et al 2012 climate studies e g gent 2017 and ecological analyses e g rogers et al 2017 among others nevertheless analysing and understanding the mechanism and behaviour of the currents in a given area of interest is generally not an easy task e g alves et al 2018 indeed ocean circulation is one of the most complex features of the earth science being the response either globally or locally to the interaction of several phenomena like tides winds water density gradients earth rotation and being influenced also by bathymetric features and coast orientation e g kamenkovich 1977 available from around the early 2000 s several ocean current databases with broad spatial coverage and long progressively increasing temporal extension established the basis for an in depth and comprehensive analysis of ocean currents in different areas these current databases can result from numerical oceanographic reanalysis e g saha et al 2010 chassignet et al 2009 or from satellite altimetry campaigns e g sudre et al 2013 in situ deployed instruments like adcp e g liu and weisberg 2005 or hf radar e g solabarrieta et al 2015 liu et al 2007b can also be sources of long time series of ocean current data however it is challenging to analyse the data and extract valuable information from a large dataset in order to address this issue a common practice is to summarize the variability of the current in a given study area using a limited number of characteristic spatial patterns or groups this can be achieved through different techniques e g the unsupervised learning algorithms of self organizing map som kohonen 1982 kohonen et al 2001 e g liu and weisberg 2005 iskandar et al 2008 and k means lloyd 1957 mcqueen 1967 e g hsieh et al 2004 solabarrieta et al 2014 once representative spatial patterns are identified the original time series can be expressed in terms of these patterns associating each time stamp to the most similar pattern the best matching units bmus this approximation provides a simplified and useful way to analyse the currents in a specific region it allows for example estimating the probability of occurrence of the different patterns investigating the presence of any recurrent temporal sequence of current states or checking if some specific situation tends to persist more than others e g jin et al 2010 liu et al 2016b weisberg and liu 2017 in some studies the analysis of pattern evolution is used to detect seasonal cycles through the data series e g liu et al 2008 solabarrieta et al 2015 recently some authors have tried to link the inter annual variability of some currents to well known climate indexes and atmospheric conditions e g tsui and wu 2012 zeng et al 2015b however there is still a lack of methodologies to deal simultaneously with the memory of a system and its connection with external parameters or forcings in order to address this issue we propose to use autoregressive logistic regression alr models as a new statistical framework for investigating ocean currents by taking into account the memory of the system and the effect of local or external factors at the same time the approach of studying environmental conditions through the analysis of representative pattern series using statistical methods derives from climatology and meteorology in these fields logistic regression models are proving to be a particularly powerful tool capable of modelling the dynamics by considering simultaneously the influence of factors of different spatial and temporal scales these factors affecting the investigated dynamic represent the predictors or covariates of the model prasad et al 2010 apply logistic regression to precipitation anomalies for all india and for two homogeneous meteorological subdivision on both sides of the country considering 36 covariates guanche et al 2014 analyse daily atmospheric circulation patterns over the northeastern atlantic by fitting an alr model considering as predictors monthly sea level pressure anomalies over the study domain long term trend effects and including a markov chain in the model to consider the autoregressive component of the analysed system an alr model with a similar structure is applied by antolínez et al 2016 to european atlantic wave climate with a complementary model to better estimate the persistence probability of the patterns cárdenas et al 2017 are able to simulate daily wind states for northeastern atlantic by applying an alr model that considers seasonality pressure anomalies over the north atlantic and a second order markov chain these studies demonstrated alr as a powerful tool capable of uncovering the most important features of the analysed variable such as its dependence on different scale covariates or the limits of its memory despite those proved capabilities logistic regression has never been explored in the field of ocean currents in this paper we introduce the use of alr models as a tool for statistical simulation of ocean current patterns considering the influence of different temporal and spatial scale predictors to show the capability of alr application in ocean currents we set up and fit a model to emulate the dynamics of the gulf of mexico gom loop current lc this current presents a very particular spatial variability and multiple oceanographic studies exist in the literature analysing how its dynamics are affected by several factors of different spatial and temporal scales these two aspects make of the loop current a perfect example for a logistic model application the rest of this paper is organized as follows in section 2 we provide a description of the study area and a brief summary of the results of previous studies analysing the effect of different scale factors over the loop current variability in section 3 we present the data used in the study the alr methodology and its application to the loop current are described in section 4 section 5 shows the analysis of the results and section 6 summarizes the main conclusions of this paper 2 study site ocean circulation in the gom is dominated by the lc system the lc is a part of the north atlantic ocean s western boundary current between the yucatan current and the florida current leading to the gulf stream after entering in the gom through the yucatan strait the lc flows following an anticyclonic looping path before exiting the gom through the strait of florida the main feature of the lc is the position of this looping path which can alternatively penetrate northwestward toward the mississippi river delta or retract toward cuba e g sturges and lugo fernández 2005 additionally it has been observed that large intrusions generally produce the shedding of anticyclonic eddies that start travelling westward in the gom at the same time that the lc began to retract back to the south toward a more direct entrance exit path e g lugo fernández 2016 multiple studies based on different kinds of analysis share a similar conclusion that determined lc events tend to be affected by analogous previous lc states alvera azcárate et al 2009 analyse altimetry derived lc geostrophic component over a period of approximately 13 years they observe that large eddy shedding tend to produce large lc retraction followed by a long period before the lc penetrates enough for a new shedding and vice versa the shedding of a small eddy results in a limited lc retreat and in a shorter time until the next intrusion and eddy shedding lugo fernández 2007 applies different statistical analyses on observational databases from published sources in his aim to understand if the lc dynamics represents a chaotic system or not he obtains that the system have a memory of about 10 18 months the time it takes for the lc to complete an entire cycle consisting of inrushing shedding and returning to its initial position liu et al 2016b analysing the temporal evolution of 16 lc patterns extracted from a 23 years altimetry derived database identify some characteristic pattern sequences recurrent over the analysed historical period in order to analyse inter annual variability of the lc some recent studies tried to link the lc variability with large scale climate indices lugo fernández 2007 argues that lc presents oscillations in time scale of 3 5 years which agrees with the time scale of north atlantic oscillation nao and of el niño southern oscillation enso muller karger et al 2015 analyse monthly atlantic multidecadal oscillation amo index enfield et al 2001 and the monthly multivariate enso index mei wolter and timlin 2011 with several oceanographic variables in the gom but do not find significant correlation between the indices and any of the considered variables zeng et al 2015b after extracting three lc pattern through a som application compare the frequency of occurrence of a specific pattern considered as an eddy shedding situation with various climate indices including oceanic niño index oni nao southern oscillation index soi and pacific decadal oscillation pdo these authors also analyse the influence on the frequency of occurrence of the selected pattern of the wind stress curl in the gom caribbean sea and bahamas area they find that the correlation of the frequency of occurrence of the considered lc pattern is 0 6 with the six month moving averaged oni considering a 90 day lag and 0 83 with the wind stress curl in the caribbean sea oey et al 2003 also investigate the affection on the lc of the upstream conditions in the caribbean sea and even further in the north atlantic through numerical experiments they find that the wind stress curl in the caribbean sea spin up eddies that drifting later toward the gom highly affect the lc eddy shedding capacity their results also reveal a significant influence on the lc eddy shedding frequency of the interactions between the north atlantic and the caribbean sea through the greater and lesser antilles passages in the present study taking advantage of this previous knowledge of the lc and its variability we will consider as possible covariates of the alr model for the lc the memory of the system the impact of the mentioned climate indices nao amo enso mei oni soi and pdo considering a temporal lag up to 1 year the local effect of the wind stress curl in the gom and the possible upstream affection of the wind stress curl in the caribbean sea in order to consider a possible connection to the north atlantic variability we will also include in the analysis the sea level pressure anomalies over the north atlantic this variable is often used when dealing with the variability of the north atlantic e g guanche et al 2014 klotzbach and gray 2008 3 data description current data in the gom are a derived product of altimetry data from the archiving validation and interpretation of satellite oceanographic data aviso several recent works have shown that this represents a good product to study the geostrophic component of the lc liu et al 2011 2016a 2016b weisberg and liu 2017 aviso is a multimission gridded sea level anomaly data set produced by the ssalto duacs with support from the cnes http www aviso altimetry fr duacs and distributed by the copernicus marine and environment monitoring service cmems http www marine copernicus eu it is a global product with horizontal resolution of 1 4 and daily temporal resolution the delayed time data are used for the period between 1 january 1993 through 6 may 2016 and the near real time version are used for the remaining time 7 may 2016 to 31 december 2016 the anomaly fields are combined with the mean dynamic topography mdt cnes cls13 rio et al 2014 produced by cls space oceanography division and distributed by aviso with support from cnes surface geostrophic currents were then calculated as the gradient of the absolute sea level similar to that in the work of liu et al 2008 the geostrophic current data used in this work are limited to the gom region within latitude and longitude ranges of 22 8 31 8 north and 98 8 81 8 west respectively and with the omission of coastal ocean regions with water depth 100 m where conventional altimetry data are not as reliable as in the open oceans due to a number of factors explained by vignudelli et al 2011 see fig 2 the climate indices data were obtained from climate explorer website of the royal netherlands meteorological institute http climexp knmi nl as mentioned in section 2 based on previous studies of the lc variability in this paper we consider the following climate indices nao amo enso mei oni soi and pdo wind stress curl was calculated data from the national center for environmental prediction ncep north american regional reanalysis narr 2005 mesinger et al 2006 wind data narr database includes several meteorological variables from 1979 to present with a horizontal resolution of approximatively 32 km 45 vertical layers and 3 hourly frequency covering the whole north american region narr is a result of the ncep eta model with the regional data assimilation system rdas and significantly improves the accuracy of temperature winds and precipitation compared to the ncep doe global reanalysis 2 in this work we analyse the local effect of the wind in the gom and the upstream effect in the caribbean sea as shown by other authors from the point of view of the hydrodynamic flow two different sub areas of the caribbean sea can be identified the venezuela colombia basin and the cayman basin separated by a shallow ridge extending from honduras and nicaragua to hispaniola called the nicaraguan rise e g alvera azcárate et al 2009 following zeng et al 2015b with the aim of considering only the most direct effect of the upstream atmospheric situation the wind over the cayman basin is considered in this study hence for the gom region wind stress curl was calculated within latitude and longitude ranges of 22 8 31 8 north and 98 8 81 8 west respectively and for the caribbean sea within latitude and longitude ranges of 17 0 21 4 north and 89 0 78 3 west respectively in order to reduce the dimensionality of the data we apply principal component analysis pca to the data and keep the principal components pcs retaining the 95 of the total variance resulting in 68 and 41 pcs for the gom and the caribbean sea domains respectively sea level pressure data are provided by the ncep climate forecast system reanalysis cfsr saha et al 2010 in this study we use cfsr monthly mean data with 1 spatial resolution within latitude and longitude ranges of 0 65 north and 65 15 east respectively in this domain we calculated the monthly anomalies with respect to the average months similarly to that for the wind stress curl data we reduce the dimensionality of the data by keeping the first 10 pcs of the obtained anomaly series which account for 95 of the total variance 4 methods 4 1 logistic regression general framework logistic regression is a special case of generalized linear model glm glms are statistical models widely used to investigate effects of explanatory variables on a response or dependent variable in practice these models link a linear combination of a group of predictor variables to a function of the response variable logistic regression models are widely used when the response variable to be modelled is categorical i e it can take exclusively one from a set of possible values in the case of logistic regression the function of the response variable linked to the covariates is the logit function this function represents the natural logarithm of the odds of each possible outcome with respect to an arbitrary baseline category based on this definition if y ϵ 1 n is the set of n ocean current patterns and the last category j is the baseline a generic multinomial logistic regression model for the j th pattern can be expressed as follows 1 log p y t j x t p y t j x t α j β j x t j 1 n 1 where x t is the vector of covariates at time t β j refers to the vector of coefficients for each of the values of the covariates at time t and α j represents the intercept of the model the left hand side of eq 1 is the logit of a generic category of the response variable the right hand side is the linear combination of the predictors as mentioned in section 2 the influences of the lc variability have been studied in previous studies including the memory of the lc system possible relationship with some climate index phases and the response to atmospheric variations here we set up an alr model taking all these features into account see fig 1 the influence of the memory of the system is explored by adding to the model autoregressive terms as a markov chain of an order equal to the number of previous states to be considered the implementation in an alr model is achieved by using a contrast matrix such as the helmert matrix de vries et al 1998 a markov chain of order d can be implemented as follow 2 x t ar d i 1 d k 1 n 1 z k t i γ ik where z k t i is the dummy variable obtained through the helmert matrix transformation and represents the previous loop current pattern lcp at time t i γ ik is the coefficient associated with lcp k at time t i note that the autoregressive component adds a set of n 1 d new parameters for each category of the model the influence of climate indices or atmospheric conditions on the logit of the lcps can be included in the model as follows 3 x t c i 1 n c c i β i c where c i is the vector of the values of the n c climate indices considered or of the n c principal components of the atmospheric field considered at time t and β i c is the vector of associated coefficients eqs 2 and 3 show how to introduce the effect of the considered variables in a logistic regression model however the actual significance of these variables must be carefully assessed after defining the response variable of our model in section 4 3 a statistical based method is described to assess the significance of each of these components an outline of the methodology followed to obtain the lcps and to define the components of the alr model is presented in fig 1 4 2 response variable characterization the response variable in an alr model is categorical i e it is made up of different categories that the dependent variable can exclusively assume in our case the lc is the response variable and the categories are represented by a set of its typical spatial patterns lcps in order to select these representing lcps we follow a three step clustering methodology which aims to extract patterns that homogenously span the space of the original data considering the most significant temporal scale for the pattern selection the three phases of this clustering methodology are presented next see fig 1 4 2 1 principal component analysis in order to avoid spatially correlated variables that may disturb the lc data clustering the pca is applied to the altimetry derived daily data of geostrophic current components through this analysis original data are redefined with respect to new orthogonal dimensions or empirical orthogonal functions eof the eigenvectors of the covariance matrix of that dataset pcs are the time evolution of the weights of the eofs the obtained eofs and the associated pcs are ordered based on the variance of the data explained by each component the first 84 pcs accounting for 95 of the total variance are then considered as examples the first two and last two eofs and the associated pcs obtained are shown in fig 2 the percentage of the total variance accounted for by each component is shown in the individual eof plot as expected the first two eofs show patterns coherent with the lc system while the last modes represent smaller scale spatial variations and higher frequencies affecting quite homogeneously the whole basin 4 2 2 wavelet spectrum analysis as a second step in the clustering process a joint pc wavelet analysis similar to that in liu et al 2016b is performed over the selected leading pcs the aim of this step of the clustering methodology consisting in a rectified wavelet spectral analysis liu et al 2007a on the obtained pcs is to identify the temporal scale under which no major variability of the gom geostrophic currents occurs in fig 2 we show as examples the results of this analysis for the first two and last two considered pcs the rectified wavelet spectra for the first two pcs accounting for 15 9 and 12 3 of the total variance respectively show that variations of these gom current components are mainly in the seasonal to annual time scales 2 months to 1 year the variations of the last two pcs are generally faster being the most energetic between 14 days red horizontal line in the wavelet spectral analysis panels of fig 2 and 1 year we have performed this analysis for the total of the 84 considered pcs and found that in all cases the major variability for each pc occurs at a time scale greater than or equal to 14 days therefore in order to reduce the source of unexpected noise that could negatively affect the statistical model the 84 daily pcs of the lc are rescaled to bi weekly frequency by calculating the average values from the daily series please note that in this work the application of pca has a dual purpose first to allow the joint pc wavelet analysis with the aim of exploring the temporal scales of the gom geostrophic current variability and eventually choosing a consistent time scale for the pattern extraction and second to reduce the current data dimensionality and simplify the clustering process 4 2 3 k means as a last step of the pattern extraction the k means algorithm technique is applied to the bi weekly lc pcs the maximum dissimilarity algorithm was used to pre select initial centroids ensuring k means technique to correctly describe the diversity of the data camus et al 2011 antolínez et al 2018 several tests were performed in order to find an equilibrium between the spatial variability of the current and the number of groups to be considered as a result of the clustering 8 centroids are selected representing 8 lcps fig 3 these typical lc conditions extracted from the 24 year geostrophic current dataset will represent the 8 categories in which the alr model will classify the lc the order of the current patterns have been arranged following a similarity criteria but only for an easier interpretation of the analysis of the results from the perspective of the alr model the order of the categories of the response variable does not affect the results the patterns show a broad variability of the lc and cover well different situations between the maximum retraction lcp 1 and the deepest intrusion lcp 10 it is interesting to see how for lcp 1 lcp 2 and lcp 3 representing retraction or mid intrusion stages the lc path stays close to the southwestern part of the west florida shelf while the loop tends to curve westward as its path advances deeper in the gom lcp 4 lcp 5 and lcp 6 in accordance with the findings of weisberg and liu 2017 the obtained spatial lcps also agree well with the results of liu et al 2016b that extract 16 spatial pattern with a 4 4 som application for sea level and current data over the eastern part of the gom alvera azcárate et al 2009 describe extreme lc intrusion as an intrusion that extends up to 92 w or 27 n lcp 7 and lcp 8 show these situations highlighting that the used clustering algorithm spans the space of the original data enough to represent these extreme conditions once the characteristic patterns are obtained the original 24 year series of gom geostrophic current is expressed in a sequence of bmus in which each time step of the gom geostrophic current is expressed by the most similar lcp extracted the frequencies of occurrence of the lcps in the historical series is in order from lcp 1 to lcp 8 the following 6 5 12 6 13 6 14 1 15 7 14 5 11 8 and 11 2 4 3 model set up building an alr model is the hardest part of applying logistic regression although in some applications the inclusion of certain predictors can be easily guessed i e the wind effect in shallow water areas or tidal harmonic components in narrow channel flows in general it may be challenging to select the set of explanatory variables that actually influence the current of interest moreover it must be considered that even if a model with several well chosen features is likely to fit the data better as the number of feature increases possible effects and interactions among some covariates increase as well agresti 2007 generally it is preferable to keep the model as simple as possible for a better interpretation of the results statistical inference based algorithms can be used to build the model one particularly useful when studying ocean currents is the forward stepwise method it consists of starting from the simplest model possible i e considering just the constant value and then adding sequentially new explanatory variables by testing if they produce a significant improvement in the model fitting quality or not several significance tests can be used like wald s test or aikake s information criteria here following guanche et al 2014 we propose the use of the likelihood ratio statistic this method compares the deviance ratio δdev the change of fitting quality for two nested models and the chi square distribution with δdf δnp n 1 degree of freedom δnp is the number of parameters added in the more complex model assuming a confidence level α 95 when δdev x 0 95 δdf 2 the increasing of fitting quality of the model including the new covariates is statistically significant as mentioned earlier in section 4 1 the aim of this analysis is to assess the influence of autoregressive terms and inter annual variability on the lcp evolution the results of the forward stepwise method based on likelihood ratio statistic are shown in table 1 model 0 the null model only considers the intercept value nesting to model 0 model i includes the effect of an autoregressive term i e it checks if the manifestation of a current pattern is influenced by the previous current spatial situation or not the increment in deviance obtained by adding this effect δdevi 1781 7 x 95 8 2 allows to accept the hypothesis that the first autoregressive term is significant in the 24 year series of lcps the autocorrelation of the series is further explored by progressively adding a second third and fourth autoregressive term in model ii model iii and model iv respectively as shown in table 1 the second and third term are significant as well even if the improvement achieved in the model fit decreases as the order of the autoregressive terms increases based on the statistical test the effect of including a fourth autoregressive term is not significant being the improvement in the deviance between model iv and model iii δdeviv 25 7 x 95 29 2 this finding that three previous steps at a bi weekly interval are significant to determine the next lc status is in concordance with the results of the study by zeng et al 2015a in which the authors develop a model based on an autoregressive artificial neural network to predict the gom sea surface height and find that six steps at weekly scale is the optimal delay for their model once the autoregressive component of the model has been assessed we test if any of the atmospheric variables and climate indices investigated in the previous studies on the lc variability can improve the goodness of fit of the alr model since the lc patterns are calculated based on bi weekly averaged current data all the predictors are first resampled at the same time scale for a concordance among lcp series and predictor time series in model v we add to model iii the wind stress curl in the gom and obtain δdevv 652 6 x 95 90 2 hence the contribution of the new predictor is significant in the next model we include the effect of the wind stress curl in the caribbean sea which results not significant δdevvi 330 0 x 95 131 2 however considering a delay between the lcp series and the series of wind stress curl in the caribbean we find that the effect of the wind in the caribbean sea with a 6 week delay significantly improves the goodness of fit of the alr model as shown by model vii in table 1 in the same way spanning different time lag of the north atlantic mslpas with respect to the lcp series we find that model viii including north atlantic mslpas with a 4 week time lag significantly improves the model δdevviii 128 3 x 95 141 2 to further improve the capability of the model we consider the previously introduced climate indices each one with a possible time lag up to 1 year however none of these indices with none of the considered lags is found to be statistically significant at all not shown here as an example in table 1 we show model ix which includes oni with 6 week lag as a covariate following the test the increment in the deviance is not significant δdevix 9 8 x 95 142 2 based on these results we choose model viii as the optimal alr model for the statistical simulation of the lcp series hence considering as covariates three autoregressive terms the pcs of the wind stress curl in the gom the pcs of the wind stress curl in the caribbean sea with a 6 week lag and the pcs of the north atlantic mslpas based on eqs 1 2 and 3 the final equation for our alr model can be written as follows 5 p y t j y t 1 y t 2 y t 3 x t wg x t wc x t slpana exp α j i 1 3 k 1 7 z k t i γ ik i 1 68 c i wg β i c wg i 1 41 c i wc β i c wc i 1 10 c i slpana β i c slpana k 1 7 exp α j i 1 3 k 1 7 z k t i γ ik i 1 68 c i wg β i c wg i 1 41 c i wc β i c wc i 1 10 c i slpana β i c slpana where y t represent the lcp representing the lc state at the time t x t represents the conditions of a covariate at the time t c i specifically refers to the i th pc of the considered covariate superscript acronyms specify the covariate wind stress curl in the gom wg wind stress curl in the caribbean sea wc and mslpas over the north atlantic slpana the proposed model has a total of 141 coefficients one for the intercept 7 for each autoregressive component 68 corresponding to the number of pcs of the wind stress curl in the gom 41 for the pcs of the wind stress curl in the caribbean sea and 10 corresponding to the pcs of the north atlantic mslpas these coefficients are adjusted to maximize the fit of the model to the historical data several available statistical software allow for an easy implementation and fit of a logistic regression model in this work scikit learn library pedregosa et al 2011 of python is used we apply the one vs rest scheme l2 penalty option a tolerance default value of 1e 4 and the liblinear solver algorithm once the predictors have been selected and the corresponding coefficients have been adjusted we use the fitted model to simulate the sequence of lcps for the whole 24 year period considering the probabilistic nature of the statistical model we run the model 100 times and obtain the average values of the probabilities 5 results 5 1 model fit diagnosis in order to check the capabilities of the selected model and also to show the improvement brought by the different predictors we present in fig 4 the comparison among the historical series of lcp annual probability of occurrence panel a and the ones obtained by the simulations provided by model v in panel b model vii in panel c and by the finally selected model viii panel d in each bar of the figure the probabilities of occurrence of the 8 lcps are stacked for every year so the yearly sum of probabilities is one as can be seen the results of the selected model panel d compare favorably with the historical series of lcps panel a to check how the model capabilities improve by progressively including these covariates in panel b we show the results of model v considering the three autoregressive terms and only the wind stress curl in the gom the model is capable of reproducing some of the variability found in the historical series but in general a lot of inconsistencies exist with respect to the historical series model vii considering also the effect of the wind stress curl in the caribbean sea with a 6 week lag has greatly improved skills to reproduce the lcp series variability this is clear for example during the period 1998 2001 that the model v is unable to replicate and that is instead simulated quite favorably by the model vii clear improvements can be seen also for the periods 2004 2005 2009 2010 and for the year 2014 by also including the variability provided by the mslpas over the north atlantic with a 4 week lag model viii is able to further expand the capability of the statistical model to reconstruct the lcps original series as shown in panel d to explore the intra annual variability of the lcp evolution and to check for the skills of the selected alr model in fig 5 we show the comparison between historical left panel and modelled right panel annual average probability of occurrence of lcps at monthly scale the main features in the intra annual scale are replicated quite well by the model historical frequency of retraction stage patterns like lcp 1 and lcp 2 are higher in summer and autumn with the maximum retraction lcp 1 more likely between june and november this feature is well reflected by the model results both historical and modelled lcp probabilities show that weak intrusion stages lcp 3 and lcp 4 are common situations from autumn to early winter and in early spring with lcp 3 more likely in autumn and lcp 4 in early spring the historical frequency of occurrence of the intermediate stages lcp 5 and lcp 6 is higher in autumn and winter with a minimum in late summer again the model is shown to be capable of reproducing such variations as represented by both panels the deeper intrusions lcp 7 and lcp 8 have a bimodal behaviour they are more likely to occur in winter and in summer and less so in spring and autumn the periodicity of these lcp agrees with the frequencies found by other authors for eddy shedding events often associated with deep intrusion states chang and oey 2012 hall and leben 2016 the comparison with the seasonality of the lcps extracted by the historical data highlights the capability of the model in properly simulating the intra annual variability as well the previous analysis of the model performance suggests that the proposed arl model is capable of reproducing the probability of occurrence of the extracted lcps next in order to quantify this capability a series of indices generally used when assessing a statistical classifier are considered sensitivity precision f score and accuracy sensitivity or true positive rate tpr is the proportion of real positive cases that are correctly predicted as positive true positives precision or positive predictive value ppv is defined as the proportion of predicted positive cases correctly true positives or incorrectly false positives that are actually true positives f score can be interpreted as a weighted average of the precision and sensitivity accuracy acc is the percentage of real positive and real negative correctly predicted the values obtained for these parameters are shown in table 2 and confirm the general good capabilities of the model average values 0 75 at the same time these parameters allow for a detailed assessment of the skills of the model in reproducing each singular lcp the model performs better in simulating lcp 1 lcp 2 and lcp 3 corresponding with retraction and weak intrusion lc stages see f score for example in particular tpr values equal to 1 for lcp 1 and lcp 2 remark that the model is able to find all the samples from the historical series corresponding to these two patterns on the other side the model weakest performance occurs with lcp 7 however with the values of the parameters 0 5 in any case acc values are higher than 0 90 for all categories these results confirm that the proposed alr model is capable of simulating the lcp sequence particularly for the patterns of lc retraction phase these four examined parameters are obtained from the confusion matrix this matrix breaks down the overall model results compared to actual samples for each category so it is useful to analyse it in detail the confusion matrix for our case is shown in fig 6 left panel as absolute counts and right panel as percentage the confusion matrix relates the categories predicted by the models to the actual ones take lp3 third row for an example the matrix indicates that the model correctly predicts the pattern true positives 68 times but it misses the correct prediction 17 times false negatives assigning 7 times to lcp 1 once to lcp 2 and so on an interesting feature extractable from fig 6 is that when the model get confused i e it does not predict the true pattern it tends to pick up some other spatial pattern not very different to the actual one an exception to this is given by lcp 3 that when missed by the model is in most cases replaced by lcp 1 or lcp 8 actually quite different to lcp 3 see fig 2 5 2 exploratory prediction exercises although it is not the main goal of this paper we perform here a set of prediction exercises for a first exploration of the model skills in predicting future stages of the lc efforts have been made in the prediction of the lc system evolution using statistical data analyses lugo fernández and leben 2010 presented a linear model for the prediction of the time between two consecutive lc eddy shedding events based on the retreat latitude of the lc at the end of the first event zeng et al 2015a developed an autoregressive artificial neural network for lc evolution forecast that is demonstrated to be reliable up to 4 to 6 weeks ahead in order to test the forecasting skills of the proposed alr methodology we recalibrate the model using the historical lcp series from 1993 to 2015 and we simulate the year 2016 we execute two simulations of the alr model to predict 6 month ahead for each run in the first simulation we try to predict from january 2016 to june 2016 and in the other from july 2016 to december 2016 we then use the historical lcp series of 2016 to evaluate the prediction results similar to what described in section 5 1 we run the model 100 times for each test and the results refer to the average of the probabilities obtained with the 100 simulations first of all we describe the actual historical evolution of lcps for the year 2016 as obtained by the historical sequence of bmus see tables 3 and 4 the year begins with the lc in a mid intrusion phase lcp 5 this configuration persists until approximately mid march i e during 5 steps in the series then the situation starts to evolve represented by lcp 6 for the next two steps 4 weeks which still shows a mid intrusion phase but is characterized as a tightened bottom of the loop next starting from around mid april the lc experiences a strong retraction represented by lcp 1 for about 6 months until mid october during the rest of the year the lc is represented by lcp 2 corresponding to a first step toward a new retraction intrusion retraction cycle since the alr model considers the effect of three autoregressive terms the prediction model is fed besides the values of the predictors for the whole simulation period by the three lcps prior to the first time stamp of simulation for the first exercise corresponding to the prediction of the first 6 months of 2016 the three lcps prior to the first simulation step are lcp 6 lcp 4 and lcp 5 in this temporal sequence the results of the first prediction exercise are shown in table 3 it is encouraging that the model predicts well the persistence of lcp 5 during the first 5 steps of simulation 10 weeks with lcp 5 the most likely predicted pattern during this period during the next step the transition from lcp 5 to lcp 6 occurs the model is capable to predict both this change and the persistence of the new pattern as shown in table 3 starting from april the forecast of the model becomes less accurate instead of lcp 1 the model wrongly predicts two weeks of further intrusion as expressed by lcp 8 followed by 6 weeks of lcp 4 eventually switching to lcp 5 although missing the retraction from lcp 6 to lcp 1 between march and april it is interesting to see how the model still forecasts a retraction of the lc but from lcp 8 to lcp 4 however neither the intensity nor the timing of the retraction is correctly predicted for the second prediction exercise the three steps prior to the first simulated time are a sequence of three lcp 1 as shown in table 4 the model correctly predicts lcp 1 as the most likely pattern to occur the model predicts that this pattern would persist for the next 6 months actually the pattern only persisted from july to september and changed to lcp 2 starting from october even if the model misses to predict lcp 2 it must be observed that both lcp 1 and lcp 2 refers to retraction lc stages see fig 2 so the predicted evolution of the lc for the last 3 months of 2016 does not diverge excessively from the actual one both forecast exercises show that the model has been capable to predict the true pattern up to 3 months ahead however these are exploratory experiments a deeper investigation should be carried on to better assess its predictive skills based on a longer validation period and a more detailed analysis moreover due to the limited length of the available historical database of the currents in the gom especially in light of the complicated variability of the studied phenomenon we suggest that longer historical databases should be used for a proper prediction assessment study 6 conclusions in this paper we introduce the autoregressive logistic regression alr models as the core of a statistical framework for ocean current pattern simulation an alr model allows statistically simulating current patterns considering simultaneously the influence of different spatial and temporal scale factors predictors covariates in order to check the capability of these models in ocean current simulation we set up run and validate an alr model for the gulf of mexico loop current as a case study firstly we extract 8 representative patterns at bi weekly interval from a 24 year 1993 to 2016 altimetry derived database of the geostrophic current in the gulf of mexico using a combination of pca wavelet spectral analysis and k means techniques in order to define a model capable of reproducing the historical evolution of these patterns we investigate the capacity of different factors as predictors of the lc variability based on previous studies we consider the following parameters autoregressive terms of different order wind stress curl over the gom and the caribben sea the north atlantic mslpas and various climate indices with different time lag we find that autoregressive terms of order 1 2 and 3 i e the patterns occurring on the six previous weeks the wind stress curl over the gom with no lag and over the caribbean sea with 6 week lag and the north atlantic mslpas with 4 week lag are statistically significant for our alr model none of the climate indices is significant as predictors for the lcp sequence the final model combines 141 coefficients one for the intercept 7 for each autoregressive component 68 for the pcs of the wind stress curl in the gom 41 for the pcs of the wind stress curl in the caribbean sea and 10 corresponding to the pcs of the north atlantic mslpas these coefficients are optimized to achieve the best fit between the model and the historical pattern series in order to check for the goodness of fit of the model we compare the simulated and historical loop current pattern series at both intra annual and inter annual scales despites the strong variability of the selected current we find that the model is capable of reproducing its behaviour at both scales a point by point comparison between actual and simulated pattern series is analysed through the confusion matrix and four derived indexes sensitivity precision f score and accuracy we find that the overall performance of the model is good average values of the three coefficients 0 75 and that it is particularly efficient in simulating the patterns representative of retraction stages lcp1 and lcp2 we also find that in the fitting process when the model does not select the true pattern it tends to pick up some other spatial pattern close to the actual one we also perform two forecast exercises for a preliminary assessment of the predictive skills of the model and found that in both cases the model has been capable of correctly predicting the actual lcp during the first 3 months of forecast for the rest of the simulated period although the predicted patterns do not match the historical lcps the model looks to some extent capable of predicting the general future behaviour of the lc even if the model forecast capabilities look promising at least a few months ahead a more complete study is required to better analyse this interesting application of the alr model the results of this study show the effectiveness of the proposed statistical framework in analysing the evolution of ocean current patterns this methodology could be implemented in practical applications like for example to emulate statistically sound oceanographic scenarios through climate based monte carlo simulations and even for probabilistic mid term forecasts of ocean current patterns acknowledgments the authors acknowledge the support of the spanish ministry of economy industry and competitiveness mineco under plvma3d tra2014 59570 r and oilhazard3d tra2017 89164 r national research projects j a a antolínez was funded by the spanish ministry of education culture and sport mecd fpu formación del profesorado universitario studentship boe a 2013 12235 y liu and r h weisberg are partially supported by the gulf of mexico research initiative gomri grant g 231804 the authors would like to thank prof fernando j mendez from the university of cantabria for fruitful discussions about logistic regression and clustering techniques and three anonymous reviewers for key suggestions 
24043,three dimensional numerical simulations of a tidally dominated estuary within the gulf of maine are performed using the regional ocean modeling system roms and validated with observations of sea surface elevation and velocity time series obtained between 1975 and 2016 the model is forced at the ocean boundary with tidal constituents m2 s2 n2 o1 k1 a time series of observed subtidal elevations and discharge from seven rivers that drain into the estuary harmonic analysis is used to determine the tidal dissipation characteristics and generation of overtides within the system amplitude decay and phase shift of the dominant semidiurnal m2 tidal component shows good agreement with observations throughout the main channel of the piscataqua river and over the channels and mudflats of the great bay the model simulates harmonic growth of the overtides across the spectrum and indicates a spatial evolution of the tide consistent with a shoaling wave that evolves from a skewed elevation profile with ebb dominance in the lower parts of the estuary to a more asymmetric pitched forward shape consistent with flood dominance the m4 constituent has spatial variation qualitatively similar to the observations but has magnitudes that are under predicted in the complex bathymetric region of the piscataqua river where much of the m2 tidal dissipation occurs the m6 tidal constituent agrees well with the observations throughout the estuary suggesting that frictional effects on harmonic growth are well modeled root mean square model data differences in velocities 0 05 m s and sea surface elevation 0 1 m agree to within about 10 of the tidal amplitudes differences between model simulations with and without subtidal oscillations in the estuary are small suggesting that interactions between the tide and other low frequency subtidal mean flows are weak and can be ignored when considering tidal dynamics including average fresh water discharge in the model does not affect the behavior of the tidal flows but can generate high frequency baroclinic velocities potentially important to mixing within the estuary keywords numerical modeling tidal dissipation nonlinear tidal evolution gulf of maine hydrodynamic model validation 1 introduction the transport and mixing of water sediment nutrients and organisms in estuarine and coastal systems is often dominated by astronomical tidal forcing of particular interest are the dynamics of shoaling tides induced by nonlinear wave interactions and energy dissipation and how that process impacts long term coastal planning and environmental conservation efforts as the tide propagates from the open ocean onto the shelf and into estuaries it becomes progressively more nonlinear and distorted leading to growth shoaling or decay dissipation of tidal amplitudes shifts in the phase of the tide and growth of tidal harmonics resulting tidal currents are difficult to predict analytically over realistic and complex bathymetry and require observation or numerical simulation to quantify evolution of tidal nonlinearities produces asymmetries in ebb flood current strength and duration boon and byrne 1981 that when averaged over a tidal cycle has been used to estimate net sediment transport and circulation patterns dronkers 1986 stronger flood currents drive the movement of coarse sediment and longer slack periods promote the deposition of fine grained sediment tidal amplitude attenuation in an estuary occurs from energy losses due to turbulent mixing and from frictional affects due to interactions with the bottom and lateral boundaries of the estuary energy dissipation of the tidal wave can be described in terms of amplitude decay of the dominant tidal constituent which for the gulf of maine is the semi diurnal m2 tide that contributes about 90 of the predicted tidal variance not all energy is dissipated due to frictional effects and some is transferred to higher harmonics overtides e g the m4 and m6 tidal constituents through nonlinear interactions and frictional effects that create tidal asymmetry aubrey and speer 1985 speer and aubrey 1985 parker 1991 a comparison of the magnitude of the m2 constituent with the first harmonic m4 is a direct measure of nonlinear interactions of the m2 tide whereas the phase difference qualitatively describes the tidal asymmetries in the system friedrichs and aubrey 1988 generation of the m6 component is largely attributed to frictional affects parker 1991 the dissipation problem is complicated by the highly nonlinear nature of tidal shoaling and propagation and the need to define representative bottom boundary conditions that characterize the interactions between tidal currents and the seabed dissipation in inlets and estuaries leads to development of local phase lags between pressure and velocities that shift slack tide periods up to a quarter of the wave period 90 deg phase shifts between sea surface elevation and along channel velocity and also impacts the evolution of tidal harmonics that are amplified and phase shifted relative to open ocean values this behavior can affect the overall transport in the estuary thus a good understanding of the spatial and temporal patterns in tidal dissipation can aid in long term coastal management and planning for example site selection for tidal renewable energy projects neill et al 2014 the tides may also interact nonlinearly with river flow storm surges and wind driven currents that vary on time scales of hours to months often observations from only a few locations are used to describe the overall dynamics of an estuary and field experiments are limited to one specific area for a discrete amount of time it is often not feasible to collect enough measurements continuously everywhere to adequately characterize the tides and associated flows thus numerical models can be used to produce system wide predictions of water levels and currents under different hydrodynamic and meteorological forcing conditions e g warner et al 2005a quantitative prediction of tidal amplitudes and currents is needed for flooding and inundation studies mooring and berthing design safe navigation interaction with structures and bottom shear stress prediction for sediment transport organism transport and nutrient fluxes in this study we discuss the implementation and validation of a three dimensional high resolution hydrodynamic model of a tidally dominated well mixed estuary located within the gulf of maine the gulf of maine has a natural resonance close to the semidiurnal m2 tidal constituent garrett 1972 enhancing the tides throughout the gulf including connected estuaries and coastal embayments including the bay of fundy in this study we examine the piscataqua river great bay estuary located within the gulf of maine at the border of new hampshire and maine fig 1 tidal forcing for the great bay is dominated by the semidiurnal m2 component of the tide has a tide range on the order of 2 4 m depending on the spring neap cycle and has variable but mostly minor freshwater river discharge it is home to both the second deepest u s naval port and portsmouth harbor which is home to some of the fastest tidal currents of any commercial port on the u s east coast the estuary has two tidal regimes a high dissipative region through the lower piscataqua river from the mouth to dover point and a low dissipative regime from dover pt through the little bay and great bay brown and trask 1980 swift and brown 1983 the former region behaves like a partially progressive wave with concomitant phase shift of the slack tidal period whereas the latter has phase shifts consistent with standing waves this behavior causes changes in the timing of tidal currents and the associated net sediment transport throughout the estuary previous modeling studies of the great bay ip et al 1998 erturk et al 2002 mclaughlin et al 2003 considered depth integrated two dimensional flow fields with the primary focus of representing the gross tidal behavior to estimate the net transport of water and sediment in the estuary the model validation process includes examination of the nonlinear tidal behavior that drives tidal asymmetry and tidal energy dissipation in terms of amplitude decay and phase lags using water level measurements and harmonic analysis modeled results are compared with coincident and previous observations and with results from the literature this study will form the basis for additional modeling aimed at examining the spatial variation in bottom shear stresses needed for sediment transport calculations horizontal and vertical mixing within the estuary and transport of larvae nutrients and carbon within the estuary section 2 describes the field site observational datasets the hydrodynamic model and grid development and the model validation and tidal analysis methodology section 3 describes model results and section 4 discusses the model observation comparison in terms of nonlinear evolution section 5 presents the conclusions of the study 2 methods 2 1 site description the great bay estuarine system is located along the new hampshire maine border within the gulf of maine in the northeastern portion of the united states fig 1 it is a recessed drowned river valley connected to the gulf of maine via the piscataqua river armstrong et al 1976 the tide range is 2 4 m over the spring neap cycle with tidal currents greater than 2 m s in the channels at maximum ebb and flood tides at low tide as much as 50 of the great bay is exposed as low lying mudflats cut with deep tidal channels the surface area of the estuary is approximately 55 km 2 measured at mean high water nhdes 2007 the volume is 156 106 m 3 and 235 106 m 3 for low and high tides respectively with a tidal prism of 79 106 m 3 swift and brown 1983 nhdes 2007 seven tributaries contribute fresh water to the system the squamscott lamprey winnicut oyster bellamy cocheco and salmon falls all feeding the upper and lower piscataqua river that flows into the gulf of maine river fluxes are determined by precipitation and runoff and regulated by dams or weirs that modulate the freshwater volume entering the system typically except during large storms or the spring melt the freshwater input is relatively small and only contributes 2 of the tidal prism short 1992 nhdes 2007 the generally small freshwater fluxes and strong tidal mixing results in weak or negligible stratification except very close to the river mouths and during periods of little rainfall the salinities at the great bay buoy fig 3 are nearly equal to the gulf of maine indicating that horizontal variation in density due to river fluxes are also weak as our interests include the ability of the numerical model to represent the vertically varying flow fields we will include model runs with and without average river discharges to evaluate the influence of baroclinic flows on the tidal behavior ocean waves outside the mouth of the estuary are strongly refracted away from the deep center channel and rapidly attenuate upstream and thus do not greatly contribute to the velocities or water level fluctuations in the estuary other studies have shown that waves can have an impact on tidal currents e g lewis et al 2014 wind driven surface gravity waves in the large lobe of the great bay proper are generally small 5 20 cm significant heights owing to the limited fetch and strong attenuation by energy loss through interactions with tidal currents and the muddy bottom or shallow aquatic vegetation eel grass meadows although waves on the great bay could be important to bottom shear stresses over the mud flats they do not substantially alter the larger scale circulation and thus are not considered further in this study wind driven mean currents may be substantial during storm conditions but are generally much weaker than the tidal currents wengrove et al 2015 and thus are also not considered in this study the bathymetry of the estuary is complex fig 2 with steep sidewalls in the main channel of the piscataqua river with water depths ranging 13 26 m ocean water flows into mouth of the piscataqua river through two channels a main entry point to the north of new castle island between new hampshire and maine and a secondary entry point through little harbor to the south of new castle tides entering little harbor flow through relatively shallow water and around several islands and join the piscataqua river between pierce island and portsmouth nh flows through the main channel make a sharp 90 deg turn around new castle at fort point and then flow around the portsmouth naval shipyard primarily to the south in the deeper channel but also the back bay a narrow shallow waterway that reconnects with the piscataqua river near pierce island the piscataqua river splits at dover pt with the main flows sharply turning south into little bay and with a smaller portion of the flow heading to the north connecting the lower piscataqua river with the upper piscataqua fed by the cocheco and salmon falls rivers to the north with average summer discharge rates of 8 54 and 15 4 m 3 s respectively nhdes 2007 the channel between the mouth at new castle island and dover pt is 12 km long and characterized by a hard rocky bottom with coarse sediment in the deep channels and steep rocky shorelines for most of the reach the flows through this part of the estuary are high exceeding 2 m s in some locations on both the flood and ebb tides once the flow enters the little bay it remains strong through the deep center channels with weaker flows up and over the bordering mud flats the oyster and bellamy rivers that flow into the little bay have average summer discharges of 0 94 and 1 32 m 3 s respectively nhdes 2007 the little bay joins the great bay at furber strait near adam s pt the deep center channel gradually shallows and bifurcates into an eastern and western branch flanked by large mud flats that dominate this portion of the estuary the squamscott lamprey and winnicut rivers all flow into this part of the estuary with average summer discharge rates of 5 3 10 0 and 0 7 m 3 s respectively nhdes 2007 for this study the tidal analysis focuses on the main channel flows from the mouth of the piscataqua river to the upper reaches squamscott river of the great bay estuary fig 1 2 2 observations field observations of horizontal currents spanning the water column and sea surface elevation from bottom pressure and tidal stations were obtained during several field experiments in 1975 2007 2009 2015 and 2016 and the continuously operating noaa tide gauge station at fort point nh station id 8423898 table 1 summarizes the dates and durations of the field studies and fig 3 shows the instrument locations observations of tidal elevations and currents within the estuary were obtained in 1975 by the university of new hampshire unh in cooperation with the national ocean survey nos summarized in swenson et al 1977 and silver and brown 1979 original data were unavailable so tidal analysis estimating m2 tidal amplitudes and phases from swift and brown 1983 is used in this study observations of bi directional currents in 0 5 1 0 m range bins and water levels from the mouth to adams pt were obtained by noaa in 2007 using six bottom mounted upward looking acoustic doppler current profilers adcps the instruments were deployed for between 41 and 45 days recovered and then moved to new locations with water depths ranging between 4 3 and 19 3 m these data are available and described online at https tidesandcurrents noaa gov observations of water levels were obtained by unh in 2009 at four locations in the great bay using bottom mounted pressure sensors and an rtk gps buoy the instruments were sampled between 30 and 120 s and deployed between 9 and 84 days and averaged over 6 min intervals following standard noaa procedures observations obtained for 7 71 days by unh in 2015 and 2016 include 1 min averaged bi directional currents in 0 25 1 0 m range bins and water levels from six adcps deployed across the great bay in water depths ranging 3 17 m bottom pressure was converted to sea surface elevation using observed bottom temperature at the instrument location and salinity obtained from the great bay coastal buoy located in the center of the great bay estuary http www opal sr unh edu data buoys great bay index shtml 2 3 hydrodynamic model the regional ocean modeling system roms haidvogel et al 2008 shchepetkin and mcwilliams 2005 is an ortho curvilinear three dimensional numerical coastal ocean circulation model that solves finite difference approximations of the reynolds averaged navier stokes rans equations using the hydrostatic and boussinesq assumptions the objectives of this study focus on the hydrodynamic component to determine the tidal dynamics which are of first order concern in validating the numerical model roms has been used in both regional e g zhang et al 2009 yang et al 2016 and estuarine modeling studies e g warner et al 2005a moriarty et al 2014 and implemented into other coupled modeling systems e g warner et al 2008 warner et al 2010 a third order upwind advection scheme is used to solve for horizontal advection a centered fourth order advection scheme is used to solve for vertical advection a k ε generic length scale gls turbulence closure model is used to calculate the horizontal and vertical eddy viscosities umlauf and burchard 2003 warner et al 2005b in conjunction with the kantha and clayson 1994 stability function within roms the wetting and drying algorithm warner et al 2013 is utilized to simulate the inundation and draining of the tide over shallow areas alternatively covered and uncovered by the tide in which the critical depth d crit is set to 10 cm once the total water depth is less than d crit no flux is allowed out of that cell and it is considered dry finally barotropic and baroclinic modes are solved separately in roms with the mode splitting algorithms described in haidvogel et al 2008 barotropic time steps in model simulations herein are 1 20 of the baroclinic time step 2 3 1 model grid the model domain is defined by a rectilinear arakawa c grid with a constant 30 by 30 m horizontal resolution fig 4 downsized by a factor of 33 1 3 in the figure there are 8 vertical layers in a terrain following σ coordinate system that is adjusted for slightly higher resolution near the surface and bottom boundaries the domain is rotated 37 deg ccw from true north to align the offshore boundary with the approximate orientation of the shoreline along the new hampshire maine coast the domain ranges 22 02 by 25 02 km 734 by 834 cells the grid elevations were defined using bathymetric data obtained from the center for coastal and ocean mapping ccom http ccom unh edu and lidar data collected by usgs noaa and usace https coast noaa gov dataviewer and interpolated onto the center of the horizontal grid cells a hierarchy was defined that weighted the most accurate recent and complete topographic and bathymetric data highest with any gaps filled with more uncertain older or less complete data sources the combined elevation grid fig 4 was then processed with the matlab easygrid routine https www myroms org wiki easygrid to create the rectilinear grid and corresponding land mask that was subsequently input into roms during model testing the grid was smoothed in locations sensitive to numerical instabilities using interpolation methods described in plant et al 2002 2 3 2 boundary conditions at the open ocean boundary south edge of the rotated domain fig 4 the model is forced by tidal and subtidal oscillations see section 2 3 3 using the implicit chapman free surface and flather depth averaged velocity boundary conditions the chapman flather conditions employ the radiation method at the boundary assuming all outgoing signals leave at the shallow water wave speed flather 1976 chapman 1985 these particular boundary conditions have been shown to be the most suitable for tidal forcing palma and matano 1998 2000 marchesiello et al 2001 carter and merrifield 2007 three dimensional baroclinic momentum equations were set to radiation and gradient conditions for velocities and tracers the eastern northern and western edges of the domain are closed the bottom boundary condition for momentum was parameterized by a simple drag coefficient assuming a logarithmic vertical velocity profile in the bottom vertical cell the drag coefficient c d is represented by 1 c d κ ln z z ob 2 where z is the vertical elevation of the mid point of the bottom cell z ob is a characteristic bottom roughness in m and κ 0 41 is the von karman coefficient kundu 1990 a range of bottom roughness values from 0 015 to 0 030 m were tested and the best fit was determined iteratively from model data comparisons of m2 tidal dissipation as a function of distance from the estuary mouth see fig 5 within each run z ob was assumed to be spatially uniform across the domain the kinematic bottom stress boundary conditions are given by 2 τ b x ρ 0 c d u u 2 v 2 3 τ b y ρ 0 c d u u 2 v 2 where τ b x and τ b y are the bottom stresses in the x and y directions respectively 2 3 3 model initialization and forcing forcing conditions at the open ocean boundary are specified in two ways the first is with an analytical representation of tidal elevations and velocities considering only the principal semidiurnal m2 n2 s2 and diurnal o1 k1 tidal constituents determined by the oregon state university global tidal prediction software package otps in conjunction with the united states east coast regional tidal solution ec2010 egbert and erofeeva 2002 the otps provided the necessary tidal amplitude and phases that correspond to the observational datasets for the 2015 field study used in the model data comparisons of velocities see section 2 4 2 and fig 6 the amplitudes and phases compared favorably with a harmonic analysis of observed water level fluctuations at fort pt for the 2015 field experiment using t tide pawlowicz et al 2002 the second forcing consists of the analytical representation of the tides and including subtidal oscillations associated with atmospheric motions obtained from low pass filtered with a 33 hr cut off period observed time series of 6 minute averaged water levels at the fort pt tidal station the subtidal motions can have amplitudes in the gulf of maine of 0 10 0 30 m brown and irish 1992 change the water depth over the shallow mudflats considerably and although the time scales of the oscillations are generally much longer than the dominant semidiurnal tides may contribute to the overall water velocities on the flood and ebb coastal ocean currents associated with barometric wind driven or other shelf motions at the offshore open boundary are assumed small consistent with observations of currents from 2007 at the most seaward instrument location pir0701 and not considered herein in each case tidal with or without subtidal forcing time series of water level fluctuations are ramped hyperbolically from rest over a 2 day period although tidal currents are included at the open boundary test simulations in which the boundary currents were set to zero and allowed to evolve with the sea surface fluctuations did not alter the results suggesting that approximating the forcing by only the pressure gradient at the mouth is reasonable consistent with geyer and maccready 2014 time series of at least 32 days are used to force the model so that tidal analysis with t tide produces amplitudes and phases of the dominant tidal constituents with confidence intervals the open ocean boundary is located about 7 5 km from the mouth of the estuary where the fort pt tide station is located the time for the tide wave to propagate this distance is small about 7 3 8 1 min based on an average water depth of 30 24 m and thus has small effect on the phase estimates about 3 3 3 9 deg when comparing to coincident observations within the estuary three dimensional simulations were performed both with and without freshwater flows based on the average summer river discharge see section 2 1 salinity varying between 6 93 and 23 54 psu and water temperature varying between 19 5 and 25 4 deg c for the various rivers for the summer of 2015 was provided by the new hampshire department of environmental services https www des nh gov organization divisions water wmb vrap data htm ocean water temperature 17 deg c and salinity 31 5 psu was assumed constant and given by typical summer values for the gulf of maine diurnal surface heating and cooling were assumed small in comparison to the tidal mixing and were ignored although the precise values of the fluctuating river discharge temperature and salinity were not used in the model the variations in temperature and salinity predicted by the model compare favorably with 2015 observations obtained in the middle of the great bay near the surface with the great bay coastal buoy http www opal sr unh edu data buoys great bay index shtml and near the bottom with the seabird instruments co located with our adcp s deployed in 2015 modeled and observed fluctuations in temperature and salinity follow tidal cycles and reveal weak vertical gradients in temperature about 1 2 deg c and salinity about 1 2 psu consistent with a well mixed great bay environment away from the river mouths during typical summer conditions in new hampshire model simulations including subtidal oscillations and river fluxes had a very weak effect on the tidal behavior and thus the results presented below will focus on the model simulations for barotropic tides this is not unexpected for the typical summer conditions examined herein but might be an important consideration during extreme storms and high runoff periods or in the very shallow depths near the water s edge over the mudflats the effect of subtidal oscillations and baroclinic flows is discussed further in section 4 in the following tidal analysis from model simulations will be compared with observations within the estuary obtained in different experiments at different time periods tidal constituents are assumed to be the same throughout our model runs focus on the 35 day period spanning the 2015 field experiment and will be compared with observed velocity and sea surface elevation time series from 2015 and tidal analysis of observations obtained during all experimental periods table 1 time resolution was determined by iteration on grid smoothing and reducing barotropic and or baroclinic time steps until numerical stability was achieved for model simulations presented herein a baroclinic time step of 1 5 s was used with barotropic time step 1 20 of that value computations were performed on a cray xe6m 200 supercomputer at the institute of earth ocean and space at the university of new hampshire and the blue waters cray xe6 supercomputer located at the university of illinois urbana champaign output over the whole domain was stored to disk at 30 min average model time intervals and for 15 min averaged intervals at specific save points corresponding to instrument locations and along a densely sampled line every 100 m along the main transect passing through the entire estuary 2 4 model validation methods model validation is accomplished in four ways the first is by conducting a tidal analysis and comparing the modeled energy decay and phase shift of the dominant m2 tidal constituent throughout the estuary with similar analysis of observations of sea surface elevation time series the second is by comparing modeled time series of the vertical variation in currents with observations the third is with cross spectral analysis between modeled and observed sea surface elevation and horizontal velocity components at single locations and with the evolution of cross spectral phase at the m2 frequency between sea surface elevation and along channel velocities the fourth is by comparing the growth and phase change of m4 and m6 tidal harmonic constituents between modeled and observed time series and by comparison of the along estuary evolution of sea surface elevation skewness and asymmetry 2 4 1 tidal dissipation and phase change as the tide propagates into shallow coastal regions and interacts with bottom topography and basin geometry it loses energy through frictional processes that result in tidal amplitude decay and phase changes relative to the open ocean value due to phasing of the tide a direct time series comparison is only possible for model runs that coincide with the specific phases of the tide during that particular field study however tidal analysis of long 30 day time series of sea surface elevation obtained at other times can be compared with non synchronous model simulations provided there are no other atmospheric effects that nonlinearly interact with the tide and do not substantially change the tidal behavior therefore we conduct a tidal analysis using t tide pawlowicz et al 2002 to decompose each time series of sea surface elevation into tidal components and compare the modeled and observed tidal constituent energy from the linear gravity wave relation 4 e 1 2 ρg a 2 where e is the total energy per unit surface area a is the amplitude of the tidal constituent and the density ρ is assumed constant throughout the estuary in this study the semidiurnal m2 tide dominates contributing about 88 of the total tidal energy at the mouth of the estuary the energy at any location within the estuary e station is normalized by the value at the estuary mouth e ocean to represent the fractional energy loss e norm as the tide progresses upstream 5 e norm a station a ocean 2 assuming the uncertainties in the tidal amplitudes δa station and δa ocean are both independent and random then the error δe norm is calculated following taylor 1982 6 δe norm e norm 2 δa station a station 2 2 δa ocean a ocean 2 initial model calibration involves testing different bottom boundary conditions and iterating to estimate the energy decay as a function of distance from the estuary mouth that best fits the observations 2 4 2 time series comparison of vertically varying currents modeled currents are computed at defined σ coordinate levels that range from σ 1 at the bottom to σ 0 at the surface the total water depth in the model is given by the elevation of the seabed relative to the model datum defined plus the corresponding fluctuating sea surface elevation the observations on the other hand are obtained from fixed upward looking adcp s with vertical bin elevations defined in a fixed coordinate system relative to the bottom the range over which the currents are observed depends on the instrument characteristics e g acoustic frequency and instrument capabilities and the height of the fluctuating sea surface relative to the bottom acoustic interference by side lobes at the surface limit the range of useable vertical bins to be less than 94 of the total instantaneous water depth and appropriate filtering methods must be employed to eliminate spurious velocities near the surface as a consequence the velocities observed with adcp s in the field further from the bottom have bins coming into and out of the water column as the tide rises and falls to compare the modeled to observed currents the modeled currents in σ coordinates are transformed to the observational coordinate system by linear interpolation over the instantaneous water level at each time step in this manner the modeled time series at the transformed upper bins also come into and out of the water surface similar to the observations care must also be taken to represent the velocities from the observations at the center of the vertical bins and the model at the defined location by the σ coordinates a representative example of the time series comparison is shown later fig 6 and described in section 3 2 2 4 3 cross spectral analysis a more complete evaluation that includes the overall behavior of the modeled velocities can better be done with cross spectral analysis that shows the energy density levels for both the model and the data as a function of frequency and the coherence and phase relationship for each frequency as our interests lie with the tidal constituents the frequency resolution of the spectra will necessarily need to be fine enough to resolve the major constituents with lowest tidal constituent the o1 diurnal variation of about 0 0417 h 1 at the same time the confidence intervals on the spectra coherence and phase must be high enough to make reasonable comparisons for the 30 day time series examined cross spectra were computed with 10 degrees of freedom dof by averaging 5 adjacent frequency bands the frequency bandwidth of the smoothed spectral estimates was 0 0069 h 1 with lowest resolved frequency of 0 0035 h 1 the 95 confidence intervals are computed for the spectral amplitudes coherences and phase only those phase estimates for frequencies with coherence greater than the 95 critical value 0 52 for 10 dof are shown phase error bars for incoherent frequencies are meaningless bendat and piersol 2000 to reduce leakage effects a hanning data window is applied to each mean corrected time series before computing the spectra 2 4 4 sea surface elevation and along channel velocity phase difference tidal analysis of the sea surface elevation and velocities can be compared to show the relative change in phase as the tide evolves up the estuary in this case the observed and modeled bi directional velocities were rotated to align with the along channel direction using standard rotary analysis gonella 1972 ellipse orientations for the dominant m2 tidal frequency define the angle of the major axis of the rotary ellipse that is used in the rotation to along channel direction we conduct a tidal analysis to decompose each time series of the along channel velocity into amplitudes and phases for each harmonic tidal constituent frequency following the same procedure for the sea surface elevation see section 2 4 1 the phase difference between the sea surface height p and along channel velocity u at the m2 tidal frequency was computed for time series at locations that span the estuary and reported as the p u phase the evolution of the p u phase for the dominant m2 tidal constituent indicates the nature of the tidal motion throughout the estuary fig 10 top panel in a progressive wave the maximum currents occur at the same time as the maximum height of the wave and the currents and amplitude are in phase in a standing wave the maximum currents occur at mid tide half way between the crest and the trough of the wave and the along channel currents are 90 deg out of phase with the sea surface height 2 4 5 tidal harmonic growth and phase difference the growth of the m4 harmonic relative to the m2 constituent is a measure of the asymmetry and non linear distortion of the tide friedrichs and aubrey 1988 following speer and aubrey 1985 the amplitude ratio a ratio and the phase difference θ diff is defined as 7 a ratio a m 4 a m 2 8 θ diff 2 θ m 2 θ m 4 where am4 and am2 are the amplitudes of the m4 and m2 tidal constituents respectively and θ m4 and θ m2 represent corresponding phase relationships in general stronger frictional effects produce larger a ratio and the corresponding θ diff describes the gross behavior of the tides with phase differences between 0 and 180 180 and 360 indicating flood ebb dominance friedrichs and aubrey 1988 flood dominant systems have characteristically stronger flood currents and longer falling than rising tides whereas ebb dominant systems have stronger ebb currents and longer rising tides the amplitudes and phases of the m2 and m4 tidal constituents are estimated with a tidal harmonic analysis using t tide that fits harmonics to the time series and computes error bars on the estimates of amplitude and phases for each constituent allowing estimates of the uncertainty in a ratio and θ diff taylor 1982 the error estimates for δa ratio and δθ diff are calculated using the following formulations 9 δa ratio δa ratio δa m 4 a m 4 2 δa m 2 a m 2 2 10 δ θ diff δ θ m 2 2 δ θ m 4 2 following taylor 1982 similar to δe norm eq 6 the third moments skewness and asymmetry of observed and modeled sea surface elevation time series are computed along the estuary following elgar and guza 1985 the normalized by the variance to the 3 2 power skewness describes the general nonlinear deviation of the wave profile from a sinusoidal shape to a peaked up waveform symmetrical about the vertical axis through the wave crest the normalized asymmetry describes the asymmetry about the vertical axis and can indicate a pitched forward or pitched backwards wave form the nature of the skewness and asymmetry is determined by the phase relationship between the primary frequency and the coupled harmonics for purely skewed peaked up stokes like wave profiles the asymmetry is zero and the primary and higher harmonics are in phase for pitched forward backward the asymmetry is nonzero and negative positive sawtooth profiles have high negative asymmetries and phase relationships between the primary and first harmonic up to 90 deg evaluation of waveforms for wind driven surface gravity waves in the ocean and their relationship to third moments can be found in elgar and guza 1985 3 results model observation comparison results comparing model simulations for barotropic tides with observations are presented here and follow the methodologies discussed previously station data are retained from the model simulations at all the sensor locations as well as from locations separated by 100 m along a transect down the main channel extending from the first sensor location outside the mouth of the estuary to the upper reaches of the great bay by the squamscott river 3 1 tidal dissipation and phase change the observed energy decay and phase change of the m2 tidal constituent relative to the value at the most seaward location along the station transect through the estuary is shown in fig 5 the most seaward observation 1 km from the ft point tidal station closely matches the predicted tidal amplitude from the otps model and used to normalize the energy e norm eq 5 also shown is the variation in the center channel water depth along the transect error bars eqs 5 6 on the energy and phase estimates are based on the t tide analysis observations show an increase in tidal energy near the mouth and then a progressive decrease in energy through the energetic narrow portion of the lower piscataqua river this decay is strong and somewhat variable between portsmouth and dover pt and in general agreement with estimates of dissipation found by swift and brown 1983 by the time the tides reach the little bay entrance 45 of the m2 tidal energy has been lost over this same reach the m2 phase has changed 50 deg significantly larger than for a simple progressive tidal wave propagating upstream with estimate of about 6 deg phase change based on shallow water wave phase speeds and average water depth of 20 m and much less than a standing wave with 90 deg phase change interestingly the tidal amplitudes increase slightly between the entrance to the little bay dover pt and the upper reaches of the great bay squamscott bridge indicating some amplification as the tide propagates into progressively shallower water additionally the phase continues to evolve approaching 70 deg suggesting that the tide here is more reflective it should be noted that the tidal extent during the flood does not end at the squamscott bridge but continues an additional 8 km inland as well as up the other rivers fig 1 also shown in fig 5 are model predictions of the m2 tidal decay and phase change for a range of apparent bottom roughness z ob from 0 015 to 0 030 m the best fit to the observation is for z ob 0 02 m the model increase in m2 energy across the shallowing great bay bathymetry is in general agreement with the observations in general the model well predicts the evolution of the tidal phase throughout the estuary 3 2 time series comparison of vertically varying currents comparisons of modeled and observed current time series for 4 days from a single location in water depth of about 5 75 m in the great bay is shown in fig 6 both the east west and north south velocity comparisons are shown for elevations relative to msl near the bottom 4 13 m mid water column 2 63 m and near the surface 1 13 m in general the modeled velocities closely follow the observations including in the upper water column were the sensor bins are coming into and out of the water as the tide rises and falls root mean square rms errors between modeled and observed time series at all elevations above the bottom range 0 035 0 049 m s and 0 047 0 055 m s for the east west and north south velocity components and 0 095 m for sea surface elevation each about 10 of the amplitude at that location in general the 10 rms error between model data time series for all sensors across the great bay from the 2015 deployment is quite good with average rms errors for sea surface elevations east west and north south velocities of 0 096 m 0 054 m s and 0 060 m s respectively 3 3 cross spectral analysis cross spectra between modeled and observed sea surface elevation east west and north south currents from a location in the great bay are shown in fig 7 modeled and observed spectral density f show similar energy distribution at the tidal constituents and compare well for the sea surface elevation and both orthogonal components of the velocity note that the noise floor associated with the observed spectra is much higher than for the model a result owing to the sampling uncertainty associated with the pressure sensors and acoustic profiling instruments as well as the model not considering baroclinic flows discussed later the coherence squared γ 2 is high 0 99 at the tidal harmonic frequencies well above the critical value 0 52 the corresponding phase at the energetic m2 frequency is 2 47 deg for the sea surface elevation time series and 8 48 and 3 98 deg for the east west and north south velocities respectively the average model data phase at the m2 frequency for all sensors in the great bay during the 2015 deployment for sea surface elevation and the bi directional velocities was 0 03 0 34 and 2 32 deg respectively 3 4 tidal harmonic growth and phase difference modeled and observed power spectra of sea surface elevation f from two locations spanning the estuary one near the mouth at fort point and the other in the great bay are shown in fig 8 the m2 tidal energy decays by about 45 as shown in fig 5 on the other hand the spectra show a sharp increase in the energy levels at the tidal harmonics in the great bay evident well beyond the m4 and m6 constituents indicating the strong growth of overtides and nonlinear evolution of the spectra the growth of the m6 harmonic exceeds that of the m4 harmonic consistent between the modeled and observed spectra to examine the spatial nonlinear evolution of the tidal spectra the m2 m4 and m6 tidal constituents as determined by t tide analysis along the center channel from the mouth to the upper reaches of the great bay is shown in fig 9 along with the depth variation along the transect the m2 tidal amplitude decays as expected modeled m4 and m6 harmonics increase from 2 to 7 of the m2 amplitude consistent with the observations interestingly the m4 amplitude first grows through the first 8 km of the piscataqua river then decays to very small value at dover pt and then grows again in the upper reaches last 3 km of the great bay over the mudflats the spatial evolution of the m4 tidal constituent is qualitatively similar to the observations but underestimates the magnitude by about a factor of 2 in the narrows of the lower piscataqua river and overestimates in the upper reaches of the great bay similar results are obtained if we include baroclinic or subtidal flows we do not fully understand why this is occurring but may arise from complexities in the bathymetry and sidewalls in this part of the estuary not well resolved in the model or from viscous or turbulent effects assumed constant throughout the model domain moreover it has been shown that locally high values of the m4 tide can occur near headlands as a result of the centrifugal component of the advection of m2 momentum parker 1984 further investigation will need to address the role of bathymetric resolution and topography in the local generation of the m4 tide the m6 tidal amplitude shows a steady increase throughout the estuary leveling off and even decaying near the squamscott over the final 3 km in the great bay the m6 tidal constituent driven primarily by frictional effects parker 1991 appears to be well modeled throughout the estuary the phase evolution across the estuary is shown in fig 10 top panel for the m2 tidal frequency at all observation stations where time series are available table 1 the modeled evolution of the p u phase closely follows that of the observations the p u phase relationship in the first 12 km of the estuary is consistently about 45 indicating a partially progressive and standing wave motion however 12 km upstream the p u phase abruptly changes to 90 deg consistent with a standing wave from dover pt through the great bay estuary this change in p u relationship is consistent with the observed tidal dissipation and relative phase change of the m2 tidal constituent fig 5 also shown in fig 10 is the evolution of the growth of the m4 relative to the m2 constituent a ratio eq 7 the modeled growth of the m4 harmonic increases through the first half of the lower piscataqua river decreasing at dover pt and then increasing again through the upper reaches of the great bay to about 8 of the m2 amplitude where the depth shallows significantly over the mudflats the evolution of the tide depends strongly on the water depth consistent with a nonlinearly shoaling tidal wave this spatial behavior is qualitatively consistent with the observations that show about twice as much harmonic growth as the model in the lower piscataqua also shown in fig 10 is θ diff eq 8 an indication of the relative importance of the ebb and flood tide to the circulation following friedrichs and aubrey 1988 although the model under predicts the growth of the m4 constituent the phase differential is qualitatively consistent with the observations the lower reaches of the estuary in the piscataqua river show ebb dominance consistent with a stronger receding tide as the estuary drains the great bay beyond adam s pt on the other hand shows a strong flood dominance indicating the flows into the bay and over the mudflats are greater than that produced by the ebb tide this behavior is consistent with the evolution of the sea surface elevation skewness and asymmetry fig 10 the skewness shows similar trend to a ratio and θ diff and is relatively low through the piscataqua river growing in the little bay and great bay suggesting a strong nonlinear evolution to the shoaling tide wave with asymmetrical form about the horizontal along channel axis the asymmetry increases in magnitude sharply in the great bay indicating a pitched forward wave profile that has shorter duration but stronger flood currents and longer duration but weaker ebb currents consistent with the flood dominance estimated from θ diff 4 discussion the tidal dissipation and phase evolution in the model is modified by the choice of apparent bottom roughness z ob a range of values for z ob were introduced in model simulations and the best fit of the model tidal analysis to the observed m2 energy and phase evolution used to determine the most appropriate value our best estimate z ob 0 02 m is consistent with swift and brown s 1983 estimates based on the 1975 observations in their work they find a range of frictional coefficients from 0 015 to 0 054 they also note that the dissipation was highest in regions where the flows were larger generally occurring in parts of the estuary where there are constrictions in the flow owing to a narrowing of the river channel our model results show that ranges of z ob from 0 015 to 0 030 m give reasonable results throughout the estuary and suggest that the dissipation is well represented with a single value this is somewhat surprising in that the character of the seafloor ranging from rocky and coarse sediments in the channels to fine sands and muds on the flats changes significantly over the estuary on the other hand the flows also change similarly that is where the flows are highest the more rocky the bottom and more coarse the sediments i e the fine material is washed away and where the flows are weak the more fine grained the sediments and the nature of the bottom changes i e with tidal channels cut through the mud and vegetation model simulations that include and exclude subtidal forcing show that the tidal dissipation based on tidal analysis and considering only the m2 tidal constituent does not change significantly fig 11 this suggests that for the conditions examined with subtidal amplitudes ranging 0 10 0 30 m over the 30 day model runs and observation periods the nonlinear interaction with the tides is weak this also suggests that tidal dissipation and phase change produced from the model simulations conducted with 2015 forcing conditions can be compared with observations taken at other times for example from all the other experiments table 1 the freshwater input to the great bay estuarine system is relatively small and during non storm conditions contributes about 2 of the tidal prism short 1992 nhdes 2007 baroclinic model simulations with average river discharge and average salinity and temperatures had a negligible effect on the tidal constituent amplitudes and phases and can generally be ignored for the great bay when considering the tidal dynamics however comparisons of modeled time series and spectra with observations suggest that baroclinic flows are present rms velocity comparisons between barotropic and baroclinic model simulations away from the rivers but within the great bay are quite similar and agree to within about 0 01 0 02 m s however in the deep channel of the little bay where the flow field is high and has strong lateral shear the baroclinic model velocities deviate from the barotropic velocities by about 0 05 0 10 m s moreover spectral comparisons show that although the energetic tidal frequencies are not strongly affected the high frequencies and the noise floor between the tidal harmonics increases for the baroclinic flows this suggests that if higher frequency flows are of interest then baroclinic models should be considered but that tidal dynamics are well modeled with barotropic approximations in this work we have not considered the effects of waves or winds on the tidal circulation and dissipation in hindsight this appears to be a reasonable assumption at least for the conditions that occurred during the various field experiments as noted by wengrove et al 2015 wind generated currents during a large storm can enhance the tidal flows when the winds are in the same direction as the current considering that the tides reverse every 12 4 h in the great bay this direct wind driven flow might have an asymmetric effect on the overall current speeds and directions sometimes in the direction of the flow and other times opposing or acting at an angle in any case the effect appears to be small even for the large wind event examined in wengrove et al 2015 and does not likely change the overall character of the tidal currents owing to the order of magnitude difference between the wind induced flows of order 0 1 m s and the tides of order 1 2 m s this may not be true closer to shore where the tidal flows are weaker and the wind induced currents may be proportionally larger the model data comparisons show that the roms model reasonably well simulates the tidal dissipation and nonlinear evolution throughout the great bay estuarine system ignoring baroclinic flow and subtidal oscillations does not strongly affect the tidal dynamics at least for typical non storm conditions for the great bay region the model makes the hydrostatic approximation and solves the rans equations in three dimensions following rectilinear horizontal grid and a vertical terrain following σ coordinate system many other models such as adcirc westerink et al 1992 fvcom chen et al 2003 delft3d lesser et al 2004 also solve the same equations with similar approximations for rectilinear or unstructured grids and would likely also produce similar results the good agreement between modeled and observed velocities across the estuary tidal channels and over the mud flats suggests that modeled currents from these fully nonlinear models would produce a good representation of the flow fields useful for sediment transport and nutrient flux studies the subject of ongoing work 5 conclusions a high resolution three dimensional hydrodynamic model roms was implemented for the piscataqua river great bay estuary using observed bathymetry and validated with several observational datasets spanning the estuary the model was able to reproduce the observed tidal dissipation characteristics including dominant semidiurnal m2 tidal amplitude decay and phase changes as well as the nonlinear growth of the m4 and m6 harmonics the model underestimates the spatial evolution of the m4 magnitude by about a factor of 2 in the narrows of the lower piscataqua river and overestimates the values in the upper reaches of the great bay toward the squamscott river this could be due to complexities in the bathymetry and sidewalls in this part of the estuary not considered in the model or from viscous or turbulent effects assumed constant throughout the model domain and should be the topic of further investigation the modeled behavior reproduces a highly dissipative partially progressive wave in the lower 12 km of the piscataqua river with 45 tidal energy loss by dover pt consistent with previous observational studies swift and brown 1983 and a nearly standing wave in the low dissipative region between dover pt and the upper reaches of the great bay the spatial evolution from the mouth upstream in the estuary of the tidal harmonics sea surface elevation skewness and asymmetry and phase relationship between the along channel velocity and sea surface time series indicates a strong nonlinear tidal evolution consistent with an ebb dominant flow in the lower piscataqua and a flood dominant flow in the great bay the good comparisons with observations suggest that the model well represents the nonlinear behavior of the tide and accurately simulates the velocity and sea surface elevation time series throughout the estuary differences between model simulations with and without subtidal oscillations or river fluxes for the great bay are small suggesting that interactions between the tide and other low frequency subtidal or baroclinic flows are weak and can be ignored when considering tidal dynamics acknowledgement funding for this work was supported by the office of naval research onr littoral geosciences and optics program under grant number n00014 14 1 0557 new hampshire sea grant project r hce 1 under grant number na14oar4170083 and with funds provided by the university of new hampshire this research is part of the blue waters sustained petascale computing project which is supported by the national science foundation awards oci 0725070 and aci 1238993 and the state of illinois blue waters is a joint effort of the university of illinois at urbana champaign and its national center for supercomputing applications computations were also performed on trillian a cray xe6m 200 supercomputer at the institute for earth ocean and space at unh supported by nsf mri program under grant phy 1229408 jon hunt provided field assistance for the 2015 field experiments the 2007 observations were obtained by karl kammerer of noaa chris sherwood of the usgs and jamie pringle of unh assisted with establishing a stable model for the simulations presented 
24043,three dimensional numerical simulations of a tidally dominated estuary within the gulf of maine are performed using the regional ocean modeling system roms and validated with observations of sea surface elevation and velocity time series obtained between 1975 and 2016 the model is forced at the ocean boundary with tidal constituents m2 s2 n2 o1 k1 a time series of observed subtidal elevations and discharge from seven rivers that drain into the estuary harmonic analysis is used to determine the tidal dissipation characteristics and generation of overtides within the system amplitude decay and phase shift of the dominant semidiurnal m2 tidal component shows good agreement with observations throughout the main channel of the piscataqua river and over the channels and mudflats of the great bay the model simulates harmonic growth of the overtides across the spectrum and indicates a spatial evolution of the tide consistent with a shoaling wave that evolves from a skewed elevation profile with ebb dominance in the lower parts of the estuary to a more asymmetric pitched forward shape consistent with flood dominance the m4 constituent has spatial variation qualitatively similar to the observations but has magnitudes that are under predicted in the complex bathymetric region of the piscataqua river where much of the m2 tidal dissipation occurs the m6 tidal constituent agrees well with the observations throughout the estuary suggesting that frictional effects on harmonic growth are well modeled root mean square model data differences in velocities 0 05 m s and sea surface elevation 0 1 m agree to within about 10 of the tidal amplitudes differences between model simulations with and without subtidal oscillations in the estuary are small suggesting that interactions between the tide and other low frequency subtidal mean flows are weak and can be ignored when considering tidal dynamics including average fresh water discharge in the model does not affect the behavior of the tidal flows but can generate high frequency baroclinic velocities potentially important to mixing within the estuary keywords numerical modeling tidal dissipation nonlinear tidal evolution gulf of maine hydrodynamic model validation 1 introduction the transport and mixing of water sediment nutrients and organisms in estuarine and coastal systems is often dominated by astronomical tidal forcing of particular interest are the dynamics of shoaling tides induced by nonlinear wave interactions and energy dissipation and how that process impacts long term coastal planning and environmental conservation efforts as the tide propagates from the open ocean onto the shelf and into estuaries it becomes progressively more nonlinear and distorted leading to growth shoaling or decay dissipation of tidal amplitudes shifts in the phase of the tide and growth of tidal harmonics resulting tidal currents are difficult to predict analytically over realistic and complex bathymetry and require observation or numerical simulation to quantify evolution of tidal nonlinearities produces asymmetries in ebb flood current strength and duration boon and byrne 1981 that when averaged over a tidal cycle has been used to estimate net sediment transport and circulation patterns dronkers 1986 stronger flood currents drive the movement of coarse sediment and longer slack periods promote the deposition of fine grained sediment tidal amplitude attenuation in an estuary occurs from energy losses due to turbulent mixing and from frictional affects due to interactions with the bottom and lateral boundaries of the estuary energy dissipation of the tidal wave can be described in terms of amplitude decay of the dominant tidal constituent which for the gulf of maine is the semi diurnal m2 tide that contributes about 90 of the predicted tidal variance not all energy is dissipated due to frictional effects and some is transferred to higher harmonics overtides e g the m4 and m6 tidal constituents through nonlinear interactions and frictional effects that create tidal asymmetry aubrey and speer 1985 speer and aubrey 1985 parker 1991 a comparison of the magnitude of the m2 constituent with the first harmonic m4 is a direct measure of nonlinear interactions of the m2 tide whereas the phase difference qualitatively describes the tidal asymmetries in the system friedrichs and aubrey 1988 generation of the m6 component is largely attributed to frictional affects parker 1991 the dissipation problem is complicated by the highly nonlinear nature of tidal shoaling and propagation and the need to define representative bottom boundary conditions that characterize the interactions between tidal currents and the seabed dissipation in inlets and estuaries leads to development of local phase lags between pressure and velocities that shift slack tide periods up to a quarter of the wave period 90 deg phase shifts between sea surface elevation and along channel velocity and also impacts the evolution of tidal harmonics that are amplified and phase shifted relative to open ocean values this behavior can affect the overall transport in the estuary thus a good understanding of the spatial and temporal patterns in tidal dissipation can aid in long term coastal management and planning for example site selection for tidal renewable energy projects neill et al 2014 the tides may also interact nonlinearly with river flow storm surges and wind driven currents that vary on time scales of hours to months often observations from only a few locations are used to describe the overall dynamics of an estuary and field experiments are limited to one specific area for a discrete amount of time it is often not feasible to collect enough measurements continuously everywhere to adequately characterize the tides and associated flows thus numerical models can be used to produce system wide predictions of water levels and currents under different hydrodynamic and meteorological forcing conditions e g warner et al 2005a quantitative prediction of tidal amplitudes and currents is needed for flooding and inundation studies mooring and berthing design safe navigation interaction with structures and bottom shear stress prediction for sediment transport organism transport and nutrient fluxes in this study we discuss the implementation and validation of a three dimensional high resolution hydrodynamic model of a tidally dominated well mixed estuary located within the gulf of maine the gulf of maine has a natural resonance close to the semidiurnal m2 tidal constituent garrett 1972 enhancing the tides throughout the gulf including connected estuaries and coastal embayments including the bay of fundy in this study we examine the piscataqua river great bay estuary located within the gulf of maine at the border of new hampshire and maine fig 1 tidal forcing for the great bay is dominated by the semidiurnal m2 component of the tide has a tide range on the order of 2 4 m depending on the spring neap cycle and has variable but mostly minor freshwater river discharge it is home to both the second deepest u s naval port and portsmouth harbor which is home to some of the fastest tidal currents of any commercial port on the u s east coast the estuary has two tidal regimes a high dissipative region through the lower piscataqua river from the mouth to dover point and a low dissipative regime from dover pt through the little bay and great bay brown and trask 1980 swift and brown 1983 the former region behaves like a partially progressive wave with concomitant phase shift of the slack tidal period whereas the latter has phase shifts consistent with standing waves this behavior causes changes in the timing of tidal currents and the associated net sediment transport throughout the estuary previous modeling studies of the great bay ip et al 1998 erturk et al 2002 mclaughlin et al 2003 considered depth integrated two dimensional flow fields with the primary focus of representing the gross tidal behavior to estimate the net transport of water and sediment in the estuary the model validation process includes examination of the nonlinear tidal behavior that drives tidal asymmetry and tidal energy dissipation in terms of amplitude decay and phase lags using water level measurements and harmonic analysis modeled results are compared with coincident and previous observations and with results from the literature this study will form the basis for additional modeling aimed at examining the spatial variation in bottom shear stresses needed for sediment transport calculations horizontal and vertical mixing within the estuary and transport of larvae nutrients and carbon within the estuary section 2 describes the field site observational datasets the hydrodynamic model and grid development and the model validation and tidal analysis methodology section 3 describes model results and section 4 discusses the model observation comparison in terms of nonlinear evolution section 5 presents the conclusions of the study 2 methods 2 1 site description the great bay estuarine system is located along the new hampshire maine border within the gulf of maine in the northeastern portion of the united states fig 1 it is a recessed drowned river valley connected to the gulf of maine via the piscataqua river armstrong et al 1976 the tide range is 2 4 m over the spring neap cycle with tidal currents greater than 2 m s in the channels at maximum ebb and flood tides at low tide as much as 50 of the great bay is exposed as low lying mudflats cut with deep tidal channels the surface area of the estuary is approximately 55 km 2 measured at mean high water nhdes 2007 the volume is 156 106 m 3 and 235 106 m 3 for low and high tides respectively with a tidal prism of 79 106 m 3 swift and brown 1983 nhdes 2007 seven tributaries contribute fresh water to the system the squamscott lamprey winnicut oyster bellamy cocheco and salmon falls all feeding the upper and lower piscataqua river that flows into the gulf of maine river fluxes are determined by precipitation and runoff and regulated by dams or weirs that modulate the freshwater volume entering the system typically except during large storms or the spring melt the freshwater input is relatively small and only contributes 2 of the tidal prism short 1992 nhdes 2007 the generally small freshwater fluxes and strong tidal mixing results in weak or negligible stratification except very close to the river mouths and during periods of little rainfall the salinities at the great bay buoy fig 3 are nearly equal to the gulf of maine indicating that horizontal variation in density due to river fluxes are also weak as our interests include the ability of the numerical model to represent the vertically varying flow fields we will include model runs with and without average river discharges to evaluate the influence of baroclinic flows on the tidal behavior ocean waves outside the mouth of the estuary are strongly refracted away from the deep center channel and rapidly attenuate upstream and thus do not greatly contribute to the velocities or water level fluctuations in the estuary other studies have shown that waves can have an impact on tidal currents e g lewis et al 2014 wind driven surface gravity waves in the large lobe of the great bay proper are generally small 5 20 cm significant heights owing to the limited fetch and strong attenuation by energy loss through interactions with tidal currents and the muddy bottom or shallow aquatic vegetation eel grass meadows although waves on the great bay could be important to bottom shear stresses over the mud flats they do not substantially alter the larger scale circulation and thus are not considered further in this study wind driven mean currents may be substantial during storm conditions but are generally much weaker than the tidal currents wengrove et al 2015 and thus are also not considered in this study the bathymetry of the estuary is complex fig 2 with steep sidewalls in the main channel of the piscataqua river with water depths ranging 13 26 m ocean water flows into mouth of the piscataqua river through two channels a main entry point to the north of new castle island between new hampshire and maine and a secondary entry point through little harbor to the south of new castle tides entering little harbor flow through relatively shallow water and around several islands and join the piscataqua river between pierce island and portsmouth nh flows through the main channel make a sharp 90 deg turn around new castle at fort point and then flow around the portsmouth naval shipyard primarily to the south in the deeper channel but also the back bay a narrow shallow waterway that reconnects with the piscataqua river near pierce island the piscataqua river splits at dover pt with the main flows sharply turning south into little bay and with a smaller portion of the flow heading to the north connecting the lower piscataqua river with the upper piscataqua fed by the cocheco and salmon falls rivers to the north with average summer discharge rates of 8 54 and 15 4 m 3 s respectively nhdes 2007 the channel between the mouth at new castle island and dover pt is 12 km long and characterized by a hard rocky bottom with coarse sediment in the deep channels and steep rocky shorelines for most of the reach the flows through this part of the estuary are high exceeding 2 m s in some locations on both the flood and ebb tides once the flow enters the little bay it remains strong through the deep center channels with weaker flows up and over the bordering mud flats the oyster and bellamy rivers that flow into the little bay have average summer discharges of 0 94 and 1 32 m 3 s respectively nhdes 2007 the little bay joins the great bay at furber strait near adam s pt the deep center channel gradually shallows and bifurcates into an eastern and western branch flanked by large mud flats that dominate this portion of the estuary the squamscott lamprey and winnicut rivers all flow into this part of the estuary with average summer discharge rates of 5 3 10 0 and 0 7 m 3 s respectively nhdes 2007 for this study the tidal analysis focuses on the main channel flows from the mouth of the piscataqua river to the upper reaches squamscott river of the great bay estuary fig 1 2 2 observations field observations of horizontal currents spanning the water column and sea surface elevation from bottom pressure and tidal stations were obtained during several field experiments in 1975 2007 2009 2015 and 2016 and the continuously operating noaa tide gauge station at fort point nh station id 8423898 table 1 summarizes the dates and durations of the field studies and fig 3 shows the instrument locations observations of tidal elevations and currents within the estuary were obtained in 1975 by the university of new hampshire unh in cooperation with the national ocean survey nos summarized in swenson et al 1977 and silver and brown 1979 original data were unavailable so tidal analysis estimating m2 tidal amplitudes and phases from swift and brown 1983 is used in this study observations of bi directional currents in 0 5 1 0 m range bins and water levels from the mouth to adams pt were obtained by noaa in 2007 using six bottom mounted upward looking acoustic doppler current profilers adcps the instruments were deployed for between 41 and 45 days recovered and then moved to new locations with water depths ranging between 4 3 and 19 3 m these data are available and described online at https tidesandcurrents noaa gov observations of water levels were obtained by unh in 2009 at four locations in the great bay using bottom mounted pressure sensors and an rtk gps buoy the instruments were sampled between 30 and 120 s and deployed between 9 and 84 days and averaged over 6 min intervals following standard noaa procedures observations obtained for 7 71 days by unh in 2015 and 2016 include 1 min averaged bi directional currents in 0 25 1 0 m range bins and water levels from six adcps deployed across the great bay in water depths ranging 3 17 m bottom pressure was converted to sea surface elevation using observed bottom temperature at the instrument location and salinity obtained from the great bay coastal buoy located in the center of the great bay estuary http www opal sr unh edu data buoys great bay index shtml 2 3 hydrodynamic model the regional ocean modeling system roms haidvogel et al 2008 shchepetkin and mcwilliams 2005 is an ortho curvilinear three dimensional numerical coastal ocean circulation model that solves finite difference approximations of the reynolds averaged navier stokes rans equations using the hydrostatic and boussinesq assumptions the objectives of this study focus on the hydrodynamic component to determine the tidal dynamics which are of first order concern in validating the numerical model roms has been used in both regional e g zhang et al 2009 yang et al 2016 and estuarine modeling studies e g warner et al 2005a moriarty et al 2014 and implemented into other coupled modeling systems e g warner et al 2008 warner et al 2010 a third order upwind advection scheme is used to solve for horizontal advection a centered fourth order advection scheme is used to solve for vertical advection a k ε generic length scale gls turbulence closure model is used to calculate the horizontal and vertical eddy viscosities umlauf and burchard 2003 warner et al 2005b in conjunction with the kantha and clayson 1994 stability function within roms the wetting and drying algorithm warner et al 2013 is utilized to simulate the inundation and draining of the tide over shallow areas alternatively covered and uncovered by the tide in which the critical depth d crit is set to 10 cm once the total water depth is less than d crit no flux is allowed out of that cell and it is considered dry finally barotropic and baroclinic modes are solved separately in roms with the mode splitting algorithms described in haidvogel et al 2008 barotropic time steps in model simulations herein are 1 20 of the baroclinic time step 2 3 1 model grid the model domain is defined by a rectilinear arakawa c grid with a constant 30 by 30 m horizontal resolution fig 4 downsized by a factor of 33 1 3 in the figure there are 8 vertical layers in a terrain following σ coordinate system that is adjusted for slightly higher resolution near the surface and bottom boundaries the domain is rotated 37 deg ccw from true north to align the offshore boundary with the approximate orientation of the shoreline along the new hampshire maine coast the domain ranges 22 02 by 25 02 km 734 by 834 cells the grid elevations were defined using bathymetric data obtained from the center for coastal and ocean mapping ccom http ccom unh edu and lidar data collected by usgs noaa and usace https coast noaa gov dataviewer and interpolated onto the center of the horizontal grid cells a hierarchy was defined that weighted the most accurate recent and complete topographic and bathymetric data highest with any gaps filled with more uncertain older or less complete data sources the combined elevation grid fig 4 was then processed with the matlab easygrid routine https www myroms org wiki easygrid to create the rectilinear grid and corresponding land mask that was subsequently input into roms during model testing the grid was smoothed in locations sensitive to numerical instabilities using interpolation methods described in plant et al 2002 2 3 2 boundary conditions at the open ocean boundary south edge of the rotated domain fig 4 the model is forced by tidal and subtidal oscillations see section 2 3 3 using the implicit chapman free surface and flather depth averaged velocity boundary conditions the chapman flather conditions employ the radiation method at the boundary assuming all outgoing signals leave at the shallow water wave speed flather 1976 chapman 1985 these particular boundary conditions have been shown to be the most suitable for tidal forcing palma and matano 1998 2000 marchesiello et al 2001 carter and merrifield 2007 three dimensional baroclinic momentum equations were set to radiation and gradient conditions for velocities and tracers the eastern northern and western edges of the domain are closed the bottom boundary condition for momentum was parameterized by a simple drag coefficient assuming a logarithmic vertical velocity profile in the bottom vertical cell the drag coefficient c d is represented by 1 c d κ ln z z ob 2 where z is the vertical elevation of the mid point of the bottom cell z ob is a characteristic bottom roughness in m and κ 0 41 is the von karman coefficient kundu 1990 a range of bottom roughness values from 0 015 to 0 030 m were tested and the best fit was determined iteratively from model data comparisons of m2 tidal dissipation as a function of distance from the estuary mouth see fig 5 within each run z ob was assumed to be spatially uniform across the domain the kinematic bottom stress boundary conditions are given by 2 τ b x ρ 0 c d u u 2 v 2 3 τ b y ρ 0 c d u u 2 v 2 where τ b x and τ b y are the bottom stresses in the x and y directions respectively 2 3 3 model initialization and forcing forcing conditions at the open ocean boundary are specified in two ways the first is with an analytical representation of tidal elevations and velocities considering only the principal semidiurnal m2 n2 s2 and diurnal o1 k1 tidal constituents determined by the oregon state university global tidal prediction software package otps in conjunction with the united states east coast regional tidal solution ec2010 egbert and erofeeva 2002 the otps provided the necessary tidal amplitude and phases that correspond to the observational datasets for the 2015 field study used in the model data comparisons of velocities see section 2 4 2 and fig 6 the amplitudes and phases compared favorably with a harmonic analysis of observed water level fluctuations at fort pt for the 2015 field experiment using t tide pawlowicz et al 2002 the second forcing consists of the analytical representation of the tides and including subtidal oscillations associated with atmospheric motions obtained from low pass filtered with a 33 hr cut off period observed time series of 6 minute averaged water levels at the fort pt tidal station the subtidal motions can have amplitudes in the gulf of maine of 0 10 0 30 m brown and irish 1992 change the water depth over the shallow mudflats considerably and although the time scales of the oscillations are generally much longer than the dominant semidiurnal tides may contribute to the overall water velocities on the flood and ebb coastal ocean currents associated with barometric wind driven or other shelf motions at the offshore open boundary are assumed small consistent with observations of currents from 2007 at the most seaward instrument location pir0701 and not considered herein in each case tidal with or without subtidal forcing time series of water level fluctuations are ramped hyperbolically from rest over a 2 day period although tidal currents are included at the open boundary test simulations in which the boundary currents were set to zero and allowed to evolve with the sea surface fluctuations did not alter the results suggesting that approximating the forcing by only the pressure gradient at the mouth is reasonable consistent with geyer and maccready 2014 time series of at least 32 days are used to force the model so that tidal analysis with t tide produces amplitudes and phases of the dominant tidal constituents with confidence intervals the open ocean boundary is located about 7 5 km from the mouth of the estuary where the fort pt tide station is located the time for the tide wave to propagate this distance is small about 7 3 8 1 min based on an average water depth of 30 24 m and thus has small effect on the phase estimates about 3 3 3 9 deg when comparing to coincident observations within the estuary three dimensional simulations were performed both with and without freshwater flows based on the average summer river discharge see section 2 1 salinity varying between 6 93 and 23 54 psu and water temperature varying between 19 5 and 25 4 deg c for the various rivers for the summer of 2015 was provided by the new hampshire department of environmental services https www des nh gov organization divisions water wmb vrap data htm ocean water temperature 17 deg c and salinity 31 5 psu was assumed constant and given by typical summer values for the gulf of maine diurnal surface heating and cooling were assumed small in comparison to the tidal mixing and were ignored although the precise values of the fluctuating river discharge temperature and salinity were not used in the model the variations in temperature and salinity predicted by the model compare favorably with 2015 observations obtained in the middle of the great bay near the surface with the great bay coastal buoy http www opal sr unh edu data buoys great bay index shtml and near the bottom with the seabird instruments co located with our adcp s deployed in 2015 modeled and observed fluctuations in temperature and salinity follow tidal cycles and reveal weak vertical gradients in temperature about 1 2 deg c and salinity about 1 2 psu consistent with a well mixed great bay environment away from the river mouths during typical summer conditions in new hampshire model simulations including subtidal oscillations and river fluxes had a very weak effect on the tidal behavior and thus the results presented below will focus on the model simulations for barotropic tides this is not unexpected for the typical summer conditions examined herein but might be an important consideration during extreme storms and high runoff periods or in the very shallow depths near the water s edge over the mudflats the effect of subtidal oscillations and baroclinic flows is discussed further in section 4 in the following tidal analysis from model simulations will be compared with observations within the estuary obtained in different experiments at different time periods tidal constituents are assumed to be the same throughout our model runs focus on the 35 day period spanning the 2015 field experiment and will be compared with observed velocity and sea surface elevation time series from 2015 and tidal analysis of observations obtained during all experimental periods table 1 time resolution was determined by iteration on grid smoothing and reducing barotropic and or baroclinic time steps until numerical stability was achieved for model simulations presented herein a baroclinic time step of 1 5 s was used with barotropic time step 1 20 of that value computations were performed on a cray xe6m 200 supercomputer at the institute of earth ocean and space at the university of new hampshire and the blue waters cray xe6 supercomputer located at the university of illinois urbana champaign output over the whole domain was stored to disk at 30 min average model time intervals and for 15 min averaged intervals at specific save points corresponding to instrument locations and along a densely sampled line every 100 m along the main transect passing through the entire estuary 2 4 model validation methods model validation is accomplished in four ways the first is by conducting a tidal analysis and comparing the modeled energy decay and phase shift of the dominant m2 tidal constituent throughout the estuary with similar analysis of observations of sea surface elevation time series the second is by comparing modeled time series of the vertical variation in currents with observations the third is with cross spectral analysis between modeled and observed sea surface elevation and horizontal velocity components at single locations and with the evolution of cross spectral phase at the m2 frequency between sea surface elevation and along channel velocities the fourth is by comparing the growth and phase change of m4 and m6 tidal harmonic constituents between modeled and observed time series and by comparison of the along estuary evolution of sea surface elevation skewness and asymmetry 2 4 1 tidal dissipation and phase change as the tide propagates into shallow coastal regions and interacts with bottom topography and basin geometry it loses energy through frictional processes that result in tidal amplitude decay and phase changes relative to the open ocean value due to phasing of the tide a direct time series comparison is only possible for model runs that coincide with the specific phases of the tide during that particular field study however tidal analysis of long 30 day time series of sea surface elevation obtained at other times can be compared with non synchronous model simulations provided there are no other atmospheric effects that nonlinearly interact with the tide and do not substantially change the tidal behavior therefore we conduct a tidal analysis using t tide pawlowicz et al 2002 to decompose each time series of sea surface elevation into tidal components and compare the modeled and observed tidal constituent energy from the linear gravity wave relation 4 e 1 2 ρg a 2 where e is the total energy per unit surface area a is the amplitude of the tidal constituent and the density ρ is assumed constant throughout the estuary in this study the semidiurnal m2 tide dominates contributing about 88 of the total tidal energy at the mouth of the estuary the energy at any location within the estuary e station is normalized by the value at the estuary mouth e ocean to represent the fractional energy loss e norm as the tide progresses upstream 5 e norm a station a ocean 2 assuming the uncertainties in the tidal amplitudes δa station and δa ocean are both independent and random then the error δe norm is calculated following taylor 1982 6 δe norm e norm 2 δa station a station 2 2 δa ocean a ocean 2 initial model calibration involves testing different bottom boundary conditions and iterating to estimate the energy decay as a function of distance from the estuary mouth that best fits the observations 2 4 2 time series comparison of vertically varying currents modeled currents are computed at defined σ coordinate levels that range from σ 1 at the bottom to σ 0 at the surface the total water depth in the model is given by the elevation of the seabed relative to the model datum defined plus the corresponding fluctuating sea surface elevation the observations on the other hand are obtained from fixed upward looking adcp s with vertical bin elevations defined in a fixed coordinate system relative to the bottom the range over which the currents are observed depends on the instrument characteristics e g acoustic frequency and instrument capabilities and the height of the fluctuating sea surface relative to the bottom acoustic interference by side lobes at the surface limit the range of useable vertical bins to be less than 94 of the total instantaneous water depth and appropriate filtering methods must be employed to eliminate spurious velocities near the surface as a consequence the velocities observed with adcp s in the field further from the bottom have bins coming into and out of the water column as the tide rises and falls to compare the modeled to observed currents the modeled currents in σ coordinates are transformed to the observational coordinate system by linear interpolation over the instantaneous water level at each time step in this manner the modeled time series at the transformed upper bins also come into and out of the water surface similar to the observations care must also be taken to represent the velocities from the observations at the center of the vertical bins and the model at the defined location by the σ coordinates a representative example of the time series comparison is shown later fig 6 and described in section 3 2 2 4 3 cross spectral analysis a more complete evaluation that includes the overall behavior of the modeled velocities can better be done with cross spectral analysis that shows the energy density levels for both the model and the data as a function of frequency and the coherence and phase relationship for each frequency as our interests lie with the tidal constituents the frequency resolution of the spectra will necessarily need to be fine enough to resolve the major constituents with lowest tidal constituent the o1 diurnal variation of about 0 0417 h 1 at the same time the confidence intervals on the spectra coherence and phase must be high enough to make reasonable comparisons for the 30 day time series examined cross spectra were computed with 10 degrees of freedom dof by averaging 5 adjacent frequency bands the frequency bandwidth of the smoothed spectral estimates was 0 0069 h 1 with lowest resolved frequency of 0 0035 h 1 the 95 confidence intervals are computed for the spectral amplitudes coherences and phase only those phase estimates for frequencies with coherence greater than the 95 critical value 0 52 for 10 dof are shown phase error bars for incoherent frequencies are meaningless bendat and piersol 2000 to reduce leakage effects a hanning data window is applied to each mean corrected time series before computing the spectra 2 4 4 sea surface elevation and along channel velocity phase difference tidal analysis of the sea surface elevation and velocities can be compared to show the relative change in phase as the tide evolves up the estuary in this case the observed and modeled bi directional velocities were rotated to align with the along channel direction using standard rotary analysis gonella 1972 ellipse orientations for the dominant m2 tidal frequency define the angle of the major axis of the rotary ellipse that is used in the rotation to along channel direction we conduct a tidal analysis to decompose each time series of the along channel velocity into amplitudes and phases for each harmonic tidal constituent frequency following the same procedure for the sea surface elevation see section 2 4 1 the phase difference between the sea surface height p and along channel velocity u at the m2 tidal frequency was computed for time series at locations that span the estuary and reported as the p u phase the evolution of the p u phase for the dominant m2 tidal constituent indicates the nature of the tidal motion throughout the estuary fig 10 top panel in a progressive wave the maximum currents occur at the same time as the maximum height of the wave and the currents and amplitude are in phase in a standing wave the maximum currents occur at mid tide half way between the crest and the trough of the wave and the along channel currents are 90 deg out of phase with the sea surface height 2 4 5 tidal harmonic growth and phase difference the growth of the m4 harmonic relative to the m2 constituent is a measure of the asymmetry and non linear distortion of the tide friedrichs and aubrey 1988 following speer and aubrey 1985 the amplitude ratio a ratio and the phase difference θ diff is defined as 7 a ratio a m 4 a m 2 8 θ diff 2 θ m 2 θ m 4 where am4 and am2 are the amplitudes of the m4 and m2 tidal constituents respectively and θ m4 and θ m2 represent corresponding phase relationships in general stronger frictional effects produce larger a ratio and the corresponding θ diff describes the gross behavior of the tides with phase differences between 0 and 180 180 and 360 indicating flood ebb dominance friedrichs and aubrey 1988 flood dominant systems have characteristically stronger flood currents and longer falling than rising tides whereas ebb dominant systems have stronger ebb currents and longer rising tides the amplitudes and phases of the m2 and m4 tidal constituents are estimated with a tidal harmonic analysis using t tide that fits harmonics to the time series and computes error bars on the estimates of amplitude and phases for each constituent allowing estimates of the uncertainty in a ratio and θ diff taylor 1982 the error estimates for δa ratio and δθ diff are calculated using the following formulations 9 δa ratio δa ratio δa m 4 a m 4 2 δa m 2 a m 2 2 10 δ θ diff δ θ m 2 2 δ θ m 4 2 following taylor 1982 similar to δe norm eq 6 the third moments skewness and asymmetry of observed and modeled sea surface elevation time series are computed along the estuary following elgar and guza 1985 the normalized by the variance to the 3 2 power skewness describes the general nonlinear deviation of the wave profile from a sinusoidal shape to a peaked up waveform symmetrical about the vertical axis through the wave crest the normalized asymmetry describes the asymmetry about the vertical axis and can indicate a pitched forward or pitched backwards wave form the nature of the skewness and asymmetry is determined by the phase relationship between the primary frequency and the coupled harmonics for purely skewed peaked up stokes like wave profiles the asymmetry is zero and the primary and higher harmonics are in phase for pitched forward backward the asymmetry is nonzero and negative positive sawtooth profiles have high negative asymmetries and phase relationships between the primary and first harmonic up to 90 deg evaluation of waveforms for wind driven surface gravity waves in the ocean and their relationship to third moments can be found in elgar and guza 1985 3 results model observation comparison results comparing model simulations for barotropic tides with observations are presented here and follow the methodologies discussed previously station data are retained from the model simulations at all the sensor locations as well as from locations separated by 100 m along a transect down the main channel extending from the first sensor location outside the mouth of the estuary to the upper reaches of the great bay by the squamscott river 3 1 tidal dissipation and phase change the observed energy decay and phase change of the m2 tidal constituent relative to the value at the most seaward location along the station transect through the estuary is shown in fig 5 the most seaward observation 1 km from the ft point tidal station closely matches the predicted tidal amplitude from the otps model and used to normalize the energy e norm eq 5 also shown is the variation in the center channel water depth along the transect error bars eqs 5 6 on the energy and phase estimates are based on the t tide analysis observations show an increase in tidal energy near the mouth and then a progressive decrease in energy through the energetic narrow portion of the lower piscataqua river this decay is strong and somewhat variable between portsmouth and dover pt and in general agreement with estimates of dissipation found by swift and brown 1983 by the time the tides reach the little bay entrance 45 of the m2 tidal energy has been lost over this same reach the m2 phase has changed 50 deg significantly larger than for a simple progressive tidal wave propagating upstream with estimate of about 6 deg phase change based on shallow water wave phase speeds and average water depth of 20 m and much less than a standing wave with 90 deg phase change interestingly the tidal amplitudes increase slightly between the entrance to the little bay dover pt and the upper reaches of the great bay squamscott bridge indicating some amplification as the tide propagates into progressively shallower water additionally the phase continues to evolve approaching 70 deg suggesting that the tide here is more reflective it should be noted that the tidal extent during the flood does not end at the squamscott bridge but continues an additional 8 km inland as well as up the other rivers fig 1 also shown in fig 5 are model predictions of the m2 tidal decay and phase change for a range of apparent bottom roughness z ob from 0 015 to 0 030 m the best fit to the observation is for z ob 0 02 m the model increase in m2 energy across the shallowing great bay bathymetry is in general agreement with the observations in general the model well predicts the evolution of the tidal phase throughout the estuary 3 2 time series comparison of vertically varying currents comparisons of modeled and observed current time series for 4 days from a single location in water depth of about 5 75 m in the great bay is shown in fig 6 both the east west and north south velocity comparisons are shown for elevations relative to msl near the bottom 4 13 m mid water column 2 63 m and near the surface 1 13 m in general the modeled velocities closely follow the observations including in the upper water column were the sensor bins are coming into and out of the water as the tide rises and falls root mean square rms errors between modeled and observed time series at all elevations above the bottom range 0 035 0 049 m s and 0 047 0 055 m s for the east west and north south velocity components and 0 095 m for sea surface elevation each about 10 of the amplitude at that location in general the 10 rms error between model data time series for all sensors across the great bay from the 2015 deployment is quite good with average rms errors for sea surface elevations east west and north south velocities of 0 096 m 0 054 m s and 0 060 m s respectively 3 3 cross spectral analysis cross spectra between modeled and observed sea surface elevation east west and north south currents from a location in the great bay are shown in fig 7 modeled and observed spectral density f show similar energy distribution at the tidal constituents and compare well for the sea surface elevation and both orthogonal components of the velocity note that the noise floor associated with the observed spectra is much higher than for the model a result owing to the sampling uncertainty associated with the pressure sensors and acoustic profiling instruments as well as the model not considering baroclinic flows discussed later the coherence squared γ 2 is high 0 99 at the tidal harmonic frequencies well above the critical value 0 52 the corresponding phase at the energetic m2 frequency is 2 47 deg for the sea surface elevation time series and 8 48 and 3 98 deg for the east west and north south velocities respectively the average model data phase at the m2 frequency for all sensors in the great bay during the 2015 deployment for sea surface elevation and the bi directional velocities was 0 03 0 34 and 2 32 deg respectively 3 4 tidal harmonic growth and phase difference modeled and observed power spectra of sea surface elevation f from two locations spanning the estuary one near the mouth at fort point and the other in the great bay are shown in fig 8 the m2 tidal energy decays by about 45 as shown in fig 5 on the other hand the spectra show a sharp increase in the energy levels at the tidal harmonics in the great bay evident well beyond the m4 and m6 constituents indicating the strong growth of overtides and nonlinear evolution of the spectra the growth of the m6 harmonic exceeds that of the m4 harmonic consistent between the modeled and observed spectra to examine the spatial nonlinear evolution of the tidal spectra the m2 m4 and m6 tidal constituents as determined by t tide analysis along the center channel from the mouth to the upper reaches of the great bay is shown in fig 9 along with the depth variation along the transect the m2 tidal amplitude decays as expected modeled m4 and m6 harmonics increase from 2 to 7 of the m2 amplitude consistent with the observations interestingly the m4 amplitude first grows through the first 8 km of the piscataqua river then decays to very small value at dover pt and then grows again in the upper reaches last 3 km of the great bay over the mudflats the spatial evolution of the m4 tidal constituent is qualitatively similar to the observations but underestimates the magnitude by about a factor of 2 in the narrows of the lower piscataqua river and overestimates in the upper reaches of the great bay similar results are obtained if we include baroclinic or subtidal flows we do not fully understand why this is occurring but may arise from complexities in the bathymetry and sidewalls in this part of the estuary not well resolved in the model or from viscous or turbulent effects assumed constant throughout the model domain moreover it has been shown that locally high values of the m4 tide can occur near headlands as a result of the centrifugal component of the advection of m2 momentum parker 1984 further investigation will need to address the role of bathymetric resolution and topography in the local generation of the m4 tide the m6 tidal amplitude shows a steady increase throughout the estuary leveling off and even decaying near the squamscott over the final 3 km in the great bay the m6 tidal constituent driven primarily by frictional effects parker 1991 appears to be well modeled throughout the estuary the phase evolution across the estuary is shown in fig 10 top panel for the m2 tidal frequency at all observation stations where time series are available table 1 the modeled evolution of the p u phase closely follows that of the observations the p u phase relationship in the first 12 km of the estuary is consistently about 45 indicating a partially progressive and standing wave motion however 12 km upstream the p u phase abruptly changes to 90 deg consistent with a standing wave from dover pt through the great bay estuary this change in p u relationship is consistent with the observed tidal dissipation and relative phase change of the m2 tidal constituent fig 5 also shown in fig 10 is the evolution of the growth of the m4 relative to the m2 constituent a ratio eq 7 the modeled growth of the m4 harmonic increases through the first half of the lower piscataqua river decreasing at dover pt and then increasing again through the upper reaches of the great bay to about 8 of the m2 amplitude where the depth shallows significantly over the mudflats the evolution of the tide depends strongly on the water depth consistent with a nonlinearly shoaling tidal wave this spatial behavior is qualitatively consistent with the observations that show about twice as much harmonic growth as the model in the lower piscataqua also shown in fig 10 is θ diff eq 8 an indication of the relative importance of the ebb and flood tide to the circulation following friedrichs and aubrey 1988 although the model under predicts the growth of the m4 constituent the phase differential is qualitatively consistent with the observations the lower reaches of the estuary in the piscataqua river show ebb dominance consistent with a stronger receding tide as the estuary drains the great bay beyond adam s pt on the other hand shows a strong flood dominance indicating the flows into the bay and over the mudflats are greater than that produced by the ebb tide this behavior is consistent with the evolution of the sea surface elevation skewness and asymmetry fig 10 the skewness shows similar trend to a ratio and θ diff and is relatively low through the piscataqua river growing in the little bay and great bay suggesting a strong nonlinear evolution to the shoaling tide wave with asymmetrical form about the horizontal along channel axis the asymmetry increases in magnitude sharply in the great bay indicating a pitched forward wave profile that has shorter duration but stronger flood currents and longer duration but weaker ebb currents consistent with the flood dominance estimated from θ diff 4 discussion the tidal dissipation and phase evolution in the model is modified by the choice of apparent bottom roughness z ob a range of values for z ob were introduced in model simulations and the best fit of the model tidal analysis to the observed m2 energy and phase evolution used to determine the most appropriate value our best estimate z ob 0 02 m is consistent with swift and brown s 1983 estimates based on the 1975 observations in their work they find a range of frictional coefficients from 0 015 to 0 054 they also note that the dissipation was highest in regions where the flows were larger generally occurring in parts of the estuary where there are constrictions in the flow owing to a narrowing of the river channel our model results show that ranges of z ob from 0 015 to 0 030 m give reasonable results throughout the estuary and suggest that the dissipation is well represented with a single value this is somewhat surprising in that the character of the seafloor ranging from rocky and coarse sediments in the channels to fine sands and muds on the flats changes significantly over the estuary on the other hand the flows also change similarly that is where the flows are highest the more rocky the bottom and more coarse the sediments i e the fine material is washed away and where the flows are weak the more fine grained the sediments and the nature of the bottom changes i e with tidal channels cut through the mud and vegetation model simulations that include and exclude subtidal forcing show that the tidal dissipation based on tidal analysis and considering only the m2 tidal constituent does not change significantly fig 11 this suggests that for the conditions examined with subtidal amplitudes ranging 0 10 0 30 m over the 30 day model runs and observation periods the nonlinear interaction with the tides is weak this also suggests that tidal dissipation and phase change produced from the model simulations conducted with 2015 forcing conditions can be compared with observations taken at other times for example from all the other experiments table 1 the freshwater input to the great bay estuarine system is relatively small and during non storm conditions contributes about 2 of the tidal prism short 1992 nhdes 2007 baroclinic model simulations with average river discharge and average salinity and temperatures had a negligible effect on the tidal constituent amplitudes and phases and can generally be ignored for the great bay when considering the tidal dynamics however comparisons of modeled time series and spectra with observations suggest that baroclinic flows are present rms velocity comparisons between barotropic and baroclinic model simulations away from the rivers but within the great bay are quite similar and agree to within about 0 01 0 02 m s however in the deep channel of the little bay where the flow field is high and has strong lateral shear the baroclinic model velocities deviate from the barotropic velocities by about 0 05 0 10 m s moreover spectral comparisons show that although the energetic tidal frequencies are not strongly affected the high frequencies and the noise floor between the tidal harmonics increases for the baroclinic flows this suggests that if higher frequency flows are of interest then baroclinic models should be considered but that tidal dynamics are well modeled with barotropic approximations in this work we have not considered the effects of waves or winds on the tidal circulation and dissipation in hindsight this appears to be a reasonable assumption at least for the conditions that occurred during the various field experiments as noted by wengrove et al 2015 wind generated currents during a large storm can enhance the tidal flows when the winds are in the same direction as the current considering that the tides reverse every 12 4 h in the great bay this direct wind driven flow might have an asymmetric effect on the overall current speeds and directions sometimes in the direction of the flow and other times opposing or acting at an angle in any case the effect appears to be small even for the large wind event examined in wengrove et al 2015 and does not likely change the overall character of the tidal currents owing to the order of magnitude difference between the wind induced flows of order 0 1 m s and the tides of order 1 2 m s this may not be true closer to shore where the tidal flows are weaker and the wind induced currents may be proportionally larger the model data comparisons show that the roms model reasonably well simulates the tidal dissipation and nonlinear evolution throughout the great bay estuarine system ignoring baroclinic flow and subtidal oscillations does not strongly affect the tidal dynamics at least for typical non storm conditions for the great bay region the model makes the hydrostatic approximation and solves the rans equations in three dimensions following rectilinear horizontal grid and a vertical terrain following σ coordinate system many other models such as adcirc westerink et al 1992 fvcom chen et al 2003 delft3d lesser et al 2004 also solve the same equations with similar approximations for rectilinear or unstructured grids and would likely also produce similar results the good agreement between modeled and observed velocities across the estuary tidal channels and over the mud flats suggests that modeled currents from these fully nonlinear models would produce a good representation of the flow fields useful for sediment transport and nutrient flux studies the subject of ongoing work 5 conclusions a high resolution three dimensional hydrodynamic model roms was implemented for the piscataqua river great bay estuary using observed bathymetry and validated with several observational datasets spanning the estuary the model was able to reproduce the observed tidal dissipation characteristics including dominant semidiurnal m2 tidal amplitude decay and phase changes as well as the nonlinear growth of the m4 and m6 harmonics the model underestimates the spatial evolution of the m4 magnitude by about a factor of 2 in the narrows of the lower piscataqua river and overestimates the values in the upper reaches of the great bay toward the squamscott river this could be due to complexities in the bathymetry and sidewalls in this part of the estuary not considered in the model or from viscous or turbulent effects assumed constant throughout the model domain and should be the topic of further investigation the modeled behavior reproduces a highly dissipative partially progressive wave in the lower 12 km of the piscataqua river with 45 tidal energy loss by dover pt consistent with previous observational studies swift and brown 1983 and a nearly standing wave in the low dissipative region between dover pt and the upper reaches of the great bay the spatial evolution from the mouth upstream in the estuary of the tidal harmonics sea surface elevation skewness and asymmetry and phase relationship between the along channel velocity and sea surface time series indicates a strong nonlinear tidal evolution consistent with an ebb dominant flow in the lower piscataqua and a flood dominant flow in the great bay the good comparisons with observations suggest that the model well represents the nonlinear behavior of the tide and accurately simulates the velocity and sea surface elevation time series throughout the estuary differences between model simulations with and without subtidal oscillations or river fluxes for the great bay are small suggesting that interactions between the tide and other low frequency subtidal or baroclinic flows are weak and can be ignored when considering tidal dynamics acknowledgement funding for this work was supported by the office of naval research onr littoral geosciences and optics program under grant number n00014 14 1 0557 new hampshire sea grant project r hce 1 under grant number na14oar4170083 and with funds provided by the university of new hampshire this research is part of the blue waters sustained petascale computing project which is supported by the national science foundation awards oci 0725070 and aci 1238993 and the state of illinois blue waters is a joint effort of the university of illinois at urbana champaign and its national center for supercomputing applications computations were also performed on trillian a cray xe6m 200 supercomputer at the institute for earth ocean and space at unh supported by nsf mri program under grant phy 1229408 jon hunt provided field assistance for the 2015 field experiments the 2007 observations were obtained by karl kammerer of noaa chris sherwood of the usgs and jamie pringle of unh assisted with establishing a stable model for the simulations presented 
24044,a phase resolved wave model is derived from an ocean circulation model for the purpose of studying wave current effects in nearshore zones one challenge is to adapt the circulation model to the specificities of wave physics this mainly concerns the consideration of non hydrostatic effects and the parametrization of wave breaking the non hydrostatic pressure is calculated using the artificial compressibility method acm the acm induced errors on wave dispersion properties are examined in detail in the context of the linear theory using idealized test cases the possible compromise between the precision achieved on non hydrostatic physics and the adjustable cpu cost of the acm method is looked at in detail the modification of the wave characteristics by the bathymetric slope and the breaking of waves are then examined from a linear slope beach laboratory experiment finally the model is evaluated on the issue of rip currents and their feedback on the wave field using a laboratory experiment of a beach with a bar intersected by channels keywords phase resolved wave modelling wave breaking rip currents non hydrostatic ocean circulation model 1 introduction the interactions between waves and current play a major role in coastal modelling studies and are important to correctly reproduce the nearshore circulation coastal engineering applications need realistic descriptions of waves and currents their transformations towards the nearshore and behaviours with artificial coastal structures for the design of harbours or dykes to prevent overtopping and to reduce submersion or erosion risks and this is only possible with a circulation model that reproduces explicitly the waves taking into account the waves in the circulation models allows on the one hand the representation of the residual flows generated by the waves such as the stokes drift mellor 2003 or the rip currents castelle et al 2016 and on the other hand to take into account turbulence generation by waves in the turbulent closure scheme which improves the oceanic surface and bottom boundary layers the vertical current shear and the vertical temperature and salinity profiles in the circulation model uchiyama et al 2010 in shallow waters when the waves effects near the bottom are no longer negligible the bottom stress is significantly increased allowing the resuspension of sand and sediments soulsby 1995 the stokes drift the undertow or rip currents then playing an important role in their transport in the water column the wave current interaction is also decisive for the mean surface level set up during storms the modification of the surface stress related to the roughness created by the waves the modelling of the marine submersion of the coastal continental surfaces bertin et al 2012 the modification of the coastline the evolution of the shape of the seabed bouharguane et al 2010 at the spatial scales of the coastal circulation models see review by klingbeil et al 2018 such studies are mainly done using a hydrostatic circulation model coupled with a spectral phase averaged wave model mcwilliams et al 2004 ardhuin et al 2008 bennis et al 2011 benetazzo et al 2013 kumar et al 2015 uchiyama et al 2017 the phase averaged wave model feeds the circulation model with stokes drift residual wave pressure momentum and turbulence production terms related to wave erosion in return the circulation model gives the wave model the variations of sea surface level and surface current the latter being able to significantly modify the wave field uchiyama et al 2009 as in the known example of rips current the choice of a phase averaged wave model is largely guided by computational time considerations as it provides mean properties of the sea state the spectral model can indeed be used with a horizontal resolution much lower than the wave wavelengths and therefore cover large areas with a reasonable number of grid points an alternative is the phase resolved model or deterministic model that explicitly describes the evolution of the free surface the swash models zijlema et al 2011 ref dif kirby and dalrymple 1983 funwave kirby et al 1998 bosz roeber and cheung 2012 nhwave ma et al 2012 are widespread examples of this type of model very often mentioned in the wave modelling literature the processes resulting from the effects of bathymetry such as refraction diffraction and reflections magne et al 2005 interaction with other waves or with the background circulation infragravity waves bertin et al 2018 the asymmetrical wave transformation at the approach of the breaking phase are in principle better represented by this type of model rusu and soares 2013 the phase resolved model is also well suited for sand mobilization which is directly dependent on orbital velocity near the bottom bouharguane et al 2010 the phase resolved model has a continuous approach of frequencies and directions while the phase averaged model solves its equations on a discrete spectrum of frequencies and directions which results in a relative inaccuracy related to frequency and direction resolution for example in michaud et al 2012 the wavewatch iii r model tolman et al 2016 is used with 36 discrete values of the direction leading to a truncation of the direction of the waves of the order of 10 however the phase resolved models require finer temporal and spatial discretisation thus larger computational resources the resolution of the horizontal grid imposes a limit on the permissible wavelengths truncating the shortest periods of the wave spectrum deterministic nearshore models should have accurate dispersion and non linear properties and thus should be governed by the navier stokes equations raoult et al 2016 using either an eulerian approach e g openfoam zhou et al 2017 or a lagrangian approach e g the meshless model sph model oger 2006 these models are appropriate for local studies although eventually limited by spurious diffusion or prohibitive computational time benoit et al 2017 simplifications can be made to overcome these problems for instance ref dif based on the mild slope equation berkhoff 1972 neglects some nonlinear effects swash can be used with depth averaged equations with appropriate assumptions to introduce the non hydrostatic pressure zijlema et al 2011 boussinesq serre 1953 or green naghdi green and naghdi 1976 models take partially into account nonlinear or dispersive effects e g bonneton et al 2011a 2011b tissier et al 2012 within the limits of the above simplifications phase resolved models should theoretically be capable of simulating the low frequency 3d processes usually modelled by circulation models since their equations are basically the same except for the hydrostatic assumption generally made in circulation models klingbeil et al 2018 as a result the phase resolved wave model is able to generate residual currents such as rip currents and therefore does not require coupling with a circulation model to simulate the coupling between waves and low frequency fields however it requires a significant horizontal and vertical resolution greater than the wavelength of the waves which limits the size of the modelled domain to date and to our knowledge most of the applications of the phase resolved models seem to concern domains whose horizontal dimensions are of the order of a few kilometres in each horizontal direction yoon 2014 the constant rise of computers capability suggests that phase resolved models could now be used on larger domains covering a significant part of the continental shelf meanwhile ocean circulation models currently used at coastal scales could be adapted to deterministic wave modelling with among other objectives the goal of linking waves and continental shelf circulations like wind induced eddies petrenko et al 2008 or to better understand momentum and turbulence transfer in the surface layer deigaard and nielsen 2018 this assumes however that their equations are suitable for phase resolved wave modelling one of the essential points is to be able to use these models in non hydrostatic mode a number of articles deal with this question for the mitgcm model marshall et al 1997 pom kanarska and maderich 2003 roms kanarska et al 2007 getm klingbeil and burchard 2013 symphonie auclair et al 2011 however at the resolutions where these models are most often used the consideration of non hydrostatic effects may not be an essential issue mckiver et al 2016 the applications envisaged so far concern more the propagation of internal gravity waves bordois et al 2017 or the convective processes paluszkiewicz et al 1994 than the high frequency surface waves one of the obstacles to the deterministic modelling of waves over large domains is the cost of the non hydrostatic solver klingbeil et al 2018 as previously mentioned in the field of phase resolved wave modelling the common alternative to this difficulty is to use simplified models adapted to the physical specificities of waves bonneton et al 2011a 2011b in the field of ocean circulation modelling the most widespread approach to the non hydrostatic problem on the contrary is to take into account the whole complexity of the problem it leads concretely to the resolution of a poisson equation for the non hydrostatic pressure this is a global approach insofar as this leads to a system of linear equations making all the points of the numerical domain interdependent its resolution is in principle costly and currently constitutes a challenge in the perspective of large numerical domains of several thousand points in each direction roullet et al 2017 the sigma coordinate widely used in coastal modelling also increases the cost of the resolution for two main reasons the first reason is that the time variation of the position of the vertical levels related to the movement of the free surface forces to update regularly the coefficients of the main matrix of the system and its possible preconditioning the possible representation of the wetting drying zones adds a similar difficulty the second reason is that the sigma coordinate transformation increases the number of nonzero coefficients of the principal matrix roullet et al 2017 propose to reduce this number by reformulating the momentum equations the same authors also evoke the perspective of the multigrid approach to significantly reduce the cost of calculation an application in the vertical direction is proposed by shi et al 2015 the alternative to the global approach is the local approach based on the direct resolution of non hydrostatic pressure various methods have been proposed so far johns 1991 klingbeil and burchard 2013 bordois et al 2017 lee et al 2006 among them lee et al 2006 hereafter l06 acm artificial compressibility method method has a reasonable numerical cost and its implementation in a model is easy a method derived from l06 is implemented in the symphonie circulation model marsaleix et al 2008 and tested in the aforementioned context namely the high frequency surface waves and the associated residual circulations the purpose of the present paper is to assess this model on the principal issues of wave modelling this concerns on the one hand the dispersion properties the vertical shear of the current the asymmetrical wave transformation at the approach of the breaking conditions the decay of the waves in the breaking zone on the other hand this concerns the residual waves related circulation such as the stokes transport the alongshore coastal drift the mean sea level rip currents and their feedback on the waves the article is organized as follows section 2 deals with the implementation of the acm method in the ocean circulation model section 3 gives an analytical solution verifying the acm equations for low amplitude waves section 4 describes academic and laboratory test cases and finally section 5 provides a summary and the conclusions 2 model description 2 1 equations for the sake of clarity the problem is first presented in a simplified way in a vertical 2d plane and presenting only the terms that dominate wave physics 1 u t g η x q x 2 w t q z where q is the non hydrostatic pressure times the water density u and w the horizontal and vertical current η the sea surface height as in l06 the hypothesis of incompressibility is retained to deduce the surface elevation 3 η t x h η udz eq 2 is in practice not calculated by the model instead the combination of eqs 1 and 2 that leads to the poisson equation is used 4 t u x w z g 2 η x 2 2 q x 2 2 q z 2 the left hand side of the poisson equation is ideally zero in the case of the incompressible hypothesis without questioning this hypothesis of incompressibility the particularity of the l06 method is however to allow the left hand side of eq 4 to be small and not to equal zero before detailing the approach of l06 some comments can be made on the possibility of not cancelling strictly the left hand side of eq 4 it can be noted for example that non hydrostatic incompressible models can use iterative solvers whose convergence is de facto imperfect precluding a strict cancellation of the left hand side of eq 4 this approximation is currently accepted because the physical coherence is preserved if the convergence of the solver is sufficiently precise we can also recall that a model such as the one used in this study does not formally represent eq 4 but its discrete approximation in the finite difference sense not strictly cancelling the left hand side of eq 4 induces an error whose order of magnitude can be compared to the discretization errors of the other terms for example given the numerical scheme used here the error made on the horizontal laplacian of q at the right hand side of eq 4 is of the order of appendix a 5 d x 2 12 4 q x 4 d x 2 12 k 4 q where k is the wavenumber this discretization error makes it possible to consider a possible error on the left hand side thus relativizing the question to the transformation of the left hand side of eq 4 into a term of artificial compressibility as described by l06 see eq 2 in l06 in practice the acm method of l06 leads to the reformulation of eq 4 as follows 6 1 α 2 2 q t 2 g 2 η x 2 2 q x 2 2 q z 2 assuming a sinusoidal form for q the artificial compressibility term at the left hand side of eq 6 is of the same order as ω α 2 q c α 2 k 2 q c is the celerity of the waves which under certain conditions can become of an order of magnitude comparable to eq 5 i e c α 2 comparable to d x 2 12 k 2 insofar as it is recommended to use the acm method with α such that c α 2 1 this point is closely examined later in the article in fact ideally in compressible theory the α constant should be the speed of the acoustic waves actually much higher than c however as pointed out by mahadevan et al 1996 this would have the consequence of requiring an extremely small time step highly increasing the computational cost this type of approach therefore generally uses a much lower α value in order to increase the time step and thus makes the cost of the calculation affordable it is therefore noted that the true compressibility of the ocean is not respected by this approach which relies more on numerical considerations than on a real physical justification resulting in the expression artificial compressibility used by l06 on the other hand apart from the eq 6 the other equations of the model strictly respect the incompressibility framework although greatly diminished by the computational time constraint just mentioned the value of α must nevertheless remain sufficiently large to allow rapid adjustment of eq 6 to variations in the hydrostatic forcing terms this property is essential for the accuracy of the non hydrostatic behaviour of the model in practice α must remain larger than the phase speed of gravity waves bordois et al 2017 the resolution of the poisson iterative eq 6 can be done with the same time step as the other equations in practice its determination is done from the stability criterion corresponding to the forward backward scheme but considering α rather than the speed of the gravity waves as it is larger the time step is therefore smaller than the one of a hydrostatic model in similar conditions but the calculation nevertheless remains affordable as we will see in the following because eq 6 is computed only once per time step of the full model 2 2 implementation in the symphonie model the model used is the ocean circulation model symphonie marsaleix et al 2008 as we focus on surface waves the effect of temperature and salinity is neglected for the sake of clarity the equations are given in a cartesian coordinate system but the model actually uses a system of generalized sigma coordinates the momentum equations are given by 7 u t uu x vu y wu z fv g η x r q x z k m u z τ u 8 v t uv x vv y wv z fu g η y r q y z k m v z τ v 9 1 α 2 2 q t 2 x u t y v t 2 q x 2 2 q y 2 2 q z 2 where u t and v t represent hydrostatic terms in practice all the terms of the right hand side of eqs 7 and 8 except r q x and r q y at this stage r at the right hand side of eqs 7 and 8 is equal to one but we will see later that a value other than 1 can be used to improve the accuracy of the dispersion relation and currents of short waves the incompressibility hypothesis gives the vertical velocity and elevation of the surface 10 u x v y w z 0 11 η t x h η udz y h η vdz the vertical diffusion coefficient is calculated by the k epsilon scheme rodi 1987 and τ u τ v are breaking terms parameterized as a horizontal diffusion 12 τ u x ν b u x 1 2 y ν b u y v x 13 τ v y ν b v y 1 2 x ν b u y v x the eddy viscosity of breaking ν b is calculated from a breaking criterion which will be discussed in a section to follow bottom and surface boundary conditions are as follows at the surface q 0 u v z 0 and at the bottom q z 0 ρ 0 k m u v z τ b x τ b y where ρ 0 is the sea water density and τ b x τ b y are the components of the bottom stress the bottom stress is parameterized as in blumberg and mellor 1987 from a quadratic relationship of the bottom current and a drag coefficient derived from a roughness length the equations are calculated with a forward backward time scheme detailed in appendix b considering that short surface waves are intrinsically three dimensional the barotropic baroclinic time splitting commonly used in circulation models for calculating separately barotropic and baroclinic velocities blumberg and mellor 1987 cannot applied here in practice all the equations are calculated with the same time step 2 3 the time step the time step is determined from the stability criterion specific to the forward backward scheme in the case of the hydrostatic model the criterion of stability with respect to surface waves would be 14 dt max dx 2 1 gh max where dx is the horizontal resolution h max is the maximum depth of the domain and therefore gh max is the maximum theoretical phase speed expected for surface waves in the non hydrostatic model the calculation of the equation for q imposes a more drastic additional criterion since it depends on α that is greater than gh max it is not trivial to find a simple expression similar to eq 14 but unsurprisingly it appears in simulations performed by the authors that the maximum value of the time step allowed by the non hydrostatic model was close to dx 2 1 α i e eq 14 with gh max replaced by α in practice in all the simulations presented the time step of the non hydrostatic model is given by a fraction 70 of this criterion to include other stability criteria than gravity waves such as those resulting from non linearities namely 15 dt 0 7 dx 2 1 α the determination of α is therefore related to the determination of the time step the larger the first the smaller is the second the quantity n which represents the ratio of the theoretical time step of the hydrostatic case to the time step of the non hydrostatic case is introduced 16 n dt max dt which using eqs 14 and 15 is equivalent to 17 n 1 0 7 α gh max n is a key parameter of the problem the larger it is the smaller is the model time step and the better is the accuracy expected on the non hydrostatic term since dt max is the time step that would normally have the hydrostatic model in similar conditions the ratio n is a good indicator of the relative additional cost associated with the nonhydrostatic effect the evaluation of this additional cost must also take into account that an additional equation for q has been added to the equations of the hydrostatic model the latter essentially counts 4 three dimensional equations two for the horizontal components of the current one for the turbulent kinetic energy and one for its dissipation rate the relative numerical excess cost of the equation for q is therefore of the order of n 4 1 4 1 25 n 3 analytical solutions analytical solutions verifying model equations in the context of linear theory are now presented we consider a 2d vertical plan a linearized version of eq 7 18 u t g η x r q x the continuity equation 19 u x w z 0 and the non hydrostatic pressure eq 6 the elevation of the surface is of the form η η 0 e i kx ωt the non hydrostatic pressure satisfying eq 6 is of the form 20 q g μ 2 η 2 p 0 e i kx ωt cosh μk z h with 21 μ 2 1 ω 2 k 2 α 2 where α appears in the left hand side of eq 6 and in the definition of n calculated by eq 17 using eq 20 and the surface condition q 0 leads to 22 q g μ 2 η 1 cosh μk z h cosh μkh using eqs 22 and 18 gives the horizontal current 23 u g k ω η 1 r μ 2 cosh μk z h cosh μkh 1 using eq 23 and the continuity eq 19 to derive the vertical velocity then applying the surface condition w η t leads to the dispersion relation 24 ω 2 k 2 g 1 r μ 2 h r μ 3 gk tanh μkh note that if r 1 and if n therefore α tends to infinity leading to the cancellation of the left hand side of eq 6 and μ 1 the dispersion relation eq 24 becomes equivalent to the usual dispersion relation kinsman 1965 25 ω 2 gk tanh kh while using eqs 23 and 25 we find the usual solution for the horizontal current kinsman 1965 26 u ωη cosh k z h sinh kh this result shows that the acm method becomes equivalent to the classical non hydrostatic incompressible method if n is sufficiently large in the following we distinguish the dispersion relations of the incompressible non hydrostatic linear theory i e eq 25 of the non hydrostatic acm theory i e eq 24 and of the hydrostatic theory i e eq 27 27 ω k gh fig 1 shows these different dispersion relations for periods ranging from 6 s to 14 s this choice will be discussed in the next section for different bathymetry values and n 5 the acm theory with r 1 provides a considerable improvement over the hydrostatic theory but overestimates the non hydrostatic effect fig 1 the dashed red curve is always below the green curve this overestimation increases with bathymetry at shallow depths the agreement is very satisfactory but the non hydrostatic effect is also less significant fig 1 h 10 m the parameter r is a way to bring acm theoretical solution closer to the non hydrostatic incompressible theory frequencies produced by eqs 24 and 25 are actually equivalent if 28 r μ 3 tanh kh kh tanh μkh μkh horizontal currents eqs 23 and 26 are identical on the surface without condition on r and μ while near the bottom they are equivalent if 29 r μ 2 1 cosh kh 1 1 cosh μkh 1 1 expressions 28 and 29 suggest the possibility of optimizing r according to the local values of the depth and the dominant period of waves this point will be addressed in future applications at this early stage of our study r will be considered constant fig 2 shows the absolute value of the relative error on the wavelength l acm l l as a function of the period l acm being deduced from eq 24 and l the true wavelength from eq 25 relative errors are more important for small periods and for large bathymetry values errors of the order of several are expectable with r 1 on the other hand it is possible to reduce the error level below 1 for all the values of period and bathymetry considered in fig 2 with r 0 9845 the fact that eqs 28 and 29 are not identical shows that it is not possible to strictly cancel the error on the current and on the dispersion relation with the same value of r however as shown in fig 3 for h 50 m and a period of 10 s eqs 28 and 29 give very similar values of r so that it is possible to find a value of r that reduces both the current and dispersion relation errors this will be discussed in the next section finally we note that a compromise on these two possible values of r is all the easier to find if n is large because eqs 28 and 29 converge both to r 1 in conclusion the analytical solutions corresponding to the linearized equations of the acm method show a satisfactory representation of the non hydrostatic effect in the range of periods concerned by the future applications of the model to the near shore zone the accuracy however depends on the setting of parameters such as n and r since the accuracy of a numerical model also depends on many other factors related to the discretization of equations such as spatio temporal resolution and relevance of numerical schemes the above theoretical analysis is now completed with numerical experiments 4 test cases 4 1 wave propagation in the context of linear theory in this section we check the ability of the model to correctly represent the physical characteristics of the simulated wave thanks to a series of test cases consisting in exploring the dispersion properties of the waves we consider here low amplitude waves 1 mm in order to be within the framework of the linear approximation for which simple analytical solutions exist kinsman 1965 the objective is to define the values of n which lead to a reasonable compromise between the computation cost consideration encouraging to lower n and the precision of the dispersion properties consideration encouraging to increase n we use a domain with a flat bottom at 50 m depth a periodic condition in the direction normal to wave propagation the horizontal resolution is 5 m and 20 vertical levels are used the waves are generated at the entrance of the domain while the fields are at rest at the initial state the opposite boundary is closed but the dimension in the direction of propagation is large enough so that the waves do not have time to reflect on it before the end of the simulation 10 min this first numerical experiment is carried out with r 1 the open boundary conditions are derived from marsaleix et al 2006 as recommended by blayo and debreu 2005 an outgoing field is defined as the difference between the total field and the incoming field we thus define anomalies of currents sea surface height and non hydrostatic pressure u u u w η η η w q q q w where u w η w q w are prescribed by the analytical solution of the short wave linear theory we then assume that the incoming short waves do not reflect at the coast because of dissipation and that waves propagating offshore are at lower frequencies they may consist of infra gravity waves if the conditions of their generations are met bertin et al 2018 the radiative conditions u gη c and q t c q x 0 are therefore applied using the long wave phase speed c gh the simulation is analyzed in order to extract the modelled wavelength and to compare it with that predicted by the theoretical dispersion relation of the incompressible linear theory eq 25 the modelled wavelength is diagnosed from the distance separating two successive identical values of the surface level in the direction of propagation the grid points chosen for the calculation of this diagnosis are located beyond a distance of 625 m from the open boundary and moreover the diagnosis is only calculated once the transient regime is reached i e once the wave front has passed these points a time average of the values obtained is finally calculated a set of simulations is done to estimate the precision on eq 25 as a function of n ranging from 2 to 20 and of the wave period the smallest period considered here is 6 s associated with a wavelength of 56 m for h 50 m represented by 11 grid points given the horizontal resolution of 5 m this minimum value is considered as a reasonable threshold with respect to the capacity of the c grid to represent the small wavelengths the maximum period considered is 14 s which is associated with a wavelength of approximately 250 m periods 14 s are not considered because their wavelengths much larger than the depths envisaged render them slightly non hydrostatic and therefore not interesting from the point of view of the authors fig 4 shows the relative error on the wavelength l l l where the true wavelength l is given by the incompressible linear theory eq 25 and l is either deduced from the model or from the acm linear theory eq 24 with r 1 errors decrease when n increases as expected from the previous section the errors are stronger when the bottom depth is important this could be an issue near the model open boundaries where the most important depths are normally expected to our knowledge near shore modelling studies mostly focus on areas where bathymetry does not exceed a few tens of meters we thus assume that 50 m could be a plausible upper limit for realistic models of the near shore zone at least for the time being in the following the case h 50 m is the subject of greater attention the model errors are generally similar to those predicted by the acm linear theory fig 4a and c a notable difference is that errors tend to zero when n increases in the case of the theory but not in the case of the model this is explained by additional sources of error of numerical origin in the case of the model these numerical errors are more important for small periods for example for a period of 6 s the residual error for large values of n is around 3 for h 50 m and around 2 for h 10 m fig 4a and b the agreement between the model and the acm linear theory is less good for h 10 m especially for a period of 6 s fig 4b and d the relative predominance of numerical errors is here related to the decrease of the wavelength caused by the decrease of h for a period of 6 s and h 10 m the wavelength is indeed around 48 m i e 10 grid points while the wavelength is around 56 m i e 11 points when h 50 m actually when h decreases causing the wavelength to decrease the resolution of the horizontal grid eventually becomes insufficient to represent the shortest waves unless wave breaking occurs before this critical situation model errors are also sensitive to vertical resolution the accuracy of the vertical profile of q and current depends on the number of vertical levels the shortest wavelengths are particularly sensitive to this error because the vertical profile of q is more non linear and is established on a smaller depth fig 5 shows that for the 6 s period the relative error is much smaller with 80 levels than with 10 or 20 using more vertical levels reduces the residual error but does not completely eliminate it the error levels obtained with 40 levels or 80 levels are however equivalent for larger periods results not shown the sensitivity of the error to the vertical resolution is smaller plausibly because the vertical profile of the fields whose characteristic scale of decay is related to the wavelength is less sheared the minimum error level around 1 5 in fig 5 case 80 levels n 20 is attributable to the time stepping method itself and no longer depends on the non hydrostatic aspects this aspect of the error can be examined in the hydrostatic model by re invoking the simple assumptions we made in section 2 1 combining the eq 1 now without term q and eq 3 the surface elevation is given by 30 2 η t 2 c 2 2 η x 2 given the numerical schemes used by our model the error is respectively of the order of dt 2 12 4 η t 4 dt 2 12 ω 4 η and c 2 dx 2 12 4 η x 4 c 2 dx 2 12 k 4 η for the left and right hand sides of eq 30 these errors are estimated from the first terms of a taylor expansion appendix c the compensation of temporal and spatial discretization errors is a characteristic of this type of discrete equation lemarié et al 2015 here we see that the two above errors are compensated when the time step is at its theoretical maximum i e dt dx c with c ω k when the time step is significantly smaller consequence of the increase of n the time dependent error becomes negligible and the error is reduced to the error of the right hand side c 2 dx 2 12 4 η x 4 also equivalent to c 2 dx 2 12 k 2 2 η x 2 and then the discretization of the eq 30 equation tends to behave like appendix c 31 2 η t 2 c 2 2 η x 2 c 2 dx 2 12 k 2 2 η x 2 so that the modelled phase speed approaches c 1 dx 2 12 k 2 c 1 dx 2 24 k 2 the relative error on the phase speed is therefore of the order of dx 2 24 k 2 2 π n 2 24 where n is the number of grid points per wavelength for the shortest wavelengths considered for which n is around 10 the relative error on c is therefore of the order of 1 5 given that the period is imposed by the forcing this level of error is therefore the one expected for the modelled wavelength and in fact it corresponds fairly well to the residual error for n 20 and 80 levels see fig 5 in summary the accuracy of the dispersion properties increases significantly with n until acm related errors become small compared to the other error sources mainly the vertical resolution and the order of precision of the numerical schemes used to calculate spatial and temporal derivatives the determination of n would lead to choosing n around 14 in the case of the experiment considered another way to choose n is to define an acceptable precision level on the dispersion properties depending on the period of the simulated waves for example a relative error of 3 is obtained for n 7 for a period of 8 s n 5 for a period of 10 s etc 4 2 improved accuracy apart from increasing n 4 2 1 distribution of vertical levels the previous paragraph shows that the accuracy decreases appreciably with the wave period at the shortest periods accuracy is more sensitive to vertical resolution increasing the number of vertical levels is an obvious but expensive way to reduce this second source of error modifying the vertical distribution of levels without changing their number is an alternative that allows a significant improvement in accuracy at small periods in the case of the 6 s period fig 5 shows that 10 irregularly distributed levels see table 1 provide a much higher accuracy than 10 equidistant levels and makes it comparable to the precision obtained with 20 equidistant levels the vertical distribution proposed here is such that the accuracy is also improved or maintained for the other periods considered results not shown it should be noted that in a realistic domain the generalized sigma coordinate makes it possible to adapt the vertical distribution to the bathymetry for example in areas where bathymetry is low relative to wave wavelength the vertical profile of current and pressure is less sheared and a more regular distribution of the levels is preferable to the irregular distribution of table 1 given the positive influence of increasing vertical resolution on accuracy the increase in vertical resolution that results from the decrease in bathymetry in the sigma formulation leads to a better representation of dispersion properties when approaching the coast 4 2 2 influence of horizontal resolution increasing the horizontal resolution without changing the other parameters especially n leads logically to decrease the level of error especially at short periods significantly increasing the horizontal resolution has a real but limited impact on the accuracy because of other sources of error such as the lack of vertical resolution the period 6 s is more critical than the other periods tested because as noted above the number of grid points per wavelength is particularly low doubling the resolution allows about 22 grid points per wavelength for h 50 m fig 4 shows that a relative error 3 is obtained with n 10 for a horizontal resolution of 2 5 m whereas this same level of error requires n 14 for a horizontal resolution of 5 m 4 2 3 tuning the nonhydrostatic pressure gradient a modification of the pressure gradient scheme is now proposed to use moderate n values while maintaining an acceptable level of accuracy on dispersion properties we consider values of n around 5 the proposed modification is suggested by the observation that the error on the wavelength is systematically negative figs 4 and 5 this is equivalent to the fact that the modelled phase speed is systematically lower than the theoretical phase speed in other words the non hydrostatic effect excessively decreases the phase speed this conjecture can be interpreted as an overestimation of the non hydrostatic component of the pressure gradient in agreement with the conclusions drawn from the analytical solution of the linearized acm equations section 3 fig 1 the proposed improvement consists in moderating the nonhydrostatic pressure gradient by multiplying the latter by a constant r slightly 1 see the non hydrostatic pressure gradient terms in eqs 7 and 8 fig 6a obtained with n 5 dt 0 032 s shows that the precision is improved when r decreases to a certain level below which the sign of the error is reversed becoming positive and the error increases again as expected from the acm linear theory the value of r that exactly cancels the error is not the same depending on the period it is around r 0 982 for the 6 s period and smaller for the other periods the general behaviour is nevertheless quite similar for all the periods and one can identify a unique value of r which makes it possible to reduce the error below 1 for all the periods fig 6a r 0 98 which corresponds to a very significant reduction of the error especially for small periods error of 8 and 5 for periods of 8 s and 6 s respectively with r 1 the general behaviour of the model is in good agreement with the analytical solution fig 6b however since the model has other sources of errors than those of the theoretical model fig 6a and b are not strictly the same and the best compromise for the value of r would be slightly larger according to eq 24 around 0 984 fig 6b although the theoretical model could certainly be used for a default setting of r fig 6a shows that errors can be further reduced with an empirical setting this numerical estimate of the best compromise on r is therefore used in the following as pointed out previously a realistic numerical domain is characterized by variable bathymetry as 50 m should be an upper limit of a near shore zone model in our future applications we now verify that the value r 0 98 which reduces the error below 1 for all periods when h 50 m remains relevant for lower values of the water depth these tests are done keeping the same time step dt 0 0 32 s and n 5 fig 7 obtained for h 20 m shows a precision greater than the simulation with h 50 m under the combined effect of the increase of the vertical resolution the same number of vertical levels distributed over a shallower water column and also the decrease in the non hydrostatic effect caused by the decrease in the ratio between the horizontal scale and the vertical scale if fig 7 suggests that the best value of r would be around 0 978 the minimum relative error is indeed close to this value for most of the periods considered the proposed value of 0 98 for h 50 m remains relevant for shallower depths because it reduces the relative error associated with the different periods between 0 25 and 0 note finally that the choice of the 50 m value as the maximum value of the bathymetry is somehow arbitrary there are certainly other ways of defining the offshore limit of the near shore zone such as the depth at which the waves are modified by the seabed in this case a maximum value based on the relative depth kh may be preferable the ability to strongly reduce or even cancel the error via the r parameter means that this parameter can be used to correct errors beyond the single error caused by the acm method the errors associated with the vertical discretization or the time stepping scheme can therefore be compensated by this method we thus examine the possible undesirable consequences of this compensating effect on other important properties of wave physics especially since according to the theoretical model the cancellation of the errors on the current and the relation of dispersion is not obtained strictly with the same value of r e g eqs 28 and 29 recall also that the shape of the vertical profile of the current is of particular interest since the intensity of the current near the bottom is crucial for the determination of the bottom friction the resuspension of sediments near the bottom the transport of sand among others fig 8a shows that for the period 6 s r 0 98 leads to a vertical profile of the current more accurate than with r 1 the latter nevertheless correctly reproduces the vertical shear the value r 0 986 being the one that would lead to a profile closest to the profile predicted by the linear non hydrostatic incompressible theory a similar conclusion is made for the period 10s fig 8b for r 0 976 the fact that a single value of r can both reduce the error on the dispersion relation and the current is finally consistent with fig 3 which shows that the values of r cancelling these two types of error are actually very close 4 3 test case with bi periodic domain we are now replacing open and closed boundary conditions at both ends of the domain in the propagation direction by periodic conditions the interest of this bi periodic domain is that it avoids reflections at the boundaries that could degrade the results the simulations can therefore be long which is of interest for testing the stability of calculations in the long term the depth is 50 m the domain has 510 points in the direction of propagation corresponding to a whole number of wavelengths here 17 for a wavelength of 150 m a theoretical period close to 10s and a phase speed close to 15 m s the fields are initialized from the linear theory the amplitude of the surface elevation is small 1 mm to remain within the framework of the linear theory nonlinear dissipation forces such as friction using a quadratic parameterization or breaking are therefore negligible in this particular case the laplacian temporal filter derived from marsaleix et al 2012 used in the equations for u v q to suppress the numerical mode is the main source of dissipation the following test quantifies its adverse effect on the long term physical mode fig 9 shows the amplitude of the surface elevation at different times given the value of the phase speed a distance of the order of 10 km cross shore size of realistic domains that we plan to use with this model is covered in about 10 min fig 9 shows that the attenuation is negligible over such a short time duration an attenuation of 10 would require at least 6 h during which the waves would travel a distance of about 300 km in our future realistic applications the open boundary condition will not be located so far from the coast the cross shore domain dimension should be at least an order of magnitude lower so this numerical attenuation should be 1 in other words small compared to the expected erosion induced by bottom friction turbulence or wave breaking 4 4 the case of a linear slope beach the hamilton and ebersole laboratory experiment hamilton and ebersole 2001 hereafter he01 svendsen et al 2003 focuses on the propagation of waves over a uniform linear slope beach their breaking and the alongshore current establishment this case test is of particular interest to us to assess the performances of the model on the important question of the parametrization of wave breaking incoming waves are monochromatic with a period of 2 5 s test 6 n in he01 the amplitude of the excited waves at the open boundary is adjusted to obtain a wave height corresponding to the conditions of test 6 n namely 0 19 m at the most offshore measurement points see the observation at 3 m from the incoming boundary on fig 10 the direction of propagation is 10 from the direction normal to the beach a description of the basin and its bathymetry is given in svendsen et al 2003 see also fig 10 the particularity of the basin lies in its pumping and recirculation system made of 20 pumps that allow having periodic conditions on the lateral boundaries in our simulation incoming waves are specified from the linear theory at the offshore boundary periodic boundary conditions are applied at the lateral boundaries of the model the dimension of the domain in the direction parallel to the beach is consistent with the condition of periodicity and the wave propagation direction of 10 in the deep part of the basin the horizontal resolution is 0 1 m and the grid has 10 vertical levels the dimension in the cross shore direction is 18 m and the deepest bathymetry offshore is 0 66 m the maximum wavelength in the deep part of the basin is of the order of 6 m a steady state is reached after about 150 s the results presented are obtained at the end of a simulation of 500 s the roughness of the bottom corresponding to this experiment is a priori not mentioned by its authors a roughness length of 3 10 5m was empirically fixed in our model in order to obtain a satisfactory agreement with the vertical profile of the measured alongshore current the basin dimensions are small compared to a real case if the ratio of the horizontal scale on vertical scale is used to deduce the characteristics that this experiment would have with dimensions 50 times larger close to the dimensions of the test cases presented in the previous section we obtain a horizontal resolution of 5 m a maximum depth of 30 m a maximum wavelength of the order of 300 m and a period of 125 s we are thus in conditions for which the problems of accuracy examined in the preceding sections are minor and make it possible to use the acm method with a small value of n and a coefficient of reduction of the gradient of q close to one in practice we use n 4 r 1 as mentioned previously we actually chose this test case for the wave breaking issue rather than the non hydrostatic specificities discussed in the previous test case wave breaking is parameterized through the horizontal diffusion of the momentum eq 13 the determination of the mixing coefficient ν b is an important aspect of this parameterization before going into the details of its calculation we examine the order of magnitude necessary for the decay of the waves inside the breaking zone to do this we consider an energy balance between the gradient of the potential energy flux g u η x and dissipation due to horizontal diffusion ν b u x 2 for the sake of clarity we limit ourselves to unidirectional propagation along the ox axis we hypothesize that within the wave breaking zone the wave height is proportional to the water depth miche 1944 32 h h γ where h is the wave height we consider here that the parameter γ is a constant of the order of 0 8 it can nevertheless be noted that the expression of γ can be more sophisticated as in guérin et al 2018 for convenience and considering that shallow depths where breaking mostly occurs allow this approximation the hypothesis of hydrostatic equilibrium is made in the following reasoning although the simulations presented are still non hydrostatic we thus consider that the current and the surface level are connected to each other according to u g c η where c gh it is finally assumed that the fields have a sinusoidal behaviour and that the wave amplitude η 0 satisfies eq 32 i e 2 η 0 h γ averaging over a period of waves the gradient of the potential energy flux is appendix d 33 guη x g 3 2 γ 2 3 16 h 1 2 h x and dissipation due to horizontal mixing is 34 ε b ν b 1 8 kgγh c 2 the equilibrium of eqs 33 and 34 leads to an estimation of the average of the mixing coefficient over a period of waves 35 ν b 3 8 g 3 2 h 1 2 h x t π 2 the latter increases with the slope of the bathymetry and vanishes when the thickness of the water column is zero the expression 35 is only an intermediate step towards a more complete formulation taking into account a triggering criterion based on the analysis of the instantaneous modelled fields but before addressing this aspect a preliminary experiment is carried out to test the relevance of the order of magnitude given by eq 35 practically eq 35 is used as the value of the horizontal diffusion coefficient in eq 13 inside the wave breaking zone the latter is arbitrarily defined from a threshold on bathymetry h 0 28 m deduced from observations of wave heights he01 in the pre breaking zone h 0 28 m ν b 0 fig 10 shows the simulated wave height and mean elevation and the corresponding observations of he01 as the laboratory experiment the simulation lasts 500 s the surface height is averaged over the last 50 s of the simulation the wave height is calculated from the difference between the maximum and minimum values of the surface level over the last 50 s of simulation the agreement is satisfactory the wave breaking trigger point is unsurprisingly well positioned since it is imposed by the criterion h 0 28 m the wave height is slightly overestimated in the breaking zone but the slope of decrease is parallel to that of the theoretical decay 2 η 0 h γ 0 8 dotted line in fig 10 it should be noted that several simulations were performed by increasing or decreasing slightly the value of ν b and that fig 10 corresponds to a 5 increase in the value predicted by eq 35 4 4 1 breaking criterion practically eq 35 is not really appropriate because it requires to arbitrarily specify the location of the onset of breaking in realistic case the breaking point of the breaking wave varies over time in particular according to the characteristics of the waves and more generally according to the surrounding hydrodynamic conditions an extension of eq 35 taking into account the instantaneous fields is therefore proposed 36 ν b ν b f b where ν b is the instantaneous coefficient ν b is the average coefficient eq 35 and f b is a function that triggers the wave breaking according to the instantaneous fields the average of f b over a period of wave is expected to be close to 1 so that the viscosity is on average close to ν b one consequence of this property is that ν b can be momentarily very high if f b is negligible over a large fraction of the wave period so that the average of ν b stays close to ν b many authors use the ratio of the wave height to the water depth the γ constant eq 32 as breaking criterion rusu and soares 2013 others use criteria based on the horizontal gradient of the modelled fields lynett 2006 for example zijlema et al 2011 propose to formulate eddy viscosity from prandtl s length and a turbulence equilibrium hypothesis which in practice links eddy viscosity to the horizontal gradient of velocities see their eq 12 roeber and cheung 2012 also use the velocity gradient see their eq 70 but in an approach that is more akin to a criterion on the froude number with gh as an approximation of the phase speed these authors emphasize that this criterion has the particularity to be more influential in areas where the slope of the waves is important in contrast to the h h criterion which favours the wave crests the criterion on the gradient is therefore in principle influencing a larger proportion of the breaking zone from the point of view of eq 36 an approach of this type should lead f b to act on a larger time interval and thus limit the maximum values of ν b on the other hand a criterion on the wave crest should lead f b to be small over a large fraction of the wave period with the aforementioned consequence of momentarily strong ν b values this consideration may be of numerical importance as there is a stability criterion which limits the eddy viscosity value for a given time step zijlema et al 2011 in the particular case of this experiment we find that a wave breaking criterion based on h h can cause the eddy viscosity to momentarily exceed the maximum value allowed by the cfl criterion i e ν b ν b max 1 4 dx 2 dt one possibility of overcoming this problem is either to reduce the time step of the model or to use a time splitting technique with a smaller time step for the calculation of the wave breaking without impacting the time step of the other equations of the model from a numerical point of view the approach proposed by roeber and cheung 2012 is interesting because their parameterization of the wave breaking should act on a longer duration limiting the maximum values of the breaking viscosity we choose this approach mainly for the aforementioned numerical issues but we propose to add a modification to that method in order to extend the influence of the breaking criterion to an even greater proportion of the breaking zone this is arguably questionable from the point of view of the realism of the process kennedy et al 2000 but it is an interesting option considering the numerical aspects mentioned above more specifically our objective is to avoid reducing the time step because of a too restrictive stability condition related to the wave breaking scheme the triggering criterion for wave breaking is inspired from roeber and cheung 2012 but is modified to be active both on the wave crest and between crests this criterion considers both velocities and their horizontal gradient the velocity gradient is scaled by the wavenumber so that the two quantities have the same order of magnitude a wave breaking criterion is then given by the ratio of the sum of these fields to the hydrostatic phase speed for simplicity the following formula is given in the case of unidirectional propagation 37 f b c 1 u s 2 1 k u s x 2 gh n 2 where u s is the surface current and k is the wavenumber fixed a priori according to the peak period prescribed at the open boundary the constant c 1 is determined empirically ideally its role is to allow the average value of f b to be of the order of unity over a period of waves so that the eddy viscosity is on average close to eq 35 it may be noted that in the idealized framework of the linear theory where the current would be of the form u 0 cos ωt kx the term in the numerator of eq 37 would simply be u 0 independent of time and in a breaking situation characterized by a current of an order of magnitude comparable to that of the phase speed the constant c 1 would simply be of the order of eq 35 the exponent n is used to adjust the selectivity of the criterion it is in particular chosen so that f b tends more rapidly towards 0 upstream of the breaking zone the simulations presented in the following use n 2 fig 11 corresponds to an eddy viscosity calculated with eqs 36 and 37 instead of eq 35 with the arbitrary hypothesis on the depth of the breaking onset the other parameters being identical to those of the previous simulation it is comparable to fig 10 the main difference is in the behaviour within the breaking zone where the curves are a little less linear in the case of fig 11 as a consequence of the non linear character of eqs 36 and 37 although the simulation is slightly less close to the observation in the case of fig 11 we prefer to use eqs 36 and 37 whose formulation is more general rather than eq 35 which suffers from its arbitrary definition of the trigger point of breaking 4 4 2 taking into account the breaking in the turbulent closure scheme the turbulence scheme used here is the k epsilon scheme rodi 1987 a wave related turbulence generation term may be taken into account when the circulation model is coupled to a phase averaged wave model michaud et al 2012 in the present case this term is not taken into account because the explicit modelling of the waves leads naturally to a significant production of turbulent kinetic energy tke through the vertical shear of the current the relevance of adding an additional production term to represent the assumed increase in turbulence in the breaking zone is an open question feddersen and trowbridge 2005 and is therefore examined here the he01 experience provides the opportunity to examine this point by providing measurements of the residual onshore current at different depths and at different distances from the edge of the basin here an additional production term p wb representing a fraction of the kinetic energy lost by the current via the horizontal mixing associated with the breaking i e eqs 36 37 is introduced into the tke equation of the k eps scheme 38 p wb c 2 ν b u x 2 u y 2 v x 2 v y 2 where c 2 is a constant between 0 and 1 eq 38 implies that a fraction of the breaking energy feeds the vertical mixing the simulated alongshore residual current is obtained by averaging the alongshore current during the last 50 s of the simulation fig 12 shows the comparison with the observed current from he01 note that the vertical axis has been normalized so that the figure is easily comparable to that of svendsen et al 2003 see their fig 11 two simulations are made one with c 2 0 the other with c 2 0 01 the two simulations generally give rather satisfactory results with the exception of the current at x 7 9m looking more closely at fig 12 we notice that the simulation taking into account p wb red lines gives a better agreement with the observations the simulation without p wb c 2 0 green lines shows indeed a current with a greater vertical gradient than that of the observation in particular at the points located in the shallowest depths increasing the vertical mixing coefficient associated with p wb reduces the current shear in the breaking zone we therefore retain at this stage that this approach can be used to improve turbulence in the wave breaking zone however in the state of our knowledge the setting of c 2 is arbitrary and would need to be estimated on other experiments including in situ data such as current profiles or more direct observations of turbulence costa et al 2017 4 5 the case of a barred beach with rip currents the interaction of waves and rip currents is an example of the complexity of possible interactions between current and waves weir et al 2011 the model is now applied to a laboratory experiment describing the propagation of monochromatic waves over a beach with a sandy bar and channels inducing rip currents haller et al 2002 haas and svendsen 2002 this experiment has already been simulated with hydrostatic circulation models coupled with a phase averaged wave model haas and warner 2009 haas et al 2003 michaud et al 2012 bruneau 2009 like these previous authors we are interested here in test b of haller et al 2002 the basin and its bathymetry are described in detail in the aforementioned articles the dimensions of the basin are approximately 17 m cross shore direction by 18 m alongshore direction the cross shore slope is 1 30 a bar located a few meters from the coast is cut by 2 channels ideally the two channels should have an identical shape and the bathymetry of the entire basin should respect an alongshore symmetry the central bar is twice as long as the two half bars touching the side walls and outside the bar the alongshore gradient is supposed to be nil however as pointed out by haas et al 2003 an important feature of this bathymetry is that it actually has irregularities compared to its idealized design resulting in dissymmetry of the flow and increasing instability of the currents the two rip currents that form at the exit of the channels are for example clearly different in their form and their intensity and their respective temporal variations seem rather decoupled a bathymetry interpolated from a survey of the wave basin is used here to better respect the real conditions of the experiment the horizontal resolution is 0 06 m and the vertical grid has 10 sigma levels the time step is 1 1 10 2 seconds monochromatic waves of period 1 s propagate in the direction perpendicular to the coast fig 13 the amplitude of the excited waves at the open boundary is adjusted to obtain a wave height corresponding to the conditions of test b namely about 4 1 cm at the most offshore measurement points see fig 14 around x 8 m their wavelength at the entrance of the domain where the depth is about 0 38 m is about 1 4 m fig 13 in good agreement with the linear non hydrostatic wave theory it may be noted that the hydrostatic approximation under the same conditions would lead to a wavelength of about 1 9 m i e an overestimation of about 35 consistently with haas and svendsen 2002 and haas et al 2003 the simulation lasts 30 min to highlight the observed temporal variations of rip currents the wave height is obtained from the difference between the minimum and maximum of the surface level obtained over time intervals of 1 s an average wave height is then calculated by averaging these values obtained every second over the simulation except the first minute to avoid the spin up phase table 2 summarizes the characteristics of the numerical grid and the calculation time the latter can be roughly extrapolated to more realistic applications considering that the scale ratio between this simulation and that of a realistic case with for example dx 3 m is about 50 in other words an area of about 1 km 1 km and a simulation duration of about 1 day the wave height and mean surface height calculated by the model are compared to the observations of the test b case figs 6 and 16 of haller et al 2002 the wave height first increases slowly towards the coast then increases sharply when the waves arrive above the bar fig 14 the breaking conditions are quickly reached and once the bar crossed the wave height is much lower and relatively constant figs 13 and 14 a second breaking zone is highlighted near the coast this behaviour of the model is in good agreement with the description of the process given in haller et al 2002 in the channel axis the wave height increases before the channel as shown in figs 13 and 14 because of the blocking by the opposite rip current fig 15 shows the horizontal distribution of the depth averaged current that is very similar to the circulation calculated with the phase averaged modelling presented by haas et al 2003 their fig 5 and haas and warner 2009 their fig 7 the rip current slows down the waves causing the waves to refract towards the centre of the channel fig 13 in agreement with the visual observation of the laboratory experiment reported by haas et al 2003 hereafter h03 see their section 6 however the rip current does not lead to a wave breaking more important than the one above the bar the wave height actually decreases more slowly in the channel than above the bar so that in the last meters before the coast the waves are higher in the channel axis than behind the bar fig 14 the mean surface level increases at the coast inducing a change in the average position of the water line allowed by the wetting drying scheme of the model the mean surface level at the coast is markedly different in the channel axis and in front of the bar in agreement with the observation fig 14 as haas and svendsen 2002 point out this difference is associated with a longshore pressure gradient that drives the flow to the channels laboratory experiments show that rip currents are unstable and are characterized by large temporal variations the authors of these experiments emphasize the impossibility of reproducing twice the same series of measurements even if the experimental conditions are a priori unchanged an exact agreement with the model is therefore not sought nevertheless we expect from the model that it gives the correct orders of magnitude of the rip currents and their variability fig 16 shows the temporal evolution of the current averaged over the vertical and over a wave period in the two channels given the above limitations the agreement with the observations presented in h03 see fig 11 a in h03 is good the rip current at y 4 6 m is a little overestimated but as in h03 it is mostly lower than the rip current at y 13 6 m the latter has the good order of magnitude with maximum around 20 cm s the amplitude and time scale of its variability are also close to the behaviour described in h03 the occurrence of the rip current is of the order of 200 s without being really regular the variability of the rip current is of the order of a few centimetres per second with a maximum standard deviation around 6 cm s in the centre of the channel at y 13 5 m and x 11 5 m fig 17 the vertical structure of the time averaged cross shore current in the rip channel fig 18 has similarities with the phase averaged modelling of haas and warner 2009 their fig 11 with a maximum intensity current around 0 2 m s between x 11 m and x 12 m at the point of maximum intensity x 11 5 m the current is fairly homogeneous on the vertical except near the bottom where the current decreases strongly in good agreement with the observation haas and svendsen 2002 their figs 17 18 19 at the exit of the channel offshore direction the current decreases significantly the vertical gradient also becomes larger and close to the bottom at x 9 m the current is even reversed positive values in fig 18 in good agreement with the observations haas and svendsen 2002 their figs 13 and 14 5 conclusions a phase resolved wave model is derived from an ocean circulation model with the aim of performing simulations of current wave interaction in nearshore areas the model is evaluated on the fundamental aspects of wave dynamics namely mainly the dispersion properties the transformation of wave characteristics by bathymetry shoaling wave breaking the influence of waves on low frequency fields such as the mean surface levels and rip currents with the exception of the post breaking zone that can be processed in some models by a hydrostatic approach the phase resolved wave modelling requires a non hydrostatic model as the representation of the non hydrostatic problem in all its complexity seems to be inevitably very expensive to date klingbeil et al 2018 a scheme allowing a reasonable compromise between on the one hand the extra cost of the computation and on the other hand an acceptable loss of accuracy of non hydrostatic properties of waves has been introduced into the circulation model it is shown that relative errors of less than a few on the dispersion relation and the vertical current profile can be obtained with non hydrostatic simulations costing 5 to 6 times more than a hydrostatic model calculated under the same conditions the non hydrostatic additional cost remains obviously high but low enough to consider simulations of realist events such as storms on very large areas and it is anyway worth noting that wave current effect modelling based on a hydrostatic circulation model normally couple the latter with a phase averaged wave model whose cost is possibly not negligible the non hydrostatic method implemented in the circulation model is derived from the method proposed by lee et al 2006 in practice the classical poisson problem is transformed into a pressure propagation equation the resolution is iterative local and does not need to solve a system of linear equations an essential aspect of this method is to properly control the various parameters related to the iterative aspect of the problem the equation of pressure propagation depends on a constant the latter being comparable to a propagation velocity which determines the speed of the adjustment of the non hydrostatic pressure to a change of the hydrostatic terms in the momentum equations the precision of the non hydrostatic behaviour depends on the fact that this adjustment is faster than the variation of the other fields a meticulous adjustment of the time step of the model is in practice necessary taking into account various parameters including the wave propagation speed and a possible tuning consisting in reducing the non hydrostatic pressure gradient so far the reduction factor is a constant finding of a compromise in a set of possible values taking into account a range of periods and depths future development will adapt this tuning to the wave peak period and bathymetry using the analytical reduction factor solution that this study established from the linearized acm equations the model is tested on idealized simulations and laboratory experiments the ability of the non hydrostatic method to accurately represent the dispersion properties of surface waves is first demonstrated in the context of the linear theory hamilton and ebersole 2001 laboratory experiment is used to evaluate the model on the issue of wave height amplification associated with beach slope as well as wave breaking the parameterization of the wave breaking is based on momentum horizontal diffusion trigger conditions being related to a froude number the tuning of the horizontal viscosity is consistent with considerations on the mechanical energy balance the reasoning can be extended to the possible production of turbulent kinetic energy in the turbulent closure scheme leading in practice to the increase in vertical turbulent viscosity if the vertical profile of the residual alongshore current can apparently be slightly improved on the other hand the fraction of the kinetic energy lost by the breaking wave which can in this way be injected into the vertical turbulence remains uncertain for the authors and requires obviously to be better understood along these lines a part of the wave spectrum will still be missing in our future realistic applications because of the limited grid resolution the smallest periods being concerned with as a result a possible underestimation of the bottom stress in shallow areas that should be taken into account finally the haller et al 2002 laboratory experiment with its bar and its two channels makes it possible to evaluate the model on the rip current problem the model correctly reproduces rips currents and their high temporal variability the feedback of the rip currents on the waves in terms of wave amplification and refraction is well represented the onset of wave breaking is also well simulated either above the bar or in the channel axis this paper does not consider some 3d aspects as the effect of tracers on the pressure gradient and turbulence which will be taken into account in future applications of this model this also includes sediments that may be of great importance for modelling of rip currents as shown by ma et al 2014 these authors show that a wave resolving non hydrostatic model is an efficient approach for the problem of resuspension of sediments by waves and the possible feedback introduced by the modification of the equation of state by sediments which can have a significant influence on the pressure gradient and the level of turbulence in practice our wave model will be coupled with the mustang sediment model le hir et al 2011 the next step is now to implement our model on realistic beaches where we will confront the results with in situ data infragravity waves and the risk of submersion are possible applications some developments are still needed to provide a realistic wave spectrum to the boundary conditions of the phase resolved wave model the phase averaged model which can be applied over large domains at a reasonable cost appears to be a good candidate for providing the boundary conditions to the phase resolved model the phase averaged wave model can also provide an estimate of the missing part of the wave spectrum in the deterministic model helping the former to improve bottom turbulence in shallow areas finally wave growth by local wind action will eventually be considered in the perspective of large areas deigaard and nielsen 2018 acknowledgements the authors acknowledge the support of the sirocco team http sirocco obs mip fr the numerofix project numerical modelling of real ocean surface mixing funded by lefe gmmc the hpc resources from calmip grant 2018 p1325 and from genci and cines grand equipement national de calcul intensif project a0040110088 appendix a the discrete version of 2 q x 2 at the left hand side of eq 4 is given by the left side of a1 q x dx q x dx 2 q x d x 2 2 q x 2 dx 2 12 4 q x 4 the right hand side of eq a1 is given by a taylor development dx 2 12 4 q x 4 being the leading term of the errors made by the numerical scheme assuming a sinusoidal shape 4 q x 4 k 4 q k the wave number leads to eq 5 appendix b time stepping scheme the equations are calculated with a forward backward time stepping scheme the alternation of the fields along the time axis is presented in fig 19 the height of the surface at time t η t is calculated first knowing η t 1 and velocities at time t ½ eq 9 gives q t knowing q t 1 q t 2 and hydrostatic tendencies at time t 1 note that the vertical laplacian of q is computed with an implicit centric scheme of the type 1 2 2 q t 2 z 2 1 2 2 q t z 2 so as not to limit the time step when the vertical mesh becomes very small which becomes the case in sigma coordinates when the bathymetry tends to zero the vertical velocity at time t ½ used for the advection terms of the equations for u and v is computed from u and v at time t ½ finally the velocities at time t ½ are calculated from the velocities the sea surface height and the nonhydrostatic pressure of the previous times vertical turbulent diffusion terms are calculated with an implicit scheme fig 19 chronology of the variables of the model at the beginning of each iterative cycle the fields at time t ½ and time t are known fig 19 appendix c the discrete version of the left hand side of eq 30 is given by the left side of c1 η t dt η t dt 2 η t d t 2 η 2 t dt 2 12 η 4 t the right hand side of eq c1 is given by a taylor development exponents in brackets indicating the order of the derivatives dt 2 12 η 4 t being the leading term of the errors made by the numerical scheme assuming a sinusoidal shape η 4 t ω 4 η t ω the wave frequency leads to dt 2 12 η 4 t dt 2 12 ω 4 η t similar reasoning can be done on the right hand side of eq 30 the leading error term of the corresponding numerical scheme is c 2 dx 2 12 4 η x 4 equivalent if a sinusoidal shape is assumed k the wavenumber to c 2 dx 2 12 k 4 η and c 2 dx 2 12 k 2 2 η x 2 then reformulate eq 30 using the first two terms of the taylor developments leads to eq 31 appendix d the potential energy flux is f guη with u gη c a sinusoidal form sinψ is assumed leading to f g 2 1 c η 0 2 sin 2 ψ a wave period averaging gives f g 2 1 2 c η 0 2 within the wave breaking zone we assume 2 η 0 h γ using c gh leads to f 1 8 g 3 2 γ 2 h 3 2 the horizontal gradient is d1 f x 3 16 g 3 2 γ 2 h 1 2 h x consider a constant diffusion coefficient ν b the dissipation term due to horizontal diffusion is ε b ν b u x 2 assuming a constant wavenumber leads to u x k u 0 cos ψ and ε b ν b k 2 u 0 2cos2 ψ a wave period averaging gives ε b ν b k 2 u 0 2 1 2 ν b k 2 1 2 g η 0 c 2 using 2 η 0 h γ leads to d2 ε b ν b 1 8 kgγh c 2 using k ω c 2 π ct t the wave period the equilibrium of eqs d1 and d2 leads to eq 35 
24044,a phase resolved wave model is derived from an ocean circulation model for the purpose of studying wave current effects in nearshore zones one challenge is to adapt the circulation model to the specificities of wave physics this mainly concerns the consideration of non hydrostatic effects and the parametrization of wave breaking the non hydrostatic pressure is calculated using the artificial compressibility method acm the acm induced errors on wave dispersion properties are examined in detail in the context of the linear theory using idealized test cases the possible compromise between the precision achieved on non hydrostatic physics and the adjustable cpu cost of the acm method is looked at in detail the modification of the wave characteristics by the bathymetric slope and the breaking of waves are then examined from a linear slope beach laboratory experiment finally the model is evaluated on the issue of rip currents and their feedback on the wave field using a laboratory experiment of a beach with a bar intersected by channels keywords phase resolved wave modelling wave breaking rip currents non hydrostatic ocean circulation model 1 introduction the interactions between waves and current play a major role in coastal modelling studies and are important to correctly reproduce the nearshore circulation coastal engineering applications need realistic descriptions of waves and currents their transformations towards the nearshore and behaviours with artificial coastal structures for the design of harbours or dykes to prevent overtopping and to reduce submersion or erosion risks and this is only possible with a circulation model that reproduces explicitly the waves taking into account the waves in the circulation models allows on the one hand the representation of the residual flows generated by the waves such as the stokes drift mellor 2003 or the rip currents castelle et al 2016 and on the other hand to take into account turbulence generation by waves in the turbulent closure scheme which improves the oceanic surface and bottom boundary layers the vertical current shear and the vertical temperature and salinity profiles in the circulation model uchiyama et al 2010 in shallow waters when the waves effects near the bottom are no longer negligible the bottom stress is significantly increased allowing the resuspension of sand and sediments soulsby 1995 the stokes drift the undertow or rip currents then playing an important role in their transport in the water column the wave current interaction is also decisive for the mean surface level set up during storms the modification of the surface stress related to the roughness created by the waves the modelling of the marine submersion of the coastal continental surfaces bertin et al 2012 the modification of the coastline the evolution of the shape of the seabed bouharguane et al 2010 at the spatial scales of the coastal circulation models see review by klingbeil et al 2018 such studies are mainly done using a hydrostatic circulation model coupled with a spectral phase averaged wave model mcwilliams et al 2004 ardhuin et al 2008 bennis et al 2011 benetazzo et al 2013 kumar et al 2015 uchiyama et al 2017 the phase averaged wave model feeds the circulation model with stokes drift residual wave pressure momentum and turbulence production terms related to wave erosion in return the circulation model gives the wave model the variations of sea surface level and surface current the latter being able to significantly modify the wave field uchiyama et al 2009 as in the known example of rips current the choice of a phase averaged wave model is largely guided by computational time considerations as it provides mean properties of the sea state the spectral model can indeed be used with a horizontal resolution much lower than the wave wavelengths and therefore cover large areas with a reasonable number of grid points an alternative is the phase resolved model or deterministic model that explicitly describes the evolution of the free surface the swash models zijlema et al 2011 ref dif kirby and dalrymple 1983 funwave kirby et al 1998 bosz roeber and cheung 2012 nhwave ma et al 2012 are widespread examples of this type of model very often mentioned in the wave modelling literature the processes resulting from the effects of bathymetry such as refraction diffraction and reflections magne et al 2005 interaction with other waves or with the background circulation infragravity waves bertin et al 2018 the asymmetrical wave transformation at the approach of the breaking phase are in principle better represented by this type of model rusu and soares 2013 the phase resolved model is also well suited for sand mobilization which is directly dependent on orbital velocity near the bottom bouharguane et al 2010 the phase resolved model has a continuous approach of frequencies and directions while the phase averaged model solves its equations on a discrete spectrum of frequencies and directions which results in a relative inaccuracy related to frequency and direction resolution for example in michaud et al 2012 the wavewatch iii r model tolman et al 2016 is used with 36 discrete values of the direction leading to a truncation of the direction of the waves of the order of 10 however the phase resolved models require finer temporal and spatial discretisation thus larger computational resources the resolution of the horizontal grid imposes a limit on the permissible wavelengths truncating the shortest periods of the wave spectrum deterministic nearshore models should have accurate dispersion and non linear properties and thus should be governed by the navier stokes equations raoult et al 2016 using either an eulerian approach e g openfoam zhou et al 2017 or a lagrangian approach e g the meshless model sph model oger 2006 these models are appropriate for local studies although eventually limited by spurious diffusion or prohibitive computational time benoit et al 2017 simplifications can be made to overcome these problems for instance ref dif based on the mild slope equation berkhoff 1972 neglects some nonlinear effects swash can be used with depth averaged equations with appropriate assumptions to introduce the non hydrostatic pressure zijlema et al 2011 boussinesq serre 1953 or green naghdi green and naghdi 1976 models take partially into account nonlinear or dispersive effects e g bonneton et al 2011a 2011b tissier et al 2012 within the limits of the above simplifications phase resolved models should theoretically be capable of simulating the low frequency 3d processes usually modelled by circulation models since their equations are basically the same except for the hydrostatic assumption generally made in circulation models klingbeil et al 2018 as a result the phase resolved wave model is able to generate residual currents such as rip currents and therefore does not require coupling with a circulation model to simulate the coupling between waves and low frequency fields however it requires a significant horizontal and vertical resolution greater than the wavelength of the waves which limits the size of the modelled domain to date and to our knowledge most of the applications of the phase resolved models seem to concern domains whose horizontal dimensions are of the order of a few kilometres in each horizontal direction yoon 2014 the constant rise of computers capability suggests that phase resolved models could now be used on larger domains covering a significant part of the continental shelf meanwhile ocean circulation models currently used at coastal scales could be adapted to deterministic wave modelling with among other objectives the goal of linking waves and continental shelf circulations like wind induced eddies petrenko et al 2008 or to better understand momentum and turbulence transfer in the surface layer deigaard and nielsen 2018 this assumes however that their equations are suitable for phase resolved wave modelling one of the essential points is to be able to use these models in non hydrostatic mode a number of articles deal with this question for the mitgcm model marshall et al 1997 pom kanarska and maderich 2003 roms kanarska et al 2007 getm klingbeil and burchard 2013 symphonie auclair et al 2011 however at the resolutions where these models are most often used the consideration of non hydrostatic effects may not be an essential issue mckiver et al 2016 the applications envisaged so far concern more the propagation of internal gravity waves bordois et al 2017 or the convective processes paluszkiewicz et al 1994 than the high frequency surface waves one of the obstacles to the deterministic modelling of waves over large domains is the cost of the non hydrostatic solver klingbeil et al 2018 as previously mentioned in the field of phase resolved wave modelling the common alternative to this difficulty is to use simplified models adapted to the physical specificities of waves bonneton et al 2011a 2011b in the field of ocean circulation modelling the most widespread approach to the non hydrostatic problem on the contrary is to take into account the whole complexity of the problem it leads concretely to the resolution of a poisson equation for the non hydrostatic pressure this is a global approach insofar as this leads to a system of linear equations making all the points of the numerical domain interdependent its resolution is in principle costly and currently constitutes a challenge in the perspective of large numerical domains of several thousand points in each direction roullet et al 2017 the sigma coordinate widely used in coastal modelling also increases the cost of the resolution for two main reasons the first reason is that the time variation of the position of the vertical levels related to the movement of the free surface forces to update regularly the coefficients of the main matrix of the system and its possible preconditioning the possible representation of the wetting drying zones adds a similar difficulty the second reason is that the sigma coordinate transformation increases the number of nonzero coefficients of the principal matrix roullet et al 2017 propose to reduce this number by reformulating the momentum equations the same authors also evoke the perspective of the multigrid approach to significantly reduce the cost of calculation an application in the vertical direction is proposed by shi et al 2015 the alternative to the global approach is the local approach based on the direct resolution of non hydrostatic pressure various methods have been proposed so far johns 1991 klingbeil and burchard 2013 bordois et al 2017 lee et al 2006 among them lee et al 2006 hereafter l06 acm artificial compressibility method method has a reasonable numerical cost and its implementation in a model is easy a method derived from l06 is implemented in the symphonie circulation model marsaleix et al 2008 and tested in the aforementioned context namely the high frequency surface waves and the associated residual circulations the purpose of the present paper is to assess this model on the principal issues of wave modelling this concerns on the one hand the dispersion properties the vertical shear of the current the asymmetrical wave transformation at the approach of the breaking conditions the decay of the waves in the breaking zone on the other hand this concerns the residual waves related circulation such as the stokes transport the alongshore coastal drift the mean sea level rip currents and their feedback on the waves the article is organized as follows section 2 deals with the implementation of the acm method in the ocean circulation model section 3 gives an analytical solution verifying the acm equations for low amplitude waves section 4 describes academic and laboratory test cases and finally section 5 provides a summary and the conclusions 2 model description 2 1 equations for the sake of clarity the problem is first presented in a simplified way in a vertical 2d plane and presenting only the terms that dominate wave physics 1 u t g η x q x 2 w t q z where q is the non hydrostatic pressure times the water density u and w the horizontal and vertical current η the sea surface height as in l06 the hypothesis of incompressibility is retained to deduce the surface elevation 3 η t x h η udz eq 2 is in practice not calculated by the model instead the combination of eqs 1 and 2 that leads to the poisson equation is used 4 t u x w z g 2 η x 2 2 q x 2 2 q z 2 the left hand side of the poisson equation is ideally zero in the case of the incompressible hypothesis without questioning this hypothesis of incompressibility the particularity of the l06 method is however to allow the left hand side of eq 4 to be small and not to equal zero before detailing the approach of l06 some comments can be made on the possibility of not cancelling strictly the left hand side of eq 4 it can be noted for example that non hydrostatic incompressible models can use iterative solvers whose convergence is de facto imperfect precluding a strict cancellation of the left hand side of eq 4 this approximation is currently accepted because the physical coherence is preserved if the convergence of the solver is sufficiently precise we can also recall that a model such as the one used in this study does not formally represent eq 4 but its discrete approximation in the finite difference sense not strictly cancelling the left hand side of eq 4 induces an error whose order of magnitude can be compared to the discretization errors of the other terms for example given the numerical scheme used here the error made on the horizontal laplacian of q at the right hand side of eq 4 is of the order of appendix a 5 d x 2 12 4 q x 4 d x 2 12 k 4 q where k is the wavenumber this discretization error makes it possible to consider a possible error on the left hand side thus relativizing the question to the transformation of the left hand side of eq 4 into a term of artificial compressibility as described by l06 see eq 2 in l06 in practice the acm method of l06 leads to the reformulation of eq 4 as follows 6 1 α 2 2 q t 2 g 2 η x 2 2 q x 2 2 q z 2 assuming a sinusoidal form for q the artificial compressibility term at the left hand side of eq 6 is of the same order as ω α 2 q c α 2 k 2 q c is the celerity of the waves which under certain conditions can become of an order of magnitude comparable to eq 5 i e c α 2 comparable to d x 2 12 k 2 insofar as it is recommended to use the acm method with α such that c α 2 1 this point is closely examined later in the article in fact ideally in compressible theory the α constant should be the speed of the acoustic waves actually much higher than c however as pointed out by mahadevan et al 1996 this would have the consequence of requiring an extremely small time step highly increasing the computational cost this type of approach therefore generally uses a much lower α value in order to increase the time step and thus makes the cost of the calculation affordable it is therefore noted that the true compressibility of the ocean is not respected by this approach which relies more on numerical considerations than on a real physical justification resulting in the expression artificial compressibility used by l06 on the other hand apart from the eq 6 the other equations of the model strictly respect the incompressibility framework although greatly diminished by the computational time constraint just mentioned the value of α must nevertheless remain sufficiently large to allow rapid adjustment of eq 6 to variations in the hydrostatic forcing terms this property is essential for the accuracy of the non hydrostatic behaviour of the model in practice α must remain larger than the phase speed of gravity waves bordois et al 2017 the resolution of the poisson iterative eq 6 can be done with the same time step as the other equations in practice its determination is done from the stability criterion corresponding to the forward backward scheme but considering α rather than the speed of the gravity waves as it is larger the time step is therefore smaller than the one of a hydrostatic model in similar conditions but the calculation nevertheless remains affordable as we will see in the following because eq 6 is computed only once per time step of the full model 2 2 implementation in the symphonie model the model used is the ocean circulation model symphonie marsaleix et al 2008 as we focus on surface waves the effect of temperature and salinity is neglected for the sake of clarity the equations are given in a cartesian coordinate system but the model actually uses a system of generalized sigma coordinates the momentum equations are given by 7 u t uu x vu y wu z fv g η x r q x z k m u z τ u 8 v t uv x vv y wv z fu g η y r q y z k m v z τ v 9 1 α 2 2 q t 2 x u t y v t 2 q x 2 2 q y 2 2 q z 2 where u t and v t represent hydrostatic terms in practice all the terms of the right hand side of eqs 7 and 8 except r q x and r q y at this stage r at the right hand side of eqs 7 and 8 is equal to one but we will see later that a value other than 1 can be used to improve the accuracy of the dispersion relation and currents of short waves the incompressibility hypothesis gives the vertical velocity and elevation of the surface 10 u x v y w z 0 11 η t x h η udz y h η vdz the vertical diffusion coefficient is calculated by the k epsilon scheme rodi 1987 and τ u τ v are breaking terms parameterized as a horizontal diffusion 12 τ u x ν b u x 1 2 y ν b u y v x 13 τ v y ν b v y 1 2 x ν b u y v x the eddy viscosity of breaking ν b is calculated from a breaking criterion which will be discussed in a section to follow bottom and surface boundary conditions are as follows at the surface q 0 u v z 0 and at the bottom q z 0 ρ 0 k m u v z τ b x τ b y where ρ 0 is the sea water density and τ b x τ b y are the components of the bottom stress the bottom stress is parameterized as in blumberg and mellor 1987 from a quadratic relationship of the bottom current and a drag coefficient derived from a roughness length the equations are calculated with a forward backward time scheme detailed in appendix b considering that short surface waves are intrinsically three dimensional the barotropic baroclinic time splitting commonly used in circulation models for calculating separately barotropic and baroclinic velocities blumberg and mellor 1987 cannot applied here in practice all the equations are calculated with the same time step 2 3 the time step the time step is determined from the stability criterion specific to the forward backward scheme in the case of the hydrostatic model the criterion of stability with respect to surface waves would be 14 dt max dx 2 1 gh max where dx is the horizontal resolution h max is the maximum depth of the domain and therefore gh max is the maximum theoretical phase speed expected for surface waves in the non hydrostatic model the calculation of the equation for q imposes a more drastic additional criterion since it depends on α that is greater than gh max it is not trivial to find a simple expression similar to eq 14 but unsurprisingly it appears in simulations performed by the authors that the maximum value of the time step allowed by the non hydrostatic model was close to dx 2 1 α i e eq 14 with gh max replaced by α in practice in all the simulations presented the time step of the non hydrostatic model is given by a fraction 70 of this criterion to include other stability criteria than gravity waves such as those resulting from non linearities namely 15 dt 0 7 dx 2 1 α the determination of α is therefore related to the determination of the time step the larger the first the smaller is the second the quantity n which represents the ratio of the theoretical time step of the hydrostatic case to the time step of the non hydrostatic case is introduced 16 n dt max dt which using eqs 14 and 15 is equivalent to 17 n 1 0 7 α gh max n is a key parameter of the problem the larger it is the smaller is the model time step and the better is the accuracy expected on the non hydrostatic term since dt max is the time step that would normally have the hydrostatic model in similar conditions the ratio n is a good indicator of the relative additional cost associated with the nonhydrostatic effect the evaluation of this additional cost must also take into account that an additional equation for q has been added to the equations of the hydrostatic model the latter essentially counts 4 three dimensional equations two for the horizontal components of the current one for the turbulent kinetic energy and one for its dissipation rate the relative numerical excess cost of the equation for q is therefore of the order of n 4 1 4 1 25 n 3 analytical solutions analytical solutions verifying model equations in the context of linear theory are now presented we consider a 2d vertical plan a linearized version of eq 7 18 u t g η x r q x the continuity equation 19 u x w z 0 and the non hydrostatic pressure eq 6 the elevation of the surface is of the form η η 0 e i kx ωt the non hydrostatic pressure satisfying eq 6 is of the form 20 q g μ 2 η 2 p 0 e i kx ωt cosh μk z h with 21 μ 2 1 ω 2 k 2 α 2 where α appears in the left hand side of eq 6 and in the definition of n calculated by eq 17 using eq 20 and the surface condition q 0 leads to 22 q g μ 2 η 1 cosh μk z h cosh μkh using eqs 22 and 18 gives the horizontal current 23 u g k ω η 1 r μ 2 cosh μk z h cosh μkh 1 using eq 23 and the continuity eq 19 to derive the vertical velocity then applying the surface condition w η t leads to the dispersion relation 24 ω 2 k 2 g 1 r μ 2 h r μ 3 gk tanh μkh note that if r 1 and if n therefore α tends to infinity leading to the cancellation of the left hand side of eq 6 and μ 1 the dispersion relation eq 24 becomes equivalent to the usual dispersion relation kinsman 1965 25 ω 2 gk tanh kh while using eqs 23 and 25 we find the usual solution for the horizontal current kinsman 1965 26 u ωη cosh k z h sinh kh this result shows that the acm method becomes equivalent to the classical non hydrostatic incompressible method if n is sufficiently large in the following we distinguish the dispersion relations of the incompressible non hydrostatic linear theory i e eq 25 of the non hydrostatic acm theory i e eq 24 and of the hydrostatic theory i e eq 27 27 ω k gh fig 1 shows these different dispersion relations for periods ranging from 6 s to 14 s this choice will be discussed in the next section for different bathymetry values and n 5 the acm theory with r 1 provides a considerable improvement over the hydrostatic theory but overestimates the non hydrostatic effect fig 1 the dashed red curve is always below the green curve this overestimation increases with bathymetry at shallow depths the agreement is very satisfactory but the non hydrostatic effect is also less significant fig 1 h 10 m the parameter r is a way to bring acm theoretical solution closer to the non hydrostatic incompressible theory frequencies produced by eqs 24 and 25 are actually equivalent if 28 r μ 3 tanh kh kh tanh μkh μkh horizontal currents eqs 23 and 26 are identical on the surface without condition on r and μ while near the bottom they are equivalent if 29 r μ 2 1 cosh kh 1 1 cosh μkh 1 1 expressions 28 and 29 suggest the possibility of optimizing r according to the local values of the depth and the dominant period of waves this point will be addressed in future applications at this early stage of our study r will be considered constant fig 2 shows the absolute value of the relative error on the wavelength l acm l l as a function of the period l acm being deduced from eq 24 and l the true wavelength from eq 25 relative errors are more important for small periods and for large bathymetry values errors of the order of several are expectable with r 1 on the other hand it is possible to reduce the error level below 1 for all the values of period and bathymetry considered in fig 2 with r 0 9845 the fact that eqs 28 and 29 are not identical shows that it is not possible to strictly cancel the error on the current and on the dispersion relation with the same value of r however as shown in fig 3 for h 50 m and a period of 10 s eqs 28 and 29 give very similar values of r so that it is possible to find a value of r that reduces both the current and dispersion relation errors this will be discussed in the next section finally we note that a compromise on these two possible values of r is all the easier to find if n is large because eqs 28 and 29 converge both to r 1 in conclusion the analytical solutions corresponding to the linearized equations of the acm method show a satisfactory representation of the non hydrostatic effect in the range of periods concerned by the future applications of the model to the near shore zone the accuracy however depends on the setting of parameters such as n and r since the accuracy of a numerical model also depends on many other factors related to the discretization of equations such as spatio temporal resolution and relevance of numerical schemes the above theoretical analysis is now completed with numerical experiments 4 test cases 4 1 wave propagation in the context of linear theory in this section we check the ability of the model to correctly represent the physical characteristics of the simulated wave thanks to a series of test cases consisting in exploring the dispersion properties of the waves we consider here low amplitude waves 1 mm in order to be within the framework of the linear approximation for which simple analytical solutions exist kinsman 1965 the objective is to define the values of n which lead to a reasonable compromise between the computation cost consideration encouraging to lower n and the precision of the dispersion properties consideration encouraging to increase n we use a domain with a flat bottom at 50 m depth a periodic condition in the direction normal to wave propagation the horizontal resolution is 5 m and 20 vertical levels are used the waves are generated at the entrance of the domain while the fields are at rest at the initial state the opposite boundary is closed but the dimension in the direction of propagation is large enough so that the waves do not have time to reflect on it before the end of the simulation 10 min this first numerical experiment is carried out with r 1 the open boundary conditions are derived from marsaleix et al 2006 as recommended by blayo and debreu 2005 an outgoing field is defined as the difference between the total field and the incoming field we thus define anomalies of currents sea surface height and non hydrostatic pressure u u u w η η η w q q q w where u w η w q w are prescribed by the analytical solution of the short wave linear theory we then assume that the incoming short waves do not reflect at the coast because of dissipation and that waves propagating offshore are at lower frequencies they may consist of infra gravity waves if the conditions of their generations are met bertin et al 2018 the radiative conditions u gη c and q t c q x 0 are therefore applied using the long wave phase speed c gh the simulation is analyzed in order to extract the modelled wavelength and to compare it with that predicted by the theoretical dispersion relation of the incompressible linear theory eq 25 the modelled wavelength is diagnosed from the distance separating two successive identical values of the surface level in the direction of propagation the grid points chosen for the calculation of this diagnosis are located beyond a distance of 625 m from the open boundary and moreover the diagnosis is only calculated once the transient regime is reached i e once the wave front has passed these points a time average of the values obtained is finally calculated a set of simulations is done to estimate the precision on eq 25 as a function of n ranging from 2 to 20 and of the wave period the smallest period considered here is 6 s associated with a wavelength of 56 m for h 50 m represented by 11 grid points given the horizontal resolution of 5 m this minimum value is considered as a reasonable threshold with respect to the capacity of the c grid to represent the small wavelengths the maximum period considered is 14 s which is associated with a wavelength of approximately 250 m periods 14 s are not considered because their wavelengths much larger than the depths envisaged render them slightly non hydrostatic and therefore not interesting from the point of view of the authors fig 4 shows the relative error on the wavelength l l l where the true wavelength l is given by the incompressible linear theory eq 25 and l is either deduced from the model or from the acm linear theory eq 24 with r 1 errors decrease when n increases as expected from the previous section the errors are stronger when the bottom depth is important this could be an issue near the model open boundaries where the most important depths are normally expected to our knowledge near shore modelling studies mostly focus on areas where bathymetry does not exceed a few tens of meters we thus assume that 50 m could be a plausible upper limit for realistic models of the near shore zone at least for the time being in the following the case h 50 m is the subject of greater attention the model errors are generally similar to those predicted by the acm linear theory fig 4a and c a notable difference is that errors tend to zero when n increases in the case of the theory but not in the case of the model this is explained by additional sources of error of numerical origin in the case of the model these numerical errors are more important for small periods for example for a period of 6 s the residual error for large values of n is around 3 for h 50 m and around 2 for h 10 m fig 4a and b the agreement between the model and the acm linear theory is less good for h 10 m especially for a period of 6 s fig 4b and d the relative predominance of numerical errors is here related to the decrease of the wavelength caused by the decrease of h for a period of 6 s and h 10 m the wavelength is indeed around 48 m i e 10 grid points while the wavelength is around 56 m i e 11 points when h 50 m actually when h decreases causing the wavelength to decrease the resolution of the horizontal grid eventually becomes insufficient to represent the shortest waves unless wave breaking occurs before this critical situation model errors are also sensitive to vertical resolution the accuracy of the vertical profile of q and current depends on the number of vertical levels the shortest wavelengths are particularly sensitive to this error because the vertical profile of q is more non linear and is established on a smaller depth fig 5 shows that for the 6 s period the relative error is much smaller with 80 levels than with 10 or 20 using more vertical levels reduces the residual error but does not completely eliminate it the error levels obtained with 40 levels or 80 levels are however equivalent for larger periods results not shown the sensitivity of the error to the vertical resolution is smaller plausibly because the vertical profile of the fields whose characteristic scale of decay is related to the wavelength is less sheared the minimum error level around 1 5 in fig 5 case 80 levels n 20 is attributable to the time stepping method itself and no longer depends on the non hydrostatic aspects this aspect of the error can be examined in the hydrostatic model by re invoking the simple assumptions we made in section 2 1 combining the eq 1 now without term q and eq 3 the surface elevation is given by 30 2 η t 2 c 2 2 η x 2 given the numerical schemes used by our model the error is respectively of the order of dt 2 12 4 η t 4 dt 2 12 ω 4 η and c 2 dx 2 12 4 η x 4 c 2 dx 2 12 k 4 η for the left and right hand sides of eq 30 these errors are estimated from the first terms of a taylor expansion appendix c the compensation of temporal and spatial discretization errors is a characteristic of this type of discrete equation lemarié et al 2015 here we see that the two above errors are compensated when the time step is at its theoretical maximum i e dt dx c with c ω k when the time step is significantly smaller consequence of the increase of n the time dependent error becomes negligible and the error is reduced to the error of the right hand side c 2 dx 2 12 4 η x 4 also equivalent to c 2 dx 2 12 k 2 2 η x 2 and then the discretization of the eq 30 equation tends to behave like appendix c 31 2 η t 2 c 2 2 η x 2 c 2 dx 2 12 k 2 2 η x 2 so that the modelled phase speed approaches c 1 dx 2 12 k 2 c 1 dx 2 24 k 2 the relative error on the phase speed is therefore of the order of dx 2 24 k 2 2 π n 2 24 where n is the number of grid points per wavelength for the shortest wavelengths considered for which n is around 10 the relative error on c is therefore of the order of 1 5 given that the period is imposed by the forcing this level of error is therefore the one expected for the modelled wavelength and in fact it corresponds fairly well to the residual error for n 20 and 80 levels see fig 5 in summary the accuracy of the dispersion properties increases significantly with n until acm related errors become small compared to the other error sources mainly the vertical resolution and the order of precision of the numerical schemes used to calculate spatial and temporal derivatives the determination of n would lead to choosing n around 14 in the case of the experiment considered another way to choose n is to define an acceptable precision level on the dispersion properties depending on the period of the simulated waves for example a relative error of 3 is obtained for n 7 for a period of 8 s n 5 for a period of 10 s etc 4 2 improved accuracy apart from increasing n 4 2 1 distribution of vertical levels the previous paragraph shows that the accuracy decreases appreciably with the wave period at the shortest periods accuracy is more sensitive to vertical resolution increasing the number of vertical levels is an obvious but expensive way to reduce this second source of error modifying the vertical distribution of levels without changing their number is an alternative that allows a significant improvement in accuracy at small periods in the case of the 6 s period fig 5 shows that 10 irregularly distributed levels see table 1 provide a much higher accuracy than 10 equidistant levels and makes it comparable to the precision obtained with 20 equidistant levels the vertical distribution proposed here is such that the accuracy is also improved or maintained for the other periods considered results not shown it should be noted that in a realistic domain the generalized sigma coordinate makes it possible to adapt the vertical distribution to the bathymetry for example in areas where bathymetry is low relative to wave wavelength the vertical profile of current and pressure is less sheared and a more regular distribution of the levels is preferable to the irregular distribution of table 1 given the positive influence of increasing vertical resolution on accuracy the increase in vertical resolution that results from the decrease in bathymetry in the sigma formulation leads to a better representation of dispersion properties when approaching the coast 4 2 2 influence of horizontal resolution increasing the horizontal resolution without changing the other parameters especially n leads logically to decrease the level of error especially at short periods significantly increasing the horizontal resolution has a real but limited impact on the accuracy because of other sources of error such as the lack of vertical resolution the period 6 s is more critical than the other periods tested because as noted above the number of grid points per wavelength is particularly low doubling the resolution allows about 22 grid points per wavelength for h 50 m fig 4 shows that a relative error 3 is obtained with n 10 for a horizontal resolution of 2 5 m whereas this same level of error requires n 14 for a horizontal resolution of 5 m 4 2 3 tuning the nonhydrostatic pressure gradient a modification of the pressure gradient scheme is now proposed to use moderate n values while maintaining an acceptable level of accuracy on dispersion properties we consider values of n around 5 the proposed modification is suggested by the observation that the error on the wavelength is systematically negative figs 4 and 5 this is equivalent to the fact that the modelled phase speed is systematically lower than the theoretical phase speed in other words the non hydrostatic effect excessively decreases the phase speed this conjecture can be interpreted as an overestimation of the non hydrostatic component of the pressure gradient in agreement with the conclusions drawn from the analytical solution of the linearized acm equations section 3 fig 1 the proposed improvement consists in moderating the nonhydrostatic pressure gradient by multiplying the latter by a constant r slightly 1 see the non hydrostatic pressure gradient terms in eqs 7 and 8 fig 6a obtained with n 5 dt 0 032 s shows that the precision is improved when r decreases to a certain level below which the sign of the error is reversed becoming positive and the error increases again as expected from the acm linear theory the value of r that exactly cancels the error is not the same depending on the period it is around r 0 982 for the 6 s period and smaller for the other periods the general behaviour is nevertheless quite similar for all the periods and one can identify a unique value of r which makes it possible to reduce the error below 1 for all the periods fig 6a r 0 98 which corresponds to a very significant reduction of the error especially for small periods error of 8 and 5 for periods of 8 s and 6 s respectively with r 1 the general behaviour of the model is in good agreement with the analytical solution fig 6b however since the model has other sources of errors than those of the theoretical model fig 6a and b are not strictly the same and the best compromise for the value of r would be slightly larger according to eq 24 around 0 984 fig 6b although the theoretical model could certainly be used for a default setting of r fig 6a shows that errors can be further reduced with an empirical setting this numerical estimate of the best compromise on r is therefore used in the following as pointed out previously a realistic numerical domain is characterized by variable bathymetry as 50 m should be an upper limit of a near shore zone model in our future applications we now verify that the value r 0 98 which reduces the error below 1 for all periods when h 50 m remains relevant for lower values of the water depth these tests are done keeping the same time step dt 0 0 32 s and n 5 fig 7 obtained for h 20 m shows a precision greater than the simulation with h 50 m under the combined effect of the increase of the vertical resolution the same number of vertical levels distributed over a shallower water column and also the decrease in the non hydrostatic effect caused by the decrease in the ratio between the horizontal scale and the vertical scale if fig 7 suggests that the best value of r would be around 0 978 the minimum relative error is indeed close to this value for most of the periods considered the proposed value of 0 98 for h 50 m remains relevant for shallower depths because it reduces the relative error associated with the different periods between 0 25 and 0 note finally that the choice of the 50 m value as the maximum value of the bathymetry is somehow arbitrary there are certainly other ways of defining the offshore limit of the near shore zone such as the depth at which the waves are modified by the seabed in this case a maximum value based on the relative depth kh may be preferable the ability to strongly reduce or even cancel the error via the r parameter means that this parameter can be used to correct errors beyond the single error caused by the acm method the errors associated with the vertical discretization or the time stepping scheme can therefore be compensated by this method we thus examine the possible undesirable consequences of this compensating effect on other important properties of wave physics especially since according to the theoretical model the cancellation of the errors on the current and the relation of dispersion is not obtained strictly with the same value of r e g eqs 28 and 29 recall also that the shape of the vertical profile of the current is of particular interest since the intensity of the current near the bottom is crucial for the determination of the bottom friction the resuspension of sediments near the bottom the transport of sand among others fig 8a shows that for the period 6 s r 0 98 leads to a vertical profile of the current more accurate than with r 1 the latter nevertheless correctly reproduces the vertical shear the value r 0 986 being the one that would lead to a profile closest to the profile predicted by the linear non hydrostatic incompressible theory a similar conclusion is made for the period 10s fig 8b for r 0 976 the fact that a single value of r can both reduce the error on the dispersion relation and the current is finally consistent with fig 3 which shows that the values of r cancelling these two types of error are actually very close 4 3 test case with bi periodic domain we are now replacing open and closed boundary conditions at both ends of the domain in the propagation direction by periodic conditions the interest of this bi periodic domain is that it avoids reflections at the boundaries that could degrade the results the simulations can therefore be long which is of interest for testing the stability of calculations in the long term the depth is 50 m the domain has 510 points in the direction of propagation corresponding to a whole number of wavelengths here 17 for a wavelength of 150 m a theoretical period close to 10s and a phase speed close to 15 m s the fields are initialized from the linear theory the amplitude of the surface elevation is small 1 mm to remain within the framework of the linear theory nonlinear dissipation forces such as friction using a quadratic parameterization or breaking are therefore negligible in this particular case the laplacian temporal filter derived from marsaleix et al 2012 used in the equations for u v q to suppress the numerical mode is the main source of dissipation the following test quantifies its adverse effect on the long term physical mode fig 9 shows the amplitude of the surface elevation at different times given the value of the phase speed a distance of the order of 10 km cross shore size of realistic domains that we plan to use with this model is covered in about 10 min fig 9 shows that the attenuation is negligible over such a short time duration an attenuation of 10 would require at least 6 h during which the waves would travel a distance of about 300 km in our future realistic applications the open boundary condition will not be located so far from the coast the cross shore domain dimension should be at least an order of magnitude lower so this numerical attenuation should be 1 in other words small compared to the expected erosion induced by bottom friction turbulence or wave breaking 4 4 the case of a linear slope beach the hamilton and ebersole laboratory experiment hamilton and ebersole 2001 hereafter he01 svendsen et al 2003 focuses on the propagation of waves over a uniform linear slope beach their breaking and the alongshore current establishment this case test is of particular interest to us to assess the performances of the model on the important question of the parametrization of wave breaking incoming waves are monochromatic with a period of 2 5 s test 6 n in he01 the amplitude of the excited waves at the open boundary is adjusted to obtain a wave height corresponding to the conditions of test 6 n namely 0 19 m at the most offshore measurement points see the observation at 3 m from the incoming boundary on fig 10 the direction of propagation is 10 from the direction normal to the beach a description of the basin and its bathymetry is given in svendsen et al 2003 see also fig 10 the particularity of the basin lies in its pumping and recirculation system made of 20 pumps that allow having periodic conditions on the lateral boundaries in our simulation incoming waves are specified from the linear theory at the offshore boundary periodic boundary conditions are applied at the lateral boundaries of the model the dimension of the domain in the direction parallel to the beach is consistent with the condition of periodicity and the wave propagation direction of 10 in the deep part of the basin the horizontal resolution is 0 1 m and the grid has 10 vertical levels the dimension in the cross shore direction is 18 m and the deepest bathymetry offshore is 0 66 m the maximum wavelength in the deep part of the basin is of the order of 6 m a steady state is reached after about 150 s the results presented are obtained at the end of a simulation of 500 s the roughness of the bottom corresponding to this experiment is a priori not mentioned by its authors a roughness length of 3 10 5m was empirically fixed in our model in order to obtain a satisfactory agreement with the vertical profile of the measured alongshore current the basin dimensions are small compared to a real case if the ratio of the horizontal scale on vertical scale is used to deduce the characteristics that this experiment would have with dimensions 50 times larger close to the dimensions of the test cases presented in the previous section we obtain a horizontal resolution of 5 m a maximum depth of 30 m a maximum wavelength of the order of 300 m and a period of 125 s we are thus in conditions for which the problems of accuracy examined in the preceding sections are minor and make it possible to use the acm method with a small value of n and a coefficient of reduction of the gradient of q close to one in practice we use n 4 r 1 as mentioned previously we actually chose this test case for the wave breaking issue rather than the non hydrostatic specificities discussed in the previous test case wave breaking is parameterized through the horizontal diffusion of the momentum eq 13 the determination of the mixing coefficient ν b is an important aspect of this parameterization before going into the details of its calculation we examine the order of magnitude necessary for the decay of the waves inside the breaking zone to do this we consider an energy balance between the gradient of the potential energy flux g u η x and dissipation due to horizontal diffusion ν b u x 2 for the sake of clarity we limit ourselves to unidirectional propagation along the ox axis we hypothesize that within the wave breaking zone the wave height is proportional to the water depth miche 1944 32 h h γ where h is the wave height we consider here that the parameter γ is a constant of the order of 0 8 it can nevertheless be noted that the expression of γ can be more sophisticated as in guérin et al 2018 for convenience and considering that shallow depths where breaking mostly occurs allow this approximation the hypothesis of hydrostatic equilibrium is made in the following reasoning although the simulations presented are still non hydrostatic we thus consider that the current and the surface level are connected to each other according to u g c η where c gh it is finally assumed that the fields have a sinusoidal behaviour and that the wave amplitude η 0 satisfies eq 32 i e 2 η 0 h γ averaging over a period of waves the gradient of the potential energy flux is appendix d 33 guη x g 3 2 γ 2 3 16 h 1 2 h x and dissipation due to horizontal mixing is 34 ε b ν b 1 8 kgγh c 2 the equilibrium of eqs 33 and 34 leads to an estimation of the average of the mixing coefficient over a period of waves 35 ν b 3 8 g 3 2 h 1 2 h x t π 2 the latter increases with the slope of the bathymetry and vanishes when the thickness of the water column is zero the expression 35 is only an intermediate step towards a more complete formulation taking into account a triggering criterion based on the analysis of the instantaneous modelled fields but before addressing this aspect a preliminary experiment is carried out to test the relevance of the order of magnitude given by eq 35 practically eq 35 is used as the value of the horizontal diffusion coefficient in eq 13 inside the wave breaking zone the latter is arbitrarily defined from a threshold on bathymetry h 0 28 m deduced from observations of wave heights he01 in the pre breaking zone h 0 28 m ν b 0 fig 10 shows the simulated wave height and mean elevation and the corresponding observations of he01 as the laboratory experiment the simulation lasts 500 s the surface height is averaged over the last 50 s of the simulation the wave height is calculated from the difference between the maximum and minimum values of the surface level over the last 50 s of simulation the agreement is satisfactory the wave breaking trigger point is unsurprisingly well positioned since it is imposed by the criterion h 0 28 m the wave height is slightly overestimated in the breaking zone but the slope of decrease is parallel to that of the theoretical decay 2 η 0 h γ 0 8 dotted line in fig 10 it should be noted that several simulations were performed by increasing or decreasing slightly the value of ν b and that fig 10 corresponds to a 5 increase in the value predicted by eq 35 4 4 1 breaking criterion practically eq 35 is not really appropriate because it requires to arbitrarily specify the location of the onset of breaking in realistic case the breaking point of the breaking wave varies over time in particular according to the characteristics of the waves and more generally according to the surrounding hydrodynamic conditions an extension of eq 35 taking into account the instantaneous fields is therefore proposed 36 ν b ν b f b where ν b is the instantaneous coefficient ν b is the average coefficient eq 35 and f b is a function that triggers the wave breaking according to the instantaneous fields the average of f b over a period of wave is expected to be close to 1 so that the viscosity is on average close to ν b one consequence of this property is that ν b can be momentarily very high if f b is negligible over a large fraction of the wave period so that the average of ν b stays close to ν b many authors use the ratio of the wave height to the water depth the γ constant eq 32 as breaking criterion rusu and soares 2013 others use criteria based on the horizontal gradient of the modelled fields lynett 2006 for example zijlema et al 2011 propose to formulate eddy viscosity from prandtl s length and a turbulence equilibrium hypothesis which in practice links eddy viscosity to the horizontal gradient of velocities see their eq 12 roeber and cheung 2012 also use the velocity gradient see their eq 70 but in an approach that is more akin to a criterion on the froude number with gh as an approximation of the phase speed these authors emphasize that this criterion has the particularity to be more influential in areas where the slope of the waves is important in contrast to the h h criterion which favours the wave crests the criterion on the gradient is therefore in principle influencing a larger proportion of the breaking zone from the point of view of eq 36 an approach of this type should lead f b to act on a larger time interval and thus limit the maximum values of ν b on the other hand a criterion on the wave crest should lead f b to be small over a large fraction of the wave period with the aforementioned consequence of momentarily strong ν b values this consideration may be of numerical importance as there is a stability criterion which limits the eddy viscosity value for a given time step zijlema et al 2011 in the particular case of this experiment we find that a wave breaking criterion based on h h can cause the eddy viscosity to momentarily exceed the maximum value allowed by the cfl criterion i e ν b ν b max 1 4 dx 2 dt one possibility of overcoming this problem is either to reduce the time step of the model or to use a time splitting technique with a smaller time step for the calculation of the wave breaking without impacting the time step of the other equations of the model from a numerical point of view the approach proposed by roeber and cheung 2012 is interesting because their parameterization of the wave breaking should act on a longer duration limiting the maximum values of the breaking viscosity we choose this approach mainly for the aforementioned numerical issues but we propose to add a modification to that method in order to extend the influence of the breaking criterion to an even greater proportion of the breaking zone this is arguably questionable from the point of view of the realism of the process kennedy et al 2000 but it is an interesting option considering the numerical aspects mentioned above more specifically our objective is to avoid reducing the time step because of a too restrictive stability condition related to the wave breaking scheme the triggering criterion for wave breaking is inspired from roeber and cheung 2012 but is modified to be active both on the wave crest and between crests this criterion considers both velocities and their horizontal gradient the velocity gradient is scaled by the wavenumber so that the two quantities have the same order of magnitude a wave breaking criterion is then given by the ratio of the sum of these fields to the hydrostatic phase speed for simplicity the following formula is given in the case of unidirectional propagation 37 f b c 1 u s 2 1 k u s x 2 gh n 2 where u s is the surface current and k is the wavenumber fixed a priori according to the peak period prescribed at the open boundary the constant c 1 is determined empirically ideally its role is to allow the average value of f b to be of the order of unity over a period of waves so that the eddy viscosity is on average close to eq 35 it may be noted that in the idealized framework of the linear theory where the current would be of the form u 0 cos ωt kx the term in the numerator of eq 37 would simply be u 0 independent of time and in a breaking situation characterized by a current of an order of magnitude comparable to that of the phase speed the constant c 1 would simply be of the order of eq 35 the exponent n is used to adjust the selectivity of the criterion it is in particular chosen so that f b tends more rapidly towards 0 upstream of the breaking zone the simulations presented in the following use n 2 fig 11 corresponds to an eddy viscosity calculated with eqs 36 and 37 instead of eq 35 with the arbitrary hypothesis on the depth of the breaking onset the other parameters being identical to those of the previous simulation it is comparable to fig 10 the main difference is in the behaviour within the breaking zone where the curves are a little less linear in the case of fig 11 as a consequence of the non linear character of eqs 36 and 37 although the simulation is slightly less close to the observation in the case of fig 11 we prefer to use eqs 36 and 37 whose formulation is more general rather than eq 35 which suffers from its arbitrary definition of the trigger point of breaking 4 4 2 taking into account the breaking in the turbulent closure scheme the turbulence scheme used here is the k epsilon scheme rodi 1987 a wave related turbulence generation term may be taken into account when the circulation model is coupled to a phase averaged wave model michaud et al 2012 in the present case this term is not taken into account because the explicit modelling of the waves leads naturally to a significant production of turbulent kinetic energy tke through the vertical shear of the current the relevance of adding an additional production term to represent the assumed increase in turbulence in the breaking zone is an open question feddersen and trowbridge 2005 and is therefore examined here the he01 experience provides the opportunity to examine this point by providing measurements of the residual onshore current at different depths and at different distances from the edge of the basin here an additional production term p wb representing a fraction of the kinetic energy lost by the current via the horizontal mixing associated with the breaking i e eqs 36 37 is introduced into the tke equation of the k eps scheme 38 p wb c 2 ν b u x 2 u y 2 v x 2 v y 2 where c 2 is a constant between 0 and 1 eq 38 implies that a fraction of the breaking energy feeds the vertical mixing the simulated alongshore residual current is obtained by averaging the alongshore current during the last 50 s of the simulation fig 12 shows the comparison with the observed current from he01 note that the vertical axis has been normalized so that the figure is easily comparable to that of svendsen et al 2003 see their fig 11 two simulations are made one with c 2 0 the other with c 2 0 01 the two simulations generally give rather satisfactory results with the exception of the current at x 7 9m looking more closely at fig 12 we notice that the simulation taking into account p wb red lines gives a better agreement with the observations the simulation without p wb c 2 0 green lines shows indeed a current with a greater vertical gradient than that of the observation in particular at the points located in the shallowest depths increasing the vertical mixing coefficient associated with p wb reduces the current shear in the breaking zone we therefore retain at this stage that this approach can be used to improve turbulence in the wave breaking zone however in the state of our knowledge the setting of c 2 is arbitrary and would need to be estimated on other experiments including in situ data such as current profiles or more direct observations of turbulence costa et al 2017 4 5 the case of a barred beach with rip currents the interaction of waves and rip currents is an example of the complexity of possible interactions between current and waves weir et al 2011 the model is now applied to a laboratory experiment describing the propagation of monochromatic waves over a beach with a sandy bar and channels inducing rip currents haller et al 2002 haas and svendsen 2002 this experiment has already been simulated with hydrostatic circulation models coupled with a phase averaged wave model haas and warner 2009 haas et al 2003 michaud et al 2012 bruneau 2009 like these previous authors we are interested here in test b of haller et al 2002 the basin and its bathymetry are described in detail in the aforementioned articles the dimensions of the basin are approximately 17 m cross shore direction by 18 m alongshore direction the cross shore slope is 1 30 a bar located a few meters from the coast is cut by 2 channels ideally the two channels should have an identical shape and the bathymetry of the entire basin should respect an alongshore symmetry the central bar is twice as long as the two half bars touching the side walls and outside the bar the alongshore gradient is supposed to be nil however as pointed out by haas et al 2003 an important feature of this bathymetry is that it actually has irregularities compared to its idealized design resulting in dissymmetry of the flow and increasing instability of the currents the two rip currents that form at the exit of the channels are for example clearly different in their form and their intensity and their respective temporal variations seem rather decoupled a bathymetry interpolated from a survey of the wave basin is used here to better respect the real conditions of the experiment the horizontal resolution is 0 06 m and the vertical grid has 10 sigma levels the time step is 1 1 10 2 seconds monochromatic waves of period 1 s propagate in the direction perpendicular to the coast fig 13 the amplitude of the excited waves at the open boundary is adjusted to obtain a wave height corresponding to the conditions of test b namely about 4 1 cm at the most offshore measurement points see fig 14 around x 8 m their wavelength at the entrance of the domain where the depth is about 0 38 m is about 1 4 m fig 13 in good agreement with the linear non hydrostatic wave theory it may be noted that the hydrostatic approximation under the same conditions would lead to a wavelength of about 1 9 m i e an overestimation of about 35 consistently with haas and svendsen 2002 and haas et al 2003 the simulation lasts 30 min to highlight the observed temporal variations of rip currents the wave height is obtained from the difference between the minimum and maximum of the surface level obtained over time intervals of 1 s an average wave height is then calculated by averaging these values obtained every second over the simulation except the first minute to avoid the spin up phase table 2 summarizes the characteristics of the numerical grid and the calculation time the latter can be roughly extrapolated to more realistic applications considering that the scale ratio between this simulation and that of a realistic case with for example dx 3 m is about 50 in other words an area of about 1 km 1 km and a simulation duration of about 1 day the wave height and mean surface height calculated by the model are compared to the observations of the test b case figs 6 and 16 of haller et al 2002 the wave height first increases slowly towards the coast then increases sharply when the waves arrive above the bar fig 14 the breaking conditions are quickly reached and once the bar crossed the wave height is much lower and relatively constant figs 13 and 14 a second breaking zone is highlighted near the coast this behaviour of the model is in good agreement with the description of the process given in haller et al 2002 in the channel axis the wave height increases before the channel as shown in figs 13 and 14 because of the blocking by the opposite rip current fig 15 shows the horizontal distribution of the depth averaged current that is very similar to the circulation calculated with the phase averaged modelling presented by haas et al 2003 their fig 5 and haas and warner 2009 their fig 7 the rip current slows down the waves causing the waves to refract towards the centre of the channel fig 13 in agreement with the visual observation of the laboratory experiment reported by haas et al 2003 hereafter h03 see their section 6 however the rip current does not lead to a wave breaking more important than the one above the bar the wave height actually decreases more slowly in the channel than above the bar so that in the last meters before the coast the waves are higher in the channel axis than behind the bar fig 14 the mean surface level increases at the coast inducing a change in the average position of the water line allowed by the wetting drying scheme of the model the mean surface level at the coast is markedly different in the channel axis and in front of the bar in agreement with the observation fig 14 as haas and svendsen 2002 point out this difference is associated with a longshore pressure gradient that drives the flow to the channels laboratory experiments show that rip currents are unstable and are characterized by large temporal variations the authors of these experiments emphasize the impossibility of reproducing twice the same series of measurements even if the experimental conditions are a priori unchanged an exact agreement with the model is therefore not sought nevertheless we expect from the model that it gives the correct orders of magnitude of the rip currents and their variability fig 16 shows the temporal evolution of the current averaged over the vertical and over a wave period in the two channels given the above limitations the agreement with the observations presented in h03 see fig 11 a in h03 is good the rip current at y 4 6 m is a little overestimated but as in h03 it is mostly lower than the rip current at y 13 6 m the latter has the good order of magnitude with maximum around 20 cm s the amplitude and time scale of its variability are also close to the behaviour described in h03 the occurrence of the rip current is of the order of 200 s without being really regular the variability of the rip current is of the order of a few centimetres per second with a maximum standard deviation around 6 cm s in the centre of the channel at y 13 5 m and x 11 5 m fig 17 the vertical structure of the time averaged cross shore current in the rip channel fig 18 has similarities with the phase averaged modelling of haas and warner 2009 their fig 11 with a maximum intensity current around 0 2 m s between x 11 m and x 12 m at the point of maximum intensity x 11 5 m the current is fairly homogeneous on the vertical except near the bottom where the current decreases strongly in good agreement with the observation haas and svendsen 2002 their figs 17 18 19 at the exit of the channel offshore direction the current decreases significantly the vertical gradient also becomes larger and close to the bottom at x 9 m the current is even reversed positive values in fig 18 in good agreement with the observations haas and svendsen 2002 their figs 13 and 14 5 conclusions a phase resolved wave model is derived from an ocean circulation model with the aim of performing simulations of current wave interaction in nearshore areas the model is evaluated on the fundamental aspects of wave dynamics namely mainly the dispersion properties the transformation of wave characteristics by bathymetry shoaling wave breaking the influence of waves on low frequency fields such as the mean surface levels and rip currents with the exception of the post breaking zone that can be processed in some models by a hydrostatic approach the phase resolved wave modelling requires a non hydrostatic model as the representation of the non hydrostatic problem in all its complexity seems to be inevitably very expensive to date klingbeil et al 2018 a scheme allowing a reasonable compromise between on the one hand the extra cost of the computation and on the other hand an acceptable loss of accuracy of non hydrostatic properties of waves has been introduced into the circulation model it is shown that relative errors of less than a few on the dispersion relation and the vertical current profile can be obtained with non hydrostatic simulations costing 5 to 6 times more than a hydrostatic model calculated under the same conditions the non hydrostatic additional cost remains obviously high but low enough to consider simulations of realist events such as storms on very large areas and it is anyway worth noting that wave current effect modelling based on a hydrostatic circulation model normally couple the latter with a phase averaged wave model whose cost is possibly not negligible the non hydrostatic method implemented in the circulation model is derived from the method proposed by lee et al 2006 in practice the classical poisson problem is transformed into a pressure propagation equation the resolution is iterative local and does not need to solve a system of linear equations an essential aspect of this method is to properly control the various parameters related to the iterative aspect of the problem the equation of pressure propagation depends on a constant the latter being comparable to a propagation velocity which determines the speed of the adjustment of the non hydrostatic pressure to a change of the hydrostatic terms in the momentum equations the precision of the non hydrostatic behaviour depends on the fact that this adjustment is faster than the variation of the other fields a meticulous adjustment of the time step of the model is in practice necessary taking into account various parameters including the wave propagation speed and a possible tuning consisting in reducing the non hydrostatic pressure gradient so far the reduction factor is a constant finding of a compromise in a set of possible values taking into account a range of periods and depths future development will adapt this tuning to the wave peak period and bathymetry using the analytical reduction factor solution that this study established from the linearized acm equations the model is tested on idealized simulations and laboratory experiments the ability of the non hydrostatic method to accurately represent the dispersion properties of surface waves is first demonstrated in the context of the linear theory hamilton and ebersole 2001 laboratory experiment is used to evaluate the model on the issue of wave height amplification associated with beach slope as well as wave breaking the parameterization of the wave breaking is based on momentum horizontal diffusion trigger conditions being related to a froude number the tuning of the horizontal viscosity is consistent with considerations on the mechanical energy balance the reasoning can be extended to the possible production of turbulent kinetic energy in the turbulent closure scheme leading in practice to the increase in vertical turbulent viscosity if the vertical profile of the residual alongshore current can apparently be slightly improved on the other hand the fraction of the kinetic energy lost by the breaking wave which can in this way be injected into the vertical turbulence remains uncertain for the authors and requires obviously to be better understood along these lines a part of the wave spectrum will still be missing in our future realistic applications because of the limited grid resolution the smallest periods being concerned with as a result a possible underestimation of the bottom stress in shallow areas that should be taken into account finally the haller et al 2002 laboratory experiment with its bar and its two channels makes it possible to evaluate the model on the rip current problem the model correctly reproduces rips currents and their high temporal variability the feedback of the rip currents on the waves in terms of wave amplification and refraction is well represented the onset of wave breaking is also well simulated either above the bar or in the channel axis this paper does not consider some 3d aspects as the effect of tracers on the pressure gradient and turbulence which will be taken into account in future applications of this model this also includes sediments that may be of great importance for modelling of rip currents as shown by ma et al 2014 these authors show that a wave resolving non hydrostatic model is an efficient approach for the problem of resuspension of sediments by waves and the possible feedback introduced by the modification of the equation of state by sediments which can have a significant influence on the pressure gradient and the level of turbulence in practice our wave model will be coupled with the mustang sediment model le hir et al 2011 the next step is now to implement our model on realistic beaches where we will confront the results with in situ data infragravity waves and the risk of submersion are possible applications some developments are still needed to provide a realistic wave spectrum to the boundary conditions of the phase resolved wave model the phase averaged model which can be applied over large domains at a reasonable cost appears to be a good candidate for providing the boundary conditions to the phase resolved model the phase averaged wave model can also provide an estimate of the missing part of the wave spectrum in the deterministic model helping the former to improve bottom turbulence in shallow areas finally wave growth by local wind action will eventually be considered in the perspective of large areas deigaard and nielsen 2018 acknowledgements the authors acknowledge the support of the sirocco team http sirocco obs mip fr the numerofix project numerical modelling of real ocean surface mixing funded by lefe gmmc the hpc resources from calmip grant 2018 p1325 and from genci and cines grand equipement national de calcul intensif project a0040110088 appendix a the discrete version of 2 q x 2 at the left hand side of eq 4 is given by the left side of a1 q x dx q x dx 2 q x d x 2 2 q x 2 dx 2 12 4 q x 4 the right hand side of eq a1 is given by a taylor development dx 2 12 4 q x 4 being the leading term of the errors made by the numerical scheme assuming a sinusoidal shape 4 q x 4 k 4 q k the wave number leads to eq 5 appendix b time stepping scheme the equations are calculated with a forward backward time stepping scheme the alternation of the fields along the time axis is presented in fig 19 the height of the surface at time t η t is calculated first knowing η t 1 and velocities at time t ½ eq 9 gives q t knowing q t 1 q t 2 and hydrostatic tendencies at time t 1 note that the vertical laplacian of q is computed with an implicit centric scheme of the type 1 2 2 q t 2 z 2 1 2 2 q t z 2 so as not to limit the time step when the vertical mesh becomes very small which becomes the case in sigma coordinates when the bathymetry tends to zero the vertical velocity at time t ½ used for the advection terms of the equations for u and v is computed from u and v at time t ½ finally the velocities at time t ½ are calculated from the velocities the sea surface height and the nonhydrostatic pressure of the previous times vertical turbulent diffusion terms are calculated with an implicit scheme fig 19 chronology of the variables of the model at the beginning of each iterative cycle the fields at time t ½ and time t are known fig 19 appendix c the discrete version of the left hand side of eq 30 is given by the left side of c1 η t dt η t dt 2 η t d t 2 η 2 t dt 2 12 η 4 t the right hand side of eq c1 is given by a taylor development exponents in brackets indicating the order of the derivatives dt 2 12 η 4 t being the leading term of the errors made by the numerical scheme assuming a sinusoidal shape η 4 t ω 4 η t ω the wave frequency leads to dt 2 12 η 4 t dt 2 12 ω 4 η t similar reasoning can be done on the right hand side of eq 30 the leading error term of the corresponding numerical scheme is c 2 dx 2 12 4 η x 4 equivalent if a sinusoidal shape is assumed k the wavenumber to c 2 dx 2 12 k 4 η and c 2 dx 2 12 k 2 2 η x 2 then reformulate eq 30 using the first two terms of the taylor developments leads to eq 31 appendix d the potential energy flux is f guη with u gη c a sinusoidal form sinψ is assumed leading to f g 2 1 c η 0 2 sin 2 ψ a wave period averaging gives f g 2 1 2 c η 0 2 within the wave breaking zone we assume 2 η 0 h γ using c gh leads to f 1 8 g 3 2 γ 2 h 3 2 the horizontal gradient is d1 f x 3 16 g 3 2 γ 2 h 1 2 h x consider a constant diffusion coefficient ν b the dissipation term due to horizontal diffusion is ε b ν b u x 2 assuming a constant wavenumber leads to u x k u 0 cos ψ and ε b ν b k 2 u 0 2cos2 ψ a wave period averaging gives ε b ν b k 2 u 0 2 1 2 ν b k 2 1 2 g η 0 c 2 using 2 η 0 h γ leads to d2 ε b ν b 1 8 kgγh c 2 using k ω c 2 π ct t the wave period the equilibrium of eqs d1 and d2 leads to eq 35 
