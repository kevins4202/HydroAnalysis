index,text
6285,new and improved drought models based on the world meteorological organization approved standardized precipitation index principally at multiple timescale horizons are providing significant benefits to the hydrological community by its widespread acceptance in the sub field of water resources management sustainable water use and precision agriculture in this research paper the existing challenges faced by a drought forecasting model trained at multiple time scales are resolved where a new multi phase multivariate empirical mode decomposition model integrated with simulated annealing and kernel ridge regression algorithms i e memd sa krr is designed to attain significantly accurate drought forecasts for 3 agricultural sites i e faisalabad islamabad and jhelum located in pakistan utilizing the multi scalar standardized precipitation index spi time series as a target variable for characterization of drought twelve multivariate datasets derived from statistically significant lagged combinations of precipitation temperature humidity that are enriched with eight synoptic scale climate mode indices and periodicity are utilized in designing a new drought model the study constructs a hybrid memd sa krr model where firstly the data are partitioned into their respective training and testing subsets after creating historically lagged spi at timescale t 1 secondly the memd algorithm is conditioned to demarcate multivariate climate indices from their training and testing sets separately into their decomposed intrinsic mode functions imfs and residues thirdly the sa method is employed to decide the most suitable imfs finally the krr algorithm is applied to the selected imfs to forecast multi scaler spi at 1 3 6 and 12 monthly forecast horizon the results are benchmarked with random forest integrated with memd and sa to develop the memd sa rf equivalent model the multi phase memd sa krr model is tested geographically in pakistan revealing that the memd sa krr hybrid model generates reliable performance in forecasting multi scaler spi series relative to the comparative models based on error analysis metrics the hybrid drought model incorporating the most pertinent synoptic scale climate drivers as the model inputs has significant implications for hydrological applications and water resources management including its potential use in drought policy and drought recovery plans keywords hybrid drought forecast model multivariate empirical mode decomposition simulated annealing kernel ridge regression 1 introduction drought is characterized as a climatological menace that can occur in arid semi arid or tropical rainforest zones keyantash and dracup 2002 vicente serrano 2016 wilhite et al 2000 drought events can last from short to long period ranging from one month to four years as recent climate change significantly affects rainfall patterns vicente serrano 2016 drought severely disturbs water resources agriculture crops energy supply and industrial sectors and it is a growing concern deo et al 2009 ipcc 2012 mcalpine et al 2007 yaseen et al 2018b long term drought significantly poses challenges to groundwater reservoirs and also cause significant water scarcity cai and cowan 2008 and the related socio economic costs dijk et al 2013 wittwer et al 2002 hydrologists and government and non government based policy makers planning to develop new strategies to manage drought risks for water management bates et al 2008 deo et al 2017c mishra and singh 2011 mouatadid et al 2018 hydrological decision support systems are considerably as important standards in characterizing different watersheds and their scenarios chen et al 2013 gebremariam et al 2014 romagnoli et al 2017 sommerlot et al 2016 but this model s accuracy can rest on the physics initial conditions and calibrations based on conceptual i e regional scale sub models and the spatio temporal aspects of the input and output variables he et al 2014 sun et al 2012 recently artificial intelligence models that do not require any details about the physics of the watershed or any associated hydrological behaviors are seen to have a good ability to forecast precipitation run off streamflow and drought events ali et al 2018b deo et al 2017a deo and şahin 2016 deo et al 2017a deo et al 2019 mouatadid et al 2018 prasad et al 2017 yaseen et al 2018a such models that can rely purely on how climate variables and related large scale indices change over time can provide great insights into a drought forecasting and forewarning system drought can be quantified by large scale climate indices in different geographic zones which is adopted to monitor drought intensity mishra and singh 2010 mishra and singh 2011 traditionally the palmer drought severity index pdsi was developed to monitor drought palmer 1965 the pdsi is beneficial in handling lengthy drought period but is not recommended for of high run off region mishra and singh 2010 mishra and singh 2011 crop moisture index cmi is developed to handle the complexities in pdsi especially to quantify agronomic droughts palmer 1968 cmi uses the rainfall record rankings to determine the positive and negative precipitation anomalies which are useful for the short term offset byun and wilhite 1999 designed the effective drought index edi for meteorological and agricultural drought events which only based on precipitation the spi is designed to overcome these hurdles and is a worldwide acceptable standard metric to monitor and investigate drought scenarios modelling spi as advocated in this research study is beneficial for future drought assessment as follows 1 this index can monitor water deficiency scenarios that based on statistical rainfall distribution ranging from monthly to seasonal and annual 2 it is considered to be a globally acceptable standardized metric dependent on normalized rainfall deficits hayes et al 1999 mckee et al 1993 yuan and zhou 2004 that models the drought behavior in a climatological diverse regions almedeij 2016 choubin et al 2016 svoboda et al 2012 3 handling multiple timescales of drought probabilistically the spi is an instrument to inspect soil moisture condition svoboda et al 2012 artificial intelligence models have been adopted to develop drought forecasting strategies based on spi utilizing several environmental parameters 1 santos et al 2009 and jalalkamali et al 2015 designed a multilayer perceptron artificial neural network to forecast the spi in us and iran respectively and 2 wavelet based adaptive neuro fuzzy inference system were developed to estimate spi by shirmohammadi et al 2013 in azerbaijan for the details of spi based drought forecasting please refer to e g adamowski et al 2012 ali et al 2018b bonaccorso et al 2003 cancelliere et al 2006 choubin et al 2016 deo et al 2017c guttman 1999 hayes et al 1999 jalalkamali et al 2015 moreira et al 2015 moreira et al 2008 paulo and pereira 2007 sönmez et al 2005 in this research work we develop an spi based hybrid artificial intelligence model for a drought prone region in pakistan where historical drought events have severely hindered socio economic and agricultural production report 1950 2015 to address some of the challenges faced due to drought events khan and gadiwala 2013 has analyzed drought patterns using spi while xie et al 2013 has forecasted spi at several spatio temporal scales in pakistan recently ahmed et al 2016 has characterized seasonal drought using spi in balochistan pakistan and ali et al 2018a modelled drought events based on lagged data of spi using adaptive neuro fuzzy inference system based ensemble anfis ensemble approach similarly ali et al 2018b has utilized some of the primary climate based datasets to forecast the spi series however studies on future drought models utilizing different synoptic scale climate mode indices are still very limited particularly in the agricultural region of pakistan owing to the variability in climate based for a drought model a suite of multi resolution analytical tools can be useful to extract embedded features in a non static time series signal that are related to a drought variable and thus they may help to improve an existing drought model to resolve this challenge the empirical mode decomposition emd method formalized by huang et al 1998 can provide a useful alterative tool to improve existing drought models as it is able to isolate the largely fluctuating signals into their respective smaller and more clearly resolved frequency components to improve a drought model since its inception the emd method has gained attention due to its self adaptability alvanitopoulos et al 2014 the emd is completely data dependent thus making it greatly useful to extract relevant features without any loss of information further the decomposed prominent features preserve the physical structure of the input temporally wu et al 2011 due to its capability to improve the forecasting accuracy of artificial intelligence models the emd algorithm has been integrated with artificial neural network ann model proving it to be a successful tool to forecast environmental variables such as solar radiation alvanitopoulos et al 2014 for instance wang et al 2018 trialed emd and local mean decomposition integrated with least squares support vector machine to forecast solar radiation in spite of its recent applicability a key issue with emd and its variant algorithm s is that it can only applicable to decompose a univariate data colominas et al 2014 torres et al 2011 wu and huang 2009 for example only the significant antecedent lagged dataset of spi can be used to forecast the future drought series this is a major limitation of the emd algorithm since the variation in drought events is immensely reliant on dynamically driven climatological factors such as atmospheric circulation so the incorporation of the relevant synoptic scale climate indices e g soi that modulate drought events is extremely important therefore these input variables need to be intelligently used into the artificial intelligence model in addition to the antecedent drought index series the recent work of ali et al 2018a which appears to be the only study that has attempted to forecasted the medium and long period droughts has used the significant lags of spi to forecast the future spi series further ali et al 2018b has employed climate dataset to forecast spi only over a short term one month period following earlier studies ali et al 2018a ali et al 2018b on short medium and long term drought forecasting this study aims to operate large scale climate indices and climate predictors to subsequently extract most if not all pertinent features where an memd rehman and mandic 2009a based hybridized modelling approach is developed to forecast multi scaler spi index the memd is an advance generalized form of emd and ceemd which demarcates multivariate inputs to performs accurate investigation of composite and nonlinear procedures rehman and mandic 2009a additionally the memd fixes the mode alignment problems arise in the joint analysis within a multi dimensional data looney and mandic 2009 the applicability of memd can be seen clearly in forecasting of evapotranspiration adarsh et al 2017 soil water hu and si 2013 crude oil price he et al 2016 solar radiation prasad et al 2019 and iceberg drift andersson et al 2017 yet this the present research is a pilot application of this novel technique in drought forecasting particularly for the agricultural region of pakistan further this paper follows earlier methodology quilty and adamowski 2018 aiming to partitioned data prior developing hybrid memd sa krr model this approach is used to circumvent the issues associated with other kinds of decomposition based models that were raised in earlier studies quilty and adamowski 2018 the novelty of this research study lies in the enhanced capability of the memd algorithm to address the non stationarity issues encountered in the model design process by using concurrent transformation of model inputs into their decomposed components that are likely to improve an existing drought forecasting model as revealed later in results section the primary issues related to the selection of best imfs i e patterns in drought model input series which are not known a priori are determined by the proposed approach by an implementation of a robust feature selection process simulated annealing sa algorithm hence a careful integration of memd and sa algorithm with kernel ridge regression krr is made to generate a much improved hybrid forecast model denoted as the memd sa krr the model is also benchmarked against memd sa rf and a standalone i e krr and rf model to forecast multi scaler spi at 1 3 6 and 12 months tested at 3 drought rich locations 2 theoretical framework a summary of the artificial intelligence model based on krr memd and sa approaches with its comparative models is now presented 2 1 kernel ridge regression krr model krr model is a machine learning model based on kernels and a ridge regression approach zhang et al 2013 which is used to deal with over fitting in the regression using regularization and the kernel technique to capture non linear relationships you et al 2018 mathematically krr can be formulated as 1 arg min 1 q o 1 q f o y o 2 λ f h 2 2 f o p 1 q α p φ x p x o where h is the hilbert normed space in eq 1 zhang et al 2013 for a given m b y m kernel matrix k is constructed by k p o φ x p x o from selected input data where y is the input q by 1 regressand vector and α is the q by 1 unknown solution vector equation 2 reduces to the following 3 k λ q i y 4 y o 1 q α o φ x o x the krr method in the model training stage is approximated α by solving eq 3 whereas this α is used in the testing phase to predict the regression of unknown sample x in eq 4 further the krr algorithm searches for optimum α and λ from the parameter set in krr linear polynomial and gaussian kernels are used to get the optimum accuracy alaoui and mahoney 2015 vovk 2013 welling 2013 you et al 2018 mathematically linear polynomial and gaussian kernel are defined as 5 φ x p x o x p t x o 6 φ x p x o x p t x o r d 7 φ x p x o exp x p x o 2 2 σ 2 where t represents the transpose and d is the dimension of the vector 2 2 multivariate empirical mode decomposition memd method the memd technique is able to fix the issues of mode mixing which handle the drawback of exhaustiveness and time consuming to understand the mathematical structure of memd we need to study the emd theory the emd is described as 8 γ s k 1 l c k s r l s where γ s c k s and r l s are representing input data the kth imf and remainder residue respectively the memd formulated by rehman and mandic 2009b uses multivariate data to decompose into multiple dimensions of imfs to avoid the issues of mode mixing incorporating white gaussian noise ur rehman and mandic 2011 the mean m s is derived as following 9 m s 1 p q 1 p e θ q s where e θ q s is referred to envelop curves with length of the vectors s 10 r s γ s m s the term r s is called a multi dimensional imf satisfying the stopping criterion the memd algorithm has been utilized in analyzing signal processing huang et al 2013 mandic et al 2013 and hydrology hu and si 2013 she et al 2015 the memd is a self adaptive algorithm which makes no assumptions a priori about the composition of the signal hu and si 2013 she et al 2015 the memd uses spline interpolation between maxima and minima to successively trace out imfs where each imf is a single periodic oscillator huang et al 2013 mandic et al 2013 the imfs cannot be predicted prior it is empirically observed from the signal since the imfs can change over time memd makes no assumptions about the stationarity of the signal or the signal components and is therefore better suited to non linear signals when analyzing signals from complex systems huang et al 2013 mandic et al 2013 2 3 simulated annealing sa model the sa is a bio inspired feature selection algorithm to find a suitable solution to an optimization technique elleithy and fattah 2012 the sa is an adaptive non deterministic algorithm which has been extensively used as optimization technique such as traveling salesman peng et al 1996 computer generated holograms taniguchi et al 1997 power efficiency wilson 1997 and heat exchangers athier et al 1997 the basic algorithm of sa involves the following steps 1 create a randomly appropriate solution 2 determine the cost of solution using some cost function 3 create another random neighbouring solution 4 compute again cost of the above new solution 5 if the new solution cost is less than the old solution cost then move to the new solution otherwise go to the following step 6 6 follow again stages 3 5 until an optimum solution is determined 2 4 random forest rf bootstraping and bagging is basically ensemble learning techinques which creates classfiers and sums the final outcomes in terms of decision trees breiman 1996 schapire et al 1998 the rf model is fundamentally a decision tree model which adopts randomly a bagging approach in the forecasting scenarios breiman 1996 each node is separated randomly by chosing preeminent possible predictors to improve accuracy that are robust to avoid overfitting breiman 2001 the steps followed in the designing of rf can be given as 1 generate bootstrapping of n trees by incorporating the predictors variables with n denotes the number of trees 2 the randomly input predictors sample m try is created to choose maximum predictors splitting by growing an unpruned regression tree 3 cumulate the aggregative predictions of n trees to forecast multi scaler spi the applicabilty of rf model can be seen in soil attribute prediction moore et al 1993 hydrology moore et al 1991 environmental management ascough ii et al 2008 drought forecasting chen et al 2012 solar index estimation deo et al 2017b rainfall forecasting ali et al 2018c and most recently forecasting soil moisture prasad et al 2018 for more comprehensive studies on rf model readers are referred to breiman 2001 liaw and wiener 2002 prasad et al 2018 robert et al 1998 segal 2004 2 5 multi scale standardized precipitation index spi the spi quantifies the wet and dry scenarios based on statistical probability theory prior to design the proposed multi phase memd sa krr model the multi scaler spi index was computed by incorporating precipitation ptcn data mckee et al 1993 in the following equation 8 11 g p t c n 1 β α γ α ptcn α 1 e x β where g ptcn indicates the probability density function α and β are the parameters determined by the maximum likelihood solution whereas γ shows the gamma function further the cumulative probability is defined as 12 g p t c n 0 p g p t c n d r f 1 β α γ α 0 p x α 1 e p β d ptcn by substituting n ptcn β in eq 13 13 g p t c n 1 γ α 0 n n α 1 e n d n the cumulative probability reduces to the following form when ptcn 0 h ptcn p 1 p g ptcn 14 with p represents the probability of zero which determines the spi index viz 15 spi n ε 0 ε 1 n ε 2 n 2 1 ω 1 n ω 2 n 2 ω 3 n 3 0 5 h p t c n 1 0 n ε 0 ε 1 n ε 2 n 2 1 ω 1 n ω 2 n 2 ω 3 n 3 0 h p t c n 0 5 where ε 0 ε 1 ε 2 ε 3 ω 1 ω 2 and ω 3 are arbitrary constants with magnitudes ε 0 2 515517 ε 1 0 802853 ε 3 0 010328 ω 1 1 432788 ω 2 0 189269 and ω 3 0 001308 mckee et al 1993 drought is characterized into three categories as moderate 1 5 spi 1 0 severe 2 0 spi 1 5 and extreme spi 2 0 3 materials and method 3 1 data twelve meteorological data series precipitation ptcn temperature t humidity h southern oscillation index soi sea surface temperatures nino3sst nino3 4sst nino4sst pacific decadal oscillation pdo indian ocean dipole iod el nino southern oscillation modoki index emi southern annular mode sam and periodicity at a monthly t lag of t 1 are acquired from pakistan meteorological department pmd 2016 national climate prediction centre nicholls 2004 sst 2018 joint institute of the study of the atmosphere and ocean jisao 2018 bureau of meteorology australia bma 2018 japan agency for marine earth science jamstec 2018 and from the british antarctic survey bas 2018 any precipitation less than 0 1 mm was replaced with the corresponding averaged value for the climatological period the multi scaler spi index was computed in r programing software using the rainfall time series data from 1981 to 2015 despite inherent complexities associated with accurate forewarning of drought synoptic scale climate mode indices that are strongly correlated with drought occurrence mishra and singh 2010 morid et al 2007 özger et al 2012 can provide a consensus on the overall behaviour of drought events as these indices can have a significant influence on rainfall and streamflow patterns ali et al 2018c andreoli and kayano 2005 chiew et al 1998 deo and şahin 2016 mcbride and nicholls 1983 mcgregor et al 2014 nicholls 1983 prasad et al 2017 yaseen et al 2018a yaseen et al 2018b for example the well known association of the inter decadal pacific oscillation over the pacific ocean is seen to influence the el nino southern oscillation enso phenomena that governs the intensity of drought events dai 2013 salinger et al 2001 the negative period of soi brings el nino episodes whereas the positive values of soi launches la nina events adnan et al 2017 philander 1983 the northern atlantic oscillation nao also has a significant influence from seasonal to inter decadal variability on atmosphere and environmental variables dickson et al 2000 hurrell 1995 souriau and yiou 2001 the iod across the eastern indian ocean carries hefty showers over east africa while drought and forest fires across the indonesian zone adnan et al 2017 ashok et al 2001 furthermore sea surface temperatures nino3sst nino3 4sst nino4sst over the southeast indian ocean may also lead to heavy precipitation priya et al 2015 terray et al 2007 the enso modoki index emi based on strong anomalous warming in the central tropical pacific and cooling in the eastern and western tropical pacific potentially impacts the temperature and rainfall patterns around the globe due to ocean atmosphere and the unique tri polar sea level pressure pattern ashok et al 2007 similarly the sam significantly influences the monsoon rainfall anomaly pal et al 2017 3 2 study locations the study locations utilized in this work are faisalabad islamabad and jhelum displayed in fig 1 the geographical climatological and drought statistics of these locations is described in table 1 further it also presents the multi scaler spi index faisalabad is categorized as desert with average yearly rainfall of 375 mm and temperature 24 2 c and it is located in the grasslands of northeast punjab table 1 servey 2016 major crops growing in faisalabad are wheat rice cotton sugarcane maize different vegetables and fruits islamabad is the capital city of pakistan it experiences a subtropical climate with four seasons winter spring summer and autumn with average monsoon and annual precipitations of 790 8 mm and 1142 1 mm respectively the heaviest precipitation in islamabad was 620 mm month recorded in july 2002 jhelum is situated in the pothohar region of the punjab province pakistan agriculture highly depends on precipitation in punjab the average annual rainfall is about 1000 mm in the rainy season of monsoon department 2010 pmd 2016 3 3 development of multi phase memd sa krr model the multi phase memd sa krr model is developed in matlab r2016b programming environment the math works inc usa all the simulations were obtained operating pentium 4 2 93 ghz dual core central processing unit the data are partitioned straightly 60 and 40 prior into training and testing subsets respectively following by quilty and adamowski 2018 as it is the most common approach for data partitioning cannas et al 2006 the first 21 years from 1981 to 2001 data were used to train the memd sa krr model while the remaining 14 years data 2002 20015 were utilized for testing moreover the cross validation or any data randomized approach cannot be adopted as time series data by definition occur in a temporal order sequence and this order or sequence must be preserved in order to keep the structure of the series intact bergmeir and benítez 2012 antecedent time time lagged inputs i e ptcn t h soi nino3sst nino3 4sst nino4sst pdo iod emi sam and periodicity at t 1 are used to develop the hybrid model to enable spi1 1 month spi3 3 month spi6 6 month and spi12 12 month forecasts as elicudated in the following steps 3 3 1 phase 1 memd process the memd method is applie to demarcates the input time series variables into imfs and residuals the input variables were incoporated in ptcn t h soi nino3sst nino3 4sst nino4sst pdo iod emi sam periodicity for decompostion by the memd algorithm additionally the predefined parameters include the ensemble number n 500 and the amplitude of the added white noise ε 0 2 ouyang et al 2016 ren et al 2015 wang et al 2013 wu and huang 2009 to acquire the same number of imfs in training and testing period the memd method is controlled using total projection a stop vector i e tolerance and threshold values and stopping criterion see table 2 total one hundred twenty imfs with residuals table 3 for multi scaler spi in faisalabad sites is extracted with each input has imfs 9 residual 1 whereas for islamabad this number is ninety six with each input has imfs 7 residual 1 for jhelum the memd algorithm disolved the input predictors into imfs 8 residual 1 no of total imfs 108 in case of spi1 spi6 spi12 while imfs 7 residual 1 no of total imfs 96 for spi3 the cross correlation is a linear method which determine the linear relationship between input predictor and target data we avoided this method and used sa method instead to select from a pool of variables as the multiple climate input predictors have significant non linearity moreover we have 12 input predictors and decomposed by memd into several sub components the resulting number of inputs is very large i e approx 120 this means if we use a manual method e g cross correlation there is a large amount of work but it uses linear method and so will not generate smart way of selecting the best inputs instead we have used sa which is a non linear approach so this method is more suitable than correlation analysis we have identified this issue and revised the paper 3 3 2 phase 2 sa algorithm the sa approach is adopted to choose the most appropriate imfs for only the training period using a feature selection strategy for model development further some parameters were defined prior that includes the number of maximum iterations 20 and the parameter initial temperature 10 the number of selected best imfs feature is kept 25 which were defined prior to run the sa model the selected imfs for each training period are described in table 3 for testing periods those imfs are used for model validation based on selected imfs of the training dataset 3 3 3 phase 3 normalization process the data are normalized between 0 1 using eq 13 and due invertible nature of the normalization the results will not be affected hsu et al 2003 the normalization is carried out following eq 16 to handle large variation in the data hsu et al 2003 16 θ norm θ θ min θ max θ min in eq 16 θ represents the input output θ min is the minimum value θ max is the maximum value of the data and θ norm is the corresponding normalized numeric value 3 3 4 phase 4 kernal ridge regression krr method in the final phase of the modeling the krr model is then applied to forecast multi scaler spi series over the 1 3 6 and 12 monthly forecasting horizons to investigate its cability to predict drought events over multiple timescales after incorporating the selected imfs for training perod with lagged at t 1 in the krr model different types of kernals i e linear polynomial and gaussian are tried it is remarkable that in this study the gaussian and polynomial kernals are the appropriate kernals to acquire the optimum memd sa krr and standalone krr model accuracy for validating the performance of the hybrid memd sa krr model the same imfs are picked in the testing period following the imfs of the training period for comparison purposes the rf model is also hybridized with the memd sa to design memd sa rf model the number of trees 1000 and predictors 5 are defined prior to develop the memd sa rf model further the standalone krr and standalone rf number of trees 1000 and predictors 3 models are also evaluated table 4 fig 2 illustrates the schematic understanding of the multi phase memd sa krr hybrid model the r and mse metrics were adopted to assess the memd sa krr accuracy in training against memd sa rf standalone krr and rf models the values of r and rmse generated by memd sa krr model for multi scaler spi forecasting at faisalabad are seen to be spi1 r 0 970 mse 0 045 spi3 r 0 993 mse 0 009 spi6 r 0 994 mse 0 007 and spi12 r 0 997 mse 0 003 these metrics for comparison models are memd sa rf r 0 968 spi1 0 985 spi3 0 994 spi6 0 996 spi12 mse 0 076 spi1 0 024 spi3 0 007 spi6 0 003 spi12 standalone krr r 0 829 spi1 0 906 spi3 0 917 spi6 0 923 spi12 mse 0 328 spi1 0 184 spi3 0 150 spi6 0 140 spi12 and standalone rf r 0 965 spi1 0 973 spi3 0 973 spi6 0 976 spi12 mse 0 111 spi1 0 052 spi3 0 047 spi6 0 044 spi12 equivalent metrics of the memd sa krr model for islamabad site 2 are found to be r 0 931 mse 0 144 spi1 r 0 974 mse 0 049 spi3 r 0 982 mse 0 031 spi6 r 0 993 mse 0 010 spi12 the values of r and mse generated by comparative models can be seen in table 4 similarly the proposed multi phase memd sa krr model reasonably performs better for site 3 jhelum as compared to other models consequently it is evident that the multi phase memd sa krr model accuracy in the testing phase as shown later is relatively high for the multi scaler spi forecasts at all tested locations 3 4 the performance assessing criterion the newly designed hybrid memd sa krr vs memd sa rf standalone krr and rf models were assessed using several distinct evaluation criterions during multi scaler spi forecasting the well known statistical metrics based on earlier approaches asce 1993 asce 2000 yen 1995 are employed in this work dawson et al 2007 deo et al 2016 legates and mccabe 1999 willmott 1981 willmott 1982 willmott 1984 i correlation coefficient r is defined as 17 r i 1 n sp i obs i spi obs i sp i for i spi for i i 1 n sp i obs i spi obs i 2 i 1 n sp i for i spi for i 2 ii willmott s index ewi is formulated as 18 e wi 1 i 1 n sp i for i s p i obs i 2 i 1 n sp i for i spi obs i sp i obs i spi obs i 2 0 e wi 1 iii nash sutcliffe efficiency ens value is described as 19 e ns 1 i 1 n sp i obs i s p i for i 2 i 1 n spi obs i spi for i 2 0 e ns 1 iv root mean square error rmse is mathematically derived as 20 rmse 1 n i 1 n sp i for i s p i obs i 2 v mean absolute error mae is expressed as 21 mae 1 n i 1 n sp i for i s p i obs i vi legates and mccabe s elm is expressed as 22 e lm 1 i 1 n sp i for i s p i obs i i 1 n sp i obs i spi obs i 0 e lm 1 vii relative percentage error rpe is stated as 23 rpe 1 n i 1 n sp i for i s p i obs i sp i obs i 100 in eq 23 sp i obs i and sp i for i shows the observed and forecasted i th magnitudes of the spi index spi obs i and spi for i are the observed and forecasted average of spi and n is the total number of tested data points 4 results based on the above developed artificial intelligence models drought forecasting has been performed for both short term 1 3 months and long term 6 12 months horizons by modeling spi as a drought indicator and utilizing synoptic scale climate mode indices and relevant climate datasets the memd algorithm applied in combination with sa and krr methods in this study aimed to design a multi phase drought forecast model denoted as memd sa krr the memd sa krr model is seen to be a well established model in terms with ability to extract features from multivariate data comprised of meteorological variables and climate indices hence selecting the best features out of extracted oscillatory modes to forecast the multi scaler spi series the performance is assessed with the help of some well known statistical measures visual and graphical plots with error distributions strategies in testing period 4 1 assessment of memd sa krr model using statistical measures in this paper the well designed multi phase forecasting model memd sa krr vs memd sa rf standalone krr and standalone rf models is numerically evaluated using several acceptable performance metrics the performance of memd sa krr model is appraised with memd sa rf standalone krr and standalone rf models utilizing r rmse and mae metrics in table 5 the hybrid memd sa krr model developed for faisalabad attain the highest magnitudes of r and lowest rmse and mae values in spi1 r 0 980 rmse 0 180 mae 0 133 in comparison with memd sa rf r 0 971 rmse 0 252 mae 0 188 standalone krr r 0 819 rmse 0 580 mae 0 440 and the standalone rf r 0 959 rmse 0 332 mae 0 254 model analogously the multi phase memd sa krr model attains highest accuracy to forecast spi3 spi6 and spi12 in response to the comparison models likewise the performance of memd sa krr model is significantly better for islamabad and jhelum table 5 to forecast multi scaler spi this confirms that memd sa krr model can be adopted as a well established data driven technique to forecast drought against memd sa rf standalone krr and standalone rf models table 6 uses multi scale ewi ens and elm criterion to analyse the performance of memd sa krr model vs memd sa rf standalone krr and standalone rf models the score of these metrics generated by memd sa krr model for faisalabad in spi1 are ewi 0 971 ens 0 959 and elm 0 818 followed by memd sa rf ewi 0 938 ens 0 920 and elm 0 743 standalone rf ewi 0 885 ens 0 861 and elm 0 653 and standalone krr ewi 0 634 ens 0 576 elm 0 399 models the multi phase memd sa krr models also outperform the counterpart models in forecasting medium scale i e spi3 and long term drought scenarios i e spi6 spi12 for site 2 islamabad again the proposed multi phase memd sa krr model appears to be the best model spi1 ewi 0 940 ens 0 900 and elm 0 693 spi3 ewi 0 993 ens 0 987 and elm 0 896 spi6 ewi 0 992 ens 0 983 and elm 0 884 and spi12 ewi 0 995 ens 0 991 and elm 0 915 the results of comparative models table 6 support the dominance of the multi phase memd sa krr over the comparative approaches the memd sa krr hybrid model is also assessed for forecasting short medium and long term drought for site 3 jhelum the acquired metrics evident from the present results are for spi1 ewi 0 911 ens 0 877 and elm 0 707 for spi3 ewi 0 994 ens 0 991 and elm 0 916 for spi6 ewi 0 995 ens 0 992 and elm 0 930 and for spi12 ewi 0 999 ens 0 998 and elm 0 967 the results of the comparative models can be tabulated in table 6 that also ascertain the superiority of memd sa krr model for the selected study locations table 7 demonstrates the magnitudes of rpe in terms of percentage produced in selected study regions based on the achieved precision site 1 faisalabad generates the maximum accurateness in forecasting multi scaler spi index where memd sa krr rpe 2 91 for spi12 rpe 6 51 for spi6 rpe 18 75 for spi3 and rpe 34 86 for spi1 jhelum is the second most responsive site in forecasting multiple drought indexes using a multi phase memd sa krr model followed by islamabad respectively the proposed multi phase memd sa krr model was seen to exhibit lower percentage errors rpe in all study locations for long term drought forecasting within the range of 10 recommended threshold for an excellent model classification 4 2 assessment of memd sa krr using visual and error distributions plots the empirical cumulative distribution function ecdf fig 3 analyses different forecasting abilities plots for each study sites the hybrid memd sa krr model was seen reasonably well against memd sa rf standalone krr and standalone rf models the generated error 0 to 1 5 for faisalabad and jhelum while 0 to 2 for islamabad in forecasting multi scaler spi fig 3 clearly proves that memd sa krr model was the most precise and responsive model fig 4 compares the memd sa krr vs the memd sa rf standalone krr and standalone rf models in the form of a boxplot the sign demonstrates the larger forecasting error fe as an outliers of the testing period the distributed error fe is confirmed with a much smaller quartile and was attained by memd sa krr method in each study location to forecast multi scaler spi followed by memd sa rf the standalone rf and standalone krr by analyzing fig 4 the preciseness of the hybrid memd sa krr method appeared to be healthier than the comparative counterparts to present accurate accounts on the forecasting ability of the prescribed data driven models fig 5 demonstrates a detailed interpretation by plotting the frequency distribution of datasets using memd sa krr method s forecasting error fe together with the relevant comparison models the acquired fe error of the memd sa krr approach was within the smallest error range 2 to forecast spi1 1 for spi3 spi6 and spi12 for faisalabad islamabad and jhelum fig 5 depicts the overall performance of the well designed memd sa krr method was better as compared to other models fig 6 describe a more tangible and conclusive information by plotting taylor diagram in terms of a statistical demonstration of how accurate the forecasted and observed multi scaler spi coincides using their correlation coefficient r for site 1 faisalabad the r of the memd sa krr model with actual was around 0 98 spi1 trailed by memd sa rf 0 97 spi1 standalone rf 0 95 spi1 and standalone krr 0 80 spi1 respectively again the multi phase memd sa krr approach was found nearer to the actual spi6 with r memd sa krr 0 999 memd sa rf 0 99 standalone rf 0 96 and standalone krr 0 90 similarly the multi phase memd sa krr was nearer in case of spi6 and spi12 in relation to other comparative models this argument is also established in site 2 and 3 where the proposed multi phase memd sa krr is closely matched with observed spi1 spi3 spi6 and spi12 as compared to the counterpart models fig 7 displays a scatterplot with goodness of fit and r2 between forecasted and observed multi scaler spi index the proposed multi phase memd sa krr model is clearly better than comparative methods in terms of r2 memd sa krr 0 960 spi1 0 994 spi3 0 996 spi6 0 999 spi12 memd sa rf 0 945 spi1 0 985 spi3 0 993 spi6 0 997 spi12 standalone krr 0 695 spi1 0 811 spi3 0 808 spi6 0 804 spi12 standalone rf 0 923 spi1 0 952 spi3 0 952 spi6 0 948 spi12 for faisalabad the proposed multi phase memd sa krr model for other sites islamabad and jhelum is reasonably good compared to counterpart models fig 7 on the basis of attaining the larger r2 value the well established memd sa krr approach shows higher accuracy against other comparison models as it is always the case long term drought indicators are much smoother than short term for example if a drought index is analyzed over 6 or 12 monthly period the changes in the behavior of drought has a lesser fluctuation hence there are no large fluctuation spikes in the long term spi time series and this is the reason why we have a better forecasting result for these horizons that is the 6 and 12 month based spi index become more stationary due to this reason the proposed model accuracy is high in long term spi index forecasting we have checked the entire results and it appears there is no issue with our current results 5 discussion limitations and future remarks in this research work the appropriateness of the hybrid memd sa krr model compared against the memd sa rf standalone krr and standalone rf models for multiple scale droughts forecasting has been investigated in comparison with the other models the hybrid memd sa rf outperformed the alternatives for all study locations thus enlightening that the memd sa krr method was well organized and effective in extracting features from climatological variables in a tangible way the performance of memd sa krr has revealed that sa algorithm was beneficial in choosing the relevant features to assist the krr in better emulating the future multi scaler spi in addition to the overall superiority of the hybrid memd sa krr over the comparative counterpart models the results also confirmed the appropriateness of simulated annealing sa in sorting out relevant feature with the assessment criterions for memd sa krr method i e tables 4 6 were remarkably improved than the hybrid memd sa rf and standalone counterpart models since the artificial intelligence models exclusively depend on past data that may significantly affect the learning and forecasting process the outcomes here establish that an appropriate feature collection should be performed carefully before to implement data driven models this is in accordance with the strategies followed in badr and fahmy 2004 cordón garcía et al 2002 mullen et al 2009 singh et al 2012 sweetlin et al 2017 other key perception is that less number of predictors input needs minimum output associations that results a parsimonious and computationally good krr model another key outcome is the unique input imfs combinations are compulsory in periodically determining future multi scaler spi table 2 in addition to hybridizing the krr and rf models with the robust sa approach an extra improvement in model preciseness was accomplished via integration of memd model to demarcate the inputs that results into hybrid memd sa krr and memd sa rf models the memd is successfully classified and segregate the relevant features inside the climatological inputs to establish a more consistent physical foundation for a particular artificial intelligence method the usefulness of empirical mode decomposition emd ensemble empirical mode decomposition eemd complete ensemble empirical mode decomposition ceemdan and improved complete ensemble empirical mode decomposition iceemdan has been revealed in numerous studies alvanitopoulos et al 2014 hong et al 2013 wang et al 2018 wu et al 2011 yet in the current research memd algorithm is employed for concurrent data pre processing of numerous climatological predictors the memd is able to identify concurrently the signal s main frequency to capture the respective features it should be noted that the feasibility of the memd algorithm huang et al 2013 mandic et al 2013 for multi scaler drought forecasting is a major advancement performed in this research study to improve the predicting ability of the standalone krr model the performance confirmed that the memd sa krr can provide better forecasts of multi scale spi for the selected study locations in contrast to the corresponding krr and rf and hybrid memd sa rf model it was undoubtedly apparent that better understandings of the physical procedure were given to the hybrid model mainly by the memd method further enabling the artificial intelligence model to effectively capture the information in the meteorological variables and large scale climate indices in relation to modelling the spi series one plausible reason for better performance of memd sa krr compared to the standalone models is perhaps attributable to the more effective translation of information on deterministic processes in meteorological variables and climate model indices resolved into various frequency levels consequently resulting in lower model errors and improved performance for multiscale spi forecasting in addition a primal advantage of the memd as utilized in this study is its self adaptive nature huang et al 2013 mandic et al 2013 which completely based on the input predictors and involves minor human effort while decomposing the inputs furthermore the memd accurately performs data driven based time frequency analysis of the complex multivariate predictors while considering the nonlinear behaviours by means of a multi channelled dynamical process rehman and mandic 2009a another significant merit of the proposed memd algorithm is that it is able to overcome the mode alignment issues which is remained unresolved in the emd and ceemd looney and mandic 2009 further memd has the ability of decompose multivariate input data while emd and ceemd are applicable to decompose a univariate data therefore the hybrid memd sa krr approach has the potential for drought management systems with historically simulated multi scaler spi this improved forecasting tool such as memd sa krr can amicably be used in an environmental modelling system that can better forecast future drought events and utilize quickly to those requirements reducing the downtimes with growing efficiency 6 conclusions in this research paper a significant contribution towards drought modelling was made by developing a reliable drought forecast model incorporating most suitable input features derived from a suite of imfs utilizing the simulated annealing sa approach here the decomposed datasets were based on the antecedent values of the meteorological variables i e precipitation temperature humidity including the most relevant climate mode indices i e soi nino3sst nino3 4sst nino4sst pdo iod emi sam and the periodicity factor i e the monthly cycle in the model s training procedure to forecast the multi scaler spi series for 3 agricultural regions in pakistan i e faisalabad islamabad and jhelum long term monthly datasets over the period 1981 2015 were decomposed for the candidate study sites in their corresponding imfs and residual factors using multivariate empirical mode decomposition algorithm in which sa was employed to screen the most suitable imfs in respect to the actual spi index the selected imfs were incorporated into the kernel ridge regression krr algorithm to design the multi phase hybrid memd sa krr model and the subsequent model s performance was compared against the hybrid memd sa rf krr and rf model several evaluation criterions r rmse mae ewi ens elm rpe were utilized to assess the multi phase memd sa krr model in modelling drought index at multiple time scales of 1 3 6 and 12 month period based on the relatively small forecast errors according to rmse and mae and the high performance metrics utilizing correlations r ewi ens and elm the accuracy of the multi phase hybrid memd sa krr model was demonstrated to be a highly potent tool in comparison to its counterpart models e g tables 4 and 5 the rpe were found to register values of approximately 34 86 18 75 6 51 2 91 for the hybrid memd sa krr model compared with 49 15 31 28 8 79 4 29 for the hybrid memd sa rf model that contrasted a value of 60 13 51 52 26 65 23 07 for standalone rf and 84 99 79 86 47 07 48 10 for standalone krr model for the faisalabad site for forecasting spi1 spi3 spi6 and spi12 respectively a remarkable amount of geographic variability in drought model performance was also evident for the multi phase memd sa krr hybrid model using rpe where a primal performance was attained for faisalabad based on rpe as compared to islamabad and jhelum this research study was of the first kind in pakistan particularly in introducing the newly designed memd approach to resolve meteorological input datasets into their relevant decomposed input signals to select the best candidate features based on sa and to forecast multi scaler spi employing krr model expanding the scope of the proposed methodology future studies can adopt the multi phase hybrid memd sa krr model in other areas such as the prediction of rainfall streamflow flood events solar radiation and energy demand to enable policymakers in the management of the climate change and energy crises issues finally drought forecast model attained in this research study can also amicably enable governments and other relevant stakeholders in water resources and crop management including decisions on infrastructural areas e g dam or irrigation operation and other hydro physical applications in the current phase of a changing climate where models can be used to make informed decisions credit authorship contribution statement mumtaz ali writing original draft conceptualization methodology software ravinesh c deo visualization conceptualization writing review editing investigation supervision tek maraseni writing review editing nathan j downs writing review editing acknowledgement this paper has utilized wheat yield acquired from bureau of statistics government of pakistan that is duly acknowledged we acknowledge that this research project was sponsored by university of southern queensland s postgraduate research scholarship 2017 2019 awarded to the first author managed by usq graduate research school 
6285,new and improved drought models based on the world meteorological organization approved standardized precipitation index principally at multiple timescale horizons are providing significant benefits to the hydrological community by its widespread acceptance in the sub field of water resources management sustainable water use and precision agriculture in this research paper the existing challenges faced by a drought forecasting model trained at multiple time scales are resolved where a new multi phase multivariate empirical mode decomposition model integrated with simulated annealing and kernel ridge regression algorithms i e memd sa krr is designed to attain significantly accurate drought forecasts for 3 agricultural sites i e faisalabad islamabad and jhelum located in pakistan utilizing the multi scalar standardized precipitation index spi time series as a target variable for characterization of drought twelve multivariate datasets derived from statistically significant lagged combinations of precipitation temperature humidity that are enriched with eight synoptic scale climate mode indices and periodicity are utilized in designing a new drought model the study constructs a hybrid memd sa krr model where firstly the data are partitioned into their respective training and testing subsets after creating historically lagged spi at timescale t 1 secondly the memd algorithm is conditioned to demarcate multivariate climate indices from their training and testing sets separately into their decomposed intrinsic mode functions imfs and residues thirdly the sa method is employed to decide the most suitable imfs finally the krr algorithm is applied to the selected imfs to forecast multi scaler spi at 1 3 6 and 12 monthly forecast horizon the results are benchmarked with random forest integrated with memd and sa to develop the memd sa rf equivalent model the multi phase memd sa krr model is tested geographically in pakistan revealing that the memd sa krr hybrid model generates reliable performance in forecasting multi scaler spi series relative to the comparative models based on error analysis metrics the hybrid drought model incorporating the most pertinent synoptic scale climate drivers as the model inputs has significant implications for hydrological applications and water resources management including its potential use in drought policy and drought recovery plans keywords hybrid drought forecast model multivariate empirical mode decomposition simulated annealing kernel ridge regression 1 introduction drought is characterized as a climatological menace that can occur in arid semi arid or tropical rainforest zones keyantash and dracup 2002 vicente serrano 2016 wilhite et al 2000 drought events can last from short to long period ranging from one month to four years as recent climate change significantly affects rainfall patterns vicente serrano 2016 drought severely disturbs water resources agriculture crops energy supply and industrial sectors and it is a growing concern deo et al 2009 ipcc 2012 mcalpine et al 2007 yaseen et al 2018b long term drought significantly poses challenges to groundwater reservoirs and also cause significant water scarcity cai and cowan 2008 and the related socio economic costs dijk et al 2013 wittwer et al 2002 hydrologists and government and non government based policy makers planning to develop new strategies to manage drought risks for water management bates et al 2008 deo et al 2017c mishra and singh 2011 mouatadid et al 2018 hydrological decision support systems are considerably as important standards in characterizing different watersheds and their scenarios chen et al 2013 gebremariam et al 2014 romagnoli et al 2017 sommerlot et al 2016 but this model s accuracy can rest on the physics initial conditions and calibrations based on conceptual i e regional scale sub models and the spatio temporal aspects of the input and output variables he et al 2014 sun et al 2012 recently artificial intelligence models that do not require any details about the physics of the watershed or any associated hydrological behaviors are seen to have a good ability to forecast precipitation run off streamflow and drought events ali et al 2018b deo et al 2017a deo and şahin 2016 deo et al 2017a deo et al 2019 mouatadid et al 2018 prasad et al 2017 yaseen et al 2018a such models that can rely purely on how climate variables and related large scale indices change over time can provide great insights into a drought forecasting and forewarning system drought can be quantified by large scale climate indices in different geographic zones which is adopted to monitor drought intensity mishra and singh 2010 mishra and singh 2011 traditionally the palmer drought severity index pdsi was developed to monitor drought palmer 1965 the pdsi is beneficial in handling lengthy drought period but is not recommended for of high run off region mishra and singh 2010 mishra and singh 2011 crop moisture index cmi is developed to handle the complexities in pdsi especially to quantify agronomic droughts palmer 1968 cmi uses the rainfall record rankings to determine the positive and negative precipitation anomalies which are useful for the short term offset byun and wilhite 1999 designed the effective drought index edi for meteorological and agricultural drought events which only based on precipitation the spi is designed to overcome these hurdles and is a worldwide acceptable standard metric to monitor and investigate drought scenarios modelling spi as advocated in this research study is beneficial for future drought assessment as follows 1 this index can monitor water deficiency scenarios that based on statistical rainfall distribution ranging from monthly to seasonal and annual 2 it is considered to be a globally acceptable standardized metric dependent on normalized rainfall deficits hayes et al 1999 mckee et al 1993 yuan and zhou 2004 that models the drought behavior in a climatological diverse regions almedeij 2016 choubin et al 2016 svoboda et al 2012 3 handling multiple timescales of drought probabilistically the spi is an instrument to inspect soil moisture condition svoboda et al 2012 artificial intelligence models have been adopted to develop drought forecasting strategies based on spi utilizing several environmental parameters 1 santos et al 2009 and jalalkamali et al 2015 designed a multilayer perceptron artificial neural network to forecast the spi in us and iran respectively and 2 wavelet based adaptive neuro fuzzy inference system were developed to estimate spi by shirmohammadi et al 2013 in azerbaijan for the details of spi based drought forecasting please refer to e g adamowski et al 2012 ali et al 2018b bonaccorso et al 2003 cancelliere et al 2006 choubin et al 2016 deo et al 2017c guttman 1999 hayes et al 1999 jalalkamali et al 2015 moreira et al 2015 moreira et al 2008 paulo and pereira 2007 sönmez et al 2005 in this research work we develop an spi based hybrid artificial intelligence model for a drought prone region in pakistan where historical drought events have severely hindered socio economic and agricultural production report 1950 2015 to address some of the challenges faced due to drought events khan and gadiwala 2013 has analyzed drought patterns using spi while xie et al 2013 has forecasted spi at several spatio temporal scales in pakistan recently ahmed et al 2016 has characterized seasonal drought using spi in balochistan pakistan and ali et al 2018a modelled drought events based on lagged data of spi using adaptive neuro fuzzy inference system based ensemble anfis ensemble approach similarly ali et al 2018b has utilized some of the primary climate based datasets to forecast the spi series however studies on future drought models utilizing different synoptic scale climate mode indices are still very limited particularly in the agricultural region of pakistan owing to the variability in climate based for a drought model a suite of multi resolution analytical tools can be useful to extract embedded features in a non static time series signal that are related to a drought variable and thus they may help to improve an existing drought model to resolve this challenge the empirical mode decomposition emd method formalized by huang et al 1998 can provide a useful alterative tool to improve existing drought models as it is able to isolate the largely fluctuating signals into their respective smaller and more clearly resolved frequency components to improve a drought model since its inception the emd method has gained attention due to its self adaptability alvanitopoulos et al 2014 the emd is completely data dependent thus making it greatly useful to extract relevant features without any loss of information further the decomposed prominent features preserve the physical structure of the input temporally wu et al 2011 due to its capability to improve the forecasting accuracy of artificial intelligence models the emd algorithm has been integrated with artificial neural network ann model proving it to be a successful tool to forecast environmental variables such as solar radiation alvanitopoulos et al 2014 for instance wang et al 2018 trialed emd and local mean decomposition integrated with least squares support vector machine to forecast solar radiation in spite of its recent applicability a key issue with emd and its variant algorithm s is that it can only applicable to decompose a univariate data colominas et al 2014 torres et al 2011 wu and huang 2009 for example only the significant antecedent lagged dataset of spi can be used to forecast the future drought series this is a major limitation of the emd algorithm since the variation in drought events is immensely reliant on dynamically driven climatological factors such as atmospheric circulation so the incorporation of the relevant synoptic scale climate indices e g soi that modulate drought events is extremely important therefore these input variables need to be intelligently used into the artificial intelligence model in addition to the antecedent drought index series the recent work of ali et al 2018a which appears to be the only study that has attempted to forecasted the medium and long period droughts has used the significant lags of spi to forecast the future spi series further ali et al 2018b has employed climate dataset to forecast spi only over a short term one month period following earlier studies ali et al 2018a ali et al 2018b on short medium and long term drought forecasting this study aims to operate large scale climate indices and climate predictors to subsequently extract most if not all pertinent features where an memd rehman and mandic 2009a based hybridized modelling approach is developed to forecast multi scaler spi index the memd is an advance generalized form of emd and ceemd which demarcates multivariate inputs to performs accurate investigation of composite and nonlinear procedures rehman and mandic 2009a additionally the memd fixes the mode alignment problems arise in the joint analysis within a multi dimensional data looney and mandic 2009 the applicability of memd can be seen clearly in forecasting of evapotranspiration adarsh et al 2017 soil water hu and si 2013 crude oil price he et al 2016 solar radiation prasad et al 2019 and iceberg drift andersson et al 2017 yet this the present research is a pilot application of this novel technique in drought forecasting particularly for the agricultural region of pakistan further this paper follows earlier methodology quilty and adamowski 2018 aiming to partitioned data prior developing hybrid memd sa krr model this approach is used to circumvent the issues associated with other kinds of decomposition based models that were raised in earlier studies quilty and adamowski 2018 the novelty of this research study lies in the enhanced capability of the memd algorithm to address the non stationarity issues encountered in the model design process by using concurrent transformation of model inputs into their decomposed components that are likely to improve an existing drought forecasting model as revealed later in results section the primary issues related to the selection of best imfs i e patterns in drought model input series which are not known a priori are determined by the proposed approach by an implementation of a robust feature selection process simulated annealing sa algorithm hence a careful integration of memd and sa algorithm with kernel ridge regression krr is made to generate a much improved hybrid forecast model denoted as the memd sa krr the model is also benchmarked against memd sa rf and a standalone i e krr and rf model to forecast multi scaler spi at 1 3 6 and 12 months tested at 3 drought rich locations 2 theoretical framework a summary of the artificial intelligence model based on krr memd and sa approaches with its comparative models is now presented 2 1 kernel ridge regression krr model krr model is a machine learning model based on kernels and a ridge regression approach zhang et al 2013 which is used to deal with over fitting in the regression using regularization and the kernel technique to capture non linear relationships you et al 2018 mathematically krr can be formulated as 1 arg min 1 q o 1 q f o y o 2 λ f h 2 2 f o p 1 q α p φ x p x o where h is the hilbert normed space in eq 1 zhang et al 2013 for a given m b y m kernel matrix k is constructed by k p o φ x p x o from selected input data where y is the input q by 1 regressand vector and α is the q by 1 unknown solution vector equation 2 reduces to the following 3 k λ q i y 4 y o 1 q α o φ x o x the krr method in the model training stage is approximated α by solving eq 3 whereas this α is used in the testing phase to predict the regression of unknown sample x in eq 4 further the krr algorithm searches for optimum α and λ from the parameter set in krr linear polynomial and gaussian kernels are used to get the optimum accuracy alaoui and mahoney 2015 vovk 2013 welling 2013 you et al 2018 mathematically linear polynomial and gaussian kernel are defined as 5 φ x p x o x p t x o 6 φ x p x o x p t x o r d 7 φ x p x o exp x p x o 2 2 σ 2 where t represents the transpose and d is the dimension of the vector 2 2 multivariate empirical mode decomposition memd method the memd technique is able to fix the issues of mode mixing which handle the drawback of exhaustiveness and time consuming to understand the mathematical structure of memd we need to study the emd theory the emd is described as 8 γ s k 1 l c k s r l s where γ s c k s and r l s are representing input data the kth imf and remainder residue respectively the memd formulated by rehman and mandic 2009b uses multivariate data to decompose into multiple dimensions of imfs to avoid the issues of mode mixing incorporating white gaussian noise ur rehman and mandic 2011 the mean m s is derived as following 9 m s 1 p q 1 p e θ q s where e θ q s is referred to envelop curves with length of the vectors s 10 r s γ s m s the term r s is called a multi dimensional imf satisfying the stopping criterion the memd algorithm has been utilized in analyzing signal processing huang et al 2013 mandic et al 2013 and hydrology hu and si 2013 she et al 2015 the memd is a self adaptive algorithm which makes no assumptions a priori about the composition of the signal hu and si 2013 she et al 2015 the memd uses spline interpolation between maxima and minima to successively trace out imfs where each imf is a single periodic oscillator huang et al 2013 mandic et al 2013 the imfs cannot be predicted prior it is empirically observed from the signal since the imfs can change over time memd makes no assumptions about the stationarity of the signal or the signal components and is therefore better suited to non linear signals when analyzing signals from complex systems huang et al 2013 mandic et al 2013 2 3 simulated annealing sa model the sa is a bio inspired feature selection algorithm to find a suitable solution to an optimization technique elleithy and fattah 2012 the sa is an adaptive non deterministic algorithm which has been extensively used as optimization technique such as traveling salesman peng et al 1996 computer generated holograms taniguchi et al 1997 power efficiency wilson 1997 and heat exchangers athier et al 1997 the basic algorithm of sa involves the following steps 1 create a randomly appropriate solution 2 determine the cost of solution using some cost function 3 create another random neighbouring solution 4 compute again cost of the above new solution 5 if the new solution cost is less than the old solution cost then move to the new solution otherwise go to the following step 6 6 follow again stages 3 5 until an optimum solution is determined 2 4 random forest rf bootstraping and bagging is basically ensemble learning techinques which creates classfiers and sums the final outcomes in terms of decision trees breiman 1996 schapire et al 1998 the rf model is fundamentally a decision tree model which adopts randomly a bagging approach in the forecasting scenarios breiman 1996 each node is separated randomly by chosing preeminent possible predictors to improve accuracy that are robust to avoid overfitting breiman 2001 the steps followed in the designing of rf can be given as 1 generate bootstrapping of n trees by incorporating the predictors variables with n denotes the number of trees 2 the randomly input predictors sample m try is created to choose maximum predictors splitting by growing an unpruned regression tree 3 cumulate the aggregative predictions of n trees to forecast multi scaler spi the applicabilty of rf model can be seen in soil attribute prediction moore et al 1993 hydrology moore et al 1991 environmental management ascough ii et al 2008 drought forecasting chen et al 2012 solar index estimation deo et al 2017b rainfall forecasting ali et al 2018c and most recently forecasting soil moisture prasad et al 2018 for more comprehensive studies on rf model readers are referred to breiman 2001 liaw and wiener 2002 prasad et al 2018 robert et al 1998 segal 2004 2 5 multi scale standardized precipitation index spi the spi quantifies the wet and dry scenarios based on statistical probability theory prior to design the proposed multi phase memd sa krr model the multi scaler spi index was computed by incorporating precipitation ptcn data mckee et al 1993 in the following equation 8 11 g p t c n 1 β α γ α ptcn α 1 e x β where g ptcn indicates the probability density function α and β are the parameters determined by the maximum likelihood solution whereas γ shows the gamma function further the cumulative probability is defined as 12 g p t c n 0 p g p t c n d r f 1 β α γ α 0 p x α 1 e p β d ptcn by substituting n ptcn β in eq 13 13 g p t c n 1 γ α 0 n n α 1 e n d n the cumulative probability reduces to the following form when ptcn 0 h ptcn p 1 p g ptcn 14 with p represents the probability of zero which determines the spi index viz 15 spi n ε 0 ε 1 n ε 2 n 2 1 ω 1 n ω 2 n 2 ω 3 n 3 0 5 h p t c n 1 0 n ε 0 ε 1 n ε 2 n 2 1 ω 1 n ω 2 n 2 ω 3 n 3 0 h p t c n 0 5 where ε 0 ε 1 ε 2 ε 3 ω 1 ω 2 and ω 3 are arbitrary constants with magnitudes ε 0 2 515517 ε 1 0 802853 ε 3 0 010328 ω 1 1 432788 ω 2 0 189269 and ω 3 0 001308 mckee et al 1993 drought is characterized into three categories as moderate 1 5 spi 1 0 severe 2 0 spi 1 5 and extreme spi 2 0 3 materials and method 3 1 data twelve meteorological data series precipitation ptcn temperature t humidity h southern oscillation index soi sea surface temperatures nino3sst nino3 4sst nino4sst pacific decadal oscillation pdo indian ocean dipole iod el nino southern oscillation modoki index emi southern annular mode sam and periodicity at a monthly t lag of t 1 are acquired from pakistan meteorological department pmd 2016 national climate prediction centre nicholls 2004 sst 2018 joint institute of the study of the atmosphere and ocean jisao 2018 bureau of meteorology australia bma 2018 japan agency for marine earth science jamstec 2018 and from the british antarctic survey bas 2018 any precipitation less than 0 1 mm was replaced with the corresponding averaged value for the climatological period the multi scaler spi index was computed in r programing software using the rainfall time series data from 1981 to 2015 despite inherent complexities associated with accurate forewarning of drought synoptic scale climate mode indices that are strongly correlated with drought occurrence mishra and singh 2010 morid et al 2007 özger et al 2012 can provide a consensus on the overall behaviour of drought events as these indices can have a significant influence on rainfall and streamflow patterns ali et al 2018c andreoli and kayano 2005 chiew et al 1998 deo and şahin 2016 mcbride and nicholls 1983 mcgregor et al 2014 nicholls 1983 prasad et al 2017 yaseen et al 2018a yaseen et al 2018b for example the well known association of the inter decadal pacific oscillation over the pacific ocean is seen to influence the el nino southern oscillation enso phenomena that governs the intensity of drought events dai 2013 salinger et al 2001 the negative period of soi brings el nino episodes whereas the positive values of soi launches la nina events adnan et al 2017 philander 1983 the northern atlantic oscillation nao also has a significant influence from seasonal to inter decadal variability on atmosphere and environmental variables dickson et al 2000 hurrell 1995 souriau and yiou 2001 the iod across the eastern indian ocean carries hefty showers over east africa while drought and forest fires across the indonesian zone adnan et al 2017 ashok et al 2001 furthermore sea surface temperatures nino3sst nino3 4sst nino4sst over the southeast indian ocean may also lead to heavy precipitation priya et al 2015 terray et al 2007 the enso modoki index emi based on strong anomalous warming in the central tropical pacific and cooling in the eastern and western tropical pacific potentially impacts the temperature and rainfall patterns around the globe due to ocean atmosphere and the unique tri polar sea level pressure pattern ashok et al 2007 similarly the sam significantly influences the monsoon rainfall anomaly pal et al 2017 3 2 study locations the study locations utilized in this work are faisalabad islamabad and jhelum displayed in fig 1 the geographical climatological and drought statistics of these locations is described in table 1 further it also presents the multi scaler spi index faisalabad is categorized as desert with average yearly rainfall of 375 mm and temperature 24 2 c and it is located in the grasslands of northeast punjab table 1 servey 2016 major crops growing in faisalabad are wheat rice cotton sugarcane maize different vegetables and fruits islamabad is the capital city of pakistan it experiences a subtropical climate with four seasons winter spring summer and autumn with average monsoon and annual precipitations of 790 8 mm and 1142 1 mm respectively the heaviest precipitation in islamabad was 620 mm month recorded in july 2002 jhelum is situated in the pothohar region of the punjab province pakistan agriculture highly depends on precipitation in punjab the average annual rainfall is about 1000 mm in the rainy season of monsoon department 2010 pmd 2016 3 3 development of multi phase memd sa krr model the multi phase memd sa krr model is developed in matlab r2016b programming environment the math works inc usa all the simulations were obtained operating pentium 4 2 93 ghz dual core central processing unit the data are partitioned straightly 60 and 40 prior into training and testing subsets respectively following by quilty and adamowski 2018 as it is the most common approach for data partitioning cannas et al 2006 the first 21 years from 1981 to 2001 data were used to train the memd sa krr model while the remaining 14 years data 2002 20015 were utilized for testing moreover the cross validation or any data randomized approach cannot be adopted as time series data by definition occur in a temporal order sequence and this order or sequence must be preserved in order to keep the structure of the series intact bergmeir and benítez 2012 antecedent time time lagged inputs i e ptcn t h soi nino3sst nino3 4sst nino4sst pdo iod emi sam and periodicity at t 1 are used to develop the hybrid model to enable spi1 1 month spi3 3 month spi6 6 month and spi12 12 month forecasts as elicudated in the following steps 3 3 1 phase 1 memd process the memd method is applie to demarcates the input time series variables into imfs and residuals the input variables were incoporated in ptcn t h soi nino3sst nino3 4sst nino4sst pdo iod emi sam periodicity for decompostion by the memd algorithm additionally the predefined parameters include the ensemble number n 500 and the amplitude of the added white noise ε 0 2 ouyang et al 2016 ren et al 2015 wang et al 2013 wu and huang 2009 to acquire the same number of imfs in training and testing period the memd method is controlled using total projection a stop vector i e tolerance and threshold values and stopping criterion see table 2 total one hundred twenty imfs with residuals table 3 for multi scaler spi in faisalabad sites is extracted with each input has imfs 9 residual 1 whereas for islamabad this number is ninety six with each input has imfs 7 residual 1 for jhelum the memd algorithm disolved the input predictors into imfs 8 residual 1 no of total imfs 108 in case of spi1 spi6 spi12 while imfs 7 residual 1 no of total imfs 96 for spi3 the cross correlation is a linear method which determine the linear relationship between input predictor and target data we avoided this method and used sa method instead to select from a pool of variables as the multiple climate input predictors have significant non linearity moreover we have 12 input predictors and decomposed by memd into several sub components the resulting number of inputs is very large i e approx 120 this means if we use a manual method e g cross correlation there is a large amount of work but it uses linear method and so will not generate smart way of selecting the best inputs instead we have used sa which is a non linear approach so this method is more suitable than correlation analysis we have identified this issue and revised the paper 3 3 2 phase 2 sa algorithm the sa approach is adopted to choose the most appropriate imfs for only the training period using a feature selection strategy for model development further some parameters were defined prior that includes the number of maximum iterations 20 and the parameter initial temperature 10 the number of selected best imfs feature is kept 25 which were defined prior to run the sa model the selected imfs for each training period are described in table 3 for testing periods those imfs are used for model validation based on selected imfs of the training dataset 3 3 3 phase 3 normalization process the data are normalized between 0 1 using eq 13 and due invertible nature of the normalization the results will not be affected hsu et al 2003 the normalization is carried out following eq 16 to handle large variation in the data hsu et al 2003 16 θ norm θ θ min θ max θ min in eq 16 θ represents the input output θ min is the minimum value θ max is the maximum value of the data and θ norm is the corresponding normalized numeric value 3 3 4 phase 4 kernal ridge regression krr method in the final phase of the modeling the krr model is then applied to forecast multi scaler spi series over the 1 3 6 and 12 monthly forecasting horizons to investigate its cability to predict drought events over multiple timescales after incorporating the selected imfs for training perod with lagged at t 1 in the krr model different types of kernals i e linear polynomial and gaussian are tried it is remarkable that in this study the gaussian and polynomial kernals are the appropriate kernals to acquire the optimum memd sa krr and standalone krr model accuracy for validating the performance of the hybrid memd sa krr model the same imfs are picked in the testing period following the imfs of the training period for comparison purposes the rf model is also hybridized with the memd sa to design memd sa rf model the number of trees 1000 and predictors 5 are defined prior to develop the memd sa rf model further the standalone krr and standalone rf number of trees 1000 and predictors 3 models are also evaluated table 4 fig 2 illustrates the schematic understanding of the multi phase memd sa krr hybrid model the r and mse metrics were adopted to assess the memd sa krr accuracy in training against memd sa rf standalone krr and rf models the values of r and rmse generated by memd sa krr model for multi scaler spi forecasting at faisalabad are seen to be spi1 r 0 970 mse 0 045 spi3 r 0 993 mse 0 009 spi6 r 0 994 mse 0 007 and spi12 r 0 997 mse 0 003 these metrics for comparison models are memd sa rf r 0 968 spi1 0 985 spi3 0 994 spi6 0 996 spi12 mse 0 076 spi1 0 024 spi3 0 007 spi6 0 003 spi12 standalone krr r 0 829 spi1 0 906 spi3 0 917 spi6 0 923 spi12 mse 0 328 spi1 0 184 spi3 0 150 spi6 0 140 spi12 and standalone rf r 0 965 spi1 0 973 spi3 0 973 spi6 0 976 spi12 mse 0 111 spi1 0 052 spi3 0 047 spi6 0 044 spi12 equivalent metrics of the memd sa krr model for islamabad site 2 are found to be r 0 931 mse 0 144 spi1 r 0 974 mse 0 049 spi3 r 0 982 mse 0 031 spi6 r 0 993 mse 0 010 spi12 the values of r and mse generated by comparative models can be seen in table 4 similarly the proposed multi phase memd sa krr model reasonably performs better for site 3 jhelum as compared to other models consequently it is evident that the multi phase memd sa krr model accuracy in the testing phase as shown later is relatively high for the multi scaler spi forecasts at all tested locations 3 4 the performance assessing criterion the newly designed hybrid memd sa krr vs memd sa rf standalone krr and rf models were assessed using several distinct evaluation criterions during multi scaler spi forecasting the well known statistical metrics based on earlier approaches asce 1993 asce 2000 yen 1995 are employed in this work dawson et al 2007 deo et al 2016 legates and mccabe 1999 willmott 1981 willmott 1982 willmott 1984 i correlation coefficient r is defined as 17 r i 1 n sp i obs i spi obs i sp i for i spi for i i 1 n sp i obs i spi obs i 2 i 1 n sp i for i spi for i 2 ii willmott s index ewi is formulated as 18 e wi 1 i 1 n sp i for i s p i obs i 2 i 1 n sp i for i spi obs i sp i obs i spi obs i 2 0 e wi 1 iii nash sutcliffe efficiency ens value is described as 19 e ns 1 i 1 n sp i obs i s p i for i 2 i 1 n spi obs i spi for i 2 0 e ns 1 iv root mean square error rmse is mathematically derived as 20 rmse 1 n i 1 n sp i for i s p i obs i 2 v mean absolute error mae is expressed as 21 mae 1 n i 1 n sp i for i s p i obs i vi legates and mccabe s elm is expressed as 22 e lm 1 i 1 n sp i for i s p i obs i i 1 n sp i obs i spi obs i 0 e lm 1 vii relative percentage error rpe is stated as 23 rpe 1 n i 1 n sp i for i s p i obs i sp i obs i 100 in eq 23 sp i obs i and sp i for i shows the observed and forecasted i th magnitudes of the spi index spi obs i and spi for i are the observed and forecasted average of spi and n is the total number of tested data points 4 results based on the above developed artificial intelligence models drought forecasting has been performed for both short term 1 3 months and long term 6 12 months horizons by modeling spi as a drought indicator and utilizing synoptic scale climate mode indices and relevant climate datasets the memd algorithm applied in combination with sa and krr methods in this study aimed to design a multi phase drought forecast model denoted as memd sa krr the memd sa krr model is seen to be a well established model in terms with ability to extract features from multivariate data comprised of meteorological variables and climate indices hence selecting the best features out of extracted oscillatory modes to forecast the multi scaler spi series the performance is assessed with the help of some well known statistical measures visual and graphical plots with error distributions strategies in testing period 4 1 assessment of memd sa krr model using statistical measures in this paper the well designed multi phase forecasting model memd sa krr vs memd sa rf standalone krr and standalone rf models is numerically evaluated using several acceptable performance metrics the performance of memd sa krr model is appraised with memd sa rf standalone krr and standalone rf models utilizing r rmse and mae metrics in table 5 the hybrid memd sa krr model developed for faisalabad attain the highest magnitudes of r and lowest rmse and mae values in spi1 r 0 980 rmse 0 180 mae 0 133 in comparison with memd sa rf r 0 971 rmse 0 252 mae 0 188 standalone krr r 0 819 rmse 0 580 mae 0 440 and the standalone rf r 0 959 rmse 0 332 mae 0 254 model analogously the multi phase memd sa krr model attains highest accuracy to forecast spi3 spi6 and spi12 in response to the comparison models likewise the performance of memd sa krr model is significantly better for islamabad and jhelum table 5 to forecast multi scaler spi this confirms that memd sa krr model can be adopted as a well established data driven technique to forecast drought against memd sa rf standalone krr and standalone rf models table 6 uses multi scale ewi ens and elm criterion to analyse the performance of memd sa krr model vs memd sa rf standalone krr and standalone rf models the score of these metrics generated by memd sa krr model for faisalabad in spi1 are ewi 0 971 ens 0 959 and elm 0 818 followed by memd sa rf ewi 0 938 ens 0 920 and elm 0 743 standalone rf ewi 0 885 ens 0 861 and elm 0 653 and standalone krr ewi 0 634 ens 0 576 elm 0 399 models the multi phase memd sa krr models also outperform the counterpart models in forecasting medium scale i e spi3 and long term drought scenarios i e spi6 spi12 for site 2 islamabad again the proposed multi phase memd sa krr model appears to be the best model spi1 ewi 0 940 ens 0 900 and elm 0 693 spi3 ewi 0 993 ens 0 987 and elm 0 896 spi6 ewi 0 992 ens 0 983 and elm 0 884 and spi12 ewi 0 995 ens 0 991 and elm 0 915 the results of comparative models table 6 support the dominance of the multi phase memd sa krr over the comparative approaches the memd sa krr hybrid model is also assessed for forecasting short medium and long term drought for site 3 jhelum the acquired metrics evident from the present results are for spi1 ewi 0 911 ens 0 877 and elm 0 707 for spi3 ewi 0 994 ens 0 991 and elm 0 916 for spi6 ewi 0 995 ens 0 992 and elm 0 930 and for spi12 ewi 0 999 ens 0 998 and elm 0 967 the results of the comparative models can be tabulated in table 6 that also ascertain the superiority of memd sa krr model for the selected study locations table 7 demonstrates the magnitudes of rpe in terms of percentage produced in selected study regions based on the achieved precision site 1 faisalabad generates the maximum accurateness in forecasting multi scaler spi index where memd sa krr rpe 2 91 for spi12 rpe 6 51 for spi6 rpe 18 75 for spi3 and rpe 34 86 for spi1 jhelum is the second most responsive site in forecasting multiple drought indexes using a multi phase memd sa krr model followed by islamabad respectively the proposed multi phase memd sa krr model was seen to exhibit lower percentage errors rpe in all study locations for long term drought forecasting within the range of 10 recommended threshold for an excellent model classification 4 2 assessment of memd sa krr using visual and error distributions plots the empirical cumulative distribution function ecdf fig 3 analyses different forecasting abilities plots for each study sites the hybrid memd sa krr model was seen reasonably well against memd sa rf standalone krr and standalone rf models the generated error 0 to 1 5 for faisalabad and jhelum while 0 to 2 for islamabad in forecasting multi scaler spi fig 3 clearly proves that memd sa krr model was the most precise and responsive model fig 4 compares the memd sa krr vs the memd sa rf standalone krr and standalone rf models in the form of a boxplot the sign demonstrates the larger forecasting error fe as an outliers of the testing period the distributed error fe is confirmed with a much smaller quartile and was attained by memd sa krr method in each study location to forecast multi scaler spi followed by memd sa rf the standalone rf and standalone krr by analyzing fig 4 the preciseness of the hybrid memd sa krr method appeared to be healthier than the comparative counterparts to present accurate accounts on the forecasting ability of the prescribed data driven models fig 5 demonstrates a detailed interpretation by plotting the frequency distribution of datasets using memd sa krr method s forecasting error fe together with the relevant comparison models the acquired fe error of the memd sa krr approach was within the smallest error range 2 to forecast spi1 1 for spi3 spi6 and spi12 for faisalabad islamabad and jhelum fig 5 depicts the overall performance of the well designed memd sa krr method was better as compared to other models fig 6 describe a more tangible and conclusive information by plotting taylor diagram in terms of a statistical demonstration of how accurate the forecasted and observed multi scaler spi coincides using their correlation coefficient r for site 1 faisalabad the r of the memd sa krr model with actual was around 0 98 spi1 trailed by memd sa rf 0 97 spi1 standalone rf 0 95 spi1 and standalone krr 0 80 spi1 respectively again the multi phase memd sa krr approach was found nearer to the actual spi6 with r memd sa krr 0 999 memd sa rf 0 99 standalone rf 0 96 and standalone krr 0 90 similarly the multi phase memd sa krr was nearer in case of spi6 and spi12 in relation to other comparative models this argument is also established in site 2 and 3 where the proposed multi phase memd sa krr is closely matched with observed spi1 spi3 spi6 and spi12 as compared to the counterpart models fig 7 displays a scatterplot with goodness of fit and r2 between forecasted and observed multi scaler spi index the proposed multi phase memd sa krr model is clearly better than comparative methods in terms of r2 memd sa krr 0 960 spi1 0 994 spi3 0 996 spi6 0 999 spi12 memd sa rf 0 945 spi1 0 985 spi3 0 993 spi6 0 997 spi12 standalone krr 0 695 spi1 0 811 spi3 0 808 spi6 0 804 spi12 standalone rf 0 923 spi1 0 952 spi3 0 952 spi6 0 948 spi12 for faisalabad the proposed multi phase memd sa krr model for other sites islamabad and jhelum is reasonably good compared to counterpart models fig 7 on the basis of attaining the larger r2 value the well established memd sa krr approach shows higher accuracy against other comparison models as it is always the case long term drought indicators are much smoother than short term for example if a drought index is analyzed over 6 or 12 monthly period the changes in the behavior of drought has a lesser fluctuation hence there are no large fluctuation spikes in the long term spi time series and this is the reason why we have a better forecasting result for these horizons that is the 6 and 12 month based spi index become more stationary due to this reason the proposed model accuracy is high in long term spi index forecasting we have checked the entire results and it appears there is no issue with our current results 5 discussion limitations and future remarks in this research work the appropriateness of the hybrid memd sa krr model compared against the memd sa rf standalone krr and standalone rf models for multiple scale droughts forecasting has been investigated in comparison with the other models the hybrid memd sa rf outperformed the alternatives for all study locations thus enlightening that the memd sa krr method was well organized and effective in extracting features from climatological variables in a tangible way the performance of memd sa krr has revealed that sa algorithm was beneficial in choosing the relevant features to assist the krr in better emulating the future multi scaler spi in addition to the overall superiority of the hybrid memd sa krr over the comparative counterpart models the results also confirmed the appropriateness of simulated annealing sa in sorting out relevant feature with the assessment criterions for memd sa krr method i e tables 4 6 were remarkably improved than the hybrid memd sa rf and standalone counterpart models since the artificial intelligence models exclusively depend on past data that may significantly affect the learning and forecasting process the outcomes here establish that an appropriate feature collection should be performed carefully before to implement data driven models this is in accordance with the strategies followed in badr and fahmy 2004 cordón garcía et al 2002 mullen et al 2009 singh et al 2012 sweetlin et al 2017 other key perception is that less number of predictors input needs minimum output associations that results a parsimonious and computationally good krr model another key outcome is the unique input imfs combinations are compulsory in periodically determining future multi scaler spi table 2 in addition to hybridizing the krr and rf models with the robust sa approach an extra improvement in model preciseness was accomplished via integration of memd model to demarcate the inputs that results into hybrid memd sa krr and memd sa rf models the memd is successfully classified and segregate the relevant features inside the climatological inputs to establish a more consistent physical foundation for a particular artificial intelligence method the usefulness of empirical mode decomposition emd ensemble empirical mode decomposition eemd complete ensemble empirical mode decomposition ceemdan and improved complete ensemble empirical mode decomposition iceemdan has been revealed in numerous studies alvanitopoulos et al 2014 hong et al 2013 wang et al 2018 wu et al 2011 yet in the current research memd algorithm is employed for concurrent data pre processing of numerous climatological predictors the memd is able to identify concurrently the signal s main frequency to capture the respective features it should be noted that the feasibility of the memd algorithm huang et al 2013 mandic et al 2013 for multi scaler drought forecasting is a major advancement performed in this research study to improve the predicting ability of the standalone krr model the performance confirmed that the memd sa krr can provide better forecasts of multi scale spi for the selected study locations in contrast to the corresponding krr and rf and hybrid memd sa rf model it was undoubtedly apparent that better understandings of the physical procedure were given to the hybrid model mainly by the memd method further enabling the artificial intelligence model to effectively capture the information in the meteorological variables and large scale climate indices in relation to modelling the spi series one plausible reason for better performance of memd sa krr compared to the standalone models is perhaps attributable to the more effective translation of information on deterministic processes in meteorological variables and climate model indices resolved into various frequency levels consequently resulting in lower model errors and improved performance for multiscale spi forecasting in addition a primal advantage of the memd as utilized in this study is its self adaptive nature huang et al 2013 mandic et al 2013 which completely based on the input predictors and involves minor human effort while decomposing the inputs furthermore the memd accurately performs data driven based time frequency analysis of the complex multivariate predictors while considering the nonlinear behaviours by means of a multi channelled dynamical process rehman and mandic 2009a another significant merit of the proposed memd algorithm is that it is able to overcome the mode alignment issues which is remained unresolved in the emd and ceemd looney and mandic 2009 further memd has the ability of decompose multivariate input data while emd and ceemd are applicable to decompose a univariate data therefore the hybrid memd sa krr approach has the potential for drought management systems with historically simulated multi scaler spi this improved forecasting tool such as memd sa krr can amicably be used in an environmental modelling system that can better forecast future drought events and utilize quickly to those requirements reducing the downtimes with growing efficiency 6 conclusions in this research paper a significant contribution towards drought modelling was made by developing a reliable drought forecast model incorporating most suitable input features derived from a suite of imfs utilizing the simulated annealing sa approach here the decomposed datasets were based on the antecedent values of the meteorological variables i e precipitation temperature humidity including the most relevant climate mode indices i e soi nino3sst nino3 4sst nino4sst pdo iod emi sam and the periodicity factor i e the monthly cycle in the model s training procedure to forecast the multi scaler spi series for 3 agricultural regions in pakistan i e faisalabad islamabad and jhelum long term monthly datasets over the period 1981 2015 were decomposed for the candidate study sites in their corresponding imfs and residual factors using multivariate empirical mode decomposition algorithm in which sa was employed to screen the most suitable imfs in respect to the actual spi index the selected imfs were incorporated into the kernel ridge regression krr algorithm to design the multi phase hybrid memd sa krr model and the subsequent model s performance was compared against the hybrid memd sa rf krr and rf model several evaluation criterions r rmse mae ewi ens elm rpe were utilized to assess the multi phase memd sa krr model in modelling drought index at multiple time scales of 1 3 6 and 12 month period based on the relatively small forecast errors according to rmse and mae and the high performance metrics utilizing correlations r ewi ens and elm the accuracy of the multi phase hybrid memd sa krr model was demonstrated to be a highly potent tool in comparison to its counterpart models e g tables 4 and 5 the rpe were found to register values of approximately 34 86 18 75 6 51 2 91 for the hybrid memd sa krr model compared with 49 15 31 28 8 79 4 29 for the hybrid memd sa rf model that contrasted a value of 60 13 51 52 26 65 23 07 for standalone rf and 84 99 79 86 47 07 48 10 for standalone krr model for the faisalabad site for forecasting spi1 spi3 spi6 and spi12 respectively a remarkable amount of geographic variability in drought model performance was also evident for the multi phase memd sa krr hybrid model using rpe where a primal performance was attained for faisalabad based on rpe as compared to islamabad and jhelum this research study was of the first kind in pakistan particularly in introducing the newly designed memd approach to resolve meteorological input datasets into their relevant decomposed input signals to select the best candidate features based on sa and to forecast multi scaler spi employing krr model expanding the scope of the proposed methodology future studies can adopt the multi phase hybrid memd sa krr model in other areas such as the prediction of rainfall streamflow flood events solar radiation and energy demand to enable policymakers in the management of the climate change and energy crises issues finally drought forecast model attained in this research study can also amicably enable governments and other relevant stakeholders in water resources and crop management including decisions on infrastructural areas e g dam or irrigation operation and other hydro physical applications in the current phase of a changing climate where models can be used to make informed decisions credit authorship contribution statement mumtaz ali writing original draft conceptualization methodology software ravinesh c deo visualization conceptualization writing review editing investigation supervision tek maraseni writing review editing nathan j downs writing review editing acknowledgement this paper has utilized wheat yield acquired from bureau of statistics government of pakistan that is duly acknowledged we acknowledge that this research project was sponsored by university of southern queensland s postgraduate research scholarship 2017 2019 awarded to the first author managed by usq graduate research school 
6286,in a semi arid climate groundwater fed endorheic lakes often have shallow depths and large areas of associated wetlands these lake wetland aquifer systems are sensitive to land use and climate changes difficulties of modeling such systems include a tracking several thousand lakes and wetlands lws and b evaluating major drivers of trends and fluctuations in lw areal coverage integrated hydrologic modeling of such systems is prohibitively expensive we combine traditional groundwater flow modeling modflow driven by modern and projected groundwater recharge gr estimates with digital elevation model dem based geographic information system gis terrain analyses this approach can be a useful diagnostic tool for estimates of decade averaged lw areas and numbers and stream baseflow facilitating conceptualization and design of more detailed modeling studies we apply the approach to simulate dynamics of thousands of groundwater fed lws in the nebraska sand hills usa under gcm projected gr changes compared to the baseline period of 2000 2009 the model indicates an increase by 35 from 2083 to 2804 km2 of decade averaged lw area by the end of the 21st century under the median gr projection with gr averaging 55 3 mm yr from 2010 through 2099 5 increase under the same gr projection the overall numbers of lw is simulated to increase by 18 from 7331 to 8666 the uncertainty range in the hydrologic response of the system caused by differences among gr projections wet and dry are also evaluated keywords lake aquifer interactions groundwater recharge gr groundwater modeling climate change aquifer response times nebraska sand hills nsh 1 introduction 1 1 general interest in lake wetland aquifer system modeling there are large lake and wetland systems that are fed by large aquifers lake wetland aquifer systems lwass in the usa canada finland hungary china kazakhstan mongolia and other countries in humid and arid climates over the last few years modeling of such systems underwent substantial developments due to the interest in land use and climate change effects with the advent of more detailed and accessible land surface dems the increase of computing resources and numerical methods developments the opportunity to incorporate and holistically simulate land surface atmosphere vegetation vadose zone groundwater and surface water processes by integrated hydrologic models ihms is being explored examples include ecologic analyses of dwindling wetland landscape connectivity in bird and animal habitats mcintyre et al 2014 wu and lane 2016 dynamics of lakes and wetlands lws that are have high densities in some areas such as the prairie pothole region liu and schwartz 2011 nebraska sand hills nsh bleed and flowerday 1998 and texas playas mcintyre et al 2014 these dynamic surface water bodies with relatively flat beds vary in areas and may dry up completely over annual or centennial time scales dependent on degrees of their connection with and changes to aquifers for example surveys in the nsh show that the number of lakes and their total area may vary from about 1 500 to 2 500 and from 317 to 455 km2 respectively over the course of a few years rundquist 1983 1 2 modeling approaches for lake wetland aquifer systems the key issue is feasibility of simulating such systems at all is because of the shallow water depth which is a result of the confluence of three associated factors 1 data availability original regional synthetic or calibrated 2 modeling facility proper models fast running computers efficient codes and trained personnel and 3 economic feasibility when society is willing to finance costly endeavors with long term commitments a criterion dictating advances in modeling appears to be the number of lws in the simulated lwas or more accurately number of wet topographic depressions that may hold surface water with fluctuating water levels models with distributed parameters using land surface topography have certain advantages over early models with lumped parameters johnson and poiani 2016 mcintyre et al 2014 one of the most appealing is visualization of lw dynamics the most common tools include modflow and hydrogeosphere hgs codes summary of some characteristics of typical studies of lwas interactions using models with distributed parameters is shown in table 1 here studies are listed in order of increasing numbers of studied lws and includes only those modeling studies with explicit lw consideration e g allen et al 2010 li et al 2008 woldeamlak et al 2007 smerdon et al 2008 and kazmierczak et al 2016 have used original physical and chemical data from their field studies for modeling the individual lakes in the tradition of early studies e g krabbenhoft et al 1990 but with more detailed data newer software better process representation and about the same labor resources ala aho et al 2015 2017 explored a number of wet topographic depressions on the order of 10 to 20 and apply the principle of parsimony to parameterizing the problem by simplifying spatial properties and calibration to available observations liu et al 2016 were working along the same lines but their numbers are still far from the order of magnitude of numbers of lw needed for regional forecasts although hgs software and concepts seem conceptually appealing due to holistic nature of the process treatment the more traditional modflow and related approach present a reasonable alternative in such circumstances feinstein et al 2010 hunt et al 2013 with existing packages and simplified vadose zone modeling treatment of individual lws is feasible if the bathymetry data are available or can be approximated unfortunately these methods cannot readily treat coalescence or disaggregation of lwass these studies were directed to reproduce relatively recent and short lake histories usually at decadal scale in duration unlike modeling studies of land use and climate change effects on groundwater resources e g li et al 2008 liu et al 2016 thus the important topic of quantitative and computerized analyses of wet topographic depression hydrologic dynamics and uncertainties due to climate change projections have not been fully explored based on analyses of the state of the art in modeling of shallow lakes in table 1 it is safe to assume that developments in ihm that permit simulations of thousands of shallow groundwater fed lws will require time on the scale of a decade or more see section 1 of the supplementary material for further discussion 1 3 path forward the major controlling factor that the aquifer has on lw long term dynamics is groundwater recharge gr there are two approaches in modeling gr as a driver of hydrological systems in one approach all estimates of gr are made internally when climate data from gcms or regional climate models are used as an input to an ihm e g erler et al 2019 goderniaux et al 2009 however it is the most resource consuming aspect of computations even when simplified inputs and efficient methods for solving the nonlinear richards equation are employed e g erler et al 2019 further modeling results must always be corroborated with external evaluation of gr ala aho et al 2015 p 398 in another computationally more effective approach climate data are converted into gr externally when feedbacks between the groundwater system and atmospheric processes are weak allen et al 2010 therefore parallel to the developments of ihm methodologies there is a need of radically different and certainly less detailed approaches for forecasting of future gr changes such studies will benefit from using the ad hoc approaches tailored to regional specifics and the areal coverage and numbers of thousands of closed basin seepage lws can be modeled numerically in a distributed fashion one example is the recent study by hanson et al 2018 that treated the entire system of 3 962 lakes in an area of 310 km2 by further simplification of modeling groundwater using the semi analytical package gflow often these models are dictated by quite pragmatic considerations e g climate effects on stream temperatures as in hunt et al 2013 another salient example of such an approach is the study of the nubian aquifer dynamics and existing surface water bodies over the holocene to modern days voss and soliman 2014 it used location of oases and lakes in the past when climate was much wetter than the present day and gr was ubiquitous to calibrate the groundwater model this philosophy is succinctly summarized by voss and soliman 2014 p 465 as follows the practical approach was to identify the fewest possible number of hydrogeologic parameters that control the long term and short term responses of the aquifer to stresses then find ranges of controlling parameter values that allow the modeled aquifer response to match the long and short term data and identify parameter values that give the best fit to these data the resulting simply structured model captures the main behavior long and short term evolution of hydraulic head including response to climate change and pumping progress in the field of gr evaluation healy 2010 and forecasts in the 21st century e g meixner et al 2016 niraula et al 2017 can generate gr outside of the modeling process and simulate the lwas more efficiently e g allen et al 2010 woldeamlak et al 2007 a brief review of studies of climate change effects on gr was recently published by smerdon 2017 generating gr estimates externally from the lwas modeling process minimizes the computational burden allowing focus on the critical traits including uncertainty in the model stresses and parameters this approach is taken for our study 1 4 scope of study modeling studies reviewed above treated lakes in humid climates when lake existence and numbers are relatively insensitive to climate and land use changes in the semi arid dune environment of the nsh the largest continuous grass stabilized dune region in the western hemisphere bleed and flowerday 1998 several thousands of shallow groundwater fed lws exist hydraulically connected with the northern high plains aquifer they depend on gr the highest rates of the entire high plains aquifer region crosbie et al 2013 rossman et al 2018b and section 2 of the supplementary material provides more information on previous studies of gr in the nsh the high gr rates in the nsh are attributed to relatively high permeability sediments making up the sandy dunes and providing steady groundwater discharge baseflow to the platte river and its tributaries irrigation in this nsh area is limited 4 of land area sridhar et al 2006 making the region ideal for analyses of the effects of climate change on streams lakes and wetlands these issues are a matter of significant concern bathke et al 2014 considering the ecological and economic significance of water resources in the region and beyond in contrast to humid environments numbers and areas of these shallow lws are prone to dramatic fluctuations from year to year havril et al 2017 tao et al 2015 like in every transient modeling exercise our approach includes three primary steps 1 defining the initial conditions 2 selecting the projections of the system driver gr and 3 concluding step of forecasting the system dynamics for these projections the first two steps with corresponding tasks results and necessary data are listed in more detail in table 2 briefly summarizing previous background publications rossman et al 2014 2018a b szilagyi and jozsa 2013 szilagyi et al 2011 the third step is simulating the future numbers and areas of lws together with stream baseflow driven by projected gr changes which is the subject of this publication 2 study area the study area is located in northwestern nebraska covering the central and western nsh region with a total area of 39 297 km2 fig 1 a the nsh is characterized by a repeating pattern of large up to 130 m tall grass stabilized dunes adjacent to thousands of individually small average 0 3 km2 lakes bleed and flowerday 1998 and when combined with wetlands they cover more than 2 000 km2 fig 1b lake density is highest in the western part where a stream network is absent due to past dune migration loope et al 1995 miao et al 2007 sand dunes are composed of eolian sand that is well sorted and fine to medium grained holocene sand deposits overlie up to 300 m of alluvial sand silt and coarse clastic sediments of the ogallala group that all together make up the thickest part of the high plains aquifer loope et al 1995 topography is highly variable sloping from west to east from an elevation of 1 357 to 618 m fig 1b endorheic lakes are located in topographic depressions in the landscape and their salinity may be partially explained by the interaction of regional groundwater flow and local land surface topography zlotnik et al 2009 minimal overland flow and high gr rates occur due to the high hydraulic conductivity of the sandy soils bleed and flowerday 1998 szilagyi et al 2011 ample evidence suggests that areas of groundwater connected lakes have varied considerably in the past in the nsh concurrently with mobilization of the dunes during prolonged drought periods the most recent of which was about 700 years ago miao et al 2007 and during pluvial periods when terraces were created up to 1 5 m above the current shorelines of lakes gosselin et al 1994 more details on the study area can be found in rossman et al 2018a b 3 groundwater flow model development 3 1 initial conditions feasibility of using hgs transient integrated groundwater surface water simulations with a variably saturated vadose zone in the nsh was assessed by direct testing but deemed to be prohibitively time consuming modflow 2005 was used to simulate unconfined groundwater flow through visual modflow flex software v 2014 2 limitations of the lake package requiring a priori knowledge of lake locations was avoided by use of the approach and methodology outlined in rossman et al 2018a water table simulations over the 21st century using parameters obtained from the calibrated steady state initial conditions rossman et al 2018a and specific yield from previous groundwater research in the nsh cf chen and chen 2004 houston et al 2013 the sources of information on the model domain discretization and assigning layer elevations and defining boundary conditions and hydrological parameters table 2 the new results based on transient simulations are provided in this paper the study area was discretized into 192 rows 348 columns and 1 layer 39 640 active grid cells the horizontal dimensions of the grid cells are a uniform 1 1 km the base of the model layer corresponds with the base of the high plains aquifer lateral boundary conditions were simulated as constant heads assigned using a raster from interpolations of a map of the 1995 water table summerside et al 2001 in the uplands and along major bounding rivers streams were simulated using the drain and river packages and distributed gr was assigned to the model using the recharge package the aquifer base was assigned as no flow boundary conditions for the transient model this study are identical to the available steady state model rossman et al 2018a and are presented in fig 2 a initial conditions for the transient model were based on estimates of average gr for years 2000 2009 from szilagyi and jozsa 2013 with adjustments made during calibration of the steady state model rossman et al 2018a the final set of values of gr are displayed in fig 2 the performance of the steady state model calibration i e initial conditions for the transient model is presented as a comparison between simulated and estimated water table contours in fig 3 necessary information regarding the initial conditions for the transient model are presented in section 3 of the supplementary material 3 2 groundwater recharge in the 21st century the proliferation of various hydroclimate projections may reach near one hundred in certain geographical areas tillman et al 2016 2017 adding to uncertainty in possible gr projections allen et al 2010 rossman et al 2018b proposed to select only a representative subset of all possible gr projections based on descriptive statistics of cumulative gr over the projection period to assess uncertainty cumulative gr spatially averaged over the model domain was evaluated for each of 48 available projections and the median projected cumulative potential gr at the end of the 21st century was selected among them median gr projection wet and dry gr projections were defined as the individual gcm runs producing spatially averaged gr closest to 1 standard deviation from the median cumulative gr fig 4 a illustrates the average gr over the modeling domain with decadal stress periods over the 21st century in the study area although rossman et al 2018b showed that cumulative potential gr does not obey a gaussian distribution and is dependent on the duration of the projection period such a pragmatic approach effectively reduces the entire projection data set down to three gr raster file sets each containing nine decades of data requiring only 27 total arrays of gr to represent future potential gr spanning 90 years further details on the sources of data and methodology associated with the gr applied to the transient model are provided in rossman et al 2018b and section 4 of the supplementary material 3 3 inference of lakes and wetlands groundwater fed endorheic lakes and wetlands emerge in topographic depressions in dune environments when the climate is conducive for significant gr resulting in a high water table waterlogging and numerous lws this simple fact was used for inference of lw locations including their simulation with a groundwater model cf rossman et al 2018a zlotnik et al 2009 using the transient groundwater model simulation lws were inferred based on the water table position with respect to dem elevations of the land surface here we provide a brief summary of the methods used carried out with arcgis and matlab 1 simulated heads were first resampled from 1 km to 30 m resolution bilinear interpolation to match the resolution of the dem and available landsat land cover dataset dappen et al 2007 2 water table depth was calculated as the difference between dem elevations and simulated hydraulic heads 3 lake and wetland pixels on 30 x 30 m grid were identified the third steps involves classification based on the water table depth lakes were identified where simulated hydraulic head exceeds the land surface in this study it was not necessary to subtract lake depth from the dem elevations or use bathymetry because lakes in the nsh are shallow 0 8 m on average with depths that are smaller than the average error associated with calibrated heads rmse equals 2 8 m in addition without subtracting out lake depths which would have to be approximated for most lakes as the average depth a satisfactory match to the spatial distribution of mapped lws from observational datasets was achieved as for wetlands it was found that identifying them in locations where the water table is within 3 m of land surface produced the best match with observed wetland areas this depth generally exceeds the root depth of prairie grasses about 1 m york et al 2002 but appears representative of the extinction depth chen and chen 2004 it should be considered as an averaged regional parameter in less explicit form it was used in woldeamlak et al 2007 and voss and soliman 2014 as for transient simulations where heads drop to more than 3 m below the dem those pixels are mapped as an upland i e lws dry up one of the potential extensions of such approach could be using these cutoff water table depths used to classify lws as calibration parameters with lw area numbers and or spatial distribution as the targets 4 results and discussion the conversion of potential gr defined as precipitation minus evapotranspiration et at the land surface into actual gr occurs after some time lag occurring as soil moisture changes induce pressure head changes that traverse the vadose zone this vadose zone lag time was assessed using the interpreted map of the water table and land surface elevation from a dem gr rates corresponding to different projections regional data on soils a near unity hydraulic gradient and calculated pressure based vadose zone pressure pulse velocity as proposed and presented by rossman et al 2014 travel times were estimated for each 1 km2 pixel in the nsh and resulting histograms for median wet and dry gr projections were compared in rossman et al 2014 these histograms indicated that at least 75 of the region has vadose zone lag times shorter than 10 years even during the driest periods rossman et al 2018b therefore we use a 10 year stress period in forcing the transient groundwater model which also used a 10 year time step potential gr values become good proxies for actual gr in every pixel and lag times caused by the vadose zone can be ignored the 10 year time step is large enough to neglect differences between potential and actual gr and short enough to detect longer term climate variations in gr otherwise a simple time lag based procedure could be used for conversion of potential gr into actual gr at the water table the transient groundwater flow model was set up to run using nine decadal stress periods with annual time steps beginning at the start of 2010 and ending at the end of 2099 to coincide with the decadal stress periods of the future gr projections the specific yield was assigned uniform value of 0 17 based on the average of all estimates from boreholes in the study area houston et al 2013 and corroborated by values used by chen and chen 2004 who developed a modflow model of a small part of the central nsh of course this value can vary spatially and is directly proportional to groundwater response times in certain areas 4 1 dynamics of stream baseflow for the dry median and wet gr projections the corresponding changes in modeled stream baseflow to interior streams were assessed with the transient model as illustrated in fig 4b selection of wet and dry gr projections based on the median projection is affected by the duration of the projection period because they are based on cumulative gr for a specific period of time in this case 90 years in fig 4a the dry projection is temporarily wetter than the median projected gr for the 2010 2019 decade only cumulative gr over the entire period of projections permits the proper designation of median wet and dry projections net stream baseflow of interior streams was 4 22 million m3 d over the calibration period representing the 2000 2009 decade by 2099 baseflows under future gr projections change by between 27 8 and 15 8 with a median scenario change of 2 3 relative to the modern period baseline 2000 2009 assessment of stabilized baseflow assuming gr rates for the 2010 2019 period are fixed for an unknown time timing assessed further in section 4 4 yields a range of baseflow changes from 1 3 to 30 8 with the median gr projection causing the least change and the dry gr projection causing a change of 5 6 refer to fig 4b for baseflow changes and absolute values simulated over the 21st century supplies of drinking water and availability of surface water for agricultural production downstream of the nsh region would be impacted by the projected changes in baseflow with human and economic benefits from increases in baseflow more specifically baseflow changes would potentially result in changes of surface water rights administration in nebraska s prior appropriation doctrine system ndnr 2019 affecting the downstream parts of the niobrara river loup river and platte river basins and availability of water supplies for the numerous cities and industrial and agricultural users in the state that depend on groundwater originating in the nsh region 4 2 dynamics of lakes and wetlands lakes and wetlands are sustained by groundwater discharge when the water table is at shallow depths beneath the land surface following the analysis of water table depth at the 30x30 m scale maps of the locations of lws from the calibrated model run results from rossman et al 2018a and from transient simulations under future gr projections including median wet and dry were produced fig 5 only those lws in the sand hills region gold color in fig 5 were evaluated quantitatively the changing areal coverage and numbers of lws simulated over the nsh region excluding river valleys in the 21st century corresponding to the gr dynamics fig 4a with decadal variation are shown in fig 6 the most likely projection represented by the one producing the cumulative gr closest to the median of 48 gcm and emissions scenario combinations suggests that gr will increase lake areas from 719 km2 to 1 191 km2 and wetlands from 1 364 km2 to 1 613 km2 by the end of the 21st century combined the overall lw areas increase from 2 083 km2 to 2 804 km2 similarly the median gr projection causes the simulated number of lakes to increase to 4 358 from 3 348 and lakes together comingling with wetlands to 8 666 from 7 331 the relative percent differences between 2099 and 2009 for all three model gr projections are shown in table 3 large changes in lw areal coverage and numbers throughout the 21st century and beyond are expected if the climate shifts toward a new regime due to long term decades to centuries climate transition the assessment methodology does not provide seasonal variations of gr which leads to uncertainty in seasonality of the lw areas and numbers but yields a strong indication of decadal trends the dry gr projection would have a great impact on habitat loss for over 300 bird species what makes the nsh the second most productive waterfowl area in the u s bleed and flowerday 1998 and would decrease the ability of sub irrigated meadows of the nsh regions interdunal valleys to receive groundwater which would impact the production of hay responsible for a third of all beef cattle in nebraska gosselin et al 2000 with respect to ecological significance there are several unique plant and fish species living in the fen ecosystems of the nsh that appear to be intolerant of drastic habitat and or environmental changes harvey et al 2007 that could be affected additionally increased dust emissions from salt layers exposed on dry lakebeds is a concern with reduced gr zlotnik et al 2012 as well as re mobilization of the sand dunes if dry conditions are extreme or persist for long periods of time something that hasn t happened at a regional scale for over 700 years miao et al 2007 4 3 lake wetland aquifer system response to groundwater recharge trends while the baseflow dynamics simulated show a close correspondence with the changes to decade average gr the lw areas are more sensitive to increases in gr than to decreases this hydrologic effect is based on geomorphology of the dune covered nsh region namely a complex nested hierarchical structure of depressions wu and lane 2016 the gr increase causes the water table to rise which envelops progressively larger areas the resulting non linear and rapid expansion of depressions reflects bathymetric or morphologic features of depressions in the nsh for small ranges of water level fluctuations hayashi and van der kamp 2000 proposed a power relationship between depth h and area a in the prairie pothole region usa where the rate of a increase is attenuated at higher h values this model does not consider coalescence of lake wetland areas and excludes smooth pothole edges therefore liu and schwartz 2011 proposed another sine function based model indicating that after a certain h level the rate of increase in a increases and may lead to coalescence of lws however bathymetry of lakes in the nsh is different the lake bottom surfaces are flat some observations show that the edges evolve into terraces on the order of 1 5 m height gosselin et al 1994 the eolian deposition of sand flattened the lake bottoms while steeper banks are the result of continuous wave action there is ample evidence of both processes based on wind speed in western nebraska zlotnik et al 2012 therefore flooding and coalescence occur when small changes in h occur near this threshold terrace height as discussed by wu and lane 2016 fig 5 if reduction of gr occurs the lake and groundwater levels below this threshold result in a much weaker relationship between water level and lw areas this phenomenon is supported by modeling the gr changes of similar magnitude but different sign see dry and wet projection lw areas in table 3 the large uncertainty bounds in the impacts to lws in the nsh quantified through use of dry and wet gr projections may also be affected by simplifying assumptions as well since not all components of the hydrologic cycle have been treated in a coupled way that can account for feedbacks the potential limitation in this regard has to do with not explicitly accounting for lw contraction expansion impacting et and thereby gr as the assumption is that this feedback is weak however possible caveats of this assumption have been verified by applying the methodology to identification of lakes for the initial baseline condition comparison of obtained lw values with landsat data proved to be satisfactory rossman et al 2018a perhaps coupling of the model to account for these feedbacks would result in less drastic changes in lw area since the feedbacks are negative et increases when lakes expand we also note that overland flow is not accounted for in the simulations which would also require modification to the processes incorporated 4 4 groundwater response time groundwater response time τ in regional groundwater systems is defined as the time for hydraulic heads to approach a new equilibrium after hydraulic perturbation and is commonly estimated using the time scale τ defined as τ α s l 2 t where l is horizontal system scale s is storage coefficient or specific yield sy t is effective transmissivity and α is form factor alley et al 2002 green et al 2011 rousseau gueutin et al 2013 urbano et al 2004 york et al 2002 in the study of rousseau gueutin et al 2013 95 proximity of simulated heads to the new equilibrium was advocated for determining τ this estimate originates from transient solutions of the linearized groundwater flow equation for hydraulic head in an aquifer strip bounded by streams or aquifer rectangle encompassing the actual domain naturally application of this seemingly simple equation is fraught with uncertainties because each of the required parameters is subject to interpretation a the irregular shape of 2d and 3d domains prevents unique definition of l b variety of perturbation types in the domain and boundary conditions e g change in gr versus change in water levels and boundary shapes affects α by orders of magnitude c transmissivity and storage coefficient estimates must be defined as a single number and d differences exist in the definition of approach as a new equilibrium is reached e g 95 proximity for example evaluation of τ for the high plains aquifer based on this approach resulted in estimates that varied by three orders of magnitude rousseau gueutin et al 2013 table 3 in fact different parts of the domain arrive to equilibrium over different periods with changing gr for our domain fig 2 the characteristic length may vary between the inter stream distances internal boundaries in the eastern part and length or width of the western part where streams are absent then l may range from 40 km to 100 km with t 2 000 m2 day sy 0 17 and α 1 the τ estimates range from 370 to 2 330 years one can expect that the baseflow from different gr projections may stabilize faster due to the lesser sensitivity to the local head disequilibria our transient numerical simulations for finding τ utilized the initial conditions of the aquifer in equilibrium with gr for years 2000 2009 then gr projections for the next decade 2010 2019 were taken from median wet and dry projections with spatially averaged gr values for this decade of 55 0 mm yr 109 3 mm yr and 62 7 mm yr respectively transient computations with identical gr were carried out with 10 year time steps after year 2020 until equilibrium was achieved i e stream baseflow stabilized and the value of τ was taken when the baseflow change reached 95 of the range between the two steady state baseflow rates time to equilibrium varied depending on the gr projection selected and was approximately 10 yrs 120 yrs and 40 yrs corresponding with the median wet and dry projections respectively similar time scales were found in the nsh when studying effects of trends in stream runoff using a lumped parameter model istanbulluoglu et al 2012 some currently occurring hydrological processes may be consequences of paleoclimatic changes over a century or more with the system adjusting to the new gr regime istanbulluoglu et al 2012 urbano et al 2004 york et al 2002 due to fundamental difficulties in finding paleoclimatic history e g bracht flyr et al 2013 voss and soliman 2014 all current models of large aquifers derive a historic condition based on either a steady state simulation or results of a spin up yet they all share this caveat which depends on the spatial scale of the system 5 conclusions the impacts of gcm projected gr changes on a system of thousands of endorheic groundwater fed lws of the nsh in the 21st century have been investigated feasibility of treatment of such systems using ihms seems limited in the foreseeable future instead of tracking the dynamics of hundreds and thousands of numerous lakes individually lws were inferred by comparing simulated elevations of the water table and land surface using terrain analysis and mapping approach with a gis this approach relies on gr which is evaluated externally without explicit consideration of the surface subsurface domain coupling the overall approach allows for traditional groundwater modeling methods to be applied the groundwater modeling begins with a simulation of the initial steady state conditions existing in equilibrium with modern gr estimated for the period from 2000 to 2009 over the 21st century three gcm based gr projections are selected from the multitude of existing models and greenhouse gas emission scenarios 16 cgms and three emissions scenario based on cumulative gr over the 90 year future period 2010 2099 minor future gr changes are predicted for the nsh region based on the median most likely gr projection however large uncertainty exists in these projections as indicated by the standard deviation of cumulative p et potential gr by the end of century this statistical measure standard deviation of the range of uncertainty determined the selected future wet and dry gr projections projected potential gr rates averaged over the future period are 74 7 mm yr 55 3 mm yr and 37 6 mm yr for the wet median and dry gr projections respectively these rates compare to a rate of 52 6 mm yr for the modern baseline gr rate as percent difference relative to the baseline these percentages are 42 5 and 29 respectively by year 2099 under the wet median and dry gr projections the number of lws is simulated to increase by 58 to 11 353 and by 18 to 8 666 and to decrease by 55 to 3 081 from a baseline of 7 331 respectively the median projection suggests that gr changes in the 21st century will increase lake areas from 719 km2 to 1 191 km2 and wetlands from 1 364 km2 to 1 613 km2 combined the overall lw areas increase from 2 083 km2 to 2 804 km2 under the median gr projection an increase by 35 by year 2099 under the wet and dry gr projections the combined areas of lws is simulated to increase by 201 to 6 261 km2 and to decrease by 71 598 km2 respectively implications from the results of this research are as follows lake and wetland areas and numbers may increase substantially for the median and wet gr projections by the end of the 21st century even one or two decades of drought or deluge may cause drastic changes in these surface water features these trends are subject to substantial uncertainty due to uncertainty in available hydroclimate projections yet a downward trends exists after 2020 for the dry gr projection that has the potential to drastically reduce stream baseflow and areal coverage of lws consequences of gr changes at large spatial scale may last significantly longer due to the long response time of the aquifer that lasts for decades to centuries therefore it is important to consider paleoclimatic history in analyses of future processes when possible emphasizing the role of initial conditions land topography and geomorphological processes in the nebraska sand hills interaction of eolian and fluvial processes in formation of the lake bathymetry result in different sensitivity of the total lw areas and numbers between gr increase and decrease and the applied methodology addresses the decadal dynamics and long term trends in the total lake and wetland area and numbers over the 21st century but offers no insight into such trends of annual or seasonal dynamics of the system some technical aspects of modeling can be improved re evaluation of dynamics of actual gr from potential gr and better estimating dynamics of et from expansion contraction of lws could be done internally by coupling the aquifer with vadose zone processes variably saturated flow and coupling subsurface with surface domain flows into one model these developments could slightly affect selection of the decadal stress periods of the groundwater model but with known trends in gr would not alter general conclusions about trends in lw dynamics or affect the fidelity of the model considering the level of uncertainty of climatic drivers and projections of the lake and wetland areas was obtained by matching simulated heads to the observed water levels with qualitative resemblance of lw distribution to the observed modern patterns landsat etc and a posteriori analysis of stream baseflow further model fidelity improvements can be achieved by revision of lw classification based on 3 m water table depth and including their spatial distribution from landsat or other remote sensing product as a target in the objective function during quantitative calibration to our knowledge this is first attempt to simulate a lake wetland aquifer system with such numbers of lws in arid conditions declaration of competing interest none acknowledgements funding was provided by the nsf igert program dge 0903469 the nebraska geological society s yatkola edwards scholarship fund and the american association of petroleum geologists we are grateful to the staff of waterloo hydrogeologic for technical support to personnel of the csd and calmit unl the nebraska department of natural resources and the usgs nebraska water science center for making available regional datasets we also acknowledge t franz and j szilagyi both unl for sharing data and their expertise appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 06 046 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
6286,in a semi arid climate groundwater fed endorheic lakes often have shallow depths and large areas of associated wetlands these lake wetland aquifer systems are sensitive to land use and climate changes difficulties of modeling such systems include a tracking several thousand lakes and wetlands lws and b evaluating major drivers of trends and fluctuations in lw areal coverage integrated hydrologic modeling of such systems is prohibitively expensive we combine traditional groundwater flow modeling modflow driven by modern and projected groundwater recharge gr estimates with digital elevation model dem based geographic information system gis terrain analyses this approach can be a useful diagnostic tool for estimates of decade averaged lw areas and numbers and stream baseflow facilitating conceptualization and design of more detailed modeling studies we apply the approach to simulate dynamics of thousands of groundwater fed lws in the nebraska sand hills usa under gcm projected gr changes compared to the baseline period of 2000 2009 the model indicates an increase by 35 from 2083 to 2804 km2 of decade averaged lw area by the end of the 21st century under the median gr projection with gr averaging 55 3 mm yr from 2010 through 2099 5 increase under the same gr projection the overall numbers of lw is simulated to increase by 18 from 7331 to 8666 the uncertainty range in the hydrologic response of the system caused by differences among gr projections wet and dry are also evaluated keywords lake aquifer interactions groundwater recharge gr groundwater modeling climate change aquifer response times nebraska sand hills nsh 1 introduction 1 1 general interest in lake wetland aquifer system modeling there are large lake and wetland systems that are fed by large aquifers lake wetland aquifer systems lwass in the usa canada finland hungary china kazakhstan mongolia and other countries in humid and arid climates over the last few years modeling of such systems underwent substantial developments due to the interest in land use and climate change effects with the advent of more detailed and accessible land surface dems the increase of computing resources and numerical methods developments the opportunity to incorporate and holistically simulate land surface atmosphere vegetation vadose zone groundwater and surface water processes by integrated hydrologic models ihms is being explored examples include ecologic analyses of dwindling wetland landscape connectivity in bird and animal habitats mcintyre et al 2014 wu and lane 2016 dynamics of lakes and wetlands lws that are have high densities in some areas such as the prairie pothole region liu and schwartz 2011 nebraska sand hills nsh bleed and flowerday 1998 and texas playas mcintyre et al 2014 these dynamic surface water bodies with relatively flat beds vary in areas and may dry up completely over annual or centennial time scales dependent on degrees of their connection with and changes to aquifers for example surveys in the nsh show that the number of lakes and their total area may vary from about 1 500 to 2 500 and from 317 to 455 km2 respectively over the course of a few years rundquist 1983 1 2 modeling approaches for lake wetland aquifer systems the key issue is feasibility of simulating such systems at all is because of the shallow water depth which is a result of the confluence of three associated factors 1 data availability original regional synthetic or calibrated 2 modeling facility proper models fast running computers efficient codes and trained personnel and 3 economic feasibility when society is willing to finance costly endeavors with long term commitments a criterion dictating advances in modeling appears to be the number of lws in the simulated lwas or more accurately number of wet topographic depressions that may hold surface water with fluctuating water levels models with distributed parameters using land surface topography have certain advantages over early models with lumped parameters johnson and poiani 2016 mcintyre et al 2014 one of the most appealing is visualization of lw dynamics the most common tools include modflow and hydrogeosphere hgs codes summary of some characteristics of typical studies of lwas interactions using models with distributed parameters is shown in table 1 here studies are listed in order of increasing numbers of studied lws and includes only those modeling studies with explicit lw consideration e g allen et al 2010 li et al 2008 woldeamlak et al 2007 smerdon et al 2008 and kazmierczak et al 2016 have used original physical and chemical data from their field studies for modeling the individual lakes in the tradition of early studies e g krabbenhoft et al 1990 but with more detailed data newer software better process representation and about the same labor resources ala aho et al 2015 2017 explored a number of wet topographic depressions on the order of 10 to 20 and apply the principle of parsimony to parameterizing the problem by simplifying spatial properties and calibration to available observations liu et al 2016 were working along the same lines but their numbers are still far from the order of magnitude of numbers of lw needed for regional forecasts although hgs software and concepts seem conceptually appealing due to holistic nature of the process treatment the more traditional modflow and related approach present a reasonable alternative in such circumstances feinstein et al 2010 hunt et al 2013 with existing packages and simplified vadose zone modeling treatment of individual lws is feasible if the bathymetry data are available or can be approximated unfortunately these methods cannot readily treat coalescence or disaggregation of lwass these studies were directed to reproduce relatively recent and short lake histories usually at decadal scale in duration unlike modeling studies of land use and climate change effects on groundwater resources e g li et al 2008 liu et al 2016 thus the important topic of quantitative and computerized analyses of wet topographic depression hydrologic dynamics and uncertainties due to climate change projections have not been fully explored based on analyses of the state of the art in modeling of shallow lakes in table 1 it is safe to assume that developments in ihm that permit simulations of thousands of shallow groundwater fed lws will require time on the scale of a decade or more see section 1 of the supplementary material for further discussion 1 3 path forward the major controlling factor that the aquifer has on lw long term dynamics is groundwater recharge gr there are two approaches in modeling gr as a driver of hydrological systems in one approach all estimates of gr are made internally when climate data from gcms or regional climate models are used as an input to an ihm e g erler et al 2019 goderniaux et al 2009 however it is the most resource consuming aspect of computations even when simplified inputs and efficient methods for solving the nonlinear richards equation are employed e g erler et al 2019 further modeling results must always be corroborated with external evaluation of gr ala aho et al 2015 p 398 in another computationally more effective approach climate data are converted into gr externally when feedbacks between the groundwater system and atmospheric processes are weak allen et al 2010 therefore parallel to the developments of ihm methodologies there is a need of radically different and certainly less detailed approaches for forecasting of future gr changes such studies will benefit from using the ad hoc approaches tailored to regional specifics and the areal coverage and numbers of thousands of closed basin seepage lws can be modeled numerically in a distributed fashion one example is the recent study by hanson et al 2018 that treated the entire system of 3 962 lakes in an area of 310 km2 by further simplification of modeling groundwater using the semi analytical package gflow often these models are dictated by quite pragmatic considerations e g climate effects on stream temperatures as in hunt et al 2013 another salient example of such an approach is the study of the nubian aquifer dynamics and existing surface water bodies over the holocene to modern days voss and soliman 2014 it used location of oases and lakes in the past when climate was much wetter than the present day and gr was ubiquitous to calibrate the groundwater model this philosophy is succinctly summarized by voss and soliman 2014 p 465 as follows the practical approach was to identify the fewest possible number of hydrogeologic parameters that control the long term and short term responses of the aquifer to stresses then find ranges of controlling parameter values that allow the modeled aquifer response to match the long and short term data and identify parameter values that give the best fit to these data the resulting simply structured model captures the main behavior long and short term evolution of hydraulic head including response to climate change and pumping progress in the field of gr evaluation healy 2010 and forecasts in the 21st century e g meixner et al 2016 niraula et al 2017 can generate gr outside of the modeling process and simulate the lwas more efficiently e g allen et al 2010 woldeamlak et al 2007 a brief review of studies of climate change effects on gr was recently published by smerdon 2017 generating gr estimates externally from the lwas modeling process minimizes the computational burden allowing focus on the critical traits including uncertainty in the model stresses and parameters this approach is taken for our study 1 4 scope of study modeling studies reviewed above treated lakes in humid climates when lake existence and numbers are relatively insensitive to climate and land use changes in the semi arid dune environment of the nsh the largest continuous grass stabilized dune region in the western hemisphere bleed and flowerday 1998 several thousands of shallow groundwater fed lws exist hydraulically connected with the northern high plains aquifer they depend on gr the highest rates of the entire high plains aquifer region crosbie et al 2013 rossman et al 2018b and section 2 of the supplementary material provides more information on previous studies of gr in the nsh the high gr rates in the nsh are attributed to relatively high permeability sediments making up the sandy dunes and providing steady groundwater discharge baseflow to the platte river and its tributaries irrigation in this nsh area is limited 4 of land area sridhar et al 2006 making the region ideal for analyses of the effects of climate change on streams lakes and wetlands these issues are a matter of significant concern bathke et al 2014 considering the ecological and economic significance of water resources in the region and beyond in contrast to humid environments numbers and areas of these shallow lws are prone to dramatic fluctuations from year to year havril et al 2017 tao et al 2015 like in every transient modeling exercise our approach includes three primary steps 1 defining the initial conditions 2 selecting the projections of the system driver gr and 3 concluding step of forecasting the system dynamics for these projections the first two steps with corresponding tasks results and necessary data are listed in more detail in table 2 briefly summarizing previous background publications rossman et al 2014 2018a b szilagyi and jozsa 2013 szilagyi et al 2011 the third step is simulating the future numbers and areas of lws together with stream baseflow driven by projected gr changes which is the subject of this publication 2 study area the study area is located in northwestern nebraska covering the central and western nsh region with a total area of 39 297 km2 fig 1 a the nsh is characterized by a repeating pattern of large up to 130 m tall grass stabilized dunes adjacent to thousands of individually small average 0 3 km2 lakes bleed and flowerday 1998 and when combined with wetlands they cover more than 2 000 km2 fig 1b lake density is highest in the western part where a stream network is absent due to past dune migration loope et al 1995 miao et al 2007 sand dunes are composed of eolian sand that is well sorted and fine to medium grained holocene sand deposits overlie up to 300 m of alluvial sand silt and coarse clastic sediments of the ogallala group that all together make up the thickest part of the high plains aquifer loope et al 1995 topography is highly variable sloping from west to east from an elevation of 1 357 to 618 m fig 1b endorheic lakes are located in topographic depressions in the landscape and their salinity may be partially explained by the interaction of regional groundwater flow and local land surface topography zlotnik et al 2009 minimal overland flow and high gr rates occur due to the high hydraulic conductivity of the sandy soils bleed and flowerday 1998 szilagyi et al 2011 ample evidence suggests that areas of groundwater connected lakes have varied considerably in the past in the nsh concurrently with mobilization of the dunes during prolonged drought periods the most recent of which was about 700 years ago miao et al 2007 and during pluvial periods when terraces were created up to 1 5 m above the current shorelines of lakes gosselin et al 1994 more details on the study area can be found in rossman et al 2018a b 3 groundwater flow model development 3 1 initial conditions feasibility of using hgs transient integrated groundwater surface water simulations with a variably saturated vadose zone in the nsh was assessed by direct testing but deemed to be prohibitively time consuming modflow 2005 was used to simulate unconfined groundwater flow through visual modflow flex software v 2014 2 limitations of the lake package requiring a priori knowledge of lake locations was avoided by use of the approach and methodology outlined in rossman et al 2018a water table simulations over the 21st century using parameters obtained from the calibrated steady state initial conditions rossman et al 2018a and specific yield from previous groundwater research in the nsh cf chen and chen 2004 houston et al 2013 the sources of information on the model domain discretization and assigning layer elevations and defining boundary conditions and hydrological parameters table 2 the new results based on transient simulations are provided in this paper the study area was discretized into 192 rows 348 columns and 1 layer 39 640 active grid cells the horizontal dimensions of the grid cells are a uniform 1 1 km the base of the model layer corresponds with the base of the high plains aquifer lateral boundary conditions were simulated as constant heads assigned using a raster from interpolations of a map of the 1995 water table summerside et al 2001 in the uplands and along major bounding rivers streams were simulated using the drain and river packages and distributed gr was assigned to the model using the recharge package the aquifer base was assigned as no flow boundary conditions for the transient model this study are identical to the available steady state model rossman et al 2018a and are presented in fig 2 a initial conditions for the transient model were based on estimates of average gr for years 2000 2009 from szilagyi and jozsa 2013 with adjustments made during calibration of the steady state model rossman et al 2018a the final set of values of gr are displayed in fig 2 the performance of the steady state model calibration i e initial conditions for the transient model is presented as a comparison between simulated and estimated water table contours in fig 3 necessary information regarding the initial conditions for the transient model are presented in section 3 of the supplementary material 3 2 groundwater recharge in the 21st century the proliferation of various hydroclimate projections may reach near one hundred in certain geographical areas tillman et al 2016 2017 adding to uncertainty in possible gr projections allen et al 2010 rossman et al 2018b proposed to select only a representative subset of all possible gr projections based on descriptive statistics of cumulative gr over the projection period to assess uncertainty cumulative gr spatially averaged over the model domain was evaluated for each of 48 available projections and the median projected cumulative potential gr at the end of the 21st century was selected among them median gr projection wet and dry gr projections were defined as the individual gcm runs producing spatially averaged gr closest to 1 standard deviation from the median cumulative gr fig 4 a illustrates the average gr over the modeling domain with decadal stress periods over the 21st century in the study area although rossman et al 2018b showed that cumulative potential gr does not obey a gaussian distribution and is dependent on the duration of the projection period such a pragmatic approach effectively reduces the entire projection data set down to three gr raster file sets each containing nine decades of data requiring only 27 total arrays of gr to represent future potential gr spanning 90 years further details on the sources of data and methodology associated with the gr applied to the transient model are provided in rossman et al 2018b and section 4 of the supplementary material 3 3 inference of lakes and wetlands groundwater fed endorheic lakes and wetlands emerge in topographic depressions in dune environments when the climate is conducive for significant gr resulting in a high water table waterlogging and numerous lws this simple fact was used for inference of lw locations including their simulation with a groundwater model cf rossman et al 2018a zlotnik et al 2009 using the transient groundwater model simulation lws were inferred based on the water table position with respect to dem elevations of the land surface here we provide a brief summary of the methods used carried out with arcgis and matlab 1 simulated heads were first resampled from 1 km to 30 m resolution bilinear interpolation to match the resolution of the dem and available landsat land cover dataset dappen et al 2007 2 water table depth was calculated as the difference between dem elevations and simulated hydraulic heads 3 lake and wetland pixels on 30 x 30 m grid were identified the third steps involves classification based on the water table depth lakes were identified where simulated hydraulic head exceeds the land surface in this study it was not necessary to subtract lake depth from the dem elevations or use bathymetry because lakes in the nsh are shallow 0 8 m on average with depths that are smaller than the average error associated with calibrated heads rmse equals 2 8 m in addition without subtracting out lake depths which would have to be approximated for most lakes as the average depth a satisfactory match to the spatial distribution of mapped lws from observational datasets was achieved as for wetlands it was found that identifying them in locations where the water table is within 3 m of land surface produced the best match with observed wetland areas this depth generally exceeds the root depth of prairie grasses about 1 m york et al 2002 but appears representative of the extinction depth chen and chen 2004 it should be considered as an averaged regional parameter in less explicit form it was used in woldeamlak et al 2007 and voss and soliman 2014 as for transient simulations where heads drop to more than 3 m below the dem those pixels are mapped as an upland i e lws dry up one of the potential extensions of such approach could be using these cutoff water table depths used to classify lws as calibration parameters with lw area numbers and or spatial distribution as the targets 4 results and discussion the conversion of potential gr defined as precipitation minus evapotranspiration et at the land surface into actual gr occurs after some time lag occurring as soil moisture changes induce pressure head changes that traverse the vadose zone this vadose zone lag time was assessed using the interpreted map of the water table and land surface elevation from a dem gr rates corresponding to different projections regional data on soils a near unity hydraulic gradient and calculated pressure based vadose zone pressure pulse velocity as proposed and presented by rossman et al 2014 travel times were estimated for each 1 km2 pixel in the nsh and resulting histograms for median wet and dry gr projections were compared in rossman et al 2014 these histograms indicated that at least 75 of the region has vadose zone lag times shorter than 10 years even during the driest periods rossman et al 2018b therefore we use a 10 year stress period in forcing the transient groundwater model which also used a 10 year time step potential gr values become good proxies for actual gr in every pixel and lag times caused by the vadose zone can be ignored the 10 year time step is large enough to neglect differences between potential and actual gr and short enough to detect longer term climate variations in gr otherwise a simple time lag based procedure could be used for conversion of potential gr into actual gr at the water table the transient groundwater flow model was set up to run using nine decadal stress periods with annual time steps beginning at the start of 2010 and ending at the end of 2099 to coincide with the decadal stress periods of the future gr projections the specific yield was assigned uniform value of 0 17 based on the average of all estimates from boreholes in the study area houston et al 2013 and corroborated by values used by chen and chen 2004 who developed a modflow model of a small part of the central nsh of course this value can vary spatially and is directly proportional to groundwater response times in certain areas 4 1 dynamics of stream baseflow for the dry median and wet gr projections the corresponding changes in modeled stream baseflow to interior streams were assessed with the transient model as illustrated in fig 4b selection of wet and dry gr projections based on the median projection is affected by the duration of the projection period because they are based on cumulative gr for a specific period of time in this case 90 years in fig 4a the dry projection is temporarily wetter than the median projected gr for the 2010 2019 decade only cumulative gr over the entire period of projections permits the proper designation of median wet and dry projections net stream baseflow of interior streams was 4 22 million m3 d over the calibration period representing the 2000 2009 decade by 2099 baseflows under future gr projections change by between 27 8 and 15 8 with a median scenario change of 2 3 relative to the modern period baseline 2000 2009 assessment of stabilized baseflow assuming gr rates for the 2010 2019 period are fixed for an unknown time timing assessed further in section 4 4 yields a range of baseflow changes from 1 3 to 30 8 with the median gr projection causing the least change and the dry gr projection causing a change of 5 6 refer to fig 4b for baseflow changes and absolute values simulated over the 21st century supplies of drinking water and availability of surface water for agricultural production downstream of the nsh region would be impacted by the projected changes in baseflow with human and economic benefits from increases in baseflow more specifically baseflow changes would potentially result in changes of surface water rights administration in nebraska s prior appropriation doctrine system ndnr 2019 affecting the downstream parts of the niobrara river loup river and platte river basins and availability of water supplies for the numerous cities and industrial and agricultural users in the state that depend on groundwater originating in the nsh region 4 2 dynamics of lakes and wetlands lakes and wetlands are sustained by groundwater discharge when the water table is at shallow depths beneath the land surface following the analysis of water table depth at the 30x30 m scale maps of the locations of lws from the calibrated model run results from rossman et al 2018a and from transient simulations under future gr projections including median wet and dry were produced fig 5 only those lws in the sand hills region gold color in fig 5 were evaluated quantitatively the changing areal coverage and numbers of lws simulated over the nsh region excluding river valleys in the 21st century corresponding to the gr dynamics fig 4a with decadal variation are shown in fig 6 the most likely projection represented by the one producing the cumulative gr closest to the median of 48 gcm and emissions scenario combinations suggests that gr will increase lake areas from 719 km2 to 1 191 km2 and wetlands from 1 364 km2 to 1 613 km2 by the end of the 21st century combined the overall lw areas increase from 2 083 km2 to 2 804 km2 similarly the median gr projection causes the simulated number of lakes to increase to 4 358 from 3 348 and lakes together comingling with wetlands to 8 666 from 7 331 the relative percent differences between 2099 and 2009 for all three model gr projections are shown in table 3 large changes in lw areal coverage and numbers throughout the 21st century and beyond are expected if the climate shifts toward a new regime due to long term decades to centuries climate transition the assessment methodology does not provide seasonal variations of gr which leads to uncertainty in seasonality of the lw areas and numbers but yields a strong indication of decadal trends the dry gr projection would have a great impact on habitat loss for over 300 bird species what makes the nsh the second most productive waterfowl area in the u s bleed and flowerday 1998 and would decrease the ability of sub irrigated meadows of the nsh regions interdunal valleys to receive groundwater which would impact the production of hay responsible for a third of all beef cattle in nebraska gosselin et al 2000 with respect to ecological significance there are several unique plant and fish species living in the fen ecosystems of the nsh that appear to be intolerant of drastic habitat and or environmental changes harvey et al 2007 that could be affected additionally increased dust emissions from salt layers exposed on dry lakebeds is a concern with reduced gr zlotnik et al 2012 as well as re mobilization of the sand dunes if dry conditions are extreme or persist for long periods of time something that hasn t happened at a regional scale for over 700 years miao et al 2007 4 3 lake wetland aquifer system response to groundwater recharge trends while the baseflow dynamics simulated show a close correspondence with the changes to decade average gr the lw areas are more sensitive to increases in gr than to decreases this hydrologic effect is based on geomorphology of the dune covered nsh region namely a complex nested hierarchical structure of depressions wu and lane 2016 the gr increase causes the water table to rise which envelops progressively larger areas the resulting non linear and rapid expansion of depressions reflects bathymetric or morphologic features of depressions in the nsh for small ranges of water level fluctuations hayashi and van der kamp 2000 proposed a power relationship between depth h and area a in the prairie pothole region usa where the rate of a increase is attenuated at higher h values this model does not consider coalescence of lake wetland areas and excludes smooth pothole edges therefore liu and schwartz 2011 proposed another sine function based model indicating that after a certain h level the rate of increase in a increases and may lead to coalescence of lws however bathymetry of lakes in the nsh is different the lake bottom surfaces are flat some observations show that the edges evolve into terraces on the order of 1 5 m height gosselin et al 1994 the eolian deposition of sand flattened the lake bottoms while steeper banks are the result of continuous wave action there is ample evidence of both processes based on wind speed in western nebraska zlotnik et al 2012 therefore flooding and coalescence occur when small changes in h occur near this threshold terrace height as discussed by wu and lane 2016 fig 5 if reduction of gr occurs the lake and groundwater levels below this threshold result in a much weaker relationship between water level and lw areas this phenomenon is supported by modeling the gr changes of similar magnitude but different sign see dry and wet projection lw areas in table 3 the large uncertainty bounds in the impacts to lws in the nsh quantified through use of dry and wet gr projections may also be affected by simplifying assumptions as well since not all components of the hydrologic cycle have been treated in a coupled way that can account for feedbacks the potential limitation in this regard has to do with not explicitly accounting for lw contraction expansion impacting et and thereby gr as the assumption is that this feedback is weak however possible caveats of this assumption have been verified by applying the methodology to identification of lakes for the initial baseline condition comparison of obtained lw values with landsat data proved to be satisfactory rossman et al 2018a perhaps coupling of the model to account for these feedbacks would result in less drastic changes in lw area since the feedbacks are negative et increases when lakes expand we also note that overland flow is not accounted for in the simulations which would also require modification to the processes incorporated 4 4 groundwater response time groundwater response time τ in regional groundwater systems is defined as the time for hydraulic heads to approach a new equilibrium after hydraulic perturbation and is commonly estimated using the time scale τ defined as τ α s l 2 t where l is horizontal system scale s is storage coefficient or specific yield sy t is effective transmissivity and α is form factor alley et al 2002 green et al 2011 rousseau gueutin et al 2013 urbano et al 2004 york et al 2002 in the study of rousseau gueutin et al 2013 95 proximity of simulated heads to the new equilibrium was advocated for determining τ this estimate originates from transient solutions of the linearized groundwater flow equation for hydraulic head in an aquifer strip bounded by streams or aquifer rectangle encompassing the actual domain naturally application of this seemingly simple equation is fraught with uncertainties because each of the required parameters is subject to interpretation a the irregular shape of 2d and 3d domains prevents unique definition of l b variety of perturbation types in the domain and boundary conditions e g change in gr versus change in water levels and boundary shapes affects α by orders of magnitude c transmissivity and storage coefficient estimates must be defined as a single number and d differences exist in the definition of approach as a new equilibrium is reached e g 95 proximity for example evaluation of τ for the high plains aquifer based on this approach resulted in estimates that varied by three orders of magnitude rousseau gueutin et al 2013 table 3 in fact different parts of the domain arrive to equilibrium over different periods with changing gr for our domain fig 2 the characteristic length may vary between the inter stream distances internal boundaries in the eastern part and length or width of the western part where streams are absent then l may range from 40 km to 100 km with t 2 000 m2 day sy 0 17 and α 1 the τ estimates range from 370 to 2 330 years one can expect that the baseflow from different gr projections may stabilize faster due to the lesser sensitivity to the local head disequilibria our transient numerical simulations for finding τ utilized the initial conditions of the aquifer in equilibrium with gr for years 2000 2009 then gr projections for the next decade 2010 2019 were taken from median wet and dry projections with spatially averaged gr values for this decade of 55 0 mm yr 109 3 mm yr and 62 7 mm yr respectively transient computations with identical gr were carried out with 10 year time steps after year 2020 until equilibrium was achieved i e stream baseflow stabilized and the value of τ was taken when the baseflow change reached 95 of the range between the two steady state baseflow rates time to equilibrium varied depending on the gr projection selected and was approximately 10 yrs 120 yrs and 40 yrs corresponding with the median wet and dry projections respectively similar time scales were found in the nsh when studying effects of trends in stream runoff using a lumped parameter model istanbulluoglu et al 2012 some currently occurring hydrological processes may be consequences of paleoclimatic changes over a century or more with the system adjusting to the new gr regime istanbulluoglu et al 2012 urbano et al 2004 york et al 2002 due to fundamental difficulties in finding paleoclimatic history e g bracht flyr et al 2013 voss and soliman 2014 all current models of large aquifers derive a historic condition based on either a steady state simulation or results of a spin up yet they all share this caveat which depends on the spatial scale of the system 5 conclusions the impacts of gcm projected gr changes on a system of thousands of endorheic groundwater fed lws of the nsh in the 21st century have been investigated feasibility of treatment of such systems using ihms seems limited in the foreseeable future instead of tracking the dynamics of hundreds and thousands of numerous lakes individually lws were inferred by comparing simulated elevations of the water table and land surface using terrain analysis and mapping approach with a gis this approach relies on gr which is evaluated externally without explicit consideration of the surface subsurface domain coupling the overall approach allows for traditional groundwater modeling methods to be applied the groundwater modeling begins with a simulation of the initial steady state conditions existing in equilibrium with modern gr estimated for the period from 2000 to 2009 over the 21st century three gcm based gr projections are selected from the multitude of existing models and greenhouse gas emission scenarios 16 cgms and three emissions scenario based on cumulative gr over the 90 year future period 2010 2099 minor future gr changes are predicted for the nsh region based on the median most likely gr projection however large uncertainty exists in these projections as indicated by the standard deviation of cumulative p et potential gr by the end of century this statistical measure standard deviation of the range of uncertainty determined the selected future wet and dry gr projections projected potential gr rates averaged over the future period are 74 7 mm yr 55 3 mm yr and 37 6 mm yr for the wet median and dry gr projections respectively these rates compare to a rate of 52 6 mm yr for the modern baseline gr rate as percent difference relative to the baseline these percentages are 42 5 and 29 respectively by year 2099 under the wet median and dry gr projections the number of lws is simulated to increase by 58 to 11 353 and by 18 to 8 666 and to decrease by 55 to 3 081 from a baseline of 7 331 respectively the median projection suggests that gr changes in the 21st century will increase lake areas from 719 km2 to 1 191 km2 and wetlands from 1 364 km2 to 1 613 km2 combined the overall lw areas increase from 2 083 km2 to 2 804 km2 under the median gr projection an increase by 35 by year 2099 under the wet and dry gr projections the combined areas of lws is simulated to increase by 201 to 6 261 km2 and to decrease by 71 598 km2 respectively implications from the results of this research are as follows lake and wetland areas and numbers may increase substantially for the median and wet gr projections by the end of the 21st century even one or two decades of drought or deluge may cause drastic changes in these surface water features these trends are subject to substantial uncertainty due to uncertainty in available hydroclimate projections yet a downward trends exists after 2020 for the dry gr projection that has the potential to drastically reduce stream baseflow and areal coverage of lws consequences of gr changes at large spatial scale may last significantly longer due to the long response time of the aquifer that lasts for decades to centuries therefore it is important to consider paleoclimatic history in analyses of future processes when possible emphasizing the role of initial conditions land topography and geomorphological processes in the nebraska sand hills interaction of eolian and fluvial processes in formation of the lake bathymetry result in different sensitivity of the total lw areas and numbers between gr increase and decrease and the applied methodology addresses the decadal dynamics and long term trends in the total lake and wetland area and numbers over the 21st century but offers no insight into such trends of annual or seasonal dynamics of the system some technical aspects of modeling can be improved re evaluation of dynamics of actual gr from potential gr and better estimating dynamics of et from expansion contraction of lws could be done internally by coupling the aquifer with vadose zone processes variably saturated flow and coupling subsurface with surface domain flows into one model these developments could slightly affect selection of the decadal stress periods of the groundwater model but with known trends in gr would not alter general conclusions about trends in lw dynamics or affect the fidelity of the model considering the level of uncertainty of climatic drivers and projections of the lake and wetland areas was obtained by matching simulated heads to the observed water levels with qualitative resemblance of lw distribution to the observed modern patterns landsat etc and a posteriori analysis of stream baseflow further model fidelity improvements can be achieved by revision of lw classification based on 3 m water table depth and including their spatial distribution from landsat or other remote sensing product as a target in the objective function during quantitative calibration to our knowledge this is first attempt to simulate a lake wetland aquifer system with such numbers of lws in arid conditions declaration of competing interest none acknowledgements funding was provided by the nsf igert program dge 0903469 the nebraska geological society s yatkola edwards scholarship fund and the american association of petroleum geologists we are grateful to the staff of waterloo hydrogeologic for technical support to personnel of the csd and calmit unl the nebraska department of natural resources and the usgs nebraska water science center for making available regional datasets we also acknowledge t franz and j szilagyi both unl for sharing data and their expertise appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 06 046 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
6287,damage caused by floods has generally increased in europe over the decades and reducing the flood risk has become a recognised priority throughout the continent floods constitute a burden on the economies of european countries and insurance companies not only because of the damage they cause but also because of the costs of structural flood defences and safety measures in general the main reasons for the increasing flood risk are presence of settlements and the growing value of assets in flood prone areas climate and environmental land use and land cover changes that always happen but have recently been accelerated because of radically different demographic conditions and technological progress statistical analyses by munich re s natcatservice database underline the high relevance of floods to european societies the reasons for the rising trends in flood losses are explained the flood risk is borne by various stakeholders individuals businesses governments and the insurance sector this paper discusses the taxonomy of floods presents flood statistics for europe and examines the reasons for flood risk increase it also examines flood risk reduction focusing on various aspects of flood insurance as a means of risk transfer keywords flood risk costs insurance flood loss disaster europe 1 introduction there have been many destructive inundations on the european continent in the last two decades such as the flash floods in northern italy france and switzerland in 2000 in the upper elbe and danube catchments in 2002 and 2013 along the lower danube in 2006 in the united kingdom in 2007 in poland in 2010 and in the adriatic region in 2014 there were also flash and urban floods in copenhagen in 2011 and in germany and france in 2016 see annual issues of munich re topics geo review of natural catastrophes munich re 2001 2018 it has not been possible to find ubiquitous flood hazard changes in observation records in europe but kundzewicz et al 2018b detected an increasing trend in the number of large floods even if the inter annual as well as the inter decadal variabilities are strong flooding causes destruction damage and suffering wiping out homes and businesses as well as injuring and killing people flood damage is not as devastating to european societies as to those in less developed parts of the world where whole countries are sometimes thrown back years in their development kron 2015 in several individual flood events material losses have exceeded us 10 billion while the number of fatalities in less developed countries is sometimes over a thousand since 1950 the number of flood fatalities in europe has usually not been as high a catastrophic storm surge hit the dutch belgian and english coasts in 1953 causing many fatalities netherlands 1835 united kingdom 307 belgium 25 there was also a killer flood in spain in 1962 which remains the only inland event in europe to produce more than 1000 fatalities however european states are subject to a significant financial burden not only from floods themselves but also from the costly flood risk reduction measures and structural flood defences in particular citizens demand action from their governments in order to protect themselves and their properties against the flood hazard flood risk reduction activities understood as dedicated efforts to reduce the flood hazard as well as exposure and vulnerability to floods can be distinguished at several levels they range from flood forecasting warning and technical control systems to the individual behaviour of a person or a company and provisions to make sure that a flood will not ruin affected entities while it is beyond doubt that loss of life must be prevented by all means the costs of prevention of monetary losses should not be out of proportion to the value of the items being protected clearly multi million expenditure from taxpayers money to protect a few houses located in a flood prone area is not justified in a longer time perspective reducing the global emission of greenhouse gases i e climate change mitigation is likely to limit the warming and accompanying effects such as an increase in intense precipitation that can generate flooding however the inert nature of the climate system means that efficient global climate policy undertaken now or soon can at best hope to curb the flood hazard at some remote time in the future but not in the near future in this paper we discuss the taxonomy of floods and we present flood statistics for europe we examine the reasons for the flood risk increase including land use and land cover change increase in the exposed values and loss potential change in public perception behavioural change as well as climate change and flood control measures we also discuss flood risk reduction with focus on insurance as a means of risk transfer 2 taxonomy of floods 2 1 coastal floods many winter storms extratropical storms in europe are dry that is they bring high wind velocities but associated precipitation totals are not large however sometimes they are wet or even very wet generating abundant and destructive precipitation gale force winter storms may be accompanied by storm surges that lead to coastal floods caused by the unusual elevation of the water level along a coast due to wind set up europe s coastline is very long exceeding 100 000 km and many highly populated and developed areas including large towns and industrial complexes are located in coastal zones european coasts and estuaries exposed to the threat of flooding are mainly those around the north and the baltic seas and to a lesser extent on the coasts of the atlantic ocean the mediterranean sea and the black sea the hazard of coastal flooding is due to a combination of storm surges tides and sea level rise presently exceeding the rate of 3 mm per year and has been increasing over decades kron 2013 catastrophic storm surges that occurred in the netherlands and the uk in 1953 as well as in the german bight and hamburg in 1962 with many fatalities and high economic losses triggered structural coastal protection measures in the countries affected the costs of defences along the coasts of the north sea have been very high but this is understandable given the loss potential of an extreme event with many millions of wealthy people living in low lying areas nevertheless the what if analysis carried out after winter storm anatol in december 1999 shows that if this storm had taken a path some 100 km further south it could have produced a water level along germany s frisian coast that substantially exceeded the dyke crests this would have inevitably resulted in a serious coastal flood disaster another winter storm xynthia in 2010 caused 65 fatalities resulting mostly from drowning in the storm surge that hit the département vendée on the french atlantic coast 2 2 convective storms and flash floods each summer europe features numerous severe convective storms scs these intense meteorological events colloquially called thunderstorms can produce such effects as gusty winds hail torrential rain and lightning sometimes they can even spawn tornadoes while the area hit by an scs is typically small compared to that affected by a spatially extensive winter storm it does not always have to be a local event a single atmospheric disturbance can generate severe thunderstorms squall line over hundreds of kilometers and torrential rains that can lead to flash flooding typically flash floods see gaume et al 2009 are caused by high intensity rainfall they can happen anywhere on steep and flat terrains so that no place is really safe during a torrential rain event the precipitation rate exceeds the infiltration and drainage capacities at the site where it occurs in flat areas water accumulates on the surface but inundation can locally assume considerable depths for instance in depressions which may not even be noticeable to the eye on sloped terrain the water gushes downwards attaining high velocities and destructive power flash floods are invariably surprising events as they cannot be forecast sufficiently early hence reacting and implementing protection measures are usually not an option flash floods can kill people 32 people died in september 2009 in western istanbul turkey when a flash flood surprised them in their homes and on a motorway kömüşcü and çelik 2013 there were 172 fatalities in a single flash flood after heavy rain in the russian federation near the black sea coast in 2012 in may and june of 2016 southern germany was hit by a sequence of flash floods that devastated several towns with total losses adding up to us 2 2 billion of which us 1 billion was insured many destructive flash e g urban floods have occurred over flat areas in europe e g copenhagen 2011 berlin 28 june and 11 july 2017 münster 2014 2 3 river and lake flooding river floods and overflowing of lakes are caused by long lasting often spatially extensive e g basin wide rainfall with a depth that largely exceeds the storage capacity of the ground or by extensive and fast snowmelt compare 2 4 the water is collected in the drainage basins forming flood waves in streams and rivers that propagate downstream in many areas of europe floods are born in the mountain and upland areas where precipitation is high and masses of water then flow down to lowland reaches the highest flood risk areas are those adjacent to the watercourses since the flooding starts from the river the sequence of areas being flooded is usually the same so that it is possible to derive a relationship between the flood intensity in terms of recurrence interval and inundated areas flood zones there can be deviations from this historical relationship due to different flood magnitudes near river confluences changes in the landscape due to previous flooding and human influence as well as flood protection measures in contrast to flash floods river floods last much longer days to weeks and typically rise more slowly so that flood forecasting and early warning can save lives and provide enough time to take adequate precautionary measures 2 4 cold season floods there is a range of conditions that can cause cold season flooding including coastal flooding induced by winter storms mentioned in 2 1 rapid snowmelt in the basin see 2 3 and ice jams much of europe is at risk of winter flooding a sudden thaw of abundant snow cover resulting from fast temperature rise can lead to snowmelt flooding midwinter or a spring thaw can produce large volumes of runoff in a relatively short period of time water cannot penetrate into the ground because it is frozen and hence impermeable then the water runs off the ground surface and flows into lakes streams and rivers possibly spilling over their banks cold season floods can be also caused by rain on ice frozen ground and rain on snow events that can trigger major floods in alpine catchments beniston and stoffel 2016 demonstrated an increase in the number of rain on snow events over decades related to climate change warming and change of winter precipitation phase long cold spells in winter can cause river surfaces to freeze leading to ice jams despite ubiquitous warming the lower odra river which forms part of the border between germany and poland typically freezes over almost every year the small bottom slope of the lower odra and the tidal backflow from the baltic sea enhance the formation of ice jams during cold weather conditions this jeopardises the dykes that along this part of the river are particularly vulnerable to breaches as reported by lindenschmidt et al 2019 ice jams on the lower odra may occur during freeze up and ice cover break up phases an ice jam occurs when a rise in the water level or a thaw breaks the ice into large chunks which become jammed at weirs and piers the slush ice frazil ice and ice blocks may accumulate to form ice jams leading to backwater effects and substantial water level rise and causing severe overbank flooding a sudden release of an ice jam can also produce flooding when the water is released it can flow downstream quickly causing a significant and fast rise in water levels a jökulhlaup is an icelandic term that has been adopted in glaciological terminology referring to any large and abrupt release of water from a subglacial or proglacial lake reservoir the term originates from a name of subglacial outburst floods from vatnajökull iceland triggered by geothermal heating and occasionally by a subglacial volcanic eruption jökulhlaups on the sparsely populated island of iceland typically only damage roads similar processes on a very large scale occurred during geological times in several places on earth e g the deglaciation of europe after the last ice age and formation of the english channel 3 flood statistics for europe munich re has been systematically collecting information on natural disasters including floods since 1974 the natcatservice ncs database run by the company covering losses caused by natural extreme events is among the world s largest and contains more than 40 000 entries the lead currency in the database is the us dollar us this means that all losses are converted from the local currency into us based on the exchange rate at the time the event occurs munich re has made a part of ncs content accessible to the public thus enabling them to conduct a variety of individual statistical analyses http natcatservice munichre com on the basis of relevant events i e those that are not marginal munich re 2018 we would obtain a rather skewed picture if we simply compared absolute amounts of flood damage over time adjustment for inflation is the minimum standardisation method that needs to be applied however it is also important to consider factors such as changes in exposed values susceptibility to water damage general wealth building cost index and others what is really needed is normalisation adjustment for inflation is performed using the consumer price index cpi of each country and taking into account fluctuations in exchange rates normalisation takes into account local changes in gdp measured in us for details of how this is done see eichner et al 2016 the most expensive flood disaster ever recorded in europe was the 2002 summer flood event which cost a total of us 21 5 billion nominal cost unadjusted for inflation and changes in exchange rate across europe there have also been five inland flood events in which at least 100 people were killed table 1 table 1 suggests that there are distinct differences in the relation of insured losses to overall losses depending on the type of event and region hit in some countries e g the uk switzerland in certain cases france private and state run insurance cover assumes a large proportion of the flood losses while in most western european countries the share is in the order of about 10 40 in eastern europe this share is low while the death toll is higher even with a relatively high number of flood events one must exercise due care and caution when dealing with them and certainly avoid fully automatic and purely mathematical analyses despite the fact that munich re gives quality control greater priority checks every single entry as thoroughly as possible and corrects entries whenever new information is available sometimes even years after an event numbers should not be crunched blindly by statistical methods but rather given due attention based on expert knowledge statistical analyses regarding floods require a large set of data but this set must also be consistent and its components clearly understood while losses due to natural hazards have increased dramatically all over the world hydrological disasters have in particular put heavy burdens on societies and insurance companies in central and western europe including costs for measures needed to protect societies against them there are inherent difficulties with the data on floods the first is that floods are as opposed to gale force winds earthquake or volcanic eruption events inherently a secondary type of natural event the primary cause is rainfall intense and or long lasting change of temperature causing snowmelt and ice jam wind causing coastal storm surge earthquake causing tsunami etc floods are not only dependent on the intensity or depth of rainfall but also for instance on the previous conditions in the catchment so called antecedent precipitation index or on flood control and prevention measures it can be difficult to differentiate between two consecutive flood events in the same region e g uk 2007 or central europe 2002 during the july 1997 odra flood there were three consecutive flood waves resulting from three separate but temporally adjacent heavy precipitation spells kundzewicz et al 1999 it can also be difficult to differentiate between the losses caused by the various phenomena wind hail flood water caused by an scs it is rare that loss quantities can be allocated to the various causes in composite events hence we are bound to live with the total losses that include everything after the storm hilal in central europe in 2008 it was not possible to disaggregate the total damage into parts caused by the flood the wind and the hail total losses amounted to us 1 7 billion in germany alone fig 1 presents a geographical overview of relevant hydrological loss events in europe in 1980 2018 the category of hydrological loss events only involves pure floods and mass movements no meteorological events are shown that include tropical cyclones extratropical storms convective storms and local storms some scs events cause flash floods but often the windstorm damage is comparably high or higher and very difficult to distinguish usually only a small proportion of flood losses are insured cf table 1 one of the reasons for this is that most of the damage affects public infrastructure facilities roads railway lines dykes river embankments bridges and other infrastructure installations such as the public water supply and sewage treatment plants that are sometimes located in flood plains e g in the 100 year flood range besides the market penetration of flood insurance for private homes is low the insurance industry has to settle claims and at least in theory eventually knows what amount had to be reimbursed after a given event even though accurate accounting is not possible in practice for a number of reasons estimating the overall loss can be based on relatively good proxy data such as extrapolating known numbers to market losses with the help of market shares and results from loss models for known portfolios uninsured losses such as damaged levees are extremely difficult to quantify often the replacement value of a structure does not match the value of the destroyed one especially when additional features are added in fact the real value of the old dyke is hard to estimate sometimes even political aspects play a role a region may have interest in inflating loss estimates in order to obtain more outside assistance or it may play the numbers down in order to mask its own deficiencies in the area of precautionary measures therefore overall flood loss estimates are in general subject to considerable uncertainty accurately segregating flood losses from a set of data is practically impossible specifying flood losses can only be done in europe for pure floods which are usually large basin wide events local and regional flash floods as a rule fall in the category severe convective storms when losses are caused by the abundance of water as well as the wind the example of germany where the number fig 2 and amount of losses fig 3 from floods and wet storms i e storms during which flooding occurred over the last four decades are displayed gives an idea of the uncertainty in the statistics of losses from hydrological disasters extratropical winter storms are not considered as flood losses that occur in this context are usually minor compared to wind damage the annual number of river floods shows no obvious changes in frequency and variability and that of flash floods only to a limited extent however the number of wet storms increases considerably graphs on the left combining the two flood categories river and pure flash flood to one graph upper right does not change anything either however the complete picture of flood related loss events in germany is only provided by the superposition of all three event types lower right this not only increases the average number of events per year from 2 3 for pure floods to 8 but we can also recognise a significant upward trend the actual annual flood losses are highly volatile fig 3 however the data do not suggest a clear upward trend signal apart from the fact that high losses seem to have become more frequent again this is different for the losses from wet storms given that it is reasonable to assume as a rule of thumb that about half of these losses are associated with the impact of flooding and the other half with other phenomena wind and hail a relevant row of bars in fig 3 represents another contribution to flood losses those from wet storms equalling per crude assumption reflecting unpublished loss attribution studies 50 of the annual loss amounts produced by wet storms 4 reasons for increasing losses 4 1 land use and land cover change the level of flood losses is a direct function of the number of people that live in exposed areas and of the material damage potential whilst the population pressure in less developed countries often leaves people with no other choice than to settle in flood prone areas the motivation in industrial countries is provided by other factors in principle flood plains and coastal plains are if one disregards the flood hazard well suited for development they are flat provide easy access to process and cooling water allow transport of raw materials and products by boat and are easy to develop with roads water and power networks and other critical lifeline infrastructure rivers are usually thought to be tamed by the construction of dykes and riparian residents and property owners feel safe especially if no major event happens in the first few years after they have settled the land in such a situation huge values are built up in the form of buildings equipment and stocks additionally many jobs and a considerable portion of gdp are dependent on the industries and businesses located on flood plains this creates a problem as soon as production or business is interrupted by inundation and people cannot work or even get to work towns and cities are interested in further development they have to make land available for housing commerce and industry many stakeholders are either not aware that there is a danger of flooding as they are not familiar with the region and naively assume that if land is released for development it will not be unsafe or they simply ignore the flood danger 4 2 increase in exposed values and their vulnerability never before have people had so many valuable but at the same time vulnerable possessions the wealth has accumulated and the damage potential has increased the rooms in the basement where people used to store coal and wood preserves and potatoes and all manner of junk have now made way for party rooms and playrooms with wall to wall carpeting upholstered furniture stereos and computers freezers and washing machines as well as central heating units in former times most belongings were barely susceptible to damage by water the ones that were could be carried to a safe place and even if they were destroyed the loss was relatively minor today s contents often suffer total damage if they come into contact with water especially electric and electronic machines appliances and other devices are highly vulnerable to humidity and the dirt and other pollution particles always contained in flood waters due to their weight or because they are fixed to the building some of these devices are difficult if not impossible to move to a higher level when a flood rises the greatest problem however is caused by presence of central heating equipment and oil storage tanks the rule of thumb being that original water caused damage can be doubled by escaping oil the situation is basically the same in commercial and industrial buildings too here electronic and electrical installations such as computer centres air conditioning control centres and elevator machinery are the typical and highly vulnerable contents of basements underground car parks for employees and customers are component parts of many new office buildings although cars are relatively easy to remove they still represent a very high loss potential which could be realised in the event of a flash flood when there is no possibility of early warning underground car parks may also constitute a deadly trap 4 3 risk perception many people still believe that flood events can be fully controlled by humans as long as appropriate technological precautions are taken flood control measures do make loss events less common a well maintained 100 year dyke is designed to withstand a 100 year flood hence it is typically sufficient by definition to protect against losses originating from a flood with a return period of less than or equal to 100 years the positive effect of this is that frequent losses and hardship can be prevented however this effect is often counterbalanced by the fact that the unjustified feeling of security it creates leads people to expose more and more objects of value to the risk of flood this sense of security is caused not only by the existence of dykes and embankments early warning systems and the availability of disaster relief organisations but also by the intentional or unintentional transmission of false information by local authorities or groups with a vested interest e g the tourist trade playing down the risk if a flood event occurs which existing safeguards cannot cope with an immense loss potential suddenly emerges flood control systems are designed to cope with a rare event with a given exceedance probability e g on average once in 100 years however it does not make sense either economically or aesthetically to protect everything to this level of safety the design of flood control measures should be geared to the population land use and the values to be protected agricultural areas should have a lower standard of protection than cities this leads from a hazard based to a risk based design process 4 4 behaviour of the people concerned people like living close to streams and rivers for aesthetic reasons pretty views the hazard associated with a river is initially accepted by many but soon forgotten if nothing i e no flood event happens only a dangerous event or a loss will wake people up due to a lack of adequate awareness in the face of a threatening flood owners are reluctant to move items of property even though they are movable and this often delays their evacuation examples are heavy washing machines and freezers filled with food the chances are high that eventually it will be too late to save them also reports suggest that some owners are reluctant to carry their belongings to a safe place because they expect that insurance or the government will pay for their losses thus enabling them to replace a used item with a new one 4 5 the effects of climate change it is indisputable that a warmer climate will lead to a higher water vapour capacity of and hence content in the atmosphere as indicated by the clausius clapeyron law the upshot will not only be larger amounts of precipitation generally but also extreme rain intensities in regional or local severe weather situations especially during the summer as observations in many places have confirmed over the past few years in no way should this be considered inconsistent with the general tendency towards drier summers in certain regions e g in southern europe instead it must be seen as an indication of greater variability in precipitation and hence more frequent extreme events at both upper and lower ends of the intensity distribution scale eea 2017 although less rain is projected in the summer it is likely to be more concentrated in time leading to more flash floods the fact that losses occur is attributable to these very extremes and not to a change in the mean values the costs that arise from flood events and in connection with them must therefore be expected to increase particularly over dense urban areas i e areas with high concentrations of values more intense convection may lead to severe local weather events that induce extreme precipitation intensities these often involve a high density of lightning strokes hailstorms and gale force gusts sometimes even tornadoes on account of the large proportion of impervious surfaces in urban areas the torrential rain runs straight into the drainage systems which are not designed to cope with such abundant water volumes with the result that underpasses cellars and sometimes even subway tunnels are flooded with water at the same time the trends observed in recent decades as well as the model calculations of future climate scenarios lead us to expect much milder and wetter winters in many regions this will have a substantial impact on the flood risk because winter precipitation will increasingly come in the form of rain rather than snow without the buffer that snow provides the liquid precipitation runs off directly into rivers and streams this effect is intensified by the fact that in winter when the level of evapotranspiration is low the soil is often almost completely saturated so that no water infiltrates furthermore for several decades many parts of europe have seen a distinct increase in westerly weather patterns during the winter these are very rainy low pressure systems that often trigger floods isolated extreme floods are nothing new a fact to which the numerous high water marks on historical buildings bear witness consequently even such exceptional floods as those that swamped central europe in the summer of 2002 or the devastating flash floods in germany in 2016 cannot be assumed to have been caused by global warming however one could evaluate change in frequency of large events like these climate conditions have already changed significantly and heavy precipitation events have become more intense in the northern part of europe and less intense in the southern part 1 https www eea europa eu data and maps indicators precipitation extremes in europe 3 assessment 1 they are likely to become more frequent in the future 2 https www eea europa eu data and maps indicators precipitation extremes in europe 3 assessment 2 this development is due at least in part to anthropogenic global warming and is likely to continue and even accelerate in the future as this tendency will be impossible to reverse for decades to come design assumptions must take into account that a river discharge that used to be exceeded on average once in 100 years in the reference interval is likely to occur more or in some parts of europe less frequently in the warmer future field et al 2012 however projections and scenarios of flood hazard and flood risk in the future are bound with considerable uncertainties this refers to climatic and non climatic factors relevant to flood risk the latter include demographic changes also immigration urbanisation economic progress and technological advances that may change the way we live over some areas there are considerable inter model differences in flood hazard projections kundzewicz et al 2017 some experts believe koutsoyiannis personal communication that there is no sound scientific basis in comparing model outputs to each other in science comparisons are made between models and reality the latter being unavailable for the future a real problem is the dramatic difference between all models and reality in particular with respect to intense precipitation causing floods see tsaknias et al 2016 anagnostopoulos et al 2010 kundzewicz and stakhiv 2010 and koutsoyiannis et al 2011 nevertheless the states the emergency services the population and the insurance industry must come to terms with the fact that more frequent and more catastrophic events are likely with generally greater losses hanson et al 2007 4 6 the effects of improved flood control and flood protection any catastrophic flood triggers discussions on how to prevent similar events in the future this may be at a local regional or national level depending on the size of event that occurred let us take an example from germany a prime example at local level are the protection works against storm surges in hamburg that were carried out after the catastrophe of 1962 kron and müller 2019 regionally saxony after 2002 and bavaria after 2005 set up long term agendas of flood related planning and precautionary measures after severe events in germany at national level the german flood protection legislation was revised as a consequence of the events in 2002 and the 2013 flood triggered a national plan for flood control and prevention initiatives thieken et al 2016 after the rhine floods in 1993 and 1995 the international commission for the protection of the rhine icpr adopted the action plan on floods 3 https www iksr org en international cooperation rhine 2020 action plan on floods 3 that aims at improving flood protection while extending and enhancing the flood plains objectives for the time horizon 2020 include the following damage risks are to be reduced by 25 compared to the reference year 1995 extreme flood stages downstream of the impounded sections are to be reduced by up to 70 cm the riparian population should be made aware of the flood risk via flood risk maps for 100 of flood hazard areas flood forecasting horizon should be significantly extended in 2010 the icpr made the third and so far last analysis of the implementation of the action plan it revealed that almost all measures planned until 2010 had been implemented effectively and successfully at the costs estimated icpr 2015 as a consequence of the eu flood directive the action plan has been complemented by the first flood risk management plan 2016 2021 for the rhine basin schulte wülwer leidig et al 2018 while it is very difficult to associate the effect of flood risk reduction efforts with the development of losses on a countrywide basis in a quantitative way it is certain that they do have an effect the problem is that the effects of climate change and land use changes that tend to aggravate losses and protection works that are aimed to reduce them cannot be separated from each other the fact that in some countries germany included losses do not clearly show an upward or downward trend is possibly attributable to this duality 5 reducing the flood risk 5 1 definition of risk the term risk is used in different ways in different situations where an object of value is at stake and where the outcome is uncertain for scientific discussions it should be defined in an unambiguous and consistent way see eu 2007 many authors e g plate and duckstein 1987 proposed to define risk as a product of probability of occurrence hazard of an event and its consequences often risk is assumed to be a function of hazard exposure and vulnerability hazard can be interpreted as the potential occurrence of a physical event that may cause adverse impacts in the ipcc context agard and schipper 2014 proposed to define exposure as the presence of people livelihoods species or ecosystems environmental functions services and resources infrastructure or economic social or cultural assets in places and settings that could be adversely affected while vulnerability as the propensity or predisposition to be adversely affected indeed vulnerability can encompass such concepts and elements as sensitivity or susceptibility to harm and lack of capacity to cope and adapt plate 2002 in the present paper risk is understood in a qualitative way as the product of a hazard and its consequences and is hence determined by three components kron 2005 1 the hazard h random variable i e the occurrence probability of a given natural event unit dimensionless 2 the exposed values or values at risk e deterministic function of discharge q i e the objects that are present at the location involved unit e g us 3 the vulnerability v deterministic function of q i e the lack of resistance to damaging destructive forces given as a percentage of the damage of e unit dimensionless in its simplest form the risk unit e g us is calculated by multiplying these three components 1 r h e v where there are neither people nor values that can be affected by a flood there is no risk clearly then h 0 but e 0 or v 0 hence r 0 vulnerability can refer to human health human vulnerability structural integrity physical vulnerability or personal wealth financial vulnerability insurance s contribution to risk control addresses the last of these factors for an insurance or a reinsurance company e is the portion of exposed values which is covered in the company s portfolio the price for insurance cover is called a premium it is typically incurred as an annual payment and covers the charge for the risk i e for potential costs in case an insured event occurs net premium plus administrative costs the annual net premium thus reflects the average annual loss of an object at risk tantamount to the annual risk r with flood discharges q being the hazard the risk is determined by 2 r q a c q f q d q where c q e v is the cost loss caused by a given discharge q and f q is the probability density function of the discharge the integration must be performed for the whole range above the flood value qa from which losses start to occur see kron 2005 5 2 observed flood losses in europe europe has given its economic situation much more capability to protect itself against natural disasters than for instance less developed countries overseas and emerging economies kron et al 2019 therefore economic flood losses in europe are high in absolute terms but rather low in relative terms percentage of gdp the number of flood related fatalities is lower than on most other continents it is possible to save lives e g by flood forecasting and warning see alfieri et al 2012 there is a paradox though the higher the level of technology the higher the risk may be this has to do with the fact that flood protection measures reduce the probability of certain areas being flooded by a 100 year flood but at the same time they create illusion of safety and encourage communities to overdevelop the protected flood prone areas the overall risk determined by the product of the reduced probability of flooding and the possibly mushrooming values exposed may therefore increase however it is not just the large flood events that generate losses in fact when added together the many small and medium sized local floods account for a considerable loss amount additionally the financial means that societies spend on flood control dykes reservoirs etc is a multiple of the costs they devote to protection against other natural hazards expected values of losses even though they may decrease must therefore be supplemented by the cost of the flood protection and mitigation measures in order to produce a fair picture via cost benefit ratio recent events have shown that we must reckon with more and more water related catastrophes in the future in contrast to coverage for windstorm losses flood insurance still has low average penetration in europe typical insured percentages in developed countries are from 10 to 40 with a few exceptions such as the united kingdom where the majority of homeowners policies include flood insurance both nominal original losses reported at the time when a loss event occurred and inflation adjusted losses are not sufficient to identify the underlying causes for the temporal development of losses considering changes in consumer price indices inflation does not account for the increase in the number of objects at risk and their value upgrades but only for their change in price assuming they remain physically unaltered however flood plains and other open space areas that were covered by water during extreme discharge situations decades ago without suffering any harm or at most only little may accommodate new urban developments commercial areas and industrial parks nowadays cf 4 2 normalisation makes it possible to compare figures of the past with those of today it attempts to apply past events to today s situation if both the former and the new conditions are known this method represents a very accurate basis for comparison however comparing two stages in history in adequate detail becomes more and more difficult the larger an area becomes and is impossible for a whole country therefore so called proxy data must be used to assess an area s development the most used proxy and actually the only one that is available globally is the gross domestic product gdp estimated for a region we have based our normalisation procedure on gdp at local level eichner et al 2016 so that we can consider regional differences in development e g coast vs inland areas fig 4 presents overall losses from relevant inland flood events in europe from 1980 to 2018 based on munich re natcatservice data the bars and the two curves shown in fig 4 refer to nominal loss values as well as inflation adjusted 2018 and normalised values respectively lugeri et al 2010 studied the spatial distribution of river flood risks for the european continent they reported a topography based flood hazard map of europe identifying flood risk for low lying areas adjacent to rivers and harnessing land use data and damage stage relationship for different land uses hazard classes were determined by the proximity to the river and the difference in elevation between the land and the closest river they drew the attention of policy makers to possible risk hotspots by considering annual average damage and highlighted regions where the threat to the economy from river floods is of major concern 5 3 normalisation of flood losses in central europe the central part of europe was chosen in this paper to demonstrate the effect of normalisation and at the same time to investigate the development of flood losses in this area the defined region comprises the northern half of france the three benelux countries germany switzerland liechtenstein and austria plus poland slovakia the czech republic and hungary fig 5 these countries form i a compact contiguous area in which even a single large flood event is normally included in its entirety and ii a group with a fairly comparable wealth and development status even if the new eu member states of poland czech republic slovakia and hungary have not reached the economic development status of old eu member states the upper danube and odra floods affect both old and new eu member states the overall flood losses in this group of countries are shown in fig 6 the inflation adjusted figures exhibit a slight upward trend of 55 million per year which is 2 8 of the average value 1999 million over the observation period after normalisation lower part of fig 6 the slope drops only slightly to 51 million per year but represents just 1 5 per year while a weak trend remains in the sequence suggesting that there is an increase in the losses even though protection measures established over the 39 year period are in effect the magnitude of this effect is not known but is certainly positive the remainder can be attributed to other causes which are not accounted for i e environmental changes including but not limited to climate change as mentioned in section 3 floods are inherently a secondary event rainfall is the primary phenomenon and therefore subject to influences that are not related to economic changes accounted for by normalisation environmental changes include loss of retention areas in catchments immediate drainage of urban areas river training to name just a few climate change is just one of these environmental changes changes in losses cannot on a large scale be attributed quantitatively to individual changes in environmental conditions the mathematical trend calculations have to be taken with caution as a statistical significance of the trends at the 5 confidence limit is not reached the lines are more for assisting the visual judgment of the sequence shown and clearly the trend direction can be altered if there is or is not another big loss event like 2002 within the next ten years in this respect logarithmic presentation fig 7 can help to reduce the volatility of the strong year to year variability of annual flood losses spanning several orders of magnitudes from a few million to several billion us or euros per year displaying the loss sequence in logarithmic values avoids the strong influence of the very large outlying bars in the extreme years 1997 with the odra flood and 2002 with the elbe danube flood in this presentation both trend values become even weaker at around 0 8 and 1 2 per year respectively and even negative for the normalised values annual flood losses in central europe have not increased significantly over the past four decades if economic development is computationally eliminated this conclusion describes the net effect resulting from the balance of loss driving factors such as environmental changes that always happen but have recently been accelerated because of the radically different demographic conditions and technological progress koutsoyiannis 2013 and mitigating factors such as flood prevention in this context it must not be forgotten that flood protection costs should therefore be included in the considerations to ensure a fair comparison on the other hand it is proven that in the long run they pay off many times over kron and müller 2019 5 4 the partnership for flood risk reduction risk and loss minimisation calls for an integrated course of action the flood risk must be carried on several shoulders the public authorities the people and enterprises affected and the financial sector in particular the insurance industry only when they all cooperate with each other in a finely tuned relationship in the spirit of a risk partnership is disaster risk reduction really effective in the european union eu there is a supra national flood risk reduction policy that member states shape accept and follow relevant activities are currently focused on the ongoing process of adopting the eu floods directive directive 2007 60 ec on the assessment and management of flood risks released in 2007 eu 2007 that followed major flood disasters in europe at the turn of the century it aims at building a common framework of policies and actions to reduce and manage the risks that floods pose to human health economic activity the environment and cultural heritage specifically the floods directive requires all eu member states to identify areas at risk from flooding to map the extent of actual floods as well as assets and humans at risk in these areas and to take adequate measures to reduce this flood risk the job of public authorities i e the state or the government is primarily to reduce the underlying risk to society as a whole they provide access to observation and early warning systems build dykes deploy flood retention areas determine the framework for the use of exposed areas by enacting statutory provisions and prepare emergency plans including programmes to alleviate recovery temporary housing financial assistance tax relief etc in some countries e g france spain flood insurance programmes are state run unlike in the case of earthquake and windstorm where homeowners themselves are responsible for ensuring their houses are properly protected the responsibility for protection from flooding is largely shifted to public authorities the most visible governmental action relates to structural flood control measures these are aimed at reducing the probability of inundation and thus lowering the hazard by keeping the water away from the people kundzewicz et al 2018a much can be done along rivers but it is very difficult to influence the hazard resulting from local torrential rain and flash flooding as cloudburst can occur anywhere in contrast to inundations of well defined riparian areas those immediately affected individuals companies communities have great potential for loss reduction the crucial point is whether they keep their risk awareness alive even those people who do not ignore the danger of flood from the very beginning often quickly forget about it especially if nothing happens for some time they rely on flood control systems and at the same time make their property more and more valuable by adding additional items that are often susceptible to water damage so that loss potential increases these people must be informed and educated to build in an appropriate manner control the exposure of their values and be ready to take action in an emergency this includes preparing for catastrophic losses by taking financial precautions e g buying insurance the main parameters of eq 1 that they are able to influence are values at risk and vulnerability both physical and financial weather events such as floods constitute a hazard only when humans encroach on unsafe e g flood prone areas hence preventive measures aim to reduce the consequences of flooding by reducing the exposure of people and property by way of prohibiting or discouraging development in areas at risk and thus keeping people away from destructive waters kundzewicz et al 2018a with spatial planning zoning and bans on development in unsafe areas it is possible to control new housing and infrastructure and to try to move the existing objects on flood plains out of harm s way a success story in this respect was reported in the netherlands within the room for the river programme that led to the relocation of farms from vulnerable areas e g rohde et al 2006 6 flood insurance aspects the true task of insurance companies is to compensate financial losses that would have a substantial impact on insured entities or even lead to their ruin they carry the financial risk from flood events that have such a low probability that they cannot be considered foreseeable insurance redistributes the burden borne by individuals across the entire community of insured entities which is ideally composed in such a way that they all have a chance of being affected even if the degrees of probability differ furthermore insurers perform educational outreach and public relations services e g by publishing brochures in which they draw attention to hazards and explain ways of dealing with them e g munich re 2008 2012 in the same way as private individuals insurance companies try to avoid volatility in their payments natural perils insurance is highly volatile large single losses from one flood event can be reduced by transferring part of the risk to the reinsurance sector in which companies often do business worldwide when catastrophic losses occur in one country they are therefore covered from all over the world thus relieving the local insurance market and possibly even preventing its collapse kron 2009 only a relatively small proportion of buildings are exposed to river floods the areas affected by river floods are always the same and inundations can occur at almost regular intervals people in these areas seek insurance while those who live some distance from a river are not interested in buying cover hence if an insurance company planned to sell individual policies as part of a voluntary insurance scheme the premiums would have to be high so that prospective policyholders would normally find them prohibitive this phenomenon is called adverse selection or anti selection it makes flood only insurance on a free market basis practically impossible adverse selection can be avoided or reduced if multi risk insurance packages are offered the portfolio is then composed of all kinds of clients those who live close to a river flood risk but also those in a geologically active region earthquake risk those on a mountain slope landslide and avalanche risk and so on nevertheless premiums for the various hazards should reflect the individual risk in mass business i e for private homes and small businesses and their contents the effort required to assess the exposure of a certain building must be seen in the context of the annual premium income for one such object which can be lower than many people think for instance in germany premium starts at an affordable level of roughly 50 equivalent to us 56 at the exchange rate of approximately 1 12 us status as at may 2019 in low risk areas since an individual assessment of the risk and calculation of an individual premium for these objects are impossible the premium must be fixed on the basis of a flat rate assumption for this zones with a similar flood earthquake landslide hazard must be identified and defined premiums within these zones are constant if insurance is not compulsory there are several possible reasons for changes in insurance coverage levels such as marketing campaigns and reactions to a flood event here some stakeholders may increase participation or may even drop their insurance assuming naively that such a large flood is unlikely to happen again in the foreseeable future this is consistent with a common and grossly incorrect understanding of a 100 year flood occurring every hundred years while in fact this is just an average return period also during economic recession stakeholders may drop their insurance policies cloudbursts and flash floods can occur anywhere the geographical spread of objects at risk and the community of insured entities are large i e the frequency of someone being hit by an extreme event is low as a consequence the premiums can be kept low too consumer demand for insurance protection can be developed on a broad front and adequate premiums can be calculated with a relatively high degree of reliability hence flood damage caused by flash floods is insurable without any problem because there is no reason to declare a place as being flash flood prone and charge elevated premiums in recent years national insurance associations and large companies in several countries have put a lot of effort into countrywide flood zoning systems and have continuously improved their flood models examples are the zürs zonierungssystem für überschwemmungsrisiko und einschätzung von umweltrisiken zoning system for inundation risk and assessment of environmental risks information system in germany as well as hora hochwasserrisikozonierung flood risk zoning in austria in most cases this was carried out in close cooperation with government agencies governments also see the advantage of having no go zones or high price zones set by the insurance industry to help them in their efforts to keep high risk areas free from residential and commercial development yet there is a problem of legacy many buildings already exist in flood prone areas including historic centres of european cities insurance terms and conditions if wisely applied can support the implementation of land use policies and building codes if risks are not insurable or only at great cost the development of hazard prone areas will be hindered and hence the speed of creating new risks will slow down in this respect legal provisions must address insurance issues before development and construction commence rather than treating insurance as a universal remedy for all kinds of mismanagement there is no reasonable insurance solution that can possibly make insurance companies pay for all the losses that may be incurred instead a certain amount of loss has to be borne by the insured entities before the insurance becomes effective i e deductibles must be introduced such a structure has advantages for both the insurer and the insured entities the insurer does not have to settle masses of small losses and saves on both loss compensation payments and administrative costs the client profits in the form of lower premiums in a similar way insurance contracts especially in industrial business often define a limit i e a maximum payout sum an important consequence of deductibles is the motivation of policyholders to do something in order to reduce their losses if people have to pay part of the loss themselves this should act as an incentive to be proactive and install precautionary measures or to rescue items in the event of a flood with proper preparedness and freedom from the responsibility to pay for small losses which may be very frequent the insurance company only has to cover a reduced risk so that the premium will be reduced too people whose exposure is so high that they cannot be granted insurance may only become eligible for cover by accepting a significant deductible however due to the low coverage some european countries such as poland have seen the government forced to act as the insurer of last resort paying for recovery after damage by natural disaster at the beginning of the july 1997 flood the then prime minister of poland cimoszewicz now member of european parliament issued a sober and objectively correct statement that those who had not been insured could not expect compensation for their losses and that there were no significant central budget reserves to be used to this effect however since the deluge turned out to be extraordinarily destructive this statement was found to be largely inadequate in the context of the grimness of the unfolding situation and raised widespread criticism as surmised by some international observers see kundzewicz et al 1999 the original statement by the pm and the non satisfactory performance of the authorities in flood management action may have contributed to the defeat of the ruling coalition in the parliamentary elections in the autumn of 1997 in contrast solidarity with flood victims can help politically positive public sentiment regarding flood management actions in germany during and after the 1962 1997 and 2002 floods likely helped respectively hamburg s then police senator later chancellor schmidt brandenburg s minister for the environment platzeck and chancellor schröder to achieve electoral and career success at european union eu level there is a european union solidarity fund eusf http ec europa eu regional policy en policy what glossary e eu solidarity fund established after the august 2002 floods that caused the highest ever material damage by a flood in europe the fund provides assistance to eu member states after major natural disasters expressing european solidarity to disaster stricken regions within the union it has already provided assistance to 24 eu member states that suffered from natural disasters including floods cf https ec europa eu regional policy sources thefunds doc interventions since 2002 pdf 7 final remarks high material losses generated by floods have occurred several times in europe in recent years development and settlement of areas near watercourses and the accumulation of sensitive values in those areas modifications of the landscape river training works urbanisation loss of natural flood plains deforestation changes in agricultural land use increase in soil compaction and impervious surfaces etc a lack of risk awareness partially on account of the excessive trust in flood control measures and influences of the changing climate are some of the causes that intensify the adverse consequences of floods but there are also widespread resistance efforts in flood prevention and flood control to counterbalance the growing burden the balance of these factors suggests that the relative economic consequences of floods in the countries in central europe are only increasing slightly reducing the flood risk must eventually be the objective transferring to insurance the residual risk which is not possible or is only very difficult to control by means of structural and non structural measures is highly recommended this is particularly true for the risk of flash floods in contrast to river flooding it is not known with sufficient lead time where and when a flash flood resulting from intense rainfall will hit practically every location is at risk at the same time the probability of being hit is so small that expensive structural flood protection measures are not reasonable compared to loss expectation this is the ideal situation for flood insurance insurance cover does not prevent the loss of one s home and personal belongings but it can save people from financial ruin risk reduction in the spirit of the eu floods directive means dedicated efforts to decrease each one of the three components of risk hazard exposure and vulnerability changes in hazard natural and anthropogenic can be difficult and costly to reduce however changes in exposure and vulnerability are predominantly human based as people either move into harm s way and increase the damage potential or see potential harm move closer to them as a result of technological measures the prophetic statement by gilbert white floods are acts of god but flood losses are largely acts of man white 1945 is of absolute validity at all times now more than ever declaration of competing interest none acknowledgments statistical data were taken from munich re natcatservice zwk wishes to acknowledge financial support from the project interpretation of change in flood related indices based on climate variability flovar funded by the national science centre of poland project number 2017 27 b st10 00924 wk and zwk wish to thank professor dr ing dr ing e h erich j plate to whom this paper is dedicated for his mentorship at the university of karlsruhe now karlsruhe institute of technology long term collaboration and friendship two reviewers of this paper professor demetris koutsoyiannis and an anonymous referee are cordially thanked for their thoughtful and constructive advice 
6287,damage caused by floods has generally increased in europe over the decades and reducing the flood risk has become a recognised priority throughout the continent floods constitute a burden on the economies of european countries and insurance companies not only because of the damage they cause but also because of the costs of structural flood defences and safety measures in general the main reasons for the increasing flood risk are presence of settlements and the growing value of assets in flood prone areas climate and environmental land use and land cover changes that always happen but have recently been accelerated because of radically different demographic conditions and technological progress statistical analyses by munich re s natcatservice database underline the high relevance of floods to european societies the reasons for the rising trends in flood losses are explained the flood risk is borne by various stakeholders individuals businesses governments and the insurance sector this paper discusses the taxonomy of floods presents flood statistics for europe and examines the reasons for flood risk increase it also examines flood risk reduction focusing on various aspects of flood insurance as a means of risk transfer keywords flood risk costs insurance flood loss disaster europe 1 introduction there have been many destructive inundations on the european continent in the last two decades such as the flash floods in northern italy france and switzerland in 2000 in the upper elbe and danube catchments in 2002 and 2013 along the lower danube in 2006 in the united kingdom in 2007 in poland in 2010 and in the adriatic region in 2014 there were also flash and urban floods in copenhagen in 2011 and in germany and france in 2016 see annual issues of munich re topics geo review of natural catastrophes munich re 2001 2018 it has not been possible to find ubiquitous flood hazard changes in observation records in europe but kundzewicz et al 2018b detected an increasing trend in the number of large floods even if the inter annual as well as the inter decadal variabilities are strong flooding causes destruction damage and suffering wiping out homes and businesses as well as injuring and killing people flood damage is not as devastating to european societies as to those in less developed parts of the world where whole countries are sometimes thrown back years in their development kron 2015 in several individual flood events material losses have exceeded us 10 billion while the number of fatalities in less developed countries is sometimes over a thousand since 1950 the number of flood fatalities in europe has usually not been as high a catastrophic storm surge hit the dutch belgian and english coasts in 1953 causing many fatalities netherlands 1835 united kingdom 307 belgium 25 there was also a killer flood in spain in 1962 which remains the only inland event in europe to produce more than 1000 fatalities however european states are subject to a significant financial burden not only from floods themselves but also from the costly flood risk reduction measures and structural flood defences in particular citizens demand action from their governments in order to protect themselves and their properties against the flood hazard flood risk reduction activities understood as dedicated efforts to reduce the flood hazard as well as exposure and vulnerability to floods can be distinguished at several levels they range from flood forecasting warning and technical control systems to the individual behaviour of a person or a company and provisions to make sure that a flood will not ruin affected entities while it is beyond doubt that loss of life must be prevented by all means the costs of prevention of monetary losses should not be out of proportion to the value of the items being protected clearly multi million expenditure from taxpayers money to protect a few houses located in a flood prone area is not justified in a longer time perspective reducing the global emission of greenhouse gases i e climate change mitigation is likely to limit the warming and accompanying effects such as an increase in intense precipitation that can generate flooding however the inert nature of the climate system means that efficient global climate policy undertaken now or soon can at best hope to curb the flood hazard at some remote time in the future but not in the near future in this paper we discuss the taxonomy of floods and we present flood statistics for europe we examine the reasons for the flood risk increase including land use and land cover change increase in the exposed values and loss potential change in public perception behavioural change as well as climate change and flood control measures we also discuss flood risk reduction with focus on insurance as a means of risk transfer 2 taxonomy of floods 2 1 coastal floods many winter storms extratropical storms in europe are dry that is they bring high wind velocities but associated precipitation totals are not large however sometimes they are wet or even very wet generating abundant and destructive precipitation gale force winter storms may be accompanied by storm surges that lead to coastal floods caused by the unusual elevation of the water level along a coast due to wind set up europe s coastline is very long exceeding 100 000 km and many highly populated and developed areas including large towns and industrial complexes are located in coastal zones european coasts and estuaries exposed to the threat of flooding are mainly those around the north and the baltic seas and to a lesser extent on the coasts of the atlantic ocean the mediterranean sea and the black sea the hazard of coastal flooding is due to a combination of storm surges tides and sea level rise presently exceeding the rate of 3 mm per year and has been increasing over decades kron 2013 catastrophic storm surges that occurred in the netherlands and the uk in 1953 as well as in the german bight and hamburg in 1962 with many fatalities and high economic losses triggered structural coastal protection measures in the countries affected the costs of defences along the coasts of the north sea have been very high but this is understandable given the loss potential of an extreme event with many millions of wealthy people living in low lying areas nevertheless the what if analysis carried out after winter storm anatol in december 1999 shows that if this storm had taken a path some 100 km further south it could have produced a water level along germany s frisian coast that substantially exceeded the dyke crests this would have inevitably resulted in a serious coastal flood disaster another winter storm xynthia in 2010 caused 65 fatalities resulting mostly from drowning in the storm surge that hit the département vendée on the french atlantic coast 2 2 convective storms and flash floods each summer europe features numerous severe convective storms scs these intense meteorological events colloquially called thunderstorms can produce such effects as gusty winds hail torrential rain and lightning sometimes they can even spawn tornadoes while the area hit by an scs is typically small compared to that affected by a spatially extensive winter storm it does not always have to be a local event a single atmospheric disturbance can generate severe thunderstorms squall line over hundreds of kilometers and torrential rains that can lead to flash flooding typically flash floods see gaume et al 2009 are caused by high intensity rainfall they can happen anywhere on steep and flat terrains so that no place is really safe during a torrential rain event the precipitation rate exceeds the infiltration and drainage capacities at the site where it occurs in flat areas water accumulates on the surface but inundation can locally assume considerable depths for instance in depressions which may not even be noticeable to the eye on sloped terrain the water gushes downwards attaining high velocities and destructive power flash floods are invariably surprising events as they cannot be forecast sufficiently early hence reacting and implementing protection measures are usually not an option flash floods can kill people 32 people died in september 2009 in western istanbul turkey when a flash flood surprised them in their homes and on a motorway kömüşcü and çelik 2013 there were 172 fatalities in a single flash flood after heavy rain in the russian federation near the black sea coast in 2012 in may and june of 2016 southern germany was hit by a sequence of flash floods that devastated several towns with total losses adding up to us 2 2 billion of which us 1 billion was insured many destructive flash e g urban floods have occurred over flat areas in europe e g copenhagen 2011 berlin 28 june and 11 july 2017 münster 2014 2 3 river and lake flooding river floods and overflowing of lakes are caused by long lasting often spatially extensive e g basin wide rainfall with a depth that largely exceeds the storage capacity of the ground or by extensive and fast snowmelt compare 2 4 the water is collected in the drainage basins forming flood waves in streams and rivers that propagate downstream in many areas of europe floods are born in the mountain and upland areas where precipitation is high and masses of water then flow down to lowland reaches the highest flood risk areas are those adjacent to the watercourses since the flooding starts from the river the sequence of areas being flooded is usually the same so that it is possible to derive a relationship between the flood intensity in terms of recurrence interval and inundated areas flood zones there can be deviations from this historical relationship due to different flood magnitudes near river confluences changes in the landscape due to previous flooding and human influence as well as flood protection measures in contrast to flash floods river floods last much longer days to weeks and typically rise more slowly so that flood forecasting and early warning can save lives and provide enough time to take adequate precautionary measures 2 4 cold season floods there is a range of conditions that can cause cold season flooding including coastal flooding induced by winter storms mentioned in 2 1 rapid snowmelt in the basin see 2 3 and ice jams much of europe is at risk of winter flooding a sudden thaw of abundant snow cover resulting from fast temperature rise can lead to snowmelt flooding midwinter or a spring thaw can produce large volumes of runoff in a relatively short period of time water cannot penetrate into the ground because it is frozen and hence impermeable then the water runs off the ground surface and flows into lakes streams and rivers possibly spilling over their banks cold season floods can be also caused by rain on ice frozen ground and rain on snow events that can trigger major floods in alpine catchments beniston and stoffel 2016 demonstrated an increase in the number of rain on snow events over decades related to climate change warming and change of winter precipitation phase long cold spells in winter can cause river surfaces to freeze leading to ice jams despite ubiquitous warming the lower odra river which forms part of the border between germany and poland typically freezes over almost every year the small bottom slope of the lower odra and the tidal backflow from the baltic sea enhance the formation of ice jams during cold weather conditions this jeopardises the dykes that along this part of the river are particularly vulnerable to breaches as reported by lindenschmidt et al 2019 ice jams on the lower odra may occur during freeze up and ice cover break up phases an ice jam occurs when a rise in the water level or a thaw breaks the ice into large chunks which become jammed at weirs and piers the slush ice frazil ice and ice blocks may accumulate to form ice jams leading to backwater effects and substantial water level rise and causing severe overbank flooding a sudden release of an ice jam can also produce flooding when the water is released it can flow downstream quickly causing a significant and fast rise in water levels a jökulhlaup is an icelandic term that has been adopted in glaciological terminology referring to any large and abrupt release of water from a subglacial or proglacial lake reservoir the term originates from a name of subglacial outburst floods from vatnajökull iceland triggered by geothermal heating and occasionally by a subglacial volcanic eruption jökulhlaups on the sparsely populated island of iceland typically only damage roads similar processes on a very large scale occurred during geological times in several places on earth e g the deglaciation of europe after the last ice age and formation of the english channel 3 flood statistics for europe munich re has been systematically collecting information on natural disasters including floods since 1974 the natcatservice ncs database run by the company covering losses caused by natural extreme events is among the world s largest and contains more than 40 000 entries the lead currency in the database is the us dollar us this means that all losses are converted from the local currency into us based on the exchange rate at the time the event occurs munich re has made a part of ncs content accessible to the public thus enabling them to conduct a variety of individual statistical analyses http natcatservice munichre com on the basis of relevant events i e those that are not marginal munich re 2018 we would obtain a rather skewed picture if we simply compared absolute amounts of flood damage over time adjustment for inflation is the minimum standardisation method that needs to be applied however it is also important to consider factors such as changes in exposed values susceptibility to water damage general wealth building cost index and others what is really needed is normalisation adjustment for inflation is performed using the consumer price index cpi of each country and taking into account fluctuations in exchange rates normalisation takes into account local changes in gdp measured in us for details of how this is done see eichner et al 2016 the most expensive flood disaster ever recorded in europe was the 2002 summer flood event which cost a total of us 21 5 billion nominal cost unadjusted for inflation and changes in exchange rate across europe there have also been five inland flood events in which at least 100 people were killed table 1 table 1 suggests that there are distinct differences in the relation of insured losses to overall losses depending on the type of event and region hit in some countries e g the uk switzerland in certain cases france private and state run insurance cover assumes a large proportion of the flood losses while in most western european countries the share is in the order of about 10 40 in eastern europe this share is low while the death toll is higher even with a relatively high number of flood events one must exercise due care and caution when dealing with them and certainly avoid fully automatic and purely mathematical analyses despite the fact that munich re gives quality control greater priority checks every single entry as thoroughly as possible and corrects entries whenever new information is available sometimes even years after an event numbers should not be crunched blindly by statistical methods but rather given due attention based on expert knowledge statistical analyses regarding floods require a large set of data but this set must also be consistent and its components clearly understood while losses due to natural hazards have increased dramatically all over the world hydrological disasters have in particular put heavy burdens on societies and insurance companies in central and western europe including costs for measures needed to protect societies against them there are inherent difficulties with the data on floods the first is that floods are as opposed to gale force winds earthquake or volcanic eruption events inherently a secondary type of natural event the primary cause is rainfall intense and or long lasting change of temperature causing snowmelt and ice jam wind causing coastal storm surge earthquake causing tsunami etc floods are not only dependent on the intensity or depth of rainfall but also for instance on the previous conditions in the catchment so called antecedent precipitation index or on flood control and prevention measures it can be difficult to differentiate between two consecutive flood events in the same region e g uk 2007 or central europe 2002 during the july 1997 odra flood there were three consecutive flood waves resulting from three separate but temporally adjacent heavy precipitation spells kundzewicz et al 1999 it can also be difficult to differentiate between the losses caused by the various phenomena wind hail flood water caused by an scs it is rare that loss quantities can be allocated to the various causes in composite events hence we are bound to live with the total losses that include everything after the storm hilal in central europe in 2008 it was not possible to disaggregate the total damage into parts caused by the flood the wind and the hail total losses amounted to us 1 7 billion in germany alone fig 1 presents a geographical overview of relevant hydrological loss events in europe in 1980 2018 the category of hydrological loss events only involves pure floods and mass movements no meteorological events are shown that include tropical cyclones extratropical storms convective storms and local storms some scs events cause flash floods but often the windstorm damage is comparably high or higher and very difficult to distinguish usually only a small proportion of flood losses are insured cf table 1 one of the reasons for this is that most of the damage affects public infrastructure facilities roads railway lines dykes river embankments bridges and other infrastructure installations such as the public water supply and sewage treatment plants that are sometimes located in flood plains e g in the 100 year flood range besides the market penetration of flood insurance for private homes is low the insurance industry has to settle claims and at least in theory eventually knows what amount had to be reimbursed after a given event even though accurate accounting is not possible in practice for a number of reasons estimating the overall loss can be based on relatively good proxy data such as extrapolating known numbers to market losses with the help of market shares and results from loss models for known portfolios uninsured losses such as damaged levees are extremely difficult to quantify often the replacement value of a structure does not match the value of the destroyed one especially when additional features are added in fact the real value of the old dyke is hard to estimate sometimes even political aspects play a role a region may have interest in inflating loss estimates in order to obtain more outside assistance or it may play the numbers down in order to mask its own deficiencies in the area of precautionary measures therefore overall flood loss estimates are in general subject to considerable uncertainty accurately segregating flood losses from a set of data is practically impossible specifying flood losses can only be done in europe for pure floods which are usually large basin wide events local and regional flash floods as a rule fall in the category severe convective storms when losses are caused by the abundance of water as well as the wind the example of germany where the number fig 2 and amount of losses fig 3 from floods and wet storms i e storms during which flooding occurred over the last four decades are displayed gives an idea of the uncertainty in the statistics of losses from hydrological disasters extratropical winter storms are not considered as flood losses that occur in this context are usually minor compared to wind damage the annual number of river floods shows no obvious changes in frequency and variability and that of flash floods only to a limited extent however the number of wet storms increases considerably graphs on the left combining the two flood categories river and pure flash flood to one graph upper right does not change anything either however the complete picture of flood related loss events in germany is only provided by the superposition of all three event types lower right this not only increases the average number of events per year from 2 3 for pure floods to 8 but we can also recognise a significant upward trend the actual annual flood losses are highly volatile fig 3 however the data do not suggest a clear upward trend signal apart from the fact that high losses seem to have become more frequent again this is different for the losses from wet storms given that it is reasonable to assume as a rule of thumb that about half of these losses are associated with the impact of flooding and the other half with other phenomena wind and hail a relevant row of bars in fig 3 represents another contribution to flood losses those from wet storms equalling per crude assumption reflecting unpublished loss attribution studies 50 of the annual loss amounts produced by wet storms 4 reasons for increasing losses 4 1 land use and land cover change the level of flood losses is a direct function of the number of people that live in exposed areas and of the material damage potential whilst the population pressure in less developed countries often leaves people with no other choice than to settle in flood prone areas the motivation in industrial countries is provided by other factors in principle flood plains and coastal plains are if one disregards the flood hazard well suited for development they are flat provide easy access to process and cooling water allow transport of raw materials and products by boat and are easy to develop with roads water and power networks and other critical lifeline infrastructure rivers are usually thought to be tamed by the construction of dykes and riparian residents and property owners feel safe especially if no major event happens in the first few years after they have settled the land in such a situation huge values are built up in the form of buildings equipment and stocks additionally many jobs and a considerable portion of gdp are dependent on the industries and businesses located on flood plains this creates a problem as soon as production or business is interrupted by inundation and people cannot work or even get to work towns and cities are interested in further development they have to make land available for housing commerce and industry many stakeholders are either not aware that there is a danger of flooding as they are not familiar with the region and naively assume that if land is released for development it will not be unsafe or they simply ignore the flood danger 4 2 increase in exposed values and their vulnerability never before have people had so many valuable but at the same time vulnerable possessions the wealth has accumulated and the damage potential has increased the rooms in the basement where people used to store coal and wood preserves and potatoes and all manner of junk have now made way for party rooms and playrooms with wall to wall carpeting upholstered furniture stereos and computers freezers and washing machines as well as central heating units in former times most belongings were barely susceptible to damage by water the ones that were could be carried to a safe place and even if they were destroyed the loss was relatively minor today s contents often suffer total damage if they come into contact with water especially electric and electronic machines appliances and other devices are highly vulnerable to humidity and the dirt and other pollution particles always contained in flood waters due to their weight or because they are fixed to the building some of these devices are difficult if not impossible to move to a higher level when a flood rises the greatest problem however is caused by presence of central heating equipment and oil storage tanks the rule of thumb being that original water caused damage can be doubled by escaping oil the situation is basically the same in commercial and industrial buildings too here electronic and electrical installations such as computer centres air conditioning control centres and elevator machinery are the typical and highly vulnerable contents of basements underground car parks for employees and customers are component parts of many new office buildings although cars are relatively easy to remove they still represent a very high loss potential which could be realised in the event of a flash flood when there is no possibility of early warning underground car parks may also constitute a deadly trap 4 3 risk perception many people still believe that flood events can be fully controlled by humans as long as appropriate technological precautions are taken flood control measures do make loss events less common a well maintained 100 year dyke is designed to withstand a 100 year flood hence it is typically sufficient by definition to protect against losses originating from a flood with a return period of less than or equal to 100 years the positive effect of this is that frequent losses and hardship can be prevented however this effect is often counterbalanced by the fact that the unjustified feeling of security it creates leads people to expose more and more objects of value to the risk of flood this sense of security is caused not only by the existence of dykes and embankments early warning systems and the availability of disaster relief organisations but also by the intentional or unintentional transmission of false information by local authorities or groups with a vested interest e g the tourist trade playing down the risk if a flood event occurs which existing safeguards cannot cope with an immense loss potential suddenly emerges flood control systems are designed to cope with a rare event with a given exceedance probability e g on average once in 100 years however it does not make sense either economically or aesthetically to protect everything to this level of safety the design of flood control measures should be geared to the population land use and the values to be protected agricultural areas should have a lower standard of protection than cities this leads from a hazard based to a risk based design process 4 4 behaviour of the people concerned people like living close to streams and rivers for aesthetic reasons pretty views the hazard associated with a river is initially accepted by many but soon forgotten if nothing i e no flood event happens only a dangerous event or a loss will wake people up due to a lack of adequate awareness in the face of a threatening flood owners are reluctant to move items of property even though they are movable and this often delays their evacuation examples are heavy washing machines and freezers filled with food the chances are high that eventually it will be too late to save them also reports suggest that some owners are reluctant to carry their belongings to a safe place because they expect that insurance or the government will pay for their losses thus enabling them to replace a used item with a new one 4 5 the effects of climate change it is indisputable that a warmer climate will lead to a higher water vapour capacity of and hence content in the atmosphere as indicated by the clausius clapeyron law the upshot will not only be larger amounts of precipitation generally but also extreme rain intensities in regional or local severe weather situations especially during the summer as observations in many places have confirmed over the past few years in no way should this be considered inconsistent with the general tendency towards drier summers in certain regions e g in southern europe instead it must be seen as an indication of greater variability in precipitation and hence more frequent extreme events at both upper and lower ends of the intensity distribution scale eea 2017 although less rain is projected in the summer it is likely to be more concentrated in time leading to more flash floods the fact that losses occur is attributable to these very extremes and not to a change in the mean values the costs that arise from flood events and in connection with them must therefore be expected to increase particularly over dense urban areas i e areas with high concentrations of values more intense convection may lead to severe local weather events that induce extreme precipitation intensities these often involve a high density of lightning strokes hailstorms and gale force gusts sometimes even tornadoes on account of the large proportion of impervious surfaces in urban areas the torrential rain runs straight into the drainage systems which are not designed to cope with such abundant water volumes with the result that underpasses cellars and sometimes even subway tunnels are flooded with water at the same time the trends observed in recent decades as well as the model calculations of future climate scenarios lead us to expect much milder and wetter winters in many regions this will have a substantial impact on the flood risk because winter precipitation will increasingly come in the form of rain rather than snow without the buffer that snow provides the liquid precipitation runs off directly into rivers and streams this effect is intensified by the fact that in winter when the level of evapotranspiration is low the soil is often almost completely saturated so that no water infiltrates furthermore for several decades many parts of europe have seen a distinct increase in westerly weather patterns during the winter these are very rainy low pressure systems that often trigger floods isolated extreme floods are nothing new a fact to which the numerous high water marks on historical buildings bear witness consequently even such exceptional floods as those that swamped central europe in the summer of 2002 or the devastating flash floods in germany in 2016 cannot be assumed to have been caused by global warming however one could evaluate change in frequency of large events like these climate conditions have already changed significantly and heavy precipitation events have become more intense in the northern part of europe and less intense in the southern part 1 https www eea europa eu data and maps indicators precipitation extremes in europe 3 assessment 1 they are likely to become more frequent in the future 2 https www eea europa eu data and maps indicators precipitation extremes in europe 3 assessment 2 this development is due at least in part to anthropogenic global warming and is likely to continue and even accelerate in the future as this tendency will be impossible to reverse for decades to come design assumptions must take into account that a river discharge that used to be exceeded on average once in 100 years in the reference interval is likely to occur more or in some parts of europe less frequently in the warmer future field et al 2012 however projections and scenarios of flood hazard and flood risk in the future are bound with considerable uncertainties this refers to climatic and non climatic factors relevant to flood risk the latter include demographic changes also immigration urbanisation economic progress and technological advances that may change the way we live over some areas there are considerable inter model differences in flood hazard projections kundzewicz et al 2017 some experts believe koutsoyiannis personal communication that there is no sound scientific basis in comparing model outputs to each other in science comparisons are made between models and reality the latter being unavailable for the future a real problem is the dramatic difference between all models and reality in particular with respect to intense precipitation causing floods see tsaknias et al 2016 anagnostopoulos et al 2010 kundzewicz and stakhiv 2010 and koutsoyiannis et al 2011 nevertheless the states the emergency services the population and the insurance industry must come to terms with the fact that more frequent and more catastrophic events are likely with generally greater losses hanson et al 2007 4 6 the effects of improved flood control and flood protection any catastrophic flood triggers discussions on how to prevent similar events in the future this may be at a local regional or national level depending on the size of event that occurred let us take an example from germany a prime example at local level are the protection works against storm surges in hamburg that were carried out after the catastrophe of 1962 kron and müller 2019 regionally saxony after 2002 and bavaria after 2005 set up long term agendas of flood related planning and precautionary measures after severe events in germany at national level the german flood protection legislation was revised as a consequence of the events in 2002 and the 2013 flood triggered a national plan for flood control and prevention initiatives thieken et al 2016 after the rhine floods in 1993 and 1995 the international commission for the protection of the rhine icpr adopted the action plan on floods 3 https www iksr org en international cooperation rhine 2020 action plan on floods 3 that aims at improving flood protection while extending and enhancing the flood plains objectives for the time horizon 2020 include the following damage risks are to be reduced by 25 compared to the reference year 1995 extreme flood stages downstream of the impounded sections are to be reduced by up to 70 cm the riparian population should be made aware of the flood risk via flood risk maps for 100 of flood hazard areas flood forecasting horizon should be significantly extended in 2010 the icpr made the third and so far last analysis of the implementation of the action plan it revealed that almost all measures planned until 2010 had been implemented effectively and successfully at the costs estimated icpr 2015 as a consequence of the eu flood directive the action plan has been complemented by the first flood risk management plan 2016 2021 for the rhine basin schulte wülwer leidig et al 2018 while it is very difficult to associate the effect of flood risk reduction efforts with the development of losses on a countrywide basis in a quantitative way it is certain that they do have an effect the problem is that the effects of climate change and land use changes that tend to aggravate losses and protection works that are aimed to reduce them cannot be separated from each other the fact that in some countries germany included losses do not clearly show an upward or downward trend is possibly attributable to this duality 5 reducing the flood risk 5 1 definition of risk the term risk is used in different ways in different situations where an object of value is at stake and where the outcome is uncertain for scientific discussions it should be defined in an unambiguous and consistent way see eu 2007 many authors e g plate and duckstein 1987 proposed to define risk as a product of probability of occurrence hazard of an event and its consequences often risk is assumed to be a function of hazard exposure and vulnerability hazard can be interpreted as the potential occurrence of a physical event that may cause adverse impacts in the ipcc context agard and schipper 2014 proposed to define exposure as the presence of people livelihoods species or ecosystems environmental functions services and resources infrastructure or economic social or cultural assets in places and settings that could be adversely affected while vulnerability as the propensity or predisposition to be adversely affected indeed vulnerability can encompass such concepts and elements as sensitivity or susceptibility to harm and lack of capacity to cope and adapt plate 2002 in the present paper risk is understood in a qualitative way as the product of a hazard and its consequences and is hence determined by three components kron 2005 1 the hazard h random variable i e the occurrence probability of a given natural event unit dimensionless 2 the exposed values or values at risk e deterministic function of discharge q i e the objects that are present at the location involved unit e g us 3 the vulnerability v deterministic function of q i e the lack of resistance to damaging destructive forces given as a percentage of the damage of e unit dimensionless in its simplest form the risk unit e g us is calculated by multiplying these three components 1 r h e v where there are neither people nor values that can be affected by a flood there is no risk clearly then h 0 but e 0 or v 0 hence r 0 vulnerability can refer to human health human vulnerability structural integrity physical vulnerability or personal wealth financial vulnerability insurance s contribution to risk control addresses the last of these factors for an insurance or a reinsurance company e is the portion of exposed values which is covered in the company s portfolio the price for insurance cover is called a premium it is typically incurred as an annual payment and covers the charge for the risk i e for potential costs in case an insured event occurs net premium plus administrative costs the annual net premium thus reflects the average annual loss of an object at risk tantamount to the annual risk r with flood discharges q being the hazard the risk is determined by 2 r q a c q f q d q where c q e v is the cost loss caused by a given discharge q and f q is the probability density function of the discharge the integration must be performed for the whole range above the flood value qa from which losses start to occur see kron 2005 5 2 observed flood losses in europe europe has given its economic situation much more capability to protect itself against natural disasters than for instance less developed countries overseas and emerging economies kron et al 2019 therefore economic flood losses in europe are high in absolute terms but rather low in relative terms percentage of gdp the number of flood related fatalities is lower than on most other continents it is possible to save lives e g by flood forecasting and warning see alfieri et al 2012 there is a paradox though the higher the level of technology the higher the risk may be this has to do with the fact that flood protection measures reduce the probability of certain areas being flooded by a 100 year flood but at the same time they create illusion of safety and encourage communities to overdevelop the protected flood prone areas the overall risk determined by the product of the reduced probability of flooding and the possibly mushrooming values exposed may therefore increase however it is not just the large flood events that generate losses in fact when added together the many small and medium sized local floods account for a considerable loss amount additionally the financial means that societies spend on flood control dykes reservoirs etc is a multiple of the costs they devote to protection against other natural hazards expected values of losses even though they may decrease must therefore be supplemented by the cost of the flood protection and mitigation measures in order to produce a fair picture via cost benefit ratio recent events have shown that we must reckon with more and more water related catastrophes in the future in contrast to coverage for windstorm losses flood insurance still has low average penetration in europe typical insured percentages in developed countries are from 10 to 40 with a few exceptions such as the united kingdom where the majority of homeowners policies include flood insurance both nominal original losses reported at the time when a loss event occurred and inflation adjusted losses are not sufficient to identify the underlying causes for the temporal development of losses considering changes in consumer price indices inflation does not account for the increase in the number of objects at risk and their value upgrades but only for their change in price assuming they remain physically unaltered however flood plains and other open space areas that were covered by water during extreme discharge situations decades ago without suffering any harm or at most only little may accommodate new urban developments commercial areas and industrial parks nowadays cf 4 2 normalisation makes it possible to compare figures of the past with those of today it attempts to apply past events to today s situation if both the former and the new conditions are known this method represents a very accurate basis for comparison however comparing two stages in history in adequate detail becomes more and more difficult the larger an area becomes and is impossible for a whole country therefore so called proxy data must be used to assess an area s development the most used proxy and actually the only one that is available globally is the gross domestic product gdp estimated for a region we have based our normalisation procedure on gdp at local level eichner et al 2016 so that we can consider regional differences in development e g coast vs inland areas fig 4 presents overall losses from relevant inland flood events in europe from 1980 to 2018 based on munich re natcatservice data the bars and the two curves shown in fig 4 refer to nominal loss values as well as inflation adjusted 2018 and normalised values respectively lugeri et al 2010 studied the spatial distribution of river flood risks for the european continent they reported a topography based flood hazard map of europe identifying flood risk for low lying areas adjacent to rivers and harnessing land use data and damage stage relationship for different land uses hazard classes were determined by the proximity to the river and the difference in elevation between the land and the closest river they drew the attention of policy makers to possible risk hotspots by considering annual average damage and highlighted regions where the threat to the economy from river floods is of major concern 5 3 normalisation of flood losses in central europe the central part of europe was chosen in this paper to demonstrate the effect of normalisation and at the same time to investigate the development of flood losses in this area the defined region comprises the northern half of france the three benelux countries germany switzerland liechtenstein and austria plus poland slovakia the czech republic and hungary fig 5 these countries form i a compact contiguous area in which even a single large flood event is normally included in its entirety and ii a group with a fairly comparable wealth and development status even if the new eu member states of poland czech republic slovakia and hungary have not reached the economic development status of old eu member states the upper danube and odra floods affect both old and new eu member states the overall flood losses in this group of countries are shown in fig 6 the inflation adjusted figures exhibit a slight upward trend of 55 million per year which is 2 8 of the average value 1999 million over the observation period after normalisation lower part of fig 6 the slope drops only slightly to 51 million per year but represents just 1 5 per year while a weak trend remains in the sequence suggesting that there is an increase in the losses even though protection measures established over the 39 year period are in effect the magnitude of this effect is not known but is certainly positive the remainder can be attributed to other causes which are not accounted for i e environmental changes including but not limited to climate change as mentioned in section 3 floods are inherently a secondary event rainfall is the primary phenomenon and therefore subject to influences that are not related to economic changes accounted for by normalisation environmental changes include loss of retention areas in catchments immediate drainage of urban areas river training to name just a few climate change is just one of these environmental changes changes in losses cannot on a large scale be attributed quantitatively to individual changes in environmental conditions the mathematical trend calculations have to be taken with caution as a statistical significance of the trends at the 5 confidence limit is not reached the lines are more for assisting the visual judgment of the sequence shown and clearly the trend direction can be altered if there is or is not another big loss event like 2002 within the next ten years in this respect logarithmic presentation fig 7 can help to reduce the volatility of the strong year to year variability of annual flood losses spanning several orders of magnitudes from a few million to several billion us or euros per year displaying the loss sequence in logarithmic values avoids the strong influence of the very large outlying bars in the extreme years 1997 with the odra flood and 2002 with the elbe danube flood in this presentation both trend values become even weaker at around 0 8 and 1 2 per year respectively and even negative for the normalised values annual flood losses in central europe have not increased significantly over the past four decades if economic development is computationally eliminated this conclusion describes the net effect resulting from the balance of loss driving factors such as environmental changes that always happen but have recently been accelerated because of the radically different demographic conditions and technological progress koutsoyiannis 2013 and mitigating factors such as flood prevention in this context it must not be forgotten that flood protection costs should therefore be included in the considerations to ensure a fair comparison on the other hand it is proven that in the long run they pay off many times over kron and müller 2019 5 4 the partnership for flood risk reduction risk and loss minimisation calls for an integrated course of action the flood risk must be carried on several shoulders the public authorities the people and enterprises affected and the financial sector in particular the insurance industry only when they all cooperate with each other in a finely tuned relationship in the spirit of a risk partnership is disaster risk reduction really effective in the european union eu there is a supra national flood risk reduction policy that member states shape accept and follow relevant activities are currently focused on the ongoing process of adopting the eu floods directive directive 2007 60 ec on the assessment and management of flood risks released in 2007 eu 2007 that followed major flood disasters in europe at the turn of the century it aims at building a common framework of policies and actions to reduce and manage the risks that floods pose to human health economic activity the environment and cultural heritage specifically the floods directive requires all eu member states to identify areas at risk from flooding to map the extent of actual floods as well as assets and humans at risk in these areas and to take adequate measures to reduce this flood risk the job of public authorities i e the state or the government is primarily to reduce the underlying risk to society as a whole they provide access to observation and early warning systems build dykes deploy flood retention areas determine the framework for the use of exposed areas by enacting statutory provisions and prepare emergency plans including programmes to alleviate recovery temporary housing financial assistance tax relief etc in some countries e g france spain flood insurance programmes are state run unlike in the case of earthquake and windstorm where homeowners themselves are responsible for ensuring their houses are properly protected the responsibility for protection from flooding is largely shifted to public authorities the most visible governmental action relates to structural flood control measures these are aimed at reducing the probability of inundation and thus lowering the hazard by keeping the water away from the people kundzewicz et al 2018a much can be done along rivers but it is very difficult to influence the hazard resulting from local torrential rain and flash flooding as cloudburst can occur anywhere in contrast to inundations of well defined riparian areas those immediately affected individuals companies communities have great potential for loss reduction the crucial point is whether they keep their risk awareness alive even those people who do not ignore the danger of flood from the very beginning often quickly forget about it especially if nothing happens for some time they rely on flood control systems and at the same time make their property more and more valuable by adding additional items that are often susceptible to water damage so that loss potential increases these people must be informed and educated to build in an appropriate manner control the exposure of their values and be ready to take action in an emergency this includes preparing for catastrophic losses by taking financial precautions e g buying insurance the main parameters of eq 1 that they are able to influence are values at risk and vulnerability both physical and financial weather events such as floods constitute a hazard only when humans encroach on unsafe e g flood prone areas hence preventive measures aim to reduce the consequences of flooding by reducing the exposure of people and property by way of prohibiting or discouraging development in areas at risk and thus keeping people away from destructive waters kundzewicz et al 2018a with spatial planning zoning and bans on development in unsafe areas it is possible to control new housing and infrastructure and to try to move the existing objects on flood plains out of harm s way a success story in this respect was reported in the netherlands within the room for the river programme that led to the relocation of farms from vulnerable areas e g rohde et al 2006 6 flood insurance aspects the true task of insurance companies is to compensate financial losses that would have a substantial impact on insured entities or even lead to their ruin they carry the financial risk from flood events that have such a low probability that they cannot be considered foreseeable insurance redistributes the burden borne by individuals across the entire community of insured entities which is ideally composed in such a way that they all have a chance of being affected even if the degrees of probability differ furthermore insurers perform educational outreach and public relations services e g by publishing brochures in which they draw attention to hazards and explain ways of dealing with them e g munich re 2008 2012 in the same way as private individuals insurance companies try to avoid volatility in their payments natural perils insurance is highly volatile large single losses from one flood event can be reduced by transferring part of the risk to the reinsurance sector in which companies often do business worldwide when catastrophic losses occur in one country they are therefore covered from all over the world thus relieving the local insurance market and possibly even preventing its collapse kron 2009 only a relatively small proportion of buildings are exposed to river floods the areas affected by river floods are always the same and inundations can occur at almost regular intervals people in these areas seek insurance while those who live some distance from a river are not interested in buying cover hence if an insurance company planned to sell individual policies as part of a voluntary insurance scheme the premiums would have to be high so that prospective policyholders would normally find them prohibitive this phenomenon is called adverse selection or anti selection it makes flood only insurance on a free market basis practically impossible adverse selection can be avoided or reduced if multi risk insurance packages are offered the portfolio is then composed of all kinds of clients those who live close to a river flood risk but also those in a geologically active region earthquake risk those on a mountain slope landslide and avalanche risk and so on nevertheless premiums for the various hazards should reflect the individual risk in mass business i e for private homes and small businesses and their contents the effort required to assess the exposure of a certain building must be seen in the context of the annual premium income for one such object which can be lower than many people think for instance in germany premium starts at an affordable level of roughly 50 equivalent to us 56 at the exchange rate of approximately 1 12 us status as at may 2019 in low risk areas since an individual assessment of the risk and calculation of an individual premium for these objects are impossible the premium must be fixed on the basis of a flat rate assumption for this zones with a similar flood earthquake landslide hazard must be identified and defined premiums within these zones are constant if insurance is not compulsory there are several possible reasons for changes in insurance coverage levels such as marketing campaigns and reactions to a flood event here some stakeholders may increase participation or may even drop their insurance assuming naively that such a large flood is unlikely to happen again in the foreseeable future this is consistent with a common and grossly incorrect understanding of a 100 year flood occurring every hundred years while in fact this is just an average return period also during economic recession stakeholders may drop their insurance policies cloudbursts and flash floods can occur anywhere the geographical spread of objects at risk and the community of insured entities are large i e the frequency of someone being hit by an extreme event is low as a consequence the premiums can be kept low too consumer demand for insurance protection can be developed on a broad front and adequate premiums can be calculated with a relatively high degree of reliability hence flood damage caused by flash floods is insurable without any problem because there is no reason to declare a place as being flash flood prone and charge elevated premiums in recent years national insurance associations and large companies in several countries have put a lot of effort into countrywide flood zoning systems and have continuously improved their flood models examples are the zürs zonierungssystem für überschwemmungsrisiko und einschätzung von umweltrisiken zoning system for inundation risk and assessment of environmental risks information system in germany as well as hora hochwasserrisikozonierung flood risk zoning in austria in most cases this was carried out in close cooperation with government agencies governments also see the advantage of having no go zones or high price zones set by the insurance industry to help them in their efforts to keep high risk areas free from residential and commercial development yet there is a problem of legacy many buildings already exist in flood prone areas including historic centres of european cities insurance terms and conditions if wisely applied can support the implementation of land use policies and building codes if risks are not insurable or only at great cost the development of hazard prone areas will be hindered and hence the speed of creating new risks will slow down in this respect legal provisions must address insurance issues before development and construction commence rather than treating insurance as a universal remedy for all kinds of mismanagement there is no reasonable insurance solution that can possibly make insurance companies pay for all the losses that may be incurred instead a certain amount of loss has to be borne by the insured entities before the insurance becomes effective i e deductibles must be introduced such a structure has advantages for both the insurer and the insured entities the insurer does not have to settle masses of small losses and saves on both loss compensation payments and administrative costs the client profits in the form of lower premiums in a similar way insurance contracts especially in industrial business often define a limit i e a maximum payout sum an important consequence of deductibles is the motivation of policyholders to do something in order to reduce their losses if people have to pay part of the loss themselves this should act as an incentive to be proactive and install precautionary measures or to rescue items in the event of a flood with proper preparedness and freedom from the responsibility to pay for small losses which may be very frequent the insurance company only has to cover a reduced risk so that the premium will be reduced too people whose exposure is so high that they cannot be granted insurance may only become eligible for cover by accepting a significant deductible however due to the low coverage some european countries such as poland have seen the government forced to act as the insurer of last resort paying for recovery after damage by natural disaster at the beginning of the july 1997 flood the then prime minister of poland cimoszewicz now member of european parliament issued a sober and objectively correct statement that those who had not been insured could not expect compensation for their losses and that there were no significant central budget reserves to be used to this effect however since the deluge turned out to be extraordinarily destructive this statement was found to be largely inadequate in the context of the grimness of the unfolding situation and raised widespread criticism as surmised by some international observers see kundzewicz et al 1999 the original statement by the pm and the non satisfactory performance of the authorities in flood management action may have contributed to the defeat of the ruling coalition in the parliamentary elections in the autumn of 1997 in contrast solidarity with flood victims can help politically positive public sentiment regarding flood management actions in germany during and after the 1962 1997 and 2002 floods likely helped respectively hamburg s then police senator later chancellor schmidt brandenburg s minister for the environment platzeck and chancellor schröder to achieve electoral and career success at european union eu level there is a european union solidarity fund eusf http ec europa eu regional policy en policy what glossary e eu solidarity fund established after the august 2002 floods that caused the highest ever material damage by a flood in europe the fund provides assistance to eu member states after major natural disasters expressing european solidarity to disaster stricken regions within the union it has already provided assistance to 24 eu member states that suffered from natural disasters including floods cf https ec europa eu regional policy sources thefunds doc interventions since 2002 pdf 7 final remarks high material losses generated by floods have occurred several times in europe in recent years development and settlement of areas near watercourses and the accumulation of sensitive values in those areas modifications of the landscape river training works urbanisation loss of natural flood plains deforestation changes in agricultural land use increase in soil compaction and impervious surfaces etc a lack of risk awareness partially on account of the excessive trust in flood control measures and influences of the changing climate are some of the causes that intensify the adverse consequences of floods but there are also widespread resistance efforts in flood prevention and flood control to counterbalance the growing burden the balance of these factors suggests that the relative economic consequences of floods in the countries in central europe are only increasing slightly reducing the flood risk must eventually be the objective transferring to insurance the residual risk which is not possible or is only very difficult to control by means of structural and non structural measures is highly recommended this is particularly true for the risk of flash floods in contrast to river flooding it is not known with sufficient lead time where and when a flash flood resulting from intense rainfall will hit practically every location is at risk at the same time the probability of being hit is so small that expensive structural flood protection measures are not reasonable compared to loss expectation this is the ideal situation for flood insurance insurance cover does not prevent the loss of one s home and personal belongings but it can save people from financial ruin risk reduction in the spirit of the eu floods directive means dedicated efforts to decrease each one of the three components of risk hazard exposure and vulnerability changes in hazard natural and anthropogenic can be difficult and costly to reduce however changes in exposure and vulnerability are predominantly human based as people either move into harm s way and increase the damage potential or see potential harm move closer to them as a result of technological measures the prophetic statement by gilbert white floods are acts of god but flood losses are largely acts of man white 1945 is of absolute validity at all times now more than ever declaration of competing interest none acknowledgments statistical data were taken from munich re natcatservice zwk wishes to acknowledge financial support from the project interpretation of change in flood related indices based on climate variability flovar funded by the national science centre of poland project number 2017 27 b st10 00924 wk and zwk wish to thank professor dr ing dr ing e h erich j plate to whom this paper is dedicated for his mentorship at the university of karlsruhe now karlsruhe institute of technology long term collaboration and friendship two reviewers of this paper professor demetris koutsoyiannis and an anonymous referee are cordially thanked for their thoughtful and constructive advice 
6288,the knowledge of the rainfall drop size distribution dsd at the land surface is essential for understanding precipitation mechanisms affecting soil erosion processes rainfall erosivity is defined as the potential of rain to cause erosion and it can be evaluated by rainfall kinetic power which is determined by dsd and raindrop terminal velocity this paper firstly deals with the raindrop terminal velocity estimate then the most widely used dsd are reviewed highlighting the difference between the raindrop size distribution per unit volume of air and that per unit area and time the reliability of the available kinetic power rainfall intensity relationships and their application in several part of the world is discussed highlighting that the use of rainfall intensity is not sufficient to determine the rainfall kinetic power everywhere finally the influence of seasonality on both raindrop size distribution and rainfall energy characteristics is investigated using dsd measurements carried out by an optical disdrometer placed at palermo experimental area keywords rainfall erosivity rainfall kinetic power rainfall intensity raindrop size distribution seasonality 1 introduction the acceleration of soil erosion process through anthropogenic perturbation has severe impacts on soil and environmental quality soil erosion reduces the long term soil productivity and it exposes subsoil which has often poor qualities for crop establishment and growth water soil erosion is a process of detachment and transport of soil particles due to rainfall and runoff and it is one the main causes of landform modelling on earth s surface at the plot and field scale when interrill erosion occurs runoff is a factor affecting the transportability of soil material detached by raindrop impact while runoff is able to cause both detachment and transport when rill erosion takes place rainfall erosivity is the main property affecting erosion processes involving both detachment of soil particles and the subsequent transport of the detached particles away from the site of detachment ferro et al 1991 mannaerts and gabriels 2000 salles and poesen 2000 according to wischmeier and smith 1978 rainfall erosivity is commonly expressed as the product of two factors the rainfall energy e and the maximum continuous 30 min intensity i30 during the individual storm nearing et al 2017 wischmeier 1959 analyzing approximately 8000 plot years of rainfall runoff and soil loss data confirmed the suitability of ei30 index for fallow and continuous row crop plots the suitability was also positively tested at different temporal scales seasonal and yearly variation yin et al 2017 the quantitative expression of energy per unit of rainfall developed by wischmeier and smith 1978 was based on the work of laws and parsons 1943 and gunn and kinzer 1949 the rainfall erosivity factor r proposed in the universal soil loss equation usle and in its revised rusle version is recognized as one of the best indicators for modelling the erosive potential of a rainstorm renard et al 1991 regional erosivity maps are used to identify areas with high potential rainfall erosivity thus with high risk of severe soil erosion ferro et al 1991 aronica and ferro 1997 ferro et al 1999 bonilla and vidal 2011 klik and konecny 2013 panagos et al 2015a b 2016a at the plot and field scales soil loss by water erosion can be related to rainfall by different precipitation properties i e rainfall intensity i mm h rainfall momentum m n m2 and rainfall kinetic energy per unit time and area named kinetic power pn j m2 h van dijk et al 2002 carollo and ferro 2015 carollo et al 2016 2017 2018 the rainfall momentum m represents the pressure or force per unit area provoking the mechanical stress and thus might be expected to be related to the breakdown of soil aggregates rose 1960 in other words when the raindrop reaches soil exerts a knock on the ground area due to the charge of momentum amount this phenomenon provokes the split of the bonds between soil particles making them available to be transport by overland flow carollo et al 2018 during years several researchers rose 1960 hudson 1971 abd elbasit et al 2010 sanchez moreno et al 2012 lim et al 2015 have investigated to understand if the raindrop impact is more dependent on m than pn or if these two variables are correlated with each other rose 1960 stated that rainfall momentum is a slightly better predictor for soil detachment than kinetic power for natural rainfall hudson 1971 demonstrated that momentum and kinetic power present a very similar relationship with rainfall intensity and the only question is which variable can be more easily and accurately measured van dijk et al 2002 other studies brodie and rosewell 2007 abd elbasit et al 2010 sanchez moreno et al 2012 using rainfall simulators showed that both kinetic energy and rainfall momentum can be used to predict soil splash in particular brodie and rosewell 2007 carrying out an analysis on rainfall intensity erosive indices relationships found that the rainfall momentum and pn could be interchangeable with each other for the estimation of the rainfall erosivity in soil erosion processes abd elbasit et al 2010 sanchez moreno et al 2012 using measurements of rainfall drop size distribution dsd and raindrop terminal velocity carried out by a laser optical disdrometer parsivel lim et al 2015 stated that the rainfall momentum is the best predictor of rainfall erosivity in korea carollo et al 2018 using measurements of dsds carried out by an optical disdrometer installed in two different mediterranean sites palermo italy el telauret spain suggested that m and pn referred to the unit volume of rainfall are related each other and thus both indices can be used to express the erosive power of the precipitation the rainfall kinetic power results from the kinetic power of single raindrops that constitute precipitation and varies with the size velocity and impact frequency of the raindrops impacting an area in unit time the rainfall kinetic power is determined by relating drop size distribution dsd to raindrop terminal velocity and rainfall intensity assouline 2009 the force applied to soil by raindrop impact is related to drop size in fact a very small raindrop striking the soil at a low impact velocity exerts very low force on the soil and causes very little erosion regardless of rainfall intensity toy et al 2002 the measured rainfall dsd at the ground level is the result of a series of physical phenomena i e raindrop collision coalescence break up that influence raindrop formation and its evolution during its falling process assouline and mualem 1989 depending on the climate specific location season the dominant microphysical rainfall processes may be different and then lead to different dsds hachani et al 2017 the rain formation is typically classified microphysically how warm or cold process warm rain formation involves the growth of droplets via collision and different models list et al 1987 hu and srivastava 1995 showed that collision coalescence and breakup processes result in an equilibrium shape to dsd regardless of overall raindrop concentration munchak et al 2012 cold rain formation occurs with the melting frozen hydrometeors such as snow or hail these frozen particles are larger than the raindrops out of which warm rain forms and melt into correspondingly larger rain drops falling larger raindrops breakup reducing their size although this process depends on the depth of the above freezing layer and the initial dsd munchak et al 2012 highlighted also that besides formation and internal processes external processes such as evaporation and size sorting can also influence the dsd evaporation preferentially acts on small drops thereby increasing the median volume diameter of the distribution i e the diameter that divides the distribution in two parts of equal volume in addiction the influence of size sorting by wind shear and turbulence on the dsd depends on the particular situation and may act to increase or decrease the median volume drop size munchak et al 2012 wiesner 1895 was a pioneer in measuring the dsd of natural rainfall arriving at the earth s surface using a piece of absorbent paper dusted with a water soluble dye van dijk et al 2002 however during the years several methods stain method flour pellet method oil immersion method photographic method raindrop spectrograph acoustic disdrometer optical disdrometer micro rain radar have been developed for measuring the drop size distribution of the precipitation at the ground and for assessing and monitoring the rainfall erosivity wiesner 1895 bentley 1904 lenard 1904 mache 1904 defant 1905 fuchs and petrjanoff 1937 laws 1941 laws and parsons 1943 gucker 1949 bowen and davidson 1951 blanchard 1952 courshee and byass 1953 mason and ramanadham 1953 mikirov 1957 hudson 1963 joss and waldvogel 1967 mutchler 1967 hall 1970 mutchler and hansen 1970 nawaby 1970 mutchler 1971 mutchler and larson 1971 kohl 1974 carter et al 1974 asseline and valentine 1978 donnadieu 1980 quinn 1981 roels 1981 lowe 1982 mccooll 1982 eigel and moore 1983 hauser et al 1984 illingworth and stevens 1987 navas et al 1990 kincaid et al 1996 tokay and short 1996 grossklaus et al 1998 campos 1999 salles and poesen 1999 löffler mang and joss 2000 nešpor et al 2000 nystuen 2001 reddy and kozu 2003 herngren 2005 lanzinger et al 2006 arnaez et al 2007 egodawatta 2007 schönhuber et al 2007 salvador et al 2009 pérez latorre et al 2010 asante 2011 maahn and kollias 2012 parsakhoo et al 2012 friedrich et al 2013 carollo and ferro 2015 kathiravelu et al 2016 carollo et al 2016 during the years several analytical forms of the raindrop size parameterization have been proposed as lognormal ln feingold and levin 1986 weibull w weibull 1951 exponential mp marshall and palmer 1948 and gamma u ulbrich 1983 distribution however at the best of our knowledge for meteorological and soil erosion purposes the most widely applied analytical form is the gamma distribution khrgian et al 1952 levin 1961 sulakvelidze 1969 sulakvelidze and dadali 1971 uijlenhoet and stricker 1999 carollo et al 2016 the knowledge of the dsd allows i to understand how the rainfall is made up ii to calculate the rainfall kinetic power pn and so iii to characterize energetically the precipitation which plays a fundamental role in soil erosion assouline and mualem 1989 the storm kinetic energy is the total value of the kinetic energy possessed by all raindrops reaching a unit area of the earth s surface during the rainfall event devices that can measure storm kinetic energy are not as commonly used as devices that measure rainfall intensity this circumstance favored the use of empirical relationships between rainfall intensity and rainfall kinetic power to estimate the storm kinetic energies needed in the prediction of soil loss by empirical soil erosion models to date many researchers proposed empirical relationships having different mathematical forms polynomial exponential logarithmical and power type for estimating kinetic power by rainfall intensity table 1 van dijk et al 2002 critically reviewed published studies of rainfall intensity kinetic power relationships evaluating their applicability for predictive purposes since the rainfall kinetic power can be calculated by dsd measurements combined with measured fall velocity or empirical laws linking terminal fall velocity and drop diameter salles et al 2002 this paper firstly deals with the raindrop terminal velocity estimate then the most widely used dsd models are reviewed highlighting also the difference between the raindrop size distribution per unit volume of air and the dsd per unit area and time a review of the reliability of the most commonly used relationships to estimate rainfall kinetic power in several part of the world is also presented and discussed finally the influence of seasonality on dsd and energy characteristics of the precipitation is also investigated using raindrops size distributions measured by an optical disdrometer at palermo experimental site 2 raindrop terminal velocity the raindrop impact velocity depends on both the falling height of each raindrop h and its diameter d davies 1942 spilhaus 1948 gunn and kinzer 1949 best 1950 kessler and wilks 1968 lui and orville 1969 sekhon and srivastava 1971 atlas and ulbrich 1977 uplinger 1981 leone and pica 1993 grosh 1996 ferro 2001 for a drop having a diameter equal to d the falling raindrop velocity grows with h until a threshold value equal to 20 m after which becomes constant and it is named rainfall terminal velocity v d expressed in m s 1 in particular the terminal velocity is reached when there is an equilibrium between the gravitational force and the raindrop aerodynamic resistance actually few terminal velocity measurements of natural raindrops are available in literature beard 1976 jayawardena and rezaur 2000 and the relationships between terminal velocity raindrop diameter were based on measurements of simulated single raindrops that fall in stagnant air lenard 1904 schmidt 1909 laws 1941 gunn and kinzer 1949 blanchard 1950 beard 1976 epema and riezebos 1983 jayawardena and rezaur 2000 theoretically the terminal velocity of a rigid and not deformed sphere having a given diameter and density that falls freely through a medium i e air having an own density can be calculated setting the gravitational force contribution equal to aerodynamic drag one that the particle meets during its motion of fall best 1950 park et al 1983 a large amount of work has been done in wind tunnels for determining the drag coefficient for rigid and not deformed spheres however considering natural rainfall a water droplet falling freely through the air is deformed by the aerodynamic forces and has been observed to vibrate strongly and spin thus producing definite departures from spherical symmetry for large drops vibrations and deformations frequently break up the droplet if it falls sufficiently far this turbulence together with the inevitable deformations of a not rigid sphere make the applicability of the theoretical calculation of the terminal velocity with measured values of the drag coefficient rather questionable gunn and kinzer 1949 ferro 2001 at date the relationship between drop size and terminal velocity is usually described by an exponential or power equation moreover all these relationships are calibrated by laboratory data collected for free falling water drops in still air using data from gunn and kinzer 1949 atlas and ulbrich 1977 proposed the following power law equation to estimate raindrop terminal velocity 1 v d a 0 d a 1 in which d is expressed in cm and v in m s 1 and a0 a1 are coefficients equal to 17 67 and 0 67 respectively atlas and ulbrich 1977 also demonstrated that a power law provides a close fit to the data of gunn and kinzer 1949 in the range 0 05 d 0 50 cm the power law proposed by sekhon and srivastava 1971 substituting in eq 1 the values of a0 14 2 and a1 0 5 allows to have a reasonable accuracy of terminal velocity estimate for drops in the range 0 1 0 3 cm but overestimates the fall velocity of drop 0 3 cm and smaller than 0 1 cm van boxel 1998 the power law relations proposed by several authors spilhaus 1948 kessler and wilks 1968 lui and orville 1969 sekhon and srivastava 1971 atlas and ulbrich 1977 grosh 1996 are widely applied even if according to van boxel 1998 can be mostly inaccurate for describing the variation of the terminal velocity with drop diameter best 1950 used measurements of terminal velocities carried out by davies 1942 and proposed the following exponential relationship 2 v d 9 43 1 exp d 17 7 1 147 where v d is the terminal velocity in m s 1 d is the drop diameter expressed in cm eq 2 adequately represents davies s experimental data for diameters ranging from 0 069 to 0 595 cm best 1950 other researchers uplinger 1981 leone and pica 1993 proposed the following exponential relationship relating terminal velocity raindrop diameter 3 v d α d exp β d in which the constants α and β are equal to 48 74 and 1 95 uplinger 1981 leone and pica 1993 using raindrop terminal velocity measurements carried out in laboratory by beard 1976 laws 1941 and gunn and kinzer 1949 estimated α 49 and β 2 taking into account that for d 0 10 cm the drop has a slow terminal velocity and small mass producing a negligible kinetic power and for d 0 55 cm the raindrops are unstable and tend to break up before reaching their terminal velocity pruppacher and pitter 1971 leone and pica 1993 considered only raindrops having a diameter varying in the range 0 10 0 55 cm cerro et al 1998 used eq 3 with the coefficients proposed by uplinger 1981 to terminal velocity estimates in spain and found that the measured values of raindrop terminal velocity were lower than those predicted by uplinger s eq 3 ferro 2001 suggested that for a raindrop having diameter d and falling with a velocity v d from a fall height h through a medium with density ρ and viscosity νc and having a velocity va this phenomenon can be expressed by the following functional relationship 4 f v d d h g ν c ρ v a 0 where f is the functional symbol and g is the acceleration due to gravity since eq 4 describes a physical phenomenon that is independent on the choice of the measurement units of the considered variables the π theorem of dimensional analysis can be applied for expressing eq 4 by four dimensionless groups π1 π2 π3 π4 5 φ π 1 π 2 π 3 π 4 0 where φ is a functional symbol choosing as independent variables from dimensional point of view g d and ρ the π1 grouping assumes the following form 6 π 1 g α d β ρ ε v d in which α β and ε are constant parameters substituting in the eq 6 the measurement units of g d and ρ the following relationship is obtained 7 0 m α s 2 α m β k g ε s 2 β m 4 ε m s 1 the α β and ε values can be determined solving the following system of equations 8a α β 4 ε 1 0 8b 2 α 2 ε 1 0 8c ε 0 the solution of the system α 1 2 β 1 2 ε 0 allows to deduce the following dimensionless group 9 π 1 v d g d using a similar procedure the other dimensionless groups are deduced 10 π 2 h d 11 π 3 ν c g 1 2 d 3 2 12 π 4 v a g 1 2 d 1 2 considering the eqs 11 and 12 the following relationship can be deduced 13 π 4 π 3 v a g 1 2 d 1 2 g 1 2 d 3 2 ν c v a d ν c in which νc is the cinematic viscosity finally the eq 5 can be expressed as follows 14 v d g d f h d v a d ν c in stagnant air condition va 0 eq 14 can be expressed using the π1 and π2 groups fig 1 shows the experimental rainfall velocity and raindrop measurements carried out by laws 1941 in the plot h d v d g d this figure highlights a clear similitude relation for all measurements carried out using h 8 m however despite the condition of the perfect similitude between the relationships π1 f π2 for h 8 m this relationship cannot be applied for the estimation of the natural raindrop terminal velocity for which h must be almost equal to 20 m ferro 2001 considering that the terminal velocity increases for d 0 56 cm until which becomes constant used the measurement carried out by many researchers laws 1941 gunn and kinzer 1949 blanchard 1950 beard 1976 epema and riezebos 1983 jayawardena and rezaur 2000 for deducing the following relationship fig 2 15 v d v max 1 exp a n d in which vmax m s 1 and an cm parameters vary with h m table 2 for calculating raindrop terminal velocity h 20 m vmax and an respectively equal to 9 5 m s 1 and 6 cm have to be applied this equation was used by carollo and ferro 2015 carollo et al 2016 and serio 2017 for estimating the kinetic power by raindrop size distribution dsd in italy based on the investigations of beard and pruppacher 1969 and beard 1976 van dijk et al 2002 used a cubic polynomial function to describe raindrop terminal velocity v m s 1 as function of d mm for drop size of 0 1 7 0 mm 16 v d 0 0000561 d 3 0 00912 d 2 0 503 d 0 254 angulo martínez et al 2016 using raindrop terminal velocity measurements carried out in spain by an optical spectro pluviometer found that eq 16 provided a better fit for smaller drop 0 01 d 0 07 cm whereas eq 1 provided a better fit for mid size drops 0 1 d 0 3 cm the uplinger 1981 equation performed similarly to eq 16 but it overestimated the raindrop terminal velocity values for the smallest drops in conclusion eq 15 is capable to reproduce the physical boundary condition for d 0 55 cm the raindrops are unstable and tend to break up and the agreement with the available measurements fig 2 states its best effectiveness in describing the drop velocity to drop size relationship 3 rain drop size distributions 3 1 types of dsd the raindrop size distribution dsd is an interesting rainfall property for its scientific implication as well as its engineering aspect assouline and mualem 1989 the dsd formation is not fully understood because of the random and complex nature of the physical phenomena that govern the aggregation and disaggregation of the raindrops however the knowledge of dsd is essential to test theoretical models and for estimating the rainfall kinetic energy which plays a fundamental role in soil sealing and erosion assouline and mualem 1989 the raindrop size distribution dsd is defined as the function stating the expected number of drops with diameters between d and d dd the study of the dsd formation and evolution allowed to distinguish two different raindrop size distributions referred to unitary volume of air or to unit area and time the distribution nv d dd represents the number of raindrops with diameters between d and d dd cm per unit volume of air m3 in which d is expressed in cm and volume in m3 and the units of nv d become cm 1 m 3 according to this definition the dsd refers to the spatial distribution of raindrops in the air which governs the raindrop concentration and to the probability distribution of raindrop sizes in the air the second form of the raindrop size distribution n d dd represents the expected number of raindrops with diameters between d and d dd cm arriving at a surface per unit area m2 and per unit time s where d is expressed in cm area in m2 and time in s and so the units n d become cm 1 m 2 s 1 for meteorological studies the size distribution of raindrops commonly refers to the number of drops present in unit volume of air nv d dd while for hydrological studies the dsd usually refers to the number of droplets n d dd that reaches a unit horizontal area during a unit time uijlenhoet and stricker 1999 suggest that nv d yields to know the static properties of the raindrop population i e the spatial size distribution of the raindrops in a volume of air which governs the raindrop concentration and the probability distribution of their sizes in addition to these static properties n d involves the dynamic properties of the raindrop population as well its velocity distribution it should be noted that since the study of the dsd has for the most part been the work of the meteorologists they considered the number of drops present in unit volume of air nv d dd even though n d dd is the distribution which is actually measured in most cases uijlenhoet and stricker 1999 considering a raindrop constituted by drops that fall freely in air i e neglecting the effects of wind turbulence and raindrop interaction the two size distributions are related by the following relationship austin 1987 hall and calder 1993 uijlenhoet and stricker 1999 carollo and ferro 2015 17 n d d d v d n v d d d in which v d is the terminal velocity of the drop having a diameter d for studying accurately the energetic characteristics of raindrops that reach the ground area the number n d dd of raindrops with diameters between d and d dd arriving at a unit area per unit time has to be determined measurements of raindrop size distributions at the ground carried out with different techniques i e photographic method acoustic impact and optical disdrometer micro rain radar suggested that averaged raindrop size distributions can generally be parameterized as unimodal positively skewed distributions characterized by few parameters uijlenhoet and stricker 1999 the most widely applied analytical forms of the raindrop size parameterization are lognormal ln weibull w exponential mp and gamma u distribution 3 2 the lognormal distribution ln several authors mueller and sims 1966 1967 levin 1971 bradley and stow 1974 markowitz 1976 bezdek and solomon 1983 feingold and levin 1986 ochou et al 2007 proposed a lognormal distribution ln for describing the rain drop size distribution nv d in an unit volume of air in different climatic conditions 18 p d n v d n t 1 2 π ln σ d d exp ln 2 d d m 2 ln 2 σ d in which p d is the probability density function of the drop diameter d nt is the total number of the drops per unit volume of air dm and σ d represent the geometric mean diameter cm and the standard deviation of the raindrop diameter cm respectively according to feingold and levin 1986 the parameters of lognormal distribution can be calculated by the following relationships 19 n t 0 n d d d 20 ln d m ln d 21 ln 2 σ d ln d ln d m 2 in which ln d is the arithmetic mean of the natural logarithm of the diameter d and ln d ln d m 2 is the mean of the variable ln d ln d m 2 notice that for large values of σ d and small values of dm the lognormal function has the concave upward shape in much the same way that the gamma distribution does when μ 0 feingold and levin 1986 the rainfall intensity can be estimated by the following equation feingold and levin 1986 22 i 7 1 10 3 n t d m 3 67 exp 6 73 ln 2 σ d the median volume diameter d0 that is the diameter that divides the distribution in two parts of equal volume is given by feingold and levin 1986 23 d 0 d m exp 3 ln 2 σ d which underlines that d0 is a scale and shape parameter as it depends on dm and σ d feingold and levin 1986 using dsd measurements carried out at hadera in israel by an optical disdrometer found that i although the value of σ d varies considerably for low rain rate for i 5 mm h is quasi constant and equal to 1 43 this result implies that the ratio d0 dm kd is constant and equal to 1 47 according to markowitz 1976 using laws and parsons data for 2 5 i 152 4 mm h the σ d parameter remains constant and equal to 1 39 ii nt increases with rainfall intensity as follows 24 n t 172 i 0 22 in agreement with gagin 1980 that proposed an increase nt with i for convective clouds in israel iii dm parameter increases with i according to the equation 25 d m 0 75 i 0 21 feingold and levin 1986 found that lognormal distribution can be fitted to observed dsds in israel better than gamma or exponential distribution they reported also that the lognormal distribution has the advantage that the statistical parameters dm σ d and nt have a physical meaning in particular the variations in the size parameter dm are indicative of the relative importance of coalescence and breakup processes and the parameter σ d reflects the effect that these processes have on the breadth of the spectrum recently ochou et al 2007 fitted the lognormal distribution to dsds detected at abidjan ivory coast dakar sénégal niamey niger and boyélé congo using jwd disdrometer the authors suggested that this distribution is preferred because its parameters have a simple geometric interpretation and the moments can be written in the form of a product of three terms each of them being a function of only one of the three parameters at date at the best of our knowledge no relationship to estimate the rainfall kinetic power has been deduced for the lognormal distribution 3 3 the weibull distribution w langmuir 1948 considered the raindrop formation as a chain reaction process in which raindrops seem to grow up to a point where turbulence raindrop collision or coalescence cause breakup into two or smaller drops which in turn repeat the same process weibull 1951 described this phenomenon assuming to have a chain consisting of n links the chain as a whole fail if any one of its links fails therefore denoting as p f the probability of failure of the chain the nonfailure probability of the chain 1 p f is equal to the probability of the simultaneous nonfailure of all the links weibull 1951 thus 26 p f 1 exp n f x where f x is the function of any load x applied to a single link the analogy of the chain with raindrop is due to the break up mechanism in other words the probability of break up of a chain constituted by n links is equal to the probability of raindrop break up into n parts of it sekine and lind 1982 stated that if in a unit volume of air a drop breaks up and a series of chain reaction processes occur the probability p d that raindrop diameter is less than d can be written as 27 1 0 d p d d d 1 p d exp d ω v δ v where p d is the probability density function of diameter d δv is the shape parameter responsible for the skew of the distribution and ωv mm is the scale parameter the weibull shape parameter δv is a pure number i e dimensionless and is also named weibull slope different values of the shape parameter can have marked effects on the behavior of the distribution when δv 1 the p d of the weibull distribution reduces to that of the exponential law proposed by marshall and palmer 1948 when δv 2 the p d of the weibull distribution reduces to rayleig distribution cohen 1965 changing thescale parameter ωv it does stretch out the existing shape in particular with δv constant the increase of ωv yields to have the distribution stretched to the right in the graph instead decreasing ωv results in the graph being shrunk to the left towards zero applying logarithms to both members of eq 27 it follows 28 ln ln 1 p d δ v ln d ln ω v assuming nv d that is the number of the raindrops in an unit volume of air equal to p d a linear equation is obtained to present the parameters of the weibull distribution 29 x ln d 30 y ln ln 1 0 d n v d d d by differentiating eq 27 and considering the total number of the drops nwv the raindrop size distribution nv d is obtained sekine and lind 1982 31 n v d n wv δ v ω v d ω v δ v 1 exp d ω v δ v where nwv m 3 δ v and ω v mm are the parameters of the distribution sekine and lind 1982 fitting eq 31 to the raindrop size distributions measured by laws and parsons 1943 in washington sander 1975 in berlin and wickerts 1982 in stockholm found that 32 n wv 1000 m 3 while δ v and ω v parameters are dependent on i according to the following relationships 33 δ v 0 95 i 0 14 34 ω v 0 26 i 0 44 jiang et al 1997 using raindrop size distributions detected by a disdrometer installed in tokyo positively verified the reliability of the weibull distribution and suggested that this theoretical probability distribution as the ulbrich one presents a peak that tends to shift toward large raindrop diameter with increasing rainfall intensity jiang et al 1997 comparing the u mp and w distributions with the experimental data found that both u and w distribution give better fit to data than mp one assouline and mualem 1989 suggested a new approach for studying the mechanism of the forces that shape the dsds through coalescence and breakup of the falling drops along their pathway from the cloud to the soil surface the authors hypothesized that the result of coalescence and breakup processes is equivalent to a series of fragmentations that are random in nature considering the assumptions that i the probability for a drop break up is proportional to its volume ii the probability that a break up of a drop of size w yields drops of size v is independent of v iii v is uniquely related to raindrop diameter assouline and mualem 1989 proposed the following probability function 35 f d i 1 exp r d n where n and r are the two parameters of the weibull distribution which are estimated by the following equations 36 d g r 1 n γ 1 1 n 37 σ 2 r 2 n γ 1 2 n γ 2 1 1 n where γ is the gamma function using dg as the scaling factor the following scaled diameter d is obtained 38 d d d g d r 1 n γ 1 1 n from eq 38 it follows 39 γ n 1 1 n d n r d n substituting eq 39 into eq 35 with 40 γ γ n 1 1 n the following dimensionless one parameter distribution function is obtained 41 f d 1 exp γ d n the power n 3 of the diameter d derives from the assumption that fragmentation probability is related to drop volume if the theoretical power n 3 is adopted in eqs 40 and 41 considering the basing assumption which relates fragmentation probability to the volume of the drop assouline and mualem 1989 proposed the following relation 42 f d 1 exp 0 71 d 3 which is a probability distribution function f d that is independent of both intensity and site where precipitation occurs named universal distribution assouline and mualem 1989 in other words the introduction of a relative raindrop diameter eq 38 transformed the theoretical dsd into a universal relationship eq 42 assouline and mualem 1989 using raindrop size distributions measured by hudson 1965 in rhodesia and by carter et al 1974 at baton rouge and holly springs observed that the measured distributions at each site are overlapped to the universal distribution eq 42 this circumstance confirms that the use of the scaled diameter d involves that eq 42 is not site specific assouline 2009 using dsds measurements carried out at princeton new jersey by smith et al 2009 found that eq 42 overestimates the probability function for small d values d 2 jiang et al 1997 using dsd measurements carried out in tokyo suggested that for i 1 88 mm h i 14 32 mm h i 46 40 mm h the weibull distribution agrees with the experimental data better than the u and mp ones instead the marshall and palmer distribution gives a good fit to the experimental data only for the lowest rainfall intensity value i 1 88 mm h at date at the best of our knowledge no relationship to estimate the rainfall kinetic power has been deduced for the weibull distribution 3 4 the marshall and palmer distribution mp marshall and palmer 1948 proposed a simple negative exponential parameterization mp for the raindrop size distribution as a fit to filter paper measurements of the dsds for rainfall intensities between 1 and 23 mm h uijlenhoet and stricker 1999 43 n v d n 0 v e x p λ v d where nv d expressed in m 3 cm 1 is equal to the number of raindrops per unit volume of air and per unit size interval having equivolume spherical diameter d cm the parameters of the exponential distribution n0 v m 3 cm 1 and λv cm 1 represent the total number of the raindrops and the scale parameter respectively marshall and palmer 1948 suggested that n0 v has a constant value equal to 8 104 m 3 cm 1 and λv varies with the rainfall rate i mm h 1 according to the following power law 44 λ v 4 1 i 0 21 ulbrich 1983 carrying out an analysis of drop size spectra of laws and parsons 1943 revealed that these data can be also represented by the marshall and palmer distribution but n0 v varies with rainfall intensity many other researchers marshall and palmer 1948 waldvogel 1974 carbone and nelson 1978 joss and gori 1978 suggested that the exponential distribution is a very good approximation to the raindrop size distribution referred to natural rainfall several authors gunn and marshall 1955 mueller 1966 waldvogel 1974 joss and gori 1978 sempere torres et al 1994 also pointed out that the data analyzed by marshall and palmer 1948 failed to include the smaller raindrops 1 mm and in particular this dsd tends to overestimate the number of both smallest drops gunn and marshall 1955 mueller 1966 waldvogel 1974 sempere torres et al 1994 and largest ones joss and gori 1978 sempere torres et al 1994 ochou et al 2007 carrying out an analysis about dsd types suggested that the mp distribution is widely used for describing the mid latitude dsds that are characterized by low to moderate intensity instead for tropical latitudes where mean rainfall intensities is 6 to 7 times higher than at mid latitudes sauvageot 1994 eq 43 is not able to give a good dsd characterization it was also observed that for i higher than 10 20 mm h 1 and for d larger than the modal value 0 1 0 2 cm the λ parameter becomes almost constant pasqualucci 1982 sauvageot and lacaux 1995 in contrast with eq 44 the constant slope observed in heavy tropical rain expresses the evolution of the dsd towards an equilibrium form in which the shape of dsds does not change any more ochou et al 2007 3 5 the ulbrich distribution u the gamma distribution u proposed by ulbrich 1983 which is a generalization of the mp one has been applied by several authors for describing dsd measurements carried out using different measurement techniques filter paper photoelectric spectrometer drop camera doppler radar optical disdrometer blanchard 1953 dingle and hardy 1962 muller 1965 caton 1966 joss and gori 1978 gori and geotis 1981 chandrasekar and bringi 1987 tokay and short 1996 ulbrich and atlas 1998 brawn and upton 2008 carollo and ferro 2015 carollo et al 2016 ulbrich s distribution is largely used because of its appropriate form for characterizing the distribution of the droplets in clouds khrgian et al 1952 aerosols levin 1961 and precipitation particles sulakvelidze 1969 sulakvelidze and dadali 1971 this theoretical distribution has the following expression 45 n v d d d n 0 v d μ v e x p λ v d d d in which μv λv and n0 v are the statistical parameters of the distribution in particular the exponent μv represents the shape parameter and it can assume positive or negative values fig 3 for positive values of μ v the dsd is concave downward has a narrow breadth and falls rapidly to zero as d 0 for negative values of μv the dsd is concave upward on semi logarithmic plot and has large breadth with increasing numbers of drops at both small and large diameters ulbrich 1983 the n0 v parameter is expressed as m 3 cm 1 μ and λv is a scale parameter for μv equal to zero eq 45 coincides with the marshall palmer distribution in agreement with uijlenhoet and stricker 1999 carollo and ferro 2015 found that combining eq 45 eq 17 and eq 1 the following relationship can be obtained 46 n d d d 17 67 d 0 67 n 0 v d μ v exp λ v d d d eq 46 can be rewritten in the following form 47 n d d d n 0 d μ exp λ d d d where n0 17 67 n0 v μ μv 0 67 and λ λv eq 47 having the same mathematical shape of eq 45 is also reliable for estimating drop size distribution per unit area and unit time ulbrich 1983 demonstrated that the median volume diameter d0 that is the diameter that divides the distribution in two part of equal volume can be expressed uniquely in term of λ for μ 0 the relationship between λ d0 and dmax i e the maximum diameter of the dsd is determined by the analytical definition of d0 as 48 2 0 d 0 d 3 n d d d 0 d max d 3 n d d d this relationship can be rewritten as follows 49 2 γ 4 μ λ d 0 γ 4 μ λ d max where γ is the gamma function fig 4 shows the relationship between λd0 and λdmax as defined by eq 49 for values of 2 μ 3 it highlights that the value of λdmax at which λd0 is close to its limiting value depends on μ ulbrich 1983 found that λd0 according to eq 49 can be assumed approximately equal to 50 λ d 0 3 67 μ which is accurate to within 0 5 for all μ 3 for rhodesian precipitations hudson 1971 represented the relationship between d 0 and rainfall intensity by a curve that presents a maximum point for i 80 mm h even if d 0 values are characterized by a low variability 2 0 d0 2 5 mm in the explored range of i 25 i 200 mm h carter et al 1974 aggregated the dsds relieved in louisiana and mississippi for 13 intensity classes and found that d 0 does not increase for rainfall intensities greater than about 65 mm h many other researches laws and parsons 1943 atlas 1953 kelkar 1959 zanchi and torri 1980 brandt 1988 jau yau et al 2008 proposed a power law for describing the relationship d 0 i implying that d 0 continues to increase indefinitely with i this last result is in contrast with other researches which stated that a maximum median volume diameter value is reached at high rainfall intensities usually above 70 100 mm h after which the d 0 becomes constant kinnell 1981 rosewell 1986 brown and foster 1987 carollo et al 2016 or even decreases hudson 1963 baruah 1973 carter et al 1974 van dijk et al 2002 when the parameters d0 and μ are known λ parameter can be estimated by the following equation 51 λ 3 67 μ d 0 rainfall intensity i mm h 1 can be calculated from the following expression salles et al 2002 carollo and ferro 2015 52 i 3 6 π 6 0 d 3 n d d d 3 6 π 6 n 0 0 d 3 μ exp λ d d d where d is expressed in cm and n d dd in m 2 s 1 taking into account that olver 1997 53 0 d 3 μ exp λ d d d γ 4 μ λ 4 μ eq 52 becomes 54 i 3 6 π 6 n 0 γ 4 μ λ 4 μ eq 54 links the dsd parameters to rainfall intensity and allows to calculate n 0 parameter as function of i and of the other two dsd parameters 55 n 0 λ 4 μ 3 6 π 6 γ 4 μ i the median drop diameter d 50 and the median volume drop diameter d 0 considering the ulbrich s distribution can be approximately calculated by the following relationships ulbrich 1983 uijlenhoet and stricker 1999 carollo et al 2016 serio 2017 56 d 50 0 67 μ λ 57 d 0 3 67 μ λ from eqs 56 and 57 the two ulbrich s parameters can be deduced 58 μ d 0 λ 3 67 59 λ 3 d 0 d 50 carollo and ferro 2015 using 23 967 dsds measured in sicily in the period june 2006 february 2012 verified the reliability of the ulbrich s distribution in sicilian environments estimating μ and λ by both maximum likelihood method ml and momentum method mm imposing the coincidence between the mean and standard deviation of the measured distribution with the theoretical statistical parameters one fig 5 shows as an example for three rainfall intensity values 9 5 52 5 and 124 5 mm h 1 the fitting of the gamma distribution u to the dsds detected in sicily this figure demonstrates the reliability of the ulbrich s distribution in the sicilian environment and its ability to reproduce the presence of large raindrops corresponding to high p d values carollo and ferro 2015 found that the parameter μ varies from 0 85 to 76 9 is generally positive 23756 dsd of the 23 967 measured 160 ones and assumes a median value of about 5 while parameter λ ranges from 3 3 to 591 5 both parameters slightly decrease as rainfall intensity increases and in agreement with results of other authors zhang et al 2003 brawn and upton 2008 are correlated 4 rainfall kinetic power relationships 4 1 rainfall kinetic power intensity relationships rainfall kinetic power pn j m2 h can be calculated by adding the contribution of each raindrop once its mass and terminal velocity are known therefore the energy characteristics of a rainfall event can be indirectly measured if its dsd and the relationship between terminal drop velocity and drop diameter are known at date many researchers proposed empirical relationships linking kinetic power to rainfall intensity having different mathematical forms polynomial exponential logarithmical and power type table 1 the most commonly used relationship for estimating pn as function of rainfall intensity is that proposed by wischmeier and smith 1978 and used in the universal soil loss equation usle according to wischmeier and smith 1978 the event rainfall runoff erosivity factor ei30 is the result of the product of the total rainfall kinetic energy of the rainstorm per unit area e mj ha 1 and i30 mm h 1 which is the maximum 30 min rainfall intensity e is due to the product of the rainfall kinetic energy per unit volume of rainfall pn i j m 2 mm 1 and the rainfall depth referred to each raining period for estimating pn i wischmeier and smith 1978 proposed the following relationship having a logarithmic form based on the measurements of drop size distribution and terminal velocity observed at washington dc by laws and parsons 1943 60 p n i 11 9 8 73 log i for i i t 11 9 8 73 log i t for i i t in which it is the intensity threshold value that according to wischmeier and smith 1978 is equal to 76 mm h 1 according to eq 60 the ratio pn i increases for rainfall intensity value less than or equal to it and then it becomes constant for rainfall intensity greater than it wischmeier and smith 1978 justified this threshold value suggesting that the median volume diameter d0 does not continue to increase when rainfall intensities exceed 76 mm h 1 for describing the same trend kinnell 1981 proposed the following relationship 61 p n i a 1 b e x p c i where a b and c are parameters according to eq 61 pn i has a finite positive value at zero intensity and approaches as well as eq 60 to an asymptotic value pn i a at high intensities brown and foster 1987 using data from brisbane and gunnedah australia holly springs mississippi miami florida washington and zimbabwe estimated a 29 j m 2 mm 1 b 0 72 and c 0 05 h mm 1 the asymptotic value a is practically coincident with the one pn i 28 3 j m 2 mm 1 obtained by eq 61 for i 76 mm h eq 61 with the parameter values proposed by brown and foster 1987 is used in revised universal soil loss equation rusle renard et al 1997 in rusle2 equation foster 2004 suggested eq 61 for estimating rainfall kinetic power with the parameter a b and c equal to 29 j m 2 mm 1 0 72 and 0 082 h mm 1 respectively mcgregor et al 1995 kinnell 1987 reported that a parameter can be assumed equal to 29 j m 2 mm 1 while b and c parameters are site specific salles et al 2002 however other researchers proposed different values of a b and c parameters confirming their dependence on geographical location table 3 mcgregor et al 1995 comparing eqs 60 and 61 verified that for 1 mm h i 35 mm h pn i values estimated by eq 61 with the coefficient values suggested by brown and foster 1987 are 12 less than those predicted by eq 60 they concluded that the annual erosivity predicted by eq 61 with parameter values suggested by mcgregor and mutchler 1976 and eq 60 were almost identical whereas pn i values predicted by eq 61 with the parameter values proposed by brown and foster 1987 were about 8 lower yin et al 2017 the underestimation of the pn i in the rusle model was also reported in australia yu 1998a b belgium verstraeten and poesen 2006 and peninsular malaysia yin et al 2017 fornis et al 2005 using disdrometric measurements carried out in central cebu philippine calibrated both eq 60 and eq 61 they verified that the calibrated exponential relationship eq 61 a 30 68 b 0 05 and c 0 03 is more reliable to predict the kinetic power per unit volume of rainfall than the equation proposed by wischmeier and smith lim et al 2015 carrying out an analysis on rainfall kinetic power by raindrop size distribution measured in korea suggested that as i increases the pn i value also increases until i is equal to 60 mm h and then it becomes constant the calibrated exponential relationship proposed by kinnell 1981 eq 61 was able to predict the kinetic power per unit volume of rainfall in particular they stated that for i 30 mm h the exponential model fitted the observed data well but it heavily underestimated the pn i values for i 60 mm h lobo and bonilla 2015 developing a sensitivity analysis of kinetic energy intensity relationships on rainfall erosivity using rainfall intensity values measured in central chile suggested that i eq 60 provides statistically equivalent erosivity values to those that are computed by eq 61 with parameter values suggested by mcgregor and mutchler 1976 ii eq 60 predicts larger erosivity estimates than eq 61 for i 5 5 mm h which is the maximum measured rainfall intensity in central chile finally authors concluded that eq 60 and eq 61 are highly sensitive to change in its regression parameters thus these equations should be calibrated based on local precipitation data to generate reliable erosivity estimates ciaccioni et al 2016 carried out an analysis on rainfall erosivity in slovenia using disdrometer data in particular they concluded that i at high values of i equations from coutinho and tomás 1995 and cerro et al 1998 table 1 tend to slightly overestimate the measured kinetic power ii the use of the equations suggested by wischmeier and smith 1978 brandt 1990 usòn and ramos 2001 and van dijk et al 2002 table 1 tend to underestimate the rainfall kinetic power therefore ciaccioni et al 2016 concluded that a general equation relating pn and i is not suitable to estimate correctly the kinetic power of the precipitation but equations as eq 61 need to be calibrated using at site data angulo martínez et al 2016 using dsds detected in spain found that the relationship between pn i and i is asymptotic and in particular pn i becomes nearly constant at an i value of about 65 mm h these authors verified the reliability of eq 61 for pn i estimates using parameter values proposed by brown and foster 1987 angulo martínez et al 2016 concluded that none of these empirical expressions can be used universally since their applicability depends on the geographical and meteorological conditions of investigated sites carollo et al 2017 using dsd detected at palermo sicily in the period june 2006 march 2014 and aggregated in intensity classes found that the wischmeier and smith 1978 relationship is fully applicable for estimating the rainfall kinetic power in sicilian environment in fact eq 60 is able to reproduce the pn i values measured at palermo experimental site for i 40 mm h even if it determines a slight overestimate for 2 i 15 mm h 1 and systematically overestimates the kinetic power for i 40 mm h 1 however this last overestimation can be avoid setting it in eq 60 equal to 40 mm h the median volume diameter d0 and pn i values measured at palermo show an increasing trend with rainfall intensity for i 40 mm h after which does not more increase with i carollo et al 2017 these results represent a further confirmation of the applicability of the wischmeier and smith 1978 approach even if the threshold value of rainfall intensity it 40 mm h resulted less than the one proposed by wischmeier and smith 1978 it 76 mm h carollo et al 2017 carollo et al 2017 also tested the reliability of the eq 61 for estimating pn i at palermo in particular they found that eq 61 with the parameters values proposed by brown and foster 1987 for i 30 mm h 1 gives values slightly greater than the ones obtained by eq 60 instead eq 61 underestimates systematically pn i especially for the lowest values of i i 20 mm h however in the whole range of i the shape of the curve corresponding to eq 61 describes the pn i i relationship better than eq 60 even if eq 61 needs to be calibrated other researchers park et al 1980 smith and de veaux 1992 uijlenhoet and stricker 1999 steiner and smith 2000 sanchez moreno et al 2012 verified the reliability of a power law relationship for which the ratio pn i does not become constant for any value of i in other words for high values of rainfall intensity a power law relationship implies a trend different from eq 60 and eq 61 sanchez moreno et al 2012 suggested that the power law equation seems to represent better than eq 60 and eq 61 the kinetic power of the rainfall in cape verde characterized by a sequence of extreme short events with high intensities salles et al 2002 carrying out an overview of many empirical relationships pn i showed that for a fixed rainfall intensity these relationships yield to very different values of kinetic power using measurements of rainfall intensity drop size distribution and drop size specific fall velocity carried out by optical laser disdrometers in five experimental stations in germany wilken et al 2018 suggested that the commonly used empirical pn i relationships table 1 overestimate the measured rainfall kinetic power furthermore in agreement to van dijk et al 2002 and meshesha et al 2014 carollo et al 2018 stated that there is no a single empirical pn i equation that can be recommended for any part of the world according to parsons and gadian 2000 salles et al 2002 concluded that a global parameter as rainfall intensity i or median volume diameter d0 is not sufficient to determine the rainfall erosivity since that kinetic power measurements are also dependent on other parameters rain type altitude climate and method of measurement 4 2 deducing rainfall kinetic power intensity relationship by dsd the kinetic power pn j m2 h knowing both dsd and terminal velocity can be calculated as salles et al 2002 62 p n 10 6 ρ π 12 0 v d 2 d 3 n d d d where ρ is the water density kg m 3 v d is the terminal velocity of the drop having a diameter d m s 1 and n d dd is the number of raindrops with diameters between d and d dd arriving at a unit area per unit time the use of eq 62 requires the knowledge of the raindrop size distribution at date at the best of our knowledge no relationship to rainfall kinetic power estimate has been deduced for the weibull and lognormal distribution applying the gamma distribution ulbrich 1983 raindrop terminal velocity estimated by eq 15 ferro 2001 and using eq 62 carollo and ferro 2015 deduced theoretically the following expression 63 p n i 10 6 9 5 2 7 2 ρ λ 4 μ 1 λ 4 μ 2 6 λ 4 μ 1 12 λ 4 μ according to eq 63 the rainfall kinetic power can be determined if both the rainfall intensity and μ and λ parameters of the dsd are known this circumstance allows to conclude that the ratio pn i depends on the intrinsic characteristics of rain carollo and ferro 2015 using approximately 24 000 dsds detected by an optical disdrometer at palermo sicily in the period june 2006 february 2012 showed that i eq 63 reproduces adequately the kinetic power measurements specially using μ and λ values obtained by momentum method ii eq 60 is fully applicable to rainfall recorded in sicily and has to be preferred to a power relationship calibrated on collected data that tends to overestimate the rainfall kinetic power for high values of i iii eq 61 yields appreciable underestimates of pn for low values of rainfall intensity carollo et al 2016 taking into account that the marshall and palmer 1948 distribution per unit time and area can be assumed formally identical to the ulbrich 1983 one by combining eq 57 and eq 63 with μ 0 67 obtained the following theoretical relationship 64 p n i 10 6 9 5 2 7 2 ρ 1 2 6 d 0 4 34 1 4 67 1 6 d 0 2 17 1 4 67 which clearly shows that the ratio pn i depends only on median volume diameter according to eq 64 if d0 assumes a constant value when i exceeded a threshold value pn i becomes constant in other words eq 64 represents a theoretical confirmation of the wischmeier and smith 1978 hypothesis finally carollo et al 2018 presented the comparison between the pairs i pn i and i d0 corresponding to palermo dataset with the ones obtained by different measurement techniques drop camera piezoelectric force transducer blotting paper in other sites of the world marshall islands new jersey alaska indonesia oregon franklin hong kong ethiopia spain they concluded that the relationships pn i i and d0 i are site specific and therefore a single like eq 60 or eq 61 is not reliable for estimating rainfall kinetic power at any site fig 6 on the contrary the experimental pairs d0 pn i relative to all available datasets resulted near to a single increasing curve that can be described by eq 64 fig 6 this theoretically derived relationship resulted fully applicable to all available datasets demonstrating that pn i d0 relationship is free from at site effects and useful to adequately characterize the rainfall erosivity serio 2017 in other words for a given rainfall intensity kinetic power depends on the raindrop size distribution reaching soil surface and the median volume diameter is an useful characteristic diameter able to synthesize whole dsd information 5 seasonal effects on precipitation the predicted soil loss is affected by the interaction between the erosive power of rainfall and the soil protection action of vegetative canopy crop canopy which is the aboveground part of vegetation that intercepts rainfall varies during the year with natural growth processes maximum erosion control is obtained using a crop having the maximum canopy during periods of high rainfall erosivity toy et al 2002 soil loss is greatest when the peak period of rainfall erosivity corresponds to the period when the soil is most exposed to raindrop impact at date very few studies have been carried out in order to understand how the seasonality could influence the raindrop formation alonge and afullo 2012 hachani et al 2017 and evolution during its fall 5 1 influence of seasonality on dsd in literature alonge and afullo 2012 carrying out a seasonal analysis using dsd measured at durban south africa suggested that seasonality influences raindrop formation in particular this analysis highlighted that for rainfall intensity i up to 10 mm h the shape of the raindrop size probability density function in durban south africa may be similar for summer autumn and spring seasons however for i 10 mm h winter season presents a raindrop size probability density function different from the other seasons having larger raindrop size alonge and afullo 2012 verified also the reliability of the ln w and u statistical models establishing that a different probabilistic model should be used for each season in particular the lognormal distribution best fits the summer and autumn the ulbrich distribution fits winter and weibull distribution fits spring season the analysis also shows that the highest number of the rain drops per unit area and time occurs in summer while the winter is characterized by the lowest number of raindrops the authors also highlighted the existence of seasonality respect to dm and σ d for the weibull model referred to raindrop size distribution per unit area and time the values of the shape parameter δ resulted similar for the seasons of summer autumn and spring in addition the scale parameter ω for the seasons of autumn and spring are also found to be close instead winter season has ω values higher than summer season for the ulbrich distribution all seasons are observed to take different values of the scaling parameter n0 however the values of the λ parameter estimated for summer and autumn are the closest alonge and afullo 2012 hachani et al 2017 used dsds detected at the cévennes vivarais region france by six ott parsivel optical disdrometers to investigate the seasonal influence on rain drop size distributions at first the ulbrich raindrop size distribution was fitted to measured dsds the analysis highlighted that the shape parameter is almost identical in spring and autumn the number of drops was most similar in autumn and winter the spring and autumn seasons present hybrid characteristics between those of winter and summer the intra annual variability of rainfall erosivity can be high panagos et al 2016a and this information can be useful for identifying the high risk periods in which soil exposure to rainfall coincides with occurring high erosive events to date no study has been carried out to know if the seasonality of drop size distribution could have any influence on rainfall erosivity i e on the energetic characteristics of rainfall that reaches soil surface 5 2 new findings seasonality on dsds detected at palermo experimental area in this section an analysis of the seasonality effects on characteristics of dsds that reach an unit area in an unit time and on erosive power of the precipitation at palermo italy is presented measurements of drop size distributions were carried out using an optical disdrometer placed at the experimental area equipped near the department of agricultural food and forestry sciences of the university of palermo fig 7 this disdrometer model odm 470 made by eigenbrodt measures drop diameters in the range 0 05 0 6 cm each drop is separately measured and registered into classes of about 0 005 cm width drop diameter is measured by registering light damping due to the passage of the drop in the control volume between two diodes fig 7 more details about construction and functioning features of disdrometer are reported in grossklaus et al 1998 and carollo and ferro 2015 the disdrometer registered 524 rainfall events in the period june 2 2006 march 10 2014 for each rainfall event only the dsds for which the rainfall intensity was 0 5 mm h and measured diameter classes were at least 20 were considered this choice excluded both rainfalls having low erosive power and dsds having a small sample size this procedure provided 42 264 dsds with a sampling time of 1 min in order to better focus the influence of seasonality and rainfall intensity on both dsd and rainfall energetic characteristics laws and parsons 1943 carter et al 1974 sauvageot and lacaux 1995 jayawardena and rezaur 2000 the 42 264 dsds named instantaneous dsds were firstly distinguished in spring april may june summer july august september autumn october november and winter december january february march and then aggregated in intensity classes having width equal to 1 mm h the rainfall distribution during the year is characterized by the highest values of monthly rainfall in autumn and winter wet period and the lowest values in spring and summer dry period in particular the mean annual value of the rainfall depth at palermo site is equal to 410 mm in the winter period while in autumn spring and summer 220 mm 120 mm and 100 mm occur respectively for each class the rainfall intensity was calculated as average of the intensities of the instantaneous dsds falling into the class this procedure allows to have number of aggregated dsds named dsds equal to 27 dsds for spring 62 for summer 79 for autumn and 74 for winter table 4 reports some characteristic data of the erosive events referred to each season fig 8 shows the number of raindrops falling in each intensity class versus mean rainfall intensity referred to each class in general for each season the number of the raindrops increases for very low values of i i 5 mm h fig 8 a while for 5 i 40 mm h fig 8 a the number of the raindrops decreases as rainfall intensity increases for winter and autumn seasons the number of the raindrops becomes constant for i 100 mm h fig 8 b the ulbrich s distribution was fitted to each dsds using the maximum likelihood method to estimate μ and λ parameters the frequency distribution of μ and λ parameters referred to each season datasets are calculated and reported in fig 9 in particular the frequency distributions of the shape parameter μ are found to be closer for summer spring and autumn rains than the winter one in fact in agreement to hachani et al 2017 the values of statistical indices mean median standard deviation and coefficient of variation table 5 are closest and suggest that a similarity in dsd shape in these seasons exists fig 9 b shows the frequency distributions of the dsd slope parameter λ referred to each season in winter the dsds are characterized by the highest values of λ parameter in addiction from spring to winter the raindrop size distributions present an increasing slope parameter value each dsd is characterized by a pair μ λ thus the possible influence of the seasonality on the μ and λ relationship has to be investigated fig 10 shows the pairs μ λ relative to precipitations occurred in each season a single relationship between μ and λ parameters can be observed for spring summer and autumn seasons which are also characterized by the nearest values of the total rainfall depth while in winter season the precipitations characterized by i 40 mm h present a μ λ relationship different from those with i 40 mm h fig 10 in addition the pairs μ λ relative to winter precipitations with i 40 mm h are quasi overlapped to spring summer and autumn ones suggesting that a dsd similarity exists therefore these results suggest that for i 40 mm h winter dsds are different from the other season ones and within winter season dsds characterized by i 40 mm h are different from those with i 40 mm h for each dsds the median volume diameter value was also determined fig 11 shows the pairs i d0 referred to each season the measurements suggest that for all rainfall intensity classes the pairs i d0 are overlapped and thus the relationship between d0 and i is not seasonally dependent fig 11 for i 40 mm h the median volume diameter shows an increasing trend with i and a single power law can be fitted fig 11 a instead for i 40 mm h d0 does not depend on rainfall intensity and can be considered quasi constant and equal to 0 25 cm fig 11 b for each season the kinetic power per unit volume of rainfall pn i by dsds was also determined by associating each diameter with the terminal velocity estimated by eq 15 with vmax and an respectively equal to 9 5 m s 1 and 6 cm fig 12 shows the comparison between pn i values versus rainfall intensity of aggregated dsds for i 40 mm h a and for i 40 mm h b in different seasons the pairs i pn i are overlapped highlighting that no season dependence of the relationship between pn i and i exists for the precipitation registered at palermo the pn i values measured at palermo show an increasing trend with rainfall intensity for i 40 mm h fig 12 a after which becomes constant pn i 26 j m2 mm fig 12 b this performance is similar to d0 i one these results represent a further confirmation of the applicability of the wischmeier and smith 1978 approach even if the threshold value of rainfall intensity it 40 mm h resulted less than the one proposed by wischmeier and smith 1978 it 76 mm h carollo et al 2017 6 conclusive remarks and research needs the rainfall kinetic power can be calculated by drop size distribution dsd measurements combined with fall velocity values measured or estimated by empirical relationship linking terminal fall velocity and drop diameter this paper presented the most commonly used relationship for raindrop terminal velocity estimate highlighting that the rainfall terminal velocity depends only on rainfall diameter and fall height the most widely used theoretical dsds were also reviewed highlighting the need to distinguish the raindrop size distribution per unit volume of air from that per unit area and time the raindrop size distribution per unit volume of air yields to know the static properties of the raindrop population i e the spatial size distribution of the raindrops in a volume of air which governs the raindrop concentration and the probability distribution of their sizes in addition to these static properties the raindrop size distribution per unit area and time involves the dynamic properties of the raindrop population as well its velocity distribution however if the effects of wind turbulence and raindrop interaction are neglected the two size distributions are related to date at the best of our knowledge no relationship to rainfall kinetic power estimate has been deduced for the weibull and lognormal distribution the reliability of the most commonly used relationships to estimate rainfall kinetic power in several part of the world was also reviewed highlighting that a single relationship pn i i is not applicable at any site furthermore a relationship relating pn i and i as those proposed by wischmeier and smith and by kinnell needs to be calibrated on data the theoretically derived relationship between pn i d0 from the marshall and palmer distribution per unit time and area suggested by carollo et al resulted fully applicable to all available datasets this result demonstrates that pn i d0 relationship is free from at site effects and useful to adequately characterize the rainfall erosivity this circumstance represents a theoretical confirmation of wischmeier and smith hypothesis which established that the trend of pn i depends strictly on the d0 one in other words the global parameter d0 is sufficient to determine the ratio pn i and it is a variable free from at site effects and useful to characterized rainfall erosivity finally an analysis on influence of seasonality in dsd and rainfall energetic characteristics at palermo experimental area was also carried out this investigation yields to conclude that the seasonality influences the raindrop formation only in winter season however the relationship between pn i and i is not season dependent as d0 and i too in other words at the best of our knowledge the erosive power of the precipitation does not depend on the season when the rainfall occurs therefore this paper highlights that future works on rainfall erosivity should be addressed to establish techniques able to give measurements of the raindrop terminal velocity to carry out contemporaneous measurements of drop size and terminal velocity by laser disdrometers to deduce relationships to estimate rainfall kinetic power by weibull and lognormal distributions to verify the reliability of the probability distribution function f d proposed by assouline and mualem to verify the theoretically derived relationship between pn i d0 suggested by carollo et al in different site of the world to find new models or techniques able to provide good estimates or measurements of the median volume diameter of the distribution to investigate the influence of the seasonality on both rain drop size distribution per unit area and time and rainfall erosivity in other site of the world to relate the seasonality of drop size distribution with the rain forming systems to investigate the effect of high resolution precipitation data on the calculation of the rinfall erosivity factor declaration of competing interest none acknowledgements all authors set up the research analyzed and interpreted the results and contributed to write the paper 
6288,the knowledge of the rainfall drop size distribution dsd at the land surface is essential for understanding precipitation mechanisms affecting soil erosion processes rainfall erosivity is defined as the potential of rain to cause erosion and it can be evaluated by rainfall kinetic power which is determined by dsd and raindrop terminal velocity this paper firstly deals with the raindrop terminal velocity estimate then the most widely used dsd are reviewed highlighting the difference between the raindrop size distribution per unit volume of air and that per unit area and time the reliability of the available kinetic power rainfall intensity relationships and their application in several part of the world is discussed highlighting that the use of rainfall intensity is not sufficient to determine the rainfall kinetic power everywhere finally the influence of seasonality on both raindrop size distribution and rainfall energy characteristics is investigated using dsd measurements carried out by an optical disdrometer placed at palermo experimental area keywords rainfall erosivity rainfall kinetic power rainfall intensity raindrop size distribution seasonality 1 introduction the acceleration of soil erosion process through anthropogenic perturbation has severe impacts on soil and environmental quality soil erosion reduces the long term soil productivity and it exposes subsoil which has often poor qualities for crop establishment and growth water soil erosion is a process of detachment and transport of soil particles due to rainfall and runoff and it is one the main causes of landform modelling on earth s surface at the plot and field scale when interrill erosion occurs runoff is a factor affecting the transportability of soil material detached by raindrop impact while runoff is able to cause both detachment and transport when rill erosion takes place rainfall erosivity is the main property affecting erosion processes involving both detachment of soil particles and the subsequent transport of the detached particles away from the site of detachment ferro et al 1991 mannaerts and gabriels 2000 salles and poesen 2000 according to wischmeier and smith 1978 rainfall erosivity is commonly expressed as the product of two factors the rainfall energy e and the maximum continuous 30 min intensity i30 during the individual storm nearing et al 2017 wischmeier 1959 analyzing approximately 8000 plot years of rainfall runoff and soil loss data confirmed the suitability of ei30 index for fallow and continuous row crop plots the suitability was also positively tested at different temporal scales seasonal and yearly variation yin et al 2017 the quantitative expression of energy per unit of rainfall developed by wischmeier and smith 1978 was based on the work of laws and parsons 1943 and gunn and kinzer 1949 the rainfall erosivity factor r proposed in the universal soil loss equation usle and in its revised rusle version is recognized as one of the best indicators for modelling the erosive potential of a rainstorm renard et al 1991 regional erosivity maps are used to identify areas with high potential rainfall erosivity thus with high risk of severe soil erosion ferro et al 1991 aronica and ferro 1997 ferro et al 1999 bonilla and vidal 2011 klik and konecny 2013 panagos et al 2015a b 2016a at the plot and field scales soil loss by water erosion can be related to rainfall by different precipitation properties i e rainfall intensity i mm h rainfall momentum m n m2 and rainfall kinetic energy per unit time and area named kinetic power pn j m2 h van dijk et al 2002 carollo and ferro 2015 carollo et al 2016 2017 2018 the rainfall momentum m represents the pressure or force per unit area provoking the mechanical stress and thus might be expected to be related to the breakdown of soil aggregates rose 1960 in other words when the raindrop reaches soil exerts a knock on the ground area due to the charge of momentum amount this phenomenon provokes the split of the bonds between soil particles making them available to be transport by overland flow carollo et al 2018 during years several researchers rose 1960 hudson 1971 abd elbasit et al 2010 sanchez moreno et al 2012 lim et al 2015 have investigated to understand if the raindrop impact is more dependent on m than pn or if these two variables are correlated with each other rose 1960 stated that rainfall momentum is a slightly better predictor for soil detachment than kinetic power for natural rainfall hudson 1971 demonstrated that momentum and kinetic power present a very similar relationship with rainfall intensity and the only question is which variable can be more easily and accurately measured van dijk et al 2002 other studies brodie and rosewell 2007 abd elbasit et al 2010 sanchez moreno et al 2012 using rainfall simulators showed that both kinetic energy and rainfall momentum can be used to predict soil splash in particular brodie and rosewell 2007 carrying out an analysis on rainfall intensity erosive indices relationships found that the rainfall momentum and pn could be interchangeable with each other for the estimation of the rainfall erosivity in soil erosion processes abd elbasit et al 2010 sanchez moreno et al 2012 using measurements of rainfall drop size distribution dsd and raindrop terminal velocity carried out by a laser optical disdrometer parsivel lim et al 2015 stated that the rainfall momentum is the best predictor of rainfall erosivity in korea carollo et al 2018 using measurements of dsds carried out by an optical disdrometer installed in two different mediterranean sites palermo italy el telauret spain suggested that m and pn referred to the unit volume of rainfall are related each other and thus both indices can be used to express the erosive power of the precipitation the rainfall kinetic power results from the kinetic power of single raindrops that constitute precipitation and varies with the size velocity and impact frequency of the raindrops impacting an area in unit time the rainfall kinetic power is determined by relating drop size distribution dsd to raindrop terminal velocity and rainfall intensity assouline 2009 the force applied to soil by raindrop impact is related to drop size in fact a very small raindrop striking the soil at a low impact velocity exerts very low force on the soil and causes very little erosion regardless of rainfall intensity toy et al 2002 the measured rainfall dsd at the ground level is the result of a series of physical phenomena i e raindrop collision coalescence break up that influence raindrop formation and its evolution during its falling process assouline and mualem 1989 depending on the climate specific location season the dominant microphysical rainfall processes may be different and then lead to different dsds hachani et al 2017 the rain formation is typically classified microphysically how warm or cold process warm rain formation involves the growth of droplets via collision and different models list et al 1987 hu and srivastava 1995 showed that collision coalescence and breakup processes result in an equilibrium shape to dsd regardless of overall raindrop concentration munchak et al 2012 cold rain formation occurs with the melting frozen hydrometeors such as snow or hail these frozen particles are larger than the raindrops out of which warm rain forms and melt into correspondingly larger rain drops falling larger raindrops breakup reducing their size although this process depends on the depth of the above freezing layer and the initial dsd munchak et al 2012 highlighted also that besides formation and internal processes external processes such as evaporation and size sorting can also influence the dsd evaporation preferentially acts on small drops thereby increasing the median volume diameter of the distribution i e the diameter that divides the distribution in two parts of equal volume in addiction the influence of size sorting by wind shear and turbulence on the dsd depends on the particular situation and may act to increase or decrease the median volume drop size munchak et al 2012 wiesner 1895 was a pioneer in measuring the dsd of natural rainfall arriving at the earth s surface using a piece of absorbent paper dusted with a water soluble dye van dijk et al 2002 however during the years several methods stain method flour pellet method oil immersion method photographic method raindrop spectrograph acoustic disdrometer optical disdrometer micro rain radar have been developed for measuring the drop size distribution of the precipitation at the ground and for assessing and monitoring the rainfall erosivity wiesner 1895 bentley 1904 lenard 1904 mache 1904 defant 1905 fuchs and petrjanoff 1937 laws 1941 laws and parsons 1943 gucker 1949 bowen and davidson 1951 blanchard 1952 courshee and byass 1953 mason and ramanadham 1953 mikirov 1957 hudson 1963 joss and waldvogel 1967 mutchler 1967 hall 1970 mutchler and hansen 1970 nawaby 1970 mutchler 1971 mutchler and larson 1971 kohl 1974 carter et al 1974 asseline and valentine 1978 donnadieu 1980 quinn 1981 roels 1981 lowe 1982 mccooll 1982 eigel and moore 1983 hauser et al 1984 illingworth and stevens 1987 navas et al 1990 kincaid et al 1996 tokay and short 1996 grossklaus et al 1998 campos 1999 salles and poesen 1999 löffler mang and joss 2000 nešpor et al 2000 nystuen 2001 reddy and kozu 2003 herngren 2005 lanzinger et al 2006 arnaez et al 2007 egodawatta 2007 schönhuber et al 2007 salvador et al 2009 pérez latorre et al 2010 asante 2011 maahn and kollias 2012 parsakhoo et al 2012 friedrich et al 2013 carollo and ferro 2015 kathiravelu et al 2016 carollo et al 2016 during the years several analytical forms of the raindrop size parameterization have been proposed as lognormal ln feingold and levin 1986 weibull w weibull 1951 exponential mp marshall and palmer 1948 and gamma u ulbrich 1983 distribution however at the best of our knowledge for meteorological and soil erosion purposes the most widely applied analytical form is the gamma distribution khrgian et al 1952 levin 1961 sulakvelidze 1969 sulakvelidze and dadali 1971 uijlenhoet and stricker 1999 carollo et al 2016 the knowledge of the dsd allows i to understand how the rainfall is made up ii to calculate the rainfall kinetic power pn and so iii to characterize energetically the precipitation which plays a fundamental role in soil erosion assouline and mualem 1989 the storm kinetic energy is the total value of the kinetic energy possessed by all raindrops reaching a unit area of the earth s surface during the rainfall event devices that can measure storm kinetic energy are not as commonly used as devices that measure rainfall intensity this circumstance favored the use of empirical relationships between rainfall intensity and rainfall kinetic power to estimate the storm kinetic energies needed in the prediction of soil loss by empirical soil erosion models to date many researchers proposed empirical relationships having different mathematical forms polynomial exponential logarithmical and power type for estimating kinetic power by rainfall intensity table 1 van dijk et al 2002 critically reviewed published studies of rainfall intensity kinetic power relationships evaluating their applicability for predictive purposes since the rainfall kinetic power can be calculated by dsd measurements combined with measured fall velocity or empirical laws linking terminal fall velocity and drop diameter salles et al 2002 this paper firstly deals with the raindrop terminal velocity estimate then the most widely used dsd models are reviewed highlighting also the difference between the raindrop size distribution per unit volume of air and the dsd per unit area and time a review of the reliability of the most commonly used relationships to estimate rainfall kinetic power in several part of the world is also presented and discussed finally the influence of seasonality on dsd and energy characteristics of the precipitation is also investigated using raindrops size distributions measured by an optical disdrometer at palermo experimental site 2 raindrop terminal velocity the raindrop impact velocity depends on both the falling height of each raindrop h and its diameter d davies 1942 spilhaus 1948 gunn and kinzer 1949 best 1950 kessler and wilks 1968 lui and orville 1969 sekhon and srivastava 1971 atlas and ulbrich 1977 uplinger 1981 leone and pica 1993 grosh 1996 ferro 2001 for a drop having a diameter equal to d the falling raindrop velocity grows with h until a threshold value equal to 20 m after which becomes constant and it is named rainfall terminal velocity v d expressed in m s 1 in particular the terminal velocity is reached when there is an equilibrium between the gravitational force and the raindrop aerodynamic resistance actually few terminal velocity measurements of natural raindrops are available in literature beard 1976 jayawardena and rezaur 2000 and the relationships between terminal velocity raindrop diameter were based on measurements of simulated single raindrops that fall in stagnant air lenard 1904 schmidt 1909 laws 1941 gunn and kinzer 1949 blanchard 1950 beard 1976 epema and riezebos 1983 jayawardena and rezaur 2000 theoretically the terminal velocity of a rigid and not deformed sphere having a given diameter and density that falls freely through a medium i e air having an own density can be calculated setting the gravitational force contribution equal to aerodynamic drag one that the particle meets during its motion of fall best 1950 park et al 1983 a large amount of work has been done in wind tunnels for determining the drag coefficient for rigid and not deformed spheres however considering natural rainfall a water droplet falling freely through the air is deformed by the aerodynamic forces and has been observed to vibrate strongly and spin thus producing definite departures from spherical symmetry for large drops vibrations and deformations frequently break up the droplet if it falls sufficiently far this turbulence together with the inevitable deformations of a not rigid sphere make the applicability of the theoretical calculation of the terminal velocity with measured values of the drag coefficient rather questionable gunn and kinzer 1949 ferro 2001 at date the relationship between drop size and terminal velocity is usually described by an exponential or power equation moreover all these relationships are calibrated by laboratory data collected for free falling water drops in still air using data from gunn and kinzer 1949 atlas and ulbrich 1977 proposed the following power law equation to estimate raindrop terminal velocity 1 v d a 0 d a 1 in which d is expressed in cm and v in m s 1 and a0 a1 are coefficients equal to 17 67 and 0 67 respectively atlas and ulbrich 1977 also demonstrated that a power law provides a close fit to the data of gunn and kinzer 1949 in the range 0 05 d 0 50 cm the power law proposed by sekhon and srivastava 1971 substituting in eq 1 the values of a0 14 2 and a1 0 5 allows to have a reasonable accuracy of terminal velocity estimate for drops in the range 0 1 0 3 cm but overestimates the fall velocity of drop 0 3 cm and smaller than 0 1 cm van boxel 1998 the power law relations proposed by several authors spilhaus 1948 kessler and wilks 1968 lui and orville 1969 sekhon and srivastava 1971 atlas and ulbrich 1977 grosh 1996 are widely applied even if according to van boxel 1998 can be mostly inaccurate for describing the variation of the terminal velocity with drop diameter best 1950 used measurements of terminal velocities carried out by davies 1942 and proposed the following exponential relationship 2 v d 9 43 1 exp d 17 7 1 147 where v d is the terminal velocity in m s 1 d is the drop diameter expressed in cm eq 2 adequately represents davies s experimental data for diameters ranging from 0 069 to 0 595 cm best 1950 other researchers uplinger 1981 leone and pica 1993 proposed the following exponential relationship relating terminal velocity raindrop diameter 3 v d α d exp β d in which the constants α and β are equal to 48 74 and 1 95 uplinger 1981 leone and pica 1993 using raindrop terminal velocity measurements carried out in laboratory by beard 1976 laws 1941 and gunn and kinzer 1949 estimated α 49 and β 2 taking into account that for d 0 10 cm the drop has a slow terminal velocity and small mass producing a negligible kinetic power and for d 0 55 cm the raindrops are unstable and tend to break up before reaching their terminal velocity pruppacher and pitter 1971 leone and pica 1993 considered only raindrops having a diameter varying in the range 0 10 0 55 cm cerro et al 1998 used eq 3 with the coefficients proposed by uplinger 1981 to terminal velocity estimates in spain and found that the measured values of raindrop terminal velocity were lower than those predicted by uplinger s eq 3 ferro 2001 suggested that for a raindrop having diameter d and falling with a velocity v d from a fall height h through a medium with density ρ and viscosity νc and having a velocity va this phenomenon can be expressed by the following functional relationship 4 f v d d h g ν c ρ v a 0 where f is the functional symbol and g is the acceleration due to gravity since eq 4 describes a physical phenomenon that is independent on the choice of the measurement units of the considered variables the π theorem of dimensional analysis can be applied for expressing eq 4 by four dimensionless groups π1 π2 π3 π4 5 φ π 1 π 2 π 3 π 4 0 where φ is a functional symbol choosing as independent variables from dimensional point of view g d and ρ the π1 grouping assumes the following form 6 π 1 g α d β ρ ε v d in which α β and ε are constant parameters substituting in the eq 6 the measurement units of g d and ρ the following relationship is obtained 7 0 m α s 2 α m β k g ε s 2 β m 4 ε m s 1 the α β and ε values can be determined solving the following system of equations 8a α β 4 ε 1 0 8b 2 α 2 ε 1 0 8c ε 0 the solution of the system α 1 2 β 1 2 ε 0 allows to deduce the following dimensionless group 9 π 1 v d g d using a similar procedure the other dimensionless groups are deduced 10 π 2 h d 11 π 3 ν c g 1 2 d 3 2 12 π 4 v a g 1 2 d 1 2 considering the eqs 11 and 12 the following relationship can be deduced 13 π 4 π 3 v a g 1 2 d 1 2 g 1 2 d 3 2 ν c v a d ν c in which νc is the cinematic viscosity finally the eq 5 can be expressed as follows 14 v d g d f h d v a d ν c in stagnant air condition va 0 eq 14 can be expressed using the π1 and π2 groups fig 1 shows the experimental rainfall velocity and raindrop measurements carried out by laws 1941 in the plot h d v d g d this figure highlights a clear similitude relation for all measurements carried out using h 8 m however despite the condition of the perfect similitude between the relationships π1 f π2 for h 8 m this relationship cannot be applied for the estimation of the natural raindrop terminal velocity for which h must be almost equal to 20 m ferro 2001 considering that the terminal velocity increases for d 0 56 cm until which becomes constant used the measurement carried out by many researchers laws 1941 gunn and kinzer 1949 blanchard 1950 beard 1976 epema and riezebos 1983 jayawardena and rezaur 2000 for deducing the following relationship fig 2 15 v d v max 1 exp a n d in which vmax m s 1 and an cm parameters vary with h m table 2 for calculating raindrop terminal velocity h 20 m vmax and an respectively equal to 9 5 m s 1 and 6 cm have to be applied this equation was used by carollo and ferro 2015 carollo et al 2016 and serio 2017 for estimating the kinetic power by raindrop size distribution dsd in italy based on the investigations of beard and pruppacher 1969 and beard 1976 van dijk et al 2002 used a cubic polynomial function to describe raindrop terminal velocity v m s 1 as function of d mm for drop size of 0 1 7 0 mm 16 v d 0 0000561 d 3 0 00912 d 2 0 503 d 0 254 angulo martínez et al 2016 using raindrop terminal velocity measurements carried out in spain by an optical spectro pluviometer found that eq 16 provided a better fit for smaller drop 0 01 d 0 07 cm whereas eq 1 provided a better fit for mid size drops 0 1 d 0 3 cm the uplinger 1981 equation performed similarly to eq 16 but it overestimated the raindrop terminal velocity values for the smallest drops in conclusion eq 15 is capable to reproduce the physical boundary condition for d 0 55 cm the raindrops are unstable and tend to break up and the agreement with the available measurements fig 2 states its best effectiveness in describing the drop velocity to drop size relationship 3 rain drop size distributions 3 1 types of dsd the raindrop size distribution dsd is an interesting rainfall property for its scientific implication as well as its engineering aspect assouline and mualem 1989 the dsd formation is not fully understood because of the random and complex nature of the physical phenomena that govern the aggregation and disaggregation of the raindrops however the knowledge of dsd is essential to test theoretical models and for estimating the rainfall kinetic energy which plays a fundamental role in soil sealing and erosion assouline and mualem 1989 the raindrop size distribution dsd is defined as the function stating the expected number of drops with diameters between d and d dd the study of the dsd formation and evolution allowed to distinguish two different raindrop size distributions referred to unitary volume of air or to unit area and time the distribution nv d dd represents the number of raindrops with diameters between d and d dd cm per unit volume of air m3 in which d is expressed in cm and volume in m3 and the units of nv d become cm 1 m 3 according to this definition the dsd refers to the spatial distribution of raindrops in the air which governs the raindrop concentration and to the probability distribution of raindrop sizes in the air the second form of the raindrop size distribution n d dd represents the expected number of raindrops with diameters between d and d dd cm arriving at a surface per unit area m2 and per unit time s where d is expressed in cm area in m2 and time in s and so the units n d become cm 1 m 2 s 1 for meteorological studies the size distribution of raindrops commonly refers to the number of drops present in unit volume of air nv d dd while for hydrological studies the dsd usually refers to the number of droplets n d dd that reaches a unit horizontal area during a unit time uijlenhoet and stricker 1999 suggest that nv d yields to know the static properties of the raindrop population i e the spatial size distribution of the raindrops in a volume of air which governs the raindrop concentration and the probability distribution of their sizes in addition to these static properties n d involves the dynamic properties of the raindrop population as well its velocity distribution it should be noted that since the study of the dsd has for the most part been the work of the meteorologists they considered the number of drops present in unit volume of air nv d dd even though n d dd is the distribution which is actually measured in most cases uijlenhoet and stricker 1999 considering a raindrop constituted by drops that fall freely in air i e neglecting the effects of wind turbulence and raindrop interaction the two size distributions are related by the following relationship austin 1987 hall and calder 1993 uijlenhoet and stricker 1999 carollo and ferro 2015 17 n d d d v d n v d d d in which v d is the terminal velocity of the drop having a diameter d for studying accurately the energetic characteristics of raindrops that reach the ground area the number n d dd of raindrops with diameters between d and d dd arriving at a unit area per unit time has to be determined measurements of raindrop size distributions at the ground carried out with different techniques i e photographic method acoustic impact and optical disdrometer micro rain radar suggested that averaged raindrop size distributions can generally be parameterized as unimodal positively skewed distributions characterized by few parameters uijlenhoet and stricker 1999 the most widely applied analytical forms of the raindrop size parameterization are lognormal ln weibull w exponential mp and gamma u distribution 3 2 the lognormal distribution ln several authors mueller and sims 1966 1967 levin 1971 bradley and stow 1974 markowitz 1976 bezdek and solomon 1983 feingold and levin 1986 ochou et al 2007 proposed a lognormal distribution ln for describing the rain drop size distribution nv d in an unit volume of air in different climatic conditions 18 p d n v d n t 1 2 π ln σ d d exp ln 2 d d m 2 ln 2 σ d in which p d is the probability density function of the drop diameter d nt is the total number of the drops per unit volume of air dm and σ d represent the geometric mean diameter cm and the standard deviation of the raindrop diameter cm respectively according to feingold and levin 1986 the parameters of lognormal distribution can be calculated by the following relationships 19 n t 0 n d d d 20 ln d m ln d 21 ln 2 σ d ln d ln d m 2 in which ln d is the arithmetic mean of the natural logarithm of the diameter d and ln d ln d m 2 is the mean of the variable ln d ln d m 2 notice that for large values of σ d and small values of dm the lognormal function has the concave upward shape in much the same way that the gamma distribution does when μ 0 feingold and levin 1986 the rainfall intensity can be estimated by the following equation feingold and levin 1986 22 i 7 1 10 3 n t d m 3 67 exp 6 73 ln 2 σ d the median volume diameter d0 that is the diameter that divides the distribution in two parts of equal volume is given by feingold and levin 1986 23 d 0 d m exp 3 ln 2 σ d which underlines that d0 is a scale and shape parameter as it depends on dm and σ d feingold and levin 1986 using dsd measurements carried out at hadera in israel by an optical disdrometer found that i although the value of σ d varies considerably for low rain rate for i 5 mm h is quasi constant and equal to 1 43 this result implies that the ratio d0 dm kd is constant and equal to 1 47 according to markowitz 1976 using laws and parsons data for 2 5 i 152 4 mm h the σ d parameter remains constant and equal to 1 39 ii nt increases with rainfall intensity as follows 24 n t 172 i 0 22 in agreement with gagin 1980 that proposed an increase nt with i for convective clouds in israel iii dm parameter increases with i according to the equation 25 d m 0 75 i 0 21 feingold and levin 1986 found that lognormal distribution can be fitted to observed dsds in israel better than gamma or exponential distribution they reported also that the lognormal distribution has the advantage that the statistical parameters dm σ d and nt have a physical meaning in particular the variations in the size parameter dm are indicative of the relative importance of coalescence and breakup processes and the parameter σ d reflects the effect that these processes have on the breadth of the spectrum recently ochou et al 2007 fitted the lognormal distribution to dsds detected at abidjan ivory coast dakar sénégal niamey niger and boyélé congo using jwd disdrometer the authors suggested that this distribution is preferred because its parameters have a simple geometric interpretation and the moments can be written in the form of a product of three terms each of them being a function of only one of the three parameters at date at the best of our knowledge no relationship to estimate the rainfall kinetic power has been deduced for the lognormal distribution 3 3 the weibull distribution w langmuir 1948 considered the raindrop formation as a chain reaction process in which raindrops seem to grow up to a point where turbulence raindrop collision or coalescence cause breakup into two or smaller drops which in turn repeat the same process weibull 1951 described this phenomenon assuming to have a chain consisting of n links the chain as a whole fail if any one of its links fails therefore denoting as p f the probability of failure of the chain the nonfailure probability of the chain 1 p f is equal to the probability of the simultaneous nonfailure of all the links weibull 1951 thus 26 p f 1 exp n f x where f x is the function of any load x applied to a single link the analogy of the chain with raindrop is due to the break up mechanism in other words the probability of break up of a chain constituted by n links is equal to the probability of raindrop break up into n parts of it sekine and lind 1982 stated that if in a unit volume of air a drop breaks up and a series of chain reaction processes occur the probability p d that raindrop diameter is less than d can be written as 27 1 0 d p d d d 1 p d exp d ω v δ v where p d is the probability density function of diameter d δv is the shape parameter responsible for the skew of the distribution and ωv mm is the scale parameter the weibull shape parameter δv is a pure number i e dimensionless and is also named weibull slope different values of the shape parameter can have marked effects on the behavior of the distribution when δv 1 the p d of the weibull distribution reduces to that of the exponential law proposed by marshall and palmer 1948 when δv 2 the p d of the weibull distribution reduces to rayleig distribution cohen 1965 changing thescale parameter ωv it does stretch out the existing shape in particular with δv constant the increase of ωv yields to have the distribution stretched to the right in the graph instead decreasing ωv results in the graph being shrunk to the left towards zero applying logarithms to both members of eq 27 it follows 28 ln ln 1 p d δ v ln d ln ω v assuming nv d that is the number of the raindrops in an unit volume of air equal to p d a linear equation is obtained to present the parameters of the weibull distribution 29 x ln d 30 y ln ln 1 0 d n v d d d by differentiating eq 27 and considering the total number of the drops nwv the raindrop size distribution nv d is obtained sekine and lind 1982 31 n v d n wv δ v ω v d ω v δ v 1 exp d ω v δ v where nwv m 3 δ v and ω v mm are the parameters of the distribution sekine and lind 1982 fitting eq 31 to the raindrop size distributions measured by laws and parsons 1943 in washington sander 1975 in berlin and wickerts 1982 in stockholm found that 32 n wv 1000 m 3 while δ v and ω v parameters are dependent on i according to the following relationships 33 δ v 0 95 i 0 14 34 ω v 0 26 i 0 44 jiang et al 1997 using raindrop size distributions detected by a disdrometer installed in tokyo positively verified the reliability of the weibull distribution and suggested that this theoretical probability distribution as the ulbrich one presents a peak that tends to shift toward large raindrop diameter with increasing rainfall intensity jiang et al 1997 comparing the u mp and w distributions with the experimental data found that both u and w distribution give better fit to data than mp one assouline and mualem 1989 suggested a new approach for studying the mechanism of the forces that shape the dsds through coalescence and breakup of the falling drops along their pathway from the cloud to the soil surface the authors hypothesized that the result of coalescence and breakup processes is equivalent to a series of fragmentations that are random in nature considering the assumptions that i the probability for a drop break up is proportional to its volume ii the probability that a break up of a drop of size w yields drops of size v is independent of v iii v is uniquely related to raindrop diameter assouline and mualem 1989 proposed the following probability function 35 f d i 1 exp r d n where n and r are the two parameters of the weibull distribution which are estimated by the following equations 36 d g r 1 n γ 1 1 n 37 σ 2 r 2 n γ 1 2 n γ 2 1 1 n where γ is the gamma function using dg as the scaling factor the following scaled diameter d is obtained 38 d d d g d r 1 n γ 1 1 n from eq 38 it follows 39 γ n 1 1 n d n r d n substituting eq 39 into eq 35 with 40 γ γ n 1 1 n the following dimensionless one parameter distribution function is obtained 41 f d 1 exp γ d n the power n 3 of the diameter d derives from the assumption that fragmentation probability is related to drop volume if the theoretical power n 3 is adopted in eqs 40 and 41 considering the basing assumption which relates fragmentation probability to the volume of the drop assouline and mualem 1989 proposed the following relation 42 f d 1 exp 0 71 d 3 which is a probability distribution function f d that is independent of both intensity and site where precipitation occurs named universal distribution assouline and mualem 1989 in other words the introduction of a relative raindrop diameter eq 38 transformed the theoretical dsd into a universal relationship eq 42 assouline and mualem 1989 using raindrop size distributions measured by hudson 1965 in rhodesia and by carter et al 1974 at baton rouge and holly springs observed that the measured distributions at each site are overlapped to the universal distribution eq 42 this circumstance confirms that the use of the scaled diameter d involves that eq 42 is not site specific assouline 2009 using dsds measurements carried out at princeton new jersey by smith et al 2009 found that eq 42 overestimates the probability function for small d values d 2 jiang et al 1997 using dsd measurements carried out in tokyo suggested that for i 1 88 mm h i 14 32 mm h i 46 40 mm h the weibull distribution agrees with the experimental data better than the u and mp ones instead the marshall and palmer distribution gives a good fit to the experimental data only for the lowest rainfall intensity value i 1 88 mm h at date at the best of our knowledge no relationship to estimate the rainfall kinetic power has been deduced for the weibull distribution 3 4 the marshall and palmer distribution mp marshall and palmer 1948 proposed a simple negative exponential parameterization mp for the raindrop size distribution as a fit to filter paper measurements of the dsds for rainfall intensities between 1 and 23 mm h uijlenhoet and stricker 1999 43 n v d n 0 v e x p λ v d where nv d expressed in m 3 cm 1 is equal to the number of raindrops per unit volume of air and per unit size interval having equivolume spherical diameter d cm the parameters of the exponential distribution n0 v m 3 cm 1 and λv cm 1 represent the total number of the raindrops and the scale parameter respectively marshall and palmer 1948 suggested that n0 v has a constant value equal to 8 104 m 3 cm 1 and λv varies with the rainfall rate i mm h 1 according to the following power law 44 λ v 4 1 i 0 21 ulbrich 1983 carrying out an analysis of drop size spectra of laws and parsons 1943 revealed that these data can be also represented by the marshall and palmer distribution but n0 v varies with rainfall intensity many other researchers marshall and palmer 1948 waldvogel 1974 carbone and nelson 1978 joss and gori 1978 suggested that the exponential distribution is a very good approximation to the raindrop size distribution referred to natural rainfall several authors gunn and marshall 1955 mueller 1966 waldvogel 1974 joss and gori 1978 sempere torres et al 1994 also pointed out that the data analyzed by marshall and palmer 1948 failed to include the smaller raindrops 1 mm and in particular this dsd tends to overestimate the number of both smallest drops gunn and marshall 1955 mueller 1966 waldvogel 1974 sempere torres et al 1994 and largest ones joss and gori 1978 sempere torres et al 1994 ochou et al 2007 carrying out an analysis about dsd types suggested that the mp distribution is widely used for describing the mid latitude dsds that are characterized by low to moderate intensity instead for tropical latitudes where mean rainfall intensities is 6 to 7 times higher than at mid latitudes sauvageot 1994 eq 43 is not able to give a good dsd characterization it was also observed that for i higher than 10 20 mm h 1 and for d larger than the modal value 0 1 0 2 cm the λ parameter becomes almost constant pasqualucci 1982 sauvageot and lacaux 1995 in contrast with eq 44 the constant slope observed in heavy tropical rain expresses the evolution of the dsd towards an equilibrium form in which the shape of dsds does not change any more ochou et al 2007 3 5 the ulbrich distribution u the gamma distribution u proposed by ulbrich 1983 which is a generalization of the mp one has been applied by several authors for describing dsd measurements carried out using different measurement techniques filter paper photoelectric spectrometer drop camera doppler radar optical disdrometer blanchard 1953 dingle and hardy 1962 muller 1965 caton 1966 joss and gori 1978 gori and geotis 1981 chandrasekar and bringi 1987 tokay and short 1996 ulbrich and atlas 1998 brawn and upton 2008 carollo and ferro 2015 carollo et al 2016 ulbrich s distribution is largely used because of its appropriate form for characterizing the distribution of the droplets in clouds khrgian et al 1952 aerosols levin 1961 and precipitation particles sulakvelidze 1969 sulakvelidze and dadali 1971 this theoretical distribution has the following expression 45 n v d d d n 0 v d μ v e x p λ v d d d in which μv λv and n0 v are the statistical parameters of the distribution in particular the exponent μv represents the shape parameter and it can assume positive or negative values fig 3 for positive values of μ v the dsd is concave downward has a narrow breadth and falls rapidly to zero as d 0 for negative values of μv the dsd is concave upward on semi logarithmic plot and has large breadth with increasing numbers of drops at both small and large diameters ulbrich 1983 the n0 v parameter is expressed as m 3 cm 1 μ and λv is a scale parameter for μv equal to zero eq 45 coincides with the marshall palmer distribution in agreement with uijlenhoet and stricker 1999 carollo and ferro 2015 found that combining eq 45 eq 17 and eq 1 the following relationship can be obtained 46 n d d d 17 67 d 0 67 n 0 v d μ v exp λ v d d d eq 46 can be rewritten in the following form 47 n d d d n 0 d μ exp λ d d d where n0 17 67 n0 v μ μv 0 67 and λ λv eq 47 having the same mathematical shape of eq 45 is also reliable for estimating drop size distribution per unit area and unit time ulbrich 1983 demonstrated that the median volume diameter d0 that is the diameter that divides the distribution in two part of equal volume can be expressed uniquely in term of λ for μ 0 the relationship between λ d0 and dmax i e the maximum diameter of the dsd is determined by the analytical definition of d0 as 48 2 0 d 0 d 3 n d d d 0 d max d 3 n d d d this relationship can be rewritten as follows 49 2 γ 4 μ λ d 0 γ 4 μ λ d max where γ is the gamma function fig 4 shows the relationship between λd0 and λdmax as defined by eq 49 for values of 2 μ 3 it highlights that the value of λdmax at which λd0 is close to its limiting value depends on μ ulbrich 1983 found that λd0 according to eq 49 can be assumed approximately equal to 50 λ d 0 3 67 μ which is accurate to within 0 5 for all μ 3 for rhodesian precipitations hudson 1971 represented the relationship between d 0 and rainfall intensity by a curve that presents a maximum point for i 80 mm h even if d 0 values are characterized by a low variability 2 0 d0 2 5 mm in the explored range of i 25 i 200 mm h carter et al 1974 aggregated the dsds relieved in louisiana and mississippi for 13 intensity classes and found that d 0 does not increase for rainfall intensities greater than about 65 mm h many other researches laws and parsons 1943 atlas 1953 kelkar 1959 zanchi and torri 1980 brandt 1988 jau yau et al 2008 proposed a power law for describing the relationship d 0 i implying that d 0 continues to increase indefinitely with i this last result is in contrast with other researches which stated that a maximum median volume diameter value is reached at high rainfall intensities usually above 70 100 mm h after which the d 0 becomes constant kinnell 1981 rosewell 1986 brown and foster 1987 carollo et al 2016 or even decreases hudson 1963 baruah 1973 carter et al 1974 van dijk et al 2002 when the parameters d0 and μ are known λ parameter can be estimated by the following equation 51 λ 3 67 μ d 0 rainfall intensity i mm h 1 can be calculated from the following expression salles et al 2002 carollo and ferro 2015 52 i 3 6 π 6 0 d 3 n d d d 3 6 π 6 n 0 0 d 3 μ exp λ d d d where d is expressed in cm and n d dd in m 2 s 1 taking into account that olver 1997 53 0 d 3 μ exp λ d d d γ 4 μ λ 4 μ eq 52 becomes 54 i 3 6 π 6 n 0 γ 4 μ λ 4 μ eq 54 links the dsd parameters to rainfall intensity and allows to calculate n 0 parameter as function of i and of the other two dsd parameters 55 n 0 λ 4 μ 3 6 π 6 γ 4 μ i the median drop diameter d 50 and the median volume drop diameter d 0 considering the ulbrich s distribution can be approximately calculated by the following relationships ulbrich 1983 uijlenhoet and stricker 1999 carollo et al 2016 serio 2017 56 d 50 0 67 μ λ 57 d 0 3 67 μ λ from eqs 56 and 57 the two ulbrich s parameters can be deduced 58 μ d 0 λ 3 67 59 λ 3 d 0 d 50 carollo and ferro 2015 using 23 967 dsds measured in sicily in the period june 2006 february 2012 verified the reliability of the ulbrich s distribution in sicilian environments estimating μ and λ by both maximum likelihood method ml and momentum method mm imposing the coincidence between the mean and standard deviation of the measured distribution with the theoretical statistical parameters one fig 5 shows as an example for three rainfall intensity values 9 5 52 5 and 124 5 mm h 1 the fitting of the gamma distribution u to the dsds detected in sicily this figure demonstrates the reliability of the ulbrich s distribution in the sicilian environment and its ability to reproduce the presence of large raindrops corresponding to high p d values carollo and ferro 2015 found that the parameter μ varies from 0 85 to 76 9 is generally positive 23756 dsd of the 23 967 measured 160 ones and assumes a median value of about 5 while parameter λ ranges from 3 3 to 591 5 both parameters slightly decrease as rainfall intensity increases and in agreement with results of other authors zhang et al 2003 brawn and upton 2008 are correlated 4 rainfall kinetic power relationships 4 1 rainfall kinetic power intensity relationships rainfall kinetic power pn j m2 h can be calculated by adding the contribution of each raindrop once its mass and terminal velocity are known therefore the energy characteristics of a rainfall event can be indirectly measured if its dsd and the relationship between terminal drop velocity and drop diameter are known at date many researchers proposed empirical relationships linking kinetic power to rainfall intensity having different mathematical forms polynomial exponential logarithmical and power type table 1 the most commonly used relationship for estimating pn as function of rainfall intensity is that proposed by wischmeier and smith 1978 and used in the universal soil loss equation usle according to wischmeier and smith 1978 the event rainfall runoff erosivity factor ei30 is the result of the product of the total rainfall kinetic energy of the rainstorm per unit area e mj ha 1 and i30 mm h 1 which is the maximum 30 min rainfall intensity e is due to the product of the rainfall kinetic energy per unit volume of rainfall pn i j m 2 mm 1 and the rainfall depth referred to each raining period for estimating pn i wischmeier and smith 1978 proposed the following relationship having a logarithmic form based on the measurements of drop size distribution and terminal velocity observed at washington dc by laws and parsons 1943 60 p n i 11 9 8 73 log i for i i t 11 9 8 73 log i t for i i t in which it is the intensity threshold value that according to wischmeier and smith 1978 is equal to 76 mm h 1 according to eq 60 the ratio pn i increases for rainfall intensity value less than or equal to it and then it becomes constant for rainfall intensity greater than it wischmeier and smith 1978 justified this threshold value suggesting that the median volume diameter d0 does not continue to increase when rainfall intensities exceed 76 mm h 1 for describing the same trend kinnell 1981 proposed the following relationship 61 p n i a 1 b e x p c i where a b and c are parameters according to eq 61 pn i has a finite positive value at zero intensity and approaches as well as eq 60 to an asymptotic value pn i a at high intensities brown and foster 1987 using data from brisbane and gunnedah australia holly springs mississippi miami florida washington and zimbabwe estimated a 29 j m 2 mm 1 b 0 72 and c 0 05 h mm 1 the asymptotic value a is practically coincident with the one pn i 28 3 j m 2 mm 1 obtained by eq 61 for i 76 mm h eq 61 with the parameter values proposed by brown and foster 1987 is used in revised universal soil loss equation rusle renard et al 1997 in rusle2 equation foster 2004 suggested eq 61 for estimating rainfall kinetic power with the parameter a b and c equal to 29 j m 2 mm 1 0 72 and 0 082 h mm 1 respectively mcgregor et al 1995 kinnell 1987 reported that a parameter can be assumed equal to 29 j m 2 mm 1 while b and c parameters are site specific salles et al 2002 however other researchers proposed different values of a b and c parameters confirming their dependence on geographical location table 3 mcgregor et al 1995 comparing eqs 60 and 61 verified that for 1 mm h i 35 mm h pn i values estimated by eq 61 with the coefficient values suggested by brown and foster 1987 are 12 less than those predicted by eq 60 they concluded that the annual erosivity predicted by eq 61 with parameter values suggested by mcgregor and mutchler 1976 and eq 60 were almost identical whereas pn i values predicted by eq 61 with the parameter values proposed by brown and foster 1987 were about 8 lower yin et al 2017 the underestimation of the pn i in the rusle model was also reported in australia yu 1998a b belgium verstraeten and poesen 2006 and peninsular malaysia yin et al 2017 fornis et al 2005 using disdrometric measurements carried out in central cebu philippine calibrated both eq 60 and eq 61 they verified that the calibrated exponential relationship eq 61 a 30 68 b 0 05 and c 0 03 is more reliable to predict the kinetic power per unit volume of rainfall than the equation proposed by wischmeier and smith lim et al 2015 carrying out an analysis on rainfall kinetic power by raindrop size distribution measured in korea suggested that as i increases the pn i value also increases until i is equal to 60 mm h and then it becomes constant the calibrated exponential relationship proposed by kinnell 1981 eq 61 was able to predict the kinetic power per unit volume of rainfall in particular they stated that for i 30 mm h the exponential model fitted the observed data well but it heavily underestimated the pn i values for i 60 mm h lobo and bonilla 2015 developing a sensitivity analysis of kinetic energy intensity relationships on rainfall erosivity using rainfall intensity values measured in central chile suggested that i eq 60 provides statistically equivalent erosivity values to those that are computed by eq 61 with parameter values suggested by mcgregor and mutchler 1976 ii eq 60 predicts larger erosivity estimates than eq 61 for i 5 5 mm h which is the maximum measured rainfall intensity in central chile finally authors concluded that eq 60 and eq 61 are highly sensitive to change in its regression parameters thus these equations should be calibrated based on local precipitation data to generate reliable erosivity estimates ciaccioni et al 2016 carried out an analysis on rainfall erosivity in slovenia using disdrometer data in particular they concluded that i at high values of i equations from coutinho and tomás 1995 and cerro et al 1998 table 1 tend to slightly overestimate the measured kinetic power ii the use of the equations suggested by wischmeier and smith 1978 brandt 1990 usòn and ramos 2001 and van dijk et al 2002 table 1 tend to underestimate the rainfall kinetic power therefore ciaccioni et al 2016 concluded that a general equation relating pn and i is not suitable to estimate correctly the kinetic power of the precipitation but equations as eq 61 need to be calibrated using at site data angulo martínez et al 2016 using dsds detected in spain found that the relationship between pn i and i is asymptotic and in particular pn i becomes nearly constant at an i value of about 65 mm h these authors verified the reliability of eq 61 for pn i estimates using parameter values proposed by brown and foster 1987 angulo martínez et al 2016 concluded that none of these empirical expressions can be used universally since their applicability depends on the geographical and meteorological conditions of investigated sites carollo et al 2017 using dsd detected at palermo sicily in the period june 2006 march 2014 and aggregated in intensity classes found that the wischmeier and smith 1978 relationship is fully applicable for estimating the rainfall kinetic power in sicilian environment in fact eq 60 is able to reproduce the pn i values measured at palermo experimental site for i 40 mm h even if it determines a slight overestimate for 2 i 15 mm h 1 and systematically overestimates the kinetic power for i 40 mm h 1 however this last overestimation can be avoid setting it in eq 60 equal to 40 mm h the median volume diameter d0 and pn i values measured at palermo show an increasing trend with rainfall intensity for i 40 mm h after which does not more increase with i carollo et al 2017 these results represent a further confirmation of the applicability of the wischmeier and smith 1978 approach even if the threshold value of rainfall intensity it 40 mm h resulted less than the one proposed by wischmeier and smith 1978 it 76 mm h carollo et al 2017 carollo et al 2017 also tested the reliability of the eq 61 for estimating pn i at palermo in particular they found that eq 61 with the parameters values proposed by brown and foster 1987 for i 30 mm h 1 gives values slightly greater than the ones obtained by eq 60 instead eq 61 underestimates systematically pn i especially for the lowest values of i i 20 mm h however in the whole range of i the shape of the curve corresponding to eq 61 describes the pn i i relationship better than eq 60 even if eq 61 needs to be calibrated other researchers park et al 1980 smith and de veaux 1992 uijlenhoet and stricker 1999 steiner and smith 2000 sanchez moreno et al 2012 verified the reliability of a power law relationship for which the ratio pn i does not become constant for any value of i in other words for high values of rainfall intensity a power law relationship implies a trend different from eq 60 and eq 61 sanchez moreno et al 2012 suggested that the power law equation seems to represent better than eq 60 and eq 61 the kinetic power of the rainfall in cape verde characterized by a sequence of extreme short events with high intensities salles et al 2002 carrying out an overview of many empirical relationships pn i showed that for a fixed rainfall intensity these relationships yield to very different values of kinetic power using measurements of rainfall intensity drop size distribution and drop size specific fall velocity carried out by optical laser disdrometers in five experimental stations in germany wilken et al 2018 suggested that the commonly used empirical pn i relationships table 1 overestimate the measured rainfall kinetic power furthermore in agreement to van dijk et al 2002 and meshesha et al 2014 carollo et al 2018 stated that there is no a single empirical pn i equation that can be recommended for any part of the world according to parsons and gadian 2000 salles et al 2002 concluded that a global parameter as rainfall intensity i or median volume diameter d0 is not sufficient to determine the rainfall erosivity since that kinetic power measurements are also dependent on other parameters rain type altitude climate and method of measurement 4 2 deducing rainfall kinetic power intensity relationship by dsd the kinetic power pn j m2 h knowing both dsd and terminal velocity can be calculated as salles et al 2002 62 p n 10 6 ρ π 12 0 v d 2 d 3 n d d d where ρ is the water density kg m 3 v d is the terminal velocity of the drop having a diameter d m s 1 and n d dd is the number of raindrops with diameters between d and d dd arriving at a unit area per unit time the use of eq 62 requires the knowledge of the raindrop size distribution at date at the best of our knowledge no relationship to rainfall kinetic power estimate has been deduced for the weibull and lognormal distribution applying the gamma distribution ulbrich 1983 raindrop terminal velocity estimated by eq 15 ferro 2001 and using eq 62 carollo and ferro 2015 deduced theoretically the following expression 63 p n i 10 6 9 5 2 7 2 ρ λ 4 μ 1 λ 4 μ 2 6 λ 4 μ 1 12 λ 4 μ according to eq 63 the rainfall kinetic power can be determined if both the rainfall intensity and μ and λ parameters of the dsd are known this circumstance allows to conclude that the ratio pn i depends on the intrinsic characteristics of rain carollo and ferro 2015 using approximately 24 000 dsds detected by an optical disdrometer at palermo sicily in the period june 2006 february 2012 showed that i eq 63 reproduces adequately the kinetic power measurements specially using μ and λ values obtained by momentum method ii eq 60 is fully applicable to rainfall recorded in sicily and has to be preferred to a power relationship calibrated on collected data that tends to overestimate the rainfall kinetic power for high values of i iii eq 61 yields appreciable underestimates of pn for low values of rainfall intensity carollo et al 2016 taking into account that the marshall and palmer 1948 distribution per unit time and area can be assumed formally identical to the ulbrich 1983 one by combining eq 57 and eq 63 with μ 0 67 obtained the following theoretical relationship 64 p n i 10 6 9 5 2 7 2 ρ 1 2 6 d 0 4 34 1 4 67 1 6 d 0 2 17 1 4 67 which clearly shows that the ratio pn i depends only on median volume diameter according to eq 64 if d0 assumes a constant value when i exceeded a threshold value pn i becomes constant in other words eq 64 represents a theoretical confirmation of the wischmeier and smith 1978 hypothesis finally carollo et al 2018 presented the comparison between the pairs i pn i and i d0 corresponding to palermo dataset with the ones obtained by different measurement techniques drop camera piezoelectric force transducer blotting paper in other sites of the world marshall islands new jersey alaska indonesia oregon franklin hong kong ethiopia spain they concluded that the relationships pn i i and d0 i are site specific and therefore a single like eq 60 or eq 61 is not reliable for estimating rainfall kinetic power at any site fig 6 on the contrary the experimental pairs d0 pn i relative to all available datasets resulted near to a single increasing curve that can be described by eq 64 fig 6 this theoretically derived relationship resulted fully applicable to all available datasets demonstrating that pn i d0 relationship is free from at site effects and useful to adequately characterize the rainfall erosivity serio 2017 in other words for a given rainfall intensity kinetic power depends on the raindrop size distribution reaching soil surface and the median volume diameter is an useful characteristic diameter able to synthesize whole dsd information 5 seasonal effects on precipitation the predicted soil loss is affected by the interaction between the erosive power of rainfall and the soil protection action of vegetative canopy crop canopy which is the aboveground part of vegetation that intercepts rainfall varies during the year with natural growth processes maximum erosion control is obtained using a crop having the maximum canopy during periods of high rainfall erosivity toy et al 2002 soil loss is greatest when the peak period of rainfall erosivity corresponds to the period when the soil is most exposed to raindrop impact at date very few studies have been carried out in order to understand how the seasonality could influence the raindrop formation alonge and afullo 2012 hachani et al 2017 and evolution during its fall 5 1 influence of seasonality on dsd in literature alonge and afullo 2012 carrying out a seasonal analysis using dsd measured at durban south africa suggested that seasonality influences raindrop formation in particular this analysis highlighted that for rainfall intensity i up to 10 mm h the shape of the raindrop size probability density function in durban south africa may be similar for summer autumn and spring seasons however for i 10 mm h winter season presents a raindrop size probability density function different from the other seasons having larger raindrop size alonge and afullo 2012 verified also the reliability of the ln w and u statistical models establishing that a different probabilistic model should be used for each season in particular the lognormal distribution best fits the summer and autumn the ulbrich distribution fits winter and weibull distribution fits spring season the analysis also shows that the highest number of the rain drops per unit area and time occurs in summer while the winter is characterized by the lowest number of raindrops the authors also highlighted the existence of seasonality respect to dm and σ d for the weibull model referred to raindrop size distribution per unit area and time the values of the shape parameter δ resulted similar for the seasons of summer autumn and spring in addition the scale parameter ω for the seasons of autumn and spring are also found to be close instead winter season has ω values higher than summer season for the ulbrich distribution all seasons are observed to take different values of the scaling parameter n0 however the values of the λ parameter estimated for summer and autumn are the closest alonge and afullo 2012 hachani et al 2017 used dsds detected at the cévennes vivarais region france by six ott parsivel optical disdrometers to investigate the seasonal influence on rain drop size distributions at first the ulbrich raindrop size distribution was fitted to measured dsds the analysis highlighted that the shape parameter is almost identical in spring and autumn the number of drops was most similar in autumn and winter the spring and autumn seasons present hybrid characteristics between those of winter and summer the intra annual variability of rainfall erosivity can be high panagos et al 2016a and this information can be useful for identifying the high risk periods in which soil exposure to rainfall coincides with occurring high erosive events to date no study has been carried out to know if the seasonality of drop size distribution could have any influence on rainfall erosivity i e on the energetic characteristics of rainfall that reaches soil surface 5 2 new findings seasonality on dsds detected at palermo experimental area in this section an analysis of the seasonality effects on characteristics of dsds that reach an unit area in an unit time and on erosive power of the precipitation at palermo italy is presented measurements of drop size distributions were carried out using an optical disdrometer placed at the experimental area equipped near the department of agricultural food and forestry sciences of the university of palermo fig 7 this disdrometer model odm 470 made by eigenbrodt measures drop diameters in the range 0 05 0 6 cm each drop is separately measured and registered into classes of about 0 005 cm width drop diameter is measured by registering light damping due to the passage of the drop in the control volume between two diodes fig 7 more details about construction and functioning features of disdrometer are reported in grossklaus et al 1998 and carollo and ferro 2015 the disdrometer registered 524 rainfall events in the period june 2 2006 march 10 2014 for each rainfall event only the dsds for which the rainfall intensity was 0 5 mm h and measured diameter classes were at least 20 were considered this choice excluded both rainfalls having low erosive power and dsds having a small sample size this procedure provided 42 264 dsds with a sampling time of 1 min in order to better focus the influence of seasonality and rainfall intensity on both dsd and rainfall energetic characteristics laws and parsons 1943 carter et al 1974 sauvageot and lacaux 1995 jayawardena and rezaur 2000 the 42 264 dsds named instantaneous dsds were firstly distinguished in spring april may june summer july august september autumn october november and winter december january february march and then aggregated in intensity classes having width equal to 1 mm h the rainfall distribution during the year is characterized by the highest values of monthly rainfall in autumn and winter wet period and the lowest values in spring and summer dry period in particular the mean annual value of the rainfall depth at palermo site is equal to 410 mm in the winter period while in autumn spring and summer 220 mm 120 mm and 100 mm occur respectively for each class the rainfall intensity was calculated as average of the intensities of the instantaneous dsds falling into the class this procedure allows to have number of aggregated dsds named dsds equal to 27 dsds for spring 62 for summer 79 for autumn and 74 for winter table 4 reports some characteristic data of the erosive events referred to each season fig 8 shows the number of raindrops falling in each intensity class versus mean rainfall intensity referred to each class in general for each season the number of the raindrops increases for very low values of i i 5 mm h fig 8 a while for 5 i 40 mm h fig 8 a the number of the raindrops decreases as rainfall intensity increases for winter and autumn seasons the number of the raindrops becomes constant for i 100 mm h fig 8 b the ulbrich s distribution was fitted to each dsds using the maximum likelihood method to estimate μ and λ parameters the frequency distribution of μ and λ parameters referred to each season datasets are calculated and reported in fig 9 in particular the frequency distributions of the shape parameter μ are found to be closer for summer spring and autumn rains than the winter one in fact in agreement to hachani et al 2017 the values of statistical indices mean median standard deviation and coefficient of variation table 5 are closest and suggest that a similarity in dsd shape in these seasons exists fig 9 b shows the frequency distributions of the dsd slope parameter λ referred to each season in winter the dsds are characterized by the highest values of λ parameter in addiction from spring to winter the raindrop size distributions present an increasing slope parameter value each dsd is characterized by a pair μ λ thus the possible influence of the seasonality on the μ and λ relationship has to be investigated fig 10 shows the pairs μ λ relative to precipitations occurred in each season a single relationship between μ and λ parameters can be observed for spring summer and autumn seasons which are also characterized by the nearest values of the total rainfall depth while in winter season the precipitations characterized by i 40 mm h present a μ λ relationship different from those with i 40 mm h fig 10 in addition the pairs μ λ relative to winter precipitations with i 40 mm h are quasi overlapped to spring summer and autumn ones suggesting that a dsd similarity exists therefore these results suggest that for i 40 mm h winter dsds are different from the other season ones and within winter season dsds characterized by i 40 mm h are different from those with i 40 mm h for each dsds the median volume diameter value was also determined fig 11 shows the pairs i d0 referred to each season the measurements suggest that for all rainfall intensity classes the pairs i d0 are overlapped and thus the relationship between d0 and i is not seasonally dependent fig 11 for i 40 mm h the median volume diameter shows an increasing trend with i and a single power law can be fitted fig 11 a instead for i 40 mm h d0 does not depend on rainfall intensity and can be considered quasi constant and equal to 0 25 cm fig 11 b for each season the kinetic power per unit volume of rainfall pn i by dsds was also determined by associating each diameter with the terminal velocity estimated by eq 15 with vmax and an respectively equal to 9 5 m s 1 and 6 cm fig 12 shows the comparison between pn i values versus rainfall intensity of aggregated dsds for i 40 mm h a and for i 40 mm h b in different seasons the pairs i pn i are overlapped highlighting that no season dependence of the relationship between pn i and i exists for the precipitation registered at palermo the pn i values measured at palermo show an increasing trend with rainfall intensity for i 40 mm h fig 12 a after which becomes constant pn i 26 j m2 mm fig 12 b this performance is similar to d0 i one these results represent a further confirmation of the applicability of the wischmeier and smith 1978 approach even if the threshold value of rainfall intensity it 40 mm h resulted less than the one proposed by wischmeier and smith 1978 it 76 mm h carollo et al 2017 6 conclusive remarks and research needs the rainfall kinetic power can be calculated by drop size distribution dsd measurements combined with fall velocity values measured or estimated by empirical relationship linking terminal fall velocity and drop diameter this paper presented the most commonly used relationship for raindrop terminal velocity estimate highlighting that the rainfall terminal velocity depends only on rainfall diameter and fall height the most widely used theoretical dsds were also reviewed highlighting the need to distinguish the raindrop size distribution per unit volume of air from that per unit area and time the raindrop size distribution per unit volume of air yields to know the static properties of the raindrop population i e the spatial size distribution of the raindrops in a volume of air which governs the raindrop concentration and the probability distribution of their sizes in addition to these static properties the raindrop size distribution per unit area and time involves the dynamic properties of the raindrop population as well its velocity distribution however if the effects of wind turbulence and raindrop interaction are neglected the two size distributions are related to date at the best of our knowledge no relationship to rainfall kinetic power estimate has been deduced for the weibull and lognormal distribution the reliability of the most commonly used relationships to estimate rainfall kinetic power in several part of the world was also reviewed highlighting that a single relationship pn i i is not applicable at any site furthermore a relationship relating pn i and i as those proposed by wischmeier and smith and by kinnell needs to be calibrated on data the theoretically derived relationship between pn i d0 from the marshall and palmer distribution per unit time and area suggested by carollo et al resulted fully applicable to all available datasets this result demonstrates that pn i d0 relationship is free from at site effects and useful to adequately characterize the rainfall erosivity this circumstance represents a theoretical confirmation of wischmeier and smith hypothesis which established that the trend of pn i depends strictly on the d0 one in other words the global parameter d0 is sufficient to determine the ratio pn i and it is a variable free from at site effects and useful to characterized rainfall erosivity finally an analysis on influence of seasonality in dsd and rainfall energetic characteristics at palermo experimental area was also carried out this investigation yields to conclude that the seasonality influences the raindrop formation only in winter season however the relationship between pn i and i is not season dependent as d0 and i too in other words at the best of our knowledge the erosive power of the precipitation does not depend on the season when the rainfall occurs therefore this paper highlights that future works on rainfall erosivity should be addressed to establish techniques able to give measurements of the raindrop terminal velocity to carry out contemporaneous measurements of drop size and terminal velocity by laser disdrometers to deduce relationships to estimate rainfall kinetic power by weibull and lognormal distributions to verify the reliability of the probability distribution function f d proposed by assouline and mualem to verify the theoretically derived relationship between pn i d0 suggested by carollo et al in different site of the world to find new models or techniques able to provide good estimates or measurements of the median volume diameter of the distribution to investigate the influence of the seasonality on both rain drop size distribution per unit area and time and rainfall erosivity in other site of the world to relate the seasonality of drop size distribution with the rain forming systems to investigate the effect of high resolution precipitation data on the calculation of the rinfall erosivity factor declaration of competing interest none acknowledgements all authors set up the research analyzed and interpreted the results and contributed to write the paper 
6289,in practice the rational operation rule derived from historical information and real time working condition can help the operators make the quasi optimal scheduling plan of hydropower reservoirs leading to significant improvements in the generation benefit as an emerging artificial intelligence method the extreme learning machine elm provides a new effective tool to derivate the reservoir operation rule however it is difficult for the standard elm method to avoid falling into local optima due to the random determination of both input hidden weights and hidden bias to enhance the elm performance this research develops a novel class based evolutionary extreme learning machine ceelm to determine the appropriate operation rule of hydropower reservoir in ceelm the k means clustering method is firstly adopted to divide all the influential factors into several disjointed sub regions with simpler patterns and then elm optimized by particle swarm intelligence is applied to identify the complex input output relationship in each cluster the results from two reservoirs of china show that our method can obtain satisfying performance in deriving operation rules of hydropower reservoir thus it can be concluded that the model s generalization capability can be improved by isolating each subclass composed of similar dataset keywords hydropower reservoir operation rule derivation k means clustering extreme learning machine particle swarm optimization 1 introduction compared with other fossil energy like coal fired plants hydropower has the unique merits of low pollutant discharge being renewable and fast start stop ability ming et al 2017a mu et al 2015 zhao et al 2015 feng et al 2019b thus hydropower is one of the most important energy resources that deserve the large scale development and utilization feng et al 2018d madani 2011 madani and lund 2009 as a basic component to produce generation hydropower reservoir is playing an important role in the integrated operation of water resource and electric power systems catalão et al 2011 feng et al 2017b shen and phanikumar 2010 hence it is important to implement the operation optimization of hydropower reservoir chang et al 2013 peng et al 2017 feng et al 2019c traditionally the results produced by deterministic operation optimization are closely related to the available knowledge of reservoir runoff however based on the existing technical technologies it is difficult or even impossible to perfectly capture the dynamic changing process of the future inflow ji et al 2014 madani 2010 chau 2007 which means that the implementation deviation will be inevitable in most cases in other words the deterministic operation optimization is on the basis of the perfect runoff knowledge madani 2010 which is not agreeable with the actual situation wu and chau 2013 wu and chau 2010 feng et al 2018b then it is necessary to develop effective methods for reservoir operation in the case where the perfectly predicted runoff cannot be gained yang et al 2018 liu et al 2006 ming et al 2017b as we know the long time historical data including both runoff and storage can be collected from operators after a period of time and then the deterministic optimization method like dynamic programming dp can be used to search for the optimal scheduling results obviously there are often lots of information hidden in the scheduling result that can be used to guide the actual operation of the reservoir under uncertainty li and huang 2008 2009 ma et al 2013 to guarantee the practicability of the scheduling scheme a feasible way is to extract the appropriate near optimal operation rule by mining from the historical information and real time working condition of hydropower reservoir liu et al 2011 yang et al 2017 liu et al 2011 the method to derivate reservoir operation rule from deterministic optimization results is called implicit stochastic optimization iso liu et al 2014 which has gotten a great deal of attention from scholars since its origin in general the iso framework is composed of the following steps ① collect the long term historical operation data of the target reservoir ② use the deterministic optimization methods to obtain the optimized scheduling results ③ use expert knowledge or statistical analysis techniques to choose dependent and independent variables ④ use artificial intelligence or other methods to derivate operation rule of hydropower reservoir ⑤ use the obtained operation rule to guide the generation schemes of hydropower reservoir where the scheduling plan may be dynamically adjusted with the change of the actual working condition based on the existing research achievements the survey results show that an appropriate operation rule obtained by iso can produce approximately 1 5 additional energy production in comparison with the regular scheduling methods ji et al 2014 due to its simplicity and convenience the operation rule where the decision variable like water discharge and power output of hydropower reservoir is often seen as a function of certain available information like the latest water level stored energy and local inflow has been widely used in the long term reservoir operation the methods for reservoir operation rule can be roughly divided into two groups the first is the traditional methods such as linear or nonlinear regression methods and the scheduling graph method the second mainly refers to the intelligence algorithms like support vector machine fuzzy set theory and artificial neural network among all the known intelligence algorithms the neural network is one of the most famous methods in the reservoir operation field wang et al 2009 for instance a neural network based simulation optimization model is developed for reservoir operation neelakantan and pundarikanthan 2000 the neural network is used to deduce reservoirs operating policy raman and chandramouli 1996 the performances of decision trees neural decision trees and fuzzy decision trees in reservoir operation rule derivation are compared wei and hsu 2008 the multiple linear regression and artificial neural networks are used to derivate the water and power operating rules for multi reservoirs zhou et al 2016 the data mining techniques represented by artificial neural network and genetic algorithm are used for real time reservoir operation bozorg haddad et al 2018 the artificial neural network are used to derivate reservoir operation rule under the simulated stationary and nonstationary inflow condition yang and ng 2017 although various degrees of success are achieved some defects should not be overlooked in the gradient based methods for the single layer feed forward neural network slfns like low efficiency and premature convergence chau 2006 taormina et al 2015 wang et al 2015 wang et al 2013 to avoid the above defects in the gradient based methods a novel approach known as extreme learning machine elm is developed to train slfns in recent years in elm the parameters input hidden weights and hidden bias are randomly generated in the preset range and then the hidden output weights are analytically obtained by the moore penrose generalized inverse method huang and chen 2007 huang et al 2011 huang et al 2012 due to the innovative solution idea elm has some prominent merits when compared with gradient based techniques like faster training speed less computational parameter and higher generalization ability elm has been widely used to solve many complex engineering optimization problems like hydrologic forecasting wind speed prediction and process controls however for all we know even to this day there are still few literatures about using elm to derivate operation rule of hydropower reservoir in order to refill this research gap this paper tries to verify the performance of elm in deriving reservoir operation rule in practice it was found that the standard elm method tends to fall into local optima with a high probability since the randomly generated input hidden weights and hidden biases remain unchanged in the training process in other words there are still improvement spaces to enhance the elm performance in deriving reservoir operation rule for the sake of enhancing the generalization ability of elm this study develops a novel class based evolutionary extreme learning machine ceelm for reservoir operation rule derivation in the ceelm method the k means clustering method is firstly used to divide the input vectors into different subspace and then different elm models are constructed in each subspace while the particle swarm optimization based on biological evolution mechanism in nature is adopted to find out the satisfying weights and basis the feasibly of the ceelm method is verified in xiaowan and hongjiadu in china finally to better understand this work the main contributions are summarized as below 1 the emerging extreme learning machine method is used to derivate the operation rule of hydropower reservoir which may be the first open report in this research field 2 a class based evolutionary extreme learning machine combining the advantages of several effective strategies is proposed to improve the elm performance 3 the practicability of the ceelm method in deriving operation rule is fully proved by the application results of two reservoirs in china providing a new effective tool for hydropower operators to solve similar engineering problems in other places the rest of this paper is organized as below section 2 gives the information of the elm method sections 3 shows the ceelm method based on the k means clustering and pso the details of the developed method for reservoir rule derivation is given in section 4 the simulation results and discussions are given in section 5 while the conclusions are summarized in section 6 2 extreme learning machine elm suppose the number of neural nodes in input layer hidden layer and output layer of the elm networks are m l and n respectively fig 1 draws the topological structure of the elm network the input layer and output layer in the elm network usually employ the linear activation function while the hidden layer uses the nonlinear activation function like sigmoid function and radial basis function niu et al 2018 taormina et al 2012 yaseen et al 2019 here the sigmoid function in eq 1 is chosen as the activation function of the hidden neurons because it can provide satisfying performance in many different scenarios like data classification dokeroglu and sevinc 2019 wind speed forecasting manohar et al 2018 fault detection li et al 2019 multi label learning cheng et al 2019 and runoff prediction wu et al 2009 1 g x 1 1 e x where x and g x represent the input value and output value of the hidden node respectively the optimization objective of elm is to minimize the total empirical errors of the training data sets with j samples x j o j which can be expressed as 2 minimize e w b β j 1 j o j o j where w denotes the connection weight matrix between the input layer and the hidden layer b is the bias of the hidden layer β is the connection weight matrix liking the hidden layer and the output layer j is the number of samples x j x j 1 x j 2 x j m r m is the input vector in the jth sample while o j o j 1 o j 2 o j n r n denotes the target output vector in the jth sample o j o j 1 o j 2 o j n r n is the simulated output vector of the elm model with respective to the input vector x j in the elm model after randomly generating the input hidden weights w and hidden bias b the optimization problem in eq 2 is equivalent to obtain the least square solution of the linear system in eq 3 based on the moore penrose generalized inverse matrix theory the weight matrix β between the hidden layer and the output layer can be estimated as β h t in which h is the generalized inverse matrix of the hidden layer outputs matrix h 3 h β t 4 h h 1 h 2 h j j l g w 1 x 1 b 1 g w 2 x 1 b 2 g w l x 1 b l g w 1 x 2 b 1 g w 2 x 2 b 2 g w l x 2 b l g w 1 x j b 1 g w 2 x j b 2 g w l x j b l j l 5 β β 1 t β 2 t β l t l n and t o 1 o 2 o j j n where w l is the connection weight vector between the input layer and the lth node in the hidden layer w i x j denotes the inner product of the weight vector w i and input vector x j b l denotes the bias of the lth node in the hidden layer β l t β l 1 β l 2 β l n r n is the weight vector connecting the lth hidden node and all the output nodes 3 class based evolutionary extreme learning machine ceelm 3 1 k means clustering method as one of the most efficient partitioning methods the k means clustering method has been used in a wide range of applications due to its simplicity and fast convergence xu and peng 2015 sun et al 2012 the k means clustering method involves an iterative process to optimize the number of specified clusters it first selects some randomly generated centroids as the initial clusters then each sample in the dataset is assigned to the closest centroid to form a set of temporary clusters and the new means to be the centroids of the observations in the new clusters will be calculated the above procedures are repeated until the stopping criteria are met then the current centroids represent the near optimal centers of all the clusters let o o 1 o i o n be a set of n data objects and each object is described by d features i 1 2 n the vector o i o i 1 o i j o i d t is the ith object from the dataset where o i j is the jth feature of the ith object the k means clustering aims to partition the n data objects into k k n different data sets s s 1 s l s k l 1 2 k the vector s l s l 1 s l j s l d t denotes the lth cluster center where s l j is the jth feature of the lth cluster center in k means clustering the total mean square error is adopted to evaluate the goodness of a clustering which is formulated as 6 f o c l 1 k m 1 k l d o l m c l 2 where k l is the number of data objects in the lth cluster o l m is the mth object of the lth cluster center c l c l 1 c l j c l d t is the center of the lth possible cluster which is defined as 7 c l 1 k l m 1 k l o l m l besides d o l m c l is the relative space distance between the data object o l m and c l many different distance metrics have been used to measure the dissimilarity between two objects and the euclidean distance is the most popular distance function thus the euclidean distance is used in this paper to measure the relative difference between o l m and c l which is defined as follows 8 d o l m c l j 1 d c l j o l j m 2 the clustering center can be seen as the optimal one as the minimum total mean square error is achieve which is defined as below 9 s arg min f o c 3 2 particle swarm optimization pso pso is a well known population based optimization approach inspired by the social behaviors of bird groups due to its feasibility pso has been successfully applied to solve different nonlinear and non differential constrained optimization problem such as network training fuzzy control and power distribution wang et al 2013 feng et al 2018c in pso each particle is treated as a potential solution for the problem to be optimized and a collection of candidate particles simultaneously search for optimal solution in the problem space every particle has two parameters including the position vector representing the possible solution and the velocity vector representing the possible moving direction taormina et al 2015 yuan et al 2009 niu et al 2018 the population will continue the iteration process until the maximum number of iterations or the terminal precision is satisfied feng et al 2017a in the evolutionary process each particle dynamically changes its vector position and velocity vector by tracking after its own best known position pbest and the global best known position gbest for the ith particle at the tth cycle the corresponding evolution equation can be expressed as below 10 v i t 1 λ t v i t a 1 r 1 pbest i t x i t a 2 r 2 gbest t x i t 11 x i t 1 x i t v i t 1 i 1 2 i where i is the number of particles t is the iteration number t 1 2 t max r 1 and r 2 are two random numbers uniformly distributed in the range of ming et al 2017a x i t x i t 1 x i t 2 x i t d r d and v i t v i t 1 v i t 2 v i t d r d are the velocity vector and position vector of the ith particle in the tth iteration respectively d is the number of decision variables of the problem at hand a 1 and a 2 are the cognitive and social learning factors respectively λ t is the inertia factor varying linearly from λ max at the beginning to λ min at the end which is given as below 12 λ t λ max λ max λ min t t max 3 3 class based evolutionary extreme learning machine ceelm according to the aforementioned methods this paper develops a novel class based evolutionary extreme learning machine ceelm approach where the k means clustering is treated as a feature selection method to divide candidate input vectors into different classes and the elm method is used to formulate the nonlinear function relationship between input vectors and output variable in each cluster the pso algorithm is adopted to search for the optimal weights and bias of elm fig 2 shows the sketch map of the ceelm method in an overall view the ceelm model is based on the classical divide and conquer idea the complicated regression problem with a larger number of training samples is firstly divided into several simpler regression subproblems with smaller number of training samples which can make an obvious improvement in the fitting precision by reducing the size of training dataset in a singleton pattern and then in each subproblem the elm model coupled with pso is driven to simulate the complicated input output relationship by minimizing the total empirical errors of training datasets in such a way the proposed ceelm method is able to capture a more compact network structure as compared with the standard elm model providing satisfactory generalization ability in practical problems in general there are five basic steps in the framework of the ceelm approach including dataset preparation parameter setting data clustering model development and practical application then the detailed procedures can be described as below step 1 dataset preparation determine the set of input variables and output variables and then the primary dataset is divided into training and testing samples that will be normalized into ming et al 2017a by 13 y o 1 y o y o min y o max y o min where y o 1 and y o are the normalized data and original data respectively y o max and y o min are the maximum and minimum of the original data respectively step 2 parameter setting define the basic computational parameters of the ceelm method like the number of clusters k the number of hidden nodes l in the elm model the maximum iterations t max the number of particles m the initial value λ max and final value λ min of the inertia factor step 3 data clustering based on the preset number of clusters k use the k means clustering technique to determine all the clustering centers where the minimal distance principle is employed to calculate the cluster tags of each sample step 3 0 set the cycle index a 1 and randomly generate k initial category centers that are recorded as st a st 1 a st i a st k a where st i a denotes the lth cluster center at the ath cycle step 3 1 use eq 8 to calculate the distance between each observation and all the current cluster centers and each observation is assigned to the specific category with the minimal distance step 3 2 based on eq 7 use the average of all the observations in each cluster to update the corresponding centroids and all the category centers can be recorded as s a s 1 a s i a s k a step 3 3 set a a 1 if the maximum iteration is met or the assignments remain changed stop the iteration and export the category centers s a otherwise set st a s a and turn to step 3 1 step 4 model development use the training dataset to develop the relevant elm models for all the clusters where pso is adopted to search for an approximate parameter combination of the elm model in each cluster step 4 0 set the cluster index k 1 step 4 1 set t 1 then the initial population is randomly generated in the problem space and each particle denotes one possible network parameter combination the ith individual at the tth iteration x i t is composed of all the input hidden weights and hidden bias which is given as below 14 x i t w 1 i t t w l i t t w l i t t b 1 i t b l i t b l i t where w l i t t denotes the weight vector linking the lth hidden neuron and the input layer b l i t is the bias of the lth hidden neuron step 4 2 use eqs 15 17 to calculate the hidden output weight matrix and the fitness value of each individual respectively to be mentioned the fitness value is equal to the root mean squared error rmse of two data series 15 β i t h i t t 16 h i t g w 1 i t x 1 b 1 i t g w 2 i t x 1 b 2 i t g w l i t x 1 b l i t g w 1 i t x 2 b 1 i t g w 2 i t x 2 b 2 i t g w l i t x 2 b l i t g w 1 i t x j k b 1 i t g w 2 i t x j k b 2 i t g w l i t x j k b l i t j k l 17 f x i t 1 j k s 1 j k o s l 1 l β l g w l i t b l i t x s 2 2 where h i t is the output matrix of hidden layer in the particle x i t h i t is the moore penrose generalized inverse of h i t j k is the number of training samples in the kth cluster f x i t is the fitness value of the particle x i t step 4 3 use eqs 18 and 19 to update the best known position of each individual and the global best known position of the swarm respectively 18 pbest i t pbest i t 1 if f pbest i t 1 f x i t x i t if f pbest i t 1 f x i t i 1 2 m 19 gbest t arg min f pbest 1 t f pbest 2 t f pbest m t step 4 4 use eq 12 to calculate the inertia factor of the current generation and then use eqs 10 and 11 to update the position vector and velocity vector of each particle respectively step 4 5 set t t 1 if t t max turn to step 4 2 otherwise the global best known position of the swarm will be treated as the optimal network parameters of the kth cluster step 4 6 set k k 1 if k k turn to step 4 1 otherwise stop the calculation and export the optimized elm models of all the clusters step 5 practical application after the training process is finished the clustering centers and the elm model for each cluster are obtained in the operating process the new input vector will be assigned to the most relevant group based on the minimal distance principle and then the calibrated elm model is chosen to forecast the possible output values to be mentioned the original input values should be normalized before starting a new operating process while the original output values of each elm model should be renormalized to the original range of output data 3 4 performance of ceelm in benchmark functions in this section the benchmark function han et al 2013 is chosen to compare the performance of three methods including the ceelm method elm and elm optimized by pso eelm which is given in eq 20 the numbers of training and testing samples are set as 1000 the noisy data uniformly distributed in 0 2 0 2 is added to all the training data while the testing dataset is not affected the parameters in three algorithms are determined by the trial and error strategy fig 3 draws the results obtained by the ceelm method it can be clearly found that the ceelm method has a strong ability in tracking the variation trend of nonlinear function table 1 shows the statistical indexes of the means of three methods in 20 independent runs it can be found that due to the selection of network parameters the ceelm s training time is longer than that of elm and eelm in addition the rmse value of ceelm is far less than elm and eelm which means that the classification strategy and swarm intelligence can improve the model s generalization ability thus the ceelm method has satisfying performance in the nonlinear function approximation problem 20 f x 1 if x 0 sin x x otherwise x 10 10 4 using ceelm to derivate operation rule of hydropower reservoir 4 1 deterministic reservoir operation optimization model to obtain data samples 4 1 1 objective function the scheduling process obtained from the traditional deterministic optimization model plays an important role in deriving reservoir operation rule because it can not only provide essential training samples but also be used to evaluate the performance of the derived operation rule generally for a multipurpose hydropower reservoir the objective function in eq 21 is often chosen to maximize the annual average energy production and the generation assurance rate in the scheduling horizon 21 maximize e 1 n i 1 n j 1 m p i j t i j g p i j 22 g p i j 0 if p i j p min b p min p i j c if p i j p min where e is the objective value n is the number of years m is the number of periods per year p i j is the reservoir s power output at the jth period of the ith year t i j is the total hours at the jth period of the ith year p min is firm output of the hydropower reservoir b and c are two positive intermediate parameters to guarantee the assurance rate of hydropower generation 4 1 2 operation constraints to ensure that all the variables are in the feasible zones some physical constraints should be considered in the modeling process feng et al 2018a niu et al 2019 ji et al 2015 like the water balance equation storage volume limits water spillage limits turbine discharge and power output limits 23 v i j v i j 1 i i j q i j s i j t i j i 1 n j 1 m 24 v i j min v i j v i j max i 1 n j 1 m 25 q i j min q i j q i j max i 1 n j 1 m 26 s i j min s i j s i j max i 1 n j 1 m 27 p i j min p i j p i j max i 1 n j 1 m where v i j i i j q i j and s i j are the storage volume local inflow turbine discharge and water spillage at the jth period of the ith year respectively v i j max and v i j min are the maximum and minimum storage volume at the jth period of the ith year respectively q i j max and q i j min are the maximum and minimum turbine discharge at the jth period of the ith year respectively s i j max and s i j min are the maximum and minimum water spillage at the jth period of the ith year respectively p i j max and p i j min are the maximum and minimum power output at the jth period of the ith year respectively 4 1 3 optimization method based on dynamic programming when the long term inflow series the initial storage and terminal storage are known the above optimization model will become a deterministic single objective optimization problem that can be easily solved by the dp recursive equation in eq 28 after using the dp method to deal with the model feng et al 2017c the obtained long term results can be treated as the samples to derivate the operation rule for the target reservoir 28 e i j v i j max e i j v i j v i j 1 e i j 1 v i j 1 where e i j v i j v i j 1 is the objective value at the jth period of the ith year e i j v i j is the optimal cumulative return from the jth period of the ith year to the first period 4 2 general computational framework fig 4 illustrates the framework of the ceelm method in deriving operation rule of hydropower reservoir and the overall process is composed of the following four parts 1 collect the historical operation data of hydropower reservoir via data acquisition like water level or data inversion like inflow while the deterministic operation model in section 4 1 is used to obtain the necessary samples where eq 21 is chosen as the optimization goal to maximize the total generation benefit of hydropower reservoir in the long run 2 use some methods like domain knowledge and correlation analysis to determine the input and output variables in deriving operation rule and then the k means clustering method is used to divide the input space into several disjoint sub regions 3 the elm models are developed to identify the complicated input output relationship in each sub region while the pso method is used to optimize the model parameters to be mentioned the goal is set to minimize the total error between the real and forecasted output values with respective to the related training samples and the elm model can be expressed as y f x 1 x i x n where y denotes the target output variable like power generation or total discharge x i denotes the ith input variable like previous inflow initial water level or future runoff and n denotes the number of input variables 4 the historical data collection and future runoff prediction tools are used to obtain the related input variables like antecedent inflows and future runoff and then the obtained elm models of all the clusters are selected to guide the operation of hydropower reservoir 5 case studies in this section two huge hydropower reservoirs in southwest china are selected as the studied objects to test the feasibility of the ceelm approach in deriving reservoir operation rules fig 5 shows the locations of hongjiadu and xiaowan reservoirs in addition the results obtained by the standard dp approach are treated as the samples to train the ceelm method while the traditional scheduling graph method is chosen as the benchmark method all the methods are encoded in java language and then implemented on a personal computer 5 1 case 1 xiaowan reservoir in the first case the xiaowan reservoir in lancang river is used to verify the performance of the proposed method with a total length of 2000 km and a drainage area of 91 000 km2 the lancang river is one of the most important hydropower bases providing electricity for developed areas in china as the leading reservoir in the middle and lower reaches of lancang river the xiaowan hydroplant has a total installed capacity of 4200 mw and a regulation storage capacity of about 10 billion m3 based on the monthly historical data from 1953 to 2008 the dp approach is adopted to obtain the optimal operation results then all the data is divided into three subsets to avoid the negative influence of over fitting including training dataset the first 30 years data to calibrate the model s parameters testing dataset the following 10 years data to monitor the training process as well as validation dataset the remaining data to verify the feasibility of the model based on the expertise the set of independent variables for xiaowan have 4 variables including the initial water level inflows and monthly index of the target period as well as the outflow of the last period then the power output is regarded as the dependent variable in order to obtain better regression ability we compare several different models where the number of neurons in the hidden layer changes from 3 to 14 and the model with the structure 4 11 1 with the minimum error are chosen for hydropower operation which has 4 independent variables 11 hidden neurons and 1 dependent variable then table 2 compares the average power outputs of five methods including the scheduling graph method sgm elm eelm ceelm and dp it can be found that two methods dp and ceelm tend to store more water in dry season so as to increase the power output at flood season which will make an obvious increase in the long term economic benefit of hydroplant meanwhile the statistical results of five methods in terms of the assurance rate and annual generation are listed in fig 6 where sgm is chosen as the benchmark method from fig 6 and table 2 it can be clearly found that as compared with the conventional sgm tool three different versions of elm can obtain obvious improvements in two objectives which means that an appropriate method is capable of increasing the overall benefit of hydropower reservoir besides compared with elm and eelm the ceelm method can obtain approximately 6 61 and 2 48 additional improvements in assurance rate besides the improvements in annual generation is reached up to 1 87 108 kwh and 1 79 108 kwh this case demonstrates the feasibility of the class based strategy in enhancing the performance of the optimization model moreover the statistical indexes of the ceelm method are very close to the optimized results of dp which fully shows the effectiveness of the proposed method in deriving reservoir operation rule considering the technical limitation of perfectly capturing the changing process of reservoir runoff it is rather difficult or even impossible to obtain the dp s scheduling results in practice hence the ceelm method is an alternative tool to provide decision support for the optimal operation of hydropower reservoir in the changing environment fig 7 shows the water levels obtained by several methods for xiaowan hydropower reservoir it can be observed that the water levels of dp and ceelm are almost identical while considerably different from three other methods sgm elm and eelm moreover the ceelm and dp can keep water at a high level as long as possible to further increase the whole operating efficiency of the xiaowan hydroplant hence compared with elm and eelm the proposed method has better performance to mimic the variation trend of running condition because its results are much close to the ideal situation obtained by dp 5 2 case 2 hongjiadu reservoir in the second case the hongjiadu reservoir located on wu river is chosen as the study site the wu river is originated from guizhou province and is seen as the largest branch on the right bank of yangtze river with a total length of 1037 km and a drainage area of 87920 km2 for hongjiadu reservoir the control area was reached up to 9900 km2 and the multi year average runoff at the dam site is about 160 m3 s after collecting the monthly data from 1952 to 2007 the first 40 years is used for training the models while the remaining data is used for validation of the training data the first 30 years data was used to train the model while the others for testing the initial analysis the detailed results of several methods for hongjiadu hydropower reservoir are listed in table 3 where bpnn denotes the standard back propagation neural network while ceelm k denotes that the number of categories is k in ceelm from table 3 the following three important conclusions can be drawn 1 in the bpnn the set of input variables only contain the power output at the first several periods while all the training samples are simultaneously considered in a single regression model in practice the generation scheduling of hydroplant is often affected by a variety of factors from natural environment and management organizations and it is difficult for a limited length of scheduling data to reflect the overall condition of those factors then the bpnn performance is worse than other methods highlighting the importance of data classification and training technique for neural network 2 the ceelm k family methods can provide satisfying scheduling results in both assurance rate and annual generation because the gap between our method and dp is rather small especially in generation benefit 3 the number of categories have an important influence on the performance of the ceelm method when the value of k increases from 1 to 12 two objectives climb up and then decline and the optimal objectives are achieved in the case that k is set to be 5 for a giving dataset a larger number of categories will reduce the size of training samples in each cluster and the size of historical data per group may be too small to provide enough knowledge for learning to sum up the feasibility of the proposed approach in deriving reservoir operation rules is again proved in this case and it is of great importance to choose an approximate size of categories based on the actual situation of reservoir 5 3 discussion on simulation results 5 3 1 performance analysis of the developed method from the above analysis it can be found that in different cases the result of the ceelm method is better than the sgm method and closer to dp the reason mainly lies in the organic integration of three effective strategies including the k means clustering method extreme learning machine and swarm intelligence firstly using the k means clustering method in regard to certain attributes the data samples are as homogeneous to other objects within the same group as possible while differing from objects that belong to other groups as much as possible then the complex input space is spilt into several disjoint subspaces to effectively reduce the computational complexity in each subspace secondary the elm method is adopted to capture the implicit relationship between input vectors and output vectors in each cluster which can achieve a good balance between generalization ability and learning speed thirdly the classical pso method is introduced to optimize the elm parameters which can avoid falling into local optima by carefully selecting the input hidden weights and hidden bias on the other hand the standard dp method suffers from the severe dimensionality problem as used to solve the high dimensional optimization problem for the sake of simplification it is assumed that there are k reservoirs in the hydropower system and m points are discretized in the feasible state space of each reservoir based on the permutation and combination theory the number of discrete state vectors is up to mk per period and the amount of calculations is directly proportional to m 2 k between any two periods feng et al 2017d zhang et al 2015 obviously the computational burden of the dp method shows an exponential growth with the increasing number of reservoirs resulting in high memory requirement and long execution time in other words it is difficult for dp to satisfy the physical demand of large scale hydropower operation problem thus compared with the traditional methods the ceelm method combining the merits of three effective tools have better performance in deriving monthly operation rule of hydropower reservoir besides the small data size and computational burden can reduce the execution time of the ceelm method in two cases demonstrating the running performance in terms of industry acceptance whereas our model has the potential to work for reservoirs with large scale dataset since it needs little training time the operation rule can be obtained by processing off line learning and actuation time for new instance the model s structure and parameters are fixed after training 5 3 2 relationship and differences between the elm based method and deep learning elm and deep learning are two kinds of different neural network based methods that have been widely employed to solve the practical problems like regression and classification generally elm and deep learning may achieve poor performance when the parameters are not well tuned besides there are also some differences between elm and deep learning for instance elm often has a single hidden layer huang 2014 while deep learning often refers to large and multilayer neural networks working directly on big raw data shen et al 2018 fang et al 2019 fang et al 2017 the training process of elm can be done in a very short time while deep learning often requires large sample sizes and long training time shen 2018 the hidden neurons of the elm model do not need to be tuned like deep learning deng et al 2015 besides due to elm s different roles of feature learning and clustering elm can be used as earlier layers in multilayer networks where the later layers are trained by deep learning huang 2015 thus the elm and deep learning have their own advantages and disadvantages and it is of great necessity to choose the appropriate method based on the features of the problem at hand 5 3 3 limitation and future work although this paper develops a solution framework for deriving reservoir operation rule several shortcomings still exist in the ceelm method ① the time complexity of the k means method may be larger than other clustering methods for a particular number of clusters and the obtained clustering centers are often affected by the initial values produced by random seeds besides the search capability of pso will be gradually reduced due to the loss of swarm diversity and when adopted to optimize the elm parameters the premature convergence problem rooted in pso will have a negative effect on the compactness of neural network liu et al 2019 ② for the cascade hydropower system the practicability of the presented method may be affected by some practical factors like information sharing mechanism differentiated scheduling requirement and system operation limit among upstream and downstream reservoirs ③ the developed method under the iso framework attempts to reproduce historic operation rules for reservoirs however the historical operation is not always the perfect representation of future operation because the runoff and operation priorities may change with time thus the future work can be deepened from the following aspects ① algorithm performance we can use more efficient methods to improve the quality of the clustering center and develop new strategies to find better parameter combination for each elm model ② engineering practicability we can explore new solution methods or optimization models feng et al 2019a to scientifically quantize the criterion attributions of the structured or semi structured data and incorporate the normalized information into optimized models to produce the operation rule of cascade hydropower system ③ model adaptivity we can develop effective forecasting models producing accurate runoff information to reduce the error originated from input variables and then use the latest real data to update the obtained models structure and parameters to improve the result accuracy by the above strategies the developed method is capable of providing satisfying operation result for hydropower reservoirs under changes in management strategy design or climates 6 conclusion in this paper a novel class based evolutionary extreme learning machine ceelm is presented for deriving the monthly operation rules of hydropower reservoir the ceelm method firstly uses the k means methods to divide the large scale input space into several disjoint subspaces and then uses the evolutionary extreme learning machine to mimic the complex input output relationship of samples per cluster the simulations from two hydropower reservoirs hongjiadu and xiaowan of china indicate that in different cases the ceelm method can provide approving scheduling results that is close to the optimal solutions obtained by the deterministic dynamic programming method thus a promising method to guide the scientific operation of hydropower system is presented here meanwhile in the future we will deepen the research on the improvements of the hybrid extreme learning machine in the operation optimization of cascade hydropower system declaration of competing interest the authors declare no conflict of interest acknowledgments this paper is supported by national natural science foundation of china 51709119 natural science foundation of hubei province 2018cfb573 and the fundamental research funds for the central universities hust 2017kfyxjj193 the writers would like to thank editors and reviewers for their valuable comments and suggestions 
6289,in practice the rational operation rule derived from historical information and real time working condition can help the operators make the quasi optimal scheduling plan of hydropower reservoirs leading to significant improvements in the generation benefit as an emerging artificial intelligence method the extreme learning machine elm provides a new effective tool to derivate the reservoir operation rule however it is difficult for the standard elm method to avoid falling into local optima due to the random determination of both input hidden weights and hidden bias to enhance the elm performance this research develops a novel class based evolutionary extreme learning machine ceelm to determine the appropriate operation rule of hydropower reservoir in ceelm the k means clustering method is firstly adopted to divide all the influential factors into several disjointed sub regions with simpler patterns and then elm optimized by particle swarm intelligence is applied to identify the complex input output relationship in each cluster the results from two reservoirs of china show that our method can obtain satisfying performance in deriving operation rules of hydropower reservoir thus it can be concluded that the model s generalization capability can be improved by isolating each subclass composed of similar dataset keywords hydropower reservoir operation rule derivation k means clustering extreme learning machine particle swarm optimization 1 introduction compared with other fossil energy like coal fired plants hydropower has the unique merits of low pollutant discharge being renewable and fast start stop ability ming et al 2017a mu et al 2015 zhao et al 2015 feng et al 2019b thus hydropower is one of the most important energy resources that deserve the large scale development and utilization feng et al 2018d madani 2011 madani and lund 2009 as a basic component to produce generation hydropower reservoir is playing an important role in the integrated operation of water resource and electric power systems catalão et al 2011 feng et al 2017b shen and phanikumar 2010 hence it is important to implement the operation optimization of hydropower reservoir chang et al 2013 peng et al 2017 feng et al 2019c traditionally the results produced by deterministic operation optimization are closely related to the available knowledge of reservoir runoff however based on the existing technical technologies it is difficult or even impossible to perfectly capture the dynamic changing process of the future inflow ji et al 2014 madani 2010 chau 2007 which means that the implementation deviation will be inevitable in most cases in other words the deterministic operation optimization is on the basis of the perfect runoff knowledge madani 2010 which is not agreeable with the actual situation wu and chau 2013 wu and chau 2010 feng et al 2018b then it is necessary to develop effective methods for reservoir operation in the case where the perfectly predicted runoff cannot be gained yang et al 2018 liu et al 2006 ming et al 2017b as we know the long time historical data including both runoff and storage can be collected from operators after a period of time and then the deterministic optimization method like dynamic programming dp can be used to search for the optimal scheduling results obviously there are often lots of information hidden in the scheduling result that can be used to guide the actual operation of the reservoir under uncertainty li and huang 2008 2009 ma et al 2013 to guarantee the practicability of the scheduling scheme a feasible way is to extract the appropriate near optimal operation rule by mining from the historical information and real time working condition of hydropower reservoir liu et al 2011 yang et al 2017 liu et al 2011 the method to derivate reservoir operation rule from deterministic optimization results is called implicit stochastic optimization iso liu et al 2014 which has gotten a great deal of attention from scholars since its origin in general the iso framework is composed of the following steps ① collect the long term historical operation data of the target reservoir ② use the deterministic optimization methods to obtain the optimized scheduling results ③ use expert knowledge or statistical analysis techniques to choose dependent and independent variables ④ use artificial intelligence or other methods to derivate operation rule of hydropower reservoir ⑤ use the obtained operation rule to guide the generation schemes of hydropower reservoir where the scheduling plan may be dynamically adjusted with the change of the actual working condition based on the existing research achievements the survey results show that an appropriate operation rule obtained by iso can produce approximately 1 5 additional energy production in comparison with the regular scheduling methods ji et al 2014 due to its simplicity and convenience the operation rule where the decision variable like water discharge and power output of hydropower reservoir is often seen as a function of certain available information like the latest water level stored energy and local inflow has been widely used in the long term reservoir operation the methods for reservoir operation rule can be roughly divided into two groups the first is the traditional methods such as linear or nonlinear regression methods and the scheduling graph method the second mainly refers to the intelligence algorithms like support vector machine fuzzy set theory and artificial neural network among all the known intelligence algorithms the neural network is one of the most famous methods in the reservoir operation field wang et al 2009 for instance a neural network based simulation optimization model is developed for reservoir operation neelakantan and pundarikanthan 2000 the neural network is used to deduce reservoirs operating policy raman and chandramouli 1996 the performances of decision trees neural decision trees and fuzzy decision trees in reservoir operation rule derivation are compared wei and hsu 2008 the multiple linear regression and artificial neural networks are used to derivate the water and power operating rules for multi reservoirs zhou et al 2016 the data mining techniques represented by artificial neural network and genetic algorithm are used for real time reservoir operation bozorg haddad et al 2018 the artificial neural network are used to derivate reservoir operation rule under the simulated stationary and nonstationary inflow condition yang and ng 2017 although various degrees of success are achieved some defects should not be overlooked in the gradient based methods for the single layer feed forward neural network slfns like low efficiency and premature convergence chau 2006 taormina et al 2015 wang et al 2015 wang et al 2013 to avoid the above defects in the gradient based methods a novel approach known as extreme learning machine elm is developed to train slfns in recent years in elm the parameters input hidden weights and hidden bias are randomly generated in the preset range and then the hidden output weights are analytically obtained by the moore penrose generalized inverse method huang and chen 2007 huang et al 2011 huang et al 2012 due to the innovative solution idea elm has some prominent merits when compared with gradient based techniques like faster training speed less computational parameter and higher generalization ability elm has been widely used to solve many complex engineering optimization problems like hydrologic forecasting wind speed prediction and process controls however for all we know even to this day there are still few literatures about using elm to derivate operation rule of hydropower reservoir in order to refill this research gap this paper tries to verify the performance of elm in deriving reservoir operation rule in practice it was found that the standard elm method tends to fall into local optima with a high probability since the randomly generated input hidden weights and hidden biases remain unchanged in the training process in other words there are still improvement spaces to enhance the elm performance in deriving reservoir operation rule for the sake of enhancing the generalization ability of elm this study develops a novel class based evolutionary extreme learning machine ceelm for reservoir operation rule derivation in the ceelm method the k means clustering method is firstly used to divide the input vectors into different subspace and then different elm models are constructed in each subspace while the particle swarm optimization based on biological evolution mechanism in nature is adopted to find out the satisfying weights and basis the feasibly of the ceelm method is verified in xiaowan and hongjiadu in china finally to better understand this work the main contributions are summarized as below 1 the emerging extreme learning machine method is used to derivate the operation rule of hydropower reservoir which may be the first open report in this research field 2 a class based evolutionary extreme learning machine combining the advantages of several effective strategies is proposed to improve the elm performance 3 the practicability of the ceelm method in deriving operation rule is fully proved by the application results of two reservoirs in china providing a new effective tool for hydropower operators to solve similar engineering problems in other places the rest of this paper is organized as below section 2 gives the information of the elm method sections 3 shows the ceelm method based on the k means clustering and pso the details of the developed method for reservoir rule derivation is given in section 4 the simulation results and discussions are given in section 5 while the conclusions are summarized in section 6 2 extreme learning machine elm suppose the number of neural nodes in input layer hidden layer and output layer of the elm networks are m l and n respectively fig 1 draws the topological structure of the elm network the input layer and output layer in the elm network usually employ the linear activation function while the hidden layer uses the nonlinear activation function like sigmoid function and radial basis function niu et al 2018 taormina et al 2012 yaseen et al 2019 here the sigmoid function in eq 1 is chosen as the activation function of the hidden neurons because it can provide satisfying performance in many different scenarios like data classification dokeroglu and sevinc 2019 wind speed forecasting manohar et al 2018 fault detection li et al 2019 multi label learning cheng et al 2019 and runoff prediction wu et al 2009 1 g x 1 1 e x where x and g x represent the input value and output value of the hidden node respectively the optimization objective of elm is to minimize the total empirical errors of the training data sets with j samples x j o j which can be expressed as 2 minimize e w b β j 1 j o j o j where w denotes the connection weight matrix between the input layer and the hidden layer b is the bias of the hidden layer β is the connection weight matrix liking the hidden layer and the output layer j is the number of samples x j x j 1 x j 2 x j m r m is the input vector in the jth sample while o j o j 1 o j 2 o j n r n denotes the target output vector in the jth sample o j o j 1 o j 2 o j n r n is the simulated output vector of the elm model with respective to the input vector x j in the elm model after randomly generating the input hidden weights w and hidden bias b the optimization problem in eq 2 is equivalent to obtain the least square solution of the linear system in eq 3 based on the moore penrose generalized inverse matrix theory the weight matrix β between the hidden layer and the output layer can be estimated as β h t in which h is the generalized inverse matrix of the hidden layer outputs matrix h 3 h β t 4 h h 1 h 2 h j j l g w 1 x 1 b 1 g w 2 x 1 b 2 g w l x 1 b l g w 1 x 2 b 1 g w 2 x 2 b 2 g w l x 2 b l g w 1 x j b 1 g w 2 x j b 2 g w l x j b l j l 5 β β 1 t β 2 t β l t l n and t o 1 o 2 o j j n where w l is the connection weight vector between the input layer and the lth node in the hidden layer w i x j denotes the inner product of the weight vector w i and input vector x j b l denotes the bias of the lth node in the hidden layer β l t β l 1 β l 2 β l n r n is the weight vector connecting the lth hidden node and all the output nodes 3 class based evolutionary extreme learning machine ceelm 3 1 k means clustering method as one of the most efficient partitioning methods the k means clustering method has been used in a wide range of applications due to its simplicity and fast convergence xu and peng 2015 sun et al 2012 the k means clustering method involves an iterative process to optimize the number of specified clusters it first selects some randomly generated centroids as the initial clusters then each sample in the dataset is assigned to the closest centroid to form a set of temporary clusters and the new means to be the centroids of the observations in the new clusters will be calculated the above procedures are repeated until the stopping criteria are met then the current centroids represent the near optimal centers of all the clusters let o o 1 o i o n be a set of n data objects and each object is described by d features i 1 2 n the vector o i o i 1 o i j o i d t is the ith object from the dataset where o i j is the jth feature of the ith object the k means clustering aims to partition the n data objects into k k n different data sets s s 1 s l s k l 1 2 k the vector s l s l 1 s l j s l d t denotes the lth cluster center where s l j is the jth feature of the lth cluster center in k means clustering the total mean square error is adopted to evaluate the goodness of a clustering which is formulated as 6 f o c l 1 k m 1 k l d o l m c l 2 where k l is the number of data objects in the lth cluster o l m is the mth object of the lth cluster center c l c l 1 c l j c l d t is the center of the lth possible cluster which is defined as 7 c l 1 k l m 1 k l o l m l besides d o l m c l is the relative space distance between the data object o l m and c l many different distance metrics have been used to measure the dissimilarity between two objects and the euclidean distance is the most popular distance function thus the euclidean distance is used in this paper to measure the relative difference between o l m and c l which is defined as follows 8 d o l m c l j 1 d c l j o l j m 2 the clustering center can be seen as the optimal one as the minimum total mean square error is achieve which is defined as below 9 s arg min f o c 3 2 particle swarm optimization pso pso is a well known population based optimization approach inspired by the social behaviors of bird groups due to its feasibility pso has been successfully applied to solve different nonlinear and non differential constrained optimization problem such as network training fuzzy control and power distribution wang et al 2013 feng et al 2018c in pso each particle is treated as a potential solution for the problem to be optimized and a collection of candidate particles simultaneously search for optimal solution in the problem space every particle has two parameters including the position vector representing the possible solution and the velocity vector representing the possible moving direction taormina et al 2015 yuan et al 2009 niu et al 2018 the population will continue the iteration process until the maximum number of iterations or the terminal precision is satisfied feng et al 2017a in the evolutionary process each particle dynamically changes its vector position and velocity vector by tracking after its own best known position pbest and the global best known position gbest for the ith particle at the tth cycle the corresponding evolution equation can be expressed as below 10 v i t 1 λ t v i t a 1 r 1 pbest i t x i t a 2 r 2 gbest t x i t 11 x i t 1 x i t v i t 1 i 1 2 i where i is the number of particles t is the iteration number t 1 2 t max r 1 and r 2 are two random numbers uniformly distributed in the range of ming et al 2017a x i t x i t 1 x i t 2 x i t d r d and v i t v i t 1 v i t 2 v i t d r d are the velocity vector and position vector of the ith particle in the tth iteration respectively d is the number of decision variables of the problem at hand a 1 and a 2 are the cognitive and social learning factors respectively λ t is the inertia factor varying linearly from λ max at the beginning to λ min at the end which is given as below 12 λ t λ max λ max λ min t t max 3 3 class based evolutionary extreme learning machine ceelm according to the aforementioned methods this paper develops a novel class based evolutionary extreme learning machine ceelm approach where the k means clustering is treated as a feature selection method to divide candidate input vectors into different classes and the elm method is used to formulate the nonlinear function relationship between input vectors and output variable in each cluster the pso algorithm is adopted to search for the optimal weights and bias of elm fig 2 shows the sketch map of the ceelm method in an overall view the ceelm model is based on the classical divide and conquer idea the complicated regression problem with a larger number of training samples is firstly divided into several simpler regression subproblems with smaller number of training samples which can make an obvious improvement in the fitting precision by reducing the size of training dataset in a singleton pattern and then in each subproblem the elm model coupled with pso is driven to simulate the complicated input output relationship by minimizing the total empirical errors of training datasets in such a way the proposed ceelm method is able to capture a more compact network structure as compared with the standard elm model providing satisfactory generalization ability in practical problems in general there are five basic steps in the framework of the ceelm approach including dataset preparation parameter setting data clustering model development and practical application then the detailed procedures can be described as below step 1 dataset preparation determine the set of input variables and output variables and then the primary dataset is divided into training and testing samples that will be normalized into ming et al 2017a by 13 y o 1 y o y o min y o max y o min where y o 1 and y o are the normalized data and original data respectively y o max and y o min are the maximum and minimum of the original data respectively step 2 parameter setting define the basic computational parameters of the ceelm method like the number of clusters k the number of hidden nodes l in the elm model the maximum iterations t max the number of particles m the initial value λ max and final value λ min of the inertia factor step 3 data clustering based on the preset number of clusters k use the k means clustering technique to determine all the clustering centers where the minimal distance principle is employed to calculate the cluster tags of each sample step 3 0 set the cycle index a 1 and randomly generate k initial category centers that are recorded as st a st 1 a st i a st k a where st i a denotes the lth cluster center at the ath cycle step 3 1 use eq 8 to calculate the distance between each observation and all the current cluster centers and each observation is assigned to the specific category with the minimal distance step 3 2 based on eq 7 use the average of all the observations in each cluster to update the corresponding centroids and all the category centers can be recorded as s a s 1 a s i a s k a step 3 3 set a a 1 if the maximum iteration is met or the assignments remain changed stop the iteration and export the category centers s a otherwise set st a s a and turn to step 3 1 step 4 model development use the training dataset to develop the relevant elm models for all the clusters where pso is adopted to search for an approximate parameter combination of the elm model in each cluster step 4 0 set the cluster index k 1 step 4 1 set t 1 then the initial population is randomly generated in the problem space and each particle denotes one possible network parameter combination the ith individual at the tth iteration x i t is composed of all the input hidden weights and hidden bias which is given as below 14 x i t w 1 i t t w l i t t w l i t t b 1 i t b l i t b l i t where w l i t t denotes the weight vector linking the lth hidden neuron and the input layer b l i t is the bias of the lth hidden neuron step 4 2 use eqs 15 17 to calculate the hidden output weight matrix and the fitness value of each individual respectively to be mentioned the fitness value is equal to the root mean squared error rmse of two data series 15 β i t h i t t 16 h i t g w 1 i t x 1 b 1 i t g w 2 i t x 1 b 2 i t g w l i t x 1 b l i t g w 1 i t x 2 b 1 i t g w 2 i t x 2 b 2 i t g w l i t x 2 b l i t g w 1 i t x j k b 1 i t g w 2 i t x j k b 2 i t g w l i t x j k b l i t j k l 17 f x i t 1 j k s 1 j k o s l 1 l β l g w l i t b l i t x s 2 2 where h i t is the output matrix of hidden layer in the particle x i t h i t is the moore penrose generalized inverse of h i t j k is the number of training samples in the kth cluster f x i t is the fitness value of the particle x i t step 4 3 use eqs 18 and 19 to update the best known position of each individual and the global best known position of the swarm respectively 18 pbest i t pbest i t 1 if f pbest i t 1 f x i t x i t if f pbest i t 1 f x i t i 1 2 m 19 gbest t arg min f pbest 1 t f pbest 2 t f pbest m t step 4 4 use eq 12 to calculate the inertia factor of the current generation and then use eqs 10 and 11 to update the position vector and velocity vector of each particle respectively step 4 5 set t t 1 if t t max turn to step 4 2 otherwise the global best known position of the swarm will be treated as the optimal network parameters of the kth cluster step 4 6 set k k 1 if k k turn to step 4 1 otherwise stop the calculation and export the optimized elm models of all the clusters step 5 practical application after the training process is finished the clustering centers and the elm model for each cluster are obtained in the operating process the new input vector will be assigned to the most relevant group based on the minimal distance principle and then the calibrated elm model is chosen to forecast the possible output values to be mentioned the original input values should be normalized before starting a new operating process while the original output values of each elm model should be renormalized to the original range of output data 3 4 performance of ceelm in benchmark functions in this section the benchmark function han et al 2013 is chosen to compare the performance of three methods including the ceelm method elm and elm optimized by pso eelm which is given in eq 20 the numbers of training and testing samples are set as 1000 the noisy data uniformly distributed in 0 2 0 2 is added to all the training data while the testing dataset is not affected the parameters in three algorithms are determined by the trial and error strategy fig 3 draws the results obtained by the ceelm method it can be clearly found that the ceelm method has a strong ability in tracking the variation trend of nonlinear function table 1 shows the statistical indexes of the means of three methods in 20 independent runs it can be found that due to the selection of network parameters the ceelm s training time is longer than that of elm and eelm in addition the rmse value of ceelm is far less than elm and eelm which means that the classification strategy and swarm intelligence can improve the model s generalization ability thus the ceelm method has satisfying performance in the nonlinear function approximation problem 20 f x 1 if x 0 sin x x otherwise x 10 10 4 using ceelm to derivate operation rule of hydropower reservoir 4 1 deterministic reservoir operation optimization model to obtain data samples 4 1 1 objective function the scheduling process obtained from the traditional deterministic optimization model plays an important role in deriving reservoir operation rule because it can not only provide essential training samples but also be used to evaluate the performance of the derived operation rule generally for a multipurpose hydropower reservoir the objective function in eq 21 is often chosen to maximize the annual average energy production and the generation assurance rate in the scheduling horizon 21 maximize e 1 n i 1 n j 1 m p i j t i j g p i j 22 g p i j 0 if p i j p min b p min p i j c if p i j p min where e is the objective value n is the number of years m is the number of periods per year p i j is the reservoir s power output at the jth period of the ith year t i j is the total hours at the jth period of the ith year p min is firm output of the hydropower reservoir b and c are two positive intermediate parameters to guarantee the assurance rate of hydropower generation 4 1 2 operation constraints to ensure that all the variables are in the feasible zones some physical constraints should be considered in the modeling process feng et al 2018a niu et al 2019 ji et al 2015 like the water balance equation storage volume limits water spillage limits turbine discharge and power output limits 23 v i j v i j 1 i i j q i j s i j t i j i 1 n j 1 m 24 v i j min v i j v i j max i 1 n j 1 m 25 q i j min q i j q i j max i 1 n j 1 m 26 s i j min s i j s i j max i 1 n j 1 m 27 p i j min p i j p i j max i 1 n j 1 m where v i j i i j q i j and s i j are the storage volume local inflow turbine discharge and water spillage at the jth period of the ith year respectively v i j max and v i j min are the maximum and minimum storage volume at the jth period of the ith year respectively q i j max and q i j min are the maximum and minimum turbine discharge at the jth period of the ith year respectively s i j max and s i j min are the maximum and minimum water spillage at the jth period of the ith year respectively p i j max and p i j min are the maximum and minimum power output at the jth period of the ith year respectively 4 1 3 optimization method based on dynamic programming when the long term inflow series the initial storage and terminal storage are known the above optimization model will become a deterministic single objective optimization problem that can be easily solved by the dp recursive equation in eq 28 after using the dp method to deal with the model feng et al 2017c the obtained long term results can be treated as the samples to derivate the operation rule for the target reservoir 28 e i j v i j max e i j v i j v i j 1 e i j 1 v i j 1 where e i j v i j v i j 1 is the objective value at the jth period of the ith year e i j v i j is the optimal cumulative return from the jth period of the ith year to the first period 4 2 general computational framework fig 4 illustrates the framework of the ceelm method in deriving operation rule of hydropower reservoir and the overall process is composed of the following four parts 1 collect the historical operation data of hydropower reservoir via data acquisition like water level or data inversion like inflow while the deterministic operation model in section 4 1 is used to obtain the necessary samples where eq 21 is chosen as the optimization goal to maximize the total generation benefit of hydropower reservoir in the long run 2 use some methods like domain knowledge and correlation analysis to determine the input and output variables in deriving operation rule and then the k means clustering method is used to divide the input space into several disjoint sub regions 3 the elm models are developed to identify the complicated input output relationship in each sub region while the pso method is used to optimize the model parameters to be mentioned the goal is set to minimize the total error between the real and forecasted output values with respective to the related training samples and the elm model can be expressed as y f x 1 x i x n where y denotes the target output variable like power generation or total discharge x i denotes the ith input variable like previous inflow initial water level or future runoff and n denotes the number of input variables 4 the historical data collection and future runoff prediction tools are used to obtain the related input variables like antecedent inflows and future runoff and then the obtained elm models of all the clusters are selected to guide the operation of hydropower reservoir 5 case studies in this section two huge hydropower reservoirs in southwest china are selected as the studied objects to test the feasibility of the ceelm approach in deriving reservoir operation rules fig 5 shows the locations of hongjiadu and xiaowan reservoirs in addition the results obtained by the standard dp approach are treated as the samples to train the ceelm method while the traditional scheduling graph method is chosen as the benchmark method all the methods are encoded in java language and then implemented on a personal computer 5 1 case 1 xiaowan reservoir in the first case the xiaowan reservoir in lancang river is used to verify the performance of the proposed method with a total length of 2000 km and a drainage area of 91 000 km2 the lancang river is one of the most important hydropower bases providing electricity for developed areas in china as the leading reservoir in the middle and lower reaches of lancang river the xiaowan hydroplant has a total installed capacity of 4200 mw and a regulation storage capacity of about 10 billion m3 based on the monthly historical data from 1953 to 2008 the dp approach is adopted to obtain the optimal operation results then all the data is divided into three subsets to avoid the negative influence of over fitting including training dataset the first 30 years data to calibrate the model s parameters testing dataset the following 10 years data to monitor the training process as well as validation dataset the remaining data to verify the feasibility of the model based on the expertise the set of independent variables for xiaowan have 4 variables including the initial water level inflows and monthly index of the target period as well as the outflow of the last period then the power output is regarded as the dependent variable in order to obtain better regression ability we compare several different models where the number of neurons in the hidden layer changes from 3 to 14 and the model with the structure 4 11 1 with the minimum error are chosen for hydropower operation which has 4 independent variables 11 hidden neurons and 1 dependent variable then table 2 compares the average power outputs of five methods including the scheduling graph method sgm elm eelm ceelm and dp it can be found that two methods dp and ceelm tend to store more water in dry season so as to increase the power output at flood season which will make an obvious increase in the long term economic benefit of hydroplant meanwhile the statistical results of five methods in terms of the assurance rate and annual generation are listed in fig 6 where sgm is chosen as the benchmark method from fig 6 and table 2 it can be clearly found that as compared with the conventional sgm tool three different versions of elm can obtain obvious improvements in two objectives which means that an appropriate method is capable of increasing the overall benefit of hydropower reservoir besides compared with elm and eelm the ceelm method can obtain approximately 6 61 and 2 48 additional improvements in assurance rate besides the improvements in annual generation is reached up to 1 87 108 kwh and 1 79 108 kwh this case demonstrates the feasibility of the class based strategy in enhancing the performance of the optimization model moreover the statistical indexes of the ceelm method are very close to the optimized results of dp which fully shows the effectiveness of the proposed method in deriving reservoir operation rule considering the technical limitation of perfectly capturing the changing process of reservoir runoff it is rather difficult or even impossible to obtain the dp s scheduling results in practice hence the ceelm method is an alternative tool to provide decision support for the optimal operation of hydropower reservoir in the changing environment fig 7 shows the water levels obtained by several methods for xiaowan hydropower reservoir it can be observed that the water levels of dp and ceelm are almost identical while considerably different from three other methods sgm elm and eelm moreover the ceelm and dp can keep water at a high level as long as possible to further increase the whole operating efficiency of the xiaowan hydroplant hence compared with elm and eelm the proposed method has better performance to mimic the variation trend of running condition because its results are much close to the ideal situation obtained by dp 5 2 case 2 hongjiadu reservoir in the second case the hongjiadu reservoir located on wu river is chosen as the study site the wu river is originated from guizhou province and is seen as the largest branch on the right bank of yangtze river with a total length of 1037 km and a drainage area of 87920 km2 for hongjiadu reservoir the control area was reached up to 9900 km2 and the multi year average runoff at the dam site is about 160 m3 s after collecting the monthly data from 1952 to 2007 the first 40 years is used for training the models while the remaining data is used for validation of the training data the first 30 years data was used to train the model while the others for testing the initial analysis the detailed results of several methods for hongjiadu hydropower reservoir are listed in table 3 where bpnn denotes the standard back propagation neural network while ceelm k denotes that the number of categories is k in ceelm from table 3 the following three important conclusions can be drawn 1 in the bpnn the set of input variables only contain the power output at the first several periods while all the training samples are simultaneously considered in a single regression model in practice the generation scheduling of hydroplant is often affected by a variety of factors from natural environment and management organizations and it is difficult for a limited length of scheduling data to reflect the overall condition of those factors then the bpnn performance is worse than other methods highlighting the importance of data classification and training technique for neural network 2 the ceelm k family methods can provide satisfying scheduling results in both assurance rate and annual generation because the gap between our method and dp is rather small especially in generation benefit 3 the number of categories have an important influence on the performance of the ceelm method when the value of k increases from 1 to 12 two objectives climb up and then decline and the optimal objectives are achieved in the case that k is set to be 5 for a giving dataset a larger number of categories will reduce the size of training samples in each cluster and the size of historical data per group may be too small to provide enough knowledge for learning to sum up the feasibility of the proposed approach in deriving reservoir operation rules is again proved in this case and it is of great importance to choose an approximate size of categories based on the actual situation of reservoir 5 3 discussion on simulation results 5 3 1 performance analysis of the developed method from the above analysis it can be found that in different cases the result of the ceelm method is better than the sgm method and closer to dp the reason mainly lies in the organic integration of three effective strategies including the k means clustering method extreme learning machine and swarm intelligence firstly using the k means clustering method in regard to certain attributes the data samples are as homogeneous to other objects within the same group as possible while differing from objects that belong to other groups as much as possible then the complex input space is spilt into several disjoint subspaces to effectively reduce the computational complexity in each subspace secondary the elm method is adopted to capture the implicit relationship between input vectors and output vectors in each cluster which can achieve a good balance between generalization ability and learning speed thirdly the classical pso method is introduced to optimize the elm parameters which can avoid falling into local optima by carefully selecting the input hidden weights and hidden bias on the other hand the standard dp method suffers from the severe dimensionality problem as used to solve the high dimensional optimization problem for the sake of simplification it is assumed that there are k reservoirs in the hydropower system and m points are discretized in the feasible state space of each reservoir based on the permutation and combination theory the number of discrete state vectors is up to mk per period and the amount of calculations is directly proportional to m 2 k between any two periods feng et al 2017d zhang et al 2015 obviously the computational burden of the dp method shows an exponential growth with the increasing number of reservoirs resulting in high memory requirement and long execution time in other words it is difficult for dp to satisfy the physical demand of large scale hydropower operation problem thus compared with the traditional methods the ceelm method combining the merits of three effective tools have better performance in deriving monthly operation rule of hydropower reservoir besides the small data size and computational burden can reduce the execution time of the ceelm method in two cases demonstrating the running performance in terms of industry acceptance whereas our model has the potential to work for reservoirs with large scale dataset since it needs little training time the operation rule can be obtained by processing off line learning and actuation time for new instance the model s structure and parameters are fixed after training 5 3 2 relationship and differences between the elm based method and deep learning elm and deep learning are two kinds of different neural network based methods that have been widely employed to solve the practical problems like regression and classification generally elm and deep learning may achieve poor performance when the parameters are not well tuned besides there are also some differences between elm and deep learning for instance elm often has a single hidden layer huang 2014 while deep learning often refers to large and multilayer neural networks working directly on big raw data shen et al 2018 fang et al 2019 fang et al 2017 the training process of elm can be done in a very short time while deep learning often requires large sample sizes and long training time shen 2018 the hidden neurons of the elm model do not need to be tuned like deep learning deng et al 2015 besides due to elm s different roles of feature learning and clustering elm can be used as earlier layers in multilayer networks where the later layers are trained by deep learning huang 2015 thus the elm and deep learning have their own advantages and disadvantages and it is of great necessity to choose the appropriate method based on the features of the problem at hand 5 3 3 limitation and future work although this paper develops a solution framework for deriving reservoir operation rule several shortcomings still exist in the ceelm method ① the time complexity of the k means method may be larger than other clustering methods for a particular number of clusters and the obtained clustering centers are often affected by the initial values produced by random seeds besides the search capability of pso will be gradually reduced due to the loss of swarm diversity and when adopted to optimize the elm parameters the premature convergence problem rooted in pso will have a negative effect on the compactness of neural network liu et al 2019 ② for the cascade hydropower system the practicability of the presented method may be affected by some practical factors like information sharing mechanism differentiated scheduling requirement and system operation limit among upstream and downstream reservoirs ③ the developed method under the iso framework attempts to reproduce historic operation rules for reservoirs however the historical operation is not always the perfect representation of future operation because the runoff and operation priorities may change with time thus the future work can be deepened from the following aspects ① algorithm performance we can use more efficient methods to improve the quality of the clustering center and develop new strategies to find better parameter combination for each elm model ② engineering practicability we can explore new solution methods or optimization models feng et al 2019a to scientifically quantize the criterion attributions of the structured or semi structured data and incorporate the normalized information into optimized models to produce the operation rule of cascade hydropower system ③ model adaptivity we can develop effective forecasting models producing accurate runoff information to reduce the error originated from input variables and then use the latest real data to update the obtained models structure and parameters to improve the result accuracy by the above strategies the developed method is capable of providing satisfying operation result for hydropower reservoirs under changes in management strategy design or climates 6 conclusion in this paper a novel class based evolutionary extreme learning machine ceelm is presented for deriving the monthly operation rules of hydropower reservoir the ceelm method firstly uses the k means methods to divide the large scale input space into several disjoint subspaces and then uses the evolutionary extreme learning machine to mimic the complex input output relationship of samples per cluster the simulations from two hydropower reservoirs hongjiadu and xiaowan of china indicate that in different cases the ceelm method can provide approving scheduling results that is close to the optimal solutions obtained by the deterministic dynamic programming method thus a promising method to guide the scientific operation of hydropower system is presented here meanwhile in the future we will deepen the research on the improvements of the hybrid extreme learning machine in the operation optimization of cascade hydropower system declaration of competing interest the authors declare no conflict of interest acknowledgments this paper is supported by national natural science foundation of china 51709119 natural science foundation of hubei province 2018cfb573 and the fundamental research funds for the central universities hust 2017kfyxjj193 the writers would like to thank editors and reviewers for their valuable comments and suggestions 
