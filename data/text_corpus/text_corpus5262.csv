index,text
26310,the paper summarizes an end to end activity connecting the global climate modeling enterprise with users of climate information in alaska the effort included retrieval of the requisite observational datasets and model output a model evaluation and selection procedure the actual downscaling by the delta method with its inherent bias adjustment and the provision of products to a range of users through visualization software that empowers users to explore the downscaled output and its sensitivities an additional software tool enables users to examine skill metrics and relative rankings of 21 global models for alaska and six other domains in the northern hemisphere the downscaled temperatures and precipitation are made available as calendar month decadal means under three different greenhouse forcing scenarios through 2100 for more than 4000 communities in alaska and western canada the visualization package displays the uncertainties inherent in the multi model ensemble projections these uncertainties are often larger than the projected changes keywords climate modeling downscaling visualization temperature precipitation alaska 1 introduction the rapid rate of climate warming in alaska thoman and brettschneider 2016 overland et al 2018 and its consequences usgcrp 2014 have created a need for products to help plan for the future global climate models run with different greenhouse gas scenarios provide climate scientists with projections of the expected large scale response to anthropogenic climate change however regional changes are not well resolved in these low resolution models precluding the detailed landscape level projections often required for understanding impacts on local communities and resources to date the downscaled products available for planning and adaptation in alaska are severely limited the goal of this paper is to document the development and characteristics of downscaled finer resolution products and associated visualization tools recently made available in alaska development of downscaled climate for alaska has historically been limited both by a challenging physical geography and by data limitations alaska is a particularly difficult region to model with tall mountains long complex coastline fig 1 a landscape surrounded by seasonally varying sea ice and large seasonal swings in temperature all of which contribute to strong gradients in temperature and precipitation fig 2 coarse resolution global climate models gcms do not adequately represent these influences on temperature and precipitation at the landscape level so downscaling of the gcm information is necessary to provide stakeholders and decision makers with tools to address practical problems such as how climate change will affect local water resources land use and infrastructure downscaling also enables correction of model biases if the downscaling is keyed to historical observational data although the lack of long term observations for a variable and location for which downscaling is desired can limit options for downscaling this limitation is especially problematic for quantities that are not routinely measured e g solar radiation soil moisture snow water equivalent over the last decade summaries of global climate model output available for the region have enabled coarse resolution estimates of regional change both temperature and precipitation are expected to increase over alaska and as with most high latitude regions model agreement on the sign of the precipitation change is favorable regional projected changes in temperature and precipitation were calculated for a region 60 72 n 103 170 w including alaska based on output from the cmip3 climate model intercomparison project version 3 generation of models christensen et al 2007 by 2080 2099 relative to 1980 1999 and for the a1b emissions scenario across 21 gcms they indicated a median annual temperature increase of 4 5 c range 2 7 c to 6 4 c with greater increases in winter median 6 0 c and autumn median 4 8 c than spring median 3 7 c and summer median 3 0 c the models projected an increase in precipitation in all seasons with a median annual increase across models of 21 range 6 to 32 with more 28 in winter and less 14 in summer a global summary of the cmip5 coupled model intercomparison project version 5 output provided by collins et al 2013 for the ipcc intergovernmental panel on climate change fifth assessment included sub regional details the cmip5 ensemble average temperature increases for the rcp representative concentration pathway 8 5 scenario are generally higher for alaska compared to global values they range from 4 5 c in southeast aleutian alaska to 8 c or more for the north slope of alaska annual precipitation increases projected for cmip5 are generally similar throughout alaska 15 for southeast alaska and the aleutians 20 for the interior and yukon kuskokwim delta 30 for the north slope while these projections provide useful coarse scale information the model output lacks the details of the topographic and coastal influences that can be important for users one of the two primary methods of transforming coarse resolution climate information to high resolution is statistical downscaling the goal of statistical downscaling is to reproduce local climate averages over timescales of a decade to several decades this requires long term high quality observational data to develop training relationships between coarser resolution model derived variables and local conditions a statistical relationship is established between large scale climate and observed local variables temperature precipitation winds over a training period this method allows downscaling to a local point at whatever time step is most finely resolved by the local observations typically monthly or daily the most common statistically downscaled variables are temperature and precipitation although winds relative humidity ocean water temperature and snow water equivalent have also been downscaled statistically the procedure implicitly includes a bias correction of the model output the so called delta method used here and described in section 3 2 obtains a bias correction from model output and corresponding local observations for a historical period the same correction is then applied to the model s future output for the particular location other statistical downscaling methods exist including several variants of quantile mapping e g maurer et al 2007 hayhoe 2010 found that the statistical downscaling is most sensitive to the driving gcm secondly to the statistical method followed by the evaluation metric statistical downscaling is relatively computationally inexpensive allowing many models scenarios to be downscaled and the methods are generally straightforward the key weakness is that one has to assume the statistical relationship developed on the historical data will not change in the future this method also requires a robust historical climatology based on observations which are not always readily available dynamical downscaling has the same ultimate goal as statistical downscaling a finer resolution climate scenario but employs a regional climate model forced at the boundaries by the large scale climate model rather than relying on statistical relationships dynamical downscaling for alaska has been conducted using lateral boundary forcing from reanalysis output e g bieniek et al 2016 bhatt et al 2007 as well as historical and future output from climate models zhang et al 2007a 2007b lader et al 2017 dynamical downscaling provides physically consistent projections of many variables and therefore sufficient data to explore future climate variability mechanisms this method is computationally expensive limiting the number of different models scenarios that can be downscaled dynamical downscaling is also a complex process requiring a relatively high level of modeling expertise to conduct biases and other errors in the models are also problematic in dynamical downscaling the present paper describes statistical downscaling for alaska with an extension of the products into western canada it complements and extends previous uses of statistical and dynamical downscaling of global model output for the contiguous united states for example the bureau of reclamation 2013 supported a downscaling of monthly temperature and precipitation covering the contiguous 48 states at 1 8 resolution for a historical period 1970 1999 and three 30 year future time slices spanning 2010 2099 similarly nasa national aeronautics and space administration nex gddp https cds nccs nasa gov nex gddp provides daily globalcoverage at 1 4 resolution for an historical period 1950 2005 and continuous projections from 2006 2100 developed using bias correction spatial disaggregation bcsd methods after thrasher et al 2012 to derive higher resolution data for regional climate change assessments nasa coordinated a statistical downscaling of maximum and minimum air temperature and precipitation from 33 of the cmip5 climate models to a very fine 800 m grid over the contiguous united states the product known as the nex dcp30 dataset https cds nccs nasa gov nex covers the historical period 1950 2005 and 21st century 2006 2099 under four representative concentration pathways rcp emissions scenarios developed for the ipcc s fifth assessment report ar5 a supporting visualization tool the national climate change viewer nccv was developed by the usgs https www2 usgs gov climate landuse clu rd nccv asp the north american regional climate change assessment program narccap is a dynamical downscaling activity in which regional climate modeling groups performed a coordinated set of high resolution simulations of north american climate mearns et al 2009 however the narccap domain boundary passes through the middle of alaska placing the state in the buffer zone where the coarse resolution global model heavily influences the regional model s solution the coordinated regional climate downscaling experiment cordex has also performed dynamical simulations for an arctic domain that includes alaska koenigk et al 2015 although the broader arctic domain necessitates a resolution of 20 50 km the statistical downscaling described here represents a twofold extension of the activities summarized above first it extends the downscaling to alaska which was not part of the domain of the fine scale products produced for the rest of the u s second the downscaling targets communities in alaska as well as western canada by including a visualization tool for the display of the historical climate and projected changes for more than 4000 specific communities these communities range from small villages with fewer than 100 people to major population centers such as fairbanks and anchorage where the population exceeds 300 000 the intent of the project was to develop an end to end system of climate downscaling connecting the global modeling enterprise with decision makers and other users in specific locations the downscaling was performed by the scenarios network for alaska and arctic planning snap at the university of alaska fairbanks it utilized the output of the global models that participated in the coupled model intercomparison project version 5 cmip5 the downscaling project had three main components 1 selection of a subset of the cmip5 models to be downscaled for alaska 2 statistical downscaling of the coarse resolution global model output to a fine scale grid with 2 km resolution and 3 the development of the visualization tool that displays output for the 2 km 2 km pixel corresponding to the particular community selected by a user in the following sections we describe these three components 2 data and models several historical databases were used in the model evaluation and in the downscaling the european center for medium range weather forecasting s era 40 reanalysis provided the observationally based fields for the model evaluation the era 40 reanalysis spans 45 years 1958 2002 and was available on a horizontal grid with 2 5 resolution in latitude and longitude the era fields used in the model evaluation were surface air temperature precipitation and sea level pressure for the downscaling of the global climate models two station based datasets of temperature and precipitation provided the historical climatologies giving users of the downscaled products the option to choose the database on which the downscaling was based section 4 the first database is the prism climatology for alaska daly et al 2008 and subsequent updates prism consists of calendar month climatologies 1961 1990 of temperature and precipitation with a spatial resolution of 2 km over alaska and western canada prism grids represent spatial interpolations of station data taking into account elevation changes and lapse rates finer spatial scale prism products exist for alaska 771 m 1971 2000 but for consistency of the results for western canada and alaska we used the 2 km prism grids in this project the second database is the university of east anglia climate research unit s cru ts 3 2 in which monthly station observations of temperature and precipitation have been binned into grid cells at a resolution of 0 5 latitude x 0 5 longitude https crudata uea ac uk cru data hrg the historical climatological of the two databases differ slightly because their construction and our interpolation of cru ts differed in the two cases for example temperature differences of a degree c or so were not uncommon for this reason users of our downscaling tool section 4 can choose either option for the baseline climatology and can compare the two sets of results if they so desire the global climate model output is from the cmip5 archive which is the archive utilized in the fifth assessment report ar5 of the intergovernmental panel on climate change ipcc 2013 as with the observational data the model output used here consisted of monthly surface air temperature precipitation and sea level pressure fields the fields were from 1 the models historical simulations late 20th century corresponding to the observational fields and 2 the models future simulations forced by the rcp 4 5 low emission rcp 6 0 mid range and rcp 8 5 high emission scenarios because the models in the cmip5 archive were run at different resolutions all fields were interpolated to a common 2 5 2 5 grid of the era 40 reanalysis the 2 5 resolution was used for the global model evaluation and selection the downscaled products described in section 4 were based on an interpolation of the global model output from the 2 5 2 5 grids to the finer 2 km resolution of the prism climatology 3 methods 3 1 model selection while the cmip5 archive includes output from more than three dozen models several considerations led to the choice of a subset of the models for the present downscaling activity first the use of the full set of 30 40 models is computationally unwieldy and tends to preclude examinations of differences among models second not all models have archived the simulations three rcp scenarios in addition to historical runs and variables at the temporal resolution required for some downscaling applications of the approximately three dozen models in the cmip5 archive only 21 contained the needed output at the time our model evaluation was performed these 21 models are listed in table 1 third there are at best diminishing returns from the inclusion of models beyond a total of 10 20 in part because models rely on similar physics and are thus not entirely independent sanderson et al 2015 finally there are indications although not conclusive evidence that retaining a subset of the models deemed to be best for a particular application can enhance the utility of the results the latter consideration has some precedents in the literature including some for arctic research but calls for caveats that we discuss below in previous applications to the arctic wang and overland 2009 chose a subset of cmip3 models on the basis of their ability to capture the seasonal cycle and mean september extent of arctic sea ice in order to optimize projections of future sea ice changes rogers et al 2015 used a two step model selection algorithm to show that the timing of an ice free arctic in september advances from 2055 to 2034 when the number of cmip5 models is filtered from a full set to the subset of five models that best capture recent sea ice trends and other hindcast metrics in an attribution study of recent arctic temperature variations fyfe et al 2013 chose a subset of five cmip5 models on the basis of their simulations of arctic temperature trends over three historical timeslices the number of models retained in these studies is consistent with walsh et al 2008 finding based on multimodel composites of historical arctic simulations that the mean absolute errors decrease as the number of best performing models in a composite increases to 5 8 but increases as additional poorer performing models are included in the composites nevertheless model selection is fraught with risks because the best performing models vary with the choice of the criterion for validation moreover different models perform best for different variables regions and other choices in validation methodology a case may be made that there is still merit in knutti et al 2010 assessment that there is little agreement on metrics to separate good from bad models given this lack of agreement our decision to utilize only a subset of the cmip5 models was based on the more practical considerations listed in the preceding paragraph computational efficiency and availability of output our strategy was to choose the model subset on the basis of the models ability to reproduce the seasonal cycle of the recent historical climate of alaska and the surrounding area in evaluating the models historical performance for the alaskan region the core statistic of the validation was a root mean square error rmse of the differences between time averaged model output for each grid point and calendar month and the observationally constrained era 40 reanalysis era 40 directly assimilates observed air temperature and sea level pressure observations into a product spanning 1958 through 2002 precipitation is computed by the model used in the data assimilation data from 1958 to 2000 were used here for the comparative evaluation of the global climate models gcms for each of the 21 cmip5 models we calculated the monthly root mean square error rmse for each of three variables surface air temperature precipitation and sea level pressure we tested the sensitivity of the model ranking to the choice of the error metric by repeating the calculations using bias corrected rmse mean absolute error mae and bias corrected mae the bias correction removed the domain average error from the error at each grid cell the model selection procedure used here has been made available through a web based application at https uasnap shinyapps io ar5eval this application incorporates various degrees of freedom choice of variable domain evaluation metric described below users can select any of the four error metrics through the model evaluation web application the alaskan domain for the model evaluation covers the area 52 72 n 130 180 w for comparison the same error statistics were also evaluated for the following six other domains fig 3 canada 49 72 n 52 141 w combined alaska canada 49 72 n 52 172 w the 48 contiguous united states 25 49 n 66 125 w the pacific islands 17 s 25 n 152 228 w and two circumpolar domains 60 90 n and 20 90 n for each domain the output from each model was interpolated to the 2 5 2 5 latitude x longitude grid of the era reanalysis the skill of the models was evaluated over all the domains of fig 3 and the skill over the different domains is compared in the results below however we focus on the alaska domain in our illustration of the methods used for skill evaluation and model selection as well as in the examples of the products presented in section 4 this focus on alaska stems from the availability of complementary downscaled information for canada produced by the pacific climate impacts consortium pcic the pcic methods and products are accessible at https www pacificclimate org data statistically downscaled climate scenarios while the model evaluation procedure has some commonalty with that used by walsh et al 2008 to select a subset of models from the previous generation cmip3 of global climate models there are several notable extensions of the procedure in the present application first the ranking of models was based on the models simulations of three variables surface air temperature precipitation sea level pressure rather than only the first two sea level pressure is a proxy for the atmospheric circulation at the surface second rather than summing ranks over all calendar months and variables as in walsh et al 2008 the ranking was performed only after a summation of the standardized rmses over all calendar months and variables third the robustness of the rmses was tested by a bootstrapping procedure in which repeated 1000 estimates of a particular rmse were calculated based on randomly selected grid cells with replacement from the domain under consideration the number of points randomly selected was equal to the total number of grid cells in the domain figs 4 and 5 provides examples of rmse values for january temperature and july precipitation from the 21 models over the alaska domain while the distributions for the different models overlap there is clear separation of the models with lower rmse versus larger rmse especially in the case of january temperature in general the distributions for the models have the least overlap for sea level pressure and the greatest overlap for precipitation in all cases the mean of the distributions is nearly identical to the rmse obtained from the original not resampled grid however plots such as figs 4 and 5 provide a measure of the robustness of the rankings of the models the lower panels of these figures show the probability based on the 1000 member sample of rmses for each model that a particular model will rank in the top five based on the rmse metric for that particular variable the probabilities are essentially 100 for the top three models in the case of january temperature and the top two models in the case of july precipitation beyond the 8th ranking model for temperature and the 10th ranking model for precipitation the probabilities that resampling would place a model in the top five are essentially zero rmse values for the three variables temperature precipitation and sea level pressure were standardized and summed and this cumulative sum was the basis for ranking the models from 1 smallest rmse to 21 largest rmse there was reasonably good consistency from one calendar month to the next in the relative rankings of the models fig 6 shows the relative errors rmses of all models in all calendar months for the alaska domain the individual cells in the error matrix are shaded with the lightest cells indicating the smallest errors the numbers in the cells are the model ranks for the calendar month the models are listed from top to bottom according to their ranks aggregated over the twelve calendar months these aggregate ranks formed the basis for our model selection the evaluation procedure was performed for the eight domains listed above for each domain aggregate ranks based on both rmse and mae mean absolute error were evaluated as shown in table 2 the domains with common areas e g alaska alaska canada 60 90 n generally had several models in common among the best performing models e g with smallest rmses and maes there is much less overlap between the lists of best performing models for the smaller and larger domains reinforcing the previous caveat that the best performing models vary by region the choice of the error metric rmse vs mae has only a minor effect on the rankings finally although not shown here there was also a tendency for the same models to have smaller rmses of all three variables in a particular domain although there were exceptions especially for precipitation as indicated by figs 4 and 5 several models had substantially smaller systematic errors than others the models also vary substantially in their projections of future changes over the alaska region fig 7 this combination of historical and future spread raises the possibility that the choice of a subset of models might offer a viable approach to narrowing the uncertainty and obtaining more robust estimates of future climate change in regions such as alaska subject to the caveats noted earlier we further evaluated this strategy by examining the errors generated by compositing subsets of n models selected from the full set of 21 the model selection was done in two ways 1 all combinations of n models and 2 the best n models based on the rmse metric fig 8 shows that the average error for a single variable temperature precipitation sea level pressure generally increases orange lines as one moves down the list of models that rank successively lower by the aggregate metric when the composite is based on the n randomly selected models the error averaged over all possible combinations of n models decreases monotonically from n 1 to n 21 black lines with range indicated by shading the error of the n model composite composed of the single set of n best performing models reaches a minimum somewhere between n 1 and n 21 blue lines for the individual variables fig 8a c while the decrease of the error with increasing n is not monotonic there are indications of a minimum in the range of n 4 to n 6 however the minimum is ill defined for the integrated three variable metric fig 8d the values of n at which the minimum is reached vary with region as well as with the variable nevertheless even in the case of multimodality where a single choice of optimal composite size may not be clear there is still a prominent decrease in rmse during the initial compositing of several models on the basis of these results and in the interest of computational economy we chose n 5 for the alaskan downscaling application 3 2 downscaling by the delta method the downscaling procedure is an application of the so called delta method in which a model s future change delta in a variable at a particular location and calendar month is added to the historical mean value of the same variable for the same location and calendar month the delta is computed as the model s change from the period of the historical climatology 1961 1990 in the case of the prism data to a future time slice e g the 2050s this delta is added to the higher resolution observationally based climatology thereby effectively bias correcting the model s output a key assumption in this procedure is that the model s bias is the same in the future time slice as in the historical reference period in all likelihood the bias will undergo some change over time thereby limiting the validity of the delta method and other statistical downscaling methods nevertheless the delta method has been widely used in downscaling applications and its validity has been found to be comparable to that of more sophisticated downscaling methods when applied to monthly fields although this is not the case for daily fields and their corresponding extremes hayhoe 2010 4 downscaled products 4 1 examples of community charts many users requiring climate information for planning or adaptation purposes are located in villages or larger population centers given our target of the north american arctic we therefore performed the downscaling for the largest available collection of community locations covering all of alaska the yukon territory british columbia alberta saskatchewan and manitoba this resulted in downscaled temperature and precipitation data for more than 4000 communities in order to illustrate the fusion of the model selection and the downscaling we focus here on alaska and apply the downscaling methodology to the five models that ranked highest by the rmse metric in the historical simulations across the alaska domain cf table 2 mri cgcm3 giss e2 r gfdl cm3 ipsl cm5a lr and ncar ccsm4 for every year and calendar month the downscaling consisted of calculating the delta value for each gcm grid cell interpolating to the same spatial resolution as the high resolution baseline climatology followed by adding these high resolution deltas to the same high resolution climatology the resulting values for the high resolution grid cell containing a particular community became the downscaled values for that community downscaled monthly values were then averaged across decadal time slices 2010 2019 2020 2029 2090 2099 the downscaled values were computed separately using the historical baselines from prism and cru ts 3 2 for 1961 1990 and separately for the rcp 4 5 rcp 6 0 and rcp 8 5 forcing scenarios in order to provide an indication of the sensitivities of the downscaled products the downscaled values for any particular community are the values for the 2 km grid cell containing that community while there is no consideration of the measurement site s location within the 2 km grid cell e g valley vs mountain the use of fine 2 km grid cells reduces the impact of within call variability for most communities however the same cannot be said for the model grid cells typically 100 200 km in size for which the average elevation or land sea fraction may be a poor representation of the community s location for this reason the bias correction inherent in the downscaling is an important attribute of the downscaling procedure fig 9 is an example of the downscaled temperatures for kotzebue a community on the northwest coast of alaska in this example the downscaling is based on the prism climatology and the rcp 6 0 mid range emissions scenario the results are shown for each calendar month x axis the gray bar represents the historical 1961 1990 climatology based on observational data while the colored bars are means for individual decades of the 21st century based on the addition of the models deltas for those decades consistent with the forcing warming is apparent in all calendar months however the warming is greater in the cold season november march than in the warm season moreover the inter model spread is generally larger than the overall change in the 5 model composite mean pointing to the range in projected changes associated with the combination of internal variability and across model differences in formulation resolution components that are coupled and other model characteristics the across model spread decreases as the averaging is performed over time slices longer than a single decade e g over 30 year period pointing to the influence of internal variability on decadal averages as a second example fig 10 shows the downscaled precipitation for mcgrath a small community in interior alaska in this case the precipitation is for the rcp 8 5 high emission scenario and is based on the cru 3 2 historical climatology we display the results based on cru 3 2 and not prism in order to avoid redundant graphics while illustrating the choices available to users there is no evidence that either of the precipitation climatologies is better for a particular region the monthly clusters of bars show that precipitation is projected to increase in all calendar months with the largest increases in the warm season corroborating similar results in walsh et al 2008 and stewart et al 2013 the across model spread is even larger than in fig 9 indicative of a general tendency for greater spread in precipitation projections than in temperature projections in this case the across model uncertainties are far larger than the changes in the composite five model mean values the spread generally increases with time indicating greater uncertainty in the late century projections than in the mid century projections the 5 model mean projections in fig 10 even show occasional decreases from one time slice to the next e g the blue bars for june and july pointing to a role of internal variability in the decadal means because internal variability is a source of uncertainty in addition to the uncertainty associated with across model differences in formulation the future changes have the character of a bumpy ride rather than a steady progression especially in the case of precipitation the values downscaled for each model were based on a single simulation ensemble member the across model spread would decrease if the estimates were based on averages of multiple ensemble members from each model rather than a single ensemble member since internal variations would be reduced by averaging over multiple simulations 4 2 user interface a key aim of the snap downscaling was the facilitation of use by stakeholders for this reason a user interface was developed not only to provide public access to the products but to encourage users to visualize and experiment with the downscaled projections for their particular locations of interest user driven exploration of the sensitivities of the output was one of the priorities in the design and implementation of the user interface this interface allows users to select different options for various calculation and display parameters the variable temperature or precipitation the units f or c inches or millimeters the reference database for the historical period prism cru 3 2 the forcing scenario rcp 4 5 rcp 6 0 rcp 8 5 and the inclusion or not of the across model ranges in the display of the projections fig 11 is a screen capture of the user interface which also provides the option to download a user created chart for a particular community as an example of the sensitivities that a user can explore fig 12 shows a comparison of the projected changes of temperature f at point hope alaska under the rcp 4 5 upper and rcp 8 5 lower scenarios the warming shows clear signs of leveling off in the rcp 4 5 scenario but continues to increase in the rcp 8 5 scenario the difference in warming between the two scenarios is approximately 10 f in the winter months december february perhaps more importantly the monthly mean temperatures in the transition months may october rise above freezing by 2100 under rcp 8 5 while remaining at or below freezing under rcp 4 5 implications for duration of the ice free season which affects over land travel as well as offshore activities e g whaling subsistence hunting are significant in coastal areas where daily activities are closely tied to the state of the land and ocean surfaces as in the preceding examples fig 12 shows the results based on only one climatology prism in order to avoid redundant graphics there is no evidence that either of the temperature climatologies is better for a particular region as a second example of exploration of sensitivities fig 13 shows the downscaled precipitation values for juneau a relatively wet location in southeast alaska based on the prism upper and cru lower reference climatologies in both cases the forcing is the rcp 8 5 scenario while the projected changes are the same in the two cases the actual amounts are larger with prism which has a wetter reference climatology for juneau during the late summer and autumn months which are juneau s wettest the differences in the two climatologies are as large as 5 cm 2 inches such differences are comparable to the projected changes from the late 1900s to the 2090s pointing to the importance of a robust base climatology in the use of downscaled climate projections the snap visualization tool for the community charts has been accessed by users within and outside of alaska it has provided reference material for the alaska section of the third u s national climate assessment stewart et al 2013 and it has provided input to climate adaptation planning efforts for alaskan communities nome eskimo community 2017 feedback from users has led to additions to the original capabilities including the capability to download user generated charts 5 software and data availability all downscaled climate data and software tools discussed in this paper were produced by the scenarios network for alaska and arctic planning snap and are available under a creative commons 4 0 international license https creativecommons org licenses by 4 0 where only attribution to snap is needed with no additional restrictions allowed the ar5 gcm evaluation tool https uasnap shinyapps io ar5eval was developed in january of 2016 using the r programming language https cran r project org shiny web application framework https shiny rstudio com the developer is matthew leonawicz mfleonawicz alaska edu the only hardware requirement is a computer with an internet connection there are no special software requirements and there is no charge for public users the raw data utilized by this app were obtained from the coupled model intercomparison project version 5 cmip5 http cmip pcmdi llnl gov cmip5 data portal html and the university of east anglia s climate research unit cru http www cru uea ac uk data the snap community charts including the downscaling software and the visualization tool https www snap uaf edu sites all modules snap community charts charts php were developed in 2009 and updated in 2015 to display the latest cmip5 climate data the community charts utilize jquery jqueryui highcharts mysql and php programming languages snap downscaled monthly climate data are available in geotiff format for download from https www snap uaf edu tools data downloads 6 conclusion the project described here represents an end to end activity connecting the global climate modeling enterprise with planners decision makers and other users in alaska the effort has included retrieval of the requisite observational datasets and model output a model evaluation and selection procedure targeted at the alaska region the actual downscaling by the delta method with its inherent bias adjustment and the provision of the data to a range of users through a visualization tool that empowers users to explore the downscaled output and its sensitivities the website s documentation of the visualization tool provides users with a summary of the main components of the downscaling but there have also been frequent requests for a reference that can be cited the present paper responds to those requests because the downscaled products have been accessible to users for several years lessons learned have begun to accumulate one lesson is that users desire the actual plots or digital data for presentation purposes or for supporting statements about potential climate change in their area second there is need for caution with regard to the internal variability that can affect decadal means but can also be obscured by compositing of projections from several models five in this case in recognition of this need for caution the decision was made to include the across model range indicators as a user option however our experience has been that many users do not realize that these range indicators include uncertainties due to both internal variability and differences in model formulations it has been necessary to make this point in a more complete framework of uncertainties in future projections e g hodson et al 2013 finally the downscaled products and visualization tool have proven to be useful for messaging about the role of human activities especially alternative futures as they may result from different emissions scenarios rcp 4 5 vs rcp 8 5 the contrasting options of the user interface we did not include the rcp 2 6 scenario because the emissions reductions with negative emissions by 2100 are so extreme that this scenario is rapidly becoming impossible to achieve using the other three primary rcp scenarios the community charts take this scenario dependence down to the local scale that is of greatest interest and concern to a user the messages conveyed by the charts are consistent with broader depictions of arctic change and overland et al 2014 adaptation and mitigation timeframes 1 climate change especially warming is already built into the system over the next few decades even under emissions reduction scenarios so adaptation will be necessary and 2 the choice of the emissions scenarios substantially alters the trajectory of local climate in the second half of the century so mitigation will ultimately make a difference in a community s future climate acknowledgments this work was supported by the alaska climate science center through a cooperative agreement g10ac00588 from the usgs and by noaa s climate program office through grants na15oar4310169 and na16oar4310162 we acknowledge the world climate research programme s working group on coupled modeling which is responsible for cmip and we thank all the climate modeling groups for producing and making available their model output for cmip the us department of energy s program for climate model diagnosis and intercomparison provides coordinating support and leads development of software infrastructure in partnership with the global organization for earth system science portals finally we thank an anonymous reviewer for insightful and constructive comments on the original submission any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government 
26310,the paper summarizes an end to end activity connecting the global climate modeling enterprise with users of climate information in alaska the effort included retrieval of the requisite observational datasets and model output a model evaluation and selection procedure the actual downscaling by the delta method with its inherent bias adjustment and the provision of products to a range of users through visualization software that empowers users to explore the downscaled output and its sensitivities an additional software tool enables users to examine skill metrics and relative rankings of 21 global models for alaska and six other domains in the northern hemisphere the downscaled temperatures and precipitation are made available as calendar month decadal means under three different greenhouse forcing scenarios through 2100 for more than 4000 communities in alaska and western canada the visualization package displays the uncertainties inherent in the multi model ensemble projections these uncertainties are often larger than the projected changes keywords climate modeling downscaling visualization temperature precipitation alaska 1 introduction the rapid rate of climate warming in alaska thoman and brettschneider 2016 overland et al 2018 and its consequences usgcrp 2014 have created a need for products to help plan for the future global climate models run with different greenhouse gas scenarios provide climate scientists with projections of the expected large scale response to anthropogenic climate change however regional changes are not well resolved in these low resolution models precluding the detailed landscape level projections often required for understanding impacts on local communities and resources to date the downscaled products available for planning and adaptation in alaska are severely limited the goal of this paper is to document the development and characteristics of downscaled finer resolution products and associated visualization tools recently made available in alaska development of downscaled climate for alaska has historically been limited both by a challenging physical geography and by data limitations alaska is a particularly difficult region to model with tall mountains long complex coastline fig 1 a landscape surrounded by seasonally varying sea ice and large seasonal swings in temperature all of which contribute to strong gradients in temperature and precipitation fig 2 coarse resolution global climate models gcms do not adequately represent these influences on temperature and precipitation at the landscape level so downscaling of the gcm information is necessary to provide stakeholders and decision makers with tools to address practical problems such as how climate change will affect local water resources land use and infrastructure downscaling also enables correction of model biases if the downscaling is keyed to historical observational data although the lack of long term observations for a variable and location for which downscaling is desired can limit options for downscaling this limitation is especially problematic for quantities that are not routinely measured e g solar radiation soil moisture snow water equivalent over the last decade summaries of global climate model output available for the region have enabled coarse resolution estimates of regional change both temperature and precipitation are expected to increase over alaska and as with most high latitude regions model agreement on the sign of the precipitation change is favorable regional projected changes in temperature and precipitation were calculated for a region 60 72 n 103 170 w including alaska based on output from the cmip3 climate model intercomparison project version 3 generation of models christensen et al 2007 by 2080 2099 relative to 1980 1999 and for the a1b emissions scenario across 21 gcms they indicated a median annual temperature increase of 4 5 c range 2 7 c to 6 4 c with greater increases in winter median 6 0 c and autumn median 4 8 c than spring median 3 7 c and summer median 3 0 c the models projected an increase in precipitation in all seasons with a median annual increase across models of 21 range 6 to 32 with more 28 in winter and less 14 in summer a global summary of the cmip5 coupled model intercomparison project version 5 output provided by collins et al 2013 for the ipcc intergovernmental panel on climate change fifth assessment included sub regional details the cmip5 ensemble average temperature increases for the rcp representative concentration pathway 8 5 scenario are generally higher for alaska compared to global values they range from 4 5 c in southeast aleutian alaska to 8 c or more for the north slope of alaska annual precipitation increases projected for cmip5 are generally similar throughout alaska 15 for southeast alaska and the aleutians 20 for the interior and yukon kuskokwim delta 30 for the north slope while these projections provide useful coarse scale information the model output lacks the details of the topographic and coastal influences that can be important for users one of the two primary methods of transforming coarse resolution climate information to high resolution is statistical downscaling the goal of statistical downscaling is to reproduce local climate averages over timescales of a decade to several decades this requires long term high quality observational data to develop training relationships between coarser resolution model derived variables and local conditions a statistical relationship is established between large scale climate and observed local variables temperature precipitation winds over a training period this method allows downscaling to a local point at whatever time step is most finely resolved by the local observations typically monthly or daily the most common statistically downscaled variables are temperature and precipitation although winds relative humidity ocean water temperature and snow water equivalent have also been downscaled statistically the procedure implicitly includes a bias correction of the model output the so called delta method used here and described in section 3 2 obtains a bias correction from model output and corresponding local observations for a historical period the same correction is then applied to the model s future output for the particular location other statistical downscaling methods exist including several variants of quantile mapping e g maurer et al 2007 hayhoe 2010 found that the statistical downscaling is most sensitive to the driving gcm secondly to the statistical method followed by the evaluation metric statistical downscaling is relatively computationally inexpensive allowing many models scenarios to be downscaled and the methods are generally straightforward the key weakness is that one has to assume the statistical relationship developed on the historical data will not change in the future this method also requires a robust historical climatology based on observations which are not always readily available dynamical downscaling has the same ultimate goal as statistical downscaling a finer resolution climate scenario but employs a regional climate model forced at the boundaries by the large scale climate model rather than relying on statistical relationships dynamical downscaling for alaska has been conducted using lateral boundary forcing from reanalysis output e g bieniek et al 2016 bhatt et al 2007 as well as historical and future output from climate models zhang et al 2007a 2007b lader et al 2017 dynamical downscaling provides physically consistent projections of many variables and therefore sufficient data to explore future climate variability mechanisms this method is computationally expensive limiting the number of different models scenarios that can be downscaled dynamical downscaling is also a complex process requiring a relatively high level of modeling expertise to conduct biases and other errors in the models are also problematic in dynamical downscaling the present paper describes statistical downscaling for alaska with an extension of the products into western canada it complements and extends previous uses of statistical and dynamical downscaling of global model output for the contiguous united states for example the bureau of reclamation 2013 supported a downscaling of monthly temperature and precipitation covering the contiguous 48 states at 1 8 resolution for a historical period 1970 1999 and three 30 year future time slices spanning 2010 2099 similarly nasa national aeronautics and space administration nex gddp https cds nccs nasa gov nex gddp provides daily globalcoverage at 1 4 resolution for an historical period 1950 2005 and continuous projections from 2006 2100 developed using bias correction spatial disaggregation bcsd methods after thrasher et al 2012 to derive higher resolution data for regional climate change assessments nasa coordinated a statistical downscaling of maximum and minimum air temperature and precipitation from 33 of the cmip5 climate models to a very fine 800 m grid over the contiguous united states the product known as the nex dcp30 dataset https cds nccs nasa gov nex covers the historical period 1950 2005 and 21st century 2006 2099 under four representative concentration pathways rcp emissions scenarios developed for the ipcc s fifth assessment report ar5 a supporting visualization tool the national climate change viewer nccv was developed by the usgs https www2 usgs gov climate landuse clu rd nccv asp the north american regional climate change assessment program narccap is a dynamical downscaling activity in which regional climate modeling groups performed a coordinated set of high resolution simulations of north american climate mearns et al 2009 however the narccap domain boundary passes through the middle of alaska placing the state in the buffer zone where the coarse resolution global model heavily influences the regional model s solution the coordinated regional climate downscaling experiment cordex has also performed dynamical simulations for an arctic domain that includes alaska koenigk et al 2015 although the broader arctic domain necessitates a resolution of 20 50 km the statistical downscaling described here represents a twofold extension of the activities summarized above first it extends the downscaling to alaska which was not part of the domain of the fine scale products produced for the rest of the u s second the downscaling targets communities in alaska as well as western canada by including a visualization tool for the display of the historical climate and projected changes for more than 4000 specific communities these communities range from small villages with fewer than 100 people to major population centers such as fairbanks and anchorage where the population exceeds 300 000 the intent of the project was to develop an end to end system of climate downscaling connecting the global modeling enterprise with decision makers and other users in specific locations the downscaling was performed by the scenarios network for alaska and arctic planning snap at the university of alaska fairbanks it utilized the output of the global models that participated in the coupled model intercomparison project version 5 cmip5 the downscaling project had three main components 1 selection of a subset of the cmip5 models to be downscaled for alaska 2 statistical downscaling of the coarse resolution global model output to a fine scale grid with 2 km resolution and 3 the development of the visualization tool that displays output for the 2 km 2 km pixel corresponding to the particular community selected by a user in the following sections we describe these three components 2 data and models several historical databases were used in the model evaluation and in the downscaling the european center for medium range weather forecasting s era 40 reanalysis provided the observationally based fields for the model evaluation the era 40 reanalysis spans 45 years 1958 2002 and was available on a horizontal grid with 2 5 resolution in latitude and longitude the era fields used in the model evaluation were surface air temperature precipitation and sea level pressure for the downscaling of the global climate models two station based datasets of temperature and precipitation provided the historical climatologies giving users of the downscaled products the option to choose the database on which the downscaling was based section 4 the first database is the prism climatology for alaska daly et al 2008 and subsequent updates prism consists of calendar month climatologies 1961 1990 of temperature and precipitation with a spatial resolution of 2 km over alaska and western canada prism grids represent spatial interpolations of station data taking into account elevation changes and lapse rates finer spatial scale prism products exist for alaska 771 m 1971 2000 but for consistency of the results for western canada and alaska we used the 2 km prism grids in this project the second database is the university of east anglia climate research unit s cru ts 3 2 in which monthly station observations of temperature and precipitation have been binned into grid cells at a resolution of 0 5 latitude x 0 5 longitude https crudata uea ac uk cru data hrg the historical climatological of the two databases differ slightly because their construction and our interpolation of cru ts differed in the two cases for example temperature differences of a degree c or so were not uncommon for this reason users of our downscaling tool section 4 can choose either option for the baseline climatology and can compare the two sets of results if they so desire the global climate model output is from the cmip5 archive which is the archive utilized in the fifth assessment report ar5 of the intergovernmental panel on climate change ipcc 2013 as with the observational data the model output used here consisted of monthly surface air temperature precipitation and sea level pressure fields the fields were from 1 the models historical simulations late 20th century corresponding to the observational fields and 2 the models future simulations forced by the rcp 4 5 low emission rcp 6 0 mid range and rcp 8 5 high emission scenarios because the models in the cmip5 archive were run at different resolutions all fields were interpolated to a common 2 5 2 5 grid of the era 40 reanalysis the 2 5 resolution was used for the global model evaluation and selection the downscaled products described in section 4 were based on an interpolation of the global model output from the 2 5 2 5 grids to the finer 2 km resolution of the prism climatology 3 methods 3 1 model selection while the cmip5 archive includes output from more than three dozen models several considerations led to the choice of a subset of the models for the present downscaling activity first the use of the full set of 30 40 models is computationally unwieldy and tends to preclude examinations of differences among models second not all models have archived the simulations three rcp scenarios in addition to historical runs and variables at the temporal resolution required for some downscaling applications of the approximately three dozen models in the cmip5 archive only 21 contained the needed output at the time our model evaluation was performed these 21 models are listed in table 1 third there are at best diminishing returns from the inclusion of models beyond a total of 10 20 in part because models rely on similar physics and are thus not entirely independent sanderson et al 2015 finally there are indications although not conclusive evidence that retaining a subset of the models deemed to be best for a particular application can enhance the utility of the results the latter consideration has some precedents in the literature including some for arctic research but calls for caveats that we discuss below in previous applications to the arctic wang and overland 2009 chose a subset of cmip3 models on the basis of their ability to capture the seasonal cycle and mean september extent of arctic sea ice in order to optimize projections of future sea ice changes rogers et al 2015 used a two step model selection algorithm to show that the timing of an ice free arctic in september advances from 2055 to 2034 when the number of cmip5 models is filtered from a full set to the subset of five models that best capture recent sea ice trends and other hindcast metrics in an attribution study of recent arctic temperature variations fyfe et al 2013 chose a subset of five cmip5 models on the basis of their simulations of arctic temperature trends over three historical timeslices the number of models retained in these studies is consistent with walsh et al 2008 finding based on multimodel composites of historical arctic simulations that the mean absolute errors decrease as the number of best performing models in a composite increases to 5 8 but increases as additional poorer performing models are included in the composites nevertheless model selection is fraught with risks because the best performing models vary with the choice of the criterion for validation moreover different models perform best for different variables regions and other choices in validation methodology a case may be made that there is still merit in knutti et al 2010 assessment that there is little agreement on metrics to separate good from bad models given this lack of agreement our decision to utilize only a subset of the cmip5 models was based on the more practical considerations listed in the preceding paragraph computational efficiency and availability of output our strategy was to choose the model subset on the basis of the models ability to reproduce the seasonal cycle of the recent historical climate of alaska and the surrounding area in evaluating the models historical performance for the alaskan region the core statistic of the validation was a root mean square error rmse of the differences between time averaged model output for each grid point and calendar month and the observationally constrained era 40 reanalysis era 40 directly assimilates observed air temperature and sea level pressure observations into a product spanning 1958 through 2002 precipitation is computed by the model used in the data assimilation data from 1958 to 2000 were used here for the comparative evaluation of the global climate models gcms for each of the 21 cmip5 models we calculated the monthly root mean square error rmse for each of three variables surface air temperature precipitation and sea level pressure we tested the sensitivity of the model ranking to the choice of the error metric by repeating the calculations using bias corrected rmse mean absolute error mae and bias corrected mae the bias correction removed the domain average error from the error at each grid cell the model selection procedure used here has been made available through a web based application at https uasnap shinyapps io ar5eval this application incorporates various degrees of freedom choice of variable domain evaluation metric described below users can select any of the four error metrics through the model evaluation web application the alaskan domain for the model evaluation covers the area 52 72 n 130 180 w for comparison the same error statistics were also evaluated for the following six other domains fig 3 canada 49 72 n 52 141 w combined alaska canada 49 72 n 52 172 w the 48 contiguous united states 25 49 n 66 125 w the pacific islands 17 s 25 n 152 228 w and two circumpolar domains 60 90 n and 20 90 n for each domain the output from each model was interpolated to the 2 5 2 5 latitude x longitude grid of the era reanalysis the skill of the models was evaluated over all the domains of fig 3 and the skill over the different domains is compared in the results below however we focus on the alaska domain in our illustration of the methods used for skill evaluation and model selection as well as in the examples of the products presented in section 4 this focus on alaska stems from the availability of complementary downscaled information for canada produced by the pacific climate impacts consortium pcic the pcic methods and products are accessible at https www pacificclimate org data statistically downscaled climate scenarios while the model evaluation procedure has some commonalty with that used by walsh et al 2008 to select a subset of models from the previous generation cmip3 of global climate models there are several notable extensions of the procedure in the present application first the ranking of models was based on the models simulations of three variables surface air temperature precipitation sea level pressure rather than only the first two sea level pressure is a proxy for the atmospheric circulation at the surface second rather than summing ranks over all calendar months and variables as in walsh et al 2008 the ranking was performed only after a summation of the standardized rmses over all calendar months and variables third the robustness of the rmses was tested by a bootstrapping procedure in which repeated 1000 estimates of a particular rmse were calculated based on randomly selected grid cells with replacement from the domain under consideration the number of points randomly selected was equal to the total number of grid cells in the domain figs 4 and 5 provides examples of rmse values for january temperature and july precipitation from the 21 models over the alaska domain while the distributions for the different models overlap there is clear separation of the models with lower rmse versus larger rmse especially in the case of january temperature in general the distributions for the models have the least overlap for sea level pressure and the greatest overlap for precipitation in all cases the mean of the distributions is nearly identical to the rmse obtained from the original not resampled grid however plots such as figs 4 and 5 provide a measure of the robustness of the rankings of the models the lower panels of these figures show the probability based on the 1000 member sample of rmses for each model that a particular model will rank in the top five based on the rmse metric for that particular variable the probabilities are essentially 100 for the top three models in the case of january temperature and the top two models in the case of july precipitation beyond the 8th ranking model for temperature and the 10th ranking model for precipitation the probabilities that resampling would place a model in the top five are essentially zero rmse values for the three variables temperature precipitation and sea level pressure were standardized and summed and this cumulative sum was the basis for ranking the models from 1 smallest rmse to 21 largest rmse there was reasonably good consistency from one calendar month to the next in the relative rankings of the models fig 6 shows the relative errors rmses of all models in all calendar months for the alaska domain the individual cells in the error matrix are shaded with the lightest cells indicating the smallest errors the numbers in the cells are the model ranks for the calendar month the models are listed from top to bottom according to their ranks aggregated over the twelve calendar months these aggregate ranks formed the basis for our model selection the evaluation procedure was performed for the eight domains listed above for each domain aggregate ranks based on both rmse and mae mean absolute error were evaluated as shown in table 2 the domains with common areas e g alaska alaska canada 60 90 n generally had several models in common among the best performing models e g with smallest rmses and maes there is much less overlap between the lists of best performing models for the smaller and larger domains reinforcing the previous caveat that the best performing models vary by region the choice of the error metric rmse vs mae has only a minor effect on the rankings finally although not shown here there was also a tendency for the same models to have smaller rmses of all three variables in a particular domain although there were exceptions especially for precipitation as indicated by figs 4 and 5 several models had substantially smaller systematic errors than others the models also vary substantially in their projections of future changes over the alaska region fig 7 this combination of historical and future spread raises the possibility that the choice of a subset of models might offer a viable approach to narrowing the uncertainty and obtaining more robust estimates of future climate change in regions such as alaska subject to the caveats noted earlier we further evaluated this strategy by examining the errors generated by compositing subsets of n models selected from the full set of 21 the model selection was done in two ways 1 all combinations of n models and 2 the best n models based on the rmse metric fig 8 shows that the average error for a single variable temperature precipitation sea level pressure generally increases orange lines as one moves down the list of models that rank successively lower by the aggregate metric when the composite is based on the n randomly selected models the error averaged over all possible combinations of n models decreases monotonically from n 1 to n 21 black lines with range indicated by shading the error of the n model composite composed of the single set of n best performing models reaches a minimum somewhere between n 1 and n 21 blue lines for the individual variables fig 8a c while the decrease of the error with increasing n is not monotonic there are indications of a minimum in the range of n 4 to n 6 however the minimum is ill defined for the integrated three variable metric fig 8d the values of n at which the minimum is reached vary with region as well as with the variable nevertheless even in the case of multimodality where a single choice of optimal composite size may not be clear there is still a prominent decrease in rmse during the initial compositing of several models on the basis of these results and in the interest of computational economy we chose n 5 for the alaskan downscaling application 3 2 downscaling by the delta method the downscaling procedure is an application of the so called delta method in which a model s future change delta in a variable at a particular location and calendar month is added to the historical mean value of the same variable for the same location and calendar month the delta is computed as the model s change from the period of the historical climatology 1961 1990 in the case of the prism data to a future time slice e g the 2050s this delta is added to the higher resolution observationally based climatology thereby effectively bias correcting the model s output a key assumption in this procedure is that the model s bias is the same in the future time slice as in the historical reference period in all likelihood the bias will undergo some change over time thereby limiting the validity of the delta method and other statistical downscaling methods nevertheless the delta method has been widely used in downscaling applications and its validity has been found to be comparable to that of more sophisticated downscaling methods when applied to monthly fields although this is not the case for daily fields and their corresponding extremes hayhoe 2010 4 downscaled products 4 1 examples of community charts many users requiring climate information for planning or adaptation purposes are located in villages or larger population centers given our target of the north american arctic we therefore performed the downscaling for the largest available collection of community locations covering all of alaska the yukon territory british columbia alberta saskatchewan and manitoba this resulted in downscaled temperature and precipitation data for more than 4000 communities in order to illustrate the fusion of the model selection and the downscaling we focus here on alaska and apply the downscaling methodology to the five models that ranked highest by the rmse metric in the historical simulations across the alaska domain cf table 2 mri cgcm3 giss e2 r gfdl cm3 ipsl cm5a lr and ncar ccsm4 for every year and calendar month the downscaling consisted of calculating the delta value for each gcm grid cell interpolating to the same spatial resolution as the high resolution baseline climatology followed by adding these high resolution deltas to the same high resolution climatology the resulting values for the high resolution grid cell containing a particular community became the downscaled values for that community downscaled monthly values were then averaged across decadal time slices 2010 2019 2020 2029 2090 2099 the downscaled values were computed separately using the historical baselines from prism and cru ts 3 2 for 1961 1990 and separately for the rcp 4 5 rcp 6 0 and rcp 8 5 forcing scenarios in order to provide an indication of the sensitivities of the downscaled products the downscaled values for any particular community are the values for the 2 km grid cell containing that community while there is no consideration of the measurement site s location within the 2 km grid cell e g valley vs mountain the use of fine 2 km grid cells reduces the impact of within call variability for most communities however the same cannot be said for the model grid cells typically 100 200 km in size for which the average elevation or land sea fraction may be a poor representation of the community s location for this reason the bias correction inherent in the downscaling is an important attribute of the downscaling procedure fig 9 is an example of the downscaled temperatures for kotzebue a community on the northwest coast of alaska in this example the downscaling is based on the prism climatology and the rcp 6 0 mid range emissions scenario the results are shown for each calendar month x axis the gray bar represents the historical 1961 1990 climatology based on observational data while the colored bars are means for individual decades of the 21st century based on the addition of the models deltas for those decades consistent with the forcing warming is apparent in all calendar months however the warming is greater in the cold season november march than in the warm season moreover the inter model spread is generally larger than the overall change in the 5 model composite mean pointing to the range in projected changes associated with the combination of internal variability and across model differences in formulation resolution components that are coupled and other model characteristics the across model spread decreases as the averaging is performed over time slices longer than a single decade e g over 30 year period pointing to the influence of internal variability on decadal averages as a second example fig 10 shows the downscaled precipitation for mcgrath a small community in interior alaska in this case the precipitation is for the rcp 8 5 high emission scenario and is based on the cru 3 2 historical climatology we display the results based on cru 3 2 and not prism in order to avoid redundant graphics while illustrating the choices available to users there is no evidence that either of the precipitation climatologies is better for a particular region the monthly clusters of bars show that precipitation is projected to increase in all calendar months with the largest increases in the warm season corroborating similar results in walsh et al 2008 and stewart et al 2013 the across model spread is even larger than in fig 9 indicative of a general tendency for greater spread in precipitation projections than in temperature projections in this case the across model uncertainties are far larger than the changes in the composite five model mean values the spread generally increases with time indicating greater uncertainty in the late century projections than in the mid century projections the 5 model mean projections in fig 10 even show occasional decreases from one time slice to the next e g the blue bars for june and july pointing to a role of internal variability in the decadal means because internal variability is a source of uncertainty in addition to the uncertainty associated with across model differences in formulation the future changes have the character of a bumpy ride rather than a steady progression especially in the case of precipitation the values downscaled for each model were based on a single simulation ensemble member the across model spread would decrease if the estimates were based on averages of multiple ensemble members from each model rather than a single ensemble member since internal variations would be reduced by averaging over multiple simulations 4 2 user interface a key aim of the snap downscaling was the facilitation of use by stakeholders for this reason a user interface was developed not only to provide public access to the products but to encourage users to visualize and experiment with the downscaled projections for their particular locations of interest user driven exploration of the sensitivities of the output was one of the priorities in the design and implementation of the user interface this interface allows users to select different options for various calculation and display parameters the variable temperature or precipitation the units f or c inches or millimeters the reference database for the historical period prism cru 3 2 the forcing scenario rcp 4 5 rcp 6 0 rcp 8 5 and the inclusion or not of the across model ranges in the display of the projections fig 11 is a screen capture of the user interface which also provides the option to download a user created chart for a particular community as an example of the sensitivities that a user can explore fig 12 shows a comparison of the projected changes of temperature f at point hope alaska under the rcp 4 5 upper and rcp 8 5 lower scenarios the warming shows clear signs of leveling off in the rcp 4 5 scenario but continues to increase in the rcp 8 5 scenario the difference in warming between the two scenarios is approximately 10 f in the winter months december february perhaps more importantly the monthly mean temperatures in the transition months may october rise above freezing by 2100 under rcp 8 5 while remaining at or below freezing under rcp 4 5 implications for duration of the ice free season which affects over land travel as well as offshore activities e g whaling subsistence hunting are significant in coastal areas where daily activities are closely tied to the state of the land and ocean surfaces as in the preceding examples fig 12 shows the results based on only one climatology prism in order to avoid redundant graphics there is no evidence that either of the temperature climatologies is better for a particular region as a second example of exploration of sensitivities fig 13 shows the downscaled precipitation values for juneau a relatively wet location in southeast alaska based on the prism upper and cru lower reference climatologies in both cases the forcing is the rcp 8 5 scenario while the projected changes are the same in the two cases the actual amounts are larger with prism which has a wetter reference climatology for juneau during the late summer and autumn months which are juneau s wettest the differences in the two climatologies are as large as 5 cm 2 inches such differences are comparable to the projected changes from the late 1900s to the 2090s pointing to the importance of a robust base climatology in the use of downscaled climate projections the snap visualization tool for the community charts has been accessed by users within and outside of alaska it has provided reference material for the alaska section of the third u s national climate assessment stewart et al 2013 and it has provided input to climate adaptation planning efforts for alaskan communities nome eskimo community 2017 feedback from users has led to additions to the original capabilities including the capability to download user generated charts 5 software and data availability all downscaled climate data and software tools discussed in this paper were produced by the scenarios network for alaska and arctic planning snap and are available under a creative commons 4 0 international license https creativecommons org licenses by 4 0 where only attribution to snap is needed with no additional restrictions allowed the ar5 gcm evaluation tool https uasnap shinyapps io ar5eval was developed in january of 2016 using the r programming language https cran r project org shiny web application framework https shiny rstudio com the developer is matthew leonawicz mfleonawicz alaska edu the only hardware requirement is a computer with an internet connection there are no special software requirements and there is no charge for public users the raw data utilized by this app were obtained from the coupled model intercomparison project version 5 cmip5 http cmip pcmdi llnl gov cmip5 data portal html and the university of east anglia s climate research unit cru http www cru uea ac uk data the snap community charts including the downscaling software and the visualization tool https www snap uaf edu sites all modules snap community charts charts php were developed in 2009 and updated in 2015 to display the latest cmip5 climate data the community charts utilize jquery jqueryui highcharts mysql and php programming languages snap downscaled monthly climate data are available in geotiff format for download from https www snap uaf edu tools data downloads 6 conclusion the project described here represents an end to end activity connecting the global climate modeling enterprise with planners decision makers and other users in alaska the effort has included retrieval of the requisite observational datasets and model output a model evaluation and selection procedure targeted at the alaska region the actual downscaling by the delta method with its inherent bias adjustment and the provision of the data to a range of users through a visualization tool that empowers users to explore the downscaled output and its sensitivities the website s documentation of the visualization tool provides users with a summary of the main components of the downscaling but there have also been frequent requests for a reference that can be cited the present paper responds to those requests because the downscaled products have been accessible to users for several years lessons learned have begun to accumulate one lesson is that users desire the actual plots or digital data for presentation purposes or for supporting statements about potential climate change in their area second there is need for caution with regard to the internal variability that can affect decadal means but can also be obscured by compositing of projections from several models five in this case in recognition of this need for caution the decision was made to include the across model range indicators as a user option however our experience has been that many users do not realize that these range indicators include uncertainties due to both internal variability and differences in model formulations it has been necessary to make this point in a more complete framework of uncertainties in future projections e g hodson et al 2013 finally the downscaled products and visualization tool have proven to be useful for messaging about the role of human activities especially alternative futures as they may result from different emissions scenarios rcp 4 5 vs rcp 8 5 the contrasting options of the user interface we did not include the rcp 2 6 scenario because the emissions reductions with negative emissions by 2100 are so extreme that this scenario is rapidly becoming impossible to achieve using the other three primary rcp scenarios the community charts take this scenario dependence down to the local scale that is of greatest interest and concern to a user the messages conveyed by the charts are consistent with broader depictions of arctic change and overland et al 2014 adaptation and mitigation timeframes 1 climate change especially warming is already built into the system over the next few decades even under emissions reduction scenarios so adaptation will be necessary and 2 the choice of the emissions scenarios substantially alters the trajectory of local climate in the second half of the century so mitigation will ultimately make a difference in a community s future climate acknowledgments this work was supported by the alaska climate science center through a cooperative agreement g10ac00588 from the usgs and by noaa s climate program office through grants na15oar4310169 and na16oar4310162 we acknowledge the world climate research programme s working group on coupled modeling which is responsible for cmip and we thank all the climate modeling groups for producing and making available their model output for cmip the us department of energy s program for climate model diagnosis and intercomparison provides coordinating support and leads development of software infrastructure in partnership with the global organization for earth system science portals finally we thank an anonymous reviewer for insightful and constructive comments on the original submission any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government 
26311,surface ozone o3 is considered an hazard to human health affecting vegetation crops and ecosystems accurate time and location o3 forecasting can help to protect citizens to unhealthy exposures when high levels are expected usually forecasting models use numerous o3 precursors as predictors limiting the reproducibility of these models to the availability of such information from data providers this study introduces a 24 h ahead hourly o3 concentrations forecasting methodology based on bagging and ensemble learning using just two predictors with lagged o3 concentrations this methodology was applied on ten year time series 2006 2015 from three major urban areas of andalusia spain its forecasting performance was contrasted with an algorithm especially designed to forecast time series exhibiting temporal patterns the proposed methodology outperforms the contrast algorithm and yields comparable results to others existing in literature its use is encouraged due to its forecasting performance and wide applicability but also as benchmark methodology keywords time series forecasting data science ozone concentration 1 introduction ozone o3 is an ubiquitous secondary photochemical air pollutant that is formed when volatile organic compounds nitrogen oxides and carbon monoxide the three ozone precursors react in the presence of short wavelength solar radiation to date surface o3 is considered as the most damaging air pollutant in terms of adverse effects on human health vegetation crops and material paoletti 2006 sicard et al 2016 concentrations of surface o3 can shift rapidly over hours and days sometimes reaching levels that can exceed prescribed thresholds considered to be safe for health particularly for the most vulnerable segments of the population predicting the temporal evolution of o3 concentration in specific urban locations emerges as a priority for guaranteeing quality of life providing the population in affected areas with accurate information and alerting them when exceptionally high levels are present any threshold value exceedance accurately forecasted in advance allows environmental authorities to apply short term pollution control measures and abatement strategies to protect the population traditionally environmental modelers have relied on multiple information to perform predictions corani and scanagatta 2016 incorporating numerous predictors related to o3 formation to the general formulation of the forecasting models however on many occasions the observation data available from monitoring sites do not ensure quality requirements or may often be limited to few parameters hence relevant o3 chemical precursors or originators traditionally used as input parameters which strongly contribute to perform better forecasts cannot be considered traditional time series ts techniques fail to forecast o3 accurately chattopadhyay and bandyopadhyay 2007 as a replacement machine learning ml techniques have emerged and proved to be more effective for o3 prediction gong and ordieres meré 2016 martínez ballesteros et al 2010 martínez ballesteros et al 2011 since 2006 ensemble forecasting has begun to receive more attention as ensemble algorithms can improve forecasting accuracy and enhance the generalization capability zhang et al 2012 however they hold the inherent limitations associated with the single ml model s accuracy this study introduces a methodology based on bagging and ensemble learning to forecast 24 h ahead hourly o3 concentrations which was evaluated using ten year 2006 2015 hourly o3 ts obtained from three major urban areas of andalusia spain the different air quality monitoring site locations permitted to evaluate the forecasting results in a wide range of pollution levels and urban scenarios the main contribution of this methodology is to use just lagged o3 concentrations as predictors without requiring the participation of any other variable related to the o3 formation in urban environments this methodology was compared with the pattern sequence based forecasting psf martínez álvarez et al 2011 bokde et al 2017 algorithm which is especially conceived to forecast on ts exhibiting regular patterns as in o3 ts a detailed revision of the existing literature was also performed to compare the forecasting performance of the proposed methodology the rest of the paper is structured as follows relevant and related works are reviewed in section 2 section 3 introduces the methodology proposed to forecast o3 when limited historical data is available results from its application to several urban environments in spain are reported and discussed in section 4 finally the conclusions drawn from this study are summarized in section 5 2 related works ozone ts forecasting problem has been addressed using a wide variety of techniques from statistical approaches to deterministic models the most recurrent techniques in literature are based on ml algorithms specially artificial neuronal networks ann and ensemble methods in this section the most relevant approaches for o3 ts forecasting are presented anns were used in pires et al 2012 where both the activation function and the number of hidden neurons are tuned using a genetic algorithm which also optimizes a threshold value that helps to differentiate between regimes of o3 behaviour different correction techniques were applied to ann to improve their performance based on the average o3 profile and training errors pires and martins 2011 anns were compared to a deterministic model named wrf chem in hoshyaripour et al 2016 resulting that the latter performs better in predicting mean and extreme o3 concentrations while ann achieved better results in predicting daily o3 values the combination of support vector regression algorithms and numerical models were studied in carro calvo et al 2017 in the work of lu et al lu and wang 2014 the authors explained the limitations of both ann and support vector machines svm in the field of the ground level o3 prediction they claim that ann based techniques can easily incur in overfitting local minima problems and they not provide interpretable models ann are black box schemes gaussian processes gp are statistical models for regression problems with an infinite dimensional generalization of multivariate normal distributions gp has been applied to o3 ts prediction in kocijan et al 2016 petelin et al 2013 specifically in these works an on line learning based variant of gp named evolving gaussian processes was used enabling the possibility of considering a mobile air quality measurement station such methodology is able to predict o3 concentrations for a specific geographical location without a large quantity of historical of measurements sequential aggregation kolesárová et al 2015 is a type of ensemble techniques where a linear sequential aggregation rule produces a weight vector based on the past observations and the past predictions the final prediction is then obtained by linearly combining the predictions of the models according to the weight vector sequential aggregation was applied to o3 prediction in debry and mallet 2014 mallet et al 2009 in the work of debry et al debry and mallet 2014 the predictions of the french platform prev air were ensembled via sequential aggregation improving original predictions bagging boosting and stacking are well known ensemble approaches that intend to improve the accuracy of a set of predictors by reducing their bias and variance bagging is designed to reduce the variance whereas boosting and stacking can help to reduce both the bias and variance such three approaches were applied to predicting the exceedances of daily maximum o3 in gong and ordieres meré 2016 specifically bagging technique was used in combination with classification regression trees and random forests breiman 2001 boosting technique was applied using stochastic gradient boosting machines friedman 2002 and adaboost freund and schapire 1996 stacking technique was implemented using a multiple linear regressor as the metalearner and support vector machines anns classification regression trees random forests adaboost and gradient boosting machines as ensembled methods fuzzy logic in combination with anns were applied in taylan 2017 using the adaptive neuro fuzzy inference system jang 1993 to predict ground level o3 concentrations different feature selection techniques were applied to o3 prediction in kocijan et al 2015 using different methods based on cost functions through a validation procedure resulting regressor selections are specific for particular geographical locations and o3 concentration intervals it must be considered that after an exhaustive search and to the best of the authors knowledge no other similar approaches as the introduced in this study have been found in literature therefore the novelty of our methodology led us to expose in this section i the more avant garde proposals to forecast o3 from a data science approach or ii the methodologies which share with ours some of the applied procedures namely bagging and ensembles the main contribution of our proposal is the ability to forecast o3 when no information about its precursors is available and when historical data are scarce it seems no other forecasting approaches allow coping with these limitations 3 methodology this section describes the methodology proposed to forecast o3 when limited information is available a linear method forecasts hourly o3 concentrations and is based on an ensemble from a set of two models m 1 and m 2 that use three linear regression models l r 1 l r 2 and l r 3 to estimate the forecasted o3 using simply actual and lagged o3 concentrations as regressors if it is denoted y h d 1 as the o3 concentration at hour h and day d 1 the observations y h d and y h d 1 for d 1 30 represent the actual and 24 h lagged hourly o3 concentrations prior to y h d respectively lr models are defined as follow 1 l r 1 y ˆ h d 1 β 0 β 1 y h d ε h d 1 2 l r 2 y ˆ h d 1 β 0 β 1 y h d 1 ε h d 1 3 l r 3 y ˆ h d 1 β 0 β 1 y h d β 2 y h d 1 ε h d 1 where β 0 is the intercept β 1 and β 2 are regression coefficients ε h d 1 is an error term and d 1 30 to verify the validity of regression models l r 1 l r 2 and l r 3 during the model building processes an study of non linearity of the data was performed using residual plots and no indication of discernible patterns or trends in the residuals were detected complementarily for the simple linear l r 1 and l r 2 and multiple l r 3 regression models hypothesis tests were carried out to confirm the association between predictor and response variables using the t and f statistics respectively which yielded statistically significant values using a linear classical ts modelling approach l r 1 and l r 2 are equivalent to an autoregressive model of first order ar 1 in which the autoregressive term is shifted back 24 h and 48 h with respect the hourly observation at time t respectively with the length of ts t 30 similarly l r 3 is equivalent to an ar 2 model with the first autoregressive term shifted back 24 h and the second one 48 h the error term ε h d 1 in equations 1 3 is equivalent to white noise with mean zero and variance one and β 0 a constant the hourly o3 forecasted value from m 1 model y ˆ m 1 is obtained after averaging the forecasting results from three lrs m 2 model uses a bagged averaging using l r 1 l r 2 and l r 3 as base regression models the y ˆ m 2 value is obtained following the next algorithm with t 10 iterations divided into two different phases 1 model generation phase for each of t iterations a sample with replacement from observations y h d y h d 1 y h d 2 d 1 30 b build l r 1 l r 2 and l r 3 from the sample c store the resulting models 2 forecasting phase for each of the t models a forecast value of o3 by averaging l r 1 l r 2 and l r 3 forecasting b return the average value y ˆ m 2 of the forecasted values the final o3 hourly forecasted value is obtained after averaging the y ˆ m 1 and y ˆ m 2 values figure 1 depicts how the proposed methodology makes a prediction first the historical data for the last 30 days are only considered for learning the algorithm let us suppose that for instance hour h at day d 1 is going to be predicted in that case only hour h at day d and hour h at day d 1 are considered d 1 30 other possible window lengths were studied 15 45 60 and 90 days and the selected one 30 days was chosen according to its forecasting performance using the quality measures described in section 4 2 apart from showing a better accuracy behaviour it seems that a window length of 30 days conveniently balances robust prediction accuracy and model training with enough recent observations and therefore allows to capture the seasonal temperature conditions governing the o3 formation in the different study periods along the year from a modelling perspective the daily pattern of hourly o3 concentrations could be assumed to be influenced by an underlying seasonal cycle which varies through the year since the above regression models l r 1 l r 2 and l r 3 do not consider this latter seasonal component its possible influence during the time span covered by the ts 30 days was studied using the a u t o a r i m a function from the f o r e c a s t package hyndman 2017 in r r core team 2017 this function allows for conducting a search over possible seasonal arima models within the order constraints provided and then returning the best model according to a bayesian criterion to that end hourly ts from the 30 days prior to the hour to be forecasted were modeled using the seasonal arima p d q p d q 24 the values of q and q were set to 0 and those of p d p and d were restrained to a maximum of 2 orders the best models were selected according to the bic criterion finally fig 2 illustrates the methodology aforementioned it can be seen that three linear regressions l r 1 l r 2 and l r 3 are created and their average is calculated y ˆ m 1 alternatively ten models are built from 10 bootstrap samples generating 10 averaged bagging models y ˆ m 2 the average of ensembles and linear regressions results in the final forecasting 4 results this section reports the results obtained by the application of the proposed methodology to the datasets described in section 4 1 the used metrics to evaluate its performance is introduced in 4 2 finally errors and comparison to other well established methods are discussed in section 4 3 4 1 data description o3 data were collected at five urban sites from three air quality monitoring networks in andalusia spain cordova jaen and seville from 2006 to 2015 following the reference monitoring method established in directive 2008 50 ec on ambient air quality and cleaner air for europe table 1 presents the type suburban urban and predominant emission sources background traffic of each monitoring site selected in this study o3 data were provided by the regional ministry of environment and land planning of andalusia seville spain after validation the cities of cordova jaen and seville are located in southern spain and during 2014 had a total population of 328 041 115 837 and 696 676 data collected from the institute of statistics and cartography from andalusia last accessed 2017 respectively 4 2 quality parameters many error measures can be used to assess a prediction performance hyndman and koehler 2006 however in the context of this study the most common are root mean square error rmse and mean absolute error mae and for this reason they were the ones selected their formulas are r m s e 1 n i 1 n y i y ˆ i 2 and m a e 1 n i 1 n y i y ˆ i where n is the number of evaluated samples y i the actual value and y ˆ i the predicted value 4 3 discussion this section includes the results obtained by the proposed methodology and the comparison to the successful psf algorithm especially designed to forecast ts with temporal patterns briefly this algorithm forecasts the behaviour of ts based on similarity of pattern sequences the prediction of a data point is provided as follows first the pattern sequence prior to the day to be predicted is extracted then this sequence is searched for within the historical data and the prediction is calculated by averaging all the samples immediately after the matched sequence tables 2 and 3 show the results in terms of rmse and mae respectively for all the five stations and years 2006 2015 as it can be seen the proposed methodology clearly outperforms results of psf for every year on average the rmse achieved is 18 16 μ g m33 whereas psf achieved 20 72 μ g m3 being the best obtained values 10 7 μ g m3 and 15 7 μ g m3 respectively with respect to mae the average results were 14 33 μ g m3 and 16 17 μ g m3 for the proposed and psf algorithms an the best values 8 1 μ g m3 and 11 8 μ g m3 respectively the seasonal arima models described previously yielded forecasting performances lower than the proposed and psf algorithms results not shown the reason the latter algorithms forecast more accurately than seasonal arima models in the context of this study remains open for further investigation for illustrative purposes only graphical results from 2015 for rmse and mae are depicted in figs 3 6 letters a b c d and e identify aljarafe asomadilla bermejales ronda del valle and torneo stations respectively in particular a temporal distribution of the rmse and mae per month and hour of the day can be seen in figs 3 and 4 respectively being more consistently obtained higher values of both statistics at night during the first six months of the year in asomadilla bermejales and ronda del valle stations this observation however cannot be extended to the rest of stations because smoother rmse and mae values are obtained in aljarafe and torneo stations during the whole year broadly speaking figs 3 and 4 show how the forecasting performance of our proposed methodology behaves along the day and year as said before aljarafe a and torneo e sites seem to experience a less acute declining of the rmse and mae performance aljarafe site is placed at the seville s outskirts and could receive o3 from a transported origin huelva city with an intense industrial activity the transported o3 during night is not coupled with the o3 genesis photochemical reactions which are light ultra violet dependent with respect the torneo site in this area it is produced a high concentration of no2 a typical marker of traffic origin which in case of adequate presence of sunlight intensity can also produce high o3 concentrations during the day if this o3 is not conveniently washed out by wind conditions it could become stagnant during night hours therefore in aljarafe and torneo sites the o3 concentrations would experience less range variations than in the rest of sites it seems that exogenous o3 concentrations present at night caused by transport from other origins or the stagnated one produced during the day could be captured by our methodology however the more wide o3 concentration ranges experienced in asomadilla bermejales and ronda del valle sites due to high o3 concentration values at midday and low at night hours would not be entirely described with our approach this aspect remains open for further investigation figs 5 and 6 illustrate the particular behaviour of the rmse and mae values per day of the week which also depends on the studied station based on tables 2 and 3 values for the rest of the years it can be concluded that rmse and mae values are approximately constant and in general robust over the 10 years studied likewise these values seem to be locally controlled by generating factors of pollution that have an effect on the performance of the proposed algorithm it is worth to note the more steady behaviour of rmse and mae in asomadilla site b a typical suburban background monitoring site as aljarafe site in this case asomadilla site experiences higher o3 concentrations due to its location far from cordova s downtown such area of the city likely being the origin of this higher o3 pollution due to transport dynamic this fact could support the performance explanation derived from figs 3 and 4 in general terms from figs 5 and 6 it can be concluded that our approach seems not to properly describe the weekend effect lower o3 concentrations during saturday and sunday since similar rmse and mae values are obtained with respect to working days despite the short available time period for learning the proposed algorithm 30 days these results must be considered satisfactory in absolute terms since similar rmse values were achieved in other consistent studies table 4 shows other rmse models applied to the same kind of data where mlp stands for multilayer perceptron svm for support vector machine mlr for multiple linear regression and ar for autorregressive model as it can be noticed all methods obtained rmse values higher than those of the proposed method and even higher than those of psf the only exception are the works in debry and mallet 2014 kumar et al 2017 and hoshyaripour et al 2016 however in debry and mallet 2014 authors considered exogenous variables such as no2 and pm10 in kumar et al 2017 temperature relative humidity and no2 and no2 and wind direction in hoshyaripour et al 2016 to generate a more robust models it is worth highlighting that the ultimate goal of this approach is to make predictions in extreme situations where short historical data and no other correlated variables are available therefore its comparison could not be considered fair 5 conclusions a new methodology based on bagging and ensembles of learning models to forecast 24 h ahead hourly surface level o3 is proposed its main novelty lies in the ability to develop these models when no information about o3 precursors is available which is new in literature this methodology was only built on two o3 variables composed by 24 h and 48 h lagged concentrations with respect the hourly concentration to be forecasted or equivalently just using the information from the two days prior the forecasting time were required modelling introduced in this work are presented ready to use without requiring further intervention from the final user to reproduce it hourly ts from five o3 monitoring sites from andalusia spain were used to test the forecasting ability of the proposed methodology the long period of study 2006 2015 and the different monitoring sites where data were obtained permitted a wide range of pollution levels contributions and locations to be considered for assessing its robustness every forecasted hourly o3 concentration was obtained after averaging the estimated o3 concentrations from two models the first one averages the estimates of three linear regression models and the second one used an bagged averaging of them the proposed methodology could pose how simply averaging few and slightly correlated models can improve the forecasting ability of ensembles the accuracy of the proposed forecasting approach outperforms the results found in the literature related studies make use of input variables involved in the o3 formation in urban environments these latter variables are provided by meteorological and air quality services which make models reproducibility dependent on the availability of such information from other similar data providers the introduced method circumvents the use of this information related to the o3 genesis widening their applicability the psf algorithm specially designed to forecast ts with temporal patterns as in the o3 case was used to compare the performance of the proposed methodology on average the rmse and mae achieved by this algorithm after studying their forecasting performance was 20 72 μm m3 and 16 17 μm m3 whereas the proposed methodology obtained 18 16 μm m33 and 14 33 μm m3 respectively the use of the proposed methodology o3 is encouraged to environmental modelers devoted to forecast surface level o3 its use is intended when no information of the precursors involved in the formation of this air pollutant is available or when historical data are scarce however their forecasting accuracy can be used to provide a benchmark performance for comparative purposes with respect to other modelling approaches data and software availability the data used in this study were kindly provided by the regional ministry of environment and land planning of andalusia seville spain please contact the corresponding author for any enquiries models were implemented using the open source programming environment r version 3 2 2 this software is available for download from www r project org and runs on unix windows and macos platforms source codes used in this study are available upon request disclaimer the views expressed are purely those of the authors and may not in any circumstances be regarded as stating an official position of the european commission acknowledgements the authors would like to thank the spanish ministry of economy and competitiveness and junta de andalucía for the support under projects tin2014 55894 c2 r and p12 tic 1728 respectively 
26311,surface ozone o3 is considered an hazard to human health affecting vegetation crops and ecosystems accurate time and location o3 forecasting can help to protect citizens to unhealthy exposures when high levels are expected usually forecasting models use numerous o3 precursors as predictors limiting the reproducibility of these models to the availability of such information from data providers this study introduces a 24 h ahead hourly o3 concentrations forecasting methodology based on bagging and ensemble learning using just two predictors with lagged o3 concentrations this methodology was applied on ten year time series 2006 2015 from three major urban areas of andalusia spain its forecasting performance was contrasted with an algorithm especially designed to forecast time series exhibiting temporal patterns the proposed methodology outperforms the contrast algorithm and yields comparable results to others existing in literature its use is encouraged due to its forecasting performance and wide applicability but also as benchmark methodology keywords time series forecasting data science ozone concentration 1 introduction ozone o3 is an ubiquitous secondary photochemical air pollutant that is formed when volatile organic compounds nitrogen oxides and carbon monoxide the three ozone precursors react in the presence of short wavelength solar radiation to date surface o3 is considered as the most damaging air pollutant in terms of adverse effects on human health vegetation crops and material paoletti 2006 sicard et al 2016 concentrations of surface o3 can shift rapidly over hours and days sometimes reaching levels that can exceed prescribed thresholds considered to be safe for health particularly for the most vulnerable segments of the population predicting the temporal evolution of o3 concentration in specific urban locations emerges as a priority for guaranteeing quality of life providing the population in affected areas with accurate information and alerting them when exceptionally high levels are present any threshold value exceedance accurately forecasted in advance allows environmental authorities to apply short term pollution control measures and abatement strategies to protect the population traditionally environmental modelers have relied on multiple information to perform predictions corani and scanagatta 2016 incorporating numerous predictors related to o3 formation to the general formulation of the forecasting models however on many occasions the observation data available from monitoring sites do not ensure quality requirements or may often be limited to few parameters hence relevant o3 chemical precursors or originators traditionally used as input parameters which strongly contribute to perform better forecasts cannot be considered traditional time series ts techniques fail to forecast o3 accurately chattopadhyay and bandyopadhyay 2007 as a replacement machine learning ml techniques have emerged and proved to be more effective for o3 prediction gong and ordieres meré 2016 martínez ballesteros et al 2010 martínez ballesteros et al 2011 since 2006 ensemble forecasting has begun to receive more attention as ensemble algorithms can improve forecasting accuracy and enhance the generalization capability zhang et al 2012 however they hold the inherent limitations associated with the single ml model s accuracy this study introduces a methodology based on bagging and ensemble learning to forecast 24 h ahead hourly o3 concentrations which was evaluated using ten year 2006 2015 hourly o3 ts obtained from three major urban areas of andalusia spain the different air quality monitoring site locations permitted to evaluate the forecasting results in a wide range of pollution levels and urban scenarios the main contribution of this methodology is to use just lagged o3 concentrations as predictors without requiring the participation of any other variable related to the o3 formation in urban environments this methodology was compared with the pattern sequence based forecasting psf martínez álvarez et al 2011 bokde et al 2017 algorithm which is especially conceived to forecast on ts exhibiting regular patterns as in o3 ts a detailed revision of the existing literature was also performed to compare the forecasting performance of the proposed methodology the rest of the paper is structured as follows relevant and related works are reviewed in section 2 section 3 introduces the methodology proposed to forecast o3 when limited historical data is available results from its application to several urban environments in spain are reported and discussed in section 4 finally the conclusions drawn from this study are summarized in section 5 2 related works ozone ts forecasting problem has been addressed using a wide variety of techniques from statistical approaches to deterministic models the most recurrent techniques in literature are based on ml algorithms specially artificial neuronal networks ann and ensemble methods in this section the most relevant approaches for o3 ts forecasting are presented anns were used in pires et al 2012 where both the activation function and the number of hidden neurons are tuned using a genetic algorithm which also optimizes a threshold value that helps to differentiate between regimes of o3 behaviour different correction techniques were applied to ann to improve their performance based on the average o3 profile and training errors pires and martins 2011 anns were compared to a deterministic model named wrf chem in hoshyaripour et al 2016 resulting that the latter performs better in predicting mean and extreme o3 concentrations while ann achieved better results in predicting daily o3 values the combination of support vector regression algorithms and numerical models were studied in carro calvo et al 2017 in the work of lu et al lu and wang 2014 the authors explained the limitations of both ann and support vector machines svm in the field of the ground level o3 prediction they claim that ann based techniques can easily incur in overfitting local minima problems and they not provide interpretable models ann are black box schemes gaussian processes gp are statistical models for regression problems with an infinite dimensional generalization of multivariate normal distributions gp has been applied to o3 ts prediction in kocijan et al 2016 petelin et al 2013 specifically in these works an on line learning based variant of gp named evolving gaussian processes was used enabling the possibility of considering a mobile air quality measurement station such methodology is able to predict o3 concentrations for a specific geographical location without a large quantity of historical of measurements sequential aggregation kolesárová et al 2015 is a type of ensemble techniques where a linear sequential aggregation rule produces a weight vector based on the past observations and the past predictions the final prediction is then obtained by linearly combining the predictions of the models according to the weight vector sequential aggregation was applied to o3 prediction in debry and mallet 2014 mallet et al 2009 in the work of debry et al debry and mallet 2014 the predictions of the french platform prev air were ensembled via sequential aggregation improving original predictions bagging boosting and stacking are well known ensemble approaches that intend to improve the accuracy of a set of predictors by reducing their bias and variance bagging is designed to reduce the variance whereas boosting and stacking can help to reduce both the bias and variance such three approaches were applied to predicting the exceedances of daily maximum o3 in gong and ordieres meré 2016 specifically bagging technique was used in combination with classification regression trees and random forests breiman 2001 boosting technique was applied using stochastic gradient boosting machines friedman 2002 and adaboost freund and schapire 1996 stacking technique was implemented using a multiple linear regressor as the metalearner and support vector machines anns classification regression trees random forests adaboost and gradient boosting machines as ensembled methods fuzzy logic in combination with anns were applied in taylan 2017 using the adaptive neuro fuzzy inference system jang 1993 to predict ground level o3 concentrations different feature selection techniques were applied to o3 prediction in kocijan et al 2015 using different methods based on cost functions through a validation procedure resulting regressor selections are specific for particular geographical locations and o3 concentration intervals it must be considered that after an exhaustive search and to the best of the authors knowledge no other similar approaches as the introduced in this study have been found in literature therefore the novelty of our methodology led us to expose in this section i the more avant garde proposals to forecast o3 from a data science approach or ii the methodologies which share with ours some of the applied procedures namely bagging and ensembles the main contribution of our proposal is the ability to forecast o3 when no information about its precursors is available and when historical data are scarce it seems no other forecasting approaches allow coping with these limitations 3 methodology this section describes the methodology proposed to forecast o3 when limited information is available a linear method forecasts hourly o3 concentrations and is based on an ensemble from a set of two models m 1 and m 2 that use three linear regression models l r 1 l r 2 and l r 3 to estimate the forecasted o3 using simply actual and lagged o3 concentrations as regressors if it is denoted y h d 1 as the o3 concentration at hour h and day d 1 the observations y h d and y h d 1 for d 1 30 represent the actual and 24 h lagged hourly o3 concentrations prior to y h d respectively lr models are defined as follow 1 l r 1 y ˆ h d 1 β 0 β 1 y h d ε h d 1 2 l r 2 y ˆ h d 1 β 0 β 1 y h d 1 ε h d 1 3 l r 3 y ˆ h d 1 β 0 β 1 y h d β 2 y h d 1 ε h d 1 where β 0 is the intercept β 1 and β 2 are regression coefficients ε h d 1 is an error term and d 1 30 to verify the validity of regression models l r 1 l r 2 and l r 3 during the model building processes an study of non linearity of the data was performed using residual plots and no indication of discernible patterns or trends in the residuals were detected complementarily for the simple linear l r 1 and l r 2 and multiple l r 3 regression models hypothesis tests were carried out to confirm the association between predictor and response variables using the t and f statistics respectively which yielded statistically significant values using a linear classical ts modelling approach l r 1 and l r 2 are equivalent to an autoregressive model of first order ar 1 in which the autoregressive term is shifted back 24 h and 48 h with respect the hourly observation at time t respectively with the length of ts t 30 similarly l r 3 is equivalent to an ar 2 model with the first autoregressive term shifted back 24 h and the second one 48 h the error term ε h d 1 in equations 1 3 is equivalent to white noise with mean zero and variance one and β 0 a constant the hourly o3 forecasted value from m 1 model y ˆ m 1 is obtained after averaging the forecasting results from three lrs m 2 model uses a bagged averaging using l r 1 l r 2 and l r 3 as base regression models the y ˆ m 2 value is obtained following the next algorithm with t 10 iterations divided into two different phases 1 model generation phase for each of t iterations a sample with replacement from observations y h d y h d 1 y h d 2 d 1 30 b build l r 1 l r 2 and l r 3 from the sample c store the resulting models 2 forecasting phase for each of the t models a forecast value of o3 by averaging l r 1 l r 2 and l r 3 forecasting b return the average value y ˆ m 2 of the forecasted values the final o3 hourly forecasted value is obtained after averaging the y ˆ m 1 and y ˆ m 2 values figure 1 depicts how the proposed methodology makes a prediction first the historical data for the last 30 days are only considered for learning the algorithm let us suppose that for instance hour h at day d 1 is going to be predicted in that case only hour h at day d and hour h at day d 1 are considered d 1 30 other possible window lengths were studied 15 45 60 and 90 days and the selected one 30 days was chosen according to its forecasting performance using the quality measures described in section 4 2 apart from showing a better accuracy behaviour it seems that a window length of 30 days conveniently balances robust prediction accuracy and model training with enough recent observations and therefore allows to capture the seasonal temperature conditions governing the o3 formation in the different study periods along the year from a modelling perspective the daily pattern of hourly o3 concentrations could be assumed to be influenced by an underlying seasonal cycle which varies through the year since the above regression models l r 1 l r 2 and l r 3 do not consider this latter seasonal component its possible influence during the time span covered by the ts 30 days was studied using the a u t o a r i m a function from the f o r e c a s t package hyndman 2017 in r r core team 2017 this function allows for conducting a search over possible seasonal arima models within the order constraints provided and then returning the best model according to a bayesian criterion to that end hourly ts from the 30 days prior to the hour to be forecasted were modeled using the seasonal arima p d q p d q 24 the values of q and q were set to 0 and those of p d p and d were restrained to a maximum of 2 orders the best models were selected according to the bic criterion finally fig 2 illustrates the methodology aforementioned it can be seen that three linear regressions l r 1 l r 2 and l r 3 are created and their average is calculated y ˆ m 1 alternatively ten models are built from 10 bootstrap samples generating 10 averaged bagging models y ˆ m 2 the average of ensembles and linear regressions results in the final forecasting 4 results this section reports the results obtained by the application of the proposed methodology to the datasets described in section 4 1 the used metrics to evaluate its performance is introduced in 4 2 finally errors and comparison to other well established methods are discussed in section 4 3 4 1 data description o3 data were collected at five urban sites from three air quality monitoring networks in andalusia spain cordova jaen and seville from 2006 to 2015 following the reference monitoring method established in directive 2008 50 ec on ambient air quality and cleaner air for europe table 1 presents the type suburban urban and predominant emission sources background traffic of each monitoring site selected in this study o3 data were provided by the regional ministry of environment and land planning of andalusia seville spain after validation the cities of cordova jaen and seville are located in southern spain and during 2014 had a total population of 328 041 115 837 and 696 676 data collected from the institute of statistics and cartography from andalusia last accessed 2017 respectively 4 2 quality parameters many error measures can be used to assess a prediction performance hyndman and koehler 2006 however in the context of this study the most common are root mean square error rmse and mean absolute error mae and for this reason they were the ones selected their formulas are r m s e 1 n i 1 n y i y ˆ i 2 and m a e 1 n i 1 n y i y ˆ i where n is the number of evaluated samples y i the actual value and y ˆ i the predicted value 4 3 discussion this section includes the results obtained by the proposed methodology and the comparison to the successful psf algorithm especially designed to forecast ts with temporal patterns briefly this algorithm forecasts the behaviour of ts based on similarity of pattern sequences the prediction of a data point is provided as follows first the pattern sequence prior to the day to be predicted is extracted then this sequence is searched for within the historical data and the prediction is calculated by averaging all the samples immediately after the matched sequence tables 2 and 3 show the results in terms of rmse and mae respectively for all the five stations and years 2006 2015 as it can be seen the proposed methodology clearly outperforms results of psf for every year on average the rmse achieved is 18 16 μ g m33 whereas psf achieved 20 72 μ g m3 being the best obtained values 10 7 μ g m3 and 15 7 μ g m3 respectively with respect to mae the average results were 14 33 μ g m3 and 16 17 μ g m3 for the proposed and psf algorithms an the best values 8 1 μ g m3 and 11 8 μ g m3 respectively the seasonal arima models described previously yielded forecasting performances lower than the proposed and psf algorithms results not shown the reason the latter algorithms forecast more accurately than seasonal arima models in the context of this study remains open for further investigation for illustrative purposes only graphical results from 2015 for rmse and mae are depicted in figs 3 6 letters a b c d and e identify aljarafe asomadilla bermejales ronda del valle and torneo stations respectively in particular a temporal distribution of the rmse and mae per month and hour of the day can be seen in figs 3 and 4 respectively being more consistently obtained higher values of both statistics at night during the first six months of the year in asomadilla bermejales and ronda del valle stations this observation however cannot be extended to the rest of stations because smoother rmse and mae values are obtained in aljarafe and torneo stations during the whole year broadly speaking figs 3 and 4 show how the forecasting performance of our proposed methodology behaves along the day and year as said before aljarafe a and torneo e sites seem to experience a less acute declining of the rmse and mae performance aljarafe site is placed at the seville s outskirts and could receive o3 from a transported origin huelva city with an intense industrial activity the transported o3 during night is not coupled with the o3 genesis photochemical reactions which are light ultra violet dependent with respect the torneo site in this area it is produced a high concentration of no2 a typical marker of traffic origin which in case of adequate presence of sunlight intensity can also produce high o3 concentrations during the day if this o3 is not conveniently washed out by wind conditions it could become stagnant during night hours therefore in aljarafe and torneo sites the o3 concentrations would experience less range variations than in the rest of sites it seems that exogenous o3 concentrations present at night caused by transport from other origins or the stagnated one produced during the day could be captured by our methodology however the more wide o3 concentration ranges experienced in asomadilla bermejales and ronda del valle sites due to high o3 concentration values at midday and low at night hours would not be entirely described with our approach this aspect remains open for further investigation figs 5 and 6 illustrate the particular behaviour of the rmse and mae values per day of the week which also depends on the studied station based on tables 2 and 3 values for the rest of the years it can be concluded that rmse and mae values are approximately constant and in general robust over the 10 years studied likewise these values seem to be locally controlled by generating factors of pollution that have an effect on the performance of the proposed algorithm it is worth to note the more steady behaviour of rmse and mae in asomadilla site b a typical suburban background monitoring site as aljarafe site in this case asomadilla site experiences higher o3 concentrations due to its location far from cordova s downtown such area of the city likely being the origin of this higher o3 pollution due to transport dynamic this fact could support the performance explanation derived from figs 3 and 4 in general terms from figs 5 and 6 it can be concluded that our approach seems not to properly describe the weekend effect lower o3 concentrations during saturday and sunday since similar rmse and mae values are obtained with respect to working days despite the short available time period for learning the proposed algorithm 30 days these results must be considered satisfactory in absolute terms since similar rmse values were achieved in other consistent studies table 4 shows other rmse models applied to the same kind of data where mlp stands for multilayer perceptron svm for support vector machine mlr for multiple linear regression and ar for autorregressive model as it can be noticed all methods obtained rmse values higher than those of the proposed method and even higher than those of psf the only exception are the works in debry and mallet 2014 kumar et al 2017 and hoshyaripour et al 2016 however in debry and mallet 2014 authors considered exogenous variables such as no2 and pm10 in kumar et al 2017 temperature relative humidity and no2 and no2 and wind direction in hoshyaripour et al 2016 to generate a more robust models it is worth highlighting that the ultimate goal of this approach is to make predictions in extreme situations where short historical data and no other correlated variables are available therefore its comparison could not be considered fair 5 conclusions a new methodology based on bagging and ensembles of learning models to forecast 24 h ahead hourly surface level o3 is proposed its main novelty lies in the ability to develop these models when no information about o3 precursors is available which is new in literature this methodology was only built on two o3 variables composed by 24 h and 48 h lagged concentrations with respect the hourly concentration to be forecasted or equivalently just using the information from the two days prior the forecasting time were required modelling introduced in this work are presented ready to use without requiring further intervention from the final user to reproduce it hourly ts from five o3 monitoring sites from andalusia spain were used to test the forecasting ability of the proposed methodology the long period of study 2006 2015 and the different monitoring sites where data were obtained permitted a wide range of pollution levels contributions and locations to be considered for assessing its robustness every forecasted hourly o3 concentration was obtained after averaging the estimated o3 concentrations from two models the first one averages the estimates of three linear regression models and the second one used an bagged averaging of them the proposed methodology could pose how simply averaging few and slightly correlated models can improve the forecasting ability of ensembles the accuracy of the proposed forecasting approach outperforms the results found in the literature related studies make use of input variables involved in the o3 formation in urban environments these latter variables are provided by meteorological and air quality services which make models reproducibility dependent on the availability of such information from other similar data providers the introduced method circumvents the use of this information related to the o3 genesis widening their applicability the psf algorithm specially designed to forecast ts with temporal patterns as in the o3 case was used to compare the performance of the proposed methodology on average the rmse and mae achieved by this algorithm after studying their forecasting performance was 20 72 μm m3 and 16 17 μm m3 whereas the proposed methodology obtained 18 16 μm m33 and 14 33 μm m3 respectively the use of the proposed methodology o3 is encouraged to environmental modelers devoted to forecast surface level o3 its use is intended when no information of the precursors involved in the formation of this air pollutant is available or when historical data are scarce however their forecasting accuracy can be used to provide a benchmark performance for comparative purposes with respect to other modelling approaches data and software availability the data used in this study were kindly provided by the regional ministry of environment and land planning of andalusia seville spain please contact the corresponding author for any enquiries models were implemented using the open source programming environment r version 3 2 2 this software is available for download from www r project org and runs on unix windows and macos platforms source codes used in this study are available upon request disclaimer the views expressed are purely those of the authors and may not in any circumstances be regarded as stating an official position of the european commission acknowledgements the authors would like to thank the spanish ministry of economy and competitiveness and junta de andalucía for the support under projects tin2014 55894 c2 r and p12 tic 1728 respectively 
26312,operational snow forecasting models contain parameters for which site specific values are often unknown as an improvement a bayesian procedure is suggested that estimates from past observations site specific parameters with confidence intervals it turned out that simultaneous estimation of all parameters was most accurate from 2 5 years of daily snow depth observations the estimates were for snow albedo 0 94 0 89 and 0 56 for snow emissivity 0 88 0 92 and 0 99 and for snow density g c m ³ 0 14 0 05 and 0 11 at the german weather stations wasserkuppe erfurt weimar and artern respectively using estimated site specific parameters ex post snow depth forecasts achieved an index of agreement ia 0 4 0 8 with past observations ia 0 3 0 8 for a 51 years period they outperformed the precision of predictions based on default parameter values 0 1 ia 0 3 the developed inverse approach is recommended for parameter estimation and snow forecasting at sub alpine stations with more or less urban impact and for application in education keywords bayesian estimation operational snow forecasting prediction performance sub alpine snow cover escimo model 1 introduction in alpine regions as well as in subalpine mountainous areas ski tourism is an important factor for regional economy depending on fluctuations in atmospheric circulation patterns so called weather types hess and brezowsky 1969 lamb 1950 the snow cover can vary considerably between the months and years the snow reliability is also threatened by climate change marke et al 2015 in order to ensure a sufficient snow depth artificial snowmaking is common practice schmidt et al 2012 and this calls for support by suitable snow models and forecasting procedures besides these needs of winter tourism snow modelling has much broader relevance to many applications such as avalanche risk warning bartelt and lehning 2002 hydrological flood predictions and hydropower leisenring and moradkhani 2011 and weather forecasting mauser and bach 2009 detailed snowpack schemes have been developed for inclusion into numerical weather prediction and climate models vionnet et al 2012 and involve a more or less explicit description of the time evolution of snow microstructure simpler snow schemes consider the snowpack as a single layer etchevers et al 2004 the processes in the interior of the snowpack and the interactions with the surroundings are described by balance equations they are parameterized featuring specific properties such as a high albedo fixed density a low thermal capacity and low thermal conductivity an advantage of such single layer models is that they capture first order processes this makes them pertinent for application in education for on site demonstrations and visualizations of the simulated snow cover directly in the field marke et al 2016 even local authorities show interest to run such simple and inexpensive models that can quickly provide estimates and forecasts during discussions with local snowmaker operators due to their low complexity the performance of single layer models is limited improvement might be possible when site specific model parameters are used instead of usually applied default values an adequate technique for the estimation of parameters is inverse modelling that can estimate model parameters from a set of observational data aster et al 2013 for an assessment of parameter uncertainties the utilization of bayesian inference is helpful for the model parameters to be estimated this method iteratively updates a non informative a priory probability distribution according to bayes theorem and utilizing the information contained in the observational data after convergence of this repeated procedure the data of the updated model parameters are sampled to obtain their posterior probability distribution spiegelhalter and rice 2009 the application of bayesian inference is particularly straightforward for a hierarchical statistical model for an outcome variable such as the snow depth a hierarchical model represents its dependence on more than one independent random variables for example the outcome snow depth depends on the parameter snow density in a different way than on the parameter snow albedo these relationships are represented by the equations of the hierarchical model see equations 1 12 below and their explanation the present study utilizes the physically based point snow surface model energy balance snow cover integrated model escimo that has been developed strasser et al 2002 and implemented in a spreadsheet escimo spread strasser and marke 2010 as an easy to use and portable software tool for the hourly simulation of the energy balance the water equivalent and melt rates of a snow cover this one dimensional approach assumes the snow cover to be a single and homogeneous pack and solves the energy and mass balance equations for the snow surface assuming simple parameterizations of the relevant processes the outcome variable of escimo is the snow water equivalent that depends on a set of parameters while some of them are physical constants such as the specific heat of snow or the melting heat of ice others strongly refer to the situation that is under consideration the latter comprise strasser and marke 2010 the soil heat flux snow albedo emissivity of snow clear sky emissivity density of snow a recession factor characterizing the albedo changes during ageing of snow the threshold snowfall needed for the albedo to switch from old aged to new snow a temperature threshold for detection of the precipitation phase liquid or solid and two parameters relating sensible and latent heat flux to wind speed kuchment and gelfan 1996 many of the model parameters can be derived from site measurements or from remote sensing data applying the amundsen alpine multiscale numerical simulation engine software tool for example for the specification of the snow albedo strasser et al 2004 such specification procedures involve tentative simulations that are evaluated by a comparison with observations best consistency of simulations and observations indicate an adequate parameter value parameters specified at a certain site are often transferred to other sites for want of better information environmental data science can provide a remedy to the scarcity in site specific parameters the integration of data routinely gathered and delivered by monitoring systems such as weather observations including snow depths with data analysis tools facilitates the estimation of the wanted parameters for many locations and time periods the results of the estimation procedure find their way into operational snow forecasting for the benefit of the public so far in this chain of data flow there is a gap because of the lack of an adequate technique providing the site specific parameters taking advantage of the escimo software code an inverse modelling approach is suggested for the estimation of model parameters from routinely observed snow data for that purpose the escimo formulas strasser and marke 2010 are re arranged into a hierarchical model 1 12 for the snow depth due to its complexity straightforward estimation techniques e g maximum likelihood are difficult to apply and therefore bayesian inference is used that provides interval estimates for the model parameters in the following the development application and evaluation of this approach is explained for the parameters snow density snow albedo snow emissivity and soil heat flux compared to all other model parameters these parameters are most influenced by the considered site parameters representing threshold values can weaken the convergence of the markov chain monte carlo mcmc simulations applied for the bayesian inference for that reason these threshold parameters will need special consideration and are not included in the present analysis in this study the escimo model was implemented firstly in openbugs lunn et al 2009 for bayesian parameter estimation and secondly in r r core team 2014 for simulations to evaluate the quality of the parameter recovery the openbugs procedure was applied to simulations calculated with pre specified parameter values for a period of 2 5 years as a result the bias and uncertainty of the estimates are plotted next the estimation procedure was applied to real snow depth data observed during 2 5 years and the estimated parameter values were compared finally these parameters were used to make predictions for periods of 2 5 and 51 years a set of performance measures was calculated from these ex post forecasts and past observations our study region is the federal state of thuringia situated in the central german uplands analyzing meteorological data measured at the stations in erfurt weimar 316 m a s l artern 164 m a s l and wasserkuppe 912 m a s l escimo is probed at sub alpine levels and with urban influences all data were provided by the german weather service dwd regionales klima informations system rekis as the escimo spread model works with long wave radiation measurements in the energy balance and these data were unavailable from the considered stations a parameterization scheme for the long wave radiation is suggested additionally the contribution of long wave radiation to the energy balance is parameterized by the snow emissivity which is one of our estimated parameters further for compatibility with the observed meteorological data which were not available at an hourly basis the time basis of escimo was changed from hourly to daily time steps exploring the capabilities and limitations of bayesian inverse modelling of snow depth in our study we hypothesize that environmental models can be re arranged to a hierarchical structure to which bayesian techniques can be applied in a straightforward manner site specific model parameters can be estimated by means of inverse modelling they vary between sites and can differ from values given in the literature strasser and marke 2010 for sub alpine sites the estimated parameters can improve the performance of escimo compared to the default parameter values suggested with escimo spread in the alpine context routinely gathered meteorological data are suitable for snow depth predictions based on escimo utilizing parameter estimates derived from past snow observations and this procedure of inverse modelling can be included into operational weather forecasting systems site specific parameter estimates suggest that the accumulation of snow depends on the altitude escimo is applicable to both hourly and daily meteorological data 2 model setup and inverse modelling the escimo model is based on the energy balance e b of a snow pack for details of each of the following model equations 1 12 see strasser and marke 2010 contributions to the energy balance 5 comprise the soil heat flux b advective energy a s and a r respectively supplied by solid s or liquid r precipitation sensible h and latent e heat fluxes as well as short 1 α g and long wave l net radiation on the basis of the individual terms given in detail by strasser and marke 2010 the energy balance is applied to melting if air temperature 273 15 k and no melt air temperature 273 15 k see equation 4 and eventually predicts the snow water equivalent s w e in mm here a hierarchical statistical model is suggested for the outcome variable snow depth s d s w e 10 ρ w ρ o in cm with the density of water ρ w see sturm et al 2010 that involves the station specific snow densities in units of ρ w 1 g c m ³ ρ o for new snow ρ m for melting snow ρ s for sublimated snow and ρ r for frozen rain on snow cover and is assumed to follow an exponential distribution f exp 1 s d t exp λ t f exp x λ e x λ λ 0 f o r x 0 x 0 e x λ var x λ 2 2 λ t max 0 s d t 1 s t 10 ρ o f o r s d t 1 0 sd t 1 0 sd t 1 s d t 1 r t ρ r s t ρ o s u b t ρ s m e l t t ρ m 10 3 s u b t 86400 e t 283550 0 f o r s d t 1 0 s d t 1 0 4 m e l t t min eb t 10 ρ m s d t 1 0 f o r s d t 1 0 eb t 0 otherwise eb t 86400 e b t 337500 5 e b t b a s t a r t h t e t 1 α g t l t ε s 6 b 2 0 7 a s t ϑ t 2100 s t 86400 0 f o r s t 0 s t 0 8 a r t ϑ t 4180 r t 86400 0 f o r r t 0 r t 0 9 h t 18 85 a b u t ϑ t ϑ s t ϑ s t min ϑ t 0 10 e t 32 82 a b u t e t e s t e t e ϑ t r e l h u m t e s t e ϑ t 0 f o r s d t 0 sd t 0 11 l t 1 s t ε c l t s t 5 67 10 8 ϑ t 273 16 4 ε s 5 67 10 8 ϑ s t 273 16 4 12 s t g t i t ε c l t 1 1 4650 e t ϑ t 273 16 exp 1 2 3 4650 e t ϑ t 273 16 0 5 the expectation value of snow depth λ results from the sum of precipitation sublimation and melting depending on whether there was snow cover on the previous day eq 2 for days without snow cover both sublimation and melting disappear equation 3 4 in case of snow cover the water equivalent of sublimation s u b equation 3 is determined by the available latent heat e equation 10 which in turn depends on the relative humidity r e l h u m and convection due to wind speed u equation 3 couples the latent heat in w m2 to the amount of sublimated snow in mm 86400 s by help of the sublimation heat of water 283550 w kg likewise equation 4 couples the energy balance 5 to the amount of melted snow by help of the melting heat of snow 337500 w kg while 5 is the energy balance 2 represents the mass balance and both are linked via sublimation and melting equations 3 and 4 melting 4 occurs only when the energy balance e b equation 5 is positive i e the energy flux is directed to the surface all energy flux densities are expressed in w m2 melting 4 is limited either by the available amount of snow or by the available energy in the model escimo spread a constant soil heat flux of b 2 w m ² was assumed 6 as soil heat flux measurements are scarcely available the advective energy supplied by snow or by rainfall on snow is proportional to the amount of precipitation the air temperature and the specific heat of snow c s 2100 j k g 1 k 1 or water c w 4200 j k g 1 k 1 respectively see equations 7 8 86400 representing the number of seconds per day the sensible heat flux h is expressed with wind speed u in m s in 9 and accordingly the latent heat flux e is calculated in 10 where e in hpa is the water vapor partial pressure calculated using the magnus formula e ϑ and e s likewise for the snow surface kuchment and gelfan 1996 the last but one term in the energy balance 5 represents the fraction 1 α of short wave radiation global radiation g in j c m ² 8 64 w m ² absorbed from the snow surface α is the albedo extending the escimo model that requires measurements of the incoming long wave radiation l this term was parameterized using the clear sky emissivity ε c l for the fraction of sky not covered by clouds equation 11 for the clear sky emissivity an empirical formula see prata 1996 was used cloud fraction is estimated by help of the solar index s equation 12 that is the ratio between measured global radiation and theoretical short wave radiation i t for clear sky conditions this effective solar constant i t involves the solar constant i o 1370 w m ² corrections by zenith angle z with cos z t sin geographic latitude sin declination cos geographic latitude cos declination cos h hour angle h π 12 t n o o n t declination 0 4102 sin 2 π 365 d o y 80 d o y day of the year and transmission coefficients crawford and duchon 1999 13 i t i o cos z t τ r τ p g τ w τ a for the transmission coefficients we have atwater and brown 1974 mcdonald 1960 meyers and dale 1983 14 τ r τ p g 1 021 0 084 m 0 00949 p 0 051 m 35 cos z t 1224 cos ² z t 1 15 τ w 1 0 077 w m 0 3 τ a 0 935 m with air pressure p in kpa optical air mass m at 101 3 kpa precipitable water w 4650 e t for e as vapor pressure in kpa and t as air temperature in k in the long wave radiation balance 11 the incoming radiation is complemented by the outgoing radiation parameterized by snow emissivity ε s a second extension of the original escimo model strasser and marke 2010 which calculates the snow water equivalent s w e in mm is the conversion into snow depth s d that involves the station specific snow density for new snow an approximation is ρ 0 0 05 g c m ³ see glossar at www dwd de this is taken into account in 2 thirdly while the original escimo was working on an hourly basis t in h here the model was used for daily observations for that purpose the 86400 s representing one day were included in 3 4 7 and 8 and the meaning of all quantities changed accordingly t in d in the calculation 13 of the effective solar constant t t n o o n was used 3 design of the study the hierarchical model was implemented in r for the calculation of simulations and additionally in openbugs see details in section 6 and lunn et al 2009 for estimations table 1 summarizes the observational variables parameters and constants for equations 1 12 for inverse modelling of the parameters snow density snow albedo snow emissivity and soil heat flux flat priors of uniform uniform uniform and exponential distributions were applied respectively in a first step aiming at a validation of the hierarchical model daily snow depths were simulated for different values of the considered four parameters snow density in a range of ρ o ρ m ρ r ρ s ρ 0 1 0 01 0 2 snow albedo in a range of α 0 1 0 1 0 9 snow emissivity in a range of ε s 0 90 0 01 0 99 and soil heat flux in a range of b 1 5 0 1 2 5 and using real meteorological data for the remaining variables at three sites erfurt weimar artern wasserkuppe for a 2 5 year period august 2010 december 2012 applying the bayesian inversion procedure see openbugs code in supporting material to these simulated data the specified four parameters were estimated finally the estimates were compared with the original parameter values used in the simulations from this validation the precision of the estimates and the occurrence of biases become obvious biases can result from dependencies between parameter estimates as most of the parameters are tied in with equation 5 their estimates might be biased in some combinations and for some values to analyze the mutual interaction of these parameters they were estimated in all 32 different combinations α in combination with others α α b α ρ α ε s α ρ b α ρ ε s α ε s b α ε s b ρ ρ in combination with others ρ ρ b ρ α ρ ε s ρ α b ρ α ε s ρ ε s b ρ ε s b α b in combination with others b b ρ b α b ε s b α ρ b α ε s b ε s ρ b ε s ρ α ε s in combination with others ε s ε s b ε s α ε s ρ ε s α b ε s α ρ ε s ρ b ε s ρ b α keeping the not mentioned parameters constant for the constant parameters the escimo default values were used table 1 the mcmc procedure run for 100 000 updates thinning 100 with rapid convergence stable solution after ca 25 000 fig 1 for α ρ ε s for b the convergence was much poorer i e the solution fluctuated always after model validation the bayesian procedure was applied to observational data gathered routinely at three weather stations erfurt weimar artern wasserkuppe and estimated the four parameters α ε s b ρ as well as their uncertainty for different parameter combinations finally the performance of the estimation procedure and the generalizability of the model were assessed from the quality of forecasts calculated with observed data for the 2 5 years and for a 51 years period january 1961 december 2012 4 results 4 1 model validation validating the suggested inverse modelling procedure all parameter estimates were compared with the parameter values used for simulating the data in result the estimates deviate more or less from the exact values represented by a red line in fig 2 for an α value used to simulate daily snow depths during the 2 5 years a distribution of estimated albedo values was obtained plotted as median 25th and 75th percentiles in fig 2 this distribution clearly changed with the α used for the simulation the distribution was also different when α was estimated jointly with other parameters for example the estimation of α alone and in combination with ρ and b provided the same results likewise results overlap for all groups involving α and ε s these results were similar for stations erfurt weimar and artern a feature that is obvious for all three stations was the occurrence of a strong bias towards larger albedo estimates if the albedo is estimated together with the snow emissivity ε s in contrast snow density and soil heat flux did not bias the albedo estimate the dependence between the estimates α ˆ and ε ˆ s is a consequence of 5 where the albedo is a coefficient of global radiation g and snow emissivity is a coefficient of the surface temperature ϑ s see equation 11 a quantitative comparison of the terms in 5 reveals that for large albedo α 0 9 the short and the long wave energies are similar during clear sky conditions the radiation and surface temperature data are collinear and this generates an association between the estimates α ˆ and ε ˆ s further the bias of α ˆ was largest for wasserkuppe and smallest for artern this might have been caused by different ageing of snow due to differences in altitude ca 800 m and short wave radiation in our validation study snow depth data were simulated for ε s 0 99 which is too large compared with the real snow emissivity estimated for wasserkuppe see below and therefore the albedo was overestimated testing a more realistic value of ε s 0 90 for simulations the bias of the estimated albedo jointly estimated with ε s was clearly reduced cf asterisk in fig 2 this finding suggests that for subalpine regions the default ε s 0 99 in escimo might have to be replaced by a more adequate value later on we will recommend values for ε s on the basis of a joint estimation of albedo and snow emissivity from observational data see below another clear and general result was that the error range of the albedo estimate decreased with increasing albedo estimations become more precise with the availability of more data that means for long spells of snow which can be generated by repeated snow fall in this way long spells represent the large albedo of fresh snow α 0 8 0 9 kraus 2008 conversely short spells of snow cover can represent aged snow α 0 45 0 90 kraus 2008 comparing different stations it turned out that albedo estimates equal the theoretical albedo only for values larger than approximately 0 5 for α 0 5 the error increases and for very small albedo values the estimate was nearly independent on the albedo used for simulations fig 2 this is similar in the plots for erfurt weimar and artern not shown here this can be understood as a consequence of the differences in the number of days without snow that was 92 89 75 in the considered time period for artern erfurt weimar and wasserkuppe respectively for large albedo values α 0 90 the bias vanished for all stations even for the albedo estimated jointly with snow emissivity and the error range diminishes to about 1 for small albedo values α 0 50 the estimation is strongly biased and therefore useless though the estimates of snow density ρ tended to be slightly biased by a joint estimation with snow emissivity fig 3 this bias was not significant because the error range included the exact agreement between theoretical and estimated values there was a general but also not significant tendency to overestimate the snow density as wasserkuppe had more days with snow cover the precision of the estimated snow density was higher compared with the other stations the estimation of snow emissivity ε s had very diverse results fig 4 for wasserkuppe all estimates cluster in three distinct groups fig 4a a joint estimation of ε s with albedo generates an overestimation and an estimation together with snow density but not with albedo generates an underestimation of the snow emissivity in the range 0 95 ε s 0 96 for erfurt weimar just those estimations involving the albedo were biased towards larger snow emissivity and all others were unbiased fig 4b for the station artern all estimations of snow emissivity were nearly not biased fig 4c for values ε s 0 92 the estimation of snow emissivity was only slightly biased our simulations confirm the above observation that estimates of albedo and snow emissivity interact therefore the accuracy of snow emissivity estimations might be improved for other values of the albedo an aim of the present study was to demonstrate the usefulness of bayesian inference for snow depth modelling to illustrate the advantage of the bayesian approach over the maximum likelihood method the latter was used as an alternative to estimate the parameter snow emissivity ε s for the station wasserkuppe for the estimation of this parameter and the situation of snow cover s d 0 without any snow or rain s r 0 equations 1 12 can be reformulated to a generalized regression model this regression with offset involves an exponential distribution of the snow depth s d t exp λ t given ρ 0 1 g c m ³ and the daily values of s d t ϑ t r e l h u m t u t an estimate of the snow emissivity can be calculated e g in r using glm with gamma distribution as generalization of the exponential and with inverse link function and results in ε ˆ s 1 11 0 14 0 97 1 25 maximum likelihood estimators approximately follow a normal distribution this interval estimate does not comply with the restriction 0 ε s 1 however in contrast the bayesian estimate ε ˆ s 0 99 0 01 clearly corresponds to the value ε s 0 99 that was used to simulate the snow depths and remains in the interval 0 1 fig 4a the estimation of the soil heat flux b was rather poor fig 5 for all stations there was not only bias but also a very large error range considering the magnitude of the energy terms in the balance equation 5 it turns out that b is smaller than all other terms by a factor of 1 10 1 100 therefore minor inaccuracies of the other energy terms will have an immense impact on the estimation of the soil heat flux another reason for the large estimation error of b is that the soil temperature that is relevant for the soil heat flux can be different escimo assumes a soil temperature equal to the snow temperature of 273 15 k and this is just an approximate parameterization of the model further the bias of the soil heat flux estimate towards lower values might be caused by the fact that the escimo equations were used for daily time steps originally it was designed for hourly time steps and was able to account for reduced soil heat fluxes during the night this makes the daily average soil heat flux smaller than the flux during daytime summarizing our validation study of all the four considered parameters we conclude that snow density was estimated most accurately and this is in agreement with the stable solution of the mcmc simulation fig 1 in contrast the estimation of soil heat flux was very poor albedo and snow emissivity estimates interact and this suggests making a joint estimation in order to have reasonable values for both 4 2 parameter estimation from observational data in an application of the suggested inverse modelling procedure the four parameters α ρ ε s b were estimated from real snow depth data recorded at three stations during 2 5 years august 2010 december 2012 for each station all parameters have been estimated in different combinations at the beginning each parameter was estimated alone and the other parameters were set to fixed values this was completed by pairs and triples of parameter estimates finally all four parameters were estimated jointly and this procedure resulted in consistent estimates extending the original escimo model that calculates the snow water equivalent s w e here the snow depth s d was modelled directly the application study demonstrated that the ratio s w e s d significantly depends on the state of the snow cover and therefore different snow densities were included for new snow ρ o for melting snow ρ m for sublimated snow ρ s and for frozen rain on snow cover ρ r into the mass balance eq 2 all snow densities are limited in their value by the density of ice that is ρ i c e 0 918 g c m ³ and this was taken into account in the inverse modelling approach joint estimations of the four densities always resulted in ρ ˆ s ρ ˆ r ρ ˆ m ρ i c e generally the estimated density of new snow was lower than for ice ρ ˆ o ρ i c e even for aged snow covered by firn or ice the density was below 0 6 g c m ³ an estimation of the snow density from observational data table 2 resulted in station specific values of ρ ˆ o 0 14 g c m ³ and ρ ˆ o 0 05 g c m ³ for wasserkuppe and erfurt weimar respectively for artern the snow density was consistently estimated as ρ ˆ o 0 11 g c m ³ obviously the snow density increases with the accumulation and ageing of snow and this occurs more likely at a mountain station wasserkuppe as opposed to an urban lowland station erfurt weimar where the time of snow cover is reduced due to urban heat island effects the snow emissivity ε ˆ s varied between stations table 2 the estimation of the soil heat flux b ˆ was very uncertain as a consequence of the dominance of the other parts in the energy balance nevertheless our estimations indicated that a value of b ˆ 1 0 w m ² is reasonable for the soil heat flux in our study region the default value in escimo was 2 0 w m ² all achieved estimates are summarized in table 2 as a recommendation for the application in escimo with data relevant for sub alpine regions the estimated albedo was site specific and decreased from wasserkuppe α ˆ 0 94 to erfurt weimar α ˆ 0 89 to artern α ˆ 0 56 these changes in the albedo correspond to the changes in the altitude of these stations 921 m 316 m 164 m a s l stations at higher altitudes experience new snowfall more often and this is associated with a larger albedo strasser and marke 2010 used albedo values between 0 45 and 0 90 combining albedometer measurements with landsat tm images for an alpine glacier strasser et al 2004 developed an exponential decay parameterization of the albedo α α min α a d d e k n with n representing the number of days since the last considerable snowfall i e at least s w e 0 5 m m h which caused an increase of the snow albedo to its maximum value α min α a d d α min is the minimum albedo of old snow α a d d is an additive albedo and k is a recession factor that is defined as 0 12 for positive temperatures and 0 05 for negative temperatures strasser and marke 2010 this parameterization was also used in escimo and specifies the maximum albedo to 0 95 but this level can be clearly exceeded by fresh snow in alpine regions fig 17 in strasser et al 2004 our results for sub alpine stations suggested that the albedo was often below this level further for our stations the additive albedo changes were not significant table 2 an important observation is that the albedo estimation is based on the energy balance eq 5 of the escimo model and this energy balance is involved merely in the amount of melting snow eq 4 that means albedo estimates are calculated only from observational data that refer to melting and therefore α ˆ is the albedo of melting snow for new snow an estimation of α is not possible using the escimo approach 1 12 this fact explains the unexpected small values of α ˆ and the insignificance of the snow ageing obtained from our observational data 4 3 assessment of forecasting performance assessing the quality of the parameters estimated by an inverse modelling of the 2 5 years observational data sd o predictions sd p were calculated for 2 5 years and 51 years periods while the former demonstrates the precision of model fit the latter represents the model s ability for generalization following performance measures were applied 16 coefficient of determination r 2 t 1 n s d t o s d o s d t p s d p i 1 n s d i o s d o 2 k 1 n s d k p s d p 2 2 17 index of agreement i a 1 t 1 n s d t o s d t p 2 i 1 n s d i p s d o s d i o s d o 2 18 and root means square error r m s e 1 n t 1 n s d t o s d t p 2 19 mean absolute error m a e 1 n t 1 n s d t o s d t p 20 mean bias error m b e 1 n t 1 n s d t o s d t p 21 and fractional bias error f b e t 1 n s d t o s d t p 0 5 i 1 n s d i o s d t p r 2 measures the percentage of variance in observed snow depths explained by the statistical model but r 2 is not able to assess a bias between observed and predicted data willmott 1982 willmott et al 1985 for a more comprehensive evaluation the index of agreement i a 0 1 was included that indicates a perfect prediction for i a 1 m a e m b e and f b e indicate systematic errors the parameter values achieved from the bayesian estimation procedure summarized in table 2 were used for snow depth predictions and an assessment of their quality table 3 results suggest that the performance was clearly improved for the parameter values estimated by the suggested bayesian procedure referring to the 2 5 years period predictions for the 51 years period were slightly less precise a conclusion is that the parameter estimation can be recommended for each specific site before escimo is used to forecast the snow depth at this site the sensitivity of forecasting performance on the soil heat flux was very low 5 conclusions and limitations the escimo model was utilized for an inverse approach estimating the model parameters from meteorological data gathered at three stations situated in the central german uplands responding to our research hypotheses we conclude that inverse modelling is suitable to estimate model parameters and to calculate predictions for snow depths at sub alpine sites with urban influences which are so far underrepresented in the scientific literature to compare and generalize the results parameter estimation is desirable for other sub alpine regions while the german weather service dwd runs 52 climate stations in thuringia including wasserkuppe that is located near the border only 5 stations have complete datasets and only 3 that represent different elevation zones were selected for the present study escimo needs daily values for air temperature and humidity air pressure precipitation wind speed global radiation and snow depth bayesian inference can handle gaps of missing data but this will drastically increase the computing time and decrease the performance for that reason the suggested procedure might be not feasible at many sub alpine stations extending the escimo code from the snow water equivalent to the snow depth we introduced snow densities for new snow melting snow sublimated snow and frozen rain on snow cover as new parameters that need to be estimated these additional parameter estimations are required because routine measurements provide just snow depths and the snow density can vary in time and space observational data of snow density are rare recently pohl et al 2014 applied a sensor network for snow density measurements and therefore an advantage of the bayesian approach is the possibility to assess this parameter specifically for each station this estimate provides valuable information about site specific conditions the validation study section 4 1 clearly demonstrated how the parameter estimates are mutually correlated and therefore a joint estimation of all parameters is recommended this procedure guarantees that all parameter values are adequate to the prevailing state of the snow cover the estimated site specific parameters improve the predictions of escimo compared to the use of default parameter values suggested with escimo spread see table 3 the occurrence of site specific values for the parameters confirmed that the accumulation of snow depends on the altitude and the urban or mountain characteristic of a station in hydrological applications of snow models such as the u s national weather service s model snow 17 anderson 2006 bayesian techniques are often utilized to predict the snow water equivalent swe together with the extent of the snow covered area sca for that purpose data assimilation techniques such as ensemble kalman filter ensemble square root filter or particle filter have been developed that utilize in situ e g snow pillows and remotely sensed e g the normalized snow density index calculated from satellite images measurements to improve model predictions leisenring and moradkhani 2011 as all these measurements are subject to uncertainties sequential bayesian updating can stochastically combine measurements with model simulations and in this way estimate the probability distributions of the filters state variables leisenring and moradkhani 2011 found a data assimilation performance of r m s e 29 51 m m for their modelled s w e in mm assimilating landsat 8 data with the output of the senorge snow model saloranta 2016 stigter et al 2017 calculated spatiotemporal predictions of swe with a resolution of 100 m and a daily time step for their alpine study area in the central himalayas they used snow albedo values of 0 713 brock et al 2000 and applied two different albedo decay models logarithmic decay for deep snow s w e 5 m m exponential decay for shallow snow s w e 5 m m the present study demonstrates the very different situation under sub alpine conditions where the estimated albedo varied between 0 56 and 0 94 and the decay was statistically not significant while saloranta 2014 specified the minimum snow density to 0 05 g cm³ for senorge predictions of swe and hopfinger 1983 the density of avalanching snow to 0 2 g cm³ for the sub alpine stations with urban impact we find snow densities of 0 05 0 14 g cm³ stigter et al 2017 used the optimized senorge model to calculate ex post predictions of snow depth and compare them with past observations for their himalaya study region the forecasting accuracy varied between r m s e 49 m m for a region where snow depths are up to 400 m m and r m s e 314 m m for a region where snow depths can reach 2000 m m the present study resulted in a forecasting accuracy between r m s e 308 m m for wasserkuppe snow depths up to 2500 m m and r m s e 43 4 m m for arten snow depths lower than 400 m m the performance measures table 3 indicate that the generalization of the model is reasonable the precision measured by r 2 i a and r m s e is not reduced when the parameters estimated from 2 5 years of observational data are used to predict the snow depths during 51 years in contrast the accuracy of the forecasts is reduced when the default parameter values derived in alpine snow studies are used cf the extremely large r m s e for wasserkuppe the always negative values for m b e and f b e indicate that the model predictions are biased to lower snow depths improvements are needed for the estimation of the soil heat flux the parameterization applied in escimo is rather simple and an involvement of the soil temperature profile might be beneficial possibly the chosen exponential prior is not optimal and might be improved a third problem to be solved with the estimation of b arises from its small value compared to the other terms in the energy balance originally escimo was developed for hourly time steps strasser and marke 2010 for compatibility with observed meteorological data for which only daily data were available we modified the time base of escimo from hourly to daily time steps and this might have an impact on the interpretation of the parameter values especially the parameters α ρ o ε s b studied here represent daily values while in the original escimo code they can vary during the day this means the parameters on a daily basis are certain averages of these hourly data and are attenuated in their extremes for example the ageing of snow appears smoothed in the daily parameters nevertheless escimo proved to be applicable to both hourly and daily data as escimo is a physically based point snow surface model all estimated parameters refer to the location where the data have been gathered our approach provides point values of snow density albedo snow emissivity and soil heat flux the model does not give any information about the changes of these parameters along the snow surface and with increasing distance to the observation site the extension of the original escimo model by a parameterization of the long wave radiation eq 11 12 proved to be beneficial as this made the presented inverse model applicable with routine monitoring data from which long wave radiation is often not available including parameters for snow densities into the original escimo model considerably improved the performance of the model because these parameters are mostly not available at the monitoring sites even the snow prediction for new datasets takes advantage of the parameters estimated by the inverse model another extension of escimo was recently suggested by marke et al 2016 who advance the model to inside canopy conditions 6 software and data availability the study is based on data gathered in the frame of the regional climate information system rekis they were freely downloaded as text files from www rekis org for the stations wasserkuppe erfurt weimar and artern as daily values for a period of 51 years january 1st 1961 until december 31st 2012 for all calculations the free language r for statistical computing www r project org was used for bayesian inference the software openbugs bugs bayesian inference using gibbs sampling ntzoufras 2011 is based on the markov chain monte carlo mcmc method gelfand and smith 1990 and is freely available at www openbugs net where all further information is provided the code for the inverse model developed by the authors is provided in the supporting material acknowledgements the authors are grateful to frank heyner klimaagentur thüringen thüringer landesanstalt für umwelt und geologie tlug for inspiring discussions and help with the data needed for the performance assessment d h was financially supported by the deutsche bundesstiftung umwelt dbu osnabrück az 20015 373 appendix a supplementary data the following are the supplementary data related to this article mmc1 mmc1 mmc2 mmc2 appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 01 010 
26312,operational snow forecasting models contain parameters for which site specific values are often unknown as an improvement a bayesian procedure is suggested that estimates from past observations site specific parameters with confidence intervals it turned out that simultaneous estimation of all parameters was most accurate from 2 5 years of daily snow depth observations the estimates were for snow albedo 0 94 0 89 and 0 56 for snow emissivity 0 88 0 92 and 0 99 and for snow density g c m ³ 0 14 0 05 and 0 11 at the german weather stations wasserkuppe erfurt weimar and artern respectively using estimated site specific parameters ex post snow depth forecasts achieved an index of agreement ia 0 4 0 8 with past observations ia 0 3 0 8 for a 51 years period they outperformed the precision of predictions based on default parameter values 0 1 ia 0 3 the developed inverse approach is recommended for parameter estimation and snow forecasting at sub alpine stations with more or less urban impact and for application in education keywords bayesian estimation operational snow forecasting prediction performance sub alpine snow cover escimo model 1 introduction in alpine regions as well as in subalpine mountainous areas ski tourism is an important factor for regional economy depending on fluctuations in atmospheric circulation patterns so called weather types hess and brezowsky 1969 lamb 1950 the snow cover can vary considerably between the months and years the snow reliability is also threatened by climate change marke et al 2015 in order to ensure a sufficient snow depth artificial snowmaking is common practice schmidt et al 2012 and this calls for support by suitable snow models and forecasting procedures besides these needs of winter tourism snow modelling has much broader relevance to many applications such as avalanche risk warning bartelt and lehning 2002 hydrological flood predictions and hydropower leisenring and moradkhani 2011 and weather forecasting mauser and bach 2009 detailed snowpack schemes have been developed for inclusion into numerical weather prediction and climate models vionnet et al 2012 and involve a more or less explicit description of the time evolution of snow microstructure simpler snow schemes consider the snowpack as a single layer etchevers et al 2004 the processes in the interior of the snowpack and the interactions with the surroundings are described by balance equations they are parameterized featuring specific properties such as a high albedo fixed density a low thermal capacity and low thermal conductivity an advantage of such single layer models is that they capture first order processes this makes them pertinent for application in education for on site demonstrations and visualizations of the simulated snow cover directly in the field marke et al 2016 even local authorities show interest to run such simple and inexpensive models that can quickly provide estimates and forecasts during discussions with local snowmaker operators due to their low complexity the performance of single layer models is limited improvement might be possible when site specific model parameters are used instead of usually applied default values an adequate technique for the estimation of parameters is inverse modelling that can estimate model parameters from a set of observational data aster et al 2013 for an assessment of parameter uncertainties the utilization of bayesian inference is helpful for the model parameters to be estimated this method iteratively updates a non informative a priory probability distribution according to bayes theorem and utilizing the information contained in the observational data after convergence of this repeated procedure the data of the updated model parameters are sampled to obtain their posterior probability distribution spiegelhalter and rice 2009 the application of bayesian inference is particularly straightforward for a hierarchical statistical model for an outcome variable such as the snow depth a hierarchical model represents its dependence on more than one independent random variables for example the outcome snow depth depends on the parameter snow density in a different way than on the parameter snow albedo these relationships are represented by the equations of the hierarchical model see equations 1 12 below and their explanation the present study utilizes the physically based point snow surface model energy balance snow cover integrated model escimo that has been developed strasser et al 2002 and implemented in a spreadsheet escimo spread strasser and marke 2010 as an easy to use and portable software tool for the hourly simulation of the energy balance the water equivalent and melt rates of a snow cover this one dimensional approach assumes the snow cover to be a single and homogeneous pack and solves the energy and mass balance equations for the snow surface assuming simple parameterizations of the relevant processes the outcome variable of escimo is the snow water equivalent that depends on a set of parameters while some of them are physical constants such as the specific heat of snow or the melting heat of ice others strongly refer to the situation that is under consideration the latter comprise strasser and marke 2010 the soil heat flux snow albedo emissivity of snow clear sky emissivity density of snow a recession factor characterizing the albedo changes during ageing of snow the threshold snowfall needed for the albedo to switch from old aged to new snow a temperature threshold for detection of the precipitation phase liquid or solid and two parameters relating sensible and latent heat flux to wind speed kuchment and gelfan 1996 many of the model parameters can be derived from site measurements or from remote sensing data applying the amundsen alpine multiscale numerical simulation engine software tool for example for the specification of the snow albedo strasser et al 2004 such specification procedures involve tentative simulations that are evaluated by a comparison with observations best consistency of simulations and observations indicate an adequate parameter value parameters specified at a certain site are often transferred to other sites for want of better information environmental data science can provide a remedy to the scarcity in site specific parameters the integration of data routinely gathered and delivered by monitoring systems such as weather observations including snow depths with data analysis tools facilitates the estimation of the wanted parameters for many locations and time periods the results of the estimation procedure find their way into operational snow forecasting for the benefit of the public so far in this chain of data flow there is a gap because of the lack of an adequate technique providing the site specific parameters taking advantage of the escimo software code an inverse modelling approach is suggested for the estimation of model parameters from routinely observed snow data for that purpose the escimo formulas strasser and marke 2010 are re arranged into a hierarchical model 1 12 for the snow depth due to its complexity straightforward estimation techniques e g maximum likelihood are difficult to apply and therefore bayesian inference is used that provides interval estimates for the model parameters in the following the development application and evaluation of this approach is explained for the parameters snow density snow albedo snow emissivity and soil heat flux compared to all other model parameters these parameters are most influenced by the considered site parameters representing threshold values can weaken the convergence of the markov chain monte carlo mcmc simulations applied for the bayesian inference for that reason these threshold parameters will need special consideration and are not included in the present analysis in this study the escimo model was implemented firstly in openbugs lunn et al 2009 for bayesian parameter estimation and secondly in r r core team 2014 for simulations to evaluate the quality of the parameter recovery the openbugs procedure was applied to simulations calculated with pre specified parameter values for a period of 2 5 years as a result the bias and uncertainty of the estimates are plotted next the estimation procedure was applied to real snow depth data observed during 2 5 years and the estimated parameter values were compared finally these parameters were used to make predictions for periods of 2 5 and 51 years a set of performance measures was calculated from these ex post forecasts and past observations our study region is the federal state of thuringia situated in the central german uplands analyzing meteorological data measured at the stations in erfurt weimar 316 m a s l artern 164 m a s l and wasserkuppe 912 m a s l escimo is probed at sub alpine levels and with urban influences all data were provided by the german weather service dwd regionales klima informations system rekis as the escimo spread model works with long wave radiation measurements in the energy balance and these data were unavailable from the considered stations a parameterization scheme for the long wave radiation is suggested additionally the contribution of long wave radiation to the energy balance is parameterized by the snow emissivity which is one of our estimated parameters further for compatibility with the observed meteorological data which were not available at an hourly basis the time basis of escimo was changed from hourly to daily time steps exploring the capabilities and limitations of bayesian inverse modelling of snow depth in our study we hypothesize that environmental models can be re arranged to a hierarchical structure to which bayesian techniques can be applied in a straightforward manner site specific model parameters can be estimated by means of inverse modelling they vary between sites and can differ from values given in the literature strasser and marke 2010 for sub alpine sites the estimated parameters can improve the performance of escimo compared to the default parameter values suggested with escimo spread in the alpine context routinely gathered meteorological data are suitable for snow depth predictions based on escimo utilizing parameter estimates derived from past snow observations and this procedure of inverse modelling can be included into operational weather forecasting systems site specific parameter estimates suggest that the accumulation of snow depends on the altitude escimo is applicable to both hourly and daily meteorological data 2 model setup and inverse modelling the escimo model is based on the energy balance e b of a snow pack for details of each of the following model equations 1 12 see strasser and marke 2010 contributions to the energy balance 5 comprise the soil heat flux b advective energy a s and a r respectively supplied by solid s or liquid r precipitation sensible h and latent e heat fluxes as well as short 1 α g and long wave l net radiation on the basis of the individual terms given in detail by strasser and marke 2010 the energy balance is applied to melting if air temperature 273 15 k and no melt air temperature 273 15 k see equation 4 and eventually predicts the snow water equivalent s w e in mm here a hierarchical statistical model is suggested for the outcome variable snow depth s d s w e 10 ρ w ρ o in cm with the density of water ρ w see sturm et al 2010 that involves the station specific snow densities in units of ρ w 1 g c m ³ ρ o for new snow ρ m for melting snow ρ s for sublimated snow and ρ r for frozen rain on snow cover and is assumed to follow an exponential distribution f exp 1 s d t exp λ t f exp x λ e x λ λ 0 f o r x 0 x 0 e x λ var x λ 2 2 λ t max 0 s d t 1 s t 10 ρ o f o r s d t 1 0 sd t 1 0 sd t 1 s d t 1 r t ρ r s t ρ o s u b t ρ s m e l t t ρ m 10 3 s u b t 86400 e t 283550 0 f o r s d t 1 0 s d t 1 0 4 m e l t t min eb t 10 ρ m s d t 1 0 f o r s d t 1 0 eb t 0 otherwise eb t 86400 e b t 337500 5 e b t b a s t a r t h t e t 1 α g t l t ε s 6 b 2 0 7 a s t ϑ t 2100 s t 86400 0 f o r s t 0 s t 0 8 a r t ϑ t 4180 r t 86400 0 f o r r t 0 r t 0 9 h t 18 85 a b u t ϑ t ϑ s t ϑ s t min ϑ t 0 10 e t 32 82 a b u t e t e s t e t e ϑ t r e l h u m t e s t e ϑ t 0 f o r s d t 0 sd t 0 11 l t 1 s t ε c l t s t 5 67 10 8 ϑ t 273 16 4 ε s 5 67 10 8 ϑ s t 273 16 4 12 s t g t i t ε c l t 1 1 4650 e t ϑ t 273 16 exp 1 2 3 4650 e t ϑ t 273 16 0 5 the expectation value of snow depth λ results from the sum of precipitation sublimation and melting depending on whether there was snow cover on the previous day eq 2 for days without snow cover both sublimation and melting disappear equation 3 4 in case of snow cover the water equivalent of sublimation s u b equation 3 is determined by the available latent heat e equation 10 which in turn depends on the relative humidity r e l h u m and convection due to wind speed u equation 3 couples the latent heat in w m2 to the amount of sublimated snow in mm 86400 s by help of the sublimation heat of water 283550 w kg likewise equation 4 couples the energy balance 5 to the amount of melted snow by help of the melting heat of snow 337500 w kg while 5 is the energy balance 2 represents the mass balance and both are linked via sublimation and melting equations 3 and 4 melting 4 occurs only when the energy balance e b equation 5 is positive i e the energy flux is directed to the surface all energy flux densities are expressed in w m2 melting 4 is limited either by the available amount of snow or by the available energy in the model escimo spread a constant soil heat flux of b 2 w m ² was assumed 6 as soil heat flux measurements are scarcely available the advective energy supplied by snow or by rainfall on snow is proportional to the amount of precipitation the air temperature and the specific heat of snow c s 2100 j k g 1 k 1 or water c w 4200 j k g 1 k 1 respectively see equations 7 8 86400 representing the number of seconds per day the sensible heat flux h is expressed with wind speed u in m s in 9 and accordingly the latent heat flux e is calculated in 10 where e in hpa is the water vapor partial pressure calculated using the magnus formula e ϑ and e s likewise for the snow surface kuchment and gelfan 1996 the last but one term in the energy balance 5 represents the fraction 1 α of short wave radiation global radiation g in j c m ² 8 64 w m ² absorbed from the snow surface α is the albedo extending the escimo model that requires measurements of the incoming long wave radiation l this term was parameterized using the clear sky emissivity ε c l for the fraction of sky not covered by clouds equation 11 for the clear sky emissivity an empirical formula see prata 1996 was used cloud fraction is estimated by help of the solar index s equation 12 that is the ratio between measured global radiation and theoretical short wave radiation i t for clear sky conditions this effective solar constant i t involves the solar constant i o 1370 w m ² corrections by zenith angle z with cos z t sin geographic latitude sin declination cos geographic latitude cos declination cos h hour angle h π 12 t n o o n t declination 0 4102 sin 2 π 365 d o y 80 d o y day of the year and transmission coefficients crawford and duchon 1999 13 i t i o cos z t τ r τ p g τ w τ a for the transmission coefficients we have atwater and brown 1974 mcdonald 1960 meyers and dale 1983 14 τ r τ p g 1 021 0 084 m 0 00949 p 0 051 m 35 cos z t 1224 cos ² z t 1 15 τ w 1 0 077 w m 0 3 τ a 0 935 m with air pressure p in kpa optical air mass m at 101 3 kpa precipitable water w 4650 e t for e as vapor pressure in kpa and t as air temperature in k in the long wave radiation balance 11 the incoming radiation is complemented by the outgoing radiation parameterized by snow emissivity ε s a second extension of the original escimo model strasser and marke 2010 which calculates the snow water equivalent s w e in mm is the conversion into snow depth s d that involves the station specific snow density for new snow an approximation is ρ 0 0 05 g c m ³ see glossar at www dwd de this is taken into account in 2 thirdly while the original escimo was working on an hourly basis t in h here the model was used for daily observations for that purpose the 86400 s representing one day were included in 3 4 7 and 8 and the meaning of all quantities changed accordingly t in d in the calculation 13 of the effective solar constant t t n o o n was used 3 design of the study the hierarchical model was implemented in r for the calculation of simulations and additionally in openbugs see details in section 6 and lunn et al 2009 for estimations table 1 summarizes the observational variables parameters and constants for equations 1 12 for inverse modelling of the parameters snow density snow albedo snow emissivity and soil heat flux flat priors of uniform uniform uniform and exponential distributions were applied respectively in a first step aiming at a validation of the hierarchical model daily snow depths were simulated for different values of the considered four parameters snow density in a range of ρ o ρ m ρ r ρ s ρ 0 1 0 01 0 2 snow albedo in a range of α 0 1 0 1 0 9 snow emissivity in a range of ε s 0 90 0 01 0 99 and soil heat flux in a range of b 1 5 0 1 2 5 and using real meteorological data for the remaining variables at three sites erfurt weimar artern wasserkuppe for a 2 5 year period august 2010 december 2012 applying the bayesian inversion procedure see openbugs code in supporting material to these simulated data the specified four parameters were estimated finally the estimates were compared with the original parameter values used in the simulations from this validation the precision of the estimates and the occurrence of biases become obvious biases can result from dependencies between parameter estimates as most of the parameters are tied in with equation 5 their estimates might be biased in some combinations and for some values to analyze the mutual interaction of these parameters they were estimated in all 32 different combinations α in combination with others α α b α ρ α ε s α ρ b α ρ ε s α ε s b α ε s b ρ ρ in combination with others ρ ρ b ρ α ρ ε s ρ α b ρ α ε s ρ ε s b ρ ε s b α b in combination with others b b ρ b α b ε s b α ρ b α ε s b ε s ρ b ε s ρ α ε s in combination with others ε s ε s b ε s α ε s ρ ε s α b ε s α ρ ε s ρ b ε s ρ b α keeping the not mentioned parameters constant for the constant parameters the escimo default values were used table 1 the mcmc procedure run for 100 000 updates thinning 100 with rapid convergence stable solution after ca 25 000 fig 1 for α ρ ε s for b the convergence was much poorer i e the solution fluctuated always after model validation the bayesian procedure was applied to observational data gathered routinely at three weather stations erfurt weimar artern wasserkuppe and estimated the four parameters α ε s b ρ as well as their uncertainty for different parameter combinations finally the performance of the estimation procedure and the generalizability of the model were assessed from the quality of forecasts calculated with observed data for the 2 5 years and for a 51 years period january 1961 december 2012 4 results 4 1 model validation validating the suggested inverse modelling procedure all parameter estimates were compared with the parameter values used for simulating the data in result the estimates deviate more or less from the exact values represented by a red line in fig 2 for an α value used to simulate daily snow depths during the 2 5 years a distribution of estimated albedo values was obtained plotted as median 25th and 75th percentiles in fig 2 this distribution clearly changed with the α used for the simulation the distribution was also different when α was estimated jointly with other parameters for example the estimation of α alone and in combination with ρ and b provided the same results likewise results overlap for all groups involving α and ε s these results were similar for stations erfurt weimar and artern a feature that is obvious for all three stations was the occurrence of a strong bias towards larger albedo estimates if the albedo is estimated together with the snow emissivity ε s in contrast snow density and soil heat flux did not bias the albedo estimate the dependence between the estimates α ˆ and ε ˆ s is a consequence of 5 where the albedo is a coefficient of global radiation g and snow emissivity is a coefficient of the surface temperature ϑ s see equation 11 a quantitative comparison of the terms in 5 reveals that for large albedo α 0 9 the short and the long wave energies are similar during clear sky conditions the radiation and surface temperature data are collinear and this generates an association between the estimates α ˆ and ε ˆ s further the bias of α ˆ was largest for wasserkuppe and smallest for artern this might have been caused by different ageing of snow due to differences in altitude ca 800 m and short wave radiation in our validation study snow depth data were simulated for ε s 0 99 which is too large compared with the real snow emissivity estimated for wasserkuppe see below and therefore the albedo was overestimated testing a more realistic value of ε s 0 90 for simulations the bias of the estimated albedo jointly estimated with ε s was clearly reduced cf asterisk in fig 2 this finding suggests that for subalpine regions the default ε s 0 99 in escimo might have to be replaced by a more adequate value later on we will recommend values for ε s on the basis of a joint estimation of albedo and snow emissivity from observational data see below another clear and general result was that the error range of the albedo estimate decreased with increasing albedo estimations become more precise with the availability of more data that means for long spells of snow which can be generated by repeated snow fall in this way long spells represent the large albedo of fresh snow α 0 8 0 9 kraus 2008 conversely short spells of snow cover can represent aged snow α 0 45 0 90 kraus 2008 comparing different stations it turned out that albedo estimates equal the theoretical albedo only for values larger than approximately 0 5 for α 0 5 the error increases and for very small albedo values the estimate was nearly independent on the albedo used for simulations fig 2 this is similar in the plots for erfurt weimar and artern not shown here this can be understood as a consequence of the differences in the number of days without snow that was 92 89 75 in the considered time period for artern erfurt weimar and wasserkuppe respectively for large albedo values α 0 90 the bias vanished for all stations even for the albedo estimated jointly with snow emissivity and the error range diminishes to about 1 for small albedo values α 0 50 the estimation is strongly biased and therefore useless though the estimates of snow density ρ tended to be slightly biased by a joint estimation with snow emissivity fig 3 this bias was not significant because the error range included the exact agreement between theoretical and estimated values there was a general but also not significant tendency to overestimate the snow density as wasserkuppe had more days with snow cover the precision of the estimated snow density was higher compared with the other stations the estimation of snow emissivity ε s had very diverse results fig 4 for wasserkuppe all estimates cluster in three distinct groups fig 4a a joint estimation of ε s with albedo generates an overestimation and an estimation together with snow density but not with albedo generates an underestimation of the snow emissivity in the range 0 95 ε s 0 96 for erfurt weimar just those estimations involving the albedo were biased towards larger snow emissivity and all others were unbiased fig 4b for the station artern all estimations of snow emissivity were nearly not biased fig 4c for values ε s 0 92 the estimation of snow emissivity was only slightly biased our simulations confirm the above observation that estimates of albedo and snow emissivity interact therefore the accuracy of snow emissivity estimations might be improved for other values of the albedo an aim of the present study was to demonstrate the usefulness of bayesian inference for snow depth modelling to illustrate the advantage of the bayesian approach over the maximum likelihood method the latter was used as an alternative to estimate the parameter snow emissivity ε s for the station wasserkuppe for the estimation of this parameter and the situation of snow cover s d 0 without any snow or rain s r 0 equations 1 12 can be reformulated to a generalized regression model this regression with offset involves an exponential distribution of the snow depth s d t exp λ t given ρ 0 1 g c m ³ and the daily values of s d t ϑ t r e l h u m t u t an estimate of the snow emissivity can be calculated e g in r using glm with gamma distribution as generalization of the exponential and with inverse link function and results in ε ˆ s 1 11 0 14 0 97 1 25 maximum likelihood estimators approximately follow a normal distribution this interval estimate does not comply with the restriction 0 ε s 1 however in contrast the bayesian estimate ε ˆ s 0 99 0 01 clearly corresponds to the value ε s 0 99 that was used to simulate the snow depths and remains in the interval 0 1 fig 4a the estimation of the soil heat flux b was rather poor fig 5 for all stations there was not only bias but also a very large error range considering the magnitude of the energy terms in the balance equation 5 it turns out that b is smaller than all other terms by a factor of 1 10 1 100 therefore minor inaccuracies of the other energy terms will have an immense impact on the estimation of the soil heat flux another reason for the large estimation error of b is that the soil temperature that is relevant for the soil heat flux can be different escimo assumes a soil temperature equal to the snow temperature of 273 15 k and this is just an approximate parameterization of the model further the bias of the soil heat flux estimate towards lower values might be caused by the fact that the escimo equations were used for daily time steps originally it was designed for hourly time steps and was able to account for reduced soil heat fluxes during the night this makes the daily average soil heat flux smaller than the flux during daytime summarizing our validation study of all the four considered parameters we conclude that snow density was estimated most accurately and this is in agreement with the stable solution of the mcmc simulation fig 1 in contrast the estimation of soil heat flux was very poor albedo and snow emissivity estimates interact and this suggests making a joint estimation in order to have reasonable values for both 4 2 parameter estimation from observational data in an application of the suggested inverse modelling procedure the four parameters α ρ ε s b were estimated from real snow depth data recorded at three stations during 2 5 years august 2010 december 2012 for each station all parameters have been estimated in different combinations at the beginning each parameter was estimated alone and the other parameters were set to fixed values this was completed by pairs and triples of parameter estimates finally all four parameters were estimated jointly and this procedure resulted in consistent estimates extending the original escimo model that calculates the snow water equivalent s w e here the snow depth s d was modelled directly the application study demonstrated that the ratio s w e s d significantly depends on the state of the snow cover and therefore different snow densities were included for new snow ρ o for melting snow ρ m for sublimated snow ρ s and for frozen rain on snow cover ρ r into the mass balance eq 2 all snow densities are limited in their value by the density of ice that is ρ i c e 0 918 g c m ³ and this was taken into account in the inverse modelling approach joint estimations of the four densities always resulted in ρ ˆ s ρ ˆ r ρ ˆ m ρ i c e generally the estimated density of new snow was lower than for ice ρ ˆ o ρ i c e even for aged snow covered by firn or ice the density was below 0 6 g c m ³ an estimation of the snow density from observational data table 2 resulted in station specific values of ρ ˆ o 0 14 g c m ³ and ρ ˆ o 0 05 g c m ³ for wasserkuppe and erfurt weimar respectively for artern the snow density was consistently estimated as ρ ˆ o 0 11 g c m ³ obviously the snow density increases with the accumulation and ageing of snow and this occurs more likely at a mountain station wasserkuppe as opposed to an urban lowland station erfurt weimar where the time of snow cover is reduced due to urban heat island effects the snow emissivity ε ˆ s varied between stations table 2 the estimation of the soil heat flux b ˆ was very uncertain as a consequence of the dominance of the other parts in the energy balance nevertheless our estimations indicated that a value of b ˆ 1 0 w m ² is reasonable for the soil heat flux in our study region the default value in escimo was 2 0 w m ² all achieved estimates are summarized in table 2 as a recommendation for the application in escimo with data relevant for sub alpine regions the estimated albedo was site specific and decreased from wasserkuppe α ˆ 0 94 to erfurt weimar α ˆ 0 89 to artern α ˆ 0 56 these changes in the albedo correspond to the changes in the altitude of these stations 921 m 316 m 164 m a s l stations at higher altitudes experience new snowfall more often and this is associated with a larger albedo strasser and marke 2010 used albedo values between 0 45 and 0 90 combining albedometer measurements with landsat tm images for an alpine glacier strasser et al 2004 developed an exponential decay parameterization of the albedo α α min α a d d e k n with n representing the number of days since the last considerable snowfall i e at least s w e 0 5 m m h which caused an increase of the snow albedo to its maximum value α min α a d d α min is the minimum albedo of old snow α a d d is an additive albedo and k is a recession factor that is defined as 0 12 for positive temperatures and 0 05 for negative temperatures strasser and marke 2010 this parameterization was also used in escimo and specifies the maximum albedo to 0 95 but this level can be clearly exceeded by fresh snow in alpine regions fig 17 in strasser et al 2004 our results for sub alpine stations suggested that the albedo was often below this level further for our stations the additive albedo changes were not significant table 2 an important observation is that the albedo estimation is based on the energy balance eq 5 of the escimo model and this energy balance is involved merely in the amount of melting snow eq 4 that means albedo estimates are calculated only from observational data that refer to melting and therefore α ˆ is the albedo of melting snow for new snow an estimation of α is not possible using the escimo approach 1 12 this fact explains the unexpected small values of α ˆ and the insignificance of the snow ageing obtained from our observational data 4 3 assessment of forecasting performance assessing the quality of the parameters estimated by an inverse modelling of the 2 5 years observational data sd o predictions sd p were calculated for 2 5 years and 51 years periods while the former demonstrates the precision of model fit the latter represents the model s ability for generalization following performance measures were applied 16 coefficient of determination r 2 t 1 n s d t o s d o s d t p s d p i 1 n s d i o s d o 2 k 1 n s d k p s d p 2 2 17 index of agreement i a 1 t 1 n s d t o s d t p 2 i 1 n s d i p s d o s d i o s d o 2 18 and root means square error r m s e 1 n t 1 n s d t o s d t p 2 19 mean absolute error m a e 1 n t 1 n s d t o s d t p 20 mean bias error m b e 1 n t 1 n s d t o s d t p 21 and fractional bias error f b e t 1 n s d t o s d t p 0 5 i 1 n s d i o s d t p r 2 measures the percentage of variance in observed snow depths explained by the statistical model but r 2 is not able to assess a bias between observed and predicted data willmott 1982 willmott et al 1985 for a more comprehensive evaluation the index of agreement i a 0 1 was included that indicates a perfect prediction for i a 1 m a e m b e and f b e indicate systematic errors the parameter values achieved from the bayesian estimation procedure summarized in table 2 were used for snow depth predictions and an assessment of their quality table 3 results suggest that the performance was clearly improved for the parameter values estimated by the suggested bayesian procedure referring to the 2 5 years period predictions for the 51 years period were slightly less precise a conclusion is that the parameter estimation can be recommended for each specific site before escimo is used to forecast the snow depth at this site the sensitivity of forecasting performance on the soil heat flux was very low 5 conclusions and limitations the escimo model was utilized for an inverse approach estimating the model parameters from meteorological data gathered at three stations situated in the central german uplands responding to our research hypotheses we conclude that inverse modelling is suitable to estimate model parameters and to calculate predictions for snow depths at sub alpine sites with urban influences which are so far underrepresented in the scientific literature to compare and generalize the results parameter estimation is desirable for other sub alpine regions while the german weather service dwd runs 52 climate stations in thuringia including wasserkuppe that is located near the border only 5 stations have complete datasets and only 3 that represent different elevation zones were selected for the present study escimo needs daily values for air temperature and humidity air pressure precipitation wind speed global radiation and snow depth bayesian inference can handle gaps of missing data but this will drastically increase the computing time and decrease the performance for that reason the suggested procedure might be not feasible at many sub alpine stations extending the escimo code from the snow water equivalent to the snow depth we introduced snow densities for new snow melting snow sublimated snow and frozen rain on snow cover as new parameters that need to be estimated these additional parameter estimations are required because routine measurements provide just snow depths and the snow density can vary in time and space observational data of snow density are rare recently pohl et al 2014 applied a sensor network for snow density measurements and therefore an advantage of the bayesian approach is the possibility to assess this parameter specifically for each station this estimate provides valuable information about site specific conditions the validation study section 4 1 clearly demonstrated how the parameter estimates are mutually correlated and therefore a joint estimation of all parameters is recommended this procedure guarantees that all parameter values are adequate to the prevailing state of the snow cover the estimated site specific parameters improve the predictions of escimo compared to the use of default parameter values suggested with escimo spread see table 3 the occurrence of site specific values for the parameters confirmed that the accumulation of snow depends on the altitude and the urban or mountain characteristic of a station in hydrological applications of snow models such as the u s national weather service s model snow 17 anderson 2006 bayesian techniques are often utilized to predict the snow water equivalent swe together with the extent of the snow covered area sca for that purpose data assimilation techniques such as ensemble kalman filter ensemble square root filter or particle filter have been developed that utilize in situ e g snow pillows and remotely sensed e g the normalized snow density index calculated from satellite images measurements to improve model predictions leisenring and moradkhani 2011 as all these measurements are subject to uncertainties sequential bayesian updating can stochastically combine measurements with model simulations and in this way estimate the probability distributions of the filters state variables leisenring and moradkhani 2011 found a data assimilation performance of r m s e 29 51 m m for their modelled s w e in mm assimilating landsat 8 data with the output of the senorge snow model saloranta 2016 stigter et al 2017 calculated spatiotemporal predictions of swe with a resolution of 100 m and a daily time step for their alpine study area in the central himalayas they used snow albedo values of 0 713 brock et al 2000 and applied two different albedo decay models logarithmic decay for deep snow s w e 5 m m exponential decay for shallow snow s w e 5 m m the present study demonstrates the very different situation under sub alpine conditions where the estimated albedo varied between 0 56 and 0 94 and the decay was statistically not significant while saloranta 2014 specified the minimum snow density to 0 05 g cm³ for senorge predictions of swe and hopfinger 1983 the density of avalanching snow to 0 2 g cm³ for the sub alpine stations with urban impact we find snow densities of 0 05 0 14 g cm³ stigter et al 2017 used the optimized senorge model to calculate ex post predictions of snow depth and compare them with past observations for their himalaya study region the forecasting accuracy varied between r m s e 49 m m for a region where snow depths are up to 400 m m and r m s e 314 m m for a region where snow depths can reach 2000 m m the present study resulted in a forecasting accuracy between r m s e 308 m m for wasserkuppe snow depths up to 2500 m m and r m s e 43 4 m m for arten snow depths lower than 400 m m the performance measures table 3 indicate that the generalization of the model is reasonable the precision measured by r 2 i a and r m s e is not reduced when the parameters estimated from 2 5 years of observational data are used to predict the snow depths during 51 years in contrast the accuracy of the forecasts is reduced when the default parameter values derived in alpine snow studies are used cf the extremely large r m s e for wasserkuppe the always negative values for m b e and f b e indicate that the model predictions are biased to lower snow depths improvements are needed for the estimation of the soil heat flux the parameterization applied in escimo is rather simple and an involvement of the soil temperature profile might be beneficial possibly the chosen exponential prior is not optimal and might be improved a third problem to be solved with the estimation of b arises from its small value compared to the other terms in the energy balance originally escimo was developed for hourly time steps strasser and marke 2010 for compatibility with observed meteorological data for which only daily data were available we modified the time base of escimo from hourly to daily time steps and this might have an impact on the interpretation of the parameter values especially the parameters α ρ o ε s b studied here represent daily values while in the original escimo code they can vary during the day this means the parameters on a daily basis are certain averages of these hourly data and are attenuated in their extremes for example the ageing of snow appears smoothed in the daily parameters nevertheless escimo proved to be applicable to both hourly and daily data as escimo is a physically based point snow surface model all estimated parameters refer to the location where the data have been gathered our approach provides point values of snow density albedo snow emissivity and soil heat flux the model does not give any information about the changes of these parameters along the snow surface and with increasing distance to the observation site the extension of the original escimo model by a parameterization of the long wave radiation eq 11 12 proved to be beneficial as this made the presented inverse model applicable with routine monitoring data from which long wave radiation is often not available including parameters for snow densities into the original escimo model considerably improved the performance of the model because these parameters are mostly not available at the monitoring sites even the snow prediction for new datasets takes advantage of the parameters estimated by the inverse model another extension of escimo was recently suggested by marke et al 2016 who advance the model to inside canopy conditions 6 software and data availability the study is based on data gathered in the frame of the regional climate information system rekis they were freely downloaded as text files from www rekis org for the stations wasserkuppe erfurt weimar and artern as daily values for a period of 51 years january 1st 1961 until december 31st 2012 for all calculations the free language r for statistical computing www r project org was used for bayesian inference the software openbugs bugs bayesian inference using gibbs sampling ntzoufras 2011 is based on the markov chain monte carlo mcmc method gelfand and smith 1990 and is freely available at www openbugs net where all further information is provided the code for the inverse model developed by the authors is provided in the supporting material acknowledgements the authors are grateful to frank heyner klimaagentur thüringen thüringer landesanstalt für umwelt und geologie tlug for inspiring discussions and help with the data needed for the performance assessment d h was financially supported by the deutsche bundesstiftung umwelt dbu osnabrück az 20015 373 appendix a supplementary data the following are the supplementary data related to this article mmc1 mmc1 mmc2 mmc2 appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 01 010 
26313,environmental processes are highly complex and their understanding involves the analysis of various quantitative and qualitative parameters physical chemical geographical etc which are more or less correlated appropriate environmental knowledge can deal with this complexity in a tractable way such knowledge is essential for solving particular environmental problems generating valuable environmental knowledge is a challenging research topic especially for environmental data science as efficient knowledge can lie behind data integrated environmental modelling uses a holistic view and can provide a possible better solution to environmental problems understanding the paper presents a knowledge modelling framework for intelligent environmental decision support systems iedss by following such a holistic perspective thus the proposed framework integrates an ontological approach and two data analysis approaches data mining and bayesian networks which are applied for the generation of a knowledge base that is used by an iedss for decision making the application of the framework is illustrated on three case studies from different environmental domains 1 water river resource management river water pollution analysis 2 air air pollution analysis ozone prediction and 3 soil soil pollution analysis keywords knowledge modelling framework integrated environmental modelling bayesian networks data mining ontology intelligent environmental decision support system 1 introduction one of the current challenging research directions in environmental sciences is integrated environmental modelling iem see e g laniak et al 2013 that can be viewed as a possible better solution to the environmental processes understanding problem iem applies a holistic view on environmental problems solving in this context various data analysis methods and techniques as e g statistical and artificial intelligence based can be combined and applied in order to provide valuable environmental knowledge for the decision making process of an intelligent environmental decision support system several artificial intelligence ai methods can be used for environmental systems modelling see e g the overviews presented in chen et al 2008 and struss 2008 examples of such ai based methods are knowledge based methods e g knowledge based systems expert systems case based reasoning and computational intelligence methods e g fuzzy inference system artificial neural networks genetic algorithms and swarm intelligence methods these methods can be used for integrated environmental modelling in decision support systems many environmental problems can be tackled with intelligent tools such as iedss a variety of iedss applications being reported in the literature as for example modulus dss for land degradation in the mediterranean oxley et al 2004 an iedss based on multi agent systems for water management urbani and delhom 2005 i ekbase an iedss for sustainable agriculture dutta et al 2014 flire dss a web based dss for floods and wildfires in urban and periurban areas kochilakis et al 2016 an overview on key challenges and best practices in environmental decision support systems development is presented in mcintosh et al 2011 while selected worldwide developed iedss tools are briefly described in gibert et al 2012 some frameworks for iedss development were also reported in the literature an example being given in sànchez marrè et al 2008a other promising model based approaches for edss development include qualitative models such as the qualitative reasoning model for algal bloom in the danube delta biosphere reserve proposed in cioaca et al 2009 the qualitative dynamic model adapted for hydroecology which is presented in heller and struss 2001 the abductive reasoning models described in wotawa et al 2010 and wotawa 2011 and other ai based models see e g the models discussed in sànchez marrè et al 2008b and struss et al 2003 the purpose of our research work is to provide a knowledge modelling framework for intelligent environmental decision support systems the paper presents an environmental knowledge modelling framework that integrates an ontological approach and two data analysis approaches data mining and bayesian networks which are applied for the generation of a knowledge base used by an iedss for decision making starting from a conceptual model of the environmental problem it is developed the problem domain ontology used in the next steps when data mining and bayesian networks are applied to generate rules from data sets and decision tables the application of the framework is shown on three case studies for solving different environmental problems in the domains of water air and soil 2 literature review a brief literature review on the ontological approach data mining and bayesian networks application in environmental science is presented 2 1 ontological approach in environmental science the ontological approach tackles expertise domain knowledge conceptualization providing a domain ontology which includes a vocabulary with terms concepts from general to particular ones and relations between concepts and axioms describing restrictions on concepts and relations actually it performs a domain knowledge modelling various applications of ontological approach in environmental applications are given in the literature e g wastewater management with ontology based edss ontowedss ceccaroni et al 2004 knowledge modelling in an air pollution control decision support system oprea 2005 flow and water quality modelling by using an ontology based knowledge management system chau 2007 integration of air quality models and 3d city models with ontologies metral et al 2008 air pollution ontology czarnecki and orłowski 2009 simulation in agricultural systems modelling based on ontology beck et al 2010 environmental impact assessment eia ontology covering the environmental experts terminology of various aspects such as water air soil habitat geophysical and socioeconomic impact garrido and requena 2011 description and classification of usda soil taxonomy up to soil series deb et al 2015 description of learning resources on organic agriculture with an agrovoc based ontology soil pollution analysis due to fertilization sánchez alonso and sicilia 2009 soil properties and processes ontology osp du et al 2014 knowledge modelling in river water quality monitoring and assessment xiaomin et al 2016 2 2 data mining in environmental science data mining dm integrates techniques from several domains as e g statistics machine learning and artificial intelligence and performs knowledge extraction from large data bases examples of dm techniques cios et al 2007 are decision trees rule algorithms hybrid algorithms artificial neural networks and statistical methods e g regression a brief description and classification of dm techniques oriented to decision making and some guidelines regarding the selection of the right technique are described in gibert et al 2010a some models and algorithms for decision making in a data driven environment are discussed in kusiak 2002 the dm approach was used in several environmental applications as e g water supply assets modelling babovic et al 2002 air pollution management policy making li and shue 2004 simulating farmers crop choices for integrated water resource management ekasingh et al 2005 water quality management using gis data mining karimipour et al 2005 analysis of water pollution effects on human health the results are used for water management by governmental authorities sokolova and fernández caballero 2007 agricultural soil profiles characterization armstrong et al 2007 air pollution monitoring and mining based on sensor grid ma et al 2008 air pollution modelling and short term air quality forecasting riga et al 2009 contaminant event locations identification in water distribution systems huang and mcbean 2009 soil carbon mapping in australia bui et al 2009 knowledge modelling in a waste water treatment plant by explicit knowledge discovery from the corresponding dynamic processes with the clustering based rules by states approach gibert et al 2010b water quality monitoring using remote sensing data mining wen and yang 2011 analysis of air pollution effects on humans including geographic heterogeneity assessment stanley young and xia 2013 air quality monitoring czechowski et al 2013 ground water quality assessment kolli and seshadri 2013 soil organic matter prediction teixeira et al 2014 soil property variation mapping by applying data mining to soil category maps du et al 2014 air pollution prediction siwek and ossowski 2016 2 3 bayesian networks in environmental science bayesian networks are directed acyclic graphs that represent the dependences between a set of variables and their joint probability distribution being probabilistic graphical models which integrate quantitative and qualitative data they are used in environmental modelling proving to be a useful support instrument for decision making in various environmental problems see e g urban air pollution forecasting cossentino et al 2001 farm irrigation in wang et al 2009 and robertson et al 2009 simulation of water flow in organic soils based on ontology kwon et al 2010 groundwater management for agriculture carmona et al 2011 air pollution related health risk assessment liu et al 2012 ghg gas emissions management in the british agricultural sector pérez miñana et al 2012 soil pollution assessment li et al 2015 water resource management in phan et al 2016 ecological risk assessment in estuarine ecosystems in mcdonald et al 2016 emergent water pollution accidents risk analysis in tang et al 2016 air pollution prediction via multi label classification corani and scanagatta 2016 an overview on bayesian networks use in environmental modelling is presented in aquillera et al 2011 another overview on bns use in environmental and resource management is described in barton et al 2012 while some good practice guidelines are synthesized in chen and pollino 2012 also it is important to note that bayesian networks can be generated by using the domain ontology see e g fenz et al 2009 2 4 review synthesis table 1 shows a synthesis of the literature review made for different environmental problems solving with the three approaches data mining bayesian networks and ontological approach within the framework of environmental decision support systems or knowledge modelling the main conclusion of the literature review is that all three approaches were applied with success to different environmental problems solving pollution prediction resource management monitoring control impact assessment water air and soil and have an important potential in generating valuable environmental knowledge however none of the studied problems were solved by applying all three approaches 3 proposed knowledge modelling framework 3 1 our approach starting from the literature review we have selected some artificial intelligence based methods data mining knowledge based systems that proved to tackle in a proper manner the problem of knowledge modelling in an iedss as environmental processes are very complex with several uncertainties and various variables we have chosen bayesian networks to manage knowledge uncertainty in such cases moreover bayesian networks handle heterogeneous data quantitative and qualitative being very useful in environmental problem description the paper proposes an environmental knowledge modelling framework which integrates three approaches for knowledge modelling ontological approach data mining and bayesian networks to be used in an iedss as shown in fig 1 the ontological approach allows problem domain conceptualization and provides concepts and relations that will be used in knowledge representation and when applying the other two approaches data mining and bayesian networks the data mining approach in our case rule induction inductive learning techniques decision trees and rules algorithms performs problem specific knowledge discovery from the available data sets i e problem specific data bases and decision tables bayesian networks are used to represent the probable dependences between different parameters as e g cause effect relations that describe the problem in our framework knowledge is represented under the rule form i e if premise then conclusion and knowledge uncertainty modelling is achieved with a probabilistic model bayesian networks a confidence factors model that quantifies the uncertainty with numerical values associated to rules generated for example with inductive learning dm techniques and a fuzzy model linguistic values associated to facts included in rules either in the premise or in the conclusion corresponding to each analyzed parameter specific ranges temporary databases t db associated to the current environmental problem context are added to the iedss databases db the knowledge bases built by our approach are kbdomain include domain knowledge under the form of rules and decision tables kbdm rules derived via inductive learning from available databases t db and decision tables kbbn probabilistic rules derived from bayesian networks and the final knowledge base kbproblem in which are integrated all rules necessary for the decision making procedure of the iedss the knowledge included in kbdomain and kbdm are using fuzzy model and confidence factors model for knowledge uncertainty representation the knowledge included in kbbn is represented as probabilistic rules finally the problem solving knowledge base kbproblem is using all the three models fuzzy model confidence factors and probabilistic model for the knowledge uncertainty representation the main problem with the mix of these three models regards rules uniformity in the case of probabilistic rules integration with the rules using the confidence factors model which needs further investigation our temporary solution was to keep all rules probabilistic and with confidence factors with human expert approval and possible correction and to apply all of them during reasoning selecting the most probable conclusion and the conclusion with the highest confidence factor finally the human expert deciding which one will be used by the decision making procedure depending on a specific problem s scenario context the basic components of a decision support system are data analysis and decision making the first component data analysis provides a part of the environmental knowledge mainly from collected data sets while the second one decision making uses this knowledge and additional problem domain knowledge for decision making the framework input is given by the environmental data sets collected data t db and db and environmental problem domain specification while the output is given by the knowledge base which contains the generated rules the quality of the environmental data used by the framework will strongly influence the derived knowledge base quality and thus the quality of the decision made by iedss therefore data cleaning checking and pre processing are required to be performed within data analysis a set of valuable guidelines for the application of pre processing techniques in the context of environmental data mining is presented in gibert et al 2016 the framework can be applied to different environmental problems e g environmental resource management monitoring analysis and control of environment quality i e quality of water air soil 3 2 framework description the proposed knowledge modelling framework has seven steps and starts with the problem analysis phase step 1 in which the problem solving domain is identified and a conceptual model of the problem is built followed by the phase of problem domain knowledge conceptualization i e problem domain ontology building step 2 and the phase of knowledge base construction via three approaches steps 3 4 5 6 knowledge acquisition from literature and human experts via inductive learning data mining decision trees and rule algorithms and through bayesian networks the results of the last methods being validated by human experts and using the problem domain ontology through reasoning the last phase of the framework is knowledge integration in the iedss knowledge base and validation step 7 through reasoning by using domain ontology and human experts image some more details related to the proposed framework are given as follows a scenario is an instantiation of the problem the conceptual model cmproblem contains only the key elements key concepts for solving the problem being a sort of initial sketch while the ontology ontodomain i e with all sub ontologies related e g to different scenarios must include more terms concepts and relations being a detailed conceptual description of the problem domain for all scenarios of the problem the environmental data set dataset t db as denoted in fig 1 contains time series data continuous data sets and or sample data discrete data sets some of the data are measurements collected with monitoring stations stationary or mobile under a monitoring network as e g an air quality monitoring network other are samples collected at different locations and moments of time as for example in river water pollution analysis while other data are characteristics of the environment area e g area geographical characteristics specific river characteristics the environmental data included in dataset can contain measurements errors e g from the automatic monitoring instrumentation noise uncertainty missing data redundancies irrelevant information etc thus some data cleaning and pre processing techniques are performed at the beginning of step 4 before applying the inductive data mining algorithms for example some data visualization tools such as boxplots time series plots two dimensional scatter plots or distributional plots can be applied in order to detect the presence of outliers errors missing values etc such problems can be solved manually or automatically by missing data imputation filtering for noise removal or other solutions usually provided within the data mining software package that is used also the cleaned and pre processed data will be properly prepared for the application of the selected data mining algorithms in order to fulfill the required assumptions feature selection and principal component analysis are examples of pre processing techniques performed for data preparation depending on the specific environmental problem available data analysis goal it is decided which are the most suitable types of data pre processing techniques required in step 4 the domain ontology ontodomain is used for concepts and relations validation in steps 3 4 5 and 7 in order to select the right concepts and relationships between them as during bayesian networks development step 5 it is used the ontology e g to define bn variables it is not necessary to include it also in step 6 when it is performed probabilistic rules extraction from bayesian networks starting from the environmental problem formulation the following issues are defined 1 a set of environmental parameters that need to be analyzed in order to solve the problem environmental variables with their associated values numerical or nominal as e g measurements or observations and or linguistic terms and a list of possible phenomena associated to the environmental problem linked to problem s scenarios 2 a data set for all some environmental parameters available past and current data with specific site area measurements observations and characteristics 3 a goal variable that need to be determined in order to solve the problem as e g the occurring of a certain phenomenon its possible effects on the environment on ecosystems or on human activity human settlements etc and the decisions that should be taken in order to minimize the negative effects the set of the environmental parameters includes area or site dependent parameters e g pollution sources specific water pollutants and specific air pollutant season dependent parameters which capture seasonality and other parameters dependent on the problem the human expert selects with higher priority the rules that include those parameters the core of the knowledge integration and validation step step 7 is detailed as follows suppose we have the following set of m rules from the three knowledge bases kbbn kbdm kbdomain that need to be integrated into one knowledge base image a rule is valid step 1 1 1 if it is in accordance with the conceptual model cmproblem or with the conceptual models of each identified scenario cmsi i e the relation between the parameters from the rule s premise and conclusion is in accordance with the conceptual model cmproblem or cmsi the correction of a rule step 1 2 2 i means the inclusion exclusion of some additional parameter s in from the rule s premise and or conclusion in accordance with the conceptual models cmproblem and cmsi also the rule correction can involve the change of its parameters name or their symbolic values in order to have uniformity in the final kbproblem the final validation of the rules included in kbproblem it is performed by the human expert through reasoning by rule chaining for all identified scenarios of the problem during validation the human expert checks the characteristics of the knowledge base coherence the conclusions provided by rule chaining are not contradictory if the initial facts are not contradictory no redundancy the rules are not repeated under different forms and completeness all useful rules for problem solving are included i e the knowledge base allows complete resolution the data mining approach rule induction is used to discover rules in data sets e g time series and decision tables and to identify the most correlated parameters which are included as nodes in the bayesian networks in order to keep simplicity several bayesian networks with smaller complexity are built and analyzed probabilistic rules are derived from them the problem domain ontology is used to design the bayesian networks and to check rules consistency as well as to guide the selection of the right rules that are derived with inductive learning techniques and bayesian networks an important benefit of using inductive rule data mining in step 4 of km framework is that the knowledge derived from problem specific datasets are priority rules to the more general knowledge derived in step 3 being problem dependent actually the rules from kbdm are refined rules of those included in kbdomain another advantage of using inductive learning techniques is their robustness to missing data cios et al 2007 the framework uses two types of inductive learning algorithms decision tree based algorithms indirect methods for rule generation from decision trees and rule based algorithms direct methods for rule generation directly from the data set depending on the environmental problems it is selected the proper inductive data mining algorithm by experimenting several ones from both types for example by choosing them from the algorithms available in the data mining software package as e g weka workbench witten et al 2011 used for the case studies described in the next section which has several types of inductive learning algorithms decision tree and rule based examples of decision tree based algorithms that can be applied under the proposed framework are id3 quinlan 1986 c4 5 quinlan 1993 implemented in weka as j48 algorithm m5 quinlan 1992 wang and witten 1997 implemented in weka as m5p algorithm cart friedman 1977 breiman et al 1984 and others e g rp algorithms reptree etc some of these algorithms use information gain criteria e g id3 c4 5 reptree while others use least square criterion e g m5 cart mean absolute deviation criterion e g cart or other criteria e g least absolute deviation the rule based algorithms considered by the framework include cn2 clark and niblett 1989 m5rules the rule based version of m5 implemented in weka and other specific algorithms as e g decision tables part oner that are implemented in weka some of the inductive learning algorithms are sequential covering algorithms learning one rule at a time as e g cn2 while others e g id3 c4 5 are simultaneous covering algorithms learning the entire data set simultaneously in the next section three case studies are presented showing the framework application to different environmental problems in the domains of water air and soil due to space limitation a more detailed description is given only for the problem of navigable river resource management presented in the first case study 4 case studies 4 1 case study 1 environmental domain water we are considering two problems from the water environmental domain water resource management and surface water pollution the application of the proposed framework is detailed as follows problem pw1 water resource management of a navigable river problem description water resource management is an environmental problem with major implications in life on earth i e human life flora and fauna agriculture industry etc as an example we have considered the problem of water resource management for a navigable river the danube river the main problems encountered in the river area due to water level variation are floods occurring during rainfalls and ice rapid smelting worsening or suspending navigation activity during drought usually in the summer season and icepack during winter season affecting hydropower plants activity and irrigation during drought in this case the main goal of the water resource management is to avoid or to reduce the negative effects of each problem occurring in the river area the danube river area as for example by informing the navigators e g sending notification when a navigation problem occurs due to drought or icepack forming the local authorities e g sending warning or alerts when floods can occur in a certain area or sending notification when drought will affect irrigation and thus the agriculture and farming or the hydropower plants management e g sending notification when the river level is affected by drought step 1 conceptual model design for pw1 starting from the problem description it was designed a conceptual model cmpw1 that is presented in fig 2 we have considered water level i e river level precipitations season and temperature as the main parameters drought and icepack as the phenomena that can occur floods as the potential effect of water level significant increase icerapidsmelting a potential effect in case a significant temperature increase is registered and irrigation navigation activity and hydropowerplant activity as the affected activities due to drought floods or icepack the parameters set and the corresponding sets for the identified phenomena effects and affected activities are given below parameters set waterlevel temperature precipitations season phenomena icepack drought effects floods icerapidsmelting affectedactivities irrigation navigationactivity hydropowerplantactivity problem pw2 surface water pollution analysis problem description surface water pollution analysis e g for rivers lakes is another important environmental problem with implications on aquatic life human health etc in this case the goal is to determine the level of surface water pollution the main sources of surface water pollution are agriculture runoff urban runoff industry transportation waste soil pollution and air pollution examples of water pollutants are nutrients fluoride chlorides ammonium and particulate matter step 1 conceptual model design for pw2 starting from the problem description we have designed a conceptual model cmpw2 that is shown in fig 3 the main parameters that were considered are water pollution source and water pollutant while the goal variable is water pollution level parameters set waterpollutionsource waterpollutant goal waterpollutionlevel step 2 ontology development for both problems pw1 and pw2 we have developed one prototype ontology for the water domain ontowater as denoted in the framework named water onto for both problems pw1 and pw2 starting from the two conceptual models cmpw1 and cmpw2 the main concepts related to water resource management for a navigable river and water pollution were included as classes the ontology implementation was done in protégé 4 3 as an owl ontology fig 4 shows the class hierarchy of water onto and a screenshot with concepts of the physicalindicator sub class as e g total suspended solids tss conductivity turbidity watertemperature apart from the concepts that were included in the water onto ontology some problem specific relations between concepts were defined as object properties examples of such relations are mayinfluence mayproduce maycause the default relations are the taxonomic ones is a and ako a kind of fig 5 shows a cause effect graph for the pw1 problem air temperature may influence water level through evaporation usually during summer when very high temperature can be registered and may cause drought the quantity of precipitations may influence the water level may produce flood when important quantity of precipitations are registered or may cause drought by lack of precipitations step 3 knowledge acquisition for both problems pw1 and pw2 in this step the main knowledge related to each problem solving pw1 and pw2 was taken from literature and human experts according to the designed conceptual models cmpw1 and cmpw2 and was described as if then rules and decision tables being included in the corresponding knowledge base i e kbpw1 domain for pw1 and kbpw2 domain for pw2 the water onto ontology was used to select the proper concepts included in knowledge representation e g parameters names the knowledge uncertainty was quantified by linguistic terms fuzzy model in the case of decision tables and confidence factors in the case of rules some examples of knowledge and their source are given below for pw1 problem problem pw1 kbpw1 domain table 2 synthesized human expert knowledge related to the main causes of the three phenomena that were considered in pw1 problem flood drought icepack and ice bridge occurrence and the affected activities or entities to whom the river resource management authority will send notifications in case of agriculture farming and human settlements local authorities will be informed while in case of river navigation periodically reports will be sent to river navigators table 3 shows the names of the parameters that were considered in solving the pw1 problem their domain and measurement unit the abbreviated name and their associated set of linguistic terms the first goal variable flood refers to the possibility of flood occurrence the second goal variable decision refers to the navigable river resource management decision and can represents no decision no problem occurred send an informing notification inf send a notification for restricted activity res or for stopping activity sto we have considered the following site area dependent river level s thresholds river warning level i e the river level above which it is possible to occur flood river flood level i e the river level above which the flood occurs and river minimum level i e the level of the river under which some activities will be affected as e g navigation irrigation hydropower plants activity with the corresponding values for a certain locality along the river where the water level is measured examples of rules related to site area dependent parameters for setting linguistic terms as e g for the river level variable riverls and the values of other variables flood decision image another set of rules are referring to the setting of linguistic terms associated to the numerical parameters set some examples are given below the estimation of the significant normal and insignificant level of precipitations during a certain season and geographical area is established by human experts the ps parameter denotes the linguistic term associated to a certain numerical value of the precipitations parameter we have to notice that each of the following three rules is established for each season the same for temperature air or water where we have the linguistic values of very high high normal low very low which depends on the season as we refer to a geographical area with temperate climate the season parameter can have a value from the following set spring summer autumn winter image decision tables are built by human expert starting from the knowledge related to pw1 problem analysis and already included in the conceptual model cmpw1 as for example water level is influenced mainly by the precipitations quantity and evaporation however during winter when icepack are formed on the river and when higher temperature are recorded possible floods can appear due to rapid icepack smelting rainfalls can increase significantly the level of the river and thus can produce floods affecting hydropower plant activity agriculture and human settlements evaporation can influence water level usually during summer when higher values of air temperature are registered and also few precipitations or lack of precipitation are recorded table 4 a b c d present selected examples from pw1 dt1 decision table for each season of a temperate climate autumn winter spring summer in which it is analyzed the possibility of occurring the phenomena of icepack ice bridge drought or flood table 5 presents some examples from the pw1 dt2 decision table in which are explicitly analyzed the possible effects of the drought and icepack phenomena on the activity of hydropower plants and navigation the parameters that were used are phenomenon riverls riverlvar navigationactivity and hydropowerplantactivity the linguistic terms used for the last two parameters are the following normal possible restricted restricted stopped we have specified a confidence factor cnf for the normal value in the first example the default value of cnf for the other examples is 100 a set of heuristic rules can be extracted from the rows of the previous decision tables however we shall see in step 4 data mining that a smaller number of rules can be extracted with inductive machine learning techniques keeping only the most representative correlated parameters other examples of heuristic rules with confidence factors established by human experts are given below image step 4 knowledge discovery via data mining for pw1 problem during the knowledge discovery step of the framework i e step 4 are performed data cleaning data pre processing and inductive data mining we have used public available datasets collected from hydro meteorological reports provided by the down stream danube river administration r a galati romania www afdj ro to cotele dunarii and the decision tables generated in the previous step the main data included in a hydro meteorological report are air temperature at 7 a m the water temperature daily average value the river level the river level variation and trend the specification of season specific phenomena e g snow level height presence of icepack or ice bridge other meteorological data such as precipitations wind speed wind direction atmospheric pressure etc these values are given for each locality observation point situated along the dunarea river however we have encountered a problem precipitations quantity missing from the collected reports which was compensated by the decision tables given by human experts we have used the weka 3 8 0 data mining software package before applying the data mining algorithms the data sets were cleaned checked and pre processed by using the available tools from weka as e g graphical visualization tool plot matrix visualizing all environmental variables filters for noise removal also some pre processing steps required by specific inductive data mining algorithms were performed as e g normalization feature selection two types of inductive learning techniques implemented in weka decision trees and rule algorithms were applied to decision tables and to the cleaned and preprocessed datasets in order to derive a set of heuristic rules and the most correlated parameters several experiments with different classifiers from the two type of inductive learning techniques were performed under the test mode 10 fold cross validation and the best results were registered the specific inductive learning algorithms that were selected in this case study are m5p reptree and j48 from the decision tree based algorithms and m5rules from the rule based algorithms we give a synthesis of some experiments that were performed providing examples of rules or correlations that were derived and included in kbdm knowledge base some details related to the data pre processing and inductive data mining steps are given for all experiments except the one performed on decision tables experiment 1 identify the correlations between water level air temperature water level variation and icepack presence during winter season january february 2017 dataset num riverdunarea2017 jan feb arff numerical values from hydro meteorological reports attributes airtemperature watertemperature riverlevel riverlevelvariation icepackpresence a1 data pre processing 1 attribute selection with best first strategy selected attributes watertemperature riverlevel riverlevelvariation 2 data normalization a2 data mining inductive learning inductive dm algorithms m5rules m5p reptree best results m5rules classifier results validated by human experts riverlevel depends on watertemperature and icepackpresence experiment 2 identify the correlations between water level air temperature water level variation and phenomenon presence in any season dataset nom riverdunarea2017 arff numerical and nominal values attributes airtemperature watertemperature riverlevel riverlevelvariation phenomenon season a1 data pre processing 1 attribute selection with best first strategy selected attributes watertemperature riverlevel riverlevelvariation 2 data normalization a2 data mining inductive learning inductive dm algorithms m5rules m5p reptree best results m5rules classifier results validated by human experts riverlevel depends on watertemperature and phenomenon drought icepack etc thus from the two experiments it was derived that airtemperature parameter is not relevant for our problem and it can be ignored in further rules actually watertemperature parameter is important also the riverlevel parameter depends on watertemperature and phenomenon the importance of the phenomenon parameter to the riverlevel parameter evolution was used in the next step of the framework for bayesian network design experiment 3 identifying the most representative rules from decision tables of kbdomain examples of rules mined from the pw1 dt1 decision tables with j48 classifier c4 5 like algorithm which gave the best results are shown in fig 6 the results for the statistic parameters are kappa statistic 0 7119 mean absolute error mae 0 1212 and root mean squared error rmse 0 286 image some of the rules from kbpw1 dm are used for building the bayesian networks e g when setting the values from the conditional probabilities tables step 5 bayesian network development pw1 problem scenarios three scenarios were identified for pw1 problem drought occurrence and its effects icepack and ice bridge occurrence and their effects flood occurrence and its effects we have combined all three scenarios into one scenario the drought flood icepack combined scenario which is shown in fig 7 as a graph in ontograf protégé 4 3 starting from combined scenario we have built in weka the bayesian network shown in fig 8 the bn nodes were set by using the water onto ontology and the relations between them as well as the conditional probability tables cpt were set according to the rules derived in the previous two steps of the framework we have to notice that in contrast with the scenario shown in fig 7 we have eliminated airtemperature from the bayesian network as it was less relevant for all three cases and we have included the riverlevel parameter which is more important in two cases drought and flood ignoring the case of icepackrapid smelting which can be managed by rules from kbdomain step 6 knowledge acquisition from bayesian networks from the bayesian network developed at the previous step we have derived the probabilistic rules examples of such rules are given below image where p is used to denote the probability step 7 knowledge integration and validation the knowledge derived in the previous steps are integrated into the final knowledge base kbpw1 according to the algorithm described in section 3 for step 7 of km framework repeated rules are eliminated while other rules are corrected as for example in case of the probabilistic rules the name of the parameters and some symbolic values are changed in order to keep rules uniformity in kbpw1 also new rules could be added in order to cover new problem s scenarios the validation of the knowledge base was performed on several scenarios of the problem via rule chaining and final result analysis by the human experts we present rule chaining in three particular cases of the pw1 problem case 1 winter phenomenon ice bridge on dunarea river image in this case the conclusion of the probabilistic rule pw1 bn3 is kept together with the conclusion of rule pw1 dm 3 and finally the last one conclusion is chosen as true in the given scenario context this simple case provided an idea about the problems that should be tackled by the knowledge integration algorithm proposed in section 3 case 2 drought scenario image in this case the conclusion of the probabilistic rule pw1 dm 7 is kept as true in the given scenario context case 3 ice rapid smelting scenario new scenario initial facts season spring airtvar increase phenomenon icepack riverls high for this case we have added the following rules during rules integration image the last rule was corrected i e the name of the parameters and some symbolic values were changed in order to keep rules uniformity in kbpw1 4 2 case study 2 environmental domain air we are considering two problems from the air environmental domain air pollution analysis pa1 and air pollution short term prediction ozone prediction pa2 the application of the proposed framework is detailed as follows problem pa1 air pollution analysis problem description we have considered the air pollution analysis problem the main air pollutants are co nox so2 pm o3 voc which are usually monitored in most countries under national air quality monitoring networks the air pollution degree is influenced apart from the air pollutants concentration levels by other factors such as meteorological ones e g wind speed wind direction precipitation relative humidity temperature solar radiation atmospheric pressure etc season time etc the main sources of air pollution are transportation traffic industrial activities and domestic sources especially heating air pollution episodes can have potential negative effects on human health being an important environmental issue that should be properly managed for increasing the quality of life in urban areas the current legislation in most worldwide countries imposed limits for the concentration of major air pollutants and population warning via public available information of air quality index general and specific to each air pollutant in polluted areas e g urban sub urban with color codes related to possible human health effects and risks and recommendation to reduce them one of the air pollution phenomenon that can affect the quality of water and soil is acid rain which can appear during rains and air pollution episodes due to some air pollutants as e g nox so2 co2 higher concentrations another air pollution problem is ozone appearance especially ground level ozone affecting soil quality which is derived from chemical reactions determined by nox voc under certain meteorological conditions e g light wind warm temperature clear sky i e stable atmospheric conditions and is increased by so2 presence step 1 conceptual model design for pa1 starting from description of the pa1 problem we have designed a conceptual model for air pollution analysis cmpa1 which is presented in fig 9 problem pa2 air pollution short term prediction ozone problem description the second problem is ozone prediction by using current and past values of ozone concentration k hours ago and current concentration of some air pollutants that can influence ozone concentration level as nox so2 and voc step 1 conceptual model design for pa2 fig 10 shows a conceptual model for next hour ozone prediction problem cmpa2 step 2 ontology development for both problems pa1 and pa2 starting from the conceptual model we have identified the main concepts and relations between them and we have developed an ontology for the air pollution domain airpollution onto 1 implemented in protégé 4 3 in owl format the current version of the ontology has 143 classes 29 object properties 22 data properties 319 logical axioms 588 axioms 43 individuals the taxonomic relations are isa and ako i e a kind of other types of relations e g has causal relations compositional relations are defined by object properties some examples of object properties are possibleinfluencedby hasmeasureunit causedby hasconcentration haseffect hasgeneral aqi hasspecific aqi hassource influencedby mayproduce definedby for example causedby haseffect and influencedby define cause effect relations while possibleinfluencedby is a possible causal relation examples of data properties are measurementunit string aqi value byte aqi colourcode string concentrationstandardlimit real concentrationvalue real step 3 knowledge acquisition for both problems pa1 and pa2 we have performed knowledge modelling for air pollution analysis the case of acid rain occurrence in a sub urban area and ozone prediction in an urban area for each problem we have taken from the conceptual model the corresponding sub model and the corresponding sub ontology from airpollution onto 1 fig 11 shows the acid rain sub ontology graph while fig 12 shows the ozone pollution sub ontology graph both depicted in ontograf some examples of rules derived in this step for both problems pa1 pa2 are given below the certainty factors are set by using domain knowledge and human experts advices also a decision table pa1 dt1 with selected examples is given in table 6 for pa1 problem and another decision table pa2 dt1 is given in table 7 image step 4 knowledge discovery via data mining for pa1 and pa2 during this step of the framework it is performed knowledge discovery starting with data cleaning and pre processing and continuing with inductive data mining on cleaned and pre processed data sets the datasets used for this case study were taken from the romanian air quality monitoring network rnmca site www calitateaer ro which are public available they include hourly measurements of air pollutants concentration and meteorological parameters at some urban monitoring stations the data were cleaned by using the graphical visualization tools of weka data pre processing included missing values imputation or removal noise removal with filters and data normalization inductive rule learning was used to identify the air pollutants that were most correlated to ozone the inductive dm algorithms that were experimented are m5p reptree randomforest decision tree based algorithms and m5rules decision tables rule based algorithms m5rules classifier provided the best results compared to decision tables reptree and randomforest showing that for ozone the most correlated air pollutants are nox and so2 with a slighter influence of co and pm10 the correlation coefficient being 0 9121 with rmse 6 6928 in case of pa2 the proper number of past hours measurement of ozone was identified by using data mining m5p classifier gave the best correlation coefficient 0 8861 for k 2 past hours and minimum root mean square error rmse 7 8987 step 5 bayesian network development pa1 pa2 one scenario was identified for each problem pa1 and pa2 the acid rain scenario and the next hour ozone prediction scenario the corresponding bayesian networks were developed in weka and are shown in figs 13 and 14 the proper number of past hours measurement of ozone was identified by using the results of step 4 given by m5p classifier from weka data mining software package as co2 is usually correlated with human activities domestic sources due to combustion of fossil fuels and we have used data from traffic or industrial sites situated in ploiesti in locations where co2 concentration has lower values we have eliminated this air pollutant from the acid rain scenario the conditional probability tables of the bayesian networks nodes were set from decision tables provided in step 3 according to problem domain knowledge step 6 knowledge acquisition from bayesian networks from the bayesian network developed at the previous step we have derived the probabilistic rules an examples of such rules is given below the probabilities are derived from bayesian networks rule 7 if clearsky true and lightwind true and warmtemp true then atmstability true p 0 45 step 7 knowledge integration and validation the knowledge derived in the previous steps are integrated into the final knowledge base kbpa1 according to the algorithm given in section 3 4 3 case study 3 environmental domain soil the last case study consider the soil pollution problem problem ps1 soil pollution problem description soil pollution in a certain geographical area e g urban suburban rural can be caused by industry waste air pollution acid rain water pollution agriculture and agrochemicals transportation and other sources e g specific to the geographical area for example industry can pollute the soil with heavy metals methane pah petroleum hydrocarbons etc also agriculture through the use of different agrochemicals such as pesticides herbicides and fertilizers can strongly increase the soil pollution level step 1 conceptual model design the conceptual model for ps1 cmps1 is depicted in fig 15 step 2 ontology development a vocabulary of terms and relations between them was defined and a prototype soil pollution ontology was implemented in protégé 4 3 fig 16 presents two screenshots with concepts sub classes in owl viz for soil pollutants and soil remediation step 3 knowledge acquisition some rules and decision tables were derived from literature and human experts the decision table ps1 dt1 the version with binary values yes no is given in table 8 and some examples of rules with nominal values are given below rule s 1 if acidrain true then soilpollution true cnf 84 rule s 2 if groundozone moderate then soilpollution true cnf 75 rule 14 if industry woodind mining mettalurgy chemicalind plasticsind electronicind then soilpollutants heavymetals during this framework step of knowledge discovery it is performed data cleaning and pre processing followed by inductive data mining on cleaned and pre processed data sets due to lack of enough data sets for this case study we have performed inductive learning only from decision tables which did not required data cleaning and pre processing step 5 bayesian network development ps1 problem s scenario one scenario was identified for the ps1 problem methane and heavy metals related soil pollution fig 17 presents the soil pollution due to methane and heavy metals scenario sub ontology graph in ontograf we have implemented a bayesian network for soil pollution analysis due to ground ozone acid rain waste and heavy metals shown in fig 18 for a more complex analysis we can integrate in the soil pollution bayesian network the bayesian network of acidrain given in fig 13 and that of ozone prediction given in fig 14 providing a more complex soil pollution bayesian network with a modular structure step 6 knowledge acquisition from bayesian networks from the bayesian network developed at the previous step we have derived the probabilistic rules examples of such rules are given below the probabilities are derived from bayesian networks image step 7 knowledge integration and validation the knowledge derived in the previous steps are integrated into the final knowledge base kbps1 according to the algorithm given in section 3 5 conclusion environmental knowledge modelling can benefit from the integration of data and knowledge driven approaches as we proposed in the framework introduced in this paper our solution combines an ontological approach knowledge driven with two analysis approaches data mining data driven and bayesian networks data and knowledge driven for the generation of a knowledge base for an iedss three case studies for different environmental problems water resource management water pollution analysis air pollution analysis ozone prediction and soil pollution analysis were described the main advantage of the environmental knowledge modelling framework is that of dealing with complex environmental problems in an incremental and modular way by smaller scenarios analysis see e g air and soil pollution analysis for acid rain and ozone ground level ozone supervised by the problem domain ontology each scenario is built starting from the conceptual model using the ontology and knowledge derived from literature human experts or via data mining of quantitative and qualitative data and implemented in bayesian networks which gives the probabilistic rules for decision making knowledge uncertainty is quantified by certainty factors and probabilities as well as in the fuzzy terms included in rules as a future work we shall investigate the possibility of generating semi automatically the rules derived from bayesian networks and we shall make a unification of the two uncertainty models probabilistic and uncertainty factors in order to have uniformity in rules description 
26313,environmental processes are highly complex and their understanding involves the analysis of various quantitative and qualitative parameters physical chemical geographical etc which are more or less correlated appropriate environmental knowledge can deal with this complexity in a tractable way such knowledge is essential for solving particular environmental problems generating valuable environmental knowledge is a challenging research topic especially for environmental data science as efficient knowledge can lie behind data integrated environmental modelling uses a holistic view and can provide a possible better solution to environmental problems understanding the paper presents a knowledge modelling framework for intelligent environmental decision support systems iedss by following such a holistic perspective thus the proposed framework integrates an ontological approach and two data analysis approaches data mining and bayesian networks which are applied for the generation of a knowledge base that is used by an iedss for decision making the application of the framework is illustrated on three case studies from different environmental domains 1 water river resource management river water pollution analysis 2 air air pollution analysis ozone prediction and 3 soil soil pollution analysis keywords knowledge modelling framework integrated environmental modelling bayesian networks data mining ontology intelligent environmental decision support system 1 introduction one of the current challenging research directions in environmental sciences is integrated environmental modelling iem see e g laniak et al 2013 that can be viewed as a possible better solution to the environmental processes understanding problem iem applies a holistic view on environmental problems solving in this context various data analysis methods and techniques as e g statistical and artificial intelligence based can be combined and applied in order to provide valuable environmental knowledge for the decision making process of an intelligent environmental decision support system several artificial intelligence ai methods can be used for environmental systems modelling see e g the overviews presented in chen et al 2008 and struss 2008 examples of such ai based methods are knowledge based methods e g knowledge based systems expert systems case based reasoning and computational intelligence methods e g fuzzy inference system artificial neural networks genetic algorithms and swarm intelligence methods these methods can be used for integrated environmental modelling in decision support systems many environmental problems can be tackled with intelligent tools such as iedss a variety of iedss applications being reported in the literature as for example modulus dss for land degradation in the mediterranean oxley et al 2004 an iedss based on multi agent systems for water management urbani and delhom 2005 i ekbase an iedss for sustainable agriculture dutta et al 2014 flire dss a web based dss for floods and wildfires in urban and periurban areas kochilakis et al 2016 an overview on key challenges and best practices in environmental decision support systems development is presented in mcintosh et al 2011 while selected worldwide developed iedss tools are briefly described in gibert et al 2012 some frameworks for iedss development were also reported in the literature an example being given in sànchez marrè et al 2008a other promising model based approaches for edss development include qualitative models such as the qualitative reasoning model for algal bloom in the danube delta biosphere reserve proposed in cioaca et al 2009 the qualitative dynamic model adapted for hydroecology which is presented in heller and struss 2001 the abductive reasoning models described in wotawa et al 2010 and wotawa 2011 and other ai based models see e g the models discussed in sànchez marrè et al 2008b and struss et al 2003 the purpose of our research work is to provide a knowledge modelling framework for intelligent environmental decision support systems the paper presents an environmental knowledge modelling framework that integrates an ontological approach and two data analysis approaches data mining and bayesian networks which are applied for the generation of a knowledge base used by an iedss for decision making starting from a conceptual model of the environmental problem it is developed the problem domain ontology used in the next steps when data mining and bayesian networks are applied to generate rules from data sets and decision tables the application of the framework is shown on three case studies for solving different environmental problems in the domains of water air and soil 2 literature review a brief literature review on the ontological approach data mining and bayesian networks application in environmental science is presented 2 1 ontological approach in environmental science the ontological approach tackles expertise domain knowledge conceptualization providing a domain ontology which includes a vocabulary with terms concepts from general to particular ones and relations between concepts and axioms describing restrictions on concepts and relations actually it performs a domain knowledge modelling various applications of ontological approach in environmental applications are given in the literature e g wastewater management with ontology based edss ontowedss ceccaroni et al 2004 knowledge modelling in an air pollution control decision support system oprea 2005 flow and water quality modelling by using an ontology based knowledge management system chau 2007 integration of air quality models and 3d city models with ontologies metral et al 2008 air pollution ontology czarnecki and orłowski 2009 simulation in agricultural systems modelling based on ontology beck et al 2010 environmental impact assessment eia ontology covering the environmental experts terminology of various aspects such as water air soil habitat geophysical and socioeconomic impact garrido and requena 2011 description and classification of usda soil taxonomy up to soil series deb et al 2015 description of learning resources on organic agriculture with an agrovoc based ontology soil pollution analysis due to fertilization sánchez alonso and sicilia 2009 soil properties and processes ontology osp du et al 2014 knowledge modelling in river water quality monitoring and assessment xiaomin et al 2016 2 2 data mining in environmental science data mining dm integrates techniques from several domains as e g statistics machine learning and artificial intelligence and performs knowledge extraction from large data bases examples of dm techniques cios et al 2007 are decision trees rule algorithms hybrid algorithms artificial neural networks and statistical methods e g regression a brief description and classification of dm techniques oriented to decision making and some guidelines regarding the selection of the right technique are described in gibert et al 2010a some models and algorithms for decision making in a data driven environment are discussed in kusiak 2002 the dm approach was used in several environmental applications as e g water supply assets modelling babovic et al 2002 air pollution management policy making li and shue 2004 simulating farmers crop choices for integrated water resource management ekasingh et al 2005 water quality management using gis data mining karimipour et al 2005 analysis of water pollution effects on human health the results are used for water management by governmental authorities sokolova and fernández caballero 2007 agricultural soil profiles characterization armstrong et al 2007 air pollution monitoring and mining based on sensor grid ma et al 2008 air pollution modelling and short term air quality forecasting riga et al 2009 contaminant event locations identification in water distribution systems huang and mcbean 2009 soil carbon mapping in australia bui et al 2009 knowledge modelling in a waste water treatment plant by explicit knowledge discovery from the corresponding dynamic processes with the clustering based rules by states approach gibert et al 2010b water quality monitoring using remote sensing data mining wen and yang 2011 analysis of air pollution effects on humans including geographic heterogeneity assessment stanley young and xia 2013 air quality monitoring czechowski et al 2013 ground water quality assessment kolli and seshadri 2013 soil organic matter prediction teixeira et al 2014 soil property variation mapping by applying data mining to soil category maps du et al 2014 air pollution prediction siwek and ossowski 2016 2 3 bayesian networks in environmental science bayesian networks are directed acyclic graphs that represent the dependences between a set of variables and their joint probability distribution being probabilistic graphical models which integrate quantitative and qualitative data they are used in environmental modelling proving to be a useful support instrument for decision making in various environmental problems see e g urban air pollution forecasting cossentino et al 2001 farm irrigation in wang et al 2009 and robertson et al 2009 simulation of water flow in organic soils based on ontology kwon et al 2010 groundwater management for agriculture carmona et al 2011 air pollution related health risk assessment liu et al 2012 ghg gas emissions management in the british agricultural sector pérez miñana et al 2012 soil pollution assessment li et al 2015 water resource management in phan et al 2016 ecological risk assessment in estuarine ecosystems in mcdonald et al 2016 emergent water pollution accidents risk analysis in tang et al 2016 air pollution prediction via multi label classification corani and scanagatta 2016 an overview on bayesian networks use in environmental modelling is presented in aquillera et al 2011 another overview on bns use in environmental and resource management is described in barton et al 2012 while some good practice guidelines are synthesized in chen and pollino 2012 also it is important to note that bayesian networks can be generated by using the domain ontology see e g fenz et al 2009 2 4 review synthesis table 1 shows a synthesis of the literature review made for different environmental problems solving with the three approaches data mining bayesian networks and ontological approach within the framework of environmental decision support systems or knowledge modelling the main conclusion of the literature review is that all three approaches were applied with success to different environmental problems solving pollution prediction resource management monitoring control impact assessment water air and soil and have an important potential in generating valuable environmental knowledge however none of the studied problems were solved by applying all three approaches 3 proposed knowledge modelling framework 3 1 our approach starting from the literature review we have selected some artificial intelligence based methods data mining knowledge based systems that proved to tackle in a proper manner the problem of knowledge modelling in an iedss as environmental processes are very complex with several uncertainties and various variables we have chosen bayesian networks to manage knowledge uncertainty in such cases moreover bayesian networks handle heterogeneous data quantitative and qualitative being very useful in environmental problem description the paper proposes an environmental knowledge modelling framework which integrates three approaches for knowledge modelling ontological approach data mining and bayesian networks to be used in an iedss as shown in fig 1 the ontological approach allows problem domain conceptualization and provides concepts and relations that will be used in knowledge representation and when applying the other two approaches data mining and bayesian networks the data mining approach in our case rule induction inductive learning techniques decision trees and rules algorithms performs problem specific knowledge discovery from the available data sets i e problem specific data bases and decision tables bayesian networks are used to represent the probable dependences between different parameters as e g cause effect relations that describe the problem in our framework knowledge is represented under the rule form i e if premise then conclusion and knowledge uncertainty modelling is achieved with a probabilistic model bayesian networks a confidence factors model that quantifies the uncertainty with numerical values associated to rules generated for example with inductive learning dm techniques and a fuzzy model linguistic values associated to facts included in rules either in the premise or in the conclusion corresponding to each analyzed parameter specific ranges temporary databases t db associated to the current environmental problem context are added to the iedss databases db the knowledge bases built by our approach are kbdomain include domain knowledge under the form of rules and decision tables kbdm rules derived via inductive learning from available databases t db and decision tables kbbn probabilistic rules derived from bayesian networks and the final knowledge base kbproblem in which are integrated all rules necessary for the decision making procedure of the iedss the knowledge included in kbdomain and kbdm are using fuzzy model and confidence factors model for knowledge uncertainty representation the knowledge included in kbbn is represented as probabilistic rules finally the problem solving knowledge base kbproblem is using all the three models fuzzy model confidence factors and probabilistic model for the knowledge uncertainty representation the main problem with the mix of these three models regards rules uniformity in the case of probabilistic rules integration with the rules using the confidence factors model which needs further investigation our temporary solution was to keep all rules probabilistic and with confidence factors with human expert approval and possible correction and to apply all of them during reasoning selecting the most probable conclusion and the conclusion with the highest confidence factor finally the human expert deciding which one will be used by the decision making procedure depending on a specific problem s scenario context the basic components of a decision support system are data analysis and decision making the first component data analysis provides a part of the environmental knowledge mainly from collected data sets while the second one decision making uses this knowledge and additional problem domain knowledge for decision making the framework input is given by the environmental data sets collected data t db and db and environmental problem domain specification while the output is given by the knowledge base which contains the generated rules the quality of the environmental data used by the framework will strongly influence the derived knowledge base quality and thus the quality of the decision made by iedss therefore data cleaning checking and pre processing are required to be performed within data analysis a set of valuable guidelines for the application of pre processing techniques in the context of environmental data mining is presented in gibert et al 2016 the framework can be applied to different environmental problems e g environmental resource management monitoring analysis and control of environment quality i e quality of water air soil 3 2 framework description the proposed knowledge modelling framework has seven steps and starts with the problem analysis phase step 1 in which the problem solving domain is identified and a conceptual model of the problem is built followed by the phase of problem domain knowledge conceptualization i e problem domain ontology building step 2 and the phase of knowledge base construction via three approaches steps 3 4 5 6 knowledge acquisition from literature and human experts via inductive learning data mining decision trees and rule algorithms and through bayesian networks the results of the last methods being validated by human experts and using the problem domain ontology through reasoning the last phase of the framework is knowledge integration in the iedss knowledge base and validation step 7 through reasoning by using domain ontology and human experts image some more details related to the proposed framework are given as follows a scenario is an instantiation of the problem the conceptual model cmproblem contains only the key elements key concepts for solving the problem being a sort of initial sketch while the ontology ontodomain i e with all sub ontologies related e g to different scenarios must include more terms concepts and relations being a detailed conceptual description of the problem domain for all scenarios of the problem the environmental data set dataset t db as denoted in fig 1 contains time series data continuous data sets and or sample data discrete data sets some of the data are measurements collected with monitoring stations stationary or mobile under a monitoring network as e g an air quality monitoring network other are samples collected at different locations and moments of time as for example in river water pollution analysis while other data are characteristics of the environment area e g area geographical characteristics specific river characteristics the environmental data included in dataset can contain measurements errors e g from the automatic monitoring instrumentation noise uncertainty missing data redundancies irrelevant information etc thus some data cleaning and pre processing techniques are performed at the beginning of step 4 before applying the inductive data mining algorithms for example some data visualization tools such as boxplots time series plots two dimensional scatter plots or distributional plots can be applied in order to detect the presence of outliers errors missing values etc such problems can be solved manually or automatically by missing data imputation filtering for noise removal or other solutions usually provided within the data mining software package that is used also the cleaned and pre processed data will be properly prepared for the application of the selected data mining algorithms in order to fulfill the required assumptions feature selection and principal component analysis are examples of pre processing techniques performed for data preparation depending on the specific environmental problem available data analysis goal it is decided which are the most suitable types of data pre processing techniques required in step 4 the domain ontology ontodomain is used for concepts and relations validation in steps 3 4 5 and 7 in order to select the right concepts and relationships between them as during bayesian networks development step 5 it is used the ontology e g to define bn variables it is not necessary to include it also in step 6 when it is performed probabilistic rules extraction from bayesian networks starting from the environmental problem formulation the following issues are defined 1 a set of environmental parameters that need to be analyzed in order to solve the problem environmental variables with their associated values numerical or nominal as e g measurements or observations and or linguistic terms and a list of possible phenomena associated to the environmental problem linked to problem s scenarios 2 a data set for all some environmental parameters available past and current data with specific site area measurements observations and characteristics 3 a goal variable that need to be determined in order to solve the problem as e g the occurring of a certain phenomenon its possible effects on the environment on ecosystems or on human activity human settlements etc and the decisions that should be taken in order to minimize the negative effects the set of the environmental parameters includes area or site dependent parameters e g pollution sources specific water pollutants and specific air pollutant season dependent parameters which capture seasonality and other parameters dependent on the problem the human expert selects with higher priority the rules that include those parameters the core of the knowledge integration and validation step step 7 is detailed as follows suppose we have the following set of m rules from the three knowledge bases kbbn kbdm kbdomain that need to be integrated into one knowledge base image a rule is valid step 1 1 1 if it is in accordance with the conceptual model cmproblem or with the conceptual models of each identified scenario cmsi i e the relation between the parameters from the rule s premise and conclusion is in accordance with the conceptual model cmproblem or cmsi the correction of a rule step 1 2 2 i means the inclusion exclusion of some additional parameter s in from the rule s premise and or conclusion in accordance with the conceptual models cmproblem and cmsi also the rule correction can involve the change of its parameters name or their symbolic values in order to have uniformity in the final kbproblem the final validation of the rules included in kbproblem it is performed by the human expert through reasoning by rule chaining for all identified scenarios of the problem during validation the human expert checks the characteristics of the knowledge base coherence the conclusions provided by rule chaining are not contradictory if the initial facts are not contradictory no redundancy the rules are not repeated under different forms and completeness all useful rules for problem solving are included i e the knowledge base allows complete resolution the data mining approach rule induction is used to discover rules in data sets e g time series and decision tables and to identify the most correlated parameters which are included as nodes in the bayesian networks in order to keep simplicity several bayesian networks with smaller complexity are built and analyzed probabilistic rules are derived from them the problem domain ontology is used to design the bayesian networks and to check rules consistency as well as to guide the selection of the right rules that are derived with inductive learning techniques and bayesian networks an important benefit of using inductive rule data mining in step 4 of km framework is that the knowledge derived from problem specific datasets are priority rules to the more general knowledge derived in step 3 being problem dependent actually the rules from kbdm are refined rules of those included in kbdomain another advantage of using inductive learning techniques is their robustness to missing data cios et al 2007 the framework uses two types of inductive learning algorithms decision tree based algorithms indirect methods for rule generation from decision trees and rule based algorithms direct methods for rule generation directly from the data set depending on the environmental problems it is selected the proper inductive data mining algorithm by experimenting several ones from both types for example by choosing them from the algorithms available in the data mining software package as e g weka workbench witten et al 2011 used for the case studies described in the next section which has several types of inductive learning algorithms decision tree and rule based examples of decision tree based algorithms that can be applied under the proposed framework are id3 quinlan 1986 c4 5 quinlan 1993 implemented in weka as j48 algorithm m5 quinlan 1992 wang and witten 1997 implemented in weka as m5p algorithm cart friedman 1977 breiman et al 1984 and others e g rp algorithms reptree etc some of these algorithms use information gain criteria e g id3 c4 5 reptree while others use least square criterion e g m5 cart mean absolute deviation criterion e g cart or other criteria e g least absolute deviation the rule based algorithms considered by the framework include cn2 clark and niblett 1989 m5rules the rule based version of m5 implemented in weka and other specific algorithms as e g decision tables part oner that are implemented in weka some of the inductive learning algorithms are sequential covering algorithms learning one rule at a time as e g cn2 while others e g id3 c4 5 are simultaneous covering algorithms learning the entire data set simultaneously in the next section three case studies are presented showing the framework application to different environmental problems in the domains of water air and soil due to space limitation a more detailed description is given only for the problem of navigable river resource management presented in the first case study 4 case studies 4 1 case study 1 environmental domain water we are considering two problems from the water environmental domain water resource management and surface water pollution the application of the proposed framework is detailed as follows problem pw1 water resource management of a navigable river problem description water resource management is an environmental problem with major implications in life on earth i e human life flora and fauna agriculture industry etc as an example we have considered the problem of water resource management for a navigable river the danube river the main problems encountered in the river area due to water level variation are floods occurring during rainfalls and ice rapid smelting worsening or suspending navigation activity during drought usually in the summer season and icepack during winter season affecting hydropower plants activity and irrigation during drought in this case the main goal of the water resource management is to avoid or to reduce the negative effects of each problem occurring in the river area the danube river area as for example by informing the navigators e g sending notification when a navigation problem occurs due to drought or icepack forming the local authorities e g sending warning or alerts when floods can occur in a certain area or sending notification when drought will affect irrigation and thus the agriculture and farming or the hydropower plants management e g sending notification when the river level is affected by drought step 1 conceptual model design for pw1 starting from the problem description it was designed a conceptual model cmpw1 that is presented in fig 2 we have considered water level i e river level precipitations season and temperature as the main parameters drought and icepack as the phenomena that can occur floods as the potential effect of water level significant increase icerapidsmelting a potential effect in case a significant temperature increase is registered and irrigation navigation activity and hydropowerplant activity as the affected activities due to drought floods or icepack the parameters set and the corresponding sets for the identified phenomena effects and affected activities are given below parameters set waterlevel temperature precipitations season phenomena icepack drought effects floods icerapidsmelting affectedactivities irrigation navigationactivity hydropowerplantactivity problem pw2 surface water pollution analysis problem description surface water pollution analysis e g for rivers lakes is another important environmental problem with implications on aquatic life human health etc in this case the goal is to determine the level of surface water pollution the main sources of surface water pollution are agriculture runoff urban runoff industry transportation waste soil pollution and air pollution examples of water pollutants are nutrients fluoride chlorides ammonium and particulate matter step 1 conceptual model design for pw2 starting from the problem description we have designed a conceptual model cmpw2 that is shown in fig 3 the main parameters that were considered are water pollution source and water pollutant while the goal variable is water pollution level parameters set waterpollutionsource waterpollutant goal waterpollutionlevel step 2 ontology development for both problems pw1 and pw2 we have developed one prototype ontology for the water domain ontowater as denoted in the framework named water onto for both problems pw1 and pw2 starting from the two conceptual models cmpw1 and cmpw2 the main concepts related to water resource management for a navigable river and water pollution were included as classes the ontology implementation was done in protégé 4 3 as an owl ontology fig 4 shows the class hierarchy of water onto and a screenshot with concepts of the physicalindicator sub class as e g total suspended solids tss conductivity turbidity watertemperature apart from the concepts that were included in the water onto ontology some problem specific relations between concepts were defined as object properties examples of such relations are mayinfluence mayproduce maycause the default relations are the taxonomic ones is a and ako a kind of fig 5 shows a cause effect graph for the pw1 problem air temperature may influence water level through evaporation usually during summer when very high temperature can be registered and may cause drought the quantity of precipitations may influence the water level may produce flood when important quantity of precipitations are registered or may cause drought by lack of precipitations step 3 knowledge acquisition for both problems pw1 and pw2 in this step the main knowledge related to each problem solving pw1 and pw2 was taken from literature and human experts according to the designed conceptual models cmpw1 and cmpw2 and was described as if then rules and decision tables being included in the corresponding knowledge base i e kbpw1 domain for pw1 and kbpw2 domain for pw2 the water onto ontology was used to select the proper concepts included in knowledge representation e g parameters names the knowledge uncertainty was quantified by linguistic terms fuzzy model in the case of decision tables and confidence factors in the case of rules some examples of knowledge and their source are given below for pw1 problem problem pw1 kbpw1 domain table 2 synthesized human expert knowledge related to the main causes of the three phenomena that were considered in pw1 problem flood drought icepack and ice bridge occurrence and the affected activities or entities to whom the river resource management authority will send notifications in case of agriculture farming and human settlements local authorities will be informed while in case of river navigation periodically reports will be sent to river navigators table 3 shows the names of the parameters that were considered in solving the pw1 problem their domain and measurement unit the abbreviated name and their associated set of linguistic terms the first goal variable flood refers to the possibility of flood occurrence the second goal variable decision refers to the navigable river resource management decision and can represents no decision no problem occurred send an informing notification inf send a notification for restricted activity res or for stopping activity sto we have considered the following site area dependent river level s thresholds river warning level i e the river level above which it is possible to occur flood river flood level i e the river level above which the flood occurs and river minimum level i e the level of the river under which some activities will be affected as e g navigation irrigation hydropower plants activity with the corresponding values for a certain locality along the river where the water level is measured examples of rules related to site area dependent parameters for setting linguistic terms as e g for the river level variable riverls and the values of other variables flood decision image another set of rules are referring to the setting of linguistic terms associated to the numerical parameters set some examples are given below the estimation of the significant normal and insignificant level of precipitations during a certain season and geographical area is established by human experts the ps parameter denotes the linguistic term associated to a certain numerical value of the precipitations parameter we have to notice that each of the following three rules is established for each season the same for temperature air or water where we have the linguistic values of very high high normal low very low which depends on the season as we refer to a geographical area with temperate climate the season parameter can have a value from the following set spring summer autumn winter image decision tables are built by human expert starting from the knowledge related to pw1 problem analysis and already included in the conceptual model cmpw1 as for example water level is influenced mainly by the precipitations quantity and evaporation however during winter when icepack are formed on the river and when higher temperature are recorded possible floods can appear due to rapid icepack smelting rainfalls can increase significantly the level of the river and thus can produce floods affecting hydropower plant activity agriculture and human settlements evaporation can influence water level usually during summer when higher values of air temperature are registered and also few precipitations or lack of precipitation are recorded table 4 a b c d present selected examples from pw1 dt1 decision table for each season of a temperate climate autumn winter spring summer in which it is analyzed the possibility of occurring the phenomena of icepack ice bridge drought or flood table 5 presents some examples from the pw1 dt2 decision table in which are explicitly analyzed the possible effects of the drought and icepack phenomena on the activity of hydropower plants and navigation the parameters that were used are phenomenon riverls riverlvar navigationactivity and hydropowerplantactivity the linguistic terms used for the last two parameters are the following normal possible restricted restricted stopped we have specified a confidence factor cnf for the normal value in the first example the default value of cnf for the other examples is 100 a set of heuristic rules can be extracted from the rows of the previous decision tables however we shall see in step 4 data mining that a smaller number of rules can be extracted with inductive machine learning techniques keeping only the most representative correlated parameters other examples of heuristic rules with confidence factors established by human experts are given below image step 4 knowledge discovery via data mining for pw1 problem during the knowledge discovery step of the framework i e step 4 are performed data cleaning data pre processing and inductive data mining we have used public available datasets collected from hydro meteorological reports provided by the down stream danube river administration r a galati romania www afdj ro to cotele dunarii and the decision tables generated in the previous step the main data included in a hydro meteorological report are air temperature at 7 a m the water temperature daily average value the river level the river level variation and trend the specification of season specific phenomena e g snow level height presence of icepack or ice bridge other meteorological data such as precipitations wind speed wind direction atmospheric pressure etc these values are given for each locality observation point situated along the dunarea river however we have encountered a problem precipitations quantity missing from the collected reports which was compensated by the decision tables given by human experts we have used the weka 3 8 0 data mining software package before applying the data mining algorithms the data sets were cleaned checked and pre processed by using the available tools from weka as e g graphical visualization tool plot matrix visualizing all environmental variables filters for noise removal also some pre processing steps required by specific inductive data mining algorithms were performed as e g normalization feature selection two types of inductive learning techniques implemented in weka decision trees and rule algorithms were applied to decision tables and to the cleaned and preprocessed datasets in order to derive a set of heuristic rules and the most correlated parameters several experiments with different classifiers from the two type of inductive learning techniques were performed under the test mode 10 fold cross validation and the best results were registered the specific inductive learning algorithms that were selected in this case study are m5p reptree and j48 from the decision tree based algorithms and m5rules from the rule based algorithms we give a synthesis of some experiments that were performed providing examples of rules or correlations that were derived and included in kbdm knowledge base some details related to the data pre processing and inductive data mining steps are given for all experiments except the one performed on decision tables experiment 1 identify the correlations between water level air temperature water level variation and icepack presence during winter season january february 2017 dataset num riverdunarea2017 jan feb arff numerical values from hydro meteorological reports attributes airtemperature watertemperature riverlevel riverlevelvariation icepackpresence a1 data pre processing 1 attribute selection with best first strategy selected attributes watertemperature riverlevel riverlevelvariation 2 data normalization a2 data mining inductive learning inductive dm algorithms m5rules m5p reptree best results m5rules classifier results validated by human experts riverlevel depends on watertemperature and icepackpresence experiment 2 identify the correlations between water level air temperature water level variation and phenomenon presence in any season dataset nom riverdunarea2017 arff numerical and nominal values attributes airtemperature watertemperature riverlevel riverlevelvariation phenomenon season a1 data pre processing 1 attribute selection with best first strategy selected attributes watertemperature riverlevel riverlevelvariation 2 data normalization a2 data mining inductive learning inductive dm algorithms m5rules m5p reptree best results m5rules classifier results validated by human experts riverlevel depends on watertemperature and phenomenon drought icepack etc thus from the two experiments it was derived that airtemperature parameter is not relevant for our problem and it can be ignored in further rules actually watertemperature parameter is important also the riverlevel parameter depends on watertemperature and phenomenon the importance of the phenomenon parameter to the riverlevel parameter evolution was used in the next step of the framework for bayesian network design experiment 3 identifying the most representative rules from decision tables of kbdomain examples of rules mined from the pw1 dt1 decision tables with j48 classifier c4 5 like algorithm which gave the best results are shown in fig 6 the results for the statistic parameters are kappa statistic 0 7119 mean absolute error mae 0 1212 and root mean squared error rmse 0 286 image some of the rules from kbpw1 dm are used for building the bayesian networks e g when setting the values from the conditional probabilities tables step 5 bayesian network development pw1 problem scenarios three scenarios were identified for pw1 problem drought occurrence and its effects icepack and ice bridge occurrence and their effects flood occurrence and its effects we have combined all three scenarios into one scenario the drought flood icepack combined scenario which is shown in fig 7 as a graph in ontograf protégé 4 3 starting from combined scenario we have built in weka the bayesian network shown in fig 8 the bn nodes were set by using the water onto ontology and the relations between them as well as the conditional probability tables cpt were set according to the rules derived in the previous two steps of the framework we have to notice that in contrast with the scenario shown in fig 7 we have eliminated airtemperature from the bayesian network as it was less relevant for all three cases and we have included the riverlevel parameter which is more important in two cases drought and flood ignoring the case of icepackrapid smelting which can be managed by rules from kbdomain step 6 knowledge acquisition from bayesian networks from the bayesian network developed at the previous step we have derived the probabilistic rules examples of such rules are given below image where p is used to denote the probability step 7 knowledge integration and validation the knowledge derived in the previous steps are integrated into the final knowledge base kbpw1 according to the algorithm described in section 3 for step 7 of km framework repeated rules are eliminated while other rules are corrected as for example in case of the probabilistic rules the name of the parameters and some symbolic values are changed in order to keep rules uniformity in kbpw1 also new rules could be added in order to cover new problem s scenarios the validation of the knowledge base was performed on several scenarios of the problem via rule chaining and final result analysis by the human experts we present rule chaining in three particular cases of the pw1 problem case 1 winter phenomenon ice bridge on dunarea river image in this case the conclusion of the probabilistic rule pw1 bn3 is kept together with the conclusion of rule pw1 dm 3 and finally the last one conclusion is chosen as true in the given scenario context this simple case provided an idea about the problems that should be tackled by the knowledge integration algorithm proposed in section 3 case 2 drought scenario image in this case the conclusion of the probabilistic rule pw1 dm 7 is kept as true in the given scenario context case 3 ice rapid smelting scenario new scenario initial facts season spring airtvar increase phenomenon icepack riverls high for this case we have added the following rules during rules integration image the last rule was corrected i e the name of the parameters and some symbolic values were changed in order to keep rules uniformity in kbpw1 4 2 case study 2 environmental domain air we are considering two problems from the air environmental domain air pollution analysis pa1 and air pollution short term prediction ozone prediction pa2 the application of the proposed framework is detailed as follows problem pa1 air pollution analysis problem description we have considered the air pollution analysis problem the main air pollutants are co nox so2 pm o3 voc which are usually monitored in most countries under national air quality monitoring networks the air pollution degree is influenced apart from the air pollutants concentration levels by other factors such as meteorological ones e g wind speed wind direction precipitation relative humidity temperature solar radiation atmospheric pressure etc season time etc the main sources of air pollution are transportation traffic industrial activities and domestic sources especially heating air pollution episodes can have potential negative effects on human health being an important environmental issue that should be properly managed for increasing the quality of life in urban areas the current legislation in most worldwide countries imposed limits for the concentration of major air pollutants and population warning via public available information of air quality index general and specific to each air pollutant in polluted areas e g urban sub urban with color codes related to possible human health effects and risks and recommendation to reduce them one of the air pollution phenomenon that can affect the quality of water and soil is acid rain which can appear during rains and air pollution episodes due to some air pollutants as e g nox so2 co2 higher concentrations another air pollution problem is ozone appearance especially ground level ozone affecting soil quality which is derived from chemical reactions determined by nox voc under certain meteorological conditions e g light wind warm temperature clear sky i e stable atmospheric conditions and is increased by so2 presence step 1 conceptual model design for pa1 starting from description of the pa1 problem we have designed a conceptual model for air pollution analysis cmpa1 which is presented in fig 9 problem pa2 air pollution short term prediction ozone problem description the second problem is ozone prediction by using current and past values of ozone concentration k hours ago and current concentration of some air pollutants that can influence ozone concentration level as nox so2 and voc step 1 conceptual model design for pa2 fig 10 shows a conceptual model for next hour ozone prediction problem cmpa2 step 2 ontology development for both problems pa1 and pa2 starting from the conceptual model we have identified the main concepts and relations between them and we have developed an ontology for the air pollution domain airpollution onto 1 implemented in protégé 4 3 in owl format the current version of the ontology has 143 classes 29 object properties 22 data properties 319 logical axioms 588 axioms 43 individuals the taxonomic relations are isa and ako i e a kind of other types of relations e g has causal relations compositional relations are defined by object properties some examples of object properties are possibleinfluencedby hasmeasureunit causedby hasconcentration haseffect hasgeneral aqi hasspecific aqi hassource influencedby mayproduce definedby for example causedby haseffect and influencedby define cause effect relations while possibleinfluencedby is a possible causal relation examples of data properties are measurementunit string aqi value byte aqi colourcode string concentrationstandardlimit real concentrationvalue real step 3 knowledge acquisition for both problems pa1 and pa2 we have performed knowledge modelling for air pollution analysis the case of acid rain occurrence in a sub urban area and ozone prediction in an urban area for each problem we have taken from the conceptual model the corresponding sub model and the corresponding sub ontology from airpollution onto 1 fig 11 shows the acid rain sub ontology graph while fig 12 shows the ozone pollution sub ontology graph both depicted in ontograf some examples of rules derived in this step for both problems pa1 pa2 are given below the certainty factors are set by using domain knowledge and human experts advices also a decision table pa1 dt1 with selected examples is given in table 6 for pa1 problem and another decision table pa2 dt1 is given in table 7 image step 4 knowledge discovery via data mining for pa1 and pa2 during this step of the framework it is performed knowledge discovery starting with data cleaning and pre processing and continuing with inductive data mining on cleaned and pre processed data sets the datasets used for this case study were taken from the romanian air quality monitoring network rnmca site www calitateaer ro which are public available they include hourly measurements of air pollutants concentration and meteorological parameters at some urban monitoring stations the data were cleaned by using the graphical visualization tools of weka data pre processing included missing values imputation or removal noise removal with filters and data normalization inductive rule learning was used to identify the air pollutants that were most correlated to ozone the inductive dm algorithms that were experimented are m5p reptree randomforest decision tree based algorithms and m5rules decision tables rule based algorithms m5rules classifier provided the best results compared to decision tables reptree and randomforest showing that for ozone the most correlated air pollutants are nox and so2 with a slighter influence of co and pm10 the correlation coefficient being 0 9121 with rmse 6 6928 in case of pa2 the proper number of past hours measurement of ozone was identified by using data mining m5p classifier gave the best correlation coefficient 0 8861 for k 2 past hours and minimum root mean square error rmse 7 8987 step 5 bayesian network development pa1 pa2 one scenario was identified for each problem pa1 and pa2 the acid rain scenario and the next hour ozone prediction scenario the corresponding bayesian networks were developed in weka and are shown in figs 13 and 14 the proper number of past hours measurement of ozone was identified by using the results of step 4 given by m5p classifier from weka data mining software package as co2 is usually correlated with human activities domestic sources due to combustion of fossil fuels and we have used data from traffic or industrial sites situated in ploiesti in locations where co2 concentration has lower values we have eliminated this air pollutant from the acid rain scenario the conditional probability tables of the bayesian networks nodes were set from decision tables provided in step 3 according to problem domain knowledge step 6 knowledge acquisition from bayesian networks from the bayesian network developed at the previous step we have derived the probabilistic rules an examples of such rules is given below the probabilities are derived from bayesian networks rule 7 if clearsky true and lightwind true and warmtemp true then atmstability true p 0 45 step 7 knowledge integration and validation the knowledge derived in the previous steps are integrated into the final knowledge base kbpa1 according to the algorithm given in section 3 4 3 case study 3 environmental domain soil the last case study consider the soil pollution problem problem ps1 soil pollution problem description soil pollution in a certain geographical area e g urban suburban rural can be caused by industry waste air pollution acid rain water pollution agriculture and agrochemicals transportation and other sources e g specific to the geographical area for example industry can pollute the soil with heavy metals methane pah petroleum hydrocarbons etc also agriculture through the use of different agrochemicals such as pesticides herbicides and fertilizers can strongly increase the soil pollution level step 1 conceptual model design the conceptual model for ps1 cmps1 is depicted in fig 15 step 2 ontology development a vocabulary of terms and relations between them was defined and a prototype soil pollution ontology was implemented in protégé 4 3 fig 16 presents two screenshots with concepts sub classes in owl viz for soil pollutants and soil remediation step 3 knowledge acquisition some rules and decision tables were derived from literature and human experts the decision table ps1 dt1 the version with binary values yes no is given in table 8 and some examples of rules with nominal values are given below rule s 1 if acidrain true then soilpollution true cnf 84 rule s 2 if groundozone moderate then soilpollution true cnf 75 rule 14 if industry woodind mining mettalurgy chemicalind plasticsind electronicind then soilpollutants heavymetals during this framework step of knowledge discovery it is performed data cleaning and pre processing followed by inductive data mining on cleaned and pre processed data sets due to lack of enough data sets for this case study we have performed inductive learning only from decision tables which did not required data cleaning and pre processing step 5 bayesian network development ps1 problem s scenario one scenario was identified for the ps1 problem methane and heavy metals related soil pollution fig 17 presents the soil pollution due to methane and heavy metals scenario sub ontology graph in ontograf we have implemented a bayesian network for soil pollution analysis due to ground ozone acid rain waste and heavy metals shown in fig 18 for a more complex analysis we can integrate in the soil pollution bayesian network the bayesian network of acidrain given in fig 13 and that of ozone prediction given in fig 14 providing a more complex soil pollution bayesian network with a modular structure step 6 knowledge acquisition from bayesian networks from the bayesian network developed at the previous step we have derived the probabilistic rules examples of such rules are given below the probabilities are derived from bayesian networks image step 7 knowledge integration and validation the knowledge derived in the previous steps are integrated into the final knowledge base kbps1 according to the algorithm given in section 3 5 conclusion environmental knowledge modelling can benefit from the integration of data and knowledge driven approaches as we proposed in the framework introduced in this paper our solution combines an ontological approach knowledge driven with two analysis approaches data mining data driven and bayesian networks data and knowledge driven for the generation of a knowledge base for an iedss three case studies for different environmental problems water resource management water pollution analysis air pollution analysis ozone prediction and soil pollution analysis were described the main advantage of the environmental knowledge modelling framework is that of dealing with complex environmental problems in an incremental and modular way by smaller scenarios analysis see e g air and soil pollution analysis for acid rain and ozone ground level ozone supervised by the problem domain ontology each scenario is built starting from the conceptual model using the ontology and knowledge derived from literature human experts or via data mining of quantitative and qualitative data and implemented in bayesian networks which gives the probabilistic rules for decision making knowledge uncertainty is quantified by certainty factors and probabilities as well as in the fuzzy terms included in rules as a future work we shall investigate the possibility of generating semi automatically the rules derived from bayesian networks and we shall make a unification of the two uncertainty models probabilistic and uncertainty factors in order to have uniformity in rules description 
26314,to minimize the contaminant migration in the soil the use of two and three layer capillary barriers is proposed as an efficient solution indeed the soil can be used as a physicochemical filter for some pollutants to discuss the effectiveness of layered soil in the reduction of organic pollutant transport an experimental and numerical model was developed in this study the adsorption capacity of the cationic dye which used as an example of adsorbed organic pollutant in a soil sand clay silty soil has been studied in batch and fixed bed column it was found that the concentration at the column outlet doesn t exceed 4 5 of the initial concentration after 214 days due to the presence of a smectite clay layer and three layer capillary barriers the transport of dye was insignificant because of the high adsorption capacity of clay and silty soil the experimental results were also confronted with a numerical simulation a finite element analysis model was employed in this study to predict the coupled process of variably saturated soils by the contaminants transported in runoff experimental and numerical results confirm that the water velocity and the kinetic adsorption are inversely proportional through the two and three layer barrier system keywords organic pollutant capillary barrier modeling adsorption transport 1 introduction the presence of human and industrial waste is the main reason of the soil contamination this waste is full of chemicals and toxic products that infiltrate into the subsurface soil which leads to the groundwater pollution there are many chemicals that can pollute water and soil which entail groundwater and soil contamination as a major problem as with many contaminants such as volatile organic compounds heavy metals fertilizers and detergents dyes have toxic mutagenic and carcinogenic effects on human health malik and grohmann 2011 chen et al 2011 some dyes cause soil contamination due to the toxic nature and harmful effects on all constituents of life for instance wastewater from textile dyeing has caused an immense pollution problem for both groundwater and soil these dyes are used as tracer on soils to detect preferential flow paths from soil surface to aquifers stein et al 1998 or in soil characterization as in the quantification of colloidal matters in soil inasmuch as the dye is up taken by adsorption onto soil particles surface flury and wai 2003 the methylene blue mb is one of the most popular dyes used in textile industries and in the coloring of paper wools cotton silk etc kumar et al 2005 the mb has also been of use in medical applications it contains an antiseptic propriety against bacterial infection and it is deployed as an antidote for cyanide poisoning reddy et al 2013 this cationic dye can cause a serious damage to the eyes of living beings and can provoke other health problems such as methemoglobinemia mall et al 2005 profuse sweating nausea and mental confusion christie 2001 in recent years many researchers are interested in soil contamination by persistent organic pollutant their studies focus on the impact of persistent pollutant on human health ferencz and balog 2010 among others concentrate on the adsorption of mb onto the soil in batch and continuous fixed bed column sana and jalila 2016 yukselen and kaya 2008 arab et al 2015 auta and hameed 2014 in order to study the impact and mobility characteristics of mb pollutant it s necessary to predict their adsorption and transport in porous media and include the factors affecting its migration and accumulation in soil which is a stratified medium soil is made up of different layers each layer has its own characteristics that make it distinct from all of the other layers soil is also a multiphase system thus the knowledge of the mechanisms and the coupled process associated with contaminant transport in soil is of great significance to deal with the remediation solutions layered soil was proposed in this work as a cover to prohibit the organic pollutant infiltration into groundwater because of the relatively high costs of traditional covers suter et al 1993 the cover with capillary barrier effects ccbe represents a promising alternative for the soil contamination problem aubertin et al 2009 khire et al 2000 li et al 2013 ng et al 2015 sadeghi et al 2014 mancarella et al 2012 predelus et al 2015 harnas et al 2014 proposed four layer cover systems with dual capillary to strengthen water storage of the cover a new three layer capillary barrier cover system is proposed by ng et al 2015 indeed the ccbe is primarily composed of two layer unsaturated soil system that employs the contrasting unsaturated hydraulic properties between both soils the experimental and numerical results showed that the three layer capillary barrier cover system performs well under extreme rainfall the performance of a ccbe is extremely dependent on the soil particle size the change in particle sizes of the soils in a ccbe derives a change in the hydraulic properties the unsaturated hydraulic properties of the soils influenced on the storage capacity of the surface layer and provoke the apparition an amount of infiltration from covers khire et al 2000 the determination of the unsaturated hydraulic proprieties is primordial in the modeling of water flow and contaminant transport whereas the methods of measuring of these soil retention properties are time consuming expensive and are not accurate because of spatial and temporal variability of these parameters many authors have proposed to minimize the number of tensiometers to 2 wessolek et al 1994 fujimakia and inoue 2003 schindler et al 2010 and simunek et al 1998 have used only one tensiometer several other researchers have resorted to numerical modeling to solve the problem by choosing time dependent and space variable boundary conditions cremer et al 2016 kumar et al 2015 the problem is difficult because of the complexities and strong heterogeneities of the geological mass it is therefore essential to know how to explore predictive models in the absence of sufficient information on heterogeneous hydraulic physical chemical properties usually the transport of species is described by fickian models of advection and dispersion equations ade following deterministic and or stochastic approaches drummond et al 2014 rumynin 2012 pedretti 2014 advection dispersion equation ade is extensively used to model several physical chemical and biological phenomena abdelkawy et al 2015 li et al 2017 song et al 2018 these processes are coupled and include water flow solute transport and reaction the advection dispersion equation could solve the transport of solute in unsaturated medium in heterogeneous porous media pandey et al 2018 and could account for the impact of retardation and song et al 2018 nevertheless the different versions of the ade model with a delay coefficient do not accurately describe btcs especially for long tails berkowitz et al 2006 cortis et al 2006 in contrast the ade cannot describe the non fickian transport or anomalous bijeljic et al 2013 neuman and tartakovsky 2009 a modified form of the convection dispersion equation including a two site kinetic retention model was used to simulate the transport and retention behavior of some nanomaterials in porous media bradford et al 2003 liang et al 2016 wang et al 2012 fang et al 2013 several models have been developed to predict the concentration profile of the dye in fixed bed column these models reproduce the experimental data in order to have a description of the adsorption mechanism recent studies have proposed the advection dispersion model to describe the dynamic behavior of a dye in fixed bed column hethnawi et al 2017 the advection dispersion adsorption equation which is the advection dispersion equation coupled with kinetic adsorption was carried out to model the transport of mb in a layered medium in this study our goals were i to study the adsorption desorption and transport behavior of mb in three soils in batch and dynamic mode ii to examine the performance of two and three layers capillary barrier cover system under organic pollutant adsorption in fixed bed column and iii to evaluate the ability of models to replicate the observed results of mb adsorption in soils under different experimental conditions besides a sensitivity analysis was carried out to study the influence of mobile immobile model parameters on the breakthrough curves 2 materials and methods 2 1 soil and solutions the soils used in this study were collected from an industrial zone in the region of sousse in tunisia the particle size distribution of sand clay and silty soil and the exchange capacity cec of clay were measured by laser diffraction particle size analyzer microtrac s3500 and by the metson method afnor nf x31 130 respectively the mineralogy of the clay sample was established by x ray diffractometer xpert pro panalytical the hydraulic parameters and the retention curves of three soils were obtained using cell compression and mini disk infiltrometer decagon devices the reactive dye methylene blue used as adsorbate basic blue 9 ci 52015 is a cationic dye with a molecular formula c16h18cin3s 3h2o and a molar mass of 373 9 g mol 1 the wavelength of maximum absorbance for mb is 663 nm 2 2 batch experiment adsorption isotherms and adsorption kinetics experiments were performed to evaluate the adsorption capacity of mb in three materials 2 2 1 adsorption isotherms the adsorption isotherms were performed in a set of glass flasks 60 ml containing 25 ml of mb solutions with different initial concentrations 0 5 6 10 14 20 25 40 mg l 1 and a 0 5 g of adsorbent added to each solution these mb solutions were kept under stirring speed of 450 rpm for 48 h to ensure that adsorption equilibrium was reached then the supernatants were fitted and centrifuged before the measurement of its absorbance the absorbance was measured using a uv vis spectrophotometer spectroflex6100 by monitoring the absorbance changes at wavelength of maximum absorbance 663 nm the mb concentrations were estimated using a calibration curve obtained by plotting the absorbance against the concentration of the mb solution the amount of dye adsorbed per unit weight of sample sand or clay at equilibrium q e mg g 1 was calculated as follow 1 q e m c e c 0 v where c 0 and c e are the initial concentration and liquid phase concentrations of dye solution at equilibrium mg l 1 respectively v is the dye volume l and m is the mass of the sample g the langmuir 1918 and freundlich 1906 models are selected to fit the adsorption isotherms the non linear form of these models is given as follow freundlich model 2 q e k f c e 1 n langmuir model 3 q e q m b c e 1 b c e k f freundlich isotherm constant mg g 1 l mg 1 1 n n n freundlich exponent q m langmuir adsorption capacity mg g 1 b langmuir isotherm constant l mg 1 2 2 2 adsorption kinetics for the adsorption kinetics experiments vials containing 1 g of sample and 20 ml of mb solutions with 150 mg l 1 initial concentration were shaken at 450 rpm for 90 min at the end of the predetermined time interval the vials were filtered then centrifuged at 2500 tr min 1 the supernatant solution was analyzed using uv visible spectrophotometer spectroflex6100 to determine the dye concentration each experiment was replicated 3 times to verify the reproducibility of the experience the adsorption capacity qt mg g 1 was calculated as 4 q t c 0 c t v m where q t is the dye concentration on adsorbent at time t mg g 1 c 0 and c t are the concentrations of the mb solution before and after adsorption respectively mg l 1 v is the volume of mb solution and m is the mass of dry sample g there are many types of the pseudo second order kinetic equation but in the dye adsorption process the types 1 and 2 describe more the fitting of kinetic models wawrzkiewicz and hubicki 2009 thus the study of pseudo second order kinetic is limited to types 1 and 2 in this work the pseudo first order kinetics may be expressed as 5 l o g q e q t log q e k 1 t 2 303 the non linear form of pseudo second order equation is given as 6 t q t 1 k 2 q e 2 1 q e 2 3 column experiment a series of fixed bed column experiments have been performed to determine the mb behavior in unsaturated soil and to study the effect of capillary barrier on mb transport the column used in the experiments was made with glass and has a dimension of 3 5 cm in diameter and 25 cm in length the column experiments were conducted at constant flow rate and at initial mb concentration under unsaturated flow conditions then the mb was eluted from the column by rainwater flushing during the experiment the effluent samples were collected at the column outlet and measured for mb concentration immediately after sampling the mb concentrations in the column effluents were determined using uv vis spectrophotometer the mb breakthrough curves btc were obtained by plotting relative concentrations c c 0 versus time t see table 1 3 breakthrough curves modeling in this study three approaches have been investigated for data fitting of mb breakthrough curves these models include thomas convection dispersion and mobile immobile model 3 1 thomas model the thomas model thomas 1944 assumes that a langmuir isotherm and second order kinetic fitted well the experimental data this model has assumed also that adsorption is limited by mass transfer with no axial dispersion is derived and has allowed the calculation of the adsorption rate constant the equation of thomas model can be described as 7 c t c 0 1 1 e q 0 m c 0 ϑ t k t h ϑ where k th is thomas rate constant ml min 1 mg 1 q 0 is equilibrium adsorption capacity mg g 1 m is the mass of the adsorbent c 0 and c t are the mb concentration in the influent and at time t mg l 1 and υ is the flow rate ml min 1 3 2 advection dispersion adsorption model a general transport and adsorption model for a porous medium was used to predict dynamic adsorption breakthrough this model is based on convection dispersion equation eq 8 for two dimensional flows through a porous bed parker and van genuchten 1984 8 θ c t ρ b c p t θ d l c u c r s where c and c p are the adsorbate concentration in the liquid and solid phase respectively kg m 3 ρ b is the bulk density kg m 3 θ is the bed water content k p is the adsorption rate m3 kg 1 u is the darcy velocity m s 1 r and s are the reaction and d l is the hydrodynamic dispersion tensor the rate of accumulation in the solid phase term c p t was accounted for the following equation 9 c p t α k l c q m c p c p α is the rate constant s 1 q m is the maximum langmuir adsorption parameter c p is the adsorbent capacity kg m 3 and k l is the langmuir adsorption parameter m3 kg 1 severe incorporation of richard equation eq 10 is to describe the darcy flow through porous media was coupled with advection dispersion equation this coupling is through darcy velocity equation eq 11 the richard equation is expressed as follows 10 c s e s h p t k h p d q m 11 u k s k r h p d where h p is the pressure head m c is the specific moisture capacity s e represents the effective saturation s is the storage coefficient k is the hydraulic conductivity k r is the relative permeability d represents the elevation m and q m is the fluid source or sink the unsaturated soil properties were described using the van genuchten model 12 k θ k s s e 2 1 1 s e 1 m m where m is the van genuchten parameter and k s denotes the hydraulic permeability the main problem for unsaturated flow lies in the choice of boundary conditions at the lower base and the upper base of the domain in addition to the strong dependence of the hydrodynamic parameters hydraulic conductivity darcy velocity pressure load the degree of water saturation or water content at the lower and upper limits of the domain varies over time especially at the beginning of the experiment and subsequently the parameters k θ q θ and h θ vary as well to overcome these difficulties we use three successive and complementary sub models to make the right choice of boundary conditions that describe the flow regime throughout the experiment the following flowchart fig 2 explains further the proposed methodology for solving the problem and determining the most appropriate boundary conditions we begin by developing a first sub model for the unsaturated flow regime with zero flux conditions on both the lower and upper surfaces of the geometry to start the flow from the upper surface second sub model the values recorded on the upper surface during the unsaturated phase first sub model are imported as boundary conditions at the upper base until flow start time flow followed by pressure values imposed by the flow we keep for the moment the zero flow condition at the lower base the calculation of the second intermediate sub model is started and the saturation time of the medium is recorded at this point the state of saturation of the medium is reached and the pressure load at the bottom surface becomes constant finally it is the third model that will be the final full model provided to the upper surface is an interpolation function describing the evolution of the capillary pressure taken from the initial time until the time of the start of the flow followed by the pressure values imposed by the flow for the condition of the bottom surface the pressure is the interpolation of the values taken from the initial time until saturation time tsaturation followed by the value of the saturation pressure the following chart fig 2 can better explain the methodology proposed to solve the problem following the strategy described above for the study of water flow and solute transport in a soil column of height 10 cm and diameter 3 5 cm the initial and boundary conditions for an unsaturated and saturated medium are described in fig 3 the initial and boundary condition for two dimensional problem used for solving this model are 13 c 0 kg m 3 t 0 z c c 0 z 10 cm nd i c i 0 z 0 cm for the richard equation the initial condition is a linear function which describes the pressure head evolution in the column at time 0s the upper and lower conditions are two interpolation functions p1 t p2 t which describe the pressure head evolution at any time throughout the experiment fig 3 3 3 mobile immobile model mim the transport equation takes into account not only the convection diffusion and dispersion but also the phenomenon of retention of the polluting particles by the solid phase of the porous medium via a distribution coefficient kd the one dimensional equation of the mim transport model for a reactive solute in unsaturated media is van genuchten and wierenga 1976 14 θ m f ρ d k d c m t θ i m 1 f ρ d k d c i m t q c m z d m θ m 2 c m z 2 θ i m 1 f ρ d k d c i m t k m c m c i m where θ m and θ im represent the mobile and immobile water fractions respectively f is the fraction mobile ρ d is the bulk density k d is the partition coefficient c m et c im are the pollutant concentrations in mobile and immobile phase respectively d m is the diffusion coefficient in the fraction mobile and q is the darcy velocity the mim model parameters are described in the following table see table 2 where d is the diffusion coefficient in liquid phase θ is the porosity λ is the longitudinal dispersivity ρ is the density c 0 is the initial concentration k d is the distribution coefficient n is the coefficient of freundlich model and α is the mass transfer coefficient 4 results and discussion 4 1 samples characterizations the results of x ray spectroscopy show the presence of smectite clay the smectite clay is characterized by its cationic exchange capacity in the interlayer spaces the value of this cec is found to be equal to 90 meq 100 g of clay the x ray diffraction pattern fig 4 a had three significant reflections at 15 6 a 3 34 a and 1 45 a the two reflection distances of 3 31 a and 1 45 a indicated the presence of silice and that the clay is a dioctahedral smectite respectively the particle size analysis of sand clay and silty soil samples shows that the fifty percent passing particle size d50 was calculated as 317 μm and 11 69 μm based on sieve sizing for sand and clay respectively fig 4 b soil water characteristic curves θ h are presented in the following figure fig 4 c comparing the saturated water content of three soils we deduce that clay have the high saturated soil water content the hydrodynamic parameters of three soils are recapitulated in table 3 4 2 adsorption models it is found that the clay has the best adsorption capacity compared to silty soil and sand the good fitting of the model with experimental data is represented by a high value of correlation coefficient for the clay and silty soil adsorbents the freundlich model provides a better fit in the mb adsorption isotherm fig 5 b and c the freundlich isotherms model predicts multilayer adsorption and describes equilibrium on heterogeneous surfaces freundlich 1906 gimbert et al 2008 for sand modeling of adsorption isotherm shows good agreement with langmuir fig 5 a this model assumes that the forces of interaction between the adsorbed molecules are negligible gimbert et al 2008 there is a single layer of adsorbate on the outer surface of the adsorbent langmuir 1918 and no further adsorption will take place if the molecule occupies the adsorption site the prediction of kinetics adsorption is needed to evaluate the adsorption efficiency using both models the pseudo first order equation represented very well the kinetic data of mb in sand and clay the applicability of langmuir isotherm for sand material implies that monolayer adsorption exist under the experimental conditions the best regression of experimental data for both sand and silty soil is using the type 1 of pseudo second order kinetic equation fig 6 b 4 3 column experiments 4 3 1 effect of the capillary barrier on mb retention the fig 7 shows the capillary barrier effect on the mb breakthrough curve by comparing homogeneous and layered porous media this figure clearly shows a difference in the mb behavior during its infiltration in unsaturated homogeneous and layered media saturated at 60 vp the layered medium exp b consists of 80 of sand distributed over a height of 20 cm at the top of the column and 20 clay corresponds to a height of 5 cm the mb injection is performed at an average flow rate of q 8 5 ml min 1 and after a flow stabilization time of 34 min during the first 60 min the concentration is zero the injected solution is pushing the liquid phase initially present in the column in the layered medium the mb solution has covered a distance of 20 cm of homogeneous sand then it is found in the contact area between the two layers this area is a transition zone in which water is trapped for some time to adapt to sudden change in characteristics of the medium this explains the retardation in the release of the effluent from the column then the concentration gradually increases to a maximum value this value is equal to c0 for a homogeneous medium after 319 min while it is equal to 0 88 c0 for a layered medium after a time of 578 min the long tail of the breakthrough curve is due to the mb reactivity with the clay fraction 4 3 2 effect of three barrier system on mb adsorption the influence of the capillary barrier system on the mb breakthrough curve was also studied using three layer medium fig 3 the mb concentration increases slowly and depends on the saturation condition and the structure of the porous medium indeed the first layer at the top of the column is a 1 cm of silty soil and the second layer is a 7 cm of sand both layers are initially saturated with water the third layers 2 cm of clay are initially unsaturated exp d the delay of the mb release after 104 days is justified by the physical and chemical heterogeneities of the porous medium the interface between two porous media having different characteristics is a capillary barrier and a trapping zone despite a slight increase in concentration gradient the steady state of mb transfer is not reached at t 214 days and outlet concentration does not exceed 4 5 of the inlet concentration fig 8 generally the btc in heterogeneous media are described by lognormal distribution and recently this log normal distribution has been endorsed for non gaussian correlation moslehi and de barros 2017 comparing the exponential distribution r2 0 94 and the gaussian distribution r2 0 97 we can conclude that the dispersion process is predominant at the beginning of the curve the exponential shape of btc proves that dispersion and sorption are the main processes in the mb transport 4 4 models validation 4 4 1 comparison of thomas model and convection dispersion adsorption model a numerical simulation using the advection dispersion adsorption model for three layer porous media is carried out adsorption is considered linear for silty soil and clay while it is langmuir type in the sand sample eq 8 describes the transport of mb in a porous medium composed of three areas of different physical and chemical properties these properties are defined in table 4 the selected geometry is very similar to the physical reality of the experimental setup fig 1 the chosen triangular mesh is refined at the interfaces between the fields and the time step retained during the simulation is 30000s in fig 2 the boundary conditions are described for short time t 170 days the convection dispersion adsorption model reproduced fairly well the experimental data and for long time t 170 days the numerical curve is well below experimental measurements and the standard error of the simulation with the model thomas gets weaker fig 9 4 4 2 kinetic adsorption in layered medium for the two layer medium and at the interface between the two layers z 5 cm the water is trapped in the capillary barrier composed of two layers sand and clay with hydraulic properties are different this entrapment causes the prevention and accumulation of water at the interface until the water pressure head becomes sufficient to allow it to penetrate li et al 2013 the water pressure gradient increase associated with the accumulation of water in the two media interface has a capillary barrier effect in this interface retention is more important predelus et al 2015 indeed after 100 min the adsorbed concentration gradient in z 5 cm interface between sand and clay materials is larger compared to homogeneous medium fig 10 this increase in retention capacity at this interface is the result of increase in water content and decreasing in interstitial water velocity which enhances the trapping of the mb solution torkzaban et al 2008 in fact the decrease of the pore water velocity is accompanied by a decrease of the adsorption kinetics velocity at the interface fig 11 the same result is found for a three layers capillary barrier system the fig 12 represents the evolution of adsorption kinetics along the column in the breakthrough time t 8 64 10 6 s and in the end of the experiment t 1 72 10 7 s indeed the adsorption kinetics is increased in two capillary barriers z 9 cm interface between silty soil and sand z 2 cm interface between sand and clay 4 4 3 simulated mb adsorption both modeling approaches available in hydrus 1d for mb transport were used and compared prior to assisting in the interpretation of the field data fig 12 shows the mb breakthrough curve and model results obtained from the advection dispersion transport equation the mobile immobile solute transport mim and thomas model the simulated data obtained by the mim model fig 13 are coherent with experimental data acquired since they prove that mb solution takes 350 h for breaks through a fixed bed column of 10 cm in height it is also expected that the shape of the breakthrough curve is not asymmetric causing a significant adjustment error for the convection dispersion adsorption model however mim model better reproduced the exponential shape profile with long tails table 5 this means the existence of an immobile water fraction in the porous medium 4 5 parameter sensitivity analysis after mim model validation a parameter sensitivity analysis of this model on the mb breakthrough curve was performed the mb transport is dominated by convection phenomena dispersion diffusion and adsorption in the presence of a mobile water fraction 4 5 1 effect of the exchange coefficient when the solute transfer coefficient α between the mobile and immobile fraction f of the liquid phase increases the increasing part of the breakthrough curve tends to be asymmetrical and spreads a long period fig 14 indeed the more the breakthrough curves are asymmetrical the more exchange between mobile water and immobile water is important note also that the more the fraction f is close to 1 the more the model s sensitivity on this exchange coefficient is low if the exchange coefficient α is very small the solute velocity in the mobile fraction and transfer can be considered by the convection dispersion but at a higher velocity than the convection dispersion adsorption model however an increase of the coefficient causes a rapid exchange and a tendency to have uniformity of the concentration in the two fractions of water which causes spreading of the breakthrough curve increasing degree of saturation of the medium and the pore velocity generates increased mobile water fraction and therefore the coefficient padilla et al 1999 nevertheless there is no expressive relationship between f and α and between f and λ pang and close 1999 but an intense inverse correlation between α and λ 4 5 2 effect of longitudinal dispersivity the influence of the local dispersivity does not make sense for low values of peclet numbers that is to say to the low velocity the increase in dispersivity causes an increase in dispersion coefficient and thereafter increasing mass diffusive flow and exchange between the mobile and immobile water regions in our case the flow velocity in the silty soil is low and thereafter a wide variation in the dispersion coefficient has no particular effect on the values breakthroughs concentrations see fig 15 4 5 3 effect of the diffusion coefficient fig 16 shows the influence of a diffusion coefficient variation on the mb breakthrough curve indeed by reducing the diffusion coefficient the breakthrough curve becomes more spread out with a delay time of breakthrough while an increase in this coefficient favors the transport of mb and causes rapid penetration of mb so the effect of this factor is most evident in mb breakthrough time a small influence on the concentration value y axis 4 5 4 effect of the distribution coefficient a variation of this parameter has an impact on the amplitude change and an offset in the time of the simulated curve the breakthrough curve is quite sensitive to a variation in distribution coefficient fig 17 indeed an increase of this coefficient resulted in a delay output of the solution a significant increase of breakthrough time a spreading of the breakthrough curve and a decrease in the outlet concentration by increasing this coefficient the solid matrix retains and traps the material and thereafter it delays the transfer the variation of distribution coefficient causes a variation in time required to reach the equilibrium musielak 2012 thereafter this coefficient is involved as a term of delay 4 5 5 effect of the freundlich coefficient fig 18 represents the breakthrough curves for different values of freundlich coefficient n it is noted that the relative concentration increases with increasing freundlich coefficient unlike the concentration of mb adsorbed on the soil grains for n 1 linear isotherm or n 1 the delay coefficient is low and the breakthrough curve becomes symmetrical this symmetry decreases with the decrease of n that is to means higher the coefficient n decreases as the curve becomes non linear in the case of a non linear isotherm n 1 the concentration of mb in the liquid phase decreases proportionally with decreasing freundlich coefficient fig 18 a which is not the case for the mb adsorbed concentration on the solid phase in fact for n 0 823 a decrease in the mb concentration in the liquid phase compared to the case of the linear isotherm n 1 was found this decrease leads to an increase of the delay coefficient which causes an increase in adsorbed concentration on the solid phase at equilibrium and for nonlinear adsorption the adsorption is more significant and the delay dispersion and are stronger and the shape of the elution curve is asymmetrical fig 18 b 5 conclusion the aim of this work was to study the mb adsorption in layered soil and to discuss the effectiveness of layered soil in the reduction of organic pollutant transport the results showed that the adsorption isotherm is described using langmuir model for sand and freundlich model for clay and silty soil also the adsorption kinetic is described using pseudo first order model for both materials sand and clay and pseudo second order for silty soil both the experimental exploration and the numerical simulation showed that the three layer capillary barrier cover system performs as inhibitor to minimize pollutant percolation therefore in capillary barrier the mb kinetic adsorption is inversely proportional to the flow velocity the comparative analysis between three models proved that mobile immobile model is the suitable one to describe the adsorption and transport of mb in unsaturated soil 
26314,to minimize the contaminant migration in the soil the use of two and three layer capillary barriers is proposed as an efficient solution indeed the soil can be used as a physicochemical filter for some pollutants to discuss the effectiveness of layered soil in the reduction of organic pollutant transport an experimental and numerical model was developed in this study the adsorption capacity of the cationic dye which used as an example of adsorbed organic pollutant in a soil sand clay silty soil has been studied in batch and fixed bed column it was found that the concentration at the column outlet doesn t exceed 4 5 of the initial concentration after 214 days due to the presence of a smectite clay layer and three layer capillary barriers the transport of dye was insignificant because of the high adsorption capacity of clay and silty soil the experimental results were also confronted with a numerical simulation a finite element analysis model was employed in this study to predict the coupled process of variably saturated soils by the contaminants transported in runoff experimental and numerical results confirm that the water velocity and the kinetic adsorption are inversely proportional through the two and three layer barrier system keywords organic pollutant capillary barrier modeling adsorption transport 1 introduction the presence of human and industrial waste is the main reason of the soil contamination this waste is full of chemicals and toxic products that infiltrate into the subsurface soil which leads to the groundwater pollution there are many chemicals that can pollute water and soil which entail groundwater and soil contamination as a major problem as with many contaminants such as volatile organic compounds heavy metals fertilizers and detergents dyes have toxic mutagenic and carcinogenic effects on human health malik and grohmann 2011 chen et al 2011 some dyes cause soil contamination due to the toxic nature and harmful effects on all constituents of life for instance wastewater from textile dyeing has caused an immense pollution problem for both groundwater and soil these dyes are used as tracer on soils to detect preferential flow paths from soil surface to aquifers stein et al 1998 or in soil characterization as in the quantification of colloidal matters in soil inasmuch as the dye is up taken by adsorption onto soil particles surface flury and wai 2003 the methylene blue mb is one of the most popular dyes used in textile industries and in the coloring of paper wools cotton silk etc kumar et al 2005 the mb has also been of use in medical applications it contains an antiseptic propriety against bacterial infection and it is deployed as an antidote for cyanide poisoning reddy et al 2013 this cationic dye can cause a serious damage to the eyes of living beings and can provoke other health problems such as methemoglobinemia mall et al 2005 profuse sweating nausea and mental confusion christie 2001 in recent years many researchers are interested in soil contamination by persistent organic pollutant their studies focus on the impact of persistent pollutant on human health ferencz and balog 2010 among others concentrate on the adsorption of mb onto the soil in batch and continuous fixed bed column sana and jalila 2016 yukselen and kaya 2008 arab et al 2015 auta and hameed 2014 in order to study the impact and mobility characteristics of mb pollutant it s necessary to predict their adsorption and transport in porous media and include the factors affecting its migration and accumulation in soil which is a stratified medium soil is made up of different layers each layer has its own characteristics that make it distinct from all of the other layers soil is also a multiphase system thus the knowledge of the mechanisms and the coupled process associated with contaminant transport in soil is of great significance to deal with the remediation solutions layered soil was proposed in this work as a cover to prohibit the organic pollutant infiltration into groundwater because of the relatively high costs of traditional covers suter et al 1993 the cover with capillary barrier effects ccbe represents a promising alternative for the soil contamination problem aubertin et al 2009 khire et al 2000 li et al 2013 ng et al 2015 sadeghi et al 2014 mancarella et al 2012 predelus et al 2015 harnas et al 2014 proposed four layer cover systems with dual capillary to strengthen water storage of the cover a new three layer capillary barrier cover system is proposed by ng et al 2015 indeed the ccbe is primarily composed of two layer unsaturated soil system that employs the contrasting unsaturated hydraulic properties between both soils the experimental and numerical results showed that the three layer capillary barrier cover system performs well under extreme rainfall the performance of a ccbe is extremely dependent on the soil particle size the change in particle sizes of the soils in a ccbe derives a change in the hydraulic properties the unsaturated hydraulic properties of the soils influenced on the storage capacity of the surface layer and provoke the apparition an amount of infiltration from covers khire et al 2000 the determination of the unsaturated hydraulic proprieties is primordial in the modeling of water flow and contaminant transport whereas the methods of measuring of these soil retention properties are time consuming expensive and are not accurate because of spatial and temporal variability of these parameters many authors have proposed to minimize the number of tensiometers to 2 wessolek et al 1994 fujimakia and inoue 2003 schindler et al 2010 and simunek et al 1998 have used only one tensiometer several other researchers have resorted to numerical modeling to solve the problem by choosing time dependent and space variable boundary conditions cremer et al 2016 kumar et al 2015 the problem is difficult because of the complexities and strong heterogeneities of the geological mass it is therefore essential to know how to explore predictive models in the absence of sufficient information on heterogeneous hydraulic physical chemical properties usually the transport of species is described by fickian models of advection and dispersion equations ade following deterministic and or stochastic approaches drummond et al 2014 rumynin 2012 pedretti 2014 advection dispersion equation ade is extensively used to model several physical chemical and biological phenomena abdelkawy et al 2015 li et al 2017 song et al 2018 these processes are coupled and include water flow solute transport and reaction the advection dispersion equation could solve the transport of solute in unsaturated medium in heterogeneous porous media pandey et al 2018 and could account for the impact of retardation and song et al 2018 nevertheless the different versions of the ade model with a delay coefficient do not accurately describe btcs especially for long tails berkowitz et al 2006 cortis et al 2006 in contrast the ade cannot describe the non fickian transport or anomalous bijeljic et al 2013 neuman and tartakovsky 2009 a modified form of the convection dispersion equation including a two site kinetic retention model was used to simulate the transport and retention behavior of some nanomaterials in porous media bradford et al 2003 liang et al 2016 wang et al 2012 fang et al 2013 several models have been developed to predict the concentration profile of the dye in fixed bed column these models reproduce the experimental data in order to have a description of the adsorption mechanism recent studies have proposed the advection dispersion model to describe the dynamic behavior of a dye in fixed bed column hethnawi et al 2017 the advection dispersion adsorption equation which is the advection dispersion equation coupled with kinetic adsorption was carried out to model the transport of mb in a layered medium in this study our goals were i to study the adsorption desorption and transport behavior of mb in three soils in batch and dynamic mode ii to examine the performance of two and three layers capillary barrier cover system under organic pollutant adsorption in fixed bed column and iii to evaluate the ability of models to replicate the observed results of mb adsorption in soils under different experimental conditions besides a sensitivity analysis was carried out to study the influence of mobile immobile model parameters on the breakthrough curves 2 materials and methods 2 1 soil and solutions the soils used in this study were collected from an industrial zone in the region of sousse in tunisia the particle size distribution of sand clay and silty soil and the exchange capacity cec of clay were measured by laser diffraction particle size analyzer microtrac s3500 and by the metson method afnor nf x31 130 respectively the mineralogy of the clay sample was established by x ray diffractometer xpert pro panalytical the hydraulic parameters and the retention curves of three soils were obtained using cell compression and mini disk infiltrometer decagon devices the reactive dye methylene blue used as adsorbate basic blue 9 ci 52015 is a cationic dye with a molecular formula c16h18cin3s 3h2o and a molar mass of 373 9 g mol 1 the wavelength of maximum absorbance for mb is 663 nm 2 2 batch experiment adsorption isotherms and adsorption kinetics experiments were performed to evaluate the adsorption capacity of mb in three materials 2 2 1 adsorption isotherms the adsorption isotherms were performed in a set of glass flasks 60 ml containing 25 ml of mb solutions with different initial concentrations 0 5 6 10 14 20 25 40 mg l 1 and a 0 5 g of adsorbent added to each solution these mb solutions were kept under stirring speed of 450 rpm for 48 h to ensure that adsorption equilibrium was reached then the supernatants were fitted and centrifuged before the measurement of its absorbance the absorbance was measured using a uv vis spectrophotometer spectroflex6100 by monitoring the absorbance changes at wavelength of maximum absorbance 663 nm the mb concentrations were estimated using a calibration curve obtained by plotting the absorbance against the concentration of the mb solution the amount of dye adsorbed per unit weight of sample sand or clay at equilibrium q e mg g 1 was calculated as follow 1 q e m c e c 0 v where c 0 and c e are the initial concentration and liquid phase concentrations of dye solution at equilibrium mg l 1 respectively v is the dye volume l and m is the mass of the sample g the langmuir 1918 and freundlich 1906 models are selected to fit the adsorption isotherms the non linear form of these models is given as follow freundlich model 2 q e k f c e 1 n langmuir model 3 q e q m b c e 1 b c e k f freundlich isotherm constant mg g 1 l mg 1 1 n n n freundlich exponent q m langmuir adsorption capacity mg g 1 b langmuir isotherm constant l mg 1 2 2 2 adsorption kinetics for the adsorption kinetics experiments vials containing 1 g of sample and 20 ml of mb solutions with 150 mg l 1 initial concentration were shaken at 450 rpm for 90 min at the end of the predetermined time interval the vials were filtered then centrifuged at 2500 tr min 1 the supernatant solution was analyzed using uv visible spectrophotometer spectroflex6100 to determine the dye concentration each experiment was replicated 3 times to verify the reproducibility of the experience the adsorption capacity qt mg g 1 was calculated as 4 q t c 0 c t v m where q t is the dye concentration on adsorbent at time t mg g 1 c 0 and c t are the concentrations of the mb solution before and after adsorption respectively mg l 1 v is the volume of mb solution and m is the mass of dry sample g there are many types of the pseudo second order kinetic equation but in the dye adsorption process the types 1 and 2 describe more the fitting of kinetic models wawrzkiewicz and hubicki 2009 thus the study of pseudo second order kinetic is limited to types 1 and 2 in this work the pseudo first order kinetics may be expressed as 5 l o g q e q t log q e k 1 t 2 303 the non linear form of pseudo second order equation is given as 6 t q t 1 k 2 q e 2 1 q e 2 3 column experiment a series of fixed bed column experiments have been performed to determine the mb behavior in unsaturated soil and to study the effect of capillary barrier on mb transport the column used in the experiments was made with glass and has a dimension of 3 5 cm in diameter and 25 cm in length the column experiments were conducted at constant flow rate and at initial mb concentration under unsaturated flow conditions then the mb was eluted from the column by rainwater flushing during the experiment the effluent samples were collected at the column outlet and measured for mb concentration immediately after sampling the mb concentrations in the column effluents were determined using uv vis spectrophotometer the mb breakthrough curves btc were obtained by plotting relative concentrations c c 0 versus time t see table 1 3 breakthrough curves modeling in this study three approaches have been investigated for data fitting of mb breakthrough curves these models include thomas convection dispersion and mobile immobile model 3 1 thomas model the thomas model thomas 1944 assumes that a langmuir isotherm and second order kinetic fitted well the experimental data this model has assumed also that adsorption is limited by mass transfer with no axial dispersion is derived and has allowed the calculation of the adsorption rate constant the equation of thomas model can be described as 7 c t c 0 1 1 e q 0 m c 0 ϑ t k t h ϑ where k th is thomas rate constant ml min 1 mg 1 q 0 is equilibrium adsorption capacity mg g 1 m is the mass of the adsorbent c 0 and c t are the mb concentration in the influent and at time t mg l 1 and υ is the flow rate ml min 1 3 2 advection dispersion adsorption model a general transport and adsorption model for a porous medium was used to predict dynamic adsorption breakthrough this model is based on convection dispersion equation eq 8 for two dimensional flows through a porous bed parker and van genuchten 1984 8 θ c t ρ b c p t θ d l c u c r s where c and c p are the adsorbate concentration in the liquid and solid phase respectively kg m 3 ρ b is the bulk density kg m 3 θ is the bed water content k p is the adsorption rate m3 kg 1 u is the darcy velocity m s 1 r and s are the reaction and d l is the hydrodynamic dispersion tensor the rate of accumulation in the solid phase term c p t was accounted for the following equation 9 c p t α k l c q m c p c p α is the rate constant s 1 q m is the maximum langmuir adsorption parameter c p is the adsorbent capacity kg m 3 and k l is the langmuir adsorption parameter m3 kg 1 severe incorporation of richard equation eq 10 is to describe the darcy flow through porous media was coupled with advection dispersion equation this coupling is through darcy velocity equation eq 11 the richard equation is expressed as follows 10 c s e s h p t k h p d q m 11 u k s k r h p d where h p is the pressure head m c is the specific moisture capacity s e represents the effective saturation s is the storage coefficient k is the hydraulic conductivity k r is the relative permeability d represents the elevation m and q m is the fluid source or sink the unsaturated soil properties were described using the van genuchten model 12 k θ k s s e 2 1 1 s e 1 m m where m is the van genuchten parameter and k s denotes the hydraulic permeability the main problem for unsaturated flow lies in the choice of boundary conditions at the lower base and the upper base of the domain in addition to the strong dependence of the hydrodynamic parameters hydraulic conductivity darcy velocity pressure load the degree of water saturation or water content at the lower and upper limits of the domain varies over time especially at the beginning of the experiment and subsequently the parameters k θ q θ and h θ vary as well to overcome these difficulties we use three successive and complementary sub models to make the right choice of boundary conditions that describe the flow regime throughout the experiment the following flowchart fig 2 explains further the proposed methodology for solving the problem and determining the most appropriate boundary conditions we begin by developing a first sub model for the unsaturated flow regime with zero flux conditions on both the lower and upper surfaces of the geometry to start the flow from the upper surface second sub model the values recorded on the upper surface during the unsaturated phase first sub model are imported as boundary conditions at the upper base until flow start time flow followed by pressure values imposed by the flow we keep for the moment the zero flow condition at the lower base the calculation of the second intermediate sub model is started and the saturation time of the medium is recorded at this point the state of saturation of the medium is reached and the pressure load at the bottom surface becomes constant finally it is the third model that will be the final full model provided to the upper surface is an interpolation function describing the evolution of the capillary pressure taken from the initial time until the time of the start of the flow followed by the pressure values imposed by the flow for the condition of the bottom surface the pressure is the interpolation of the values taken from the initial time until saturation time tsaturation followed by the value of the saturation pressure the following chart fig 2 can better explain the methodology proposed to solve the problem following the strategy described above for the study of water flow and solute transport in a soil column of height 10 cm and diameter 3 5 cm the initial and boundary conditions for an unsaturated and saturated medium are described in fig 3 the initial and boundary condition for two dimensional problem used for solving this model are 13 c 0 kg m 3 t 0 z c c 0 z 10 cm nd i c i 0 z 0 cm for the richard equation the initial condition is a linear function which describes the pressure head evolution in the column at time 0s the upper and lower conditions are two interpolation functions p1 t p2 t which describe the pressure head evolution at any time throughout the experiment fig 3 3 3 mobile immobile model mim the transport equation takes into account not only the convection diffusion and dispersion but also the phenomenon of retention of the polluting particles by the solid phase of the porous medium via a distribution coefficient kd the one dimensional equation of the mim transport model for a reactive solute in unsaturated media is van genuchten and wierenga 1976 14 θ m f ρ d k d c m t θ i m 1 f ρ d k d c i m t q c m z d m θ m 2 c m z 2 θ i m 1 f ρ d k d c i m t k m c m c i m where θ m and θ im represent the mobile and immobile water fractions respectively f is the fraction mobile ρ d is the bulk density k d is the partition coefficient c m et c im are the pollutant concentrations in mobile and immobile phase respectively d m is the diffusion coefficient in the fraction mobile and q is the darcy velocity the mim model parameters are described in the following table see table 2 where d is the diffusion coefficient in liquid phase θ is the porosity λ is the longitudinal dispersivity ρ is the density c 0 is the initial concentration k d is the distribution coefficient n is the coefficient of freundlich model and α is the mass transfer coefficient 4 results and discussion 4 1 samples characterizations the results of x ray spectroscopy show the presence of smectite clay the smectite clay is characterized by its cationic exchange capacity in the interlayer spaces the value of this cec is found to be equal to 90 meq 100 g of clay the x ray diffraction pattern fig 4 a had three significant reflections at 15 6 a 3 34 a and 1 45 a the two reflection distances of 3 31 a and 1 45 a indicated the presence of silice and that the clay is a dioctahedral smectite respectively the particle size analysis of sand clay and silty soil samples shows that the fifty percent passing particle size d50 was calculated as 317 μm and 11 69 μm based on sieve sizing for sand and clay respectively fig 4 b soil water characteristic curves θ h are presented in the following figure fig 4 c comparing the saturated water content of three soils we deduce that clay have the high saturated soil water content the hydrodynamic parameters of three soils are recapitulated in table 3 4 2 adsorption models it is found that the clay has the best adsorption capacity compared to silty soil and sand the good fitting of the model with experimental data is represented by a high value of correlation coefficient for the clay and silty soil adsorbents the freundlich model provides a better fit in the mb adsorption isotherm fig 5 b and c the freundlich isotherms model predicts multilayer adsorption and describes equilibrium on heterogeneous surfaces freundlich 1906 gimbert et al 2008 for sand modeling of adsorption isotherm shows good agreement with langmuir fig 5 a this model assumes that the forces of interaction between the adsorbed molecules are negligible gimbert et al 2008 there is a single layer of adsorbate on the outer surface of the adsorbent langmuir 1918 and no further adsorption will take place if the molecule occupies the adsorption site the prediction of kinetics adsorption is needed to evaluate the adsorption efficiency using both models the pseudo first order equation represented very well the kinetic data of mb in sand and clay the applicability of langmuir isotherm for sand material implies that monolayer adsorption exist under the experimental conditions the best regression of experimental data for both sand and silty soil is using the type 1 of pseudo second order kinetic equation fig 6 b 4 3 column experiments 4 3 1 effect of the capillary barrier on mb retention the fig 7 shows the capillary barrier effect on the mb breakthrough curve by comparing homogeneous and layered porous media this figure clearly shows a difference in the mb behavior during its infiltration in unsaturated homogeneous and layered media saturated at 60 vp the layered medium exp b consists of 80 of sand distributed over a height of 20 cm at the top of the column and 20 clay corresponds to a height of 5 cm the mb injection is performed at an average flow rate of q 8 5 ml min 1 and after a flow stabilization time of 34 min during the first 60 min the concentration is zero the injected solution is pushing the liquid phase initially present in the column in the layered medium the mb solution has covered a distance of 20 cm of homogeneous sand then it is found in the contact area between the two layers this area is a transition zone in which water is trapped for some time to adapt to sudden change in characteristics of the medium this explains the retardation in the release of the effluent from the column then the concentration gradually increases to a maximum value this value is equal to c0 for a homogeneous medium after 319 min while it is equal to 0 88 c0 for a layered medium after a time of 578 min the long tail of the breakthrough curve is due to the mb reactivity with the clay fraction 4 3 2 effect of three barrier system on mb adsorption the influence of the capillary barrier system on the mb breakthrough curve was also studied using three layer medium fig 3 the mb concentration increases slowly and depends on the saturation condition and the structure of the porous medium indeed the first layer at the top of the column is a 1 cm of silty soil and the second layer is a 7 cm of sand both layers are initially saturated with water the third layers 2 cm of clay are initially unsaturated exp d the delay of the mb release after 104 days is justified by the physical and chemical heterogeneities of the porous medium the interface between two porous media having different characteristics is a capillary barrier and a trapping zone despite a slight increase in concentration gradient the steady state of mb transfer is not reached at t 214 days and outlet concentration does not exceed 4 5 of the inlet concentration fig 8 generally the btc in heterogeneous media are described by lognormal distribution and recently this log normal distribution has been endorsed for non gaussian correlation moslehi and de barros 2017 comparing the exponential distribution r2 0 94 and the gaussian distribution r2 0 97 we can conclude that the dispersion process is predominant at the beginning of the curve the exponential shape of btc proves that dispersion and sorption are the main processes in the mb transport 4 4 models validation 4 4 1 comparison of thomas model and convection dispersion adsorption model a numerical simulation using the advection dispersion adsorption model for three layer porous media is carried out adsorption is considered linear for silty soil and clay while it is langmuir type in the sand sample eq 8 describes the transport of mb in a porous medium composed of three areas of different physical and chemical properties these properties are defined in table 4 the selected geometry is very similar to the physical reality of the experimental setup fig 1 the chosen triangular mesh is refined at the interfaces between the fields and the time step retained during the simulation is 30000s in fig 2 the boundary conditions are described for short time t 170 days the convection dispersion adsorption model reproduced fairly well the experimental data and for long time t 170 days the numerical curve is well below experimental measurements and the standard error of the simulation with the model thomas gets weaker fig 9 4 4 2 kinetic adsorption in layered medium for the two layer medium and at the interface between the two layers z 5 cm the water is trapped in the capillary barrier composed of two layers sand and clay with hydraulic properties are different this entrapment causes the prevention and accumulation of water at the interface until the water pressure head becomes sufficient to allow it to penetrate li et al 2013 the water pressure gradient increase associated with the accumulation of water in the two media interface has a capillary barrier effect in this interface retention is more important predelus et al 2015 indeed after 100 min the adsorbed concentration gradient in z 5 cm interface between sand and clay materials is larger compared to homogeneous medium fig 10 this increase in retention capacity at this interface is the result of increase in water content and decreasing in interstitial water velocity which enhances the trapping of the mb solution torkzaban et al 2008 in fact the decrease of the pore water velocity is accompanied by a decrease of the adsorption kinetics velocity at the interface fig 11 the same result is found for a three layers capillary barrier system the fig 12 represents the evolution of adsorption kinetics along the column in the breakthrough time t 8 64 10 6 s and in the end of the experiment t 1 72 10 7 s indeed the adsorption kinetics is increased in two capillary barriers z 9 cm interface between silty soil and sand z 2 cm interface between sand and clay 4 4 3 simulated mb adsorption both modeling approaches available in hydrus 1d for mb transport were used and compared prior to assisting in the interpretation of the field data fig 12 shows the mb breakthrough curve and model results obtained from the advection dispersion transport equation the mobile immobile solute transport mim and thomas model the simulated data obtained by the mim model fig 13 are coherent with experimental data acquired since they prove that mb solution takes 350 h for breaks through a fixed bed column of 10 cm in height it is also expected that the shape of the breakthrough curve is not asymmetric causing a significant adjustment error for the convection dispersion adsorption model however mim model better reproduced the exponential shape profile with long tails table 5 this means the existence of an immobile water fraction in the porous medium 4 5 parameter sensitivity analysis after mim model validation a parameter sensitivity analysis of this model on the mb breakthrough curve was performed the mb transport is dominated by convection phenomena dispersion diffusion and adsorption in the presence of a mobile water fraction 4 5 1 effect of the exchange coefficient when the solute transfer coefficient α between the mobile and immobile fraction f of the liquid phase increases the increasing part of the breakthrough curve tends to be asymmetrical and spreads a long period fig 14 indeed the more the breakthrough curves are asymmetrical the more exchange between mobile water and immobile water is important note also that the more the fraction f is close to 1 the more the model s sensitivity on this exchange coefficient is low if the exchange coefficient α is very small the solute velocity in the mobile fraction and transfer can be considered by the convection dispersion but at a higher velocity than the convection dispersion adsorption model however an increase of the coefficient causes a rapid exchange and a tendency to have uniformity of the concentration in the two fractions of water which causes spreading of the breakthrough curve increasing degree of saturation of the medium and the pore velocity generates increased mobile water fraction and therefore the coefficient padilla et al 1999 nevertheless there is no expressive relationship between f and α and between f and λ pang and close 1999 but an intense inverse correlation between α and λ 4 5 2 effect of longitudinal dispersivity the influence of the local dispersivity does not make sense for low values of peclet numbers that is to say to the low velocity the increase in dispersivity causes an increase in dispersion coefficient and thereafter increasing mass diffusive flow and exchange between the mobile and immobile water regions in our case the flow velocity in the silty soil is low and thereafter a wide variation in the dispersion coefficient has no particular effect on the values breakthroughs concentrations see fig 15 4 5 3 effect of the diffusion coefficient fig 16 shows the influence of a diffusion coefficient variation on the mb breakthrough curve indeed by reducing the diffusion coefficient the breakthrough curve becomes more spread out with a delay time of breakthrough while an increase in this coefficient favors the transport of mb and causes rapid penetration of mb so the effect of this factor is most evident in mb breakthrough time a small influence on the concentration value y axis 4 5 4 effect of the distribution coefficient a variation of this parameter has an impact on the amplitude change and an offset in the time of the simulated curve the breakthrough curve is quite sensitive to a variation in distribution coefficient fig 17 indeed an increase of this coefficient resulted in a delay output of the solution a significant increase of breakthrough time a spreading of the breakthrough curve and a decrease in the outlet concentration by increasing this coefficient the solid matrix retains and traps the material and thereafter it delays the transfer the variation of distribution coefficient causes a variation in time required to reach the equilibrium musielak 2012 thereafter this coefficient is involved as a term of delay 4 5 5 effect of the freundlich coefficient fig 18 represents the breakthrough curves for different values of freundlich coefficient n it is noted that the relative concentration increases with increasing freundlich coefficient unlike the concentration of mb adsorbed on the soil grains for n 1 linear isotherm or n 1 the delay coefficient is low and the breakthrough curve becomes symmetrical this symmetry decreases with the decrease of n that is to means higher the coefficient n decreases as the curve becomes non linear in the case of a non linear isotherm n 1 the concentration of mb in the liquid phase decreases proportionally with decreasing freundlich coefficient fig 18 a which is not the case for the mb adsorbed concentration on the solid phase in fact for n 0 823 a decrease in the mb concentration in the liquid phase compared to the case of the linear isotherm n 1 was found this decrease leads to an increase of the delay coefficient which causes an increase in adsorbed concentration on the solid phase at equilibrium and for nonlinear adsorption the adsorption is more significant and the delay dispersion and are stronger and the shape of the elution curve is asymmetrical fig 18 b 5 conclusion the aim of this work was to study the mb adsorption in layered soil and to discuss the effectiveness of layered soil in the reduction of organic pollutant transport the results showed that the adsorption isotherm is described using langmuir model for sand and freundlich model for clay and silty soil also the adsorption kinetic is described using pseudo first order model for both materials sand and clay and pseudo second order for silty soil both the experimental exploration and the numerical simulation showed that the three layer capillary barrier cover system performs as inhibitor to minimize pollutant percolation therefore in capillary barrier the mb kinetic adsorption is inversely proportional to the flow velocity the comparative analysis between three models proved that mobile immobile model is the suitable one to describe the adsorption and transport of mb in unsaturated soil 
