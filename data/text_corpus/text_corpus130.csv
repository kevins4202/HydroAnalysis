index,text
650,aquifers with quasi linear flow pattern are frequently envisaged in fractured zones in oil gas or enhanced geothermal reservoirs or in civil engineering where cut off walls are constructed the water flow towards a well in this linear aquifer system has been long investigated under darcian flow condition but remains an open issue for non darcian flow in this study a general linearization approximation strategy is suggested for the forchheimer equation and an analytical solution is proposed by using laplace transform for non darcian flow towards a well in aquifers laterally bounded by no flow barriers numerical simulations using the finite volume method prove that the linearization approximation performs best when it takes the mean of two commonly used strategies and the analytical model is sufficiently accurate at late times for observation wells located moderately far from the source the proposed model was applied to data interpretation of the pumping tests at the changheba dam foundation bounded by two cut off walls in southwest china where the drawdown curves can be divided into 1d flow transitional flow and 2d flow stages as a result of lateral flow through weathered bedrocks at late times the proposed model provides a valuable tool for characterizing the hydraulic properties of aquifers and reservoirs with a linear flow pattern and for assessing the possible leakage through the lateral barriers by type curve matching keywords analytical solution linear flow pattern forchheimer flow constant rate test 1 introduction the groundwater flow towards a well may exhibit features of linear flow pattern i e all the flow lines are straight and parallel towards the source when an aquifer is bounded by parallel linear no flow boundaries walker and roberts 2003 this flow pattern frequently occurs in fracture zones having much higher permeability than the surrounding rocks gringarten et al 1974 jenkins and prentice 1982 sen 1986 1987 wen et al 2006 in aquifers laterally bounded by natural ehlig economides and economides 1985 or artificial barriers e g concrete cut off walls in civil engineering wu et al 2015 wang et al 2017 in oil or gas reservoirs having a predominantly long narrow shape miller 1962 nutakki and matter 1982 el banbi and wattenbarger 1998 escobar et al 2007 and in hydraulically fractured wells in shale gas and enhanced geothermal reservoirs wattenbarger et al 1998 ladner and haring 2009 ahmadi et al 2010 nobakht and clarkson 2012 behmanesh et al 2018 therefore the linear flow in aquifers has gained interests for decades not only in groundwater modeling and management but also in oil gas and geothermal production in the sense of flow dimensions barker 1988 chang and yortsos 1990 walker and roberts 2003 le borgne et al 2004 walker et al 2006 liu et al 2016 2017 the linear flow is also called one dimensional 1d flow on the condition that the darcy s law applies analytical drawdown solutions have been developed for 1d transient flow towards a well pumped at constant flow rates using variable separation method ferris et al 1950 laplace transform miller 1962 or fourier transform jenkins and prentice 1982 a generalized radial flow model was also proposed by barker 1988 for constant rate tests in aquifers with a non integer flow dimension n 1 n 3 which reduces to miller s 1962 1d flow model for n 1 an alternative method to solve this problem is to transform the aquifer of finite extent into one of seemingly infinite extent by introducing image wells and superposing the theis 1935 solutions ferris et al 1962 kruseman and ridder 1990 but with the increase of flow velocity or reynolds number re in the formations the flow behavior may deviate from the darcy s equation due to non negligible inertial losses forchheimer 1901 izbash 1931 bear 1979 venkataraman and rao 1998 bordier and zimmer 2000 chen et al 2015a b c and hence interpretive models were further developed for 1d non darcian flow for instance wen et al 2006 developed an izbash s 1931 law based model for non darcian flow in a single confined vertical fracture towards a well by using boltzmann transform liu et al 2016 proposed an izbash s law based generalized radial flow model 1 n 3 by means of laplace transform and linearization approximation which applies to 1d non darcian flow for n 1 the major limitation with these models is that the exponent in the izbash s law was treated as a constant between 1 and 2 which becomes inaccurate to describe the laminar flow when the flow velocity or re becomes small liu et al 2016 2017 to overcome this limitation interpretive models were also developed based on the forchheimer s 1901 law which has rigorous theoretical background and clearer physical meaning irmay 1958 hassanizadeh and gray 1987 chen et al 2001 sen 1987 presented a forchheimer s law based analytical solution for the extended well tests in fractured formations by boltzmann transform method but later on this method was found to be problematic in a rigorous mathematical sense camacho v and vasquez c 1992 wen et al 2008 2009 moutsopoulos and tsihrintzis 2005 derived approximate similarity solutions for 1d transient forchheimer flow with a head rise but no external sink source terms were considered liu et al 2017 proposed an analytical model for forchheimer flow in generalized radial formations 1 n 3 based on the concept of drawdown or buildup decomposition but this model becomes less accurate when the flow dimension is smaller than 1 5 therefore seeking an accurate analytical model for non darcian flow in a linear flow system remains an open issue in this study the goal is to develop an analytical solution for forchheimer flow towards a well in a linear aquifer system laterally bounded by no flow boundaries where the flow eventually becomes one dimensional at late times or at locations moderately far away from the source a general linearization approximation strategy is suggested for the forchheimer s 1901 equation and an analytical model is established through laplace transform which is represented as a function of time and the distance from the source and can be readily reduced to miller s 1962 model for 1d darcian flow two dimensional numerical simulations based on the finite volume method are conducted to calibrate the constant in the linearization approximation and to validate the accuracy of the analytical model the proposed model is applied to data interpretation of the pumping tests performed at the changheba dam foundation between two cut off walls in southwest china chen et al 2016 the proposed model provides a tool for characterizing the hydraulic properties of aquifers with a linear flow pattern and for assessing the possible leakage through the lateral barriers by type curve matching 2 model development consider a constant rate test crt performed in a confined aquifer with no flow lateral boundaries fig 1 a where the pumping well of radius r w is located at the center of the aquifer the thickness of the aquifer is b and the width between the two impermeable lateral boundaries is l this is essentially a bounded two dimensional problem and a proper simplification is needed to obtain the analytical solutions under darcian flow conditions analytical solutions are available for this flow system by applying the well image theory e g ferris et al 1962 nutakki and matter 1982 kruseman and ridder 1990 dewandel et al 2014 or by assuming 1d flow geometry fig 1b ferris et al 1950 miller 1962 jenkins and prentice 1982 a comparison between these two types of solutions appendix a shows that the 1d flow solution of drawdown becomes sufficiently accurate at late times or at locations moderately far away from the source e g x l where x is the coordinate along the flow direction indicating that this 2d problem fig 1a can be reasonably approximated by the 1d flow system represented in fig 1b under these conditions therefore this study focuses on development of a 1d flow model for this flow system fig 1b under non darcian flow conditions where the principle of superposition is no longer valid and hence the well image method does not apply mathias and todman 2010 mijic et al 2013 the continuity equation of the 1d flow system fig 1b can be written as sen 1987 wen et al 2006 liu et al 2016 2017 1 s s h t q x where h is the drawdown q is the specific discharge ss is the specific storage coefficient x is the coordinate along the flow direction and t is time the initial and boundary conditions are given by 2a h x 0 0 2b h t 0 2c lim x 0 2 b l q q where q is the constant volumetric rate pumped from the source at x 0 note that in this study the wellbore storage is not considered i e the well radius rw is assumed to be infinitesimal the water flow in the formation is assumed to obey the forchheimer s 1901 law 3 h x q k 1 q k 2 2 where k 1 is the intrinsic hydraulic conductivity of the formation quantified by the inverse of the initial slope of the h x q curve and k 2 is the inertial hydraulic conductivity representing the degree of deviation from linearity as q increases liu et al 2017 zhou et al 2019 obviously eq 3 reduces to darcy s law as k 2 or q becomes small enough it should be noted that under transient flow conditions the right hand side of eq 3 contains a rate dependent term of q irmay 1958 but as clarified by irmay 1958 this term is unimportant and takes effect only initially during a fraction of a second and it can be neglected for reducing the uncertainties in parameterization substituting eq 3 to eq 1 yields a non linear governing equation hard to be analytically solved wen et al 2008 liu et al 2016 to overcome this difficulty linearization approximation is commonly adopted ikoku and ramey 1979 odeh and yang 1979 wen et al 2008 liu et al 2016 which replaces the specific discharge at any location x in the nonlinear term with q q 2bl here it is interesting to note that there are two strategies to apply this linearization approximation the first one is to directly modify the quadratic term in the forchheimer s eq 3 which is equivalent to replacing the intrinsic hydraulic conductivity k 1 with an apparent hydraulic conductivity k a accounting for the inertial effects choi et al 1997 mijic et al 2013 4a h x q 1 k 1 q k 2 2 q 1 k 1 q 2 b l k 2 2 2 h x 2 1 k 1 q 2 b l k 2 2 q x the second strategy is to use this approximation after taking partial derivatives of eq 3 with respect to x which is similar to the procedure adopted by wen et al 2008 and liu et al 2016 for linearizing the non linear term in the izbash s 1931 law 4b h x q k 1 q k 2 2 2 h x 2 1 k 1 2 q k 2 2 q x 1 k 1 q b l k 2 2 q x obviously eq 4b estimates an inertial effect two times higher than eq 4a therefore a general linearization approximation for the forchheimer s law can be suggested as 5 2 h x 2 1 k 1 q λ b l k 2 2 q x where λ is a constant that can be calibrated by numerical solutions or crt data according to eq 4 a reasonable range of λ is between 1 and 2 more discussion on the range and the best estimate of λ will be presented later in section 3 2 substituting eq 5 into eq 1 yields 6a 2 h x 2 a h t 6b a s s 1 k 1 q λ b l k 2 2 eq 6a is a linear partial differential equation which can be solved by the laplace transform 7 2 h x 2 a p h 0 where p is the laplace variable and h is the laplace transform of h accordingly the boundary conditions eqs 2b and 2c are transformed to 8a h p 0 8b lim x 0 h x 1 p q 2 b l k 1 q 2 b l k 2 2 note that eq 7 is a form of the bessel s equation and its general solution reads ikoku and ramey 1979 wen et al 2008 liu et al 2016 9 h x p x 0 5 c 1 i 0 5 a p x c 2 k 0 5 a p x where iν z and kν z are the first and second kind modified bessel functions respectively and c 1 and c 2 are two integration constants dependent on the boundary conditions the far field boundary condition in eq 8a implies c 1 0 and hence eq 9 reduces to 10 h x p c 2 x 0 5 k 0 5 a p x recalling the properties of kν z zk ν z zk ν z zk ν 1 z and k ν z k ν z we obtain the derivative of h with respect to x 11 h x a p c 2 x 0 5 k 0 5 a p x the constant c 2 is then determined by using the properties of kν z lim z 0 ν 0 k ν z γ ν 2 z 2 ν and substituting eq 8b to eq 11 12 c 2 2 q 2 b l k 1 q 2 b l k 2 2 π a 0 5 p 1 25 substituting eq 12 into eq 10 yields the drawdown in the laplace domain h 13 h x p f p 1 25 k 0 5 a p where f and a are two p independent parameters 14 f 2 q 2 b l k 1 q 2 b l k 2 2 π a 0 5 x 0 5 a a x eq 13 can be solved by the analytical laplace inversion method 15 h x t f 1 2 a 2 0 5 γ 0 5 a 2 4 t where γ v u is the complementary incomplete gamma function substituting eq 14 into eq 15 and using the property of the gamma function γ 0 5 u 2 e x u π e r f c u we obtain the final expression of the 1d non darcian flow model 16a h x t x q 2 b l k 1 q 2 b l k 2 2 e u π u e r f c u 16b u a x 2 4 t where u is a dimensionless parameter and erfc u is the complementary error function e r f c u 1 2 π 0 u e η 2 d η the proposed model eq 16 immediately reduces to the 1d darcian flow model appendix a as the inertial effect becomes negligible i e k 2 the late time analytical solution is of particular concern for crt data interpretation because the late time solution normally becomes accurate enough after a certain time of pumping wu 2002a b liu et al 2016 2017 the late time solution can be obtained by taking the limit of eq 16 as t 17 h x t q 2 b l k 1 q 2 b l k 2 2 2 π a t x it can be inferred from eq 17 that the hydraulic gradient h x given in eq 3 at late times t becomes time independent or in other words h x tends to be at a quasi steady state and the discharge across any cross section becomes q q 2bl at late times this favorable property has also been revealed in the analytical models by liu et al 2016 2017 furthermore it is interesting to observe from eq 17 that the drawdown h is proportional to t 0 5 at late times which gives a slope of 0 5 in the log log plot of drawdown versus time this is actually a common behavior of 1d flow miller 1962 jenkins and prentice 1982 barker 1988 leveinen 2000 le borgne et al 2004 by adopting the following dimensionless transformations 18 u f s s x 2 4 k 1 t w u f 2 b l k 1 q x h x t β d q k 1 b l k 2 2 where uf is the time factor w uf is the non darcian well function and βd is a dimensionless variable reflecting the degree of nonlinearity the linearization solution eq 16 can be rewritten in dimensionless form as 19a w u f 1 β d 2 e u u π e r f c u 19b u 1 β d λ u f here again eq 19 reduces to the darcian flow solution appendix a when βd 0 for comparison we recall sen s 1987 model established based on the forchheimer s equation using the boltzmann transform for the extended well problem in fractured formations 20 h x t q 2 b l k 1 q 2 b l k 2 2 x e u f 1 q k 1 2 b l k 2 2 1 e u f 2 d x the dimensionless form of sen s model is given by 21 w u f 2 β d 4 u f u f e η η 1 β d 2 1 e η 2 d η 3 numerical validation the development of the proposed model involves two approximations one is the neglect of the radial well in fig 1a and the other is the linearization approximation given in eq 5 it remains unclear however what uncertainties could result from these approximations and how the constant λ should be parameterized in this section we address these issues by numerical simulations numerical models have been developed for forchheimer flow to a well using finite difference method choi et al 1997 ewing and lin 2001 wu 2002a b mathias et al 2008 mijic et al 2013 liu et al 2017 finite element method ewing et al 1999 kolditz 2001 zhang and xing 2012 chen et al 2015b and finite volume method ewing and lin 2001 moutsopoulos and tsihrintzis 2005 for the two dimensional model with no flow boundaries in this study fig 1a we use the finite volume method for its good conservation and high adaptability to unstructured grids 3 1 numerical setup a quarter of the domain with an infinite boundary xe 106 m along the x axis fig 2 is used for numerical simulations by utilizing the symmetry conditions of the flow problem fig 1a quadrilateral grids are adopted to discretize the domain with curved cells around the pumping well fig 2 to ensure high resolution near the well the grid points are logarithmically spaced away from the source the domain is discretized into 500 and 100 segments along the x and y directions respectively which results in a total of 50 000 grid cells the boundaries at x 0 y 0 and y l 2 are no flow boundaries the infinite boundary at x xe is a zero drawdown boundary and the well screen is a discharge boundary since the well storage is not considered the well screen radius rw is set small enough rw 10 3 m to reduce numerical errors the time scale is also logarithmically discretized to allow for the rapid head change at early times the head and velocity at each cell control volume are represented by the node at the center of the cell and the head change of the cell is contributed to the discharge from the four neighboring cells the continuity equation for an individual cell i is given by 22 s s a i δ h i δ t j 1 4 s i g n h i j h i q i j l i j s i n θ i j where ai is the area of the cell hi is the drawdown of the cell δhi is the head change of the cell δt is the time interval qij j 1 2 3 4 is the discharge contributed by the neighboring cells lij is the length of the connection with the neighboring cells fig 2 hij j 1 2 3 4 is the drawdown of the neighboring cells and θij is the angle between qij and lij note that θij is not always equal to 90 due to the use of curved cells near the pumping well the forchheimer s law eq 3 can be rewritten in the following discretized form 23a q i j 1 f i h i j h i r i j 23b f i 1 k 1 q i k 2 2 23c q i q i 1 s i n θ i 1 q i 3 s i n θ i 3 2 2 q i 2 s i n θ i 2 q i 4 s i n θ i 4 2 2 where rij is the distance between the centers of cell i and its neighboring cell j j 1 2 3 4 qi is the magnitude of the flux at cell i and fi is a nonlinear factor dependent on qi choi et al 1997 mijic et al 2013 substituting eq 23 into eq 22 yields the implicit difference form for any inner cell i 24a h i h i 0 δ t j 1 4 s i g n h i j h i 1 f i l i j s i n θ i j h i j h i s s a i r i j where h i 0 is the initial drawdown at a time step for the cells on the discharge boundary eq 24a becomes 24b h i h i 0 δ t q 4 s s a i n b j 2 4 s i g n h i j h i 1 f i l i j sin θ i j h i j h i s s a i r i j where q is the total pumping rate b is the thickness of the aquifer and n is the number of segments on the discharge boundary n 20 in this simulation no flow or infinity boundaries are implemented using image nodes outside the boundary denoting the drawdown by hie one has hie hi 0 on the no flow boundaries and hie 0 on the infinity boundary we develop a finite volume code in matlab in which eq 24 is integrated by the stiff integrator ode15s available in matlab which has been successfully applied to numerical simulations for non darcian flow mathias et al 2008 wen et al 2008 the nonlinear equation eq 24 can be effectively solved with an iterative procedure mijic et al 2013 chen et al 2015b by initially assigning 1 k 1 to fi and obtaining the initial head distribution in darcian flow condition the flux qi is calculated by eq 23a and eq 23c using the known head values at the previous iteration and then by updating the nonlinear factor fi the head distribution is re calculated in the next iteration this iteration procedure above is repeated at each time step until the difference between the results in two consecutive iterations satisfies the following convergence criterion 25 h m 1 h m 2 δ h m 2 where h is the vector of nodal drawdown m is the iteration step 2 denotes the euclidean norm of a vector and δ is a user specified error tolerance δ 1 10 5 in this study 3 2 numerical results for ease of presentation we introduce the following dimensionless transformations 26 x d x l b d b l y d y l t d 4 k 1 t s s l 2 h d 2 k 1 l q h x y t fig 3 plots the numerical solutions with xd 1 and βd 10 at various coordinates xd 0 5 10 and yd 0 0 5 fig 3a shows that the numerical solutions hd versus td become independent of the y coordinate when the observations wells are located moderately far away from the source e g xd 1 which can be easily satisfied in field conditions or at late times for xd 1 fig 3b shows that along the x axis yd 0 the non darcian well function w uf eq 18 converges to a single curve when 1 uf 10 which indicates again that the flow system exhibits the feature of one dimensional flow for xd 1 at late times to eliminate the error caused by the presence of well we use the numerical results at xd 1 and yd 0 in the following for further validating the proposed linearization solutions fig 4 plots a comparison between the numerical results and the analytical solutions eq 19 for one dimensional non darcian flow with increasing degree of nonlinearity i e βd increases from 1 to 50 fig 5 shows a semi log plot of the relative error ε defined as the radio of the difference between the analytical and numerical solutions over the numerical results against the dimensionless time 1 uf for comparison the analytical solutions by sen s 1987 model eq 21 are also plotted fig 4 shows that all the 2d numerical and 1d analytical solutions become parallel straight lines with a slope of 0 5 in a log log scale at late times which further justifies the one dimensional simplification for the constant rate test in a stripped aquifer bounded by two parallel barriers the value of λ has a significant impact on the accuracy of the linearization solution eq 19 especially for larger βd compared to the numerical solutions the value of λ 1 underestimates the drawdown at late times whereas the value of λ 2 yields an overestimation with their relative errors at late times ranging 5 13 12 26 15 20 and 18 29 for βd 1 5 10 and 50 respectively fig 5 a λ value out of the range between 1 and 2 e g 0 5 and 5 results in greater errors which justifies the range of λ suggested by eq 5 but interestingly we find that the mean value of λ in its reasonable range i e λ 1 5 yields a perfect match between the analytical and numerical solutions at late times with a relative error below 0 7 2 0 at late times for any βd between 1 and 50 fig 5 fig 6 further plots the variation of error at late times the error becomes stable at late times fig 5 with λ evidencing that the error is minimized in the vicinity of λ 1 5 for varying degree of nonlinearity therefore eq 5 with λ 1 5 provides a quite reasonable linearization approximation strategy for forchheimer flow with linear flow pattern at early times however the linearization solutions for any values of λ between 1 and 2 underestimate the drawdown and this discrepancy increases with increasing degree of nonlinearity fig 4 a phenomenon that has been well understood when applying the approximation ikoku and ramey 1979 odeh and yang 1979 vongvuthipornchai and raghavan 1987 wen et al 2009 liu et al 2016 the linearization solution with λ 1 5 becomes sufficiently accurate as uf 0 02 for βd 1 5 or uf 5 10 3 for βd 10 50 fig 5 which is practically achievable for example it readily becomes applicable when t 250 s for a pumping test in the flow system with b 10 m l 10 m q 1 10 3 m3 s k 1 5 10 5 m s k 2 1 10 5 m s ss 1 10 5 m 1 and x 10 m βd 5 furthermore figs 4 and 5 show that our proposed model eq 19 with λ 1 5 performs much better than sen s 1987 model with boltzmann transform eqs 20 and 21 which consistently overestimates the drawdown during the whole pumping period for any value of βd as a matter of fact if the dimensionless drawdown for non darcian flow can be solved by the boltzmann transform the solution must be a function of the boltzmann variable wen et al 2009 but sen s solution eq 20 is an integral from x to obviously not a function of the boltzmann variable i e x 2 t which illustrates the defects in solving this non darcian flow problem by boltzmann transform 4 application to field data 4 1 methodology for hydraulic properties estimation as aforementioned for crts in an aquifer with no flow lateral boundaries our proposed 1d flow solution eqs 16 and 19 with λ 1 5 becomes sufficiently accurate at late times if the observation wells or piezometers are placed moderately far from the source i e xd 1 or x l this analytical solution actually represents the distribution and evolution of drawdown as a function of three hydraulic parameters k 1 k 2 and ss which could therefore be estimated from the late time test data fig 7 plots the type curves of our proposed analytical model for 1d non darcian flow βd 1 50 and 1d darcian flow βd 0 as well as the type curve of theis 1935 model for 2d darcian flow as a distinct feature of 1d flow the type curves in the log log plot appear as straight lines with a slope of 0 5 at late times irrespective of the degree of flow nonlinearity but with the increase of βd it takes longer time for the drawdown to reach the h t 0 5 scaling and the drawdown becomes larger at late times and smaller at early times this feature is similar to the type curves of non darcian flow in two dimensional radial formations sen 1988 mathias et al 2008 for crts with a single observation well the aquifer parameters can only be roughly estimated by applying the type curve matching procedure similar to that of the theis 1935 method on condition that the degree of flow nonlinearity is relatively low e g βd 5 this is because under this condition the proposed model already becomes sufficiently accurate before the quasi steady state at which the slope of the type curve becomes 0 5 is attained figs 4a and 4b in more practical cases where the test lasts long enough with two or more observation wells the aquifer parameters k 1 k 2 and ss can be directly determined by fitting the late time solution eq 17 to the late time drawdown measurements with a nonlinear least squares optimization method 4 2 data interpretation of crts at changheba dam foundation in this section the proposed model is applied to interpret the crt data obtained in the changheba rockfill dam foundation chen et al 2016 the dam with a height of 240 m is located on the upper dadu river in kangding county sichuan province china the dam foundation was excavated during september 2011 april 2013 after the construction of the cofferdams was completed in july 2011 and the main and secondary cut off walls together with the grout curtain connecting to the secondary cut off wall were finished at the end of 2012 fig 8 during excavation of the foundation a severe leakage event occurred into the foundation with the total amount of discharge reaching 6000 m3 h in dry seasons and 10 000 12 000 m3 h in raining seasons zhou et al 2015 the crts were therefore performed in the dam foundation during june july 2013 as one of the measures to evaluate this leakage event the aquifer system at the dam site consists of intrusive rocks formed between the jingning and chengjiang periods and quaternary deposits overlying the bedrocks fig 8c the unconsolidated deposits are 60 70 m thick and vary from fluvioglacial to alluvial types from bottom to top these deposits are dense and highly heterogeneous typically containing 10 20 silt and sand 30 50 gravel 20 35 cobble and 5 25 boulder based on 62 laboratory permeability tests and in situ lugeon tests the hydraulic conductivity of the deposits was found to vary in one order of magnitude between 2 0 10 4 and 6 0 10 3 m s zhou et al 2015 the bedrocks are mainly composed of granite and quartz diorite the top of the bedrocks typically with a thickness of 10 30 m is moderately weathered below which the bedrocks are weakly weathered and unaltered the hydraulic conductivity of the moderately weathered rocks was estimated to vary in the range of 3 8 10 6 6 8 10 5 m s from 251 lugeon tests zhou et al 2015 two pumping tests were performed in the dam foundation between the main and secondary cut off walls fig 8b three wells nos 5 6 and 7 r w 78 mm were used for both tests with one as the pumping well well 6 in test 1 and 5 in test 2 and the other two as the observation wells the pumping test conditions are listed in table 1 the aquifer system was not only laterally bounded by the concrete cut off walls 1 4 and 1 2 m thick respectively but also confined because of the construction of a concrete base over the deposits for the clay core of the rockfill dam fig 8c the thickness of the flow zone in the aquifer system was assumed to be the depth of the wells b 30 m chen et al 2016 and the width was the distance between the two cut off walls l 11 m the distance x between the pumping and observation wells was no less than 30 m fig 8b which justifies the condition xd x l 1 therefore we suppose that the proposed 1d flow model applies to these two tests applying the type curve matching procedure to the test data see the inserts in figs 9 a and b we find that for both pumping tests only the data at early times i e t 3000 s for both tests well matches the type curves of 1d flow at late times the test data deviates from the type curves of 1d flow i e the slope of the drawdown versus time curves in a log log plot deviates from 0 5 for both tests instead the semi log plots of the drawdown data at late times i e t 7000 s for both tests exhibit parallel straight lines see figs 9a and b which indicates the flow tends to 2d radial flow in both tests at late times cooper and jacob 1946 barker 1988 liu et al 2017 the flow gradually transitioned from 1d to 2d during the time period between 3000 s and 7000 s the underlying reason for this phenomenon is that the cut off walls were only constructed at a depth onto the top of the moderately weathered bedrocks fig 8c inverse modeling based on site characterization data and field observations zhou et al 2015 showed that the moderately weathered bedrocks have a representative hydraulic conductivity value of 1 77 10 5 m s and hence provided flow paths for the leakage occurring during the excavation of the dam foundation this layer of rocks had negligible effect on the pumping tests at early times but became important as the pumping proceeded leading to the deviation of the flow from 1d behavior using the early time drawdown data 1d flow the hydraulic parameters of the aquifer could be obtained by curve fitting to eq 17 with λ 1 5 for non darcian condition and eq a3 for darcian condition both analytical models agree very well to the early time data r 2 0 99 and the estimated hydraulic parameters from the data of the two tests are listed in table 2 obviously these estimated parameters represent the properties of the unconsolidated deposits since the lateral flow through the weathered bedrocks into the test system did not yet become dominant at this stage as shown in table 2 the darcian solution eq a3 yields a lower estimate of the intrinsic hydraulic conductivity k 1 and a higher estimate of the specific storage coefficient ss compared to the estimates when the non darcian effect is incorporated which has been widely reported in the literature elsworth and doe 1986 quinn et al 2011 2013 ghane et al 2014 2016 chen et al 2015a b liu et al 2017 to quantify the degree of nonlinearity in the flow system we adopt the widely used non darcian effect index defined as the ratio of nonlinear hydraulic gradient to the total hydraulic gradient zeng and grigg 2006 27 e q k 2 2 q k 1 q k 2 2 where e is the non darcian effect index ranging between 0 and 1 it is generally accepted that the non darcian effect cannot be neglected for e 10 zeng and grigg 2006 using the hydraulic parameters interpreted from different observation wells the calculated e value for the tests ranges between 1 82 and 14 50 table 2 this indicates that the non darcian effect of flow in the aquifer system at early times was not dominant but it did occur in certain degree of nonlinearity this does not devalue the propose model because the non darcian effect would immediately become more significant if the pumping rate was further increased the estimated hydraulic conductivity k 1 ranges from 1 18 10 3 to 2 52 10 3 m s table 2 which is within the range 2 10 4 6 10 3 m s obtained by laboratory and field tests implying that the estimated hydraulic properties are representative of the site conditions the late time data from both tests t 7000 s exhibits the feature of 2d radial flow figs 9a and b and the non darcian effect is expected to be even weaker at late times because of lower flow velocity in 2d flow condition therefore the late time drawdown data can be interpreted with the theis 1935 model with the estimated aquifer parameters listed in table 3 the results show that the estimated hydraulic conductivity k 1 lies in 1 61 2 32 10 5 m s for the two tests obviously these k 1 values are out of the range of the deposits hydraulic conductivity 2 10 4 6 10 3 m s obtained by laboratory and field tests and more likely represent the hydraulic properties of the moderately weathered bedrocks underlying the deposits whose hydraulic conductivity was found to vary between 3 8 10 6 and 6 8 10 5 m s by lugeon tests and to possess a representative value of 1 77 10 5 m s by inverse modeling zhou et al 2015 this further confirms that the lateral flow through the weathered bedrocks played a dominant role in the pumping tests at late times and in the leakage occurring at the dam foundation fig 8c it should be noted that as a limitation similar to other interpretive models homogeneity assumption is made in the development of the proposed model the heterogeneity of the aquifer system and the scaling effect of the hydraulic properties can not be directly represented by the analytical model but can be evaluated using the observation wells at different locations leveinen 2000 le borgne et al 2004 the differences in hydraulic properties interpreted from different tests and different observations in tables 2 and 3 roughly indicate the heterogeneity of the aquifer system 5 conclusions this study presented a 1d non darcian flow solution for constant rate pumping tests in confined aquifers bounded by no flow lateral boundaries commonly envisaged in fracture zones enhanced geothermal reservoirs and civil engineering with cut off walls the flow system was assumed to be one dimensional and follow the forchheimer s law a general linearization approximation strategy was suggested for the forchheimer equation and an analytical model was then proposed by laplace transform two dimensional numerical simulations prove that the linearization approximation performs best when the constant λ takes its mean in the reasonable range λ 1 5 and the analytical model is sufficiently accurate at late times or at observation wells located moderately far from the source the proposed model performs better than sen s 1987 model developed with boltzmann transform which tends to overestimate the drawdown during the whole test period the pumping tests performed at the changheba dam foundation bounded by two cut off walls in southwest china were interpreted with the proposed model showing that the early time data matches well the type curve of 1d flow model while the late time data exhibits a 2d radial flow pattern due to the lateral flow in high permeability weathered crystalline rocks underlying the unconsolidated deposits it is interestingly shown that the aquifer parameters interpreted with the 1d flow model from the early time data and with the 2d radial flow model from the late time data well represent the aquifer properties of the unconsolidated deposits and the weathered rocks respectively acknowledgments this work is supported by the national key r d program of china no 2018yfc0407001 and the national natural science foundation of china no 51579188 appendix a for crts in a confined aquifer with no flow lateral boundaries under darcian flow conditions fig a1 the well image theory is widely used to transform the finite flow system into an aquifer of infinite areal extent the drawdown of an observation well can be represented by the sum of drawdown values calculated by theis 1935 equation from the pumping well located at the center of the finite aquifer and the image wells located across the boundaries a1 h x y t q 4 π k 1 b w s s 4 k 1 t x 2 y 2 n 1 2 3 w s s 4 k 1 t n l y 2 x 2 n 1 2 3 w s s 4 k 1 t n l y 2 x 2 where w u is the theis well function or exponential integral function k 1 is the hydraulic conductivity of the formation ss is the specific storage coefficient h is the drawdown of the observation well b is the thickness of the confined aquifer l is the distance between the two impermeable lateral boundaries x and y are the coordinates of the observation well parallel and perpendicular to the boundaries and t is time using eq 26 eq a1 can be rewritten in its dimensionless form as a2 h d x d y d t d 1 2 π b d w x d 2 y d 2 t d n 1 2 3 w x d 2 n y d 2 t d n 1 2 3 w x d 2 n y d 2 t d the 1d flow solution on the other hand provides an alternative way for simplification of the problem which reads a3a h x t q x 2 k 1 b l e u d π u d e r f c u d a3b u d s s x 2 4 k 1 t where ud is a dimensionless parameter the dimensionless form of eq a3 reads a4a h d x d t d x d b d e u d π u d e r f c u d a4b u d x d 2 t d fig a2 plots a comparison between the dimensionless drawdown curves calculated by the image method eq a2 and the 1d flow solution eq a4 at different coordinates xd 0 5 10 yd 0 0 5 with bd 0 1 10 for different aquifer thickness bd the plots show that the 1d flow solution agrees perfectly with the results obtained by the image method at late times if the observation wells are located close to the source e g xd 1 or throughout the tests for larger x coordinates e g xd 1 furthermore the drawdown response becomes independent of the y coordinate when the observation wells are located moderately far from the source e g xd 1 this implies that for this kind of flow system the 1d darcian flow solution becomes adequately accurate at moderately distant locations or at late times which motivates us to develop a 1d non darcian analytical solution for this flow system 
650,aquifers with quasi linear flow pattern are frequently envisaged in fractured zones in oil gas or enhanced geothermal reservoirs or in civil engineering where cut off walls are constructed the water flow towards a well in this linear aquifer system has been long investigated under darcian flow condition but remains an open issue for non darcian flow in this study a general linearization approximation strategy is suggested for the forchheimer equation and an analytical solution is proposed by using laplace transform for non darcian flow towards a well in aquifers laterally bounded by no flow barriers numerical simulations using the finite volume method prove that the linearization approximation performs best when it takes the mean of two commonly used strategies and the analytical model is sufficiently accurate at late times for observation wells located moderately far from the source the proposed model was applied to data interpretation of the pumping tests at the changheba dam foundation bounded by two cut off walls in southwest china where the drawdown curves can be divided into 1d flow transitional flow and 2d flow stages as a result of lateral flow through weathered bedrocks at late times the proposed model provides a valuable tool for characterizing the hydraulic properties of aquifers and reservoirs with a linear flow pattern and for assessing the possible leakage through the lateral barriers by type curve matching keywords analytical solution linear flow pattern forchheimer flow constant rate test 1 introduction the groundwater flow towards a well may exhibit features of linear flow pattern i e all the flow lines are straight and parallel towards the source when an aquifer is bounded by parallel linear no flow boundaries walker and roberts 2003 this flow pattern frequently occurs in fracture zones having much higher permeability than the surrounding rocks gringarten et al 1974 jenkins and prentice 1982 sen 1986 1987 wen et al 2006 in aquifers laterally bounded by natural ehlig economides and economides 1985 or artificial barriers e g concrete cut off walls in civil engineering wu et al 2015 wang et al 2017 in oil or gas reservoirs having a predominantly long narrow shape miller 1962 nutakki and matter 1982 el banbi and wattenbarger 1998 escobar et al 2007 and in hydraulically fractured wells in shale gas and enhanced geothermal reservoirs wattenbarger et al 1998 ladner and haring 2009 ahmadi et al 2010 nobakht and clarkson 2012 behmanesh et al 2018 therefore the linear flow in aquifers has gained interests for decades not only in groundwater modeling and management but also in oil gas and geothermal production in the sense of flow dimensions barker 1988 chang and yortsos 1990 walker and roberts 2003 le borgne et al 2004 walker et al 2006 liu et al 2016 2017 the linear flow is also called one dimensional 1d flow on the condition that the darcy s law applies analytical drawdown solutions have been developed for 1d transient flow towards a well pumped at constant flow rates using variable separation method ferris et al 1950 laplace transform miller 1962 or fourier transform jenkins and prentice 1982 a generalized radial flow model was also proposed by barker 1988 for constant rate tests in aquifers with a non integer flow dimension n 1 n 3 which reduces to miller s 1962 1d flow model for n 1 an alternative method to solve this problem is to transform the aquifer of finite extent into one of seemingly infinite extent by introducing image wells and superposing the theis 1935 solutions ferris et al 1962 kruseman and ridder 1990 but with the increase of flow velocity or reynolds number re in the formations the flow behavior may deviate from the darcy s equation due to non negligible inertial losses forchheimer 1901 izbash 1931 bear 1979 venkataraman and rao 1998 bordier and zimmer 2000 chen et al 2015a b c and hence interpretive models were further developed for 1d non darcian flow for instance wen et al 2006 developed an izbash s 1931 law based model for non darcian flow in a single confined vertical fracture towards a well by using boltzmann transform liu et al 2016 proposed an izbash s law based generalized radial flow model 1 n 3 by means of laplace transform and linearization approximation which applies to 1d non darcian flow for n 1 the major limitation with these models is that the exponent in the izbash s law was treated as a constant between 1 and 2 which becomes inaccurate to describe the laminar flow when the flow velocity or re becomes small liu et al 2016 2017 to overcome this limitation interpretive models were also developed based on the forchheimer s 1901 law which has rigorous theoretical background and clearer physical meaning irmay 1958 hassanizadeh and gray 1987 chen et al 2001 sen 1987 presented a forchheimer s law based analytical solution for the extended well tests in fractured formations by boltzmann transform method but later on this method was found to be problematic in a rigorous mathematical sense camacho v and vasquez c 1992 wen et al 2008 2009 moutsopoulos and tsihrintzis 2005 derived approximate similarity solutions for 1d transient forchheimer flow with a head rise but no external sink source terms were considered liu et al 2017 proposed an analytical model for forchheimer flow in generalized radial formations 1 n 3 based on the concept of drawdown or buildup decomposition but this model becomes less accurate when the flow dimension is smaller than 1 5 therefore seeking an accurate analytical model for non darcian flow in a linear flow system remains an open issue in this study the goal is to develop an analytical solution for forchheimer flow towards a well in a linear aquifer system laterally bounded by no flow boundaries where the flow eventually becomes one dimensional at late times or at locations moderately far away from the source a general linearization approximation strategy is suggested for the forchheimer s 1901 equation and an analytical model is established through laplace transform which is represented as a function of time and the distance from the source and can be readily reduced to miller s 1962 model for 1d darcian flow two dimensional numerical simulations based on the finite volume method are conducted to calibrate the constant in the linearization approximation and to validate the accuracy of the analytical model the proposed model is applied to data interpretation of the pumping tests performed at the changheba dam foundation between two cut off walls in southwest china chen et al 2016 the proposed model provides a tool for characterizing the hydraulic properties of aquifers with a linear flow pattern and for assessing the possible leakage through the lateral barriers by type curve matching 2 model development consider a constant rate test crt performed in a confined aquifer with no flow lateral boundaries fig 1 a where the pumping well of radius r w is located at the center of the aquifer the thickness of the aquifer is b and the width between the two impermeable lateral boundaries is l this is essentially a bounded two dimensional problem and a proper simplification is needed to obtain the analytical solutions under darcian flow conditions analytical solutions are available for this flow system by applying the well image theory e g ferris et al 1962 nutakki and matter 1982 kruseman and ridder 1990 dewandel et al 2014 or by assuming 1d flow geometry fig 1b ferris et al 1950 miller 1962 jenkins and prentice 1982 a comparison between these two types of solutions appendix a shows that the 1d flow solution of drawdown becomes sufficiently accurate at late times or at locations moderately far away from the source e g x l where x is the coordinate along the flow direction indicating that this 2d problem fig 1a can be reasonably approximated by the 1d flow system represented in fig 1b under these conditions therefore this study focuses on development of a 1d flow model for this flow system fig 1b under non darcian flow conditions where the principle of superposition is no longer valid and hence the well image method does not apply mathias and todman 2010 mijic et al 2013 the continuity equation of the 1d flow system fig 1b can be written as sen 1987 wen et al 2006 liu et al 2016 2017 1 s s h t q x where h is the drawdown q is the specific discharge ss is the specific storage coefficient x is the coordinate along the flow direction and t is time the initial and boundary conditions are given by 2a h x 0 0 2b h t 0 2c lim x 0 2 b l q q where q is the constant volumetric rate pumped from the source at x 0 note that in this study the wellbore storage is not considered i e the well radius rw is assumed to be infinitesimal the water flow in the formation is assumed to obey the forchheimer s 1901 law 3 h x q k 1 q k 2 2 where k 1 is the intrinsic hydraulic conductivity of the formation quantified by the inverse of the initial slope of the h x q curve and k 2 is the inertial hydraulic conductivity representing the degree of deviation from linearity as q increases liu et al 2017 zhou et al 2019 obviously eq 3 reduces to darcy s law as k 2 or q becomes small enough it should be noted that under transient flow conditions the right hand side of eq 3 contains a rate dependent term of q irmay 1958 but as clarified by irmay 1958 this term is unimportant and takes effect only initially during a fraction of a second and it can be neglected for reducing the uncertainties in parameterization substituting eq 3 to eq 1 yields a non linear governing equation hard to be analytically solved wen et al 2008 liu et al 2016 to overcome this difficulty linearization approximation is commonly adopted ikoku and ramey 1979 odeh and yang 1979 wen et al 2008 liu et al 2016 which replaces the specific discharge at any location x in the nonlinear term with q q 2bl here it is interesting to note that there are two strategies to apply this linearization approximation the first one is to directly modify the quadratic term in the forchheimer s eq 3 which is equivalent to replacing the intrinsic hydraulic conductivity k 1 with an apparent hydraulic conductivity k a accounting for the inertial effects choi et al 1997 mijic et al 2013 4a h x q 1 k 1 q k 2 2 q 1 k 1 q 2 b l k 2 2 2 h x 2 1 k 1 q 2 b l k 2 2 q x the second strategy is to use this approximation after taking partial derivatives of eq 3 with respect to x which is similar to the procedure adopted by wen et al 2008 and liu et al 2016 for linearizing the non linear term in the izbash s 1931 law 4b h x q k 1 q k 2 2 2 h x 2 1 k 1 2 q k 2 2 q x 1 k 1 q b l k 2 2 q x obviously eq 4b estimates an inertial effect two times higher than eq 4a therefore a general linearization approximation for the forchheimer s law can be suggested as 5 2 h x 2 1 k 1 q λ b l k 2 2 q x where λ is a constant that can be calibrated by numerical solutions or crt data according to eq 4 a reasonable range of λ is between 1 and 2 more discussion on the range and the best estimate of λ will be presented later in section 3 2 substituting eq 5 into eq 1 yields 6a 2 h x 2 a h t 6b a s s 1 k 1 q λ b l k 2 2 eq 6a is a linear partial differential equation which can be solved by the laplace transform 7 2 h x 2 a p h 0 where p is the laplace variable and h is the laplace transform of h accordingly the boundary conditions eqs 2b and 2c are transformed to 8a h p 0 8b lim x 0 h x 1 p q 2 b l k 1 q 2 b l k 2 2 note that eq 7 is a form of the bessel s equation and its general solution reads ikoku and ramey 1979 wen et al 2008 liu et al 2016 9 h x p x 0 5 c 1 i 0 5 a p x c 2 k 0 5 a p x where iν z and kν z are the first and second kind modified bessel functions respectively and c 1 and c 2 are two integration constants dependent on the boundary conditions the far field boundary condition in eq 8a implies c 1 0 and hence eq 9 reduces to 10 h x p c 2 x 0 5 k 0 5 a p x recalling the properties of kν z zk ν z zk ν z zk ν 1 z and k ν z k ν z we obtain the derivative of h with respect to x 11 h x a p c 2 x 0 5 k 0 5 a p x the constant c 2 is then determined by using the properties of kν z lim z 0 ν 0 k ν z γ ν 2 z 2 ν and substituting eq 8b to eq 11 12 c 2 2 q 2 b l k 1 q 2 b l k 2 2 π a 0 5 p 1 25 substituting eq 12 into eq 10 yields the drawdown in the laplace domain h 13 h x p f p 1 25 k 0 5 a p where f and a are two p independent parameters 14 f 2 q 2 b l k 1 q 2 b l k 2 2 π a 0 5 x 0 5 a a x eq 13 can be solved by the analytical laplace inversion method 15 h x t f 1 2 a 2 0 5 γ 0 5 a 2 4 t where γ v u is the complementary incomplete gamma function substituting eq 14 into eq 15 and using the property of the gamma function γ 0 5 u 2 e x u π e r f c u we obtain the final expression of the 1d non darcian flow model 16a h x t x q 2 b l k 1 q 2 b l k 2 2 e u π u e r f c u 16b u a x 2 4 t where u is a dimensionless parameter and erfc u is the complementary error function e r f c u 1 2 π 0 u e η 2 d η the proposed model eq 16 immediately reduces to the 1d darcian flow model appendix a as the inertial effect becomes negligible i e k 2 the late time analytical solution is of particular concern for crt data interpretation because the late time solution normally becomes accurate enough after a certain time of pumping wu 2002a b liu et al 2016 2017 the late time solution can be obtained by taking the limit of eq 16 as t 17 h x t q 2 b l k 1 q 2 b l k 2 2 2 π a t x it can be inferred from eq 17 that the hydraulic gradient h x given in eq 3 at late times t becomes time independent or in other words h x tends to be at a quasi steady state and the discharge across any cross section becomes q q 2bl at late times this favorable property has also been revealed in the analytical models by liu et al 2016 2017 furthermore it is interesting to observe from eq 17 that the drawdown h is proportional to t 0 5 at late times which gives a slope of 0 5 in the log log plot of drawdown versus time this is actually a common behavior of 1d flow miller 1962 jenkins and prentice 1982 barker 1988 leveinen 2000 le borgne et al 2004 by adopting the following dimensionless transformations 18 u f s s x 2 4 k 1 t w u f 2 b l k 1 q x h x t β d q k 1 b l k 2 2 where uf is the time factor w uf is the non darcian well function and βd is a dimensionless variable reflecting the degree of nonlinearity the linearization solution eq 16 can be rewritten in dimensionless form as 19a w u f 1 β d 2 e u u π e r f c u 19b u 1 β d λ u f here again eq 19 reduces to the darcian flow solution appendix a when βd 0 for comparison we recall sen s 1987 model established based on the forchheimer s equation using the boltzmann transform for the extended well problem in fractured formations 20 h x t q 2 b l k 1 q 2 b l k 2 2 x e u f 1 q k 1 2 b l k 2 2 1 e u f 2 d x the dimensionless form of sen s model is given by 21 w u f 2 β d 4 u f u f e η η 1 β d 2 1 e η 2 d η 3 numerical validation the development of the proposed model involves two approximations one is the neglect of the radial well in fig 1a and the other is the linearization approximation given in eq 5 it remains unclear however what uncertainties could result from these approximations and how the constant λ should be parameterized in this section we address these issues by numerical simulations numerical models have been developed for forchheimer flow to a well using finite difference method choi et al 1997 ewing and lin 2001 wu 2002a b mathias et al 2008 mijic et al 2013 liu et al 2017 finite element method ewing et al 1999 kolditz 2001 zhang and xing 2012 chen et al 2015b and finite volume method ewing and lin 2001 moutsopoulos and tsihrintzis 2005 for the two dimensional model with no flow boundaries in this study fig 1a we use the finite volume method for its good conservation and high adaptability to unstructured grids 3 1 numerical setup a quarter of the domain with an infinite boundary xe 106 m along the x axis fig 2 is used for numerical simulations by utilizing the symmetry conditions of the flow problem fig 1a quadrilateral grids are adopted to discretize the domain with curved cells around the pumping well fig 2 to ensure high resolution near the well the grid points are logarithmically spaced away from the source the domain is discretized into 500 and 100 segments along the x and y directions respectively which results in a total of 50 000 grid cells the boundaries at x 0 y 0 and y l 2 are no flow boundaries the infinite boundary at x xe is a zero drawdown boundary and the well screen is a discharge boundary since the well storage is not considered the well screen radius rw is set small enough rw 10 3 m to reduce numerical errors the time scale is also logarithmically discretized to allow for the rapid head change at early times the head and velocity at each cell control volume are represented by the node at the center of the cell and the head change of the cell is contributed to the discharge from the four neighboring cells the continuity equation for an individual cell i is given by 22 s s a i δ h i δ t j 1 4 s i g n h i j h i q i j l i j s i n θ i j where ai is the area of the cell hi is the drawdown of the cell δhi is the head change of the cell δt is the time interval qij j 1 2 3 4 is the discharge contributed by the neighboring cells lij is the length of the connection with the neighboring cells fig 2 hij j 1 2 3 4 is the drawdown of the neighboring cells and θij is the angle between qij and lij note that θij is not always equal to 90 due to the use of curved cells near the pumping well the forchheimer s law eq 3 can be rewritten in the following discretized form 23a q i j 1 f i h i j h i r i j 23b f i 1 k 1 q i k 2 2 23c q i q i 1 s i n θ i 1 q i 3 s i n θ i 3 2 2 q i 2 s i n θ i 2 q i 4 s i n θ i 4 2 2 where rij is the distance between the centers of cell i and its neighboring cell j j 1 2 3 4 qi is the magnitude of the flux at cell i and fi is a nonlinear factor dependent on qi choi et al 1997 mijic et al 2013 substituting eq 23 into eq 22 yields the implicit difference form for any inner cell i 24a h i h i 0 δ t j 1 4 s i g n h i j h i 1 f i l i j s i n θ i j h i j h i s s a i r i j where h i 0 is the initial drawdown at a time step for the cells on the discharge boundary eq 24a becomes 24b h i h i 0 δ t q 4 s s a i n b j 2 4 s i g n h i j h i 1 f i l i j sin θ i j h i j h i s s a i r i j where q is the total pumping rate b is the thickness of the aquifer and n is the number of segments on the discharge boundary n 20 in this simulation no flow or infinity boundaries are implemented using image nodes outside the boundary denoting the drawdown by hie one has hie hi 0 on the no flow boundaries and hie 0 on the infinity boundary we develop a finite volume code in matlab in which eq 24 is integrated by the stiff integrator ode15s available in matlab which has been successfully applied to numerical simulations for non darcian flow mathias et al 2008 wen et al 2008 the nonlinear equation eq 24 can be effectively solved with an iterative procedure mijic et al 2013 chen et al 2015b by initially assigning 1 k 1 to fi and obtaining the initial head distribution in darcian flow condition the flux qi is calculated by eq 23a and eq 23c using the known head values at the previous iteration and then by updating the nonlinear factor fi the head distribution is re calculated in the next iteration this iteration procedure above is repeated at each time step until the difference between the results in two consecutive iterations satisfies the following convergence criterion 25 h m 1 h m 2 δ h m 2 where h is the vector of nodal drawdown m is the iteration step 2 denotes the euclidean norm of a vector and δ is a user specified error tolerance δ 1 10 5 in this study 3 2 numerical results for ease of presentation we introduce the following dimensionless transformations 26 x d x l b d b l y d y l t d 4 k 1 t s s l 2 h d 2 k 1 l q h x y t fig 3 plots the numerical solutions with xd 1 and βd 10 at various coordinates xd 0 5 10 and yd 0 0 5 fig 3a shows that the numerical solutions hd versus td become independent of the y coordinate when the observations wells are located moderately far away from the source e g xd 1 which can be easily satisfied in field conditions or at late times for xd 1 fig 3b shows that along the x axis yd 0 the non darcian well function w uf eq 18 converges to a single curve when 1 uf 10 which indicates again that the flow system exhibits the feature of one dimensional flow for xd 1 at late times to eliminate the error caused by the presence of well we use the numerical results at xd 1 and yd 0 in the following for further validating the proposed linearization solutions fig 4 plots a comparison between the numerical results and the analytical solutions eq 19 for one dimensional non darcian flow with increasing degree of nonlinearity i e βd increases from 1 to 50 fig 5 shows a semi log plot of the relative error ε defined as the radio of the difference between the analytical and numerical solutions over the numerical results against the dimensionless time 1 uf for comparison the analytical solutions by sen s 1987 model eq 21 are also plotted fig 4 shows that all the 2d numerical and 1d analytical solutions become parallel straight lines with a slope of 0 5 in a log log scale at late times which further justifies the one dimensional simplification for the constant rate test in a stripped aquifer bounded by two parallel barriers the value of λ has a significant impact on the accuracy of the linearization solution eq 19 especially for larger βd compared to the numerical solutions the value of λ 1 underestimates the drawdown at late times whereas the value of λ 2 yields an overestimation with their relative errors at late times ranging 5 13 12 26 15 20 and 18 29 for βd 1 5 10 and 50 respectively fig 5 a λ value out of the range between 1 and 2 e g 0 5 and 5 results in greater errors which justifies the range of λ suggested by eq 5 but interestingly we find that the mean value of λ in its reasonable range i e λ 1 5 yields a perfect match between the analytical and numerical solutions at late times with a relative error below 0 7 2 0 at late times for any βd between 1 and 50 fig 5 fig 6 further plots the variation of error at late times the error becomes stable at late times fig 5 with λ evidencing that the error is minimized in the vicinity of λ 1 5 for varying degree of nonlinearity therefore eq 5 with λ 1 5 provides a quite reasonable linearization approximation strategy for forchheimer flow with linear flow pattern at early times however the linearization solutions for any values of λ between 1 and 2 underestimate the drawdown and this discrepancy increases with increasing degree of nonlinearity fig 4 a phenomenon that has been well understood when applying the approximation ikoku and ramey 1979 odeh and yang 1979 vongvuthipornchai and raghavan 1987 wen et al 2009 liu et al 2016 the linearization solution with λ 1 5 becomes sufficiently accurate as uf 0 02 for βd 1 5 or uf 5 10 3 for βd 10 50 fig 5 which is practically achievable for example it readily becomes applicable when t 250 s for a pumping test in the flow system with b 10 m l 10 m q 1 10 3 m3 s k 1 5 10 5 m s k 2 1 10 5 m s ss 1 10 5 m 1 and x 10 m βd 5 furthermore figs 4 and 5 show that our proposed model eq 19 with λ 1 5 performs much better than sen s 1987 model with boltzmann transform eqs 20 and 21 which consistently overestimates the drawdown during the whole pumping period for any value of βd as a matter of fact if the dimensionless drawdown for non darcian flow can be solved by the boltzmann transform the solution must be a function of the boltzmann variable wen et al 2009 but sen s solution eq 20 is an integral from x to obviously not a function of the boltzmann variable i e x 2 t which illustrates the defects in solving this non darcian flow problem by boltzmann transform 4 application to field data 4 1 methodology for hydraulic properties estimation as aforementioned for crts in an aquifer with no flow lateral boundaries our proposed 1d flow solution eqs 16 and 19 with λ 1 5 becomes sufficiently accurate at late times if the observation wells or piezometers are placed moderately far from the source i e xd 1 or x l this analytical solution actually represents the distribution and evolution of drawdown as a function of three hydraulic parameters k 1 k 2 and ss which could therefore be estimated from the late time test data fig 7 plots the type curves of our proposed analytical model for 1d non darcian flow βd 1 50 and 1d darcian flow βd 0 as well as the type curve of theis 1935 model for 2d darcian flow as a distinct feature of 1d flow the type curves in the log log plot appear as straight lines with a slope of 0 5 at late times irrespective of the degree of flow nonlinearity but with the increase of βd it takes longer time for the drawdown to reach the h t 0 5 scaling and the drawdown becomes larger at late times and smaller at early times this feature is similar to the type curves of non darcian flow in two dimensional radial formations sen 1988 mathias et al 2008 for crts with a single observation well the aquifer parameters can only be roughly estimated by applying the type curve matching procedure similar to that of the theis 1935 method on condition that the degree of flow nonlinearity is relatively low e g βd 5 this is because under this condition the proposed model already becomes sufficiently accurate before the quasi steady state at which the slope of the type curve becomes 0 5 is attained figs 4a and 4b in more practical cases where the test lasts long enough with two or more observation wells the aquifer parameters k 1 k 2 and ss can be directly determined by fitting the late time solution eq 17 to the late time drawdown measurements with a nonlinear least squares optimization method 4 2 data interpretation of crts at changheba dam foundation in this section the proposed model is applied to interpret the crt data obtained in the changheba rockfill dam foundation chen et al 2016 the dam with a height of 240 m is located on the upper dadu river in kangding county sichuan province china the dam foundation was excavated during september 2011 april 2013 after the construction of the cofferdams was completed in july 2011 and the main and secondary cut off walls together with the grout curtain connecting to the secondary cut off wall were finished at the end of 2012 fig 8 during excavation of the foundation a severe leakage event occurred into the foundation with the total amount of discharge reaching 6000 m3 h in dry seasons and 10 000 12 000 m3 h in raining seasons zhou et al 2015 the crts were therefore performed in the dam foundation during june july 2013 as one of the measures to evaluate this leakage event the aquifer system at the dam site consists of intrusive rocks formed between the jingning and chengjiang periods and quaternary deposits overlying the bedrocks fig 8c the unconsolidated deposits are 60 70 m thick and vary from fluvioglacial to alluvial types from bottom to top these deposits are dense and highly heterogeneous typically containing 10 20 silt and sand 30 50 gravel 20 35 cobble and 5 25 boulder based on 62 laboratory permeability tests and in situ lugeon tests the hydraulic conductivity of the deposits was found to vary in one order of magnitude between 2 0 10 4 and 6 0 10 3 m s zhou et al 2015 the bedrocks are mainly composed of granite and quartz diorite the top of the bedrocks typically with a thickness of 10 30 m is moderately weathered below which the bedrocks are weakly weathered and unaltered the hydraulic conductivity of the moderately weathered rocks was estimated to vary in the range of 3 8 10 6 6 8 10 5 m s from 251 lugeon tests zhou et al 2015 two pumping tests were performed in the dam foundation between the main and secondary cut off walls fig 8b three wells nos 5 6 and 7 r w 78 mm were used for both tests with one as the pumping well well 6 in test 1 and 5 in test 2 and the other two as the observation wells the pumping test conditions are listed in table 1 the aquifer system was not only laterally bounded by the concrete cut off walls 1 4 and 1 2 m thick respectively but also confined because of the construction of a concrete base over the deposits for the clay core of the rockfill dam fig 8c the thickness of the flow zone in the aquifer system was assumed to be the depth of the wells b 30 m chen et al 2016 and the width was the distance between the two cut off walls l 11 m the distance x between the pumping and observation wells was no less than 30 m fig 8b which justifies the condition xd x l 1 therefore we suppose that the proposed 1d flow model applies to these two tests applying the type curve matching procedure to the test data see the inserts in figs 9 a and b we find that for both pumping tests only the data at early times i e t 3000 s for both tests well matches the type curves of 1d flow at late times the test data deviates from the type curves of 1d flow i e the slope of the drawdown versus time curves in a log log plot deviates from 0 5 for both tests instead the semi log plots of the drawdown data at late times i e t 7000 s for both tests exhibit parallel straight lines see figs 9a and b which indicates the flow tends to 2d radial flow in both tests at late times cooper and jacob 1946 barker 1988 liu et al 2017 the flow gradually transitioned from 1d to 2d during the time period between 3000 s and 7000 s the underlying reason for this phenomenon is that the cut off walls were only constructed at a depth onto the top of the moderately weathered bedrocks fig 8c inverse modeling based on site characterization data and field observations zhou et al 2015 showed that the moderately weathered bedrocks have a representative hydraulic conductivity value of 1 77 10 5 m s and hence provided flow paths for the leakage occurring during the excavation of the dam foundation this layer of rocks had negligible effect on the pumping tests at early times but became important as the pumping proceeded leading to the deviation of the flow from 1d behavior using the early time drawdown data 1d flow the hydraulic parameters of the aquifer could be obtained by curve fitting to eq 17 with λ 1 5 for non darcian condition and eq a3 for darcian condition both analytical models agree very well to the early time data r 2 0 99 and the estimated hydraulic parameters from the data of the two tests are listed in table 2 obviously these estimated parameters represent the properties of the unconsolidated deposits since the lateral flow through the weathered bedrocks into the test system did not yet become dominant at this stage as shown in table 2 the darcian solution eq a3 yields a lower estimate of the intrinsic hydraulic conductivity k 1 and a higher estimate of the specific storage coefficient ss compared to the estimates when the non darcian effect is incorporated which has been widely reported in the literature elsworth and doe 1986 quinn et al 2011 2013 ghane et al 2014 2016 chen et al 2015a b liu et al 2017 to quantify the degree of nonlinearity in the flow system we adopt the widely used non darcian effect index defined as the ratio of nonlinear hydraulic gradient to the total hydraulic gradient zeng and grigg 2006 27 e q k 2 2 q k 1 q k 2 2 where e is the non darcian effect index ranging between 0 and 1 it is generally accepted that the non darcian effect cannot be neglected for e 10 zeng and grigg 2006 using the hydraulic parameters interpreted from different observation wells the calculated e value for the tests ranges between 1 82 and 14 50 table 2 this indicates that the non darcian effect of flow in the aquifer system at early times was not dominant but it did occur in certain degree of nonlinearity this does not devalue the propose model because the non darcian effect would immediately become more significant if the pumping rate was further increased the estimated hydraulic conductivity k 1 ranges from 1 18 10 3 to 2 52 10 3 m s table 2 which is within the range 2 10 4 6 10 3 m s obtained by laboratory and field tests implying that the estimated hydraulic properties are representative of the site conditions the late time data from both tests t 7000 s exhibits the feature of 2d radial flow figs 9a and b and the non darcian effect is expected to be even weaker at late times because of lower flow velocity in 2d flow condition therefore the late time drawdown data can be interpreted with the theis 1935 model with the estimated aquifer parameters listed in table 3 the results show that the estimated hydraulic conductivity k 1 lies in 1 61 2 32 10 5 m s for the two tests obviously these k 1 values are out of the range of the deposits hydraulic conductivity 2 10 4 6 10 3 m s obtained by laboratory and field tests and more likely represent the hydraulic properties of the moderately weathered bedrocks underlying the deposits whose hydraulic conductivity was found to vary between 3 8 10 6 and 6 8 10 5 m s by lugeon tests and to possess a representative value of 1 77 10 5 m s by inverse modeling zhou et al 2015 this further confirms that the lateral flow through the weathered bedrocks played a dominant role in the pumping tests at late times and in the leakage occurring at the dam foundation fig 8c it should be noted that as a limitation similar to other interpretive models homogeneity assumption is made in the development of the proposed model the heterogeneity of the aquifer system and the scaling effect of the hydraulic properties can not be directly represented by the analytical model but can be evaluated using the observation wells at different locations leveinen 2000 le borgne et al 2004 the differences in hydraulic properties interpreted from different tests and different observations in tables 2 and 3 roughly indicate the heterogeneity of the aquifer system 5 conclusions this study presented a 1d non darcian flow solution for constant rate pumping tests in confined aquifers bounded by no flow lateral boundaries commonly envisaged in fracture zones enhanced geothermal reservoirs and civil engineering with cut off walls the flow system was assumed to be one dimensional and follow the forchheimer s law a general linearization approximation strategy was suggested for the forchheimer equation and an analytical model was then proposed by laplace transform two dimensional numerical simulations prove that the linearization approximation performs best when the constant λ takes its mean in the reasonable range λ 1 5 and the analytical model is sufficiently accurate at late times or at observation wells located moderately far from the source the proposed model performs better than sen s 1987 model developed with boltzmann transform which tends to overestimate the drawdown during the whole test period the pumping tests performed at the changheba dam foundation bounded by two cut off walls in southwest china were interpreted with the proposed model showing that the early time data matches well the type curve of 1d flow model while the late time data exhibits a 2d radial flow pattern due to the lateral flow in high permeability weathered crystalline rocks underlying the unconsolidated deposits it is interestingly shown that the aquifer parameters interpreted with the 1d flow model from the early time data and with the 2d radial flow model from the late time data well represent the aquifer properties of the unconsolidated deposits and the weathered rocks respectively acknowledgments this work is supported by the national key r d program of china no 2018yfc0407001 and the national natural science foundation of china no 51579188 appendix a for crts in a confined aquifer with no flow lateral boundaries under darcian flow conditions fig a1 the well image theory is widely used to transform the finite flow system into an aquifer of infinite areal extent the drawdown of an observation well can be represented by the sum of drawdown values calculated by theis 1935 equation from the pumping well located at the center of the finite aquifer and the image wells located across the boundaries a1 h x y t q 4 π k 1 b w s s 4 k 1 t x 2 y 2 n 1 2 3 w s s 4 k 1 t n l y 2 x 2 n 1 2 3 w s s 4 k 1 t n l y 2 x 2 where w u is the theis well function or exponential integral function k 1 is the hydraulic conductivity of the formation ss is the specific storage coefficient h is the drawdown of the observation well b is the thickness of the confined aquifer l is the distance between the two impermeable lateral boundaries x and y are the coordinates of the observation well parallel and perpendicular to the boundaries and t is time using eq 26 eq a1 can be rewritten in its dimensionless form as a2 h d x d y d t d 1 2 π b d w x d 2 y d 2 t d n 1 2 3 w x d 2 n y d 2 t d n 1 2 3 w x d 2 n y d 2 t d the 1d flow solution on the other hand provides an alternative way for simplification of the problem which reads a3a h x t q x 2 k 1 b l e u d π u d e r f c u d a3b u d s s x 2 4 k 1 t where ud is a dimensionless parameter the dimensionless form of eq a3 reads a4a h d x d t d x d b d e u d π u d e r f c u d a4b u d x d 2 t d fig a2 plots a comparison between the dimensionless drawdown curves calculated by the image method eq a2 and the 1d flow solution eq a4 at different coordinates xd 0 5 10 yd 0 0 5 with bd 0 1 10 for different aquifer thickness bd the plots show that the 1d flow solution agrees perfectly with the results obtained by the image method at late times if the observation wells are located close to the source e g xd 1 or throughout the tests for larger x coordinates e g xd 1 furthermore the drawdown response becomes independent of the y coordinate when the observation wells are located moderately far from the source e g xd 1 this implies that for this kind of flow system the 1d darcian flow solution becomes adequately accurate at moderately distant locations or at late times which motivates us to develop a 1d non darcian analytical solution for this flow system 
651,despite numerous advances in continental scale hydrologic modeling and improvements in global land surface models an accurate representation of regional water table depth wtd remains a challenge data assimilation of observations from the gravity recovery and climate experiment grace mission leads to improvements in the accuracy of hydrologic models ultimately resulting in more reliable estimates of lumped water storage however the usually shallow groundwater compartment of many models presents a problem with grace assimilation techniques as these satellite observations also represent changes in deeper soils and aquifers to improve the accuracy of modeled groundwater estimates and allow the representation of wtd at finer spatial scales we implemented a simple yet novel approach to integrate grace data by augmenting the variable infiltration capacity vic hydrologic model first the subsurface model structural representation was modified by incorporating an additional fourth soil layer of varying depth up to 1000 m in vic as the bottom groundwater layer this addition allows the model to reproduce water storage variability not only in shallow soils but also in deeper groundwater in order to allow integration of the full grace observed variability second a direct insertion scheme was developed that integrates the high temporal daily and spatial 6 94 km resolution model outputs to match the grace resolution performs the integration and then disaggregates the updated model state after the assimilation step simulations were performed with and without direct insertion over the three largest river basins in california and including the central valley in order to test the augmented model s ability to capture seasonal and inter annual trends in the water table this is the first ever fusion of grace total water storage change observations with hydrologic simulations aiming at the determination of water table depth dynamics at spatial scales potentially useful for local water management keywords groundwater water table depth trends variable infiltration capacity model grace observations data integration water resources management 1 introduction water in the saturated zone of the ground i e below the water table accounts for nearly 96 of all liquid freshwater kundzewicz et al 2008 and plays a critical role in the natural environment despite its ubiquitous nature groundwater storage is highly variable both spatially and temporally because of the heterogeneous physical processes that govern it groundwater plays a significant role in sorting vegetation or even driving physiological adaptation within a given species orellana et al 2012 therefore controlling to a large extent the spatial distribution of niches groundwater dependent ecosystems specifically in the presence of a shallow water table groundwater reaches the root zone and thus sustains local vegetation even during long non rainy seasons murray et al 2003 eamus and froend 2006 stampoulis et al 2016 although unseen deep beneath the ground and well insulated from several environmental parameters groundwater is in a continuous interchange with other hydrologic components of the water cycle its interactions with the surface and subsurface unsaturated zone water have been established through numerous observations globally gutowski et al 2002 york et al 2002 liang et al 2003 chen and hu 2004 yeh and eltahir 2005 patton et al 2005 chow et al 2006 holt et al 2006 fan et al 2007 these linkages however are not uniform in space or time and change among different soil types e g clay versus sandy soils furthermore groundwater has been characterized as one of the most significant drivers of land surface energy and water exchange with the atmosphere through the increase of soil moisture and its effect on plant water uptake yeh and famiglietti 2009 lo and famiglietti 2010 2011 by and large there is growing evidence of the feedbacks between groundwater and soil moisture as well as on groundwater effects on land atmospheric energy exchanges and therefore weather and climate bierkens and van den hurk 2007 anyah et al 2008 maxwell and kollet 2008 jiang et al 2009 lowry and loheide 2010 rihani et al 2010 maxwell et al 2011 williams and maxwell 2011 taylor et al 2013 condon et al 2013 soylu et al 2014 maxwell et al 2015 in addition when existing in sufficient quantities groundwater can provide long term base flow to rivers and lakes maintaining mesic conditions in terrestrial ecosystems during long dry periods and therefore enabling the continuation of their vital services apart from playing a central role in sustaining ecosystems groundwater also acts as the key strategic reserve that ensures global water security famiglietti 2014 taylor et al 2013 influencing human survival and quality of life more than 38 of the world s population lives in arid or semi arid regions where groundwater is the only reliable freshwater resource eamus et al 2015 in the conterminous united states alone more than 40 of the total water consumption is water drawn from wells maupin et al 2014 overall large quantities of groundwater are being abstracted in several countries at rates that exceed those of recharge richey et al 2015 gorelick and zheng 2015 groundwater is hence increasingly becoming a geopolitical and strategic resource while its exploitation is already of global concern it is evident therefore that the accurate assessment and sustainable management of groundwater supplies is not only of paramount importance to future water security but also a complicated and challenging task especially in arid or semiarid climates or regions with limited resources for mitigation and adaptation 2 background one way to meet this challenge is by using fully distributed models that account for complex hydrologic properties and boundary conditions so far simple parameterizations of subsurface flow have been used in land surface models lsms without explicit representation of the water table depth moreover the initial aim of these models was to represent the physical processes of water movement and energy transfer in the soil plant atmosphere continuum to provide a land surface condition for atmospheric models as such most lsms do not simulate subsurface lateral water movement famiglietti and wood 1991 famiglietti and wood 1994 although several studies gulden et al 2007 liang et al 2003 yeh and eltahir 2005 maxwell and miller 2005 incorporated groundwater processes into lsms they did not account for three dimensional subsurface and surface flow in recent years several improvements have been made ranging from tuning parameters in lsms to examine the extent to which atmospheric and land surface processes are influenced by groundwater dynamics schumacher et al 2018 khaki et al 2018a b swenson and lawrence 2015 lo et al 2013 lo and famiglietti 2010 liang et al 2003 maxwell and miller 2005 yeh and eltahir 2005 york et al 2002 quinn et al 1995 to fully coupling of three dimensional subsurface hydrodynamics with land surface and atmospheric models maxwell et al 2007 kollet and maxwell 2008a niu et al 2007 koster et al 2000 seuffert et al 2002 a model development effort that has included simple incorporation of subsurface hydrodynamics in a lsm is the catchment land surface model clsm developed at the nasa goddard space flight center koster et al 2000 clsm includes a groundwater layer which simulates groundwater storage variations 1 m below the land surface by using an empirical approximation to richards equation to calculate the time evolution of bulk root zone soil moisture although the main advantage of clsm lies in its ability to represent surface and subsurface hydrologic states this model has limited information from its physics or forcings to adequately represent groundwater variability moreover niu et al 2011 have augmented the widely used and robust community lsm by introducing a framework for multiple options to parameterize selected processes noah mp in this effort the authors added among others a simple groundwater model with a topmodel based runoff scheme the new version of the model accounts for the exchange of water between the soil and the aquifer however this only applies to unconfined aquifers furthermore more recently clark et al 2015 described the importance of utilizing lsms to model the distinctly heterogeneous natural system because of the former s distributed nature in addition swenson and lawrence 2015 improved community land model simulations by implementing changes in the model s structure and parameterization using satellite observations specifically they limited the drainage in the model via the use of a soil column of fixed depth and changed the parameterization of the subsurface lateral flow so that base flow is zero in unsaturated conditions these modifications enabled a better representation of water storage variability and improved the agreement with total surface and subsurface water storage tws observations derived from the gravity recovery and climate experiment grace satellite mission finally swenson and lawrence 2015 calculated the soil thickness parameters that best reproduce grace tws variability and using those values along with the zero flow lower boundary condition performed a simulation which compared to standard simulations exhibited a better performance with respect to grace tws variability in the aim of eliminating some of their discrete disadvantages certain studies kollet and maxwell 2008a have investigated the coupling of both lsms and groundwater models however weaknesses such as the disparity in spatial scales and resolutions applied in these models as well as the significantly high demands in terms of computation times especially for applications over large domains still remain largely unsolved apart from local to global scale modeling ground based and satellite observations of groundwater have considerably enhanced our understanding of the complex water dynamics in the saturated zone for centuries observation or monitoring wells have been the traditional approach to measuring subsurface water however although such measurements can provide a relatively high level of accuracy and useful insight into the seasonal variability of groundwater the lack of sufficiently extensive networks of observation wells limits their applicability moreover well data are often characterized by inconsistencies due to spatial and or temporal data gaps as well as human errors caused by the labor intensive data acquisition methods used furthermore the cost of installing and maintaining an adequately dense well network can frequently be prohibitive long term ground based monitoring of groundwater over large scales is therefore a challenging task especially in developing nations or regions with limited resources some of the aforementioned issues have been partially resolved via the various applications of remote sensing due to various limitations characterizing remote sensing techniques groundwater monitoring from space has always been an intriguing challenge becker 2006 grace the joint nasa and german aerospace center dlr mission is the first satellite mission that measures aggregated changes in water mass near the earth s surface by measuring regional changes in the earth s gravity field tapley et al 2004 although grace observations provide a unique opportunity to quantitatively monitor the precious resource of groundwater at global scales richey et al 2015 utilizing them within the scope of water resources management and decision making faces several challenges and limitations specifically the coarse spatial resolution 300 km that characterize grace data along with the fact that they account for multiple water storage components groundwater soil moisture surface water reservoirs and snow make many applications difficult famiglietti and rodell 2013 validation of the use of grace satellite data has been shown in several recent studies where the authors compare estimates of groundwater storage changes using grace observations for example in california s central valley scanlon et al 2012 in the colorado river basin castle et al 2014 in the middle east forootan et al 2014 with ground based water table level data and show a general correspondence between grace and groundwater level data derived from monitoring wells both model estimates and remote sensing or in situ observations are characterized by their own unique strengths and weaknesses combining the strengths of the aforementioned sources of information and often mitigating against their weaknesses is achieved through data assimilation da through da remote sensing or in situ observations are merged with model estimates in the aim of increasing the spatiotemporal resolution as well as the accuracy of the investigated variable hydrologic da is a common practice which results in the reduction of the ambiguity in model predictions as well as the improvement over observations recently several studies have focused on the assimilation of grace observations into hydrologic models or lsms the majority of them showing that grace da improves model outputs zaitchik et al 2008 assimilated grace tws anomalies into the clsm using an ensemble kalman smoother enks enabling the spatiotemporal disaggregation of the tws components groundwater surface and root zone soil moisture as well as snow over the mississippi river basin the same approach was also used by houborg et al 2012 for the determination of drought conditions in north america while li et al 2012 used it to improve hydrological modeling in river catchments situated in europe reager et al 2015 compared observations open loop ol and grace assimilated clsm outputs of various hydrologic variables to assess the improvement that da provides in the model s potential to be utilized for flood analysis over the missouri river basin girotto et al 2016 also assimilated grace derived tws changes into lsms to improve soil moisture modeling while forman et al 2012 performed assimilation of grace tws changes in a snow dominated basin to improve snow water equivalent swe simulations additionally girotto et al 2017 used a 3 d ensemble kalman filter to assimilate grace tws observations into clsm to examine whether the ability of grace to capture the anthropogenic impacts on groundwater can correct the model simulations among other findings they showed that the usually shallow groundwater compartment of the models presents a problem with grace assimilation techniques as these observations account for much deeper aquifers other studies su et al 2010 widiastuti 2009 also provide evidence of hydrologic model improvement through grace da almost all of the above studies show that grace da leads to statistically significant improvements in the accuracy of the models nevertheless these results vary greatly among different assimilation techniques hydrologic models and geographical regions as such more studies are needed to provide further insight into novel approaches that can improve parameter calibration in hydrological modeling despite the numerous advances in hydrologic modeling and improvements in lsms an accurate representation of the water table depth wtd does not yet exist groundwater extractions which lower the water table lo and famiglietti 2013 irrigation increasing et as well as land use changes further confound the effort to realistically estimate wtd especially under extreme wet or dry conditions moreover wtd monitoring is highly insufficient in developing regions and as such information on global wtd is lacking severely hindering water resources management efforts konikow 2015 castle et al 2014 taylor et al 2013 ho et al 2016 from the above review of the state of the art it appears that to advance water resources management and decision support systems we need to improve hydrological prediction and reduce uncertainty in this study we present a novel methodology with which data integration of grace tws observations with hydrologic simulations of the augmented version of a macroscale hydrologic model is enabled ultimately allowing an improved representation of the long period i e greater than monthly variability in the wtd at the local scale moreover we jointly assess the temporal fluctuations of both surface water and deep groundwater at high spatial resolution and this information can be used toward long term water resources management and planning which is especially useful in semi arid regions 3 methods and data in this study in a more realistic approach to account for groundwater variability we augment the variable infiltration capacity vic hydrologic model as the core component of a high resolution modeling system by adding a fourth soil layer of great and varying depth 1000 m that accounts for groundwater not only in shallow depths but also in deep aquifers to this aim a water table was initialized in vic using global wtd data derived from numerous well observations and groundwater model simulations the role of the new 4 layer version of vic hereafter named vic 4l is to enable the direct insertion of grace tws observations ultimately allowing the representation of the wtd at the local scale 3 1 models and forcing data the hydrologic model used in this study is the variable infiltration capacity vic model vic is a robust and widely used macroscale hydrologic model liang et al 1994 and is distributed under the gnu gpl v2 0 license here we used the vic 4 2 d version available from https github com uw hydro vic git website vic requires meteorological input variables including precipitation minimum and maximum temperature tmin tmax and wind in the present implementation we use parameter elevation relationships on independent slopes model prism derived by the prism climate group oregon state university http prism oregonstate edu precipitation minimum and maximum temperature daly et al 2008 as well as wind fields from modern era retrospective analysis for research and applications merra rienecker et al 2011 the updated version of the latter merra 2 is also available for future endeavors gelaro et al 2017 in general vic can solve full water and energy balances to simulate major elements of earth s water cycle it outputs a variety of variables including soil moisture evaporation snow cover swe runoff among others a simple vic calibration was performed by employing a technique aiming at matching surface and subsurface runoff between a previously calibrated vic version 4 0 3 used in maurer et al 2002 and the version used in this analysis 4 2 d specifically three vic soil parameters the variable infiltration curve parameter the maximum velocity of baseflow parameter and the depth of the bottom soil layer were optimized via the implementation of 200 monte carlo iterations matching the runoff ratio between the two aforementioned versions of vic 3 2 model augmentation the subsurface in vic is typically represented as three layers that control the generation of surface runoff and baseflow these layers typically have a total depth that does not exceed a couple of meters below the land surface rendering the model with insufficient subsurface characterization to capture hydrologic responses with regards to groundwater as mentioned above to initialize a water table in the hydrologic model a fourth soil layer of varying depth 1000 m was added as the bottom layer of the vic model in order to provide a simple representation of both shallow and deep groundwater storage changes the exact depth of each model pixel in this new soil layer was determined using wtd data obtained from a global data set at a spatial resolution of 1 km 30 arc second fan et al 2013 figure 1 this gridded wtd data set is derived from observations at nearly 2 million well sites compiled from government archives and published literature as well as groundwater model simulations fan et al 2013 for the purposes of this study before creating the fourth soil layer in vic all values of the wtd grid were doubled to ensure that deep aquifers will be well represented and accounted for and a subset accounting for the geographical domain of interest was created this subset was then spatially aggregated to match the model s resolution and added to the modelvia the implementation of several changes in the input model files e g global parameter file soil parameter file to account for the addition of the fourth soil layer for vic 4l to be able to perform simulations providing estimates of the water equivalent height for each grid cell and for all four soil layers the additional soil layer should be initialized in the model to this aim using the wtd value as well as the area of each grid cell the volume of the saturated soil column was calculated the porosity for that particular grid cell for the purpose of this study we assume its change with depth is negligible was then calculated using the bulk density and particle density obtained from vic s input soil parameter file using the formula 1 porosity 1 bulk density particle density 100 finally for each grid cell the volume of the saturated column was multiplied by the porosity to yield the water equivalent height in the fourth layer this was done to ensure consistency in the units of the output of the soil moisture content of all four soil layers vic 4l was then able to perform simulations that provide a simple representation of both shallow and deep groundwater storage changes fig 1 shows a schematic of the depths of all four layers of vic 4l layer 1 being the top layer and layer 4 being the bottom layer as well as the gridded wtd data over north america used for the initialization of the fourth soil layer in vic and the direct insertion domain explained in detail below along with topographical features of the latter ol runs were performed to test the augmented model s ability to capture seasonal and inter annual trends of groundwater fig 2 presents the daily time series of the vic 4l simulated moisture content of the three top soil layers of vic 4l as well as groundwater anomalies expressed as deviations of the water equivalent height spatially averaged over each of the three basins in central valley i e the direct insertion domain fig 1 as fig 2 suggests vic 4l represents well the seasonal and inter annual trends of both soil moisture and groundwater while the moisture content response lag time increases with the depth of the layer trends shift to the right with increasing layer number the ultimate goal of augmenting vic was to allow grace integration in the aim of improving the accuracy of groundwater estimates and at the same time downscaling the grace observations to spatial resolutions that will be of greater use to water resources managers vic 4l was run at 0 0625 6 94 km and at the daily resolution for the period 2002 16 with and without integrating nasa s grace tws observations however to accommodate a proper spin up that would enable the model to be adequately equilibrated and thus have a reasonably realistic groundwater state at the beginning of the direct insertion period the entire time period jan 01 1990 june 30 2016 of the available vic 4l meteorological input files were used to run the model three consecutive times followed by one more final run for the first 12 years 1990 2001 this resulted in achieving a spin up period of 93 full years and the simulation period covering the entire grace coverage period april 2002 june 2016 3 3 grace observations grace consists of two twin satellites at a distance of 220 km launched in 2002 the orbits of which are monitored in the aim of producing monthly estimates of the earth s gravity field tapley et al 2004 the changes in these estimates over land are in turn converted to changes in tws which ultimately reflect among others groundwater dynamics monthly jpl mass concentration blocks mascons solution based jpl rl05m tws change estimates in 3 3 equal area caps sampled at 0 5 resolution watkins et al 2015 were used http grace jpl nasa gov data get data jpl global mascons after applying the appropriate scaling factors since each grace estimate represents the surface mass anomaly relative to the baseline average over the period january 2004 december 2009 and the investigated period in this study is 2002 16 the mean value over this time period 2004 09 was removed from the grace data moreover the spatial errors in grace mascons are not correlated and therefore no localization is needed furthermore since the mascon grid level information is independent wiese et al 2016 watkins et al 2015 a direct insertion or bias correction method that corrects the model information at the grace scale can be implemented 3 4 direct insertion of grace observations for the direct insertion to obtain absolute tws estimates average ol simulated tws derived from the sum of vic outputs for swe soil moisture of the three top soil layers and groundwater from the fourth additional soil layer over the same period was added to the grace tws change estimates in the integration scheme key steps in the direct insertion process include the agreement of scales between the model and grace observations as well as the temporal aggregation of model output to the grace temporal sampling these steps are done prior and post integration and fig 3 presents the aforementioned steps of the pre and post integration approach in the form of maps specifically before direct insertion is performed vic simulated swe soil moisture for the model s top three layers and groundwater model s fourth bottom layer are aggregated for each pixel and daily time step fig 3 to match the temporal resolution of grace the daily tws is converted to monthly averaged tws for the entire time period fig 3 the monthly tws grid is subsequently resampled to match the grace mascon spatial resolution 0 5 for each monthly time step fig 3 the assimilation technique used in this study is that of the computationally efficient direct insertion although this rather simplistic technique disregards the information provided by the model and assumes that each observation is perfect preserving their errors it serves as a first step toward acquiring the capability to effectively integrate observations and model observations over a large domain and at fine spatial scales specifically the tws anomaly derived from the model s simulations is replaced by that from grace as shown below 2 t w s v i c t w s a v i c t w s m e a n v i c 3 t w s v i c u p d a t e d t w s a g r a c e t w s m e a n v i c where twsmean vic is the long term monthly average tws derived from the model following the direct insertion which takes place at the grace spatial resolution and at the monthly scale inverse spatial and temporal corrections as shown in detail in fig 4 downscale the updated tws values to match the model s spatial 0 0625 and temporal daily resolution finally using the model derived soil parameters bulk density particle density to calculate porosity and after isolating the effect of the moisture content of the top three soil layers and swe on the simulated tws contributions of swe and soil moisture of the three top soil layers to tws are insignificant compared to that of groundwater the groundwater equivalent height is calculated for each time step and pixel within the modeling domain which is then converted to wtd in meters below the ground surface both before and after the direct insertion 3 5 modeling domain for direct insertion the region over which vic was run for the grace data integration study is the three largest river basins in california including the central valley fig 1 the vast agricultural region low elevation regions of the domain also known as the great valley of california is drained by the sacramento and san joaquin rivers extending almost 400 miles along its latitudinal axis this area occupies about 20 000 sq mi and is characterized by very low relief and its close proximity to several topographically complex regions i e the sierra nevada the tehachapi mountains and the coast ranges agricultural activities in the great valley play an important role in the economy of the state of california as more than 250 different crops are grown in this valley with an estimated value of 17 billion year california water science center https ca water usgs gov moreover 17 of the total irrigated land in the u s a is in the central valley while 20 of the nation s groundwater demand is supplied from pumping central valley aquifers making it the second most pumped aquifer system in the u s a california water science center https ca water usgs gov the central valley can be divided into three major basins the sacramento valley in the north the san joaquin basin in the middle and the tulare basin in the south each of these basins include both regions of the great valley as well as the surrounding topographically very complex areas 3 6 validation with well observations both ol and direct insertion runs were validated against in situ well observations provided by the california department of water resources dwr www water ca gov waterdatalibrary while 19 845 groundwater sites wells that are providing groundwater level data are available over the entire region of central valley 9739 wells provide data for the period 2002 16 a major shortcoming of in situ groundwater data is that they are typically characterized by highly varying sampling frequencies ranging from daily to only one measurement in several years which along with the fact that each well site covers different time periods make the use of this dataset an intriguing challenge to ensure therefore the availability of high quality continuous annual data selecting shorter but of great importance to the region periods was deemed necessary the two latest drought periods in california 2007 09 and 2011 15 were chosen for validation of the model as they had a great impact on the state mann and gleick 2015 and hence are of significant hydrologic importance only those wells providing annual data for the same month of three 2007 09 or five 2011 15 consecutive years were selected moreover for wells that are characterized by two values per year one observation made within the first half of the year and one within the second half only the latter was used wells that fall within the same pixel are averaged the final selection resulted in 1285 wells for the 2007 09 drought period and 292 wells for the most recent 2011 15 drought period all well sites are located within the low elevation central valley region of california the exact geographic location of each well site for both of the investigated periods is shown in fig 5 in both cases the well locations are spatially uniformly distributed among the three basins of the domain the validation of vic 4l groundwater outputs was conducted for both aforementioned drought periods at the annual scale and after converting vic 4l groundwater output units from millimeters of water equivalent height to depth to water level in meters below the land surface as indicated in section 3 5 similarly the dwr well measurements were also converted from feet to meters of depth to water level to ensure like to like comparisons all unit transformations are being accommodated by a groundwater validation code module which also identifies the exact latitude and longitude values of each well site that is situated within the central valley domain and based on this information locates the corresponding pixel unique 0 0625 0 0625 area that encompasses the station s geographic location moreover the module identifies the observation date of the in situ data and corresponds it to the exact daily time step of the respective grid cell of the simulated three dimensional wtd matrix annual observations from each station are then compared to the model simulated wtd data before and after grace direct insertion and validation metrics or quantities of interest used for comparison are calculated both for each station separately as well as at larger scales specifically at the river basin level sacramento valley san joaquin and tulare basins by summarizing the metrics variables over these regions grid cells of the simulation domain falling outside central valley mostly high elevation regions in the north and east were excluded using a mask validation metrics used for comparison are correlation coefficient cc wtd change wtdenddate wtdstartdate and wtd slope linear least squares slope of wtd time series 4 results 4 1 direct insertion of grace observations fig 6 presents the effect of grace direct insertion in the form of maps of tws over the central valley at the daily model resolution the temporal daily trend of tws was calculated as the linear least squares slope for each pixel and for the entire 14 year period april 2002 april 2016 the statistical significance of the trend for each pixel was also calculated the tws daily trend derived from the ol run fig 6a is statistically significant p 0 0001 over the entire region of central valley and shows negative slopes decreasing tws over most of the domain with a few exceptions in the mountainous regions on the east and north areas in the great valley are generally characterized by smaller negative values with very small spatial variability indicating changes of smaller magnitude moreover regions of the great valley along the eastern transition line between low and high elevation areas exhibit positive slopes which means that the tws is overall increasing over these areas the mountainous regions display a greater spatial variability although the great majority of pixels exhibit lower negative values of slope this phenomenon may be attributed to orographic precipitation in the form of rain or snow owing to the very complex topography of the study domain precipitation varies greatly especially along the longitudinal axis as the terrain changes from west to east from low elevation and moderate sloping relief to significantly steeper and higher elevation ground as such eastern higher altitudes are subject to more frequent precipitation events most of which are regional orographically induced rain or snow resulting in the overall positive trends displayed in these regions the grace integration run yields tws values that are overall characterized by stronger trends that are also statistically significant p 0 0001 fig 6b over most of the central valley tws exhibits stronger negative slopes compared to those of the ol run specifically the mountains on the east and north west are characterized by greater in terms of absolute value negative slopes than the great valley regions while the foothills that showed small increase in tws with time from the ol run exhibit negative values for the most part in contrast grace integration had a different effect in the north where most pixels are characterized by positive or near zero slope values the temporal variation of tws for each pixel is shown in terms of standard deviation both before fig 6c and after fig 6d direct insertion areas of high elevation are characterized by greater temporal variability higher standard deviation values in terms of tws while the opposite is true over the valley regions of california the direct insertion increases the standard deviation values over the valleys while the opposite impact is true for the complex terrain areas mountains the difference between the standard deviation of the daily grace integration tws and the standard deviation of the daily ol tws is shown in fig 6e the bipolar effect of the direct insertion is evident reducing the standard deviation over the mountains while increasing it over the valleys fig 7 shows the temporal trends of tws for the period april 2002 june 2016 with and without grace integration for the three major basins of central valley the spatially averaged monthly grace observations of tws anomalies twsa are also shown along the grace integration tws curve for each basin in all three cases the model blue curve captures both intra and inter annual variability in terms of tws as the unusually wet winter of 2011 as well as the recent multi year drought period 2012 16 are clearly shown the grace integration tws daily time series red curve shows little improvement over sacramento river basin while its effect is more pronounced over the san joaquin and tulare basins more specifically in the two latter basins direct insertion increases tws during the 2003 07 period while the opposite is true during 2011 16 the grace data integration impact is negligible during the period 2008 10 a point to note is that a comparison among the three basins reveals that the impact of direct insertion increases with increasing dryness of the region the same is also true during periods of stronger deviations of the average hydrologic patterns this finding emphasizes the importance of grace in improving hydrologic modeling skill via the implementation of a computationally efficient assimilation technique i e direct insertion as these observations add a critical piece of information that these models fail to account for groundwater depletion related to irrigated agriculture which is often the case in dry regions or generally during periods of water shortages the effect of grace direct insertion in terms of wtd is presented in fig 8 fig 8a shows the vic simulated average daily wtd for the period april 1 2002 june 30 2016 over central valley as expected great depths for water table are shown in the mountainous regions whereas the valley overall exhibits low wtd values fig 8b presents the average daily wtd difference in meters below the ground surface between grace direct insertion and ol for most of the study domain direct insertion increases wtd in other words integrating grace observations deepens the water table the northern region of central valley seems to be characterized by a consistently wtd decreasing effect of grace data integration this could be attributed to either a model related discrepancy or a problematic retrieval indicating consistently low tws anomaly of grace over this region potentially due to certain topographical features or the existence of water bodies the daily trend of the ol wtd was also calculated as the linear least squares slope for each pixel and for the entire 14 year period april 2002 april 2016 fig 8c the statistical significance of the trend for each pixel was also calculated overall positive slopes retreating water table characterize the high elevation regions in the east and northwest while the foothills on the west side of the mountains show negative trends in wtd rising water table the valleys exhibit slightly positive values in terms of slope and present very little spatial variation all pixels are characterized by statistically significant p 0 0001 trends the effect of grace direct insertion on the wtd trends is shown in fig 8d all regions characterized by positive slopes in the ol run are characterized by stronger trends that are also statistically significant p 0 0001 with greater positive slopes moreover the foothills of the mountains exhibit mostly positive trends and the spatial variability in the mountainous regions is reduced compared to the ol run in addition direct insertion has a varying wtd increasing effect in the low elevation regions especially within central valley in contrast many regions in the northeast of the investigated domain exhibit a wtd decreasing trend after grace data integration indicating an overall rise in the water table during the investigated 14 year period the standard deviation of the daily wtd differences between grace direct insertion and ol is shown in fig 8e in all mountainous regions the effect of grace integration has greater temporal variations than that in the low elevation valley areas moreover the standard deviation of the differences varies significantly over the complex terrain regions indicating a significant effect of topography on grace data integration comparing the temporal variability in terms of standard deviation between the grace direct insertion and ol wtd for each pixel shows that the integration of the satellite observations lowers the temporal variations of wtd over the high elevation areas while the opposite is true for low elevation pixels fig 8f the temporal evolution of the wtd averaged over the entire central valley is shown in fig 9 top before and after grace direct insertion the time series of the daily spatially averaged wtd simulated by vic ol shows a decrease of wtd rise in the water table in the winter of 2006 followed by an increase of wtd drop in the water table throughout the next 3 years in 2011 wtd once again decreases because of the wet winter of that year and during the drought period of 2012 16 wtd exhibited an increase the direct insertion run shows similar patterns however its effect on the wtd is clearly decreasing during the first four years 2002 06 and increasing during the recent drought years evidently as shown in this graph corrections due to grace observations are of greater magnitude during periods characterized by stronger deviations from the average hydrologic conditions fig 9 bottom also shows the time series of the daily wtd difference in meters below the ground surface between the grace integration and ol runs the difference is clearly minimal during the period 2008 10 indicating a small impact of the grace observations on the other hand its negative positive values during the first last simulation years show that grace data can be of significant value to studies for periods characterized by extreme hydrologic conditions in this case droughts 4 2 validation with well measurements fig 10 presents the distribution of the cc values calculated between wtd derived from the dwr wells and that of the model simulation specific grid cell that encompasses the well location before and after grace integration for the two drought periods of 2007 09 and 2011 15 during both time periods the vast majority of wells are characterized by high cc values ranging from 0 6 to 1 0 the number of wells exhibiting negative cc values is smaller in the longer drought period as expected when grace observations are integrated cc values are in general higher especially in the upper positive ranges specifically although the direct insertion does not seem to affect negative correlations positive ones are shifted toward higher values with a significant increase in the 0 8 1 0 bin most of the other categories of positive cc values also present an increase however rather small the positive effect of grace integration is also evident in fig 11 most wells are characterized by a positive change in the cc value following the direct insertion during both investigated drought periods in other words grace data integration improves the agreement of the temporal trends especially in regions for which the model captures well the temporal variability of groundwater level this is true for both drought periods with a larger impact during the 2007 09 period fig 12 shows the correlation via the use of scatter plots between wtd change and slope describing the trend derived from the ol simulation and that of grace direct insertion for both of the investigated drought periods these are calculated only for those grid cells encompassing the well locations in central valley and are inter compared to provide a more holistic picture of the grace data integration impact on the model s performance overall the direct insertion seems to increase the temporal variability of the simulated wtd by a factor of 3 or 4 while similarly there is a 3 fold increase of the slope values to further investigate the effect of grace direct insertion from a more quantitative perspective the above parameters wtd change slope were calculated by averaging the wtd values derived from wells or model simulations over each of the major river basins within the central valley not including surrounding regions of higher elevation in the greater direct insertion domain and inter compared table 1 presents these values for both drought periods of 2007 09 and 2011 15 and for each of the three basins in central valley for both periods wells are rather uniformly distributed among the three regions specifically for the 2007 09 period 66 wells are situated in sacramento river 395 wells in san joaquin river and 824 wells in tulare while for 2011 15 these numbers are 71 107 and 114 respectively the change and slope of wtd as well as the cc between the wtd derived by the wells and that derived by the model before and after direct insertion were calculated after averaging the wells over each basin and all the model grid cells regardless of whether a well is situated in them or not that fall within the central valley region of each of the three major river basins of california in terms of wtd change grace data integration improves the model s performance by increasing the model s temporal variability of groundwater and thus better capturing the inter annual changes in the wtd more specifically the ol simulation resulted in a wtd change of 0 0825 m indicating a decline in the water level in sacramento river over the period 2007 09 while the respective average well derived wtd change was 0 696 m with grace direct insertion this difference was reduced by 30 while a 47 improvement was observed for the same period over the much drier tulare similarly the wtd slope derived by the ol simulations is substantially smaller than that derived by the wells however integration improvement in all cases ranges from 25 to more than 50 moreover on average cc values are high in all cases except sacramento river during 2007 09 and increase after the direct insertion with the exception of san joaquin river in 2007 09 which exhibits a minor decrease in the cc value after grace data integration the higher impact of grace direct insertion in terms of improving the agreement between temporal trends was observed in sacramento river for both drought periods by and large however grace data integration has the greatest positive effect on the driest basin in central valley i e tulare basin 5 conclusions in the monitoring of groundwater resources by local water managers day to day fluctuations in wtd are less important than seasonal and longer variability and trends in the state of groundwater storage these trends help to characterize the potential rate of depletion over long time frames in response to changes in the natural supply of water e g during drought and changes in the depletion of water due to variable human consumption in response to this conceptual framework this study presents a simple yet novel methodology to enable data fusion of grace tws observations to the augmented version of a macroscale hydrologic model vic 4l ultimately allowing an improved representation of the long period i e greater than monthly variability in the wtd at the local scale such information specifically the long term temporal variability of wtd is of immense value to water resource managers moreover the wtd is an indication of groundwater quantity and therefore quality fluctuations with significant impacts on the environment and human welfare in addition this study provides holistic insight into the temporal fluctuations of both surface water and deep groundwater at high spatial resolution and this information can be used toward long term water resources management and planning as well as climate change adaptation additionally the results of this analysis provide an opportunity to examine the effect of grace observations on hydrologic simulations over the investigated topographically and climatologically very complex region of the three major river basins in california including central valley specifically corrections due to grace observations are of greater magnitude during periods characterized by stronger deviations from the average hydrologic conditions moreover the effect of direct insertion is greater over high elevation regions while in all mountainous regions the model simulated tws has greater temporal variations than that in the low elevation valley areas furthermore hydrologic representations in drier regions in central valley such as the tulare basin were affected to a greater extent by grace data integration compared to those in wetter regions indicating the significant value of grace observations in accounting for large scale groundwater depletion in heavily irrigated regions 10 times more intense than in other regions of the central valley especially during drought periods scanlon et al 2012 faunt 2009 in addition although not shown here the total simulated with direct insertion groundwater volume rate yielded 6 3 km3 year which is in agreement with the findings of famiglietti et al 2011 who calculated the total central valley groundwater depletion rate to be 6 1 km3 year despite the relatively limited time period of grace coverage 15 years all basins showed improvement in the model s skill after grace data integration in terms of capturing the long term trends of seasonal inter annual variability and thus better representing the response of groundwater to natural or human made alterations of the hydrologic cycle although irrigation was not accounted for in the model integrating grace observations with vic simulations improved the model s performance by more accurately representing the changes in tws owing to the fact that grace anomalies introduce the anthropogenic effect e g groundwater pumping irrigation into hydrologic simulations a more realistic hydrologic representation was achieved these findings are derived through the comparisons of individual pixels with wells that are situated within that 0 0625 0 0625 area and which do not accurately represent the mean area wtd nevertheless well observations are currently the best available source of data that can be used to validate groundwater simulations such findings increase confidence in application of grace for monitoring groundwater in hydrologically vulnerable regions or areas that are frequently subject to extreme hydrologic conditions droughts or floods however the existence of dense well networks particularly over water stressed regions where groundwater depletion occurs at high rates during droughts that continuously monitor water table levels is also of paramount importance these networks would provide significantly improved information for estimating groundwater storage which would in turn be integrated into advanced hydrologic models thus resulting in hydrologic state estimates of higher accuracy and resolution although various model development efforts koster et al 2000 seuffert et al 2002 maxwell et al 2007 niu et al 2007 kollet and maxwell 2006 kollet and maxwell 2008a clark et al 2015 aim at simulating groundwater storage variations below the land surface they very frequently exhibit a behavior similar to that of a bucket model other model coupling efforts kollet and maxwell 2008a are plagued by the disparity in spatial scales and resolutions as well as the high computational demands this study provides a holistic insight into reproducing water storage variability not only in shallow soils but also in deeper groundwater by jointly assessing the temporal fluctuations of water in both surface as well as deep aquifers at high spatial resolution moreover integration of grace observations enables a more accurate representation of wtd especially in regions where groundwater extractions and climatological factors confound any effort to estimate wtd thus providing information of utmost importance for long term water resources management and planning in arid or semi arid regions currently missing from the existing framework is an optimal assimilation technique such as ensemble kalman smoother enks or ensemble kalman filter enkf such techniques can replace the direct insertion scheme currently used for grace assimilation to encompass the uncertainty in both model estimates and observations thus achieving a better representation of the wtd moreover grace observations are characterized by spatially correlated errors which often impact assimilation results removal of or accounting for such errors is therefore crucial for achieving more realistic assimilation schemes khaki et al 2018a b piretzidis et al 2018 and should be considered for future grace assimilation efforts our main premise is that apart from improving the assimilation scheme the current work can further be augmented by also adjusting the model parameters that govern groundwater flow e g hydraulic conductivity as well as expanding the direct insertion domain for the updated grace assimilation framework via its implementation over the entire region of the western united states this will be of critical importance as among the main characteristics of this region are groundwater extractions which lower the water table irrigation increasing evapotranspiration as well as land use changes which further confound the effort to realistically estimate wtd especially under extreme hydrologic conditions moreover the recently launched grace follow on satellites will extend the existing 15 year data record and thus enable a greater number of their applications the improved representation of the wtd that will be provided through grace assimilation will better characterize the spatial heterogeneity and temporal variations of the interlinked surface and subsurface water dynamics such approach will ultimately lead to the more reliable estimation of water storage which is of utmost importance for quantifying and managing available water resources especially in the water stressed regions of the western united states acknowledgments the research described in this paper was carried out at the jet propulsion laboratory california institute of technology under a contract with the national aeronautics and space administration this work was supported by nasa research announcements nra nnx12ak64g we also acknowledge the editor and the three anonymous reviewers for their constructive criticism and helpful comments that enabled us to greatly improve this manuscript 
651,despite numerous advances in continental scale hydrologic modeling and improvements in global land surface models an accurate representation of regional water table depth wtd remains a challenge data assimilation of observations from the gravity recovery and climate experiment grace mission leads to improvements in the accuracy of hydrologic models ultimately resulting in more reliable estimates of lumped water storage however the usually shallow groundwater compartment of many models presents a problem with grace assimilation techniques as these satellite observations also represent changes in deeper soils and aquifers to improve the accuracy of modeled groundwater estimates and allow the representation of wtd at finer spatial scales we implemented a simple yet novel approach to integrate grace data by augmenting the variable infiltration capacity vic hydrologic model first the subsurface model structural representation was modified by incorporating an additional fourth soil layer of varying depth up to 1000 m in vic as the bottom groundwater layer this addition allows the model to reproduce water storage variability not only in shallow soils but also in deeper groundwater in order to allow integration of the full grace observed variability second a direct insertion scheme was developed that integrates the high temporal daily and spatial 6 94 km resolution model outputs to match the grace resolution performs the integration and then disaggregates the updated model state after the assimilation step simulations were performed with and without direct insertion over the three largest river basins in california and including the central valley in order to test the augmented model s ability to capture seasonal and inter annual trends in the water table this is the first ever fusion of grace total water storage change observations with hydrologic simulations aiming at the determination of water table depth dynamics at spatial scales potentially useful for local water management keywords groundwater water table depth trends variable infiltration capacity model grace observations data integration water resources management 1 introduction water in the saturated zone of the ground i e below the water table accounts for nearly 96 of all liquid freshwater kundzewicz et al 2008 and plays a critical role in the natural environment despite its ubiquitous nature groundwater storage is highly variable both spatially and temporally because of the heterogeneous physical processes that govern it groundwater plays a significant role in sorting vegetation or even driving physiological adaptation within a given species orellana et al 2012 therefore controlling to a large extent the spatial distribution of niches groundwater dependent ecosystems specifically in the presence of a shallow water table groundwater reaches the root zone and thus sustains local vegetation even during long non rainy seasons murray et al 2003 eamus and froend 2006 stampoulis et al 2016 although unseen deep beneath the ground and well insulated from several environmental parameters groundwater is in a continuous interchange with other hydrologic components of the water cycle its interactions with the surface and subsurface unsaturated zone water have been established through numerous observations globally gutowski et al 2002 york et al 2002 liang et al 2003 chen and hu 2004 yeh and eltahir 2005 patton et al 2005 chow et al 2006 holt et al 2006 fan et al 2007 these linkages however are not uniform in space or time and change among different soil types e g clay versus sandy soils furthermore groundwater has been characterized as one of the most significant drivers of land surface energy and water exchange with the atmosphere through the increase of soil moisture and its effect on plant water uptake yeh and famiglietti 2009 lo and famiglietti 2010 2011 by and large there is growing evidence of the feedbacks between groundwater and soil moisture as well as on groundwater effects on land atmospheric energy exchanges and therefore weather and climate bierkens and van den hurk 2007 anyah et al 2008 maxwell and kollet 2008 jiang et al 2009 lowry and loheide 2010 rihani et al 2010 maxwell et al 2011 williams and maxwell 2011 taylor et al 2013 condon et al 2013 soylu et al 2014 maxwell et al 2015 in addition when existing in sufficient quantities groundwater can provide long term base flow to rivers and lakes maintaining mesic conditions in terrestrial ecosystems during long dry periods and therefore enabling the continuation of their vital services apart from playing a central role in sustaining ecosystems groundwater also acts as the key strategic reserve that ensures global water security famiglietti 2014 taylor et al 2013 influencing human survival and quality of life more than 38 of the world s population lives in arid or semi arid regions where groundwater is the only reliable freshwater resource eamus et al 2015 in the conterminous united states alone more than 40 of the total water consumption is water drawn from wells maupin et al 2014 overall large quantities of groundwater are being abstracted in several countries at rates that exceed those of recharge richey et al 2015 gorelick and zheng 2015 groundwater is hence increasingly becoming a geopolitical and strategic resource while its exploitation is already of global concern it is evident therefore that the accurate assessment and sustainable management of groundwater supplies is not only of paramount importance to future water security but also a complicated and challenging task especially in arid or semiarid climates or regions with limited resources for mitigation and adaptation 2 background one way to meet this challenge is by using fully distributed models that account for complex hydrologic properties and boundary conditions so far simple parameterizations of subsurface flow have been used in land surface models lsms without explicit representation of the water table depth moreover the initial aim of these models was to represent the physical processes of water movement and energy transfer in the soil plant atmosphere continuum to provide a land surface condition for atmospheric models as such most lsms do not simulate subsurface lateral water movement famiglietti and wood 1991 famiglietti and wood 1994 although several studies gulden et al 2007 liang et al 2003 yeh and eltahir 2005 maxwell and miller 2005 incorporated groundwater processes into lsms they did not account for three dimensional subsurface and surface flow in recent years several improvements have been made ranging from tuning parameters in lsms to examine the extent to which atmospheric and land surface processes are influenced by groundwater dynamics schumacher et al 2018 khaki et al 2018a b swenson and lawrence 2015 lo et al 2013 lo and famiglietti 2010 liang et al 2003 maxwell and miller 2005 yeh and eltahir 2005 york et al 2002 quinn et al 1995 to fully coupling of three dimensional subsurface hydrodynamics with land surface and atmospheric models maxwell et al 2007 kollet and maxwell 2008a niu et al 2007 koster et al 2000 seuffert et al 2002 a model development effort that has included simple incorporation of subsurface hydrodynamics in a lsm is the catchment land surface model clsm developed at the nasa goddard space flight center koster et al 2000 clsm includes a groundwater layer which simulates groundwater storage variations 1 m below the land surface by using an empirical approximation to richards equation to calculate the time evolution of bulk root zone soil moisture although the main advantage of clsm lies in its ability to represent surface and subsurface hydrologic states this model has limited information from its physics or forcings to adequately represent groundwater variability moreover niu et al 2011 have augmented the widely used and robust community lsm by introducing a framework for multiple options to parameterize selected processes noah mp in this effort the authors added among others a simple groundwater model with a topmodel based runoff scheme the new version of the model accounts for the exchange of water between the soil and the aquifer however this only applies to unconfined aquifers furthermore more recently clark et al 2015 described the importance of utilizing lsms to model the distinctly heterogeneous natural system because of the former s distributed nature in addition swenson and lawrence 2015 improved community land model simulations by implementing changes in the model s structure and parameterization using satellite observations specifically they limited the drainage in the model via the use of a soil column of fixed depth and changed the parameterization of the subsurface lateral flow so that base flow is zero in unsaturated conditions these modifications enabled a better representation of water storage variability and improved the agreement with total surface and subsurface water storage tws observations derived from the gravity recovery and climate experiment grace satellite mission finally swenson and lawrence 2015 calculated the soil thickness parameters that best reproduce grace tws variability and using those values along with the zero flow lower boundary condition performed a simulation which compared to standard simulations exhibited a better performance with respect to grace tws variability in the aim of eliminating some of their discrete disadvantages certain studies kollet and maxwell 2008a have investigated the coupling of both lsms and groundwater models however weaknesses such as the disparity in spatial scales and resolutions applied in these models as well as the significantly high demands in terms of computation times especially for applications over large domains still remain largely unsolved apart from local to global scale modeling ground based and satellite observations of groundwater have considerably enhanced our understanding of the complex water dynamics in the saturated zone for centuries observation or monitoring wells have been the traditional approach to measuring subsurface water however although such measurements can provide a relatively high level of accuracy and useful insight into the seasonal variability of groundwater the lack of sufficiently extensive networks of observation wells limits their applicability moreover well data are often characterized by inconsistencies due to spatial and or temporal data gaps as well as human errors caused by the labor intensive data acquisition methods used furthermore the cost of installing and maintaining an adequately dense well network can frequently be prohibitive long term ground based monitoring of groundwater over large scales is therefore a challenging task especially in developing nations or regions with limited resources some of the aforementioned issues have been partially resolved via the various applications of remote sensing due to various limitations characterizing remote sensing techniques groundwater monitoring from space has always been an intriguing challenge becker 2006 grace the joint nasa and german aerospace center dlr mission is the first satellite mission that measures aggregated changes in water mass near the earth s surface by measuring regional changes in the earth s gravity field tapley et al 2004 although grace observations provide a unique opportunity to quantitatively monitor the precious resource of groundwater at global scales richey et al 2015 utilizing them within the scope of water resources management and decision making faces several challenges and limitations specifically the coarse spatial resolution 300 km that characterize grace data along with the fact that they account for multiple water storage components groundwater soil moisture surface water reservoirs and snow make many applications difficult famiglietti and rodell 2013 validation of the use of grace satellite data has been shown in several recent studies where the authors compare estimates of groundwater storage changes using grace observations for example in california s central valley scanlon et al 2012 in the colorado river basin castle et al 2014 in the middle east forootan et al 2014 with ground based water table level data and show a general correspondence between grace and groundwater level data derived from monitoring wells both model estimates and remote sensing or in situ observations are characterized by their own unique strengths and weaknesses combining the strengths of the aforementioned sources of information and often mitigating against their weaknesses is achieved through data assimilation da through da remote sensing or in situ observations are merged with model estimates in the aim of increasing the spatiotemporal resolution as well as the accuracy of the investigated variable hydrologic da is a common practice which results in the reduction of the ambiguity in model predictions as well as the improvement over observations recently several studies have focused on the assimilation of grace observations into hydrologic models or lsms the majority of them showing that grace da improves model outputs zaitchik et al 2008 assimilated grace tws anomalies into the clsm using an ensemble kalman smoother enks enabling the spatiotemporal disaggregation of the tws components groundwater surface and root zone soil moisture as well as snow over the mississippi river basin the same approach was also used by houborg et al 2012 for the determination of drought conditions in north america while li et al 2012 used it to improve hydrological modeling in river catchments situated in europe reager et al 2015 compared observations open loop ol and grace assimilated clsm outputs of various hydrologic variables to assess the improvement that da provides in the model s potential to be utilized for flood analysis over the missouri river basin girotto et al 2016 also assimilated grace derived tws changes into lsms to improve soil moisture modeling while forman et al 2012 performed assimilation of grace tws changes in a snow dominated basin to improve snow water equivalent swe simulations additionally girotto et al 2017 used a 3 d ensemble kalman filter to assimilate grace tws observations into clsm to examine whether the ability of grace to capture the anthropogenic impacts on groundwater can correct the model simulations among other findings they showed that the usually shallow groundwater compartment of the models presents a problem with grace assimilation techniques as these observations account for much deeper aquifers other studies su et al 2010 widiastuti 2009 also provide evidence of hydrologic model improvement through grace da almost all of the above studies show that grace da leads to statistically significant improvements in the accuracy of the models nevertheless these results vary greatly among different assimilation techniques hydrologic models and geographical regions as such more studies are needed to provide further insight into novel approaches that can improve parameter calibration in hydrological modeling despite the numerous advances in hydrologic modeling and improvements in lsms an accurate representation of the water table depth wtd does not yet exist groundwater extractions which lower the water table lo and famiglietti 2013 irrigation increasing et as well as land use changes further confound the effort to realistically estimate wtd especially under extreme wet or dry conditions moreover wtd monitoring is highly insufficient in developing regions and as such information on global wtd is lacking severely hindering water resources management efforts konikow 2015 castle et al 2014 taylor et al 2013 ho et al 2016 from the above review of the state of the art it appears that to advance water resources management and decision support systems we need to improve hydrological prediction and reduce uncertainty in this study we present a novel methodology with which data integration of grace tws observations with hydrologic simulations of the augmented version of a macroscale hydrologic model is enabled ultimately allowing an improved representation of the long period i e greater than monthly variability in the wtd at the local scale moreover we jointly assess the temporal fluctuations of both surface water and deep groundwater at high spatial resolution and this information can be used toward long term water resources management and planning which is especially useful in semi arid regions 3 methods and data in this study in a more realistic approach to account for groundwater variability we augment the variable infiltration capacity vic hydrologic model as the core component of a high resolution modeling system by adding a fourth soil layer of great and varying depth 1000 m that accounts for groundwater not only in shallow depths but also in deep aquifers to this aim a water table was initialized in vic using global wtd data derived from numerous well observations and groundwater model simulations the role of the new 4 layer version of vic hereafter named vic 4l is to enable the direct insertion of grace tws observations ultimately allowing the representation of the wtd at the local scale 3 1 models and forcing data the hydrologic model used in this study is the variable infiltration capacity vic model vic is a robust and widely used macroscale hydrologic model liang et al 1994 and is distributed under the gnu gpl v2 0 license here we used the vic 4 2 d version available from https github com uw hydro vic git website vic requires meteorological input variables including precipitation minimum and maximum temperature tmin tmax and wind in the present implementation we use parameter elevation relationships on independent slopes model prism derived by the prism climate group oregon state university http prism oregonstate edu precipitation minimum and maximum temperature daly et al 2008 as well as wind fields from modern era retrospective analysis for research and applications merra rienecker et al 2011 the updated version of the latter merra 2 is also available for future endeavors gelaro et al 2017 in general vic can solve full water and energy balances to simulate major elements of earth s water cycle it outputs a variety of variables including soil moisture evaporation snow cover swe runoff among others a simple vic calibration was performed by employing a technique aiming at matching surface and subsurface runoff between a previously calibrated vic version 4 0 3 used in maurer et al 2002 and the version used in this analysis 4 2 d specifically three vic soil parameters the variable infiltration curve parameter the maximum velocity of baseflow parameter and the depth of the bottom soil layer were optimized via the implementation of 200 monte carlo iterations matching the runoff ratio between the two aforementioned versions of vic 3 2 model augmentation the subsurface in vic is typically represented as three layers that control the generation of surface runoff and baseflow these layers typically have a total depth that does not exceed a couple of meters below the land surface rendering the model with insufficient subsurface characterization to capture hydrologic responses with regards to groundwater as mentioned above to initialize a water table in the hydrologic model a fourth soil layer of varying depth 1000 m was added as the bottom layer of the vic model in order to provide a simple representation of both shallow and deep groundwater storage changes the exact depth of each model pixel in this new soil layer was determined using wtd data obtained from a global data set at a spatial resolution of 1 km 30 arc second fan et al 2013 figure 1 this gridded wtd data set is derived from observations at nearly 2 million well sites compiled from government archives and published literature as well as groundwater model simulations fan et al 2013 for the purposes of this study before creating the fourth soil layer in vic all values of the wtd grid were doubled to ensure that deep aquifers will be well represented and accounted for and a subset accounting for the geographical domain of interest was created this subset was then spatially aggregated to match the model s resolution and added to the modelvia the implementation of several changes in the input model files e g global parameter file soil parameter file to account for the addition of the fourth soil layer for vic 4l to be able to perform simulations providing estimates of the water equivalent height for each grid cell and for all four soil layers the additional soil layer should be initialized in the model to this aim using the wtd value as well as the area of each grid cell the volume of the saturated soil column was calculated the porosity for that particular grid cell for the purpose of this study we assume its change with depth is negligible was then calculated using the bulk density and particle density obtained from vic s input soil parameter file using the formula 1 porosity 1 bulk density particle density 100 finally for each grid cell the volume of the saturated column was multiplied by the porosity to yield the water equivalent height in the fourth layer this was done to ensure consistency in the units of the output of the soil moisture content of all four soil layers vic 4l was then able to perform simulations that provide a simple representation of both shallow and deep groundwater storage changes fig 1 shows a schematic of the depths of all four layers of vic 4l layer 1 being the top layer and layer 4 being the bottom layer as well as the gridded wtd data over north america used for the initialization of the fourth soil layer in vic and the direct insertion domain explained in detail below along with topographical features of the latter ol runs were performed to test the augmented model s ability to capture seasonal and inter annual trends of groundwater fig 2 presents the daily time series of the vic 4l simulated moisture content of the three top soil layers of vic 4l as well as groundwater anomalies expressed as deviations of the water equivalent height spatially averaged over each of the three basins in central valley i e the direct insertion domain fig 1 as fig 2 suggests vic 4l represents well the seasonal and inter annual trends of both soil moisture and groundwater while the moisture content response lag time increases with the depth of the layer trends shift to the right with increasing layer number the ultimate goal of augmenting vic was to allow grace integration in the aim of improving the accuracy of groundwater estimates and at the same time downscaling the grace observations to spatial resolutions that will be of greater use to water resources managers vic 4l was run at 0 0625 6 94 km and at the daily resolution for the period 2002 16 with and without integrating nasa s grace tws observations however to accommodate a proper spin up that would enable the model to be adequately equilibrated and thus have a reasonably realistic groundwater state at the beginning of the direct insertion period the entire time period jan 01 1990 june 30 2016 of the available vic 4l meteorological input files were used to run the model three consecutive times followed by one more final run for the first 12 years 1990 2001 this resulted in achieving a spin up period of 93 full years and the simulation period covering the entire grace coverage period april 2002 june 2016 3 3 grace observations grace consists of two twin satellites at a distance of 220 km launched in 2002 the orbits of which are monitored in the aim of producing monthly estimates of the earth s gravity field tapley et al 2004 the changes in these estimates over land are in turn converted to changes in tws which ultimately reflect among others groundwater dynamics monthly jpl mass concentration blocks mascons solution based jpl rl05m tws change estimates in 3 3 equal area caps sampled at 0 5 resolution watkins et al 2015 were used http grace jpl nasa gov data get data jpl global mascons after applying the appropriate scaling factors since each grace estimate represents the surface mass anomaly relative to the baseline average over the period january 2004 december 2009 and the investigated period in this study is 2002 16 the mean value over this time period 2004 09 was removed from the grace data moreover the spatial errors in grace mascons are not correlated and therefore no localization is needed furthermore since the mascon grid level information is independent wiese et al 2016 watkins et al 2015 a direct insertion or bias correction method that corrects the model information at the grace scale can be implemented 3 4 direct insertion of grace observations for the direct insertion to obtain absolute tws estimates average ol simulated tws derived from the sum of vic outputs for swe soil moisture of the three top soil layers and groundwater from the fourth additional soil layer over the same period was added to the grace tws change estimates in the integration scheme key steps in the direct insertion process include the agreement of scales between the model and grace observations as well as the temporal aggregation of model output to the grace temporal sampling these steps are done prior and post integration and fig 3 presents the aforementioned steps of the pre and post integration approach in the form of maps specifically before direct insertion is performed vic simulated swe soil moisture for the model s top three layers and groundwater model s fourth bottom layer are aggregated for each pixel and daily time step fig 3 to match the temporal resolution of grace the daily tws is converted to monthly averaged tws for the entire time period fig 3 the monthly tws grid is subsequently resampled to match the grace mascon spatial resolution 0 5 for each monthly time step fig 3 the assimilation technique used in this study is that of the computationally efficient direct insertion although this rather simplistic technique disregards the information provided by the model and assumes that each observation is perfect preserving their errors it serves as a first step toward acquiring the capability to effectively integrate observations and model observations over a large domain and at fine spatial scales specifically the tws anomaly derived from the model s simulations is replaced by that from grace as shown below 2 t w s v i c t w s a v i c t w s m e a n v i c 3 t w s v i c u p d a t e d t w s a g r a c e t w s m e a n v i c where twsmean vic is the long term monthly average tws derived from the model following the direct insertion which takes place at the grace spatial resolution and at the monthly scale inverse spatial and temporal corrections as shown in detail in fig 4 downscale the updated tws values to match the model s spatial 0 0625 and temporal daily resolution finally using the model derived soil parameters bulk density particle density to calculate porosity and after isolating the effect of the moisture content of the top three soil layers and swe on the simulated tws contributions of swe and soil moisture of the three top soil layers to tws are insignificant compared to that of groundwater the groundwater equivalent height is calculated for each time step and pixel within the modeling domain which is then converted to wtd in meters below the ground surface both before and after the direct insertion 3 5 modeling domain for direct insertion the region over which vic was run for the grace data integration study is the three largest river basins in california including the central valley fig 1 the vast agricultural region low elevation regions of the domain also known as the great valley of california is drained by the sacramento and san joaquin rivers extending almost 400 miles along its latitudinal axis this area occupies about 20 000 sq mi and is characterized by very low relief and its close proximity to several topographically complex regions i e the sierra nevada the tehachapi mountains and the coast ranges agricultural activities in the great valley play an important role in the economy of the state of california as more than 250 different crops are grown in this valley with an estimated value of 17 billion year california water science center https ca water usgs gov moreover 17 of the total irrigated land in the u s a is in the central valley while 20 of the nation s groundwater demand is supplied from pumping central valley aquifers making it the second most pumped aquifer system in the u s a california water science center https ca water usgs gov the central valley can be divided into three major basins the sacramento valley in the north the san joaquin basin in the middle and the tulare basin in the south each of these basins include both regions of the great valley as well as the surrounding topographically very complex areas 3 6 validation with well observations both ol and direct insertion runs were validated against in situ well observations provided by the california department of water resources dwr www water ca gov waterdatalibrary while 19 845 groundwater sites wells that are providing groundwater level data are available over the entire region of central valley 9739 wells provide data for the period 2002 16 a major shortcoming of in situ groundwater data is that they are typically characterized by highly varying sampling frequencies ranging from daily to only one measurement in several years which along with the fact that each well site covers different time periods make the use of this dataset an intriguing challenge to ensure therefore the availability of high quality continuous annual data selecting shorter but of great importance to the region periods was deemed necessary the two latest drought periods in california 2007 09 and 2011 15 were chosen for validation of the model as they had a great impact on the state mann and gleick 2015 and hence are of significant hydrologic importance only those wells providing annual data for the same month of three 2007 09 or five 2011 15 consecutive years were selected moreover for wells that are characterized by two values per year one observation made within the first half of the year and one within the second half only the latter was used wells that fall within the same pixel are averaged the final selection resulted in 1285 wells for the 2007 09 drought period and 292 wells for the most recent 2011 15 drought period all well sites are located within the low elevation central valley region of california the exact geographic location of each well site for both of the investigated periods is shown in fig 5 in both cases the well locations are spatially uniformly distributed among the three basins of the domain the validation of vic 4l groundwater outputs was conducted for both aforementioned drought periods at the annual scale and after converting vic 4l groundwater output units from millimeters of water equivalent height to depth to water level in meters below the land surface as indicated in section 3 5 similarly the dwr well measurements were also converted from feet to meters of depth to water level to ensure like to like comparisons all unit transformations are being accommodated by a groundwater validation code module which also identifies the exact latitude and longitude values of each well site that is situated within the central valley domain and based on this information locates the corresponding pixel unique 0 0625 0 0625 area that encompasses the station s geographic location moreover the module identifies the observation date of the in situ data and corresponds it to the exact daily time step of the respective grid cell of the simulated three dimensional wtd matrix annual observations from each station are then compared to the model simulated wtd data before and after grace direct insertion and validation metrics or quantities of interest used for comparison are calculated both for each station separately as well as at larger scales specifically at the river basin level sacramento valley san joaquin and tulare basins by summarizing the metrics variables over these regions grid cells of the simulation domain falling outside central valley mostly high elevation regions in the north and east were excluded using a mask validation metrics used for comparison are correlation coefficient cc wtd change wtdenddate wtdstartdate and wtd slope linear least squares slope of wtd time series 4 results 4 1 direct insertion of grace observations fig 6 presents the effect of grace direct insertion in the form of maps of tws over the central valley at the daily model resolution the temporal daily trend of tws was calculated as the linear least squares slope for each pixel and for the entire 14 year period april 2002 april 2016 the statistical significance of the trend for each pixel was also calculated the tws daily trend derived from the ol run fig 6a is statistically significant p 0 0001 over the entire region of central valley and shows negative slopes decreasing tws over most of the domain with a few exceptions in the mountainous regions on the east and north areas in the great valley are generally characterized by smaller negative values with very small spatial variability indicating changes of smaller magnitude moreover regions of the great valley along the eastern transition line between low and high elevation areas exhibit positive slopes which means that the tws is overall increasing over these areas the mountainous regions display a greater spatial variability although the great majority of pixels exhibit lower negative values of slope this phenomenon may be attributed to orographic precipitation in the form of rain or snow owing to the very complex topography of the study domain precipitation varies greatly especially along the longitudinal axis as the terrain changes from west to east from low elevation and moderate sloping relief to significantly steeper and higher elevation ground as such eastern higher altitudes are subject to more frequent precipitation events most of which are regional orographically induced rain or snow resulting in the overall positive trends displayed in these regions the grace integration run yields tws values that are overall characterized by stronger trends that are also statistically significant p 0 0001 fig 6b over most of the central valley tws exhibits stronger negative slopes compared to those of the ol run specifically the mountains on the east and north west are characterized by greater in terms of absolute value negative slopes than the great valley regions while the foothills that showed small increase in tws with time from the ol run exhibit negative values for the most part in contrast grace integration had a different effect in the north where most pixels are characterized by positive or near zero slope values the temporal variation of tws for each pixel is shown in terms of standard deviation both before fig 6c and after fig 6d direct insertion areas of high elevation are characterized by greater temporal variability higher standard deviation values in terms of tws while the opposite is true over the valley regions of california the direct insertion increases the standard deviation values over the valleys while the opposite impact is true for the complex terrain areas mountains the difference between the standard deviation of the daily grace integration tws and the standard deviation of the daily ol tws is shown in fig 6e the bipolar effect of the direct insertion is evident reducing the standard deviation over the mountains while increasing it over the valleys fig 7 shows the temporal trends of tws for the period april 2002 june 2016 with and without grace integration for the three major basins of central valley the spatially averaged monthly grace observations of tws anomalies twsa are also shown along the grace integration tws curve for each basin in all three cases the model blue curve captures both intra and inter annual variability in terms of tws as the unusually wet winter of 2011 as well as the recent multi year drought period 2012 16 are clearly shown the grace integration tws daily time series red curve shows little improvement over sacramento river basin while its effect is more pronounced over the san joaquin and tulare basins more specifically in the two latter basins direct insertion increases tws during the 2003 07 period while the opposite is true during 2011 16 the grace data integration impact is negligible during the period 2008 10 a point to note is that a comparison among the three basins reveals that the impact of direct insertion increases with increasing dryness of the region the same is also true during periods of stronger deviations of the average hydrologic patterns this finding emphasizes the importance of grace in improving hydrologic modeling skill via the implementation of a computationally efficient assimilation technique i e direct insertion as these observations add a critical piece of information that these models fail to account for groundwater depletion related to irrigated agriculture which is often the case in dry regions or generally during periods of water shortages the effect of grace direct insertion in terms of wtd is presented in fig 8 fig 8a shows the vic simulated average daily wtd for the period april 1 2002 june 30 2016 over central valley as expected great depths for water table are shown in the mountainous regions whereas the valley overall exhibits low wtd values fig 8b presents the average daily wtd difference in meters below the ground surface between grace direct insertion and ol for most of the study domain direct insertion increases wtd in other words integrating grace observations deepens the water table the northern region of central valley seems to be characterized by a consistently wtd decreasing effect of grace data integration this could be attributed to either a model related discrepancy or a problematic retrieval indicating consistently low tws anomaly of grace over this region potentially due to certain topographical features or the existence of water bodies the daily trend of the ol wtd was also calculated as the linear least squares slope for each pixel and for the entire 14 year period april 2002 april 2016 fig 8c the statistical significance of the trend for each pixel was also calculated overall positive slopes retreating water table characterize the high elevation regions in the east and northwest while the foothills on the west side of the mountains show negative trends in wtd rising water table the valleys exhibit slightly positive values in terms of slope and present very little spatial variation all pixels are characterized by statistically significant p 0 0001 trends the effect of grace direct insertion on the wtd trends is shown in fig 8d all regions characterized by positive slopes in the ol run are characterized by stronger trends that are also statistically significant p 0 0001 with greater positive slopes moreover the foothills of the mountains exhibit mostly positive trends and the spatial variability in the mountainous regions is reduced compared to the ol run in addition direct insertion has a varying wtd increasing effect in the low elevation regions especially within central valley in contrast many regions in the northeast of the investigated domain exhibit a wtd decreasing trend after grace data integration indicating an overall rise in the water table during the investigated 14 year period the standard deviation of the daily wtd differences between grace direct insertion and ol is shown in fig 8e in all mountainous regions the effect of grace integration has greater temporal variations than that in the low elevation valley areas moreover the standard deviation of the differences varies significantly over the complex terrain regions indicating a significant effect of topography on grace data integration comparing the temporal variability in terms of standard deviation between the grace direct insertion and ol wtd for each pixel shows that the integration of the satellite observations lowers the temporal variations of wtd over the high elevation areas while the opposite is true for low elevation pixels fig 8f the temporal evolution of the wtd averaged over the entire central valley is shown in fig 9 top before and after grace direct insertion the time series of the daily spatially averaged wtd simulated by vic ol shows a decrease of wtd rise in the water table in the winter of 2006 followed by an increase of wtd drop in the water table throughout the next 3 years in 2011 wtd once again decreases because of the wet winter of that year and during the drought period of 2012 16 wtd exhibited an increase the direct insertion run shows similar patterns however its effect on the wtd is clearly decreasing during the first four years 2002 06 and increasing during the recent drought years evidently as shown in this graph corrections due to grace observations are of greater magnitude during periods characterized by stronger deviations from the average hydrologic conditions fig 9 bottom also shows the time series of the daily wtd difference in meters below the ground surface between the grace integration and ol runs the difference is clearly minimal during the period 2008 10 indicating a small impact of the grace observations on the other hand its negative positive values during the first last simulation years show that grace data can be of significant value to studies for periods characterized by extreme hydrologic conditions in this case droughts 4 2 validation with well measurements fig 10 presents the distribution of the cc values calculated between wtd derived from the dwr wells and that of the model simulation specific grid cell that encompasses the well location before and after grace integration for the two drought periods of 2007 09 and 2011 15 during both time periods the vast majority of wells are characterized by high cc values ranging from 0 6 to 1 0 the number of wells exhibiting negative cc values is smaller in the longer drought period as expected when grace observations are integrated cc values are in general higher especially in the upper positive ranges specifically although the direct insertion does not seem to affect negative correlations positive ones are shifted toward higher values with a significant increase in the 0 8 1 0 bin most of the other categories of positive cc values also present an increase however rather small the positive effect of grace integration is also evident in fig 11 most wells are characterized by a positive change in the cc value following the direct insertion during both investigated drought periods in other words grace data integration improves the agreement of the temporal trends especially in regions for which the model captures well the temporal variability of groundwater level this is true for both drought periods with a larger impact during the 2007 09 period fig 12 shows the correlation via the use of scatter plots between wtd change and slope describing the trend derived from the ol simulation and that of grace direct insertion for both of the investigated drought periods these are calculated only for those grid cells encompassing the well locations in central valley and are inter compared to provide a more holistic picture of the grace data integration impact on the model s performance overall the direct insertion seems to increase the temporal variability of the simulated wtd by a factor of 3 or 4 while similarly there is a 3 fold increase of the slope values to further investigate the effect of grace direct insertion from a more quantitative perspective the above parameters wtd change slope were calculated by averaging the wtd values derived from wells or model simulations over each of the major river basins within the central valley not including surrounding regions of higher elevation in the greater direct insertion domain and inter compared table 1 presents these values for both drought periods of 2007 09 and 2011 15 and for each of the three basins in central valley for both periods wells are rather uniformly distributed among the three regions specifically for the 2007 09 period 66 wells are situated in sacramento river 395 wells in san joaquin river and 824 wells in tulare while for 2011 15 these numbers are 71 107 and 114 respectively the change and slope of wtd as well as the cc between the wtd derived by the wells and that derived by the model before and after direct insertion were calculated after averaging the wells over each basin and all the model grid cells regardless of whether a well is situated in them or not that fall within the central valley region of each of the three major river basins of california in terms of wtd change grace data integration improves the model s performance by increasing the model s temporal variability of groundwater and thus better capturing the inter annual changes in the wtd more specifically the ol simulation resulted in a wtd change of 0 0825 m indicating a decline in the water level in sacramento river over the period 2007 09 while the respective average well derived wtd change was 0 696 m with grace direct insertion this difference was reduced by 30 while a 47 improvement was observed for the same period over the much drier tulare similarly the wtd slope derived by the ol simulations is substantially smaller than that derived by the wells however integration improvement in all cases ranges from 25 to more than 50 moreover on average cc values are high in all cases except sacramento river during 2007 09 and increase after the direct insertion with the exception of san joaquin river in 2007 09 which exhibits a minor decrease in the cc value after grace data integration the higher impact of grace direct insertion in terms of improving the agreement between temporal trends was observed in sacramento river for both drought periods by and large however grace data integration has the greatest positive effect on the driest basin in central valley i e tulare basin 5 conclusions in the monitoring of groundwater resources by local water managers day to day fluctuations in wtd are less important than seasonal and longer variability and trends in the state of groundwater storage these trends help to characterize the potential rate of depletion over long time frames in response to changes in the natural supply of water e g during drought and changes in the depletion of water due to variable human consumption in response to this conceptual framework this study presents a simple yet novel methodology to enable data fusion of grace tws observations to the augmented version of a macroscale hydrologic model vic 4l ultimately allowing an improved representation of the long period i e greater than monthly variability in the wtd at the local scale such information specifically the long term temporal variability of wtd is of immense value to water resource managers moreover the wtd is an indication of groundwater quantity and therefore quality fluctuations with significant impacts on the environment and human welfare in addition this study provides holistic insight into the temporal fluctuations of both surface water and deep groundwater at high spatial resolution and this information can be used toward long term water resources management and planning as well as climate change adaptation additionally the results of this analysis provide an opportunity to examine the effect of grace observations on hydrologic simulations over the investigated topographically and climatologically very complex region of the three major river basins in california including central valley specifically corrections due to grace observations are of greater magnitude during periods characterized by stronger deviations from the average hydrologic conditions moreover the effect of direct insertion is greater over high elevation regions while in all mountainous regions the model simulated tws has greater temporal variations than that in the low elevation valley areas furthermore hydrologic representations in drier regions in central valley such as the tulare basin were affected to a greater extent by grace data integration compared to those in wetter regions indicating the significant value of grace observations in accounting for large scale groundwater depletion in heavily irrigated regions 10 times more intense than in other regions of the central valley especially during drought periods scanlon et al 2012 faunt 2009 in addition although not shown here the total simulated with direct insertion groundwater volume rate yielded 6 3 km3 year which is in agreement with the findings of famiglietti et al 2011 who calculated the total central valley groundwater depletion rate to be 6 1 km3 year despite the relatively limited time period of grace coverage 15 years all basins showed improvement in the model s skill after grace data integration in terms of capturing the long term trends of seasonal inter annual variability and thus better representing the response of groundwater to natural or human made alterations of the hydrologic cycle although irrigation was not accounted for in the model integrating grace observations with vic simulations improved the model s performance by more accurately representing the changes in tws owing to the fact that grace anomalies introduce the anthropogenic effect e g groundwater pumping irrigation into hydrologic simulations a more realistic hydrologic representation was achieved these findings are derived through the comparisons of individual pixels with wells that are situated within that 0 0625 0 0625 area and which do not accurately represent the mean area wtd nevertheless well observations are currently the best available source of data that can be used to validate groundwater simulations such findings increase confidence in application of grace for monitoring groundwater in hydrologically vulnerable regions or areas that are frequently subject to extreme hydrologic conditions droughts or floods however the existence of dense well networks particularly over water stressed regions where groundwater depletion occurs at high rates during droughts that continuously monitor water table levels is also of paramount importance these networks would provide significantly improved information for estimating groundwater storage which would in turn be integrated into advanced hydrologic models thus resulting in hydrologic state estimates of higher accuracy and resolution although various model development efforts koster et al 2000 seuffert et al 2002 maxwell et al 2007 niu et al 2007 kollet and maxwell 2006 kollet and maxwell 2008a clark et al 2015 aim at simulating groundwater storage variations below the land surface they very frequently exhibit a behavior similar to that of a bucket model other model coupling efforts kollet and maxwell 2008a are plagued by the disparity in spatial scales and resolutions as well as the high computational demands this study provides a holistic insight into reproducing water storage variability not only in shallow soils but also in deeper groundwater by jointly assessing the temporal fluctuations of water in both surface as well as deep aquifers at high spatial resolution moreover integration of grace observations enables a more accurate representation of wtd especially in regions where groundwater extractions and climatological factors confound any effort to estimate wtd thus providing information of utmost importance for long term water resources management and planning in arid or semi arid regions currently missing from the existing framework is an optimal assimilation technique such as ensemble kalman smoother enks or ensemble kalman filter enkf such techniques can replace the direct insertion scheme currently used for grace assimilation to encompass the uncertainty in both model estimates and observations thus achieving a better representation of the wtd moreover grace observations are characterized by spatially correlated errors which often impact assimilation results removal of or accounting for such errors is therefore crucial for achieving more realistic assimilation schemes khaki et al 2018a b piretzidis et al 2018 and should be considered for future grace assimilation efforts our main premise is that apart from improving the assimilation scheme the current work can further be augmented by also adjusting the model parameters that govern groundwater flow e g hydraulic conductivity as well as expanding the direct insertion domain for the updated grace assimilation framework via its implementation over the entire region of the western united states this will be of critical importance as among the main characteristics of this region are groundwater extractions which lower the water table irrigation increasing evapotranspiration as well as land use changes which further confound the effort to realistically estimate wtd especially under extreme hydrologic conditions moreover the recently launched grace follow on satellites will extend the existing 15 year data record and thus enable a greater number of their applications the improved representation of the wtd that will be provided through grace assimilation will better characterize the spatial heterogeneity and temporal variations of the interlinked surface and subsurface water dynamics such approach will ultimately lead to the more reliable estimation of water storage which is of utmost importance for quantifying and managing available water resources especially in the water stressed regions of the western united states acknowledgments the research described in this paper was carried out at the jet propulsion laboratory california institute of technology under a contract with the national aeronautics and space administration this work was supported by nasa research announcements nra nnx12ak64g we also acknowledge the editor and the three anonymous reviewers for their constructive criticism and helpful comments that enabled us to greatly improve this manuscript 
652,a method to link bivariate statistical analysis and hydrodynamic modeling for flood hazard estimation in tidal channels and estuaries is presented and discussed for the general case where flood hazards are linked to upstream riverine discharge q and downstream ocean level h using a bivariate approach there are many possible combinations of q and h that jointly reflect a specific return period t raising questions about the best choice as boundary forcing in a hydrodynamic model we show first of all how possible q and h values depend on whether the definition of t corresponds to the probability of exceedance of h or q or h and q we also show that flood hazards defined by or return periods are more conservative than and return periods finally we introduce a new composite water surface profile to represent the spatially distributed hazard for return period t the composite profile synthesizes hydrodynamic model results from the and hazard scenario and two scenarios based on traditional univariate analysis a marginal q scenario and a marginal h scenario keywords coastal flood hazards bivariate statistical analysis hydrodynamic modeling compound flooding 1 introduction flood risk is increasing in coastal cities around the world due to several factors including population growth economic development sea level rise subsidence land use changes and intensification of rainfall hallegatte et al 2013 hanson et al 2011 by the year 2100 between 0 2 4 6 of global population and 0 3 9 3 of global gross domestic product may be exposed to coastal flooding if no adaptation occurs hinkel et al 2014 management of flood risk relies on statistical and hydrodynamic modeling to delineate populations and assets exposed to flooding anticipate and monetize the consequences of flooding and develop cost effective and socially robust interventions including infrastructure projects insurance programs land use and building code policy changes and emergency preparedness and response measures sayers et al 2013 luke et al 2018 to address risks statistical and hydrodynamic modeling is linked to delineate spatial fields of the intensity of flooding e g depth and velocity for a set of exceedance probabilities information which is subsequently used to estimate average annual losses based on exposed assets and their vulnerability to damage scawthorn et al 2006 the linking of statistical and hydrodynamic modeling is straightforward when addressing a single hazard such as river discharge q flood risk is modeled by first performing univariate frequency analysis of annual maximum discharge to estimate extreme values q t for yearly return periods t e g q 100 for 100 year return period discharge here the hat notation indicates annual maximum discharge and the subscript refers to the return period second hydrodynamic modeling is performed with q t as a boundary condition to characterize spatial fields of water surface elevation at each return period η t x where x represents distance along the river fema 2018 however coastal hazard assessment must account for interaction of river flooding intense rainfall storm surge and waves and the likelihood of a coincidence in extreme and non extreme levels of these hazards which is also known as compounding effects gallien et al 2018 moftakhari et al 2017 one of the most important compounding effects is the interaction of river discharge and the downstream ocean level h in tidal channels and estuaries of the world s 32 largest cities 22 are located on estuaries ross 1995 at which the interactions between q and h play a major role in flood risk estimation ward et al 2018 in the u s alone 140 million people 50 of total population live on the coast in close proximity to an estuary kennish 2004 fig 1 illustrates the estuarine flood hazard problem the objective is to estimate spatially distributed extreme water levels η t x for return period t in a tidal channel or estuary we assume knowledge of the system geometry e g bed elevation channel width and shape and resistance to flow e g manning resistance coefficient we also assume that gauges provide time series records of boundary conditions river discharge measurements representative of what enters the reach q t and water level measurements h t representative of the downstream end of the reach hence the key question becomes how can statistical and hydrodynamic modeling be linked for the estuarine setting involving two gage records characterizing two different aspects of hydrodynamic extremes put another way can the existing paradigm of univariate flood hazard modeling described above for rivers be extended to account for a second gage in a relatively simple and straightforward way we write this mathematically as follows 1 η t x f q t h t p x where p x represents the parameters describing the channel geometry and resistance properties note that eq 1 can be extended for two dimensional flood hazard levels by interpreting x as two dimensional vector representing geographical coordinates in the absence of robust models for extreme water levels in estuaries overly simplistic bathtub models have received widespread use for estimating coastal flooding hazards and the resulting human exposure at regional torresan et al 2012 and national international levels dasgupta et al 2011 hinkel et al 2010 bathtub models simply take an estimated extreme water level and extrapolate it inland to estimate population and assets exposed to flooding which neglects the potential for flood stage to change with distance inland as a consequence of riverine forcing and or tidal damping amplification lanzoni and seminara 1998 this points to the potential for underestimation of flood consequences on the other hand bathtub models may also overestimate flood consequences by failing to account for flood defenses and the role of friction inertia and storage in flooding dynamics gallien et al 2014 sanders 2017 by linking statistical analysis and hydrodynamic models more robust estimates of extreme water levels become possible as well as mechanistic routing of flood water into adjacent urban areas to estimate flood impacts gallien et al 2014 2011 existing methods for flood hazard assessment solving eq 1 in tidal channels are limited hoitink and jay 2016 in particular for the case where q t and h t are statistically independent fema 2015 recommends the following procedure to estimate the hazard for return period t 1 univariate analysis of annual maximum river discharge to estimate q t and univariate analysis of annual maximum total water level to estimate h t 2 a pair of hydrodynamic model simulations with one forced by q t and a non extreme h value usually chosen as mean higher high water and the other forced by h t and a non extreme q value and 3 synthesis of the two hydrodynamic model simulations based on the pointwise maximum water level across the two simulations an appealing aspect of the fema approach is that only two hydrodynamic simulations are required for each return period which is important because resources for flood mapping are limited burby 2001 and because hydrodynamic flood simulation is computationally demanding i e many hours for one simulation especially for urban areas where fine resolution grids are needed to accurately depict flooding e g gallien et al 2011 2014 hence the fema 2015 approach is aligned with needs for simple and efficient assessment approaches nevertheless there are significant limitations for example univariate statistical analysis is not appropriate when q t and h t exhibit statistical dependence also known as compound risks leonard et al 2014 moftakhari et al 2017 zscheischler et al 2018 additionally even when q t and h t are independent extreme water levels may occur over the length of the tidal reach due to the interaction of non extreme boundary forcing values fema 2015 2018 does not presently offer guidance to address this situation broadly the fema 2015 guidance recommends multi hazard assessment based on the predominant hazards yet limitations of this approach are increasingly being recognized hillier et al 2015 the aforementioned challenges of linking statistical and hydrodynamic modeling can only be partly overcome with improved access to and reduced costs of high performance computing systems that map flood hazards through monte carlo simulation that is monte carlo simulation can be applied to depict thousands of scenarios based on different combinations of q t and h t and depict spatially varied flood hazards based on the frequency of the pointwise exceedance of a water level threshold purvis et al 2008 however bivariate statistical analysis is needed in place of univariate analysis to properly describe the correlation structure of the hazard drivers and for sampling representative combinations of hazard drivers in monte carlo simulations the objective of this paper is to present a solution method for eq 1 that accounts for statistical correlation structure and physical compounding effects e g backwater between boundary forcing values q t and h t while using only a small number of hydrodynamic model simulations building on the existing methodology recommended by fema 2015 we present a four step method as follows 1 bivariate statistical analysis of q and h records to yield possible q t h t pairs for return period t 2 selection of n specific q t h t pairs for hydrodynamic modeling here we recommend n 4 more detail will follow although other options are possible 3 hydrodynamic modeling of n scenarios defined by q t h t pairs identified in step 1 to yield spatial distributions of extreme water levels η t i x i 1 n note that the subscript on η references return period and the superscript references the scenario 4 synthesis of hydrodynamic modeling results η t i x i 1 n to yield η t x the remainder of the paper presents this method in detail along with applications section 2 presents methods and materials including data used in this study the bivariate statistical analysis methods to determine all possible q t h t pairs identification of four specific pairs useful for hydrodynamic modeling and methods for one dimensional steady flow analysis and two dimensional unsteady analysis of extreme water levels section 3 presents results of one dimensional steady flow analysis from several sites showing differences in water level profiles arising from the q t h t pairs and results of two dimensional unsteady analysis at a single site where we further examine the limitations of the 1d modeling and the added benefits of 2d modeling for assessment of coastal flood hazards here we also introduce an extension of the fema 2015 method that offers potential to systematically improve the assessment of coastal flood hazards by linking bivariate statistical analysis and hydrodynamic modeling we close the paper with discussion section 4 and conclusions section 5 broadly this work shows that extending the univariate paradigm of river flood hazard assessment to a bivariate paradigm of coastal flood hazard assessment is not straightforward as has been reported for many other types of compound hazards kappes et al 2012 nevertheless a systematic approach is possible and shown herein furthermore results point to the possibility that the existing fema approach underestimates flood hazards where compounding effects are strong and we present a simple method to make a better estimate 2 methods and materials 2 1 data analysis herein focuses on tidal channels estuaries in southern california see fig 2 where flood hazards are affected by extreme ocean levels and flood discharges the los angeles river lar the santa ana river sar and newport bay nb for ocean level analysis hourly ocean water level measurements were obtained from the national oceanic and atmospheric administration noaa los angeles tide gauge gauge id 9410660 at hourly intervals tide gauge measurements capture water level fluctuations from combined effects of tides storm surge and other factors that affect sea levels on hourly and longer time scales a reading that is sometimes called total water level twl discharge measurements for the lar were obtained from los angeles county department of public works ladpw station f319 r lar at wardlow road and consisted of 92 years of annual maximum discharge data between 1928 2014 discharge measurements for the sar were obtained from usgs gauge 11078000 santa ana river at santa ana and consisted of 94 years of annual maximum discharge data between 1923 and 2017 river discharge measurements for newport bay were obtained from the orange county department of environmental resources gauge 226 san diego creek at campus drive and consists of 39 years of instantaneous discharge 1978 2016 the san gabriel river and coyote creek see fig 2 are not considered in this study since bivariate statistical analysis see section 3 1 showed no correlation between q and h likely because of strong flow regulation from whittier narrows dam located approximately 30 km from the coastline topographic and bathymetric data for lar and sar were taken from the 1 m resolution 2014 us army corps of engineers national coastal mapping program topobathy lidar dem topographic and bathymetric data for nb were based a dem reported by gallien et al 2011 which merged several sources of topographic and bathymetric data 2 2 bivariate statistical analysis statistical analysis of extreme values of discharge and water level impacting a tidal reach or estuary are based on records of annual maximum values although threshold based approaches are also possible here we use a hat notation to indicate annual maxima data from the records of upstream discharge and downstream water level q and h respectively record lengths of several decades or more are preferred to enable estimation of water levels at relatively low frequencies e g return periods of 50 years or greater bivariate statistical analysis begins with a test for correlation structure while either linear or rank correlation coefficient measures can be used to assess the significance of the dependence between variables tail dependence measures are important for summarizing how extremes tend to occur simultaneously coles et al 1999 hao and singh 2016 here we employ joint density approaches over other alternatives hawkes et al 2002 heffernan and tawn 2004 neal et al 2013 zheng et al 2015 due to their flexibility and computational mathematical benefits hawkes 2006 salvadori et al 2015 according to sklar s theorem sklar 1959 there exists a bivariate copula function c q h 0 1 0 1 0 1 that formulates the joint distribution f q h of the pair q h with marginal distributions f q and f h for all q h r 2 as 2 f q h q h c q h f q q f h h the multivariate model is constructed by fitting suitable univariate laws on the marginals and an appropriate copula on the observed pairs genest and favre 2007 salvadori et al 2007 here we use the method of sadegh et al 2018 which comprehensively analyzes the dependence structure of multiple drivers of flooding and models them using copula functions to estimate return design values and their underlying uncertainties this approach first selects a marginal distribution from 17 univariate distributions based on measures of goodness of fit including akaike information criterion aic and bayesian information criterion bic and then chooses a copula model from 26 copula functions copula model parameters are inferred through a bayesian inference approach with markov chain monte carlo sadegh et al 2018 2017 the joint probability can refer to the exceedance of q and h or the exceedance of q or h salvadori et al 2016 and a case can be made for the relevance of both to coastal flood hazard assessment first of all risk assessment should reflect the possibility that flooding is caused by either extreme river discharge or extreme ocean levels which is consistent with the or scenario on the other hand hydrodynamic modeling involves the simultaneous occurrence of an upstream discharge and downstream water level consistent with the and scenario newport bay data are used to illustrate this process fig 3 presents the outcome of bivariate statistical analysis using both the or and the and hazard scenarios using the method of sadegh et al 2018 there is no statistically significant correlation between river flow and ocean water level at newport bay but correlation was found between river flow and non tidal residual ntr defined as the difference between twl and the astronomical tide level hence bivariate statistical analysis that takes correlation structure into account is presented here using river flow and ntr fig 3 illustrates the similarities and differences between univariate and bivariate statistical analysis as well as the relative complexity introduced by the copula based and and or hazard scenarios in particular fig 3 shows plots of the marginal distributions of ntr fig 3a and river flow fig 3c representative of what has traditionally been used for univariate flood hazard assessment while fig 3b shows the copula based and and or hazard scenarios the and and or hazard scenarios are shown as iso return period curve for t 50 year within a two dimensional space whereby every point corresponds to a possible q t h t pair for use in hydrodynamic modeling additionally along each iso return period curve there is a point of maximum probability density which represents the most likely q t h t pairs given the correlation structure note that the most likely q t h t pair for the or hazard scenario exceeds extreme values given by the marginal distributions while the most likely q t h t pair for the and hazard scenario falls below the values given by the marginal distributions this shows that the or hazard scenario will lead to boundary forcing that is more conservative meaning a more cautious approach to risk management perspective than the and hazard scenario given theoretical characteristics of the or iso return period curves salvadori et al 2016 boundary forcing associated with q t h t pairs at the ends of the or curve may far exceed values given by the marginal distributions for the same return period this shows that the or scenario creates seemingly unrealistic highly conservative hazard scenarios in areas of low probability density hydrodynamic modeling of compound flood hazards for return period t is proposed based on four specific q t h t pairs taken from bivariate statistical analysis as shown in fig 3 s1 a marginal q scenario defined by the t year return period river discharge and a non extreme water level downstream typically taken as mean higher high water s2 a marginal h scenario defined by the t year return period ocean water level and a non extreme river flow typically taken as the daily average flow s3 an and scenario based on the q t h t pair with the highest probability density along the and iso return period curve s4 an or scenario based on the q t h t pair with the highest probability density along the or iso return period curve fema 2015 presently recommends hydrodynamic modeling of s1 and s2 and estimation of η t x based on the maximum of the two hence two additional scenarios s3 and s4 are considered here as a way of leveraging bivariate statistical analysis 2 3 hydrodynamic modeling dynamic changes in water surface elevation within estuaries can be modeled with reasonable accuracy using shallow water hydrodynamic models that assume a constant fluid density and a depth averaged horizontal velocity sanders et al 2010 fema 2015 2018 estuaries involve the mixing of riverine and ocean water with different densities and may be characterized by strong vertical density stratification that acts as a major control on the velocity distribution and transport geyer 2010 monismith 2010 consequently three dimensional models that account for variable density from salinity and temperature are often needed to estimate velocity distributions jay 2010 however the expression of density effects on surface water elevation is weak and can be neglected when predicting extreme water levels for the purpose of flood hazard modeling friedrichs 2010 in this study water levels are modeled by solving one dimensional constant density steady state shallow water models chow 2009 and two dimensional constant density depth averaged shallow water equations kim et al 2015 when a model is set up for estuaries tidal embayments or tidal channels the required boundary conditions correspond to a time series of river discharge at the upstream boundary q t and a time series of water level h t at the downstream boundary the upstream and downstream boundaries of the modeled spatial domain are generally placed adequately apart that compounding effects are avoided this is not however always possible in practice because tide gauges may be located within estuaries as an aside we note that two dimensional models require a spatial distribution of boundary forcing and in practice hydrodynamic models include methods to distribute the total volumetric flow rate q t across the inflow boundary while the water level h t is typically assumed to be uniform across the outflow boundary estuaries may also experience water level variability from internal forcing by winds and waves and accounting for these effects is outside the scope of this study however the role of regional winds waves and atmospheric pressure on water levels is captured by this approach based on the measurement of water levels at the tide gauge used for bivariate flood hazard assessment 2 3 1 1d steady state modeling one dimensional 1d steady state modeling of coastal flood hazards is useful as a first approximation of flood hazard levels along tidal channels and estuaries and can be done quickly with low computational effort application of 1d analysis at several sites is performed to study how differences in the selection process for q t h t pairs can affect the estimation of flood hazard levels flood hazard levels are computed by solving the gradually varied flow equation under the assumption of a rectangular channel with spatially variable manning nm width w and depth h as follows chow 2009 3 d η d x s f 1 f r 2 where x represents distance measured inland from the mouth of the estuary fr q gh3w2 1 2 is the froude number and sf nm q w 2 h10 3 represents the friction slope eq 3 is integrated with geometrical data for each site nm 0 032 m 1 3 s the downstream water level boundary given by h and a river discharge given by q numerical integration is performed with the 4th 5th order runge kutta scheme ode45 supported by matlab mathworks natick ma we note that the relatively simple channel geometry and resistance approximation is used herein to examine the relative differences between profiles from scenarios s1 s4 and not to estimate flood hazard along these rivers in an absolute sense more detailed geometry and resistance modeling will change the absolute value of flood hazard heights but have little impact on the relative difference between scenarios 2 3 2 2d unsteady modeling two dimensional 2d unsteady modeling is performed at one of the four sites newport bay to characterize limitations of the 1d steady state approximation and to study how the relative timing of the flood peak and high tide level can affect the flood hazard characterization the 2d model brezo begnudelli et al 2008 kim et al 2015 is applied based on a previous validation at newport bay gallien et al 2011 2014 brezo relies on an unstructured mesh of triangular elements with varying size to capture the bay s topography and bathymetry the model was originally setup to predict flood impacts in the urbanized portions of the newport bay city of newport beach california 2008 it features a fine resolution mesh with 3 m average linear resolution cells across streets and land parcels while across the upper newport bay the average linear cell resolution is approximately 15 m the boundary conditions in brezo are setup to specify the riverine discharge q entering the upper newport bay at the outlet of the san diego creek while water level h is specified along a boundary placed a short distance offshore of the embayment for unsteady analysis riverine discharge entering newport bay was modeled with a triangular hydrograph with a peak value given by q and a time of rise and total flood duration set to 3 and 8 h respectively based on analysis of instantaneous discharge measurements additionally ocean water level changes were modeled using a sinusoidal function with a 12 h period based on semidiurnal tides such that the maximum ocean water level equals h these approximations followed preliminary modeling which demonstrated that flood heights in the upper bay were much more sensitive to the magnitude of the peak flow than the duration of the event within the range of observed values and that high water levels were not sensitive to the precise shape of the tidal forcing we note that this may not be true in all systems and thus these approximations are not presented as a generalization but rather as a reasonable simplification given specific site conditions fig 4 presents the sinusoidal ocean forcing and the triangular inflow hydrograph to report the sensitivity of maximum water levels to the relative timing of the peak inflow and peak high tide the or hazard scenario was repeated using an inflow hydrograph that was shifted forward and backwater by as much as 6 h as shown in fig 4 all modeling results are expressed in metric units and referenced to the nad83 state plane horizontal coordinate system and navd88 vertical datum 3 results 3 1 bivariate statistical analysis correlation analysis between q and h defined by twl revealed no statistical significance at these southern california sites due to relatively small storm surges compared to variability in high tide levels attributed to astronomical factors however correlation was found using ntr as a surrogate for h at three of the four sites considered lar sar and nb kendall tau and spearman rho correlation coefficients between variables q and h defined using ntr are presented in table 1 along with p values a p value of less than 0 05 suggests a correlation at 5 significance level table 1 also shows the distribution functions that best describe the univariate distribution of river flow and ntr additionally the joe bivariate copula function was found to best describe the correlation structure between q and h in all sites flood hazard levels q and h for t 50 year for scenarios s1 s4 are presented in table 2 note that the marginal q scenarios all use a downstream water level corresponding to mean higher high water and all of the marginal h scenarios use a small relative to the extreme flows river discharge average daily flow taken as 10 m3 s these results show that the lar has larger river discharge values than sar and nb yet somewhat surprisingly nb has larger river discharge values than sar despite a much smaller watershed area this is attributed to control of runoff by dams also note that the water level of the marginal h scenario differs between the three sites despite all three relying on the los angeles tide gauge this is attributed to differences in the record length arising from joint probability analysis of tide gauge data and river gauge data 3 2 1d flood hazard analysis 1d steady state water surface profiles were computed for the 12 sets of q t h t pairs presented in table 2 corresponding to s1 s4 at lar sar and nb and are presented in fig 5 a c and e respectively focusing first on the marginal h and marginal q scenarios these results shows that there is a transition in the dominant factor controlling flood hazard levels along the length of the system with h controlling flood hazards near the outlet and q controlling flood hazards further inland the length of oceanic control is relatively long for nb 4 km and lar 2 km and relatively short sar 300 m water levels from the and scenario are lower than the higher of the two marginal scenarios at inflow and outflow boundaries but higher within an interior region where the marginal profiles intersect on the other hand the or scenario yields a water surface profile that is always above both marginal scenario profiles conceptually these results show that the or scenario represents a more conservative i e cautious representation of the spatially variable water surface profile associated with return period t than the and scenario which is expected based on the magnitude of the boundary forcing see table 2 fema 2015 guidance recommends mapping of flood hazard levels in tidally affected reaches based on the pointwise maximum of the two marginal scenarios this profile labeled fema is presented in fig 5b d and f for lar sar and nb respectively alongside the and or and a proposed composite profile based on the pointwise maximum of the h marginal q marginal and and hazard scenarios at all three sites the fema method underestimates the t year return period water level compared to the and scenario within a region where the marginal water surface profiles intersect hence the proposed composite profile is slightly higher than the fema method profile where there are physical compounding effects due to the interaction of river and oceanic influences on water levels otherwise the proposed composite profile tracks the fema method profile 3 3 2d flood hazard analysis two dimensional modeling of flood hazards in newport bay leads to spatially and temporally distributed water levels hence flood hazards are mapped based on the point maximum water level attained over an unsteady simulation covering the rise and fall of a flood peak with magnitude q and the rise and fall of an ocean tide of height h results are presented first for the case of the temporal coincidence in the two peaks and later the sensitivity of the results to the time lag between peaks is shown a comparison of extreme water level scenarios s1 s4 along the main channel of newport bay using 1d and 2d methods is shown in fig 6 for each scenario the modeling method has little impact on water profiles in the lower bay x 4000 m while differences are evident in upper bay x 4000 m and attributed mainly to differences in the treatment of complex system topography bathymetry that is the 1d model assumes a rectangular cross sectional with a width and depth based on the main channel while the 2d model resolves both channel and floodplain marsh topography allowing for greater conveyance at higher flood stages similar to the 1d model results presented earlier the 2d results show that the and scenario predicts lower flood hazard levels compared to the maximum of the marginal profiles at the mouth x 0 and head x 10 000 m and higher flood hazard levels near where the marginal profiles intersect x 4000 m additionally the or scenario predicts the highest water levels everywhere a magnified view of water surface profiles at the mouth is presented in fig 6e 1d and 6 g 2d and at x 4000 m in fig 6f 1d and 6 h 2d fig 6 also shows the fema 2015 composite profile and the proposed composite water surface profile which takes the pointwise maximum of the marginal scenarios and the and scenario in this case the 2d modeling predicts a smaller difference between composite profiles 6 cm than 1d modeling 15 cm and this is attributed mainly to the treatment of complex topography nevertheless small height differences can be significant with respect to the delineation of flood hazard zones where floodplain topography is relatively flat for example a vertical height of 6 cm on a slope of 1 1000 implies a 60 m change in horizontal position which is larger than many land parcels in developed areas fig 7 a shows the spatial distribution of the differences between water surface levels based on the fema method and the proposed method the largest differences dη 1 cm are found in the lower nb and are maximum at the constriction between upper and lower bay located at pacific coast highway fig 7a also shows differences in the western part of nb off line from the main channel connecting san diego creek to the mouth of nb the effect of the relative timing of peak inflow and high tide to achieve maximum water surface elevations using the or hazard scenario was found to be relatively small compared to differences between hazard scenarios i e and vs or fig 7b shows color contours of the difference in water surface elevations between the scenario where hydrograph peaks are matched in time peak at time 9 h in fig 4 and water surface elevations obtained by shifting the peaks in time to achieve maximum water level in the uppermost section of the upper bay maximum water level is achieved by delaying peak river discharge by one hour peak q at hour 10 in fig 4 while in the lower bay maximum water level is achieved by advancing peak q by one hour peak q at hour 8 in fig 4 the difference between water levels based on the timing of high tides vs peak flow was found to be less than 3 cm across the majority of the bay this constitutes only about a third of the difference between the proposed composite profile and the fema 2015 composite profile this result provides a posteriori validation of selecting q and h as the basis for bivariate statistical analysis at this site as opposed to other system attributes such as the time lag between river flow and high tide 4 discussion multivariate statistical analysis is limited here to two variables bivariate analysis chosen to represent annual maximum river discharge and ocean level hazard scenarios beyond two variables are possible using copula based methods however expansion to higher dimensions can have drawbacks including uncertainty bounds so large that no conclusion may be drawn from its results see bevacqua et al 2017 for example for the sites considered the importance of the randomness of annual maximum river discharge and ocean level over other variables justifies the formulation of the bivariate statistical analysis problem around these two variables in systems where flood hazards are controlled by randomness in other factors such as waves or rainfall or even uncertain internal processes a different approach would be needed examples of internal processes include the frictional interaction between streamflow and tidal levels kukulka and jay 2003a b moftakhari et al 2013 2016 applications at a broader set of sites are warranted to better understand the broader applicability of the proposed method of bivariate analysis for flood hazard assessment in tidal channels and estuaries composite profiles derived from the pointwise maximum of water levels predicted by two or more hydrodynamic modeling scenarios offer a practical approach for delineating compound flood hazards in tidal channels and estuaries for a return period t only a limited number of relatively expensive hydrodynamic model simulations need to be completed despite an infinite number of possible forcing scenarios based on bivariate statistical analysis the most likely and hazard scenario was identified as a promising candidate for extending fema 2015 guidance on flood hazard mapping in tidal channels and estuaries that is results here suggest that the fema 2015 method may underestimate the flood hazard level over an interior section of tidal reaches and estuaries where high water levels are sensitive to both riverine discharge and ocean levels in nb this section corresponds to the urbanized lower bay where exposure and vulnerability to flooding is highest moreover an important implication is that extreme water levels may be higher at certain points within a system from combinations of river discharge and ocean heights that both fall below the return levels given by the marginal distribution i e univariate analysis on the other hand the most likely or hazard scenario results in boundary forcing that exceeds the return levels given by univariate analysis and it produces water levels that are higher than the marginal scenarios and the and hazard scenario the or hazard scenario could be useful when there is interest in using a single hydrodynamic modeling scenario to represent compound flood hazard levels and to avoid the need to compute composite profiles from multiple hydrodynamic modeling scenarios importantly the or hazard represents a more conservative interpretation of the t year return period hazard compared to traditional univariate assessment methods as well the and hazard scenario this paper points to the possibility of a more robust framework for mapping coastal flood hazards in tidal channels in estuaries that takes advantage of recent advances in multivariate statistical modeling e g sadegh et al 2018 and hydrodynamic coastal flood hazard mapping e g gallien et al 2011 luke et al 2018 and is in line with the limited resources and past practices of flood hazard mapping burby 2001 fema 2015 in short the method involves 1 bivariate statistical analysis where correlations in extreme values exist 2 selection of a limited number of q t h t for return period t 3 hydrodynamic modeling of the chosen pairs to produce extreme water levels and 4 synthesis of the model results to provide a spatial distribution of water level associated with return period t if this approach is taken to be more robust than existing methods and more research will be needed at a broader set of sites to make this assessment then the limited testing presented herein points to the existence of compound flood hazards that are presently underestimated by the existing fema method for tidal channels fema 2015 that is at all three sites there was a reach of the channel where the proposed composite profile accounting for the and hazard scenario was higher than the composite profile accounting only for marginal scenarios as a result of physical compounding effects recent research has also shown that flood hazard zones in the u s are underestimated due to poor representation of pluvial flood hazards e g wing et al 2017 finally we note that the hydrodynamic modeling shown here for newport bay is for only a single return period to demonstrate the hybrid statistical hydrodynamic framework assessment of flood hazard levels corresonding to other return periods follows the same approach 5 conclusions a method of linking statistical analysis with hydrodynamic modeling to estimate spatially distributed compound flood hazard levels in tidal channels and estuaries is presented for cases where flood hazards are associated with both high river discharge upstream forcing and high ocean levels downstream forcing bivariate statistical analysis is introduced to create combinations of river discharge and ocean levels suited for hydrodynamic modeling and extreme water levels produced by hydrodynamic models are synthesized to create a composite water surface profile representative of return period t the method accounts for compound flood hazards in two ways first it accounts for statistical correlation between upstream and downstream forcing which represents one dimension of compound hazards secondly hydrodynamic modeling accounts for physical compounding effects importantly this work shows that water levels at interior points of a tidal channel or estuary resulting from the bivariate and hazard scenario can be higher than water levels from marginal scenarios even though the boundary forcing is smaller than the corresponding marginal scenarios this is attributed to physical compounding effects i e nonlinear interactions between discharge and water level described by shallow water wave theory this work also shows that if a single scenario is needed to depict spatially distributed compound flood hazard levels the bivariate or hazard can be used and results here show that it provides a conservative assessment of the t year return period hazard acknowledgments this research was made possible by grants from the national science foundation hazards sees program award dms 1331611 and the national oceanic and atmospheric administration ecological effects of sea level rise program award na16nos4780206 who support is gratefully acknowledged 
652,a method to link bivariate statistical analysis and hydrodynamic modeling for flood hazard estimation in tidal channels and estuaries is presented and discussed for the general case where flood hazards are linked to upstream riverine discharge q and downstream ocean level h using a bivariate approach there are many possible combinations of q and h that jointly reflect a specific return period t raising questions about the best choice as boundary forcing in a hydrodynamic model we show first of all how possible q and h values depend on whether the definition of t corresponds to the probability of exceedance of h or q or h and q we also show that flood hazards defined by or return periods are more conservative than and return periods finally we introduce a new composite water surface profile to represent the spatially distributed hazard for return period t the composite profile synthesizes hydrodynamic model results from the and hazard scenario and two scenarios based on traditional univariate analysis a marginal q scenario and a marginal h scenario keywords coastal flood hazards bivariate statistical analysis hydrodynamic modeling compound flooding 1 introduction flood risk is increasing in coastal cities around the world due to several factors including population growth economic development sea level rise subsidence land use changes and intensification of rainfall hallegatte et al 2013 hanson et al 2011 by the year 2100 between 0 2 4 6 of global population and 0 3 9 3 of global gross domestic product may be exposed to coastal flooding if no adaptation occurs hinkel et al 2014 management of flood risk relies on statistical and hydrodynamic modeling to delineate populations and assets exposed to flooding anticipate and monetize the consequences of flooding and develop cost effective and socially robust interventions including infrastructure projects insurance programs land use and building code policy changes and emergency preparedness and response measures sayers et al 2013 luke et al 2018 to address risks statistical and hydrodynamic modeling is linked to delineate spatial fields of the intensity of flooding e g depth and velocity for a set of exceedance probabilities information which is subsequently used to estimate average annual losses based on exposed assets and their vulnerability to damage scawthorn et al 2006 the linking of statistical and hydrodynamic modeling is straightforward when addressing a single hazard such as river discharge q flood risk is modeled by first performing univariate frequency analysis of annual maximum discharge to estimate extreme values q t for yearly return periods t e g q 100 for 100 year return period discharge here the hat notation indicates annual maximum discharge and the subscript refers to the return period second hydrodynamic modeling is performed with q t as a boundary condition to characterize spatial fields of water surface elevation at each return period η t x where x represents distance along the river fema 2018 however coastal hazard assessment must account for interaction of river flooding intense rainfall storm surge and waves and the likelihood of a coincidence in extreme and non extreme levels of these hazards which is also known as compounding effects gallien et al 2018 moftakhari et al 2017 one of the most important compounding effects is the interaction of river discharge and the downstream ocean level h in tidal channels and estuaries of the world s 32 largest cities 22 are located on estuaries ross 1995 at which the interactions between q and h play a major role in flood risk estimation ward et al 2018 in the u s alone 140 million people 50 of total population live on the coast in close proximity to an estuary kennish 2004 fig 1 illustrates the estuarine flood hazard problem the objective is to estimate spatially distributed extreme water levels η t x for return period t in a tidal channel or estuary we assume knowledge of the system geometry e g bed elevation channel width and shape and resistance to flow e g manning resistance coefficient we also assume that gauges provide time series records of boundary conditions river discharge measurements representative of what enters the reach q t and water level measurements h t representative of the downstream end of the reach hence the key question becomes how can statistical and hydrodynamic modeling be linked for the estuarine setting involving two gage records characterizing two different aspects of hydrodynamic extremes put another way can the existing paradigm of univariate flood hazard modeling described above for rivers be extended to account for a second gage in a relatively simple and straightforward way we write this mathematically as follows 1 η t x f q t h t p x where p x represents the parameters describing the channel geometry and resistance properties note that eq 1 can be extended for two dimensional flood hazard levels by interpreting x as two dimensional vector representing geographical coordinates in the absence of robust models for extreme water levels in estuaries overly simplistic bathtub models have received widespread use for estimating coastal flooding hazards and the resulting human exposure at regional torresan et al 2012 and national international levels dasgupta et al 2011 hinkel et al 2010 bathtub models simply take an estimated extreme water level and extrapolate it inland to estimate population and assets exposed to flooding which neglects the potential for flood stage to change with distance inland as a consequence of riverine forcing and or tidal damping amplification lanzoni and seminara 1998 this points to the potential for underestimation of flood consequences on the other hand bathtub models may also overestimate flood consequences by failing to account for flood defenses and the role of friction inertia and storage in flooding dynamics gallien et al 2014 sanders 2017 by linking statistical analysis and hydrodynamic models more robust estimates of extreme water levels become possible as well as mechanistic routing of flood water into adjacent urban areas to estimate flood impacts gallien et al 2014 2011 existing methods for flood hazard assessment solving eq 1 in tidal channels are limited hoitink and jay 2016 in particular for the case where q t and h t are statistically independent fema 2015 recommends the following procedure to estimate the hazard for return period t 1 univariate analysis of annual maximum river discharge to estimate q t and univariate analysis of annual maximum total water level to estimate h t 2 a pair of hydrodynamic model simulations with one forced by q t and a non extreme h value usually chosen as mean higher high water and the other forced by h t and a non extreme q value and 3 synthesis of the two hydrodynamic model simulations based on the pointwise maximum water level across the two simulations an appealing aspect of the fema approach is that only two hydrodynamic simulations are required for each return period which is important because resources for flood mapping are limited burby 2001 and because hydrodynamic flood simulation is computationally demanding i e many hours for one simulation especially for urban areas where fine resolution grids are needed to accurately depict flooding e g gallien et al 2011 2014 hence the fema 2015 approach is aligned with needs for simple and efficient assessment approaches nevertheless there are significant limitations for example univariate statistical analysis is not appropriate when q t and h t exhibit statistical dependence also known as compound risks leonard et al 2014 moftakhari et al 2017 zscheischler et al 2018 additionally even when q t and h t are independent extreme water levels may occur over the length of the tidal reach due to the interaction of non extreme boundary forcing values fema 2015 2018 does not presently offer guidance to address this situation broadly the fema 2015 guidance recommends multi hazard assessment based on the predominant hazards yet limitations of this approach are increasingly being recognized hillier et al 2015 the aforementioned challenges of linking statistical and hydrodynamic modeling can only be partly overcome with improved access to and reduced costs of high performance computing systems that map flood hazards through monte carlo simulation that is monte carlo simulation can be applied to depict thousands of scenarios based on different combinations of q t and h t and depict spatially varied flood hazards based on the frequency of the pointwise exceedance of a water level threshold purvis et al 2008 however bivariate statistical analysis is needed in place of univariate analysis to properly describe the correlation structure of the hazard drivers and for sampling representative combinations of hazard drivers in monte carlo simulations the objective of this paper is to present a solution method for eq 1 that accounts for statistical correlation structure and physical compounding effects e g backwater between boundary forcing values q t and h t while using only a small number of hydrodynamic model simulations building on the existing methodology recommended by fema 2015 we present a four step method as follows 1 bivariate statistical analysis of q and h records to yield possible q t h t pairs for return period t 2 selection of n specific q t h t pairs for hydrodynamic modeling here we recommend n 4 more detail will follow although other options are possible 3 hydrodynamic modeling of n scenarios defined by q t h t pairs identified in step 1 to yield spatial distributions of extreme water levels η t i x i 1 n note that the subscript on η references return period and the superscript references the scenario 4 synthesis of hydrodynamic modeling results η t i x i 1 n to yield η t x the remainder of the paper presents this method in detail along with applications section 2 presents methods and materials including data used in this study the bivariate statistical analysis methods to determine all possible q t h t pairs identification of four specific pairs useful for hydrodynamic modeling and methods for one dimensional steady flow analysis and two dimensional unsteady analysis of extreme water levels section 3 presents results of one dimensional steady flow analysis from several sites showing differences in water level profiles arising from the q t h t pairs and results of two dimensional unsteady analysis at a single site where we further examine the limitations of the 1d modeling and the added benefits of 2d modeling for assessment of coastal flood hazards here we also introduce an extension of the fema 2015 method that offers potential to systematically improve the assessment of coastal flood hazards by linking bivariate statistical analysis and hydrodynamic modeling we close the paper with discussion section 4 and conclusions section 5 broadly this work shows that extending the univariate paradigm of river flood hazard assessment to a bivariate paradigm of coastal flood hazard assessment is not straightforward as has been reported for many other types of compound hazards kappes et al 2012 nevertheless a systematic approach is possible and shown herein furthermore results point to the possibility that the existing fema approach underestimates flood hazards where compounding effects are strong and we present a simple method to make a better estimate 2 methods and materials 2 1 data analysis herein focuses on tidal channels estuaries in southern california see fig 2 where flood hazards are affected by extreme ocean levels and flood discharges the los angeles river lar the santa ana river sar and newport bay nb for ocean level analysis hourly ocean water level measurements were obtained from the national oceanic and atmospheric administration noaa los angeles tide gauge gauge id 9410660 at hourly intervals tide gauge measurements capture water level fluctuations from combined effects of tides storm surge and other factors that affect sea levels on hourly and longer time scales a reading that is sometimes called total water level twl discharge measurements for the lar were obtained from los angeles county department of public works ladpw station f319 r lar at wardlow road and consisted of 92 years of annual maximum discharge data between 1928 2014 discharge measurements for the sar were obtained from usgs gauge 11078000 santa ana river at santa ana and consisted of 94 years of annual maximum discharge data between 1923 and 2017 river discharge measurements for newport bay were obtained from the orange county department of environmental resources gauge 226 san diego creek at campus drive and consists of 39 years of instantaneous discharge 1978 2016 the san gabriel river and coyote creek see fig 2 are not considered in this study since bivariate statistical analysis see section 3 1 showed no correlation between q and h likely because of strong flow regulation from whittier narrows dam located approximately 30 km from the coastline topographic and bathymetric data for lar and sar were taken from the 1 m resolution 2014 us army corps of engineers national coastal mapping program topobathy lidar dem topographic and bathymetric data for nb were based a dem reported by gallien et al 2011 which merged several sources of topographic and bathymetric data 2 2 bivariate statistical analysis statistical analysis of extreme values of discharge and water level impacting a tidal reach or estuary are based on records of annual maximum values although threshold based approaches are also possible here we use a hat notation to indicate annual maxima data from the records of upstream discharge and downstream water level q and h respectively record lengths of several decades or more are preferred to enable estimation of water levels at relatively low frequencies e g return periods of 50 years or greater bivariate statistical analysis begins with a test for correlation structure while either linear or rank correlation coefficient measures can be used to assess the significance of the dependence between variables tail dependence measures are important for summarizing how extremes tend to occur simultaneously coles et al 1999 hao and singh 2016 here we employ joint density approaches over other alternatives hawkes et al 2002 heffernan and tawn 2004 neal et al 2013 zheng et al 2015 due to their flexibility and computational mathematical benefits hawkes 2006 salvadori et al 2015 according to sklar s theorem sklar 1959 there exists a bivariate copula function c q h 0 1 0 1 0 1 that formulates the joint distribution f q h of the pair q h with marginal distributions f q and f h for all q h r 2 as 2 f q h q h c q h f q q f h h the multivariate model is constructed by fitting suitable univariate laws on the marginals and an appropriate copula on the observed pairs genest and favre 2007 salvadori et al 2007 here we use the method of sadegh et al 2018 which comprehensively analyzes the dependence structure of multiple drivers of flooding and models them using copula functions to estimate return design values and their underlying uncertainties this approach first selects a marginal distribution from 17 univariate distributions based on measures of goodness of fit including akaike information criterion aic and bayesian information criterion bic and then chooses a copula model from 26 copula functions copula model parameters are inferred through a bayesian inference approach with markov chain monte carlo sadegh et al 2018 2017 the joint probability can refer to the exceedance of q and h or the exceedance of q or h salvadori et al 2016 and a case can be made for the relevance of both to coastal flood hazard assessment first of all risk assessment should reflect the possibility that flooding is caused by either extreme river discharge or extreme ocean levels which is consistent with the or scenario on the other hand hydrodynamic modeling involves the simultaneous occurrence of an upstream discharge and downstream water level consistent with the and scenario newport bay data are used to illustrate this process fig 3 presents the outcome of bivariate statistical analysis using both the or and the and hazard scenarios using the method of sadegh et al 2018 there is no statistically significant correlation between river flow and ocean water level at newport bay but correlation was found between river flow and non tidal residual ntr defined as the difference between twl and the astronomical tide level hence bivariate statistical analysis that takes correlation structure into account is presented here using river flow and ntr fig 3 illustrates the similarities and differences between univariate and bivariate statistical analysis as well as the relative complexity introduced by the copula based and and or hazard scenarios in particular fig 3 shows plots of the marginal distributions of ntr fig 3a and river flow fig 3c representative of what has traditionally been used for univariate flood hazard assessment while fig 3b shows the copula based and and or hazard scenarios the and and or hazard scenarios are shown as iso return period curve for t 50 year within a two dimensional space whereby every point corresponds to a possible q t h t pair for use in hydrodynamic modeling additionally along each iso return period curve there is a point of maximum probability density which represents the most likely q t h t pairs given the correlation structure note that the most likely q t h t pair for the or hazard scenario exceeds extreme values given by the marginal distributions while the most likely q t h t pair for the and hazard scenario falls below the values given by the marginal distributions this shows that the or hazard scenario will lead to boundary forcing that is more conservative meaning a more cautious approach to risk management perspective than the and hazard scenario given theoretical characteristics of the or iso return period curves salvadori et al 2016 boundary forcing associated with q t h t pairs at the ends of the or curve may far exceed values given by the marginal distributions for the same return period this shows that the or scenario creates seemingly unrealistic highly conservative hazard scenarios in areas of low probability density hydrodynamic modeling of compound flood hazards for return period t is proposed based on four specific q t h t pairs taken from bivariate statistical analysis as shown in fig 3 s1 a marginal q scenario defined by the t year return period river discharge and a non extreme water level downstream typically taken as mean higher high water s2 a marginal h scenario defined by the t year return period ocean water level and a non extreme river flow typically taken as the daily average flow s3 an and scenario based on the q t h t pair with the highest probability density along the and iso return period curve s4 an or scenario based on the q t h t pair with the highest probability density along the or iso return period curve fema 2015 presently recommends hydrodynamic modeling of s1 and s2 and estimation of η t x based on the maximum of the two hence two additional scenarios s3 and s4 are considered here as a way of leveraging bivariate statistical analysis 2 3 hydrodynamic modeling dynamic changes in water surface elevation within estuaries can be modeled with reasonable accuracy using shallow water hydrodynamic models that assume a constant fluid density and a depth averaged horizontal velocity sanders et al 2010 fema 2015 2018 estuaries involve the mixing of riverine and ocean water with different densities and may be characterized by strong vertical density stratification that acts as a major control on the velocity distribution and transport geyer 2010 monismith 2010 consequently three dimensional models that account for variable density from salinity and temperature are often needed to estimate velocity distributions jay 2010 however the expression of density effects on surface water elevation is weak and can be neglected when predicting extreme water levels for the purpose of flood hazard modeling friedrichs 2010 in this study water levels are modeled by solving one dimensional constant density steady state shallow water models chow 2009 and two dimensional constant density depth averaged shallow water equations kim et al 2015 when a model is set up for estuaries tidal embayments or tidal channels the required boundary conditions correspond to a time series of river discharge at the upstream boundary q t and a time series of water level h t at the downstream boundary the upstream and downstream boundaries of the modeled spatial domain are generally placed adequately apart that compounding effects are avoided this is not however always possible in practice because tide gauges may be located within estuaries as an aside we note that two dimensional models require a spatial distribution of boundary forcing and in practice hydrodynamic models include methods to distribute the total volumetric flow rate q t across the inflow boundary while the water level h t is typically assumed to be uniform across the outflow boundary estuaries may also experience water level variability from internal forcing by winds and waves and accounting for these effects is outside the scope of this study however the role of regional winds waves and atmospheric pressure on water levels is captured by this approach based on the measurement of water levels at the tide gauge used for bivariate flood hazard assessment 2 3 1 1d steady state modeling one dimensional 1d steady state modeling of coastal flood hazards is useful as a first approximation of flood hazard levels along tidal channels and estuaries and can be done quickly with low computational effort application of 1d analysis at several sites is performed to study how differences in the selection process for q t h t pairs can affect the estimation of flood hazard levels flood hazard levels are computed by solving the gradually varied flow equation under the assumption of a rectangular channel with spatially variable manning nm width w and depth h as follows chow 2009 3 d η d x s f 1 f r 2 where x represents distance measured inland from the mouth of the estuary fr q gh3w2 1 2 is the froude number and sf nm q w 2 h10 3 represents the friction slope eq 3 is integrated with geometrical data for each site nm 0 032 m 1 3 s the downstream water level boundary given by h and a river discharge given by q numerical integration is performed with the 4th 5th order runge kutta scheme ode45 supported by matlab mathworks natick ma we note that the relatively simple channel geometry and resistance approximation is used herein to examine the relative differences between profiles from scenarios s1 s4 and not to estimate flood hazard along these rivers in an absolute sense more detailed geometry and resistance modeling will change the absolute value of flood hazard heights but have little impact on the relative difference between scenarios 2 3 2 2d unsteady modeling two dimensional 2d unsteady modeling is performed at one of the four sites newport bay to characterize limitations of the 1d steady state approximation and to study how the relative timing of the flood peak and high tide level can affect the flood hazard characterization the 2d model brezo begnudelli et al 2008 kim et al 2015 is applied based on a previous validation at newport bay gallien et al 2011 2014 brezo relies on an unstructured mesh of triangular elements with varying size to capture the bay s topography and bathymetry the model was originally setup to predict flood impacts in the urbanized portions of the newport bay city of newport beach california 2008 it features a fine resolution mesh with 3 m average linear resolution cells across streets and land parcels while across the upper newport bay the average linear cell resolution is approximately 15 m the boundary conditions in brezo are setup to specify the riverine discharge q entering the upper newport bay at the outlet of the san diego creek while water level h is specified along a boundary placed a short distance offshore of the embayment for unsteady analysis riverine discharge entering newport bay was modeled with a triangular hydrograph with a peak value given by q and a time of rise and total flood duration set to 3 and 8 h respectively based on analysis of instantaneous discharge measurements additionally ocean water level changes were modeled using a sinusoidal function with a 12 h period based on semidiurnal tides such that the maximum ocean water level equals h these approximations followed preliminary modeling which demonstrated that flood heights in the upper bay were much more sensitive to the magnitude of the peak flow than the duration of the event within the range of observed values and that high water levels were not sensitive to the precise shape of the tidal forcing we note that this may not be true in all systems and thus these approximations are not presented as a generalization but rather as a reasonable simplification given specific site conditions fig 4 presents the sinusoidal ocean forcing and the triangular inflow hydrograph to report the sensitivity of maximum water levels to the relative timing of the peak inflow and peak high tide the or hazard scenario was repeated using an inflow hydrograph that was shifted forward and backwater by as much as 6 h as shown in fig 4 all modeling results are expressed in metric units and referenced to the nad83 state plane horizontal coordinate system and navd88 vertical datum 3 results 3 1 bivariate statistical analysis correlation analysis between q and h defined by twl revealed no statistical significance at these southern california sites due to relatively small storm surges compared to variability in high tide levels attributed to astronomical factors however correlation was found using ntr as a surrogate for h at three of the four sites considered lar sar and nb kendall tau and spearman rho correlation coefficients between variables q and h defined using ntr are presented in table 1 along with p values a p value of less than 0 05 suggests a correlation at 5 significance level table 1 also shows the distribution functions that best describe the univariate distribution of river flow and ntr additionally the joe bivariate copula function was found to best describe the correlation structure between q and h in all sites flood hazard levels q and h for t 50 year for scenarios s1 s4 are presented in table 2 note that the marginal q scenarios all use a downstream water level corresponding to mean higher high water and all of the marginal h scenarios use a small relative to the extreme flows river discharge average daily flow taken as 10 m3 s these results show that the lar has larger river discharge values than sar and nb yet somewhat surprisingly nb has larger river discharge values than sar despite a much smaller watershed area this is attributed to control of runoff by dams also note that the water level of the marginal h scenario differs between the three sites despite all three relying on the los angeles tide gauge this is attributed to differences in the record length arising from joint probability analysis of tide gauge data and river gauge data 3 2 1d flood hazard analysis 1d steady state water surface profiles were computed for the 12 sets of q t h t pairs presented in table 2 corresponding to s1 s4 at lar sar and nb and are presented in fig 5 a c and e respectively focusing first on the marginal h and marginal q scenarios these results shows that there is a transition in the dominant factor controlling flood hazard levels along the length of the system with h controlling flood hazards near the outlet and q controlling flood hazards further inland the length of oceanic control is relatively long for nb 4 km and lar 2 km and relatively short sar 300 m water levels from the and scenario are lower than the higher of the two marginal scenarios at inflow and outflow boundaries but higher within an interior region where the marginal profiles intersect on the other hand the or scenario yields a water surface profile that is always above both marginal scenario profiles conceptually these results show that the or scenario represents a more conservative i e cautious representation of the spatially variable water surface profile associated with return period t than the and scenario which is expected based on the magnitude of the boundary forcing see table 2 fema 2015 guidance recommends mapping of flood hazard levels in tidally affected reaches based on the pointwise maximum of the two marginal scenarios this profile labeled fema is presented in fig 5b d and f for lar sar and nb respectively alongside the and or and a proposed composite profile based on the pointwise maximum of the h marginal q marginal and and hazard scenarios at all three sites the fema method underestimates the t year return period water level compared to the and scenario within a region where the marginal water surface profiles intersect hence the proposed composite profile is slightly higher than the fema method profile where there are physical compounding effects due to the interaction of river and oceanic influences on water levels otherwise the proposed composite profile tracks the fema method profile 3 3 2d flood hazard analysis two dimensional modeling of flood hazards in newport bay leads to spatially and temporally distributed water levels hence flood hazards are mapped based on the point maximum water level attained over an unsteady simulation covering the rise and fall of a flood peak with magnitude q and the rise and fall of an ocean tide of height h results are presented first for the case of the temporal coincidence in the two peaks and later the sensitivity of the results to the time lag between peaks is shown a comparison of extreme water level scenarios s1 s4 along the main channel of newport bay using 1d and 2d methods is shown in fig 6 for each scenario the modeling method has little impact on water profiles in the lower bay x 4000 m while differences are evident in upper bay x 4000 m and attributed mainly to differences in the treatment of complex system topography bathymetry that is the 1d model assumes a rectangular cross sectional with a width and depth based on the main channel while the 2d model resolves both channel and floodplain marsh topography allowing for greater conveyance at higher flood stages similar to the 1d model results presented earlier the 2d results show that the and scenario predicts lower flood hazard levels compared to the maximum of the marginal profiles at the mouth x 0 and head x 10 000 m and higher flood hazard levels near where the marginal profiles intersect x 4000 m additionally the or scenario predicts the highest water levels everywhere a magnified view of water surface profiles at the mouth is presented in fig 6e 1d and 6 g 2d and at x 4000 m in fig 6f 1d and 6 h 2d fig 6 also shows the fema 2015 composite profile and the proposed composite water surface profile which takes the pointwise maximum of the marginal scenarios and the and scenario in this case the 2d modeling predicts a smaller difference between composite profiles 6 cm than 1d modeling 15 cm and this is attributed mainly to the treatment of complex topography nevertheless small height differences can be significant with respect to the delineation of flood hazard zones where floodplain topography is relatively flat for example a vertical height of 6 cm on a slope of 1 1000 implies a 60 m change in horizontal position which is larger than many land parcels in developed areas fig 7 a shows the spatial distribution of the differences between water surface levels based on the fema method and the proposed method the largest differences dη 1 cm are found in the lower nb and are maximum at the constriction between upper and lower bay located at pacific coast highway fig 7a also shows differences in the western part of nb off line from the main channel connecting san diego creek to the mouth of nb the effect of the relative timing of peak inflow and high tide to achieve maximum water surface elevations using the or hazard scenario was found to be relatively small compared to differences between hazard scenarios i e and vs or fig 7b shows color contours of the difference in water surface elevations between the scenario where hydrograph peaks are matched in time peak at time 9 h in fig 4 and water surface elevations obtained by shifting the peaks in time to achieve maximum water level in the uppermost section of the upper bay maximum water level is achieved by delaying peak river discharge by one hour peak q at hour 10 in fig 4 while in the lower bay maximum water level is achieved by advancing peak q by one hour peak q at hour 8 in fig 4 the difference between water levels based on the timing of high tides vs peak flow was found to be less than 3 cm across the majority of the bay this constitutes only about a third of the difference between the proposed composite profile and the fema 2015 composite profile this result provides a posteriori validation of selecting q and h as the basis for bivariate statistical analysis at this site as opposed to other system attributes such as the time lag between river flow and high tide 4 discussion multivariate statistical analysis is limited here to two variables bivariate analysis chosen to represent annual maximum river discharge and ocean level hazard scenarios beyond two variables are possible using copula based methods however expansion to higher dimensions can have drawbacks including uncertainty bounds so large that no conclusion may be drawn from its results see bevacqua et al 2017 for example for the sites considered the importance of the randomness of annual maximum river discharge and ocean level over other variables justifies the formulation of the bivariate statistical analysis problem around these two variables in systems where flood hazards are controlled by randomness in other factors such as waves or rainfall or even uncertain internal processes a different approach would be needed examples of internal processes include the frictional interaction between streamflow and tidal levels kukulka and jay 2003a b moftakhari et al 2013 2016 applications at a broader set of sites are warranted to better understand the broader applicability of the proposed method of bivariate analysis for flood hazard assessment in tidal channels and estuaries composite profiles derived from the pointwise maximum of water levels predicted by two or more hydrodynamic modeling scenarios offer a practical approach for delineating compound flood hazards in tidal channels and estuaries for a return period t only a limited number of relatively expensive hydrodynamic model simulations need to be completed despite an infinite number of possible forcing scenarios based on bivariate statistical analysis the most likely and hazard scenario was identified as a promising candidate for extending fema 2015 guidance on flood hazard mapping in tidal channels and estuaries that is results here suggest that the fema 2015 method may underestimate the flood hazard level over an interior section of tidal reaches and estuaries where high water levels are sensitive to both riverine discharge and ocean levels in nb this section corresponds to the urbanized lower bay where exposure and vulnerability to flooding is highest moreover an important implication is that extreme water levels may be higher at certain points within a system from combinations of river discharge and ocean heights that both fall below the return levels given by the marginal distribution i e univariate analysis on the other hand the most likely or hazard scenario results in boundary forcing that exceeds the return levels given by univariate analysis and it produces water levels that are higher than the marginal scenarios and the and hazard scenario the or hazard scenario could be useful when there is interest in using a single hydrodynamic modeling scenario to represent compound flood hazard levels and to avoid the need to compute composite profiles from multiple hydrodynamic modeling scenarios importantly the or hazard represents a more conservative interpretation of the t year return period hazard compared to traditional univariate assessment methods as well the and hazard scenario this paper points to the possibility of a more robust framework for mapping coastal flood hazards in tidal channels in estuaries that takes advantage of recent advances in multivariate statistical modeling e g sadegh et al 2018 and hydrodynamic coastal flood hazard mapping e g gallien et al 2011 luke et al 2018 and is in line with the limited resources and past practices of flood hazard mapping burby 2001 fema 2015 in short the method involves 1 bivariate statistical analysis where correlations in extreme values exist 2 selection of a limited number of q t h t for return period t 3 hydrodynamic modeling of the chosen pairs to produce extreme water levels and 4 synthesis of the model results to provide a spatial distribution of water level associated with return period t if this approach is taken to be more robust than existing methods and more research will be needed at a broader set of sites to make this assessment then the limited testing presented herein points to the existence of compound flood hazards that are presently underestimated by the existing fema method for tidal channels fema 2015 that is at all three sites there was a reach of the channel where the proposed composite profile accounting for the and hazard scenario was higher than the composite profile accounting only for marginal scenarios as a result of physical compounding effects recent research has also shown that flood hazard zones in the u s are underestimated due to poor representation of pluvial flood hazards e g wing et al 2017 finally we note that the hydrodynamic modeling shown here for newport bay is for only a single return period to demonstrate the hybrid statistical hydrodynamic framework assessment of flood hazard levels corresonding to other return periods follows the same approach 5 conclusions a method of linking statistical analysis with hydrodynamic modeling to estimate spatially distributed compound flood hazard levels in tidal channels and estuaries is presented for cases where flood hazards are associated with both high river discharge upstream forcing and high ocean levels downstream forcing bivariate statistical analysis is introduced to create combinations of river discharge and ocean levels suited for hydrodynamic modeling and extreme water levels produced by hydrodynamic models are synthesized to create a composite water surface profile representative of return period t the method accounts for compound flood hazards in two ways first it accounts for statistical correlation between upstream and downstream forcing which represents one dimension of compound hazards secondly hydrodynamic modeling accounts for physical compounding effects importantly this work shows that water levels at interior points of a tidal channel or estuary resulting from the bivariate and hazard scenario can be higher than water levels from marginal scenarios even though the boundary forcing is smaller than the corresponding marginal scenarios this is attributed to physical compounding effects i e nonlinear interactions between discharge and water level described by shallow water wave theory this work also shows that if a single scenario is needed to depict spatially distributed compound flood hazard levels the bivariate or hazard can be used and results here show that it provides a conservative assessment of the t year return period hazard acknowledgments this research was made possible by grants from the national science foundation hazards sees program award dms 1331611 and the national oceanic and atmospheric administration ecological effects of sea level rise program award na16nos4780206 who support is gratefully acknowledged 
653,characterization of flow topology is essential to understand the effects of the heterogeneity and dimensionality of geological formations on the mixing of inert solute clouds in these same geological formations in this work we numerically study two indicators of flow topology the averaged vorticity magnitude ω av and the averaged positive second invariant qav of the deformation tensor u in steady darcy flows through exponentially correlated lognormal hydraulic conductivity fields k our numerical results allow us to establish the relationships between the two indicators considered here and the hydraulic conductivity variance σ 2 in 2d and 3d highlighting the role played by the spatial structure of these porous media on flow topology and indirectly on mixing this work leads us to assess the maximum dilution index e max indicator of mixing theoretically known to increase monotonically in steady darcy flows through isotropic heterogeneous porous media our numerical results allow us to test this hypothesis by establishing the relationship between the slope a of maximum dilution index e max and the averaged positive second invariant qav of deformation tensor u the parameters of this relationship depend on molecular diffusion and dimensionality of problem considered keywords heterogeneity hydraulic conductivity darcy flow flow topology vorticity second invariant of deformation tensor dilution index 1 introduction heterogeneous hydraulic conductivity fields occur naturally in geological media dagan 1984 zheng et al 2011 they induce flow fluctuations responsible for spreading inert solute clouds furthermore as the size of inert solute clouds increases higher concentration gradients appear they are dissipated by local dispersion through a mass exchange between high and low concentration areas this mechanism is called dilution dentz et al 2011 fiori 2001 herrera et al 2017 kapoor and kitanidis 1998 urroz et al 1995 weeks and sposito 1998 the combination of spreading and dilution changes the size of these inert solute clouds and the water volume occupied by the inert solutes leading to mixing with the connate water cirpka 2002 dentz et al 2011 dou and zhou 2014 de dreuzy et al 2012 herrera et al 2017 kitanidis 1994 rolle et al 2011 the flow fluctuations also generate shear stresses that stretch the inert solute clouds further improving mixing kitanidis 1994 tennekes et al 1972 weeks and sposito 1998 used to explain mixing of inert solute clouds in geological media arnold and khesin 1999 barros et al 2012 bear 1972 chen and meiburg 1998 chiogna et al 2016 2015 2014 cirpka et al 2015 de jong 1960 kapoor 1997 shvidler 1982 sposito 1994 2001 white and horne 1987 ye et al 2015 flow topology consists of describing the fluid motion the cauchy stokes decomposition theorem states that the fluid motion can be broken down into four components translation rotation dilation or contraction the two last components are described by the vorticity ω bear 1972 chen and meiburg 1998 de jong 1960 kapoor 1997 sposito 1994 white and horne 1987 and the second invariant q of the deformation tensor u chakraborty et al 2005 hunt et al 1988 jeong and hussain 1995 zhou et al 1999 respectively for isotropic heterogeneous hydraulic conductivity fields k the vorticity ω can be given by de jong 1960 bear 1972 white and horne 1987 sposito 1994 kapoor 1997 and chen and meiburg 1998 1 ω x u x l n k x this expression was used by mahani et al 2009 as an indicator of the impact of hydraulic conductivity heterogeneity on the flow for improving the coarse grid generation used in the reservoir simulation the coarse grid built allows the engineers to keep the flow topology because the four components of fluid motion translation rotation dilation or contraction are preserved in isotropic heterogeneous hydraulic conductivity fields k the correlation function of vorticity ω has been investigated by shvidler 1982 and kapoor 1997 by using a perturbation theory which is only available for low hydraulic conductivity variances σ 2 further 2d numerical results have been given by meiburg and homsy 1987 and rashid et al 2012 in 3d dato et al 2016 have studied the averaged vorticity magnitude ω av in porous media formed of a matrix with a constant hydraulic conductivity k 0 in which spheroidal inclusions with a distinct hydraulic conductivity k were immersed the logarithm of hydraulic conductivity contrast k ln k k 0 can be evaluated from a normal distribution characterized by a zero mean mk and a variance σ k 2 in turbulent flow theory the coupled vorticity stream function ω ψ can be used to identify three different flow zones where the stream function ψ has particular behaviours chakraborty et al 2005 hunt et al 1988 jeong and hussain 1995 zhou et al 1999 the eddy zones have a high value of vorticity ω the convergence zones have an irrotational straining motion with a strong convergence or divergence of the stream function ψ the streaming zones are zones where the stream function ψ is fairly straight hunt et al 1988 in the case of incompressible flows a classification of these three zones can be obtained with the second invariant q of the deformation tensor u 2 q x t r a c e u x 2 2 where q quantifies the ratio between the vorticity ω and the shear rate τ when q is positive ω dominates over τ which indicates the presence of eddy zones the two other zones are identified by negative values of q hunt et al 1988 jeong and hussain 1995 for incompressible flows in two dimensional porous media the second invariant q of deformation tensor u can be compared to the okubo weiss parameter ζ dimotakis 2005 okubo 1970 weiss 1991 3 ζ x 4 d e t u x 4 q x in their work barros et al 2012 de barros et al have established the relationship between the okubo weiss parameter ζ and the dilution index e introduced by kitanidis 1994 4 e t e x p s t 4 π d m ζ 2 ζ ω 2 ζ c o s h ζ t 2 ζ ω 2 ζ ω t 2 with dm the coefficient of molecular diffusion this relationship shows that the dilution index e is controlled by the okubo weiss parameter ζ characterizing flow topology as the second invariant q of deformation tensor u the dilution index e is a quantitative measure of dilution estimating the water volume occupied by the inert solute clouds in the special case of a closed domain the maximum value of the dilution index e max reaches the size of domain ω for an unbounded domain e max can t be defined with ω but with a gaussian cloud e max can be defined as a function of second moments of solute clouds 5 e m a x t 2 π n 2 e x p n 2 d e t s t 1 2 where n is the dimension of the domain and s the second moment tensor by revisiting the borden and cape cod experiments thierrin and kitanidis 1994 observed that the maximum dilution index e max should increase monotonically with a slope a d l n e d t n 2 t but the data did not show that an asymptotic behaviour is reached in this paper we study steady darcy flows through three dimensional isotropic heterogeneous porous media we use the averaged vorticity magnitude ω av to quantify the third component of fluid motion rotation we estimate the averaged positive second invariant qav informing about the dilution index e and we verify the hypothesis of an asymptotic behaviour of maximum dilution index e max in three dimensional isotropic heterogeneous porous media in order to reach our three objectives we use high performance computing with the numerical model paradis parallel dispersion available in the software platform h2olab section 2 paradis performs large scale and finely resolved monte carlo simulations for estimating the trajectory of inert particles in heterogeneous porous media characterized by an exponentially correlated log normal isotropic hydraulic conductivity fields beaudoin and de dreuzy 2013 beaudoin et al 2010 de dreuzy et al 2007 2008 two analyses are performed by means of these monte carlo simulations in 2d and 3d flow topology is investigated sections 3 and 4 the analysis of the averaged vorticity magnitude ω av and the averaged positive second invariant qav of deformation tensor u enables to etablish their relationship with the hydraulic conductivity variance σ 2 a study of asymptotic behaviour of maximum dilution index e max is performed section 4 the relationship between the slope a of maximum dilution index e max and the averaged positive second invariant qav of deformation tensor u is etablished 2 numerical model paradis based on the monte carlo method the numerical model paradis has been used to study the influence of heterogeneous hydraulic conductivity fields on macrodispersion in the past beaudoin and de dreuzy 2013 beaudoin et al 2010 de dreuzy et al 2007 2008 in this work the numerical model paradis is used to estimate the vorticity ω and the second invariant q of deformation tensor u from the darcy velocity u and the maximum dilution index e max from the second order moment s the darcy velocity u and the second order moment s are evaluated with model assumptions and numerical schemes recalled here 2 1 darcy velocity u the darcy velocity u is generated in heterogeneous porous media whose hydraulic conductivity k is isotropic and follows a lognormal distribution y l n k characterized by a gaussian correlation function c 6 c r σ 2 e x p r λ where r is the vector between two points and λ is the correlation length this model is very classic in the context of porous media dartois et al 2018 de dreuzy et al 2012 in this work the correlation length λ is fixed to 10 m and the hydraulic conductivity variance σ 2 varies between 0 and 6 25 benson et al 2001 rubin 2003 the mean of the lognormal distribution my is equal to 0 m s the isotropic heterogeneous hydraulic conductivity field k is generated within a computational domain of dimensions lx ly 4096 m 2048 m in 2d and lx ly lz 512 m 256 m 256 m in 3d with the directions x y and z the same resolution δ 1 m is used in all directions the number of grid cells is then 8 4 million for the two cases the isotropic heterogeneous hydraulic conductivity field k is evaluated by means of the fourier method gutjahr 1989 pardo igúzquiza and chica olmo 1993 using the parallel library fftw frigo and johnson 2005 roberts and bowman 2018 the fourier method is the most common of the spectral methods the representation of random fields in fourier spaces makes these spectral methods more efficient than other methods such as sequential gaussian methods hebe et al 2014 fig 1 shows an isotropic heterogeneous hydraulic conductivity field k top using a logarithm scale and the corresponding histogram bottom in 2d left and 3d right with a hydraulic conductivity variance σ 2 9 and the dimensions of the computational domain given previously the numerical results show that the distribution of values does not depend on the dimensionality of porous media studied and follows well a lognormal distribution in 2d and 3d the minimum value of k is equal to 3 10 3 m s in 2d and 2 10 3 m s in 3d the maximum value of k is equal to 667 m s in 2d and 200 m s in 3d the value of my is equal to 4 10 4 m s in 2d and 2 10 4 m s in 3d the transverse dimensions of computational domain ly 2048 m in 2d and ly lz 256 m 256 m in 3d have been chosen to simulate a computational domain long enough for the flow to reach the asymptotic regime of macrodispersion its longitudinal dimension lx has been fixed follownig a convergence study in figs 2 and 3 the mean of the logarithm distribution my and the hydraulic conductivity variance σ 2 have been plotted as a function of the longitudinal dimension lx in 2d left and 3d right with two values of hydraulic conductivity variance σ 2 1 and 9 convergence is reached when lx 4096 m in 2d and 512 m in 3d the hydraulic head ϕ and the darcy velocity u are computed from the mass conservation equation and the darcy s law considering a steady flow of an incompressible fluid in non deformable porous media with an unit porosity the flow equations are given by 7 u x 0 with u x k x ϕ x the boundary conditions are fixed hydraulic heads on the vertical sides and periodic conditions on the horizontal sides of computational domains the main flow direction is parallel to the axis x because of these boundary conditions the previous equations are then discretized using a finite volume scheme with a harmonic composition rule for the hydraulic conductivity between adjacent mesh cells eymard et al 2000 2007 the linear system giving the hydraulic head ϕ is solved by using the algebraic multi grid iterative method implemented in hypre erhel et al 2009 falgout et al 2005 the darcy velocity u is then computed on each grid face 2 2 second order moment s characterizing the spreading of a particle cloud the elements sij of second order moment s are defined by kitanidis 1994 thierrin and kitanidis 1994 and tartakovsky et al 2009 8 s i j t 1 m ω x i t x i t x j t x j t d m where m is the total mass of injected particles xi or xj is the coordinate of the location x of a particle in the direction i or j with i or j 1 2 and 3 only in 3d in the direction i the coordinate x i of the location x of the centroid of particle cloud is given by 9 x i t 1 m ω x i t d m assuming a number np of injected particles with an unit mass the discretized form of previous equations is given by 10 s i j t 1 n p k 1 k n p x i k t x i t x j k t x j t 11 x i t 1 n p k 1 k n p x i k t the kth particle moves from the location x k t to the location x k t δ t according to 12 x k t δ t x k t u x k t δ t 2 d m δ t z i where dm is the coefficient of molecular diffusion z is a random number drawn from a gaussian distribution of mean 0 and variance 1 and i is an unit vector with uniformly distributed orientation the ratio of advection diffusion is characterized by the peclet number p e λ u m e a n d m with umean the mean flow velocity huysmans and dassargues 2005 zheng and bennet 2002 four values of peclet number are studied here pe 10 50 100 and 1000 thus different degrees of dominance of advection over diffusion are studied the previous equation is the discretized form of langevin equation solution of fokker planck equation similar to the advection diffusion equation delay et al 2005 hoteit et al 2002 ramirez et al 2008 salamon et al 2006 the time step δt is adapted along the trajectory of the kth particle in order to maintain the stability of the euler scheme by following a cfl courant friedrichs levy condition 13 δ t 1 n α min δ x u x k t δ x 2 2 d m where u x k t is the norm of darcy velocity at the location x k t and nα is a positive integer representing the order of the time step number performed by the kth particle in the cell to ensure the accuracy of the euler scheme nα is fixed to 10 beaudoin and de dreuzy 2013 beaudoin et al 2010 de dreuzy et al 2007 2008 within grid cells the darcy velocity u is derived from linear interpolations in both directions as it is the sole interpolation scheme that verifies the continuity equation cheng and droniou 2019 jimenez et al 2005 kuznetsov and repin 2003 pollock 1988 prevost et al 2001 np particles are injected in an injection window of size 0 8 lx in 2d or 0 8 lx 0 8 ly in 3d orthogonal to the darcy flow and located at least five correlation lengths downstream from the side of the system the number of injected particles np 10 000 and the number of monte carlo simulations mc 100 have been chosen to ensure the convergence of the monte carlo approach beaudoin and de dreuzy 2013 beaudoin et al 2010 de dreuzy et al 2007 2008 3 vorticity ω the vorticity ω is defined as being the rotational of the flow velocity u 14 ω x u x the components of vorticity ω are evaluated from gradients of the flow velocity u by using a central finite difference scheme on regular grids with an uniform grid spacing δ 1 m as in the work by dato et al 2016 the vorticity magnitude ω and its averaged value ω av are used for quantifying the complexity of flow topology because the mean value of vorticity ω vanishes for the hydraulic conductivity fields studied here shvidler 1982 the word magnitude indicates here the vector norm fig 4 shows the vorticity magnitude field ω top and the corresponding histogram bottom in 2d left and 3d right with a hydraulic conductivity variance σ 2 6 25 the spatial structure of porous media influences the fluctuations of vorticity ω the minimum value of ω is equal to 1 9 10 9 s 1 in 2d and 3 5 10 7 s 1 in 3d the maximum value of ω is equal to 16 s 1 in 2d and 172 s 1 in 3d high values of the vorticity magnitude ω appear more frequently in 3d for example a vorticity magnitude ω 50 s 1 has a frequency of 0 in 2d and 90 in 3d the influence of the grid size on the estimation of the averaged vorticity magnitude ω av has been investigated the averaged vorticity magnitude ω av is given by 15 ω a v 1 ω ω ω x d ω where ω is the surface ω l x l y in 2d or volume ω l x l y l z in 3d of computational domains in fig 5 the averaged vorticity magnitude ω av has been plotted as a function of the mesh number nbm with a hydraulic conductivity variance σ 2 4 in 3d the mesh number nbm is equal to lxlylz δ3 in 3d the convergence of ω av is etablished for n b m 2 1 10 6 corresponding to a computational domain of dimensions lx ly lz 128 m 128 m 128 m fig 6 shows the averaged vorticity magnitude ω av obtained by the present model of porous media in 2d and 3d for various values of the hydraulic conductivity variance σ 2 the numerical results obtained by dato et al 2016 are also reported but only as a function of the variance σ k 2 of the log hydraulic conductivity contrast k ln k k 0 in 3d dato et al 2016 studied 3d porous media made up of spheroidal inclusions with a distinct hydraulic conductivity k submerged into a homogeneous porous matrix with a hydraulic conductivity k 0 the shape of spheroidal inclusions is characterized by the eccentricity e 1 β 2 with β c a the axis ratio between the two main axes the largest a and the smallest c of these inclusions if β 1 the inclusions are spheres otherwise they are ellipsoids two types of ellipsoidal shape can be used if two axes of inclusions are small axes with c the inclusions are called prolate inclusions thin ellipsoidal shape if two axes of inclusions are large axes with a the inclusions are called oblate inclusions thick ellipsoidal shape this model of hydraulic conductivity structure called multi indicator model mim has been proposed by dagan et al 2003 and fiori et al 2003 in this work spherical inclusions with β 1 prolate inclusions with β 0 1 and oblate inclusions with β 0 1 are considered as for dato et al 2016 an increase in the hydraulic conductivity variance σ 2 leads to an increase in the averaged vorticity magnitude ω av in 3d the averaged vorticity magnitude ω av is well fitted by the function 0 16 σ 4 1 02 σ 2 0 88 showing a quadratic evolution in the hydraulic conductivity variance σ 2 in 2d the fitting function becomes 0 04 σ 2 0 07 so the averaged vorticity magnitude ω av increases linearly with the hydraulic conductivity variance σ 2 these two fitting functions have been evaluated by using a classical method of least squares whose correlation coefficient was upper to 0 99 4 second invariant q for an incompressible fluid the second invariant q of deformation tensor u is given by 16 q x 1 2 t r a c e u x 2 a central finite difference scheme was used for estimating the components of the deformation tensor u on a regular grid in fig 7 the positive values of q top and the corresponding histogram bottom are shown in 2d left and 3d right with a hydraulic conductivity variance σ 2 6 25 in the two cases the zones with a significant positive value of q are located arround cells with highest values of the magnitude vorticity ω the difference of magnitude between 2d and 3d is significant and is confirmed with the histogram the minimum positive value of q is equal to 7 4 10 14 s 2 in 2d and 3 5 10 15 s 2 in 3d the maximum positive value of q is equal to 40 s 2 in 2d and 5020 s 2 in 3d the mean positive value of q is equal to 0 03 s 2 in 2d and 0 015 s 2 in 3d in 3d the positive values of q are more present in the lower and higher spectrum than in 2d for example there are around one thousand samples for q 10 s 2 in 3d but only about 50 in 2d the distribution of q leads us to believe that the flow is more dominated by the local flow rotation θ in 3d than in 2d the influence of the order of the central finite difference scheme on the estimation of the average positive second invariant qav of deformation tensor u was also investigated as for the averaged vorticity magnitude ω av see eq 14 the averaged positive second invariant qav of deformation tensor u is given by 17 q a v 1 ω ω q x d ω in fig 8 qav has been plotted as a function of the order of the central finite difference scheme acc in 3d with a hydraulic conductivity variance σ 2 4 the first order central finite difference scheme seems to give the correct acccuracy for the estimation of qav fig 9 shows the averaged positive second invariant qav of deformation tensor u obtained by the present model of porous media in 2d and 3d for various values of the hydraulic conductivity variance σ 2 as for the averaged vorticity magnitude ω av an increase of σ 2 leads to an increase of qav in 2d the fitting function 0 0026σ 2 shows that qav increases linearly with σ 2 in 3d qav increases exponentially rather than quadratically for σ 2 2 25 the fitting function is given by 0 005σ 4 for σ 2 2 25 and 0 0172exp 0 516σ 2 for σ 2 2 25 these three fitting functions have been evaluated by using a classical method of least squares whose correlation coefficient was greater than 0 98 fig 10 shows the averaged positive second invariant qav of deformation tensor u obtained by the present model of porous media in 2d and 3d as a function of the averaged vorticity magnitude ω av the same classical method of least squares was again used to estimate the fitting function in 2d an 3d the averaged positive second invariant qav of deformation tensor u is well fitted by a power function 0 37 ω a v 2 33 in 2d and 0 0006 ω a v 2 55 in 3d the correlation coefficient of least squares was greater than 0 99 these relationships between ω av and qav in 2d and 3d could be used to facilitate the estimation of qav if ω was already known 5 maximum dilution index e max in fig 11 the logarithm of the maximum dilution index ln e max is plotted as a function of the logarithm of time ln t for various values of the hydraulic conductivity variance σ 2 with a value of peclet number pe 20 in 2d and 3d e max having been estimated using eq 4 kitanidis 1994 for ln t 3 we can observe a monotonically increasing characterized by a constant slope noted a in 2d and 3d a is indicated in the figure with a linear function black solid line this behaviour is in agreement with the concept of dilution index proposed by kitanidis 1994 we can also observed that the dimensionality of the problem studied affects the value of the maximum dilution index e max e max is higher in 3d than in 2d the heterogeneity of the porous medium also has the same effect e max increases with σ 2 in 2d and 3d these two findings have already been shown in the laboratory by ye et al 2015 fig 12 presents the slope a as a function of the averaged positive second invariant qav of the deformation tensor u for various values of peclet number pe in 2d and 3d the behaviour of a is given by a power function black solid line 18 a a q a v b fig 13 shows the two coefficients a and b as functions of peclet number pe a is given by an inverse function 1 54 p e 0 73 in 2d and 4 16 p e 0 93 in 3d b seems to be a constant depending on the dimensionality of the problem studied 0 03 in 2d and 0 08 in 3d the effect of molecular diffusion on the dilution has been identified in the past cirpka et al 2015 kapoor and kitanidis 1998 rolle et al 2013 2012 rolle and kitanidis 2014 tartakovsky et al 2009 the maximum dilution index e max decreases with the peclet number pe the solute is more dilute for large values of diffusivity this effect is taken into account with an inverse correlation between the parameter a of the previous equation and the peclet number pe the parameters of this function depend on the dimensionality of the problem studied 6 conclusions the relationships between the variance σ 2 of a hydraulic conductivity field k and the two indicators of flow topology considered here the averaged vorticity magnitude ωav and the averaged positive second invariant qav of the deformation tensor u were established the numerical results obtained with the numerical model paradis have allowed us to study the role played by the spatial structure of heterogeneous porous media on flow topology and indirectly on mixing of inert solute clouds the relationships with the hydraulic conductivity variance σ 2 have been found to be linear for both the two indicators of flow topology in 2d these relationships with the hydraulic conductivity variance σ 2 become quadratic for the averaged vorticity magnitude ω av and exponential for the averaged positive second invariant qav of the deformation tensor u in 3d this difference in behaviour between 2d and 3d shows that the two dimensional flow fields cannot generate sufficient vorticity or sufficient dilation or contraction to induce significant deformations of inerte solute clouds the comparison with the numerical results obtained by dato et al 2016 shows that the gaussian correlation hydraulic conductivity and multi indicator models have similar behaviours in 3d the gaussian correlation hydraulic conductivity model gives numerical results close to those obtained with the multi indicator model for spherical inclusions for σ 2 2 25 and those obtained with the multi indicator model for a ratio β 0 1 regardless of the type of spheroidal inclusions for σ 2 2 25 this change of behaviour can only be explained by differences in the hydraulic conductivity structures at higher order of hydraulic conductivity variance σ 2 de dreuzy et al 2008 fiori et al 2008 mixing of inert solute clouds in steady darcy flows through exponentially correlated lognormal hydraulic conductivity fields k was characterized by the maximum dilution index e max in his concept of dilution kitanidis 1994 showed theorically that this physical quantity should increase monotonically the slope a of this monotonic increase was numerically determined the relationship between the slope a and the averaged positive second invariant qav of deformation tensor u was etablished in 2d and 3d a power function relates the slope a and the averaged positive second invariant qav of the deformation tensor u the parameters of this power function depend on the peclet number pe and the dimensionality of the problem studied these conclusions are specific to the model conditions studied flow topology was analyzed in steady darcy flows through exponentially correlated lognormal hydraulic conductivity fields k the hydraulic conductivity variance σ 2 was varied between 0 25 to 6 25 the correlation length λ was fixed to 10 m it has not been discussed how flow topology varies if the hydraulic conductivity model is changed in the eq 5 or if the correlation length λ is varied mixing was analyzed by estimating the maximum dilution index e max from the second order moment s of inert solute clouds with the eq 4 the second order moment s needs the evaluation of particle trajectories see eq 7 the particle trajectories were evaluated by considering only molecular diffusion in the dispersion term of transport equation see eq 11 the impact of a fluctuating local dispersion on mixing has not been studied acknowledgments the french national research agency anr is acknowledged for its financial founding through the h2mno4 project anr 12 monu 0012 
653,characterization of flow topology is essential to understand the effects of the heterogeneity and dimensionality of geological formations on the mixing of inert solute clouds in these same geological formations in this work we numerically study two indicators of flow topology the averaged vorticity magnitude ω av and the averaged positive second invariant qav of the deformation tensor u in steady darcy flows through exponentially correlated lognormal hydraulic conductivity fields k our numerical results allow us to establish the relationships between the two indicators considered here and the hydraulic conductivity variance σ 2 in 2d and 3d highlighting the role played by the spatial structure of these porous media on flow topology and indirectly on mixing this work leads us to assess the maximum dilution index e max indicator of mixing theoretically known to increase monotonically in steady darcy flows through isotropic heterogeneous porous media our numerical results allow us to test this hypothesis by establishing the relationship between the slope a of maximum dilution index e max and the averaged positive second invariant qav of deformation tensor u the parameters of this relationship depend on molecular diffusion and dimensionality of problem considered keywords heterogeneity hydraulic conductivity darcy flow flow topology vorticity second invariant of deformation tensor dilution index 1 introduction heterogeneous hydraulic conductivity fields occur naturally in geological media dagan 1984 zheng et al 2011 they induce flow fluctuations responsible for spreading inert solute clouds furthermore as the size of inert solute clouds increases higher concentration gradients appear they are dissipated by local dispersion through a mass exchange between high and low concentration areas this mechanism is called dilution dentz et al 2011 fiori 2001 herrera et al 2017 kapoor and kitanidis 1998 urroz et al 1995 weeks and sposito 1998 the combination of spreading and dilution changes the size of these inert solute clouds and the water volume occupied by the inert solutes leading to mixing with the connate water cirpka 2002 dentz et al 2011 dou and zhou 2014 de dreuzy et al 2012 herrera et al 2017 kitanidis 1994 rolle et al 2011 the flow fluctuations also generate shear stresses that stretch the inert solute clouds further improving mixing kitanidis 1994 tennekes et al 1972 weeks and sposito 1998 used to explain mixing of inert solute clouds in geological media arnold and khesin 1999 barros et al 2012 bear 1972 chen and meiburg 1998 chiogna et al 2016 2015 2014 cirpka et al 2015 de jong 1960 kapoor 1997 shvidler 1982 sposito 1994 2001 white and horne 1987 ye et al 2015 flow topology consists of describing the fluid motion the cauchy stokes decomposition theorem states that the fluid motion can be broken down into four components translation rotation dilation or contraction the two last components are described by the vorticity ω bear 1972 chen and meiburg 1998 de jong 1960 kapoor 1997 sposito 1994 white and horne 1987 and the second invariant q of the deformation tensor u chakraborty et al 2005 hunt et al 1988 jeong and hussain 1995 zhou et al 1999 respectively for isotropic heterogeneous hydraulic conductivity fields k the vorticity ω can be given by de jong 1960 bear 1972 white and horne 1987 sposito 1994 kapoor 1997 and chen and meiburg 1998 1 ω x u x l n k x this expression was used by mahani et al 2009 as an indicator of the impact of hydraulic conductivity heterogeneity on the flow for improving the coarse grid generation used in the reservoir simulation the coarse grid built allows the engineers to keep the flow topology because the four components of fluid motion translation rotation dilation or contraction are preserved in isotropic heterogeneous hydraulic conductivity fields k the correlation function of vorticity ω has been investigated by shvidler 1982 and kapoor 1997 by using a perturbation theory which is only available for low hydraulic conductivity variances σ 2 further 2d numerical results have been given by meiburg and homsy 1987 and rashid et al 2012 in 3d dato et al 2016 have studied the averaged vorticity magnitude ω av in porous media formed of a matrix with a constant hydraulic conductivity k 0 in which spheroidal inclusions with a distinct hydraulic conductivity k were immersed the logarithm of hydraulic conductivity contrast k ln k k 0 can be evaluated from a normal distribution characterized by a zero mean mk and a variance σ k 2 in turbulent flow theory the coupled vorticity stream function ω ψ can be used to identify three different flow zones where the stream function ψ has particular behaviours chakraborty et al 2005 hunt et al 1988 jeong and hussain 1995 zhou et al 1999 the eddy zones have a high value of vorticity ω the convergence zones have an irrotational straining motion with a strong convergence or divergence of the stream function ψ the streaming zones are zones where the stream function ψ is fairly straight hunt et al 1988 in the case of incompressible flows a classification of these three zones can be obtained with the second invariant q of the deformation tensor u 2 q x t r a c e u x 2 2 where q quantifies the ratio between the vorticity ω and the shear rate τ when q is positive ω dominates over τ which indicates the presence of eddy zones the two other zones are identified by negative values of q hunt et al 1988 jeong and hussain 1995 for incompressible flows in two dimensional porous media the second invariant q of deformation tensor u can be compared to the okubo weiss parameter ζ dimotakis 2005 okubo 1970 weiss 1991 3 ζ x 4 d e t u x 4 q x in their work barros et al 2012 de barros et al have established the relationship between the okubo weiss parameter ζ and the dilution index e introduced by kitanidis 1994 4 e t e x p s t 4 π d m ζ 2 ζ ω 2 ζ c o s h ζ t 2 ζ ω 2 ζ ω t 2 with dm the coefficient of molecular diffusion this relationship shows that the dilution index e is controlled by the okubo weiss parameter ζ characterizing flow topology as the second invariant q of deformation tensor u the dilution index e is a quantitative measure of dilution estimating the water volume occupied by the inert solute clouds in the special case of a closed domain the maximum value of the dilution index e max reaches the size of domain ω for an unbounded domain e max can t be defined with ω but with a gaussian cloud e max can be defined as a function of second moments of solute clouds 5 e m a x t 2 π n 2 e x p n 2 d e t s t 1 2 where n is the dimension of the domain and s the second moment tensor by revisiting the borden and cape cod experiments thierrin and kitanidis 1994 observed that the maximum dilution index e max should increase monotonically with a slope a d l n e d t n 2 t but the data did not show that an asymptotic behaviour is reached in this paper we study steady darcy flows through three dimensional isotropic heterogeneous porous media we use the averaged vorticity magnitude ω av to quantify the third component of fluid motion rotation we estimate the averaged positive second invariant qav informing about the dilution index e and we verify the hypothesis of an asymptotic behaviour of maximum dilution index e max in three dimensional isotropic heterogeneous porous media in order to reach our three objectives we use high performance computing with the numerical model paradis parallel dispersion available in the software platform h2olab section 2 paradis performs large scale and finely resolved monte carlo simulations for estimating the trajectory of inert particles in heterogeneous porous media characterized by an exponentially correlated log normal isotropic hydraulic conductivity fields beaudoin and de dreuzy 2013 beaudoin et al 2010 de dreuzy et al 2007 2008 two analyses are performed by means of these monte carlo simulations in 2d and 3d flow topology is investigated sections 3 and 4 the analysis of the averaged vorticity magnitude ω av and the averaged positive second invariant qav of deformation tensor u enables to etablish their relationship with the hydraulic conductivity variance σ 2 a study of asymptotic behaviour of maximum dilution index e max is performed section 4 the relationship between the slope a of maximum dilution index e max and the averaged positive second invariant qav of deformation tensor u is etablished 2 numerical model paradis based on the monte carlo method the numerical model paradis has been used to study the influence of heterogeneous hydraulic conductivity fields on macrodispersion in the past beaudoin and de dreuzy 2013 beaudoin et al 2010 de dreuzy et al 2007 2008 in this work the numerical model paradis is used to estimate the vorticity ω and the second invariant q of deformation tensor u from the darcy velocity u and the maximum dilution index e max from the second order moment s the darcy velocity u and the second order moment s are evaluated with model assumptions and numerical schemes recalled here 2 1 darcy velocity u the darcy velocity u is generated in heterogeneous porous media whose hydraulic conductivity k is isotropic and follows a lognormal distribution y l n k characterized by a gaussian correlation function c 6 c r σ 2 e x p r λ where r is the vector between two points and λ is the correlation length this model is very classic in the context of porous media dartois et al 2018 de dreuzy et al 2012 in this work the correlation length λ is fixed to 10 m and the hydraulic conductivity variance σ 2 varies between 0 and 6 25 benson et al 2001 rubin 2003 the mean of the lognormal distribution my is equal to 0 m s the isotropic heterogeneous hydraulic conductivity field k is generated within a computational domain of dimensions lx ly 4096 m 2048 m in 2d and lx ly lz 512 m 256 m 256 m in 3d with the directions x y and z the same resolution δ 1 m is used in all directions the number of grid cells is then 8 4 million for the two cases the isotropic heterogeneous hydraulic conductivity field k is evaluated by means of the fourier method gutjahr 1989 pardo igúzquiza and chica olmo 1993 using the parallel library fftw frigo and johnson 2005 roberts and bowman 2018 the fourier method is the most common of the spectral methods the representation of random fields in fourier spaces makes these spectral methods more efficient than other methods such as sequential gaussian methods hebe et al 2014 fig 1 shows an isotropic heterogeneous hydraulic conductivity field k top using a logarithm scale and the corresponding histogram bottom in 2d left and 3d right with a hydraulic conductivity variance σ 2 9 and the dimensions of the computational domain given previously the numerical results show that the distribution of values does not depend on the dimensionality of porous media studied and follows well a lognormal distribution in 2d and 3d the minimum value of k is equal to 3 10 3 m s in 2d and 2 10 3 m s in 3d the maximum value of k is equal to 667 m s in 2d and 200 m s in 3d the value of my is equal to 4 10 4 m s in 2d and 2 10 4 m s in 3d the transverse dimensions of computational domain ly 2048 m in 2d and ly lz 256 m 256 m in 3d have been chosen to simulate a computational domain long enough for the flow to reach the asymptotic regime of macrodispersion its longitudinal dimension lx has been fixed follownig a convergence study in figs 2 and 3 the mean of the logarithm distribution my and the hydraulic conductivity variance σ 2 have been plotted as a function of the longitudinal dimension lx in 2d left and 3d right with two values of hydraulic conductivity variance σ 2 1 and 9 convergence is reached when lx 4096 m in 2d and 512 m in 3d the hydraulic head ϕ and the darcy velocity u are computed from the mass conservation equation and the darcy s law considering a steady flow of an incompressible fluid in non deformable porous media with an unit porosity the flow equations are given by 7 u x 0 with u x k x ϕ x the boundary conditions are fixed hydraulic heads on the vertical sides and periodic conditions on the horizontal sides of computational domains the main flow direction is parallel to the axis x because of these boundary conditions the previous equations are then discretized using a finite volume scheme with a harmonic composition rule for the hydraulic conductivity between adjacent mesh cells eymard et al 2000 2007 the linear system giving the hydraulic head ϕ is solved by using the algebraic multi grid iterative method implemented in hypre erhel et al 2009 falgout et al 2005 the darcy velocity u is then computed on each grid face 2 2 second order moment s characterizing the spreading of a particle cloud the elements sij of second order moment s are defined by kitanidis 1994 thierrin and kitanidis 1994 and tartakovsky et al 2009 8 s i j t 1 m ω x i t x i t x j t x j t d m where m is the total mass of injected particles xi or xj is the coordinate of the location x of a particle in the direction i or j with i or j 1 2 and 3 only in 3d in the direction i the coordinate x i of the location x of the centroid of particle cloud is given by 9 x i t 1 m ω x i t d m assuming a number np of injected particles with an unit mass the discretized form of previous equations is given by 10 s i j t 1 n p k 1 k n p x i k t x i t x j k t x j t 11 x i t 1 n p k 1 k n p x i k t the kth particle moves from the location x k t to the location x k t δ t according to 12 x k t δ t x k t u x k t δ t 2 d m δ t z i where dm is the coefficient of molecular diffusion z is a random number drawn from a gaussian distribution of mean 0 and variance 1 and i is an unit vector with uniformly distributed orientation the ratio of advection diffusion is characterized by the peclet number p e λ u m e a n d m with umean the mean flow velocity huysmans and dassargues 2005 zheng and bennet 2002 four values of peclet number are studied here pe 10 50 100 and 1000 thus different degrees of dominance of advection over diffusion are studied the previous equation is the discretized form of langevin equation solution of fokker planck equation similar to the advection diffusion equation delay et al 2005 hoteit et al 2002 ramirez et al 2008 salamon et al 2006 the time step δt is adapted along the trajectory of the kth particle in order to maintain the stability of the euler scheme by following a cfl courant friedrichs levy condition 13 δ t 1 n α min δ x u x k t δ x 2 2 d m where u x k t is the norm of darcy velocity at the location x k t and nα is a positive integer representing the order of the time step number performed by the kth particle in the cell to ensure the accuracy of the euler scheme nα is fixed to 10 beaudoin and de dreuzy 2013 beaudoin et al 2010 de dreuzy et al 2007 2008 within grid cells the darcy velocity u is derived from linear interpolations in both directions as it is the sole interpolation scheme that verifies the continuity equation cheng and droniou 2019 jimenez et al 2005 kuznetsov and repin 2003 pollock 1988 prevost et al 2001 np particles are injected in an injection window of size 0 8 lx in 2d or 0 8 lx 0 8 ly in 3d orthogonal to the darcy flow and located at least five correlation lengths downstream from the side of the system the number of injected particles np 10 000 and the number of monte carlo simulations mc 100 have been chosen to ensure the convergence of the monte carlo approach beaudoin and de dreuzy 2013 beaudoin et al 2010 de dreuzy et al 2007 2008 3 vorticity ω the vorticity ω is defined as being the rotational of the flow velocity u 14 ω x u x the components of vorticity ω are evaluated from gradients of the flow velocity u by using a central finite difference scheme on regular grids with an uniform grid spacing δ 1 m as in the work by dato et al 2016 the vorticity magnitude ω and its averaged value ω av are used for quantifying the complexity of flow topology because the mean value of vorticity ω vanishes for the hydraulic conductivity fields studied here shvidler 1982 the word magnitude indicates here the vector norm fig 4 shows the vorticity magnitude field ω top and the corresponding histogram bottom in 2d left and 3d right with a hydraulic conductivity variance σ 2 6 25 the spatial structure of porous media influences the fluctuations of vorticity ω the minimum value of ω is equal to 1 9 10 9 s 1 in 2d and 3 5 10 7 s 1 in 3d the maximum value of ω is equal to 16 s 1 in 2d and 172 s 1 in 3d high values of the vorticity magnitude ω appear more frequently in 3d for example a vorticity magnitude ω 50 s 1 has a frequency of 0 in 2d and 90 in 3d the influence of the grid size on the estimation of the averaged vorticity magnitude ω av has been investigated the averaged vorticity magnitude ω av is given by 15 ω a v 1 ω ω ω x d ω where ω is the surface ω l x l y in 2d or volume ω l x l y l z in 3d of computational domains in fig 5 the averaged vorticity magnitude ω av has been plotted as a function of the mesh number nbm with a hydraulic conductivity variance σ 2 4 in 3d the mesh number nbm is equal to lxlylz δ3 in 3d the convergence of ω av is etablished for n b m 2 1 10 6 corresponding to a computational domain of dimensions lx ly lz 128 m 128 m 128 m fig 6 shows the averaged vorticity magnitude ω av obtained by the present model of porous media in 2d and 3d for various values of the hydraulic conductivity variance σ 2 the numerical results obtained by dato et al 2016 are also reported but only as a function of the variance σ k 2 of the log hydraulic conductivity contrast k ln k k 0 in 3d dato et al 2016 studied 3d porous media made up of spheroidal inclusions with a distinct hydraulic conductivity k submerged into a homogeneous porous matrix with a hydraulic conductivity k 0 the shape of spheroidal inclusions is characterized by the eccentricity e 1 β 2 with β c a the axis ratio between the two main axes the largest a and the smallest c of these inclusions if β 1 the inclusions are spheres otherwise they are ellipsoids two types of ellipsoidal shape can be used if two axes of inclusions are small axes with c the inclusions are called prolate inclusions thin ellipsoidal shape if two axes of inclusions are large axes with a the inclusions are called oblate inclusions thick ellipsoidal shape this model of hydraulic conductivity structure called multi indicator model mim has been proposed by dagan et al 2003 and fiori et al 2003 in this work spherical inclusions with β 1 prolate inclusions with β 0 1 and oblate inclusions with β 0 1 are considered as for dato et al 2016 an increase in the hydraulic conductivity variance σ 2 leads to an increase in the averaged vorticity magnitude ω av in 3d the averaged vorticity magnitude ω av is well fitted by the function 0 16 σ 4 1 02 σ 2 0 88 showing a quadratic evolution in the hydraulic conductivity variance σ 2 in 2d the fitting function becomes 0 04 σ 2 0 07 so the averaged vorticity magnitude ω av increases linearly with the hydraulic conductivity variance σ 2 these two fitting functions have been evaluated by using a classical method of least squares whose correlation coefficient was upper to 0 99 4 second invariant q for an incompressible fluid the second invariant q of deformation tensor u is given by 16 q x 1 2 t r a c e u x 2 a central finite difference scheme was used for estimating the components of the deformation tensor u on a regular grid in fig 7 the positive values of q top and the corresponding histogram bottom are shown in 2d left and 3d right with a hydraulic conductivity variance σ 2 6 25 in the two cases the zones with a significant positive value of q are located arround cells with highest values of the magnitude vorticity ω the difference of magnitude between 2d and 3d is significant and is confirmed with the histogram the minimum positive value of q is equal to 7 4 10 14 s 2 in 2d and 3 5 10 15 s 2 in 3d the maximum positive value of q is equal to 40 s 2 in 2d and 5020 s 2 in 3d the mean positive value of q is equal to 0 03 s 2 in 2d and 0 015 s 2 in 3d in 3d the positive values of q are more present in the lower and higher spectrum than in 2d for example there are around one thousand samples for q 10 s 2 in 3d but only about 50 in 2d the distribution of q leads us to believe that the flow is more dominated by the local flow rotation θ in 3d than in 2d the influence of the order of the central finite difference scheme on the estimation of the average positive second invariant qav of deformation tensor u was also investigated as for the averaged vorticity magnitude ω av see eq 14 the averaged positive second invariant qav of deformation tensor u is given by 17 q a v 1 ω ω q x d ω in fig 8 qav has been plotted as a function of the order of the central finite difference scheme acc in 3d with a hydraulic conductivity variance σ 2 4 the first order central finite difference scheme seems to give the correct acccuracy for the estimation of qav fig 9 shows the averaged positive second invariant qav of deformation tensor u obtained by the present model of porous media in 2d and 3d for various values of the hydraulic conductivity variance σ 2 as for the averaged vorticity magnitude ω av an increase of σ 2 leads to an increase of qav in 2d the fitting function 0 0026σ 2 shows that qav increases linearly with σ 2 in 3d qav increases exponentially rather than quadratically for σ 2 2 25 the fitting function is given by 0 005σ 4 for σ 2 2 25 and 0 0172exp 0 516σ 2 for σ 2 2 25 these three fitting functions have been evaluated by using a classical method of least squares whose correlation coefficient was greater than 0 98 fig 10 shows the averaged positive second invariant qav of deformation tensor u obtained by the present model of porous media in 2d and 3d as a function of the averaged vorticity magnitude ω av the same classical method of least squares was again used to estimate the fitting function in 2d an 3d the averaged positive second invariant qav of deformation tensor u is well fitted by a power function 0 37 ω a v 2 33 in 2d and 0 0006 ω a v 2 55 in 3d the correlation coefficient of least squares was greater than 0 99 these relationships between ω av and qav in 2d and 3d could be used to facilitate the estimation of qav if ω was already known 5 maximum dilution index e max in fig 11 the logarithm of the maximum dilution index ln e max is plotted as a function of the logarithm of time ln t for various values of the hydraulic conductivity variance σ 2 with a value of peclet number pe 20 in 2d and 3d e max having been estimated using eq 4 kitanidis 1994 for ln t 3 we can observe a monotonically increasing characterized by a constant slope noted a in 2d and 3d a is indicated in the figure with a linear function black solid line this behaviour is in agreement with the concept of dilution index proposed by kitanidis 1994 we can also observed that the dimensionality of the problem studied affects the value of the maximum dilution index e max e max is higher in 3d than in 2d the heterogeneity of the porous medium also has the same effect e max increases with σ 2 in 2d and 3d these two findings have already been shown in the laboratory by ye et al 2015 fig 12 presents the slope a as a function of the averaged positive second invariant qav of the deformation tensor u for various values of peclet number pe in 2d and 3d the behaviour of a is given by a power function black solid line 18 a a q a v b fig 13 shows the two coefficients a and b as functions of peclet number pe a is given by an inverse function 1 54 p e 0 73 in 2d and 4 16 p e 0 93 in 3d b seems to be a constant depending on the dimensionality of the problem studied 0 03 in 2d and 0 08 in 3d the effect of molecular diffusion on the dilution has been identified in the past cirpka et al 2015 kapoor and kitanidis 1998 rolle et al 2013 2012 rolle and kitanidis 2014 tartakovsky et al 2009 the maximum dilution index e max decreases with the peclet number pe the solute is more dilute for large values of diffusivity this effect is taken into account with an inverse correlation between the parameter a of the previous equation and the peclet number pe the parameters of this function depend on the dimensionality of the problem studied 6 conclusions the relationships between the variance σ 2 of a hydraulic conductivity field k and the two indicators of flow topology considered here the averaged vorticity magnitude ωav and the averaged positive second invariant qav of the deformation tensor u were established the numerical results obtained with the numerical model paradis have allowed us to study the role played by the spatial structure of heterogeneous porous media on flow topology and indirectly on mixing of inert solute clouds the relationships with the hydraulic conductivity variance σ 2 have been found to be linear for both the two indicators of flow topology in 2d these relationships with the hydraulic conductivity variance σ 2 become quadratic for the averaged vorticity magnitude ω av and exponential for the averaged positive second invariant qav of the deformation tensor u in 3d this difference in behaviour between 2d and 3d shows that the two dimensional flow fields cannot generate sufficient vorticity or sufficient dilation or contraction to induce significant deformations of inerte solute clouds the comparison with the numerical results obtained by dato et al 2016 shows that the gaussian correlation hydraulic conductivity and multi indicator models have similar behaviours in 3d the gaussian correlation hydraulic conductivity model gives numerical results close to those obtained with the multi indicator model for spherical inclusions for σ 2 2 25 and those obtained with the multi indicator model for a ratio β 0 1 regardless of the type of spheroidal inclusions for σ 2 2 25 this change of behaviour can only be explained by differences in the hydraulic conductivity structures at higher order of hydraulic conductivity variance σ 2 de dreuzy et al 2008 fiori et al 2008 mixing of inert solute clouds in steady darcy flows through exponentially correlated lognormal hydraulic conductivity fields k was characterized by the maximum dilution index e max in his concept of dilution kitanidis 1994 showed theorically that this physical quantity should increase monotonically the slope a of this monotonic increase was numerically determined the relationship between the slope a and the averaged positive second invariant qav of deformation tensor u was etablished in 2d and 3d a power function relates the slope a and the averaged positive second invariant qav of the deformation tensor u the parameters of this power function depend on the peclet number pe and the dimensionality of the problem studied these conclusions are specific to the model conditions studied flow topology was analyzed in steady darcy flows through exponentially correlated lognormal hydraulic conductivity fields k the hydraulic conductivity variance σ 2 was varied between 0 25 to 6 25 the correlation length λ was fixed to 10 m it has not been discussed how flow topology varies if the hydraulic conductivity model is changed in the eq 5 or if the correlation length λ is varied mixing was analyzed by estimating the maximum dilution index e max from the second order moment s of inert solute clouds with the eq 4 the second order moment s needs the evaluation of particle trajectories see eq 7 the particle trajectories were evaluated by considering only molecular diffusion in the dispersion term of transport equation see eq 11 the impact of a fluctuating local dispersion on mixing has not been studied acknowledgments the french national research agency anr is acknowledged for its financial founding through the h2mno4 project anr 12 monu 0012 
654,the degree with which to parameterize a computer model that is to be used for risk based resource management decision support has been a topic of much discussion in the environmental modeling industry and remains a difficult choice facing practitioners high dimensional parameterization schemes allow for a more robust expression of model input uncertainty over traditional lower dimensional schemes but often incur a higher computational burden and require greater understanding of inverse problem theory to implement effectively however a number of significant questions remain such as what level of parameterization is needed to adequately express uncertainty for a given decision relevant simulated output and to what extent can a simplified parameterization be adopted while maintaining the ability of the model to serve as a decision support tool this study addresses these questions among others by using empirical paired complex simple model analyses to investigate the consequences of reduced parameterization on decision relevant simulated outputs in terms of bias incursion and underestimation of uncertainty a bayesian decision analysis approach is adopted to facilitate evaluation of parameterization reduction outcomes not only in terms of the prior and posterior probability density functions of decision relevant simulated outputs but also in terms of the management decisions that would be made on their basis two integrated surface water groundwater model case study examples are presented the first is a complex synthetic model used to forecast groundwater abstraction induced changes in ecologically sensitive streamflow characteristics and the second is a real world regional scale model hauraki plains new zealand used to simulate nitrate loading impacts on water quality it is shown empirically that for some decision relevant simulated outputs even relatively high dimensional parameterization schemes 2 000 adjustable parameters display significant bias in simulated outputs as a result of improper parameter compensation induced through history matching relative to complex parameterization schemes 100 000 adjustable parameters ultimately leading to incorrect decisions and resource management action for other decision relevant simulated outputs however reduced parameterization schemes may be appropriate for resource management decision making especially when considering a prior uncertainty stance i e without undertaking history matching and when considering differences between simulated outputs that do not depend on local scale heterogeneity keywords environmental model decision making parameterization uncertainty quantification model error history matching 1 introduction the subjective process of parameterizing a computer model for the purposes of history matching and or uncertainty quantification involves nominating numerical quantities used by the model that are uncertain and representing this uncertainty quantitatively this constitutes one of the most difficult aspects of the environmental modeling process a number of studies have shown that the degree to which a model is parameterized can have significant impacts on a model s capacity to reproduce historical observations and quantify uncertainty for example cooley 2004 cooley and christensen 2006 gallagher and doherty 2007 show that the representation of spatially distributed environmental properties in a lumped or aggregated manner within a model may lead to significant structural noise e g doherty and welter 2010 refsgaard et al 2012 refsgaard et al 2006 which has implications for simulated outputs that are matched to field observations through history matching and for simulated outputs which underpin a model s utility in the decision support context i e forecasts moore and doherty 2005 showed that parameter zonation may lead to critical underestimation in forecast uncertainty fienen et al 2010 highlighted the importance of adopting a spatially distributed parameterization scheme to meaningfully quantify forecast uncertainty towards optimizing monitoring design networks most recently gosses and wohling 2019 demonstrated the potential outcomes of spatial parameterization and discretization simplification in terms of forecast bias and uncertainty variance however despite this widely varying parameterization schemes are still advocated for and employed in the literature e g hunt et al 2007 voss 2011a voss 2011b the concept of environmental model simplicity complexity which is commonly represented conceptually by parameterization dimensionality continues to receive considerable attention in the literature e g castilla rho 2017 simmons and hunt 2012 the need for the selection of an appropriate level of model simplicity complexity to be forecast and data targeted termed defensible complexity in a recent review paper by anneli 2017 has been discussed by a number of studies e g doherty and simmons 2013 ferre 2017 however there is presently limited guidance on and no benchmark for judging the appropriateness or otherwise of appropriate level of parameterization in the context of risk based resource management decision making hunt et al 2007 presented an argument for increased parameterization or highly parameterized approaches they argued that models employing a high degree of parameterization detail where the number of adjustable parameters is typically 100 afford two main advantages first these models provide a basis for enhanced expression of the uncertainty associated with model inputs and therefore forecasts made by the model second these models allow for enhanced and more appropriate data assimilation from field observations to parameters therefore providing a greater potential uncertainty reduction through history matching as well as less potential for bias incurred as a result of history matching these factors are of particular significance they underpin a model s ability to fulfill its role as a decision support tool freeze et al 1990 tartakovsky 2013 however high dimensional parameterization schemes are typically computationally burdensome to implement i e when using traditional high fidelity gradient based inversion methods doherty 2015 poeter et al 2014 moreover these schemes often require greater understanding of inverse problem theory to implement effectively doherty and hunt 2010 and doherty et al 2010a b provided guidance on how to practically implement some highly parameterized approaches including e g the use of pilot points as a spatial parameterization device numerous studies have adopted highly parameterized models towards informing management e g doherty 2003 dausman et al 2010 knowling et al 2015 cui et al 2018 voss 2011a b in contrast argued that simplified parameterization herein referred to as reduced parameterisation approaches be adopted these approaches involve the consideration of a small number of parameters 10 e g through enforcing hydraulic property homogeneity such an approach is advocated on a philosophical basis that focuses on the need to capture only the most important overall system processes and behaviour the metric for which is based on how well the model reproduces the primary signal components in the observation data the aim underpinning this approach is that the modeling process will allow for new system understanding to be gained by the practitioner e g by identifying the most influential parameter combinations voss 2011b this approach also promulgates over determined or well posed inverse problems many examples of this form of parameterization exist in the literature e g li et al 2009 michael and voss 2009 sun et al 2011 ala aho et al 2017 a number of recent studies that adopt paired complex simple model analyses have shown the propensity for simple models to display bias in important simulated outputs which may be amplified or even generated by the history matching process these simple models also generally underestimate uncertainty in important simulated outputs e g doherty and christensen 2011 watson et al 2013 white et al 2014 given the simplified nature of all environmental models this insight can be extended to even the most complex physics based numerical models as these are still gross simplifications of the natural systems they simulate the concept of a simple model can be thought of in this study as a model that possesses a reduced parameterization scheme relative to a complex model counterpart with a more detailed parameterization the concept of a complex simple model pair is adopted herein while reduced parameterization schemes may theoretically exhibit a compromised ability to express uncertainty in decision relevant simulated outputs the extent to which various commonly applied parameterization schemes impact the stochastic expression of decision relevant simulated outputs and decisions made on the basis of these outputs is presently unknown this gives rise to some key questions what level of parameterization is needed to adequately express uncertainty for a given decision relevant simulated output to what extent can parameterization be reduced without degrading the ability of the model to serve as a decision support tool the primary objective of this study is to address these questions in the context of two common water management decision making contexts this study contributes to the on going debate of model simplicity complexity by using empirical paired complex simple model analyses to characterize the ramifications of using reduced parameterization schemes in terms of the bias and variance uncertainty in decision relevant simulated outputs a bayesian decision analysis approach is adopted this approach is applied to investigate the consequences of commonly employed parameterization reduction strategies not only in terms of the prior and posterior probability density functions of decision relevant simulated outputs but also in terms of the management decisions that would be made on their basis two integrated surface water groundwater modeling case studies are considered first a complex synthetic model used to forecast ecologically sensitive streamflow characteristics and their response to groundwater abstraction and second a model of the hauraki plains new zealand used to forecast nitrate loading impacts on water quality to our knowledge this is the first empirically based study aimed at understanding model parameterization reduction outcomes in the decision making context the intention of this study is not to identify a best parameterization scheme for a given decision relevant simulated output or for multiple decision relevant simulated outputs but rather to reveal the factors governing the appropriateness or otherwise of commonly employed parameterization schemes for various decision relevant simulated outputs and provide guidance on that basis 2 methodology 2 1 overview the impact of parameterization reduction on resource management decision making is explored herein using two integrated surface water groundwater models the first concerns outcomes of parameterization reduction on simulating ecologically important streamflow characteristics and their changes in response to groundwater abstraction also commonly referred to as stream depletion e g barlow and leake 2012 using a complex synthetic model the second concerns outcomes of parameterization reduction for simulating nutrient loading change impacts on water quality using a real world regional scale model hauraki plains new zealand both case studies are approached from a bayesian perspective involving the comparison between prior and posterior probability density functions pdfs of decision relevant simulated outputs made by the models with varying levels of parameterization to understand the ramifications of adopting reduced parameterization schemes representing a fundamental form of model simplification the analyses are undertaken with respect to a model with the most detailed parameterization paired complex simple model analyses are adopted to characterize bias and variance in decision relevant simulated outputs related to parameterization reduction this is achieved by treating the model with the most detailed parameterization as unbiased and as the best available means to represent simulated output uncertainty compared to models with reduced parameterization many studies have adopted this approach to numerical experimentation e g doherty and christensen 2011 oliver and alfonzo 2018 white et al 2014 three parameterization schemes are investigated for each of the case studies as follows coarse a reduced parameterization scheme where aquifer properties e g hydraulic conductivity porosity are considered in a spatially uniform sense only this approach ensures that the number of adjustable parameters is such that the inverse problem is over determined voss 2011b suggested the use of two to four adjustable parameters as a starting point with perhaps only one or two more if necessary in such an approach intermediate a spatially and temporally distributed parameterization scheme where aquifer property and boundary condition heterogeneity are considered while maintaining a number of adjustable parameters that is low enough e g approximately 2 500 such that history matching and uncertainty analysis methodologies that rely on the population of a high fidelity jacobian matrix and the singular value decomposition thereof can be employed this scheme is designed to follow the guidance in doherty et al 2010a doherty and hunt 2010 doherty et al 2010b including the use of pilot points as a spatial parameterization device fine a highly detailed spatially and temporally distributed aquifer property and and boundary condition parameterization scheme this approach results in a very large number of adjustable parameters e g 100 000 including model discretization scale parameters and time step scale parameters this scheme relies on recently developed ensemble based history matching and uncertainty analysis methodologies that alleviate the computational burden associated with computation of jacobian matrices e g chen and oliver 2013 white 2018 numerical experiments involving these parameterization schemes for both case studies employ an iterative ensemble smoother ies methodology pestpp ies white 2018 as a basis for history matching and posterior uncertainty quantification use of the ies method is deemed necessary given that exploration of the fine parameterization scheme was only possible from a computational standpoint using this approach moreover use of a consistent methodology among the parameterization experiments ensures that the causal factors of performance variability between experiments can be attributed to parameterization only because the ies method relaxes the relation between number of parameters and number of forward model evaluations required a similar number of model evaluations are used for all parameterization experiments an ensemble size of 100 is employed for all experiments this is deemed sufficient to avoid under utilization of observation data i e under fitting on the basis of a solution space dimensionality exploration moore and doherty 2005 performed on the intermediate parameterization scheme of the hauraki plains case study see supplementary information the python scripting packages flopy bakker et al 2016 and pyemu white et al 2016 are used to construct the numerical models and the pest doherty 2015 interface for these models respectively 2 2 bayesian and decision analysis a bayesian approach is adopted whereby both prior and posterior pdfs of the decision relevant simulated outputs are assessed each of which is referred to herein as an output pdf individual prior and posterior output pdfs for each parameterization scheme are formed by propagating prior and posterior parameter ensembles through the models respectively on the basis of these output pdfs we investigate the impact of parameterization choices on the stochastic nature of decision relevant simulated outputs any differences between output pdfs can be attributed directly to changes in parameterization in order to understand how decision makers may choose to implement a management action in a probabilistic context and how this is influenced by model parameterization we place the simulated outputs in a simple i e normative bayesian decision analysis context parmigiani and inoue 2009 the concepts underpinning our decision analysis approach are presented in the appendix the approach involves defining a decision threshold that represents the critical value at which decision makers would choose to implement a particular resource management action a decision threshold is defined for each decision relevant simulated output pdf this approach facilitates direct evaluation of the role that parameterization reduction plays in risk based decision making furthermore since the model with the finest parameterization is treated as the best available and unbiased estimator of simulated outputs the concepts of type i i e false positive and type ii i e false negative statistical errors are considered in combination with decision thresholds to evaluate decisions in terms of their correctness or otherwise made on the basis of models with reduced parameterization schemes any discrepancy between the output pdfs based on different parameterization schemes with respect to the decision threshold may yield inappropriate decision making because a reduced parameterization model either overstates type i error or understates type ii error the simulated effectiveness of a potential resource management action compared to the corresponding output pdf of the fine parameterization model 3 synthetic model case study the consequence of parameterization reduction on ecologically important streamflow characteristics and their changes in response to groundwater abstraction is investigated using a synthetic surface water unsaturated zone groundwater model the model represents a 1 476 km2 hypothetical hillslope catchment displaying significant surface water groundwater interaction fig 1 modflow nwt niswonger et al 2011 is used to simulate steady state and transient groundwater surface water and unsaturated flow conditions within the catchment the model comprises 3 layers 200 rows and 144 columns with a uniform horizontal grid spacing of 250 m following simulation of steady state conditions as a means of producing dynamic equilibrium initial conditions a transient model simulation is undertaken using daily stress periods spanning a three year period salient aspects of the synthetic model to the simulation of ecologically important streamflow characteristics such as low flow reliability and their changes in response to groundwater abstraction are as follows rainfall partitioning into evapotranspiration surface runoff and groundwater recharge is simulated using the unsaturated zone flow uzf package which employs a kinematic wave approximation to the richards equation niswonger et al 2006 surface runoff is routed directly to neighboring stream segments streamflow and stream aquifer interaction is simulated using the streamflow routing sfr package niswonger and prudic 2005 streamflow generation occurs entirely within the catchment i e no stream inflow is specified at upper stream reaches 3 1 decision relevant simulated outputs decision relevant simulated outputs for the synthetic model case study relate to ecologically important streamflow characteristics in particular stream low flow reliability and its response to groundwater abstraction ecologically important streamflow is characterized here by considering low flow conditions and specifically q95 the streamflow rate that is exceeded 95 of the time the following low flow streamflow characteristics are treated as decision relevant simulated outputs 1 nconsecday q95 base the maximum number of consecutive days where the streamflow rate is below q95 in the absence of groundwater abstraction 2 nconsecday q95 abstr the maximum number of consecutive days where the streamflow rate is below q95 in the presence of groundwater abstraction 3 δnconsecday q95 the change in the maximum number of consecutive days where the streamflow rate is below q95 in response to groundwater abstraction these simulated outputs are collected at two locations of interest red triangles fig 1 during the third year of simulation groundwater abstraction induced changes in streamflow characteristics for the simulated output δnconsecday q95 is calculated by differencing outputs between two model scenario runs as follows the base model which does not simulate abstraction the abstraction model which simulates three water supply bores black triangles fig 1 operating during summer months using the specified flux wel package niswonger et al 2011 3 2 the truth model and history matching a reference model referred to herein as the truth model is used in the synthetic model analysis for two purposes first to serve as a basis for generating the observation dataset against which models with different levels of parameterization are history matched second to provide a robust basis for evaluating the performance of different parameterization strategies with respect to decision relevant simulated outputs the truth model parameter set is selected from an ensemble of 1000 stochastic prior parameter realizations of the complex fine parameterization model the truth realization produced between the 60th and 95th percentile for a range of decision relevant simulated outputs outputs collected from the truth model comprising the observation dataset spanning the first two years of the simulation used for history matching include daily streamflow rates at five locations green triangles fig 1 monthly groundwater levels at nine locations within the upper aquifer layer black crosses fig 1 independent gaussian noise with zero mean and a standard deviation of 0 1 m for groundwater levels and 10 of the truth model simulated values for streamflow rates is added to the truth model derived outputs serving as history matching observations 3 3 numerical experiments details of the three model parameterization schemes investigated for the synthetic model case study are summarized in table 1 it is worth noting that while spatial parameterization is commonplace doherty and hunt 2010 temporal parameterization is rarely employed exceptions include knowling and werner 2017 masterson et al 2016 the uncertainty associated with time varying parameters such as groundwater recharge rates and their contribution to decision relevant simulated output uncertainty is often neglected given the transient nature of the decision relevant simulated outputs considered in the synthetic model case study temporal parameterization was deemed appropriate geostatistical correlation is specified for spatially distributed parameter types using an exponential variogram with a range of 2 500 m and a sill proportional to the expected prior variance see supplementary information for a summary of parameter information prior variance terms are assumed equivalent between parameterization schemes to reflect current modeling practice correlation between temporal parameters is also specified using an exponential variogram with a range of nine stress periods i e nine days non spatially and temporally distributed parameters are assumed to be uncorrelated 3 4 results the ability of the synthetic model to reproduce historical observations decreases as the parameterization detail is reduced ensemble simulated versus observed plots are presented in the supplementary information this finding is not surprising given that more detailed parameterization schemes allow for enhanced flexibility in assimilating the information contained within field observations via parameter conditioning in general considerable differences are apparent in the pdfs of decision relevant simulated outputs across the three parameterization schemes figs 2 and 3 the primary features apparent in figs 2 and 3 are expanded on below fine parameterization scheme the absolute output pdfs for the fine parameterization scheme figs 2 and 3 a and b display first and second moment changes between prior and posterior stances the prior absolute output pdfs display multi modal behavior in particular for forecast location s 2 the corresponding posterior pdfs however almost entirely collapse on one of these modes for the difference output pdfs the fine parameterization scheme figs 2 and 3 c also yield smaller second moments for a posterior stance these results indicate that the history matching process conditioned parameters or parameter combinations in the fine scheme that influence both the absolute and difference simulated outputs of interest the similar informedness of absolute and difference simulated outputs suggests that the potentially damaging information signal present in the observation data is not cancelled out through differencing prior absolute pdfs for reduced parameterization schemes relative to fine scheme the prior pdfs of the absolute outputs for the intermediate scheme figs 2 and 3 d and e exhibit similar second moments compared to those for the fine scheme small second moments for prior absolute pdfs are evident however for the coarse scheme figs 2 and 3 g and h indicating uncertainty underestimation such underestimation of uncertainty suggests that the absolute simulated outputs depend on parameterization detail that is present in the fine scheme but not in a reduced parameterization scheme prior difference pdfs for reduced parameterization schemes relative to fine scheme the prior pdfs of the difference outputs for the intermediate scheme like those of the absolute outputs also display similar first and second moments to the fine scheme figs 2 and 3 f similar first and second moments for the prior difference pdfs are also evident for the coarse scheme figs 2 and 3 i in contrast to the prior absolute pdfs these findings highlight the protection against uncertainty underestimation afforded when adopting a prior uncertainty stance as opposed to a posterior stance discussed below with an appropriately formulated modeling objective e g consideration of difference decision relevant simulated outputs prior versus posterior pdfs for reduced parameterization schemes the posterior pdfs of absolute and difference outputs for both the intermediate and coarse schemes figs 2 and 3 d i like the fine scheme display first and second moment changes relative to their corresponding prior pdfs the extent of these changes is shown to be related to the degree of parameterization this suggests that both the absolute and difference simulated outputs are overly sensitive to the upscaled aggregated parameters present in the reduced parameterization schemes parameters that are informed by observations used for history matching the extent to which changes in the output pdfs through history matching increases with more significant parameterization reduction suggests the presence of inappropriate simplification and subsequent null space entrainment the adjustment of simple model parameters through history matching that are un informed in the complex model and should therefore remain at expert knowledge based values e g doherty and christensen 2011 white et al 2014 posterior pdfs for reduced parameterization schemes relative to fine scheme for forecast location s 1 the posterior pdfs of absolute and difference outputs for both the intermediate and coarse schemes display similar first moments relative to those of the fine scheme i e indicating no significant bias fig 2 d i while the posterior pdfs for the intermediate scheme also display relatively similar second moments to the fine scheme the posterior pdfs for the coarse scheme exhibit degenerate i e numerically zero variance second moments this has significant implications for effective decision support underestimating uncertainty and therefore risk often leads to model failure e g doherty and simmons 2013 for forecast location s 2 however which is less spatially integrating than location s 1 the posterior pdfs for both the intermediate and coarse schemes generally display considerable history matching induced bias fig 3 d i moreover the posterior pdfs for the intermediate scheme is diffuse compared to the fine scheme whereas the posterior pdfs for the coarse scheme are comprised almost entirely of one or two simulated output values the computational times associated with models employing the three parameterization schemes are generally equivalent see supplementary information this indicates that spatial and temporal heterogeneity did not generally increase run times increased parameterization did however cause model non convergence e g of the 100 prior realizations for the coarse intermediate and fine parameterization schemes 99 90 and 84 realizations converged respectively 4 hauraki plains case study the influence of parameterization reduction on the simulated fate and transport of nitrate in a linked hydrologic model of the hauraki plains new zealand fig 4 is also investigated the model simulates groundwater and surface water flow with modflow nwt niswonger et al 2011 and simulates advective and dispersive nitrate transport in surface water and groundwater with mt3d usgs bedekar et al 2016 the general structure of the model is described in white 2018 briefly 7 layers with spatially distributed thickness 124 rows and 70 columns of a uniform 1 km grid steady state flow conditions historic and forecast periods constant nitrate loading conditions transient tritium loading conditions separate history matching and forecast time periods are simulated the historic time period encompasses 1955 2018 with annual time steps to capture the transient nature of tritium loading and the temporal evolution of the simulated nitrate distribution the initial concentrations of nitrate and tritium are assumed to be representative of the conditions before intense dairy practice and before atmospheric nuclear bomb testing respectively the forecast time period spans a period of 10 years from 2018 with annual time steps the initial nitrate concentrations for this forecast period are the final simulated concentrations from the historic period two scenarios are constructed around nitrate loading in the forecast period using the base historic period history matched nitrate loading rate using a 20 reduction of the base rate the streamflow transport sft package of mt3d usgs is used to simulate the transfer of nitrate from the groundwater system to the surface water system and the subsequent transport of nitrate in the surface water system to the firth of thames 4 1 decision relevant simulated outputs the simulated outputs of interest for resource management taken from scenario model outputs include the cumulative nitrate mass discharged to the firth of thames over the 10 year forecast period kg an in stream nitrate concentration mg l piako fig 4 a groundwater nitrate concentration mg l blue spring fig 4 also considered are difference or change outputs of interest by subtracting the 20 reduction scenario pdf from the corresponding base case pdf 4 1 1 history matching history matching is undertaken using several types of long term average groundwater flow system observations and transient nitrate and tritium transport observations including groundwater levels surface water flows groundwater and surface water nitrate concentrations groundwater tritium concentrations a total of 571 observations are used to condition model parameters through history matching the location of these observations within the model domain is included in the supplementary information additionally inequality constraints are specified to penalize simulated groundwater levels significantly above land surface in the uppermost model layer see white 2018 for more details 4 2 numerical experiments similar to the synthetic case study we employ three parameterization schemes to evaluate how parameterization choices influence the stochastic expression of important simulated outcomes table 2 summarizes these parameterization schemes for spatially distributed parameter types geostatistical correlation is specified using an exponential variogram with a sill proportional to the expected prior variance and range of 4 000 m see supplementary information for a summary of parameter information prior variance is equivalent between parameterization schemes to reflect current modeling practice remaining parameters are assumed to be uncorrelated in the prior uncertainty stance additionally distance based localization e g chen and oliver 2017 is used to eliminate spurious long distance cross correlations within the ies framework for the fine parameterization scheme this is accomplished using exponential variograms with sill of 1 0 and specified ranges for each unique observation data type used for history matching as follows surface water flow 40 000 m groundwater levels 20 000 m groundwater nitrate concentration 20 000 m surface water nitrate concentration 40 000 m groundwater tritium concentration 60 000 m for each individual observation the distance to each spatially distributed discretization scale parameter is used with the corresponding variogram to calculate the geostatistical strictly positive correlation between the observation location and the parameter location this value is then used to dampen parameter adjustments within the ies by multiplying the observation ensemble by the requisite localization matrix in a hadamard product sense see chen and oliver 2017 for more information on covariance inflation localization schemes 4 3 results the degree to which historical observations are reproduced generally decreases as parameterization detail is reduced as expected ensemble simulated versus observed plots are given in the supplementary information this trend is particularly pronounced for observation data types that depend on fine scale heterogeneity e g spatially distributed nitrate and tritium concentrations in general considerable differences in the pdfs of decision relevant simulated outputs across the three parameterization schemes are evident figs 5 7 a brief summary of patterns revealed in figs 5 7 is provided below for all three outputs of interest the fine parameterization scheme yields a similar second moment and only slight changes in the first moment between the prior and posterior uncertainty stances this indicates that history matching has only slightly conditioned the parameters or parameter combinations present in the fine scheme that influence the outputs of interest while the intermediate scheme yields more diffuse prior pdfs compared to the fine scheme the posterior pdfs display similar second moments to the fine scheme the larger second moment reduction for the intermediate scheme relative to the fine scheme suggests an inappropriately high level of conditioning of the parameters present in the intermediate scheme effectively overstating the value of the information in the observations the prior pdf second moments for the coarse scheme are significantly smaller than those of the fine scheme for all decision relevant simulated outputs except for the differenced firth of thames cumulative nitrate load output fig 5 i this indicates that the appropriateness of a coarse parameterization scheme from a prior stance is highly dependent on the decision relevant simulated output in question such an approach may be appropriate for processed e g differenced transformed and spatially integrated water quality related simulated outputs all of the coarse posterior pdfs however are nearly degenerate this indicates that the coarse parameterization scheme does not have the flexibility complexity to simultaneously express uncertainty notwithstanding producing similar first moments compared to the intermediate scheme for important simulated outputs while adopting a posterior stance this precludes effective decision support for the simulated cumulative nitrate load to the firth of thames fig 5 the posterior pdfs for the intermediate parameterization scheme while not reliably capturing the absolute nitrate load due to history matching induced bias fig 5 a and b vs d and e appropriately captures the percent change in nitrate load fig 5 c vs f this indicates that while a reduced parameterization scheme with some ability to express spatial heterogeneity and boundary condition uncertainty may not be suitable for simulating absolute outcomes it may be suitable for simulating changes however for all simulated outputs of interest at the piako and blue springs sites figs 6 and 7 the posterior pdfs for the intermediate scheme display bias this highlights that the intermediate scheme possess a greater ability to reliably simulate spatially integrated rather than local quantities this is due to the sensitivity of local transport simulated outputs to fine scale heterogeneity the coarse parameterization scheme offers considerable benefits related to model run times the mean run time is less than half that for the fine scheme see supplementary information this can be attributed to the lack of heterogeneity and the resulting well conditioned system of equations solved by the forward model 5 decision analysis while the prior and posterior output pdfs of models with both fine and reduced parameterization schemes generally display substantially different behaviors across the various decision relevant simulated outputs for both case studies the implications of these discrepancies in terms of resource management decision making requires further investigation to this end we employ decision theory concepts introduced above and presented in detail in the appendix to quantitatively assess the degree of similarity or otherwise in the output pdfs in terms of summary metrics which represent how the probabilistic outcomes from each of the parameterization experiments might compare in a risk based management decision context the decision thresholds representing a simulated outcome that would lead to the implementation of some given management action are taken as the red dashed lines shown on the difference output pdfs for both case studies 5 1 synthetic model a decision analysis concerning the changes in ecologically sensitive streamflow characteristics in response to abstraction is first explored using the synthetic model the truth values for the difference pdfs red dashed lines figs 2 and 3 are treated as decision thresholds i e consider that a maximum of the truth value of δnconsecday q95 can be tolerated from an ecological sustainability perspective all of the parameterization schemes indicate non zero probability that δnconsecday q95 decision thresholds will be exceeded from a prior stance figs 2 and 3 however when adopting a posterior stance threshold exceedance probabilities vary significantly among the different parameterization schemes for location of interest s 1 both the fine and intermediate posterior pdfs indicate non zero exceedance probability the degenerate posterior pdf for the coarse scheme however conveys absolute certainty that δnconsecday q95 will be 1 notwithstanding that is equal to the truth value and that the probability of threshold exceedance is zero this may yield a statistical type ii i e false negative error in decision making contexts similarly for location s 2 the coarse scheme shows zero posterior threshold exceedance probability whereas the posterior difference pdf for the fine and intermediate schemes yield non zero exceedance probability significantly non zero probability for the intermediate scheme giving rise to the albeit low probability potential for erroneous decisions relative to the fine scheme table 3 summarizes the decision difficulty d and decision outcome similarity s for the difference output pdfs analyzed d values exhibit a reduction following history matching where only minor first moment changes toward the decision threshold are evident this serves as additional evidence for the presence of decision relevant information in the observation dataset while prior d values are approximately equivalent across the parameterization schemes posterior d values are zero or near zero i e an easy decision for the coarse scheme for forecast location s 1 and for the fine scheme for forecast location s 2 a d value of zero means the entire distribution is entirely on one side of the decision threshold all s values when adopting a prior uncertainty stance are equal to 1 0 for both the intermediate and coarse schemes this suggests that equivalent management action would be undertaken on the basis of the prior pdfs for the intermediate and coarse parameterization models compared to those for the fine parameterization model the posterior pdfs for the intermediate and coarse schemes however produce considerably lower s values this contextualizes the potential shortcomings regarding history matching models with reduced parameterization schemes e g uncertainty underestimation compromising effective risk based decision support through consideration of a management decision threshold interestingly though despite the ill effects of history matching reduced parameterization models revealed above the same decision outcomes would have generally been reached i e 93 of the time for the intermediate scheme and 68 of the time for the coarse scheme highlighting the fundamental role of the decision threshold definition 5 2 hauraki plains a decision analysis concerning the hauraki plains 20 nitrate loading change scenario is now explored consider that a minimum of 10 reduction in nitrate load to the firth of thames fig 5 c f and i red dashed line is required to justify a 20 nitrate loading reduction action in this situation both the fine and intermediate schemes indicate a non zero probability of achieving the required reduction from both a prior and posterior stance however the coarse parameterization degenerate posterior incorrectly conveys absolute certainty that the nitrate load reduction will be 9 which may yield a type ii i e false negative error a similar hypothetical construction can be made for the outputs of interest at piako and blue springs sites using a reduction threshold target of 0 5 m g l nitrate for the piako site fig 6 the fine parameterization posterior indicates that the change in nitrate concentration will be greater than 0 5 m g l c while both the intermediate and coarse scheme pdfs are biased towards lower concentrations f i yielding type ii i e false negative errors which would ultimately lead to an incorrect decision to not implement the 20 reduction action however if the same decision construct is applied to the blue spring site both the intermediate and coarse schemes would lead to the same decision as the fine parameterization the decision outcome comparison between the fine and reduced parameterization model pdfs is summarized in table 4 generally the s values indicates that the prior pdfs for the intermediate and coarse parameterization models yield similar management action as the fine parameterization model furthermore posterior pdfs for reduced parameterization models yield improved s values for both firth and blue springs site but decreased s values for the piako site 6 discussion both case study results presented herein demonstrate that use of reduced parameterization schemes may suffer from several ill effects such as corrupted first and second moments of decision relevant simulated output pdfs given that the two case studies represent common yet widely differing decision support modeling applications the relationships revealed between parameterization reduction and the bias and uncertainty underestimation accompanying the different types of decision relevant simulated outputs are expected to be broadly generalizable i e transferable to other modeling studies much of the following discussion serves to support this our analysis shows that for a model with a coarse parameterization scheme e g involving spatially uniform aquifer property parameters to serve an appropriate basis for decision making support satisfaction of the following conditions can mitigate these ill effects each of which is discussed in more detail below a prior uncertainty stance is adopted i e no parameter conditioning through history matching is undertaken the decision relevant simulated output concerns quantities that do not depend on fine scale spatial heterogeneity herein including ecologically important streamflow characteristics and basin wide integrated nitrate loading changes the decision relevant simulated output concerns differences or changes in quantities herein including changes in nitrate loading scenarios the first condition reflects that any conditioning of highly spatially aggregated parameters is shown to cause degenerate posterior pdfs for most decision relevant simulated outputs this is due to the fact that use of such a reduced parameterization scheme forms a well posed inverse problem through implicit a priori regularization e g moore and doherty 2006 in which the parameter ensemble collapses into an artificially distinct region of maximum likelihood the degenerate nature of the posterior output pdfs preclude the coarse model s utility for risk based i e probabilistic decision making support posterior output pdfs for the coarse parameterization scheme not only display a severe underestimation of uncertainty but also display bias in most cases these issues are a result of inappropriate parameter compensation through history matching e g clark and vrugt 2006 white et al 2014 the unknowable magnitude of bias and underestimation of uncertainty ultimately leads to a unknowable risk of incorrect resource management action notwithstanding this a number of correct management decisions for a range of different simulated outputs are possible using the coarse scheme following history matching this is despite the relatively poor fits obtained with the coarse parameterization scheme underscoring that the level of fit to measurements of past system states is not directly related to model decision support utility the second and third conditions collectively reflect that the appropriateness of a coarse parameterization scheme is shown to be highly dependent on the decision relevant simulated output in question specifically the second condition reflects that the coarse scheme is shown to be inappropriate when decision relevant simulated outputs depend on relatively local scale property heterogeneity heterogeneity that occurs at a smaller spatial scale than is represented by the parameterization scheme this condition is particularly important for water quality related simulated outputs which are known to be more sensitive to fine scale heterogeneity e g riva et al 2008 yoon and mckenna 2012 zheng and gorelick 2003 for simulated outputs that are sensitive to local scale transport properties and processes e g surface water nitrate concentration at the piako site in the hauraki plains example the prior pdf second moment is significantly underestimated due to neglecting heterogeneity in aquifer conductivity and porosity this may invalidate the use of coarse parameterization schemes for decision support that is concerned with spatially discrete outputs of interest such as concentrations in contrast the coarse scheme is shown to be appropriate when adopting a prior stance for simulated outputs that integrate transport properties and processes over large or entire catchment areas e g cumulative nitrate load to the firth of thames in the hauraki plains furthermore the coarse scheme is shown to be better suited to spatially integrated decision relevant simulated outputs concerning water quantity related questions through avoidance of significant posterior bias the third condition reflects that the coarse scheme is shown to be inappropriate for simulating decision relevant outputs in absolute terms for the nitrate transport related questions considered even for spatially integrated simulated outputs and when adopting a prior uncertainty stance only after simulated output differencing is undertaken are the ill effects of employing a coarse parameterization scheme alleviated this highlights the potential significant benefit in appropriately formulating the decision relevant simulated outputs that underpin decision support modeling investigations in terms of e g differences or transposes as supported by sepulveda and doherty 2015 and cui et al 2018 casting the water quantity related simulated outputs in terms of differences is not as effective in mitigating these ill effects due to the information signal present in the observation data the misdirection of which to upscaled aggregated parameters can be damaging is not cancelled out through differencing the above conditions underpinning an appropriate basis for decision making support for the coarse parameterization scheme also hold in general terms for the intermediate parameterization scheme that is even a highly parameterized scheme e g doherty and hunt 2010 hunt et al 2007 involving 2500 spatially and temporally variable parameters serves best as a robust decision support tool either from a prior standpoint or from a posterior standpoint except where simulated outputs are absolute or are sensitive to local scale property heterogeneity otherwise the potential for bias in simulated outputs and therefore the potential for wrong decisions is marked e g decision making for the change in surface water concentration at the piako site may incur a false negative i e type ii statistical error notwithstanding this the intermediate scheme generally displays superior performance compared to the coarse scheme this is evidenced by the fact that the intermediate scheme displays apparent immunity to bias for spatially integrated water quantity related simulated outputs and for differenced and spatially integrated water quality related simulated outputs this lack of bias raises the potential for a second moment adjustment term to be applied to overcome any potential uncertainty underestimation towards rectifying decision support reliability discussed below the intermediate scheme also generally yields non degenerate posterior output pdfs underestimation of uncertainty associated with decision relevant simulated outputs is not as significant for the intermediate scheme compared to the coarse scheme due to the flexibility afforded by enhanced expression of prior parameter uncertainty moreover the intermediate scheme pdfs generally share similar second moments to the fine scheme these factors collectively contribute to the superior decision analysis statistics evident for the intermediate scheme compared the coarse scheme the general similarity in the second moments of the posterior output pdfs for the intermediate and fine parameterization schemes is of particular interest this suggests that use of parameterization schemes such as pilot point based schemes form a robust basis i e do not significantly underestimate simulated output variance for first order second moment fosm uncertainty estimation techniques that are commonly used for e g data worth and optimal monitoring design analyses e g fienen et al 2010 wohling et al 2016 the apparent safety afforded by the three conditions described above when using a reduced parameterization schemes in the decision support context is an encouraging finding this is because of the fact that these conditions are commonly satisfied though not commonly employed in real world decision support modeling analyses the first condition is met when decision relevant information within the observation data are lacking or when parameter conditioning is considered unnecessary from either a study objective standpoint e g where more conservative uncertainty estimates surrounding a decision relevant simulated output are satisfactory or a time money resource constraint standpoint it is important to note however that the employment of complex physics based models from only a prior uncertainty stance is in stark contrast to current environmental modeling practice this is a reflection of the fact that observation data are expensive and difficult to collect in combination with the widespread belief that an environmental model contains appropriately detailed receptacles i e parameters for expressing information contained within observation data while field observation data are central to the development of a sound system understanding and conceptual model the results presented herein indicate that use of observation data for history matching regional scale numerical models should be approached with caution i e in such a way that the potentially damaging side effects of parameter conditioning are considered e g doherty and christensen 2011 oliver and alfonzo 2018 e g by processing observation data as discussed in detail below the second and third conditions are met e g where water quantity issues involving simulation of the change in catchment scale water budget components in response to climate variability and or water use scenarios e g knowling et al 2015 post et al 2018 or water quality issues involving simulation of spatially and or temporally integrated mass flux changes e g the firth of thames decision relevant simulated output considered in the current study are the focus of a modeling investigation where decision relevant simulated outputs exhibit local scale aquifer property dependence e g small scale tracer tests this calls to question the suitability of a regional scale numerical model altogether compared to that of e g a local scale advective dispersive transport models or sub models e g li et al 2006 sreekanth and moore 2018 deployment of environmental models in not only a decision relevant simulated output specific manner but also in a manner where decision relevant simulated outputs concern relative or differenced model quantities the importance of which has been demonstrated by e g cui et al 2018 sepulveda and doherty 2015 is also in stark contrast to current environmental modeling practice notwithstanding the conditions discussed above the potential for significant bias in e g spatially localized water quantity and quality related simulated outputs for the intermediate schemes arising from history matching induced parameter compensation and null space entrainment defined earlier suggests the need for even further parameterization detail where parameters need to be conditioned to reduce the uncertainty surrounding decision relevant simulated outputs enhanced parameterization detail will afford further protection against bias by reducing the extent of parameter entrainment during history matching e g doherty and christensen 2011 watson et al 2013 additional means to mitigate against these ill effects are the observation noise or weight adjustment techniques these effectively result in a lesser fit to observation data being sought through history matching e g christensen 2017 cooley and christensen 2006 doherty 2015 oliver and alfonzo 2018 nevertheless in order to minimize the potential for bias it is encouraged that the parameterization scheme adopted be as fine as possible e g the fine parameterization schemes adopted herein whereby expression of model input uncertainty is commensurate with the model spatial and temporal discretization handling of such high parameter dimensions however requires application of efficient ensemble based history matching and uncertainty quantification algorithms e g chen and oliver 2013 white 2018 and scripting tools or other specialized software e g bakker et al 2016 white et al 2016 a potential alternative means towards addressing these ill effects includes the use of decision targeted parameterization schemes e g parameterization densification around the location of the decision relevant simulated output and its recharge area such an approach is in accordance with modeling objective focused model construction parameterization and history matching it is difficult to discern a general widely applicable pattern whereby a model with a reduced parameterization scheme can be deployed reliably for decision making when it is necessary to assimilate information from observations in this case discretization scale parameterization affords the most robust decision support in the absence of this level of parameterization improved and targeted objective function formulations may also provide protection against inappropriate parameter compensation and hence decision relevant output bias by removing the signal components that would otherwise inform parameters that were excluded a number of previous studies have demonstrated the usefulness of observation filtering i e omitting non decision relevant information containing observations and observation processing such as transforming and differencing e g knowling and werner 2017 white et al 2014 we suggest that more work be focused in this area nevertheless if history matching is deemed necessary because the available observation dataset contains decision relevant information or in order to reduce uncertainty in the parameters that influence the decision relevant simulated outputs then the state observations used for conditioning must not be sensitive to missing parameterization detail otherwise the potential for parameter compensation induced bias in decision relevant simulated outputs is large which may lead to incorrect resource management action these findings highlight that designing an appropriate and targeted model parameterization scheme requires not only a thorough understanding of the physics of the system being simulated but also knowledge of how information is transferred from observations to parameters and subsequently to simulated outputs in bayesian inverse problems our use of the ies method white 2018 for all parameterization schemes herein allows for comparisons to be drawn between these schemes on a consistent algorithmic platform as stated posterior exploration for the fine scheme would have been virtually impossible from a computational resource perspective using traditional inversion methods i e that use finite difference perturbation for gradient computation it is worth noting however that higher fidelity methods could have been applied for the intermediate and coarse schemes e g null space monte carlo tonkin and doherty 2009 for the intermediate scheme and markov chain monte carlo e g vrugt 2016 for the coarse scheme however the extent to which use of higher fidelity methods for the intermediate and coarse schemes may yield discrepancies compared to use of lower rank methods such as those based on ensembles e g chen and oliver 2012 chen and oliver 2013 is presently unknown and requires investigation this is notwithanding the approximate nature of any posterior other than those obtained via rejection sampling tarantola 2005 future work will extend on previous studies that explore posterior uncertainty estimation algorithm performance e g keating et al 2010 and explore its pairing with parameterization dimensionality which are linked in practice it is also worth noting that our use of ies between schemes means that the performance of any scheme relative to another is independent of the number of runs required given the highly variable performance of different schemes for different decision relevant simulated outputs and under different uncertainty stances this represents an important finding we also encourage future work towards addressing the uncertainty underestimation of reduced parameterization schemes and the bias introduced through history matching with these schemes a follow on paper will explore the trade off between bias and variance in decision relevant simulated outputs and the potential utility of structural error terms e g doherty and christensen 2011 doherty and welter 2010 white et al 2014 to alleviate the detrimental outcomes of simplification and history matching leading to wrong decisions for example use of structural error terms may be of benefit in overcoming potential uncertainty underestimation when using reduced parameterization schemes that do not incur bias in decision relevant simulated outputs 7 conclusions this study investigates the ramifications of parameterization reduction or simplification in the context of environmental management decision making two case studies are considered involving several types of decision relevant simulated outputs it is demonstrated that the parameterization process the subjective expression of model input uncertainty plays a fundamental role in the decision support context the extent to which a model can reliably support decision making is shown empirically to be highly dependent on not only the level of parameterization detail employed by a model but also the decision relevant simulated output of interest and whether or not parameters are subjected to conditioning through history matching we demonstrate the ill effects of inappropriate parameterization namely the potential for bias and underestimation of uncertainty accompanying decision relevant simulated outputs and the resulting potential for incorrect decision making and resource management action the case specific nature of these ill effects is also demonstrated these findings highlight the need for parameterization simplification strategies to be targeted with respect to decision relevant simulated outputs and data assimilation needs they also highlight that the parameterization aspect of the modeling process should be undertaken in a decision relevant manner which requires definition at early stages of a modeling project to support management this study reveals that the detrimental outcomes of reduced parameterization in the environmental model based decision support context can be mitigated via the following primary factors deployment of a model on the basis of prior parameter i e uncalibrated realizations adopting a prior uncertainty stance with reduced parameterization models is found to yield similar decision outcomes compared to the finest parameterization scheme considered in most cases this approach is in stark contrast to current environmental modeling practice a reflection of the fact that observation data are expensive and difficult to collect in combination with the widely accepted belief that an environmental model contains appropriately detailed receptacles i e parameters for expressing information contained within observation data however we demonstrate that parameter conditioning through history matching i e calibrating with a reduced parameterization scheme may incur significant biases in decision relevant simulated outputs these biases arise from parameter compensation e g clark and vrugt 2006 white et al 2014 as a result of using an overly simplified parameterization scheme appropriate formulation of decision relevant simulated outputs i e selecting an appropriate question for the modeling analysis to address we demonstrate that focusing on difference or change simulation outputs that do not depend on local scale heterogeneity provide defence against ill effects of reduced parameterization of regional scale models that are used for decision support this is shown to be particularly important for water quality related simulated outputs for example changes in basin wide integrated nitrate load simulated output pdfs produced by reduced parameterization models are shown to be significantly less prone to bias and uncertainty underestimation and therefore provide a more robust basis for decision making under uncertainty than spatially discrete stream and spring concentration change outputs ultimately modeling practitioners must balance choices and assumptions related to parameterization with the many other model construction and deployment choices that must be made when constructing a real world environmental model for decision support purposes we encourage practitioners to critically consider model simplification decisions related to parameterization including through use of the general guidance provided on the basis of the empirical findings presented herein where the impact of such parameterization simplification on decision relevant simulated outputs is unknown and where history matching is deemed necessary to reduce the uncertainty of simulated outputs of interest we suggest to err on the side of caution employing as finer parameterization scheme as possible given numerical stability and model run time constraints acknowledgments we would like to acknowledge brioch hemmings and zara rawlinson for help building the hauraki plains model and john hadfield bevan jenkins and sung soo koh at waikato regional council new zealand for providing several of the datasets for the hauraki plains model we wish to thank brioch hemmings mike fienen and john doherty for their helpful review comments we also acknowledge ty ferre and an anonymous reviewer whose comments helped improve the paper the data files scripts and software used in the synthetic model and hauraki plains model simplification analyses can be obtained from the corresponding author this research was performed as part of the smart models for aquifer management programme funded by the ministry of business innovation and employment new zealand as well as by a commercial contract with waikato regional council appendix decision analysis concepts for a given decision relevant simulated output pdf and associated decision threshold the following quantities are defined 1 a 1 n n 1 if ϕ n ϕ 0 0 otherwise and 2 b 1 n n 1 if ϕ n ϕ 0 0 otherwise where ϕn is an individual value of simulated output which taken collectively over all n ϕ values forms the output pdf conceptually a and b measure the probability of the decision relevant simulated output being less than or equal to or greater than the decision threshold respectively with these quantities we define the decision difficulty d chlumpsky 2017 as 3 d 1 a b 1 a b which measures how the samples in the output pdf are distributed about the decision threshold if equal numbers of ϕ values are on either side of the threshold d is equal to 1 indicating a difficult decision there is equal probability of the simulated output being higher or lower than the threshold whereas if all ϕ values are on one side of the threshold d is equal to 0 indicating an easy decision a metric can also be defined to compare two output pdfs against the same decision threshold which we term decision outcome similarity s as 4 s 1 a 1 a 2 2 b 1 b 2 2 2 where subscripts of a and b correspond to different output pdfs s ranges from 0 the output pdfs are exclusively on opposite sides of the decision threshold to 1 the output pdfs are exclusively on the same side of the decision threshold supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 04 010 appendix b supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
654,the degree with which to parameterize a computer model that is to be used for risk based resource management decision support has been a topic of much discussion in the environmental modeling industry and remains a difficult choice facing practitioners high dimensional parameterization schemes allow for a more robust expression of model input uncertainty over traditional lower dimensional schemes but often incur a higher computational burden and require greater understanding of inverse problem theory to implement effectively however a number of significant questions remain such as what level of parameterization is needed to adequately express uncertainty for a given decision relevant simulated output and to what extent can a simplified parameterization be adopted while maintaining the ability of the model to serve as a decision support tool this study addresses these questions among others by using empirical paired complex simple model analyses to investigate the consequences of reduced parameterization on decision relevant simulated outputs in terms of bias incursion and underestimation of uncertainty a bayesian decision analysis approach is adopted to facilitate evaluation of parameterization reduction outcomes not only in terms of the prior and posterior probability density functions of decision relevant simulated outputs but also in terms of the management decisions that would be made on their basis two integrated surface water groundwater model case study examples are presented the first is a complex synthetic model used to forecast groundwater abstraction induced changes in ecologically sensitive streamflow characteristics and the second is a real world regional scale model hauraki plains new zealand used to simulate nitrate loading impacts on water quality it is shown empirically that for some decision relevant simulated outputs even relatively high dimensional parameterization schemes 2 000 adjustable parameters display significant bias in simulated outputs as a result of improper parameter compensation induced through history matching relative to complex parameterization schemes 100 000 adjustable parameters ultimately leading to incorrect decisions and resource management action for other decision relevant simulated outputs however reduced parameterization schemes may be appropriate for resource management decision making especially when considering a prior uncertainty stance i e without undertaking history matching and when considering differences between simulated outputs that do not depend on local scale heterogeneity keywords environmental model decision making parameterization uncertainty quantification model error history matching 1 introduction the subjective process of parameterizing a computer model for the purposes of history matching and or uncertainty quantification involves nominating numerical quantities used by the model that are uncertain and representing this uncertainty quantitatively this constitutes one of the most difficult aspects of the environmental modeling process a number of studies have shown that the degree to which a model is parameterized can have significant impacts on a model s capacity to reproduce historical observations and quantify uncertainty for example cooley 2004 cooley and christensen 2006 gallagher and doherty 2007 show that the representation of spatially distributed environmental properties in a lumped or aggregated manner within a model may lead to significant structural noise e g doherty and welter 2010 refsgaard et al 2012 refsgaard et al 2006 which has implications for simulated outputs that are matched to field observations through history matching and for simulated outputs which underpin a model s utility in the decision support context i e forecasts moore and doherty 2005 showed that parameter zonation may lead to critical underestimation in forecast uncertainty fienen et al 2010 highlighted the importance of adopting a spatially distributed parameterization scheme to meaningfully quantify forecast uncertainty towards optimizing monitoring design networks most recently gosses and wohling 2019 demonstrated the potential outcomes of spatial parameterization and discretization simplification in terms of forecast bias and uncertainty variance however despite this widely varying parameterization schemes are still advocated for and employed in the literature e g hunt et al 2007 voss 2011a voss 2011b the concept of environmental model simplicity complexity which is commonly represented conceptually by parameterization dimensionality continues to receive considerable attention in the literature e g castilla rho 2017 simmons and hunt 2012 the need for the selection of an appropriate level of model simplicity complexity to be forecast and data targeted termed defensible complexity in a recent review paper by anneli 2017 has been discussed by a number of studies e g doherty and simmons 2013 ferre 2017 however there is presently limited guidance on and no benchmark for judging the appropriateness or otherwise of appropriate level of parameterization in the context of risk based resource management decision making hunt et al 2007 presented an argument for increased parameterization or highly parameterized approaches they argued that models employing a high degree of parameterization detail where the number of adjustable parameters is typically 100 afford two main advantages first these models provide a basis for enhanced expression of the uncertainty associated with model inputs and therefore forecasts made by the model second these models allow for enhanced and more appropriate data assimilation from field observations to parameters therefore providing a greater potential uncertainty reduction through history matching as well as less potential for bias incurred as a result of history matching these factors are of particular significance they underpin a model s ability to fulfill its role as a decision support tool freeze et al 1990 tartakovsky 2013 however high dimensional parameterization schemes are typically computationally burdensome to implement i e when using traditional high fidelity gradient based inversion methods doherty 2015 poeter et al 2014 moreover these schemes often require greater understanding of inverse problem theory to implement effectively doherty and hunt 2010 and doherty et al 2010a b provided guidance on how to practically implement some highly parameterized approaches including e g the use of pilot points as a spatial parameterization device numerous studies have adopted highly parameterized models towards informing management e g doherty 2003 dausman et al 2010 knowling et al 2015 cui et al 2018 voss 2011a b in contrast argued that simplified parameterization herein referred to as reduced parameterisation approaches be adopted these approaches involve the consideration of a small number of parameters 10 e g through enforcing hydraulic property homogeneity such an approach is advocated on a philosophical basis that focuses on the need to capture only the most important overall system processes and behaviour the metric for which is based on how well the model reproduces the primary signal components in the observation data the aim underpinning this approach is that the modeling process will allow for new system understanding to be gained by the practitioner e g by identifying the most influential parameter combinations voss 2011b this approach also promulgates over determined or well posed inverse problems many examples of this form of parameterization exist in the literature e g li et al 2009 michael and voss 2009 sun et al 2011 ala aho et al 2017 a number of recent studies that adopt paired complex simple model analyses have shown the propensity for simple models to display bias in important simulated outputs which may be amplified or even generated by the history matching process these simple models also generally underestimate uncertainty in important simulated outputs e g doherty and christensen 2011 watson et al 2013 white et al 2014 given the simplified nature of all environmental models this insight can be extended to even the most complex physics based numerical models as these are still gross simplifications of the natural systems they simulate the concept of a simple model can be thought of in this study as a model that possesses a reduced parameterization scheme relative to a complex model counterpart with a more detailed parameterization the concept of a complex simple model pair is adopted herein while reduced parameterization schemes may theoretically exhibit a compromised ability to express uncertainty in decision relevant simulated outputs the extent to which various commonly applied parameterization schemes impact the stochastic expression of decision relevant simulated outputs and decisions made on the basis of these outputs is presently unknown this gives rise to some key questions what level of parameterization is needed to adequately express uncertainty for a given decision relevant simulated output to what extent can parameterization be reduced without degrading the ability of the model to serve as a decision support tool the primary objective of this study is to address these questions in the context of two common water management decision making contexts this study contributes to the on going debate of model simplicity complexity by using empirical paired complex simple model analyses to characterize the ramifications of using reduced parameterization schemes in terms of the bias and variance uncertainty in decision relevant simulated outputs a bayesian decision analysis approach is adopted this approach is applied to investigate the consequences of commonly employed parameterization reduction strategies not only in terms of the prior and posterior probability density functions of decision relevant simulated outputs but also in terms of the management decisions that would be made on their basis two integrated surface water groundwater modeling case studies are considered first a complex synthetic model used to forecast ecologically sensitive streamflow characteristics and their response to groundwater abstraction and second a model of the hauraki plains new zealand used to forecast nitrate loading impacts on water quality to our knowledge this is the first empirically based study aimed at understanding model parameterization reduction outcomes in the decision making context the intention of this study is not to identify a best parameterization scheme for a given decision relevant simulated output or for multiple decision relevant simulated outputs but rather to reveal the factors governing the appropriateness or otherwise of commonly employed parameterization schemes for various decision relevant simulated outputs and provide guidance on that basis 2 methodology 2 1 overview the impact of parameterization reduction on resource management decision making is explored herein using two integrated surface water groundwater models the first concerns outcomes of parameterization reduction on simulating ecologically important streamflow characteristics and their changes in response to groundwater abstraction also commonly referred to as stream depletion e g barlow and leake 2012 using a complex synthetic model the second concerns outcomes of parameterization reduction for simulating nutrient loading change impacts on water quality using a real world regional scale model hauraki plains new zealand both case studies are approached from a bayesian perspective involving the comparison between prior and posterior probability density functions pdfs of decision relevant simulated outputs made by the models with varying levels of parameterization to understand the ramifications of adopting reduced parameterization schemes representing a fundamental form of model simplification the analyses are undertaken with respect to a model with the most detailed parameterization paired complex simple model analyses are adopted to characterize bias and variance in decision relevant simulated outputs related to parameterization reduction this is achieved by treating the model with the most detailed parameterization as unbiased and as the best available means to represent simulated output uncertainty compared to models with reduced parameterization many studies have adopted this approach to numerical experimentation e g doherty and christensen 2011 oliver and alfonzo 2018 white et al 2014 three parameterization schemes are investigated for each of the case studies as follows coarse a reduced parameterization scheme where aquifer properties e g hydraulic conductivity porosity are considered in a spatially uniform sense only this approach ensures that the number of adjustable parameters is such that the inverse problem is over determined voss 2011b suggested the use of two to four adjustable parameters as a starting point with perhaps only one or two more if necessary in such an approach intermediate a spatially and temporally distributed parameterization scheme where aquifer property and boundary condition heterogeneity are considered while maintaining a number of adjustable parameters that is low enough e g approximately 2 500 such that history matching and uncertainty analysis methodologies that rely on the population of a high fidelity jacobian matrix and the singular value decomposition thereof can be employed this scheme is designed to follow the guidance in doherty et al 2010a doherty and hunt 2010 doherty et al 2010b including the use of pilot points as a spatial parameterization device fine a highly detailed spatially and temporally distributed aquifer property and and boundary condition parameterization scheme this approach results in a very large number of adjustable parameters e g 100 000 including model discretization scale parameters and time step scale parameters this scheme relies on recently developed ensemble based history matching and uncertainty analysis methodologies that alleviate the computational burden associated with computation of jacobian matrices e g chen and oliver 2013 white 2018 numerical experiments involving these parameterization schemes for both case studies employ an iterative ensemble smoother ies methodology pestpp ies white 2018 as a basis for history matching and posterior uncertainty quantification use of the ies method is deemed necessary given that exploration of the fine parameterization scheme was only possible from a computational standpoint using this approach moreover use of a consistent methodology among the parameterization experiments ensures that the causal factors of performance variability between experiments can be attributed to parameterization only because the ies method relaxes the relation between number of parameters and number of forward model evaluations required a similar number of model evaluations are used for all parameterization experiments an ensemble size of 100 is employed for all experiments this is deemed sufficient to avoid under utilization of observation data i e under fitting on the basis of a solution space dimensionality exploration moore and doherty 2005 performed on the intermediate parameterization scheme of the hauraki plains case study see supplementary information the python scripting packages flopy bakker et al 2016 and pyemu white et al 2016 are used to construct the numerical models and the pest doherty 2015 interface for these models respectively 2 2 bayesian and decision analysis a bayesian approach is adopted whereby both prior and posterior pdfs of the decision relevant simulated outputs are assessed each of which is referred to herein as an output pdf individual prior and posterior output pdfs for each parameterization scheme are formed by propagating prior and posterior parameter ensembles through the models respectively on the basis of these output pdfs we investigate the impact of parameterization choices on the stochastic nature of decision relevant simulated outputs any differences between output pdfs can be attributed directly to changes in parameterization in order to understand how decision makers may choose to implement a management action in a probabilistic context and how this is influenced by model parameterization we place the simulated outputs in a simple i e normative bayesian decision analysis context parmigiani and inoue 2009 the concepts underpinning our decision analysis approach are presented in the appendix the approach involves defining a decision threshold that represents the critical value at which decision makers would choose to implement a particular resource management action a decision threshold is defined for each decision relevant simulated output pdf this approach facilitates direct evaluation of the role that parameterization reduction plays in risk based decision making furthermore since the model with the finest parameterization is treated as the best available and unbiased estimator of simulated outputs the concepts of type i i e false positive and type ii i e false negative statistical errors are considered in combination with decision thresholds to evaluate decisions in terms of their correctness or otherwise made on the basis of models with reduced parameterization schemes any discrepancy between the output pdfs based on different parameterization schemes with respect to the decision threshold may yield inappropriate decision making because a reduced parameterization model either overstates type i error or understates type ii error the simulated effectiveness of a potential resource management action compared to the corresponding output pdf of the fine parameterization model 3 synthetic model case study the consequence of parameterization reduction on ecologically important streamflow characteristics and their changes in response to groundwater abstraction is investigated using a synthetic surface water unsaturated zone groundwater model the model represents a 1 476 km2 hypothetical hillslope catchment displaying significant surface water groundwater interaction fig 1 modflow nwt niswonger et al 2011 is used to simulate steady state and transient groundwater surface water and unsaturated flow conditions within the catchment the model comprises 3 layers 200 rows and 144 columns with a uniform horizontal grid spacing of 250 m following simulation of steady state conditions as a means of producing dynamic equilibrium initial conditions a transient model simulation is undertaken using daily stress periods spanning a three year period salient aspects of the synthetic model to the simulation of ecologically important streamflow characteristics such as low flow reliability and their changes in response to groundwater abstraction are as follows rainfall partitioning into evapotranspiration surface runoff and groundwater recharge is simulated using the unsaturated zone flow uzf package which employs a kinematic wave approximation to the richards equation niswonger et al 2006 surface runoff is routed directly to neighboring stream segments streamflow and stream aquifer interaction is simulated using the streamflow routing sfr package niswonger and prudic 2005 streamflow generation occurs entirely within the catchment i e no stream inflow is specified at upper stream reaches 3 1 decision relevant simulated outputs decision relevant simulated outputs for the synthetic model case study relate to ecologically important streamflow characteristics in particular stream low flow reliability and its response to groundwater abstraction ecologically important streamflow is characterized here by considering low flow conditions and specifically q95 the streamflow rate that is exceeded 95 of the time the following low flow streamflow characteristics are treated as decision relevant simulated outputs 1 nconsecday q95 base the maximum number of consecutive days where the streamflow rate is below q95 in the absence of groundwater abstraction 2 nconsecday q95 abstr the maximum number of consecutive days where the streamflow rate is below q95 in the presence of groundwater abstraction 3 δnconsecday q95 the change in the maximum number of consecutive days where the streamflow rate is below q95 in response to groundwater abstraction these simulated outputs are collected at two locations of interest red triangles fig 1 during the third year of simulation groundwater abstraction induced changes in streamflow characteristics for the simulated output δnconsecday q95 is calculated by differencing outputs between two model scenario runs as follows the base model which does not simulate abstraction the abstraction model which simulates three water supply bores black triangles fig 1 operating during summer months using the specified flux wel package niswonger et al 2011 3 2 the truth model and history matching a reference model referred to herein as the truth model is used in the synthetic model analysis for two purposes first to serve as a basis for generating the observation dataset against which models with different levels of parameterization are history matched second to provide a robust basis for evaluating the performance of different parameterization strategies with respect to decision relevant simulated outputs the truth model parameter set is selected from an ensemble of 1000 stochastic prior parameter realizations of the complex fine parameterization model the truth realization produced between the 60th and 95th percentile for a range of decision relevant simulated outputs outputs collected from the truth model comprising the observation dataset spanning the first two years of the simulation used for history matching include daily streamflow rates at five locations green triangles fig 1 monthly groundwater levels at nine locations within the upper aquifer layer black crosses fig 1 independent gaussian noise with zero mean and a standard deviation of 0 1 m for groundwater levels and 10 of the truth model simulated values for streamflow rates is added to the truth model derived outputs serving as history matching observations 3 3 numerical experiments details of the three model parameterization schemes investigated for the synthetic model case study are summarized in table 1 it is worth noting that while spatial parameterization is commonplace doherty and hunt 2010 temporal parameterization is rarely employed exceptions include knowling and werner 2017 masterson et al 2016 the uncertainty associated with time varying parameters such as groundwater recharge rates and their contribution to decision relevant simulated output uncertainty is often neglected given the transient nature of the decision relevant simulated outputs considered in the synthetic model case study temporal parameterization was deemed appropriate geostatistical correlation is specified for spatially distributed parameter types using an exponential variogram with a range of 2 500 m and a sill proportional to the expected prior variance see supplementary information for a summary of parameter information prior variance terms are assumed equivalent between parameterization schemes to reflect current modeling practice correlation between temporal parameters is also specified using an exponential variogram with a range of nine stress periods i e nine days non spatially and temporally distributed parameters are assumed to be uncorrelated 3 4 results the ability of the synthetic model to reproduce historical observations decreases as the parameterization detail is reduced ensemble simulated versus observed plots are presented in the supplementary information this finding is not surprising given that more detailed parameterization schemes allow for enhanced flexibility in assimilating the information contained within field observations via parameter conditioning in general considerable differences are apparent in the pdfs of decision relevant simulated outputs across the three parameterization schemes figs 2 and 3 the primary features apparent in figs 2 and 3 are expanded on below fine parameterization scheme the absolute output pdfs for the fine parameterization scheme figs 2 and 3 a and b display first and second moment changes between prior and posterior stances the prior absolute output pdfs display multi modal behavior in particular for forecast location s 2 the corresponding posterior pdfs however almost entirely collapse on one of these modes for the difference output pdfs the fine parameterization scheme figs 2 and 3 c also yield smaller second moments for a posterior stance these results indicate that the history matching process conditioned parameters or parameter combinations in the fine scheme that influence both the absolute and difference simulated outputs of interest the similar informedness of absolute and difference simulated outputs suggests that the potentially damaging information signal present in the observation data is not cancelled out through differencing prior absolute pdfs for reduced parameterization schemes relative to fine scheme the prior pdfs of the absolute outputs for the intermediate scheme figs 2 and 3 d and e exhibit similar second moments compared to those for the fine scheme small second moments for prior absolute pdfs are evident however for the coarse scheme figs 2 and 3 g and h indicating uncertainty underestimation such underestimation of uncertainty suggests that the absolute simulated outputs depend on parameterization detail that is present in the fine scheme but not in a reduced parameterization scheme prior difference pdfs for reduced parameterization schemes relative to fine scheme the prior pdfs of the difference outputs for the intermediate scheme like those of the absolute outputs also display similar first and second moments to the fine scheme figs 2 and 3 f similar first and second moments for the prior difference pdfs are also evident for the coarse scheme figs 2 and 3 i in contrast to the prior absolute pdfs these findings highlight the protection against uncertainty underestimation afforded when adopting a prior uncertainty stance as opposed to a posterior stance discussed below with an appropriately formulated modeling objective e g consideration of difference decision relevant simulated outputs prior versus posterior pdfs for reduced parameterization schemes the posterior pdfs of absolute and difference outputs for both the intermediate and coarse schemes figs 2 and 3 d i like the fine scheme display first and second moment changes relative to their corresponding prior pdfs the extent of these changes is shown to be related to the degree of parameterization this suggests that both the absolute and difference simulated outputs are overly sensitive to the upscaled aggregated parameters present in the reduced parameterization schemes parameters that are informed by observations used for history matching the extent to which changes in the output pdfs through history matching increases with more significant parameterization reduction suggests the presence of inappropriate simplification and subsequent null space entrainment the adjustment of simple model parameters through history matching that are un informed in the complex model and should therefore remain at expert knowledge based values e g doherty and christensen 2011 white et al 2014 posterior pdfs for reduced parameterization schemes relative to fine scheme for forecast location s 1 the posterior pdfs of absolute and difference outputs for both the intermediate and coarse schemes display similar first moments relative to those of the fine scheme i e indicating no significant bias fig 2 d i while the posterior pdfs for the intermediate scheme also display relatively similar second moments to the fine scheme the posterior pdfs for the coarse scheme exhibit degenerate i e numerically zero variance second moments this has significant implications for effective decision support underestimating uncertainty and therefore risk often leads to model failure e g doherty and simmons 2013 for forecast location s 2 however which is less spatially integrating than location s 1 the posterior pdfs for both the intermediate and coarse schemes generally display considerable history matching induced bias fig 3 d i moreover the posterior pdfs for the intermediate scheme is diffuse compared to the fine scheme whereas the posterior pdfs for the coarse scheme are comprised almost entirely of one or two simulated output values the computational times associated with models employing the three parameterization schemes are generally equivalent see supplementary information this indicates that spatial and temporal heterogeneity did not generally increase run times increased parameterization did however cause model non convergence e g of the 100 prior realizations for the coarse intermediate and fine parameterization schemes 99 90 and 84 realizations converged respectively 4 hauraki plains case study the influence of parameterization reduction on the simulated fate and transport of nitrate in a linked hydrologic model of the hauraki plains new zealand fig 4 is also investigated the model simulates groundwater and surface water flow with modflow nwt niswonger et al 2011 and simulates advective and dispersive nitrate transport in surface water and groundwater with mt3d usgs bedekar et al 2016 the general structure of the model is described in white 2018 briefly 7 layers with spatially distributed thickness 124 rows and 70 columns of a uniform 1 km grid steady state flow conditions historic and forecast periods constant nitrate loading conditions transient tritium loading conditions separate history matching and forecast time periods are simulated the historic time period encompasses 1955 2018 with annual time steps to capture the transient nature of tritium loading and the temporal evolution of the simulated nitrate distribution the initial concentrations of nitrate and tritium are assumed to be representative of the conditions before intense dairy practice and before atmospheric nuclear bomb testing respectively the forecast time period spans a period of 10 years from 2018 with annual time steps the initial nitrate concentrations for this forecast period are the final simulated concentrations from the historic period two scenarios are constructed around nitrate loading in the forecast period using the base historic period history matched nitrate loading rate using a 20 reduction of the base rate the streamflow transport sft package of mt3d usgs is used to simulate the transfer of nitrate from the groundwater system to the surface water system and the subsequent transport of nitrate in the surface water system to the firth of thames 4 1 decision relevant simulated outputs the simulated outputs of interest for resource management taken from scenario model outputs include the cumulative nitrate mass discharged to the firth of thames over the 10 year forecast period kg an in stream nitrate concentration mg l piako fig 4 a groundwater nitrate concentration mg l blue spring fig 4 also considered are difference or change outputs of interest by subtracting the 20 reduction scenario pdf from the corresponding base case pdf 4 1 1 history matching history matching is undertaken using several types of long term average groundwater flow system observations and transient nitrate and tritium transport observations including groundwater levels surface water flows groundwater and surface water nitrate concentrations groundwater tritium concentrations a total of 571 observations are used to condition model parameters through history matching the location of these observations within the model domain is included in the supplementary information additionally inequality constraints are specified to penalize simulated groundwater levels significantly above land surface in the uppermost model layer see white 2018 for more details 4 2 numerical experiments similar to the synthetic case study we employ three parameterization schemes to evaluate how parameterization choices influence the stochastic expression of important simulated outcomes table 2 summarizes these parameterization schemes for spatially distributed parameter types geostatistical correlation is specified using an exponential variogram with a sill proportional to the expected prior variance and range of 4 000 m see supplementary information for a summary of parameter information prior variance is equivalent between parameterization schemes to reflect current modeling practice remaining parameters are assumed to be uncorrelated in the prior uncertainty stance additionally distance based localization e g chen and oliver 2017 is used to eliminate spurious long distance cross correlations within the ies framework for the fine parameterization scheme this is accomplished using exponential variograms with sill of 1 0 and specified ranges for each unique observation data type used for history matching as follows surface water flow 40 000 m groundwater levels 20 000 m groundwater nitrate concentration 20 000 m surface water nitrate concentration 40 000 m groundwater tritium concentration 60 000 m for each individual observation the distance to each spatially distributed discretization scale parameter is used with the corresponding variogram to calculate the geostatistical strictly positive correlation between the observation location and the parameter location this value is then used to dampen parameter adjustments within the ies by multiplying the observation ensemble by the requisite localization matrix in a hadamard product sense see chen and oliver 2017 for more information on covariance inflation localization schemes 4 3 results the degree to which historical observations are reproduced generally decreases as parameterization detail is reduced as expected ensemble simulated versus observed plots are given in the supplementary information this trend is particularly pronounced for observation data types that depend on fine scale heterogeneity e g spatially distributed nitrate and tritium concentrations in general considerable differences in the pdfs of decision relevant simulated outputs across the three parameterization schemes are evident figs 5 7 a brief summary of patterns revealed in figs 5 7 is provided below for all three outputs of interest the fine parameterization scheme yields a similar second moment and only slight changes in the first moment between the prior and posterior uncertainty stances this indicates that history matching has only slightly conditioned the parameters or parameter combinations present in the fine scheme that influence the outputs of interest while the intermediate scheme yields more diffuse prior pdfs compared to the fine scheme the posterior pdfs display similar second moments to the fine scheme the larger second moment reduction for the intermediate scheme relative to the fine scheme suggests an inappropriately high level of conditioning of the parameters present in the intermediate scheme effectively overstating the value of the information in the observations the prior pdf second moments for the coarse scheme are significantly smaller than those of the fine scheme for all decision relevant simulated outputs except for the differenced firth of thames cumulative nitrate load output fig 5 i this indicates that the appropriateness of a coarse parameterization scheme from a prior stance is highly dependent on the decision relevant simulated output in question such an approach may be appropriate for processed e g differenced transformed and spatially integrated water quality related simulated outputs all of the coarse posterior pdfs however are nearly degenerate this indicates that the coarse parameterization scheme does not have the flexibility complexity to simultaneously express uncertainty notwithstanding producing similar first moments compared to the intermediate scheme for important simulated outputs while adopting a posterior stance this precludes effective decision support for the simulated cumulative nitrate load to the firth of thames fig 5 the posterior pdfs for the intermediate parameterization scheme while not reliably capturing the absolute nitrate load due to history matching induced bias fig 5 a and b vs d and e appropriately captures the percent change in nitrate load fig 5 c vs f this indicates that while a reduced parameterization scheme with some ability to express spatial heterogeneity and boundary condition uncertainty may not be suitable for simulating absolute outcomes it may be suitable for simulating changes however for all simulated outputs of interest at the piako and blue springs sites figs 6 and 7 the posterior pdfs for the intermediate scheme display bias this highlights that the intermediate scheme possess a greater ability to reliably simulate spatially integrated rather than local quantities this is due to the sensitivity of local transport simulated outputs to fine scale heterogeneity the coarse parameterization scheme offers considerable benefits related to model run times the mean run time is less than half that for the fine scheme see supplementary information this can be attributed to the lack of heterogeneity and the resulting well conditioned system of equations solved by the forward model 5 decision analysis while the prior and posterior output pdfs of models with both fine and reduced parameterization schemes generally display substantially different behaviors across the various decision relevant simulated outputs for both case studies the implications of these discrepancies in terms of resource management decision making requires further investigation to this end we employ decision theory concepts introduced above and presented in detail in the appendix to quantitatively assess the degree of similarity or otherwise in the output pdfs in terms of summary metrics which represent how the probabilistic outcomes from each of the parameterization experiments might compare in a risk based management decision context the decision thresholds representing a simulated outcome that would lead to the implementation of some given management action are taken as the red dashed lines shown on the difference output pdfs for both case studies 5 1 synthetic model a decision analysis concerning the changes in ecologically sensitive streamflow characteristics in response to abstraction is first explored using the synthetic model the truth values for the difference pdfs red dashed lines figs 2 and 3 are treated as decision thresholds i e consider that a maximum of the truth value of δnconsecday q95 can be tolerated from an ecological sustainability perspective all of the parameterization schemes indicate non zero probability that δnconsecday q95 decision thresholds will be exceeded from a prior stance figs 2 and 3 however when adopting a posterior stance threshold exceedance probabilities vary significantly among the different parameterization schemes for location of interest s 1 both the fine and intermediate posterior pdfs indicate non zero exceedance probability the degenerate posterior pdf for the coarse scheme however conveys absolute certainty that δnconsecday q95 will be 1 notwithstanding that is equal to the truth value and that the probability of threshold exceedance is zero this may yield a statistical type ii i e false negative error in decision making contexts similarly for location s 2 the coarse scheme shows zero posterior threshold exceedance probability whereas the posterior difference pdf for the fine and intermediate schemes yield non zero exceedance probability significantly non zero probability for the intermediate scheme giving rise to the albeit low probability potential for erroneous decisions relative to the fine scheme table 3 summarizes the decision difficulty d and decision outcome similarity s for the difference output pdfs analyzed d values exhibit a reduction following history matching where only minor first moment changes toward the decision threshold are evident this serves as additional evidence for the presence of decision relevant information in the observation dataset while prior d values are approximately equivalent across the parameterization schemes posterior d values are zero or near zero i e an easy decision for the coarse scheme for forecast location s 1 and for the fine scheme for forecast location s 2 a d value of zero means the entire distribution is entirely on one side of the decision threshold all s values when adopting a prior uncertainty stance are equal to 1 0 for both the intermediate and coarse schemes this suggests that equivalent management action would be undertaken on the basis of the prior pdfs for the intermediate and coarse parameterization models compared to those for the fine parameterization model the posterior pdfs for the intermediate and coarse schemes however produce considerably lower s values this contextualizes the potential shortcomings regarding history matching models with reduced parameterization schemes e g uncertainty underestimation compromising effective risk based decision support through consideration of a management decision threshold interestingly though despite the ill effects of history matching reduced parameterization models revealed above the same decision outcomes would have generally been reached i e 93 of the time for the intermediate scheme and 68 of the time for the coarse scheme highlighting the fundamental role of the decision threshold definition 5 2 hauraki plains a decision analysis concerning the hauraki plains 20 nitrate loading change scenario is now explored consider that a minimum of 10 reduction in nitrate load to the firth of thames fig 5 c f and i red dashed line is required to justify a 20 nitrate loading reduction action in this situation both the fine and intermediate schemes indicate a non zero probability of achieving the required reduction from both a prior and posterior stance however the coarse parameterization degenerate posterior incorrectly conveys absolute certainty that the nitrate load reduction will be 9 which may yield a type ii i e false negative error a similar hypothetical construction can be made for the outputs of interest at piako and blue springs sites using a reduction threshold target of 0 5 m g l nitrate for the piako site fig 6 the fine parameterization posterior indicates that the change in nitrate concentration will be greater than 0 5 m g l c while both the intermediate and coarse scheme pdfs are biased towards lower concentrations f i yielding type ii i e false negative errors which would ultimately lead to an incorrect decision to not implement the 20 reduction action however if the same decision construct is applied to the blue spring site both the intermediate and coarse schemes would lead to the same decision as the fine parameterization the decision outcome comparison between the fine and reduced parameterization model pdfs is summarized in table 4 generally the s values indicates that the prior pdfs for the intermediate and coarse parameterization models yield similar management action as the fine parameterization model furthermore posterior pdfs for reduced parameterization models yield improved s values for both firth and blue springs site but decreased s values for the piako site 6 discussion both case study results presented herein demonstrate that use of reduced parameterization schemes may suffer from several ill effects such as corrupted first and second moments of decision relevant simulated output pdfs given that the two case studies represent common yet widely differing decision support modeling applications the relationships revealed between parameterization reduction and the bias and uncertainty underestimation accompanying the different types of decision relevant simulated outputs are expected to be broadly generalizable i e transferable to other modeling studies much of the following discussion serves to support this our analysis shows that for a model with a coarse parameterization scheme e g involving spatially uniform aquifer property parameters to serve an appropriate basis for decision making support satisfaction of the following conditions can mitigate these ill effects each of which is discussed in more detail below a prior uncertainty stance is adopted i e no parameter conditioning through history matching is undertaken the decision relevant simulated output concerns quantities that do not depend on fine scale spatial heterogeneity herein including ecologically important streamflow characteristics and basin wide integrated nitrate loading changes the decision relevant simulated output concerns differences or changes in quantities herein including changes in nitrate loading scenarios the first condition reflects that any conditioning of highly spatially aggregated parameters is shown to cause degenerate posterior pdfs for most decision relevant simulated outputs this is due to the fact that use of such a reduced parameterization scheme forms a well posed inverse problem through implicit a priori regularization e g moore and doherty 2006 in which the parameter ensemble collapses into an artificially distinct region of maximum likelihood the degenerate nature of the posterior output pdfs preclude the coarse model s utility for risk based i e probabilistic decision making support posterior output pdfs for the coarse parameterization scheme not only display a severe underestimation of uncertainty but also display bias in most cases these issues are a result of inappropriate parameter compensation through history matching e g clark and vrugt 2006 white et al 2014 the unknowable magnitude of bias and underestimation of uncertainty ultimately leads to a unknowable risk of incorrect resource management action notwithstanding this a number of correct management decisions for a range of different simulated outputs are possible using the coarse scheme following history matching this is despite the relatively poor fits obtained with the coarse parameterization scheme underscoring that the level of fit to measurements of past system states is not directly related to model decision support utility the second and third conditions collectively reflect that the appropriateness of a coarse parameterization scheme is shown to be highly dependent on the decision relevant simulated output in question specifically the second condition reflects that the coarse scheme is shown to be inappropriate when decision relevant simulated outputs depend on relatively local scale property heterogeneity heterogeneity that occurs at a smaller spatial scale than is represented by the parameterization scheme this condition is particularly important for water quality related simulated outputs which are known to be more sensitive to fine scale heterogeneity e g riva et al 2008 yoon and mckenna 2012 zheng and gorelick 2003 for simulated outputs that are sensitive to local scale transport properties and processes e g surface water nitrate concentration at the piako site in the hauraki plains example the prior pdf second moment is significantly underestimated due to neglecting heterogeneity in aquifer conductivity and porosity this may invalidate the use of coarse parameterization schemes for decision support that is concerned with spatially discrete outputs of interest such as concentrations in contrast the coarse scheme is shown to be appropriate when adopting a prior stance for simulated outputs that integrate transport properties and processes over large or entire catchment areas e g cumulative nitrate load to the firth of thames in the hauraki plains furthermore the coarse scheme is shown to be better suited to spatially integrated decision relevant simulated outputs concerning water quantity related questions through avoidance of significant posterior bias the third condition reflects that the coarse scheme is shown to be inappropriate for simulating decision relevant outputs in absolute terms for the nitrate transport related questions considered even for spatially integrated simulated outputs and when adopting a prior uncertainty stance only after simulated output differencing is undertaken are the ill effects of employing a coarse parameterization scheme alleviated this highlights the potential significant benefit in appropriately formulating the decision relevant simulated outputs that underpin decision support modeling investigations in terms of e g differences or transposes as supported by sepulveda and doherty 2015 and cui et al 2018 casting the water quantity related simulated outputs in terms of differences is not as effective in mitigating these ill effects due to the information signal present in the observation data the misdirection of which to upscaled aggregated parameters can be damaging is not cancelled out through differencing the above conditions underpinning an appropriate basis for decision making support for the coarse parameterization scheme also hold in general terms for the intermediate parameterization scheme that is even a highly parameterized scheme e g doherty and hunt 2010 hunt et al 2007 involving 2500 spatially and temporally variable parameters serves best as a robust decision support tool either from a prior standpoint or from a posterior standpoint except where simulated outputs are absolute or are sensitive to local scale property heterogeneity otherwise the potential for bias in simulated outputs and therefore the potential for wrong decisions is marked e g decision making for the change in surface water concentration at the piako site may incur a false negative i e type ii statistical error notwithstanding this the intermediate scheme generally displays superior performance compared to the coarse scheme this is evidenced by the fact that the intermediate scheme displays apparent immunity to bias for spatially integrated water quantity related simulated outputs and for differenced and spatially integrated water quality related simulated outputs this lack of bias raises the potential for a second moment adjustment term to be applied to overcome any potential uncertainty underestimation towards rectifying decision support reliability discussed below the intermediate scheme also generally yields non degenerate posterior output pdfs underestimation of uncertainty associated with decision relevant simulated outputs is not as significant for the intermediate scheme compared to the coarse scheme due to the flexibility afforded by enhanced expression of prior parameter uncertainty moreover the intermediate scheme pdfs generally share similar second moments to the fine scheme these factors collectively contribute to the superior decision analysis statistics evident for the intermediate scheme compared the coarse scheme the general similarity in the second moments of the posterior output pdfs for the intermediate and fine parameterization schemes is of particular interest this suggests that use of parameterization schemes such as pilot point based schemes form a robust basis i e do not significantly underestimate simulated output variance for first order second moment fosm uncertainty estimation techniques that are commonly used for e g data worth and optimal monitoring design analyses e g fienen et al 2010 wohling et al 2016 the apparent safety afforded by the three conditions described above when using a reduced parameterization schemes in the decision support context is an encouraging finding this is because of the fact that these conditions are commonly satisfied though not commonly employed in real world decision support modeling analyses the first condition is met when decision relevant information within the observation data are lacking or when parameter conditioning is considered unnecessary from either a study objective standpoint e g where more conservative uncertainty estimates surrounding a decision relevant simulated output are satisfactory or a time money resource constraint standpoint it is important to note however that the employment of complex physics based models from only a prior uncertainty stance is in stark contrast to current environmental modeling practice this is a reflection of the fact that observation data are expensive and difficult to collect in combination with the widespread belief that an environmental model contains appropriately detailed receptacles i e parameters for expressing information contained within observation data while field observation data are central to the development of a sound system understanding and conceptual model the results presented herein indicate that use of observation data for history matching regional scale numerical models should be approached with caution i e in such a way that the potentially damaging side effects of parameter conditioning are considered e g doherty and christensen 2011 oliver and alfonzo 2018 e g by processing observation data as discussed in detail below the second and third conditions are met e g where water quantity issues involving simulation of the change in catchment scale water budget components in response to climate variability and or water use scenarios e g knowling et al 2015 post et al 2018 or water quality issues involving simulation of spatially and or temporally integrated mass flux changes e g the firth of thames decision relevant simulated output considered in the current study are the focus of a modeling investigation where decision relevant simulated outputs exhibit local scale aquifer property dependence e g small scale tracer tests this calls to question the suitability of a regional scale numerical model altogether compared to that of e g a local scale advective dispersive transport models or sub models e g li et al 2006 sreekanth and moore 2018 deployment of environmental models in not only a decision relevant simulated output specific manner but also in a manner where decision relevant simulated outputs concern relative or differenced model quantities the importance of which has been demonstrated by e g cui et al 2018 sepulveda and doherty 2015 is also in stark contrast to current environmental modeling practice notwithstanding the conditions discussed above the potential for significant bias in e g spatially localized water quantity and quality related simulated outputs for the intermediate schemes arising from history matching induced parameter compensation and null space entrainment defined earlier suggests the need for even further parameterization detail where parameters need to be conditioned to reduce the uncertainty surrounding decision relevant simulated outputs enhanced parameterization detail will afford further protection against bias by reducing the extent of parameter entrainment during history matching e g doherty and christensen 2011 watson et al 2013 additional means to mitigate against these ill effects are the observation noise or weight adjustment techniques these effectively result in a lesser fit to observation data being sought through history matching e g christensen 2017 cooley and christensen 2006 doherty 2015 oliver and alfonzo 2018 nevertheless in order to minimize the potential for bias it is encouraged that the parameterization scheme adopted be as fine as possible e g the fine parameterization schemes adopted herein whereby expression of model input uncertainty is commensurate with the model spatial and temporal discretization handling of such high parameter dimensions however requires application of efficient ensemble based history matching and uncertainty quantification algorithms e g chen and oliver 2013 white 2018 and scripting tools or other specialized software e g bakker et al 2016 white et al 2016 a potential alternative means towards addressing these ill effects includes the use of decision targeted parameterization schemes e g parameterization densification around the location of the decision relevant simulated output and its recharge area such an approach is in accordance with modeling objective focused model construction parameterization and history matching it is difficult to discern a general widely applicable pattern whereby a model with a reduced parameterization scheme can be deployed reliably for decision making when it is necessary to assimilate information from observations in this case discretization scale parameterization affords the most robust decision support in the absence of this level of parameterization improved and targeted objective function formulations may also provide protection against inappropriate parameter compensation and hence decision relevant output bias by removing the signal components that would otherwise inform parameters that were excluded a number of previous studies have demonstrated the usefulness of observation filtering i e omitting non decision relevant information containing observations and observation processing such as transforming and differencing e g knowling and werner 2017 white et al 2014 we suggest that more work be focused in this area nevertheless if history matching is deemed necessary because the available observation dataset contains decision relevant information or in order to reduce uncertainty in the parameters that influence the decision relevant simulated outputs then the state observations used for conditioning must not be sensitive to missing parameterization detail otherwise the potential for parameter compensation induced bias in decision relevant simulated outputs is large which may lead to incorrect resource management action these findings highlight that designing an appropriate and targeted model parameterization scheme requires not only a thorough understanding of the physics of the system being simulated but also knowledge of how information is transferred from observations to parameters and subsequently to simulated outputs in bayesian inverse problems our use of the ies method white 2018 for all parameterization schemes herein allows for comparisons to be drawn between these schemes on a consistent algorithmic platform as stated posterior exploration for the fine scheme would have been virtually impossible from a computational resource perspective using traditional inversion methods i e that use finite difference perturbation for gradient computation it is worth noting however that higher fidelity methods could have been applied for the intermediate and coarse schemes e g null space monte carlo tonkin and doherty 2009 for the intermediate scheme and markov chain monte carlo e g vrugt 2016 for the coarse scheme however the extent to which use of higher fidelity methods for the intermediate and coarse schemes may yield discrepancies compared to use of lower rank methods such as those based on ensembles e g chen and oliver 2012 chen and oliver 2013 is presently unknown and requires investigation this is notwithanding the approximate nature of any posterior other than those obtained via rejection sampling tarantola 2005 future work will extend on previous studies that explore posterior uncertainty estimation algorithm performance e g keating et al 2010 and explore its pairing with parameterization dimensionality which are linked in practice it is also worth noting that our use of ies between schemes means that the performance of any scheme relative to another is independent of the number of runs required given the highly variable performance of different schemes for different decision relevant simulated outputs and under different uncertainty stances this represents an important finding we also encourage future work towards addressing the uncertainty underestimation of reduced parameterization schemes and the bias introduced through history matching with these schemes a follow on paper will explore the trade off between bias and variance in decision relevant simulated outputs and the potential utility of structural error terms e g doherty and christensen 2011 doherty and welter 2010 white et al 2014 to alleviate the detrimental outcomes of simplification and history matching leading to wrong decisions for example use of structural error terms may be of benefit in overcoming potential uncertainty underestimation when using reduced parameterization schemes that do not incur bias in decision relevant simulated outputs 7 conclusions this study investigates the ramifications of parameterization reduction or simplification in the context of environmental management decision making two case studies are considered involving several types of decision relevant simulated outputs it is demonstrated that the parameterization process the subjective expression of model input uncertainty plays a fundamental role in the decision support context the extent to which a model can reliably support decision making is shown empirically to be highly dependent on not only the level of parameterization detail employed by a model but also the decision relevant simulated output of interest and whether or not parameters are subjected to conditioning through history matching we demonstrate the ill effects of inappropriate parameterization namely the potential for bias and underestimation of uncertainty accompanying decision relevant simulated outputs and the resulting potential for incorrect decision making and resource management action the case specific nature of these ill effects is also demonstrated these findings highlight the need for parameterization simplification strategies to be targeted with respect to decision relevant simulated outputs and data assimilation needs they also highlight that the parameterization aspect of the modeling process should be undertaken in a decision relevant manner which requires definition at early stages of a modeling project to support management this study reveals that the detrimental outcomes of reduced parameterization in the environmental model based decision support context can be mitigated via the following primary factors deployment of a model on the basis of prior parameter i e uncalibrated realizations adopting a prior uncertainty stance with reduced parameterization models is found to yield similar decision outcomes compared to the finest parameterization scheme considered in most cases this approach is in stark contrast to current environmental modeling practice a reflection of the fact that observation data are expensive and difficult to collect in combination with the widely accepted belief that an environmental model contains appropriately detailed receptacles i e parameters for expressing information contained within observation data however we demonstrate that parameter conditioning through history matching i e calibrating with a reduced parameterization scheme may incur significant biases in decision relevant simulated outputs these biases arise from parameter compensation e g clark and vrugt 2006 white et al 2014 as a result of using an overly simplified parameterization scheme appropriate formulation of decision relevant simulated outputs i e selecting an appropriate question for the modeling analysis to address we demonstrate that focusing on difference or change simulation outputs that do not depend on local scale heterogeneity provide defence against ill effects of reduced parameterization of regional scale models that are used for decision support this is shown to be particularly important for water quality related simulated outputs for example changes in basin wide integrated nitrate load simulated output pdfs produced by reduced parameterization models are shown to be significantly less prone to bias and uncertainty underestimation and therefore provide a more robust basis for decision making under uncertainty than spatially discrete stream and spring concentration change outputs ultimately modeling practitioners must balance choices and assumptions related to parameterization with the many other model construction and deployment choices that must be made when constructing a real world environmental model for decision support purposes we encourage practitioners to critically consider model simplification decisions related to parameterization including through use of the general guidance provided on the basis of the empirical findings presented herein where the impact of such parameterization simplification on decision relevant simulated outputs is unknown and where history matching is deemed necessary to reduce the uncertainty of simulated outputs of interest we suggest to err on the side of caution employing as finer parameterization scheme as possible given numerical stability and model run time constraints acknowledgments we would like to acknowledge brioch hemmings and zara rawlinson for help building the hauraki plains model and john hadfield bevan jenkins and sung soo koh at waikato regional council new zealand for providing several of the datasets for the hauraki plains model we wish to thank brioch hemmings mike fienen and john doherty for their helpful review comments we also acknowledge ty ferre and an anonymous reviewer whose comments helped improve the paper the data files scripts and software used in the synthetic model and hauraki plains model simplification analyses can be obtained from the corresponding author this research was performed as part of the smart models for aquifer management programme funded by the ministry of business innovation and employment new zealand as well as by a commercial contract with waikato regional council appendix decision analysis concepts for a given decision relevant simulated output pdf and associated decision threshold the following quantities are defined 1 a 1 n n 1 if ϕ n ϕ 0 0 otherwise and 2 b 1 n n 1 if ϕ n ϕ 0 0 otherwise where ϕn is an individual value of simulated output which taken collectively over all n ϕ values forms the output pdf conceptually a and b measure the probability of the decision relevant simulated output being less than or equal to or greater than the decision threshold respectively with these quantities we define the decision difficulty d chlumpsky 2017 as 3 d 1 a b 1 a b which measures how the samples in the output pdf are distributed about the decision threshold if equal numbers of ϕ values are on either side of the threshold d is equal to 1 indicating a difficult decision there is equal probability of the simulated output being higher or lower than the threshold whereas if all ϕ values are on one side of the threshold d is equal to 0 indicating an easy decision a metric can also be defined to compare two output pdfs against the same decision threshold which we term decision outcome similarity s as 4 s 1 a 1 a 2 2 b 1 b 2 2 2 where subscripts of a and b correspond to different output pdfs s ranges from 0 the output pdfs are exclusively on opposite sides of the decision threshold to 1 the output pdfs are exclusively on the same side of the decision threshold supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 04 010 appendix b supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
