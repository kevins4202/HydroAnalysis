index,text
26050,terrestrial and aquatic mercury biogeochemical processes are integrated into the soil and water assessment tool swat to simulate the watershed scale hg behaviors the swat hg was successfully calibrated and validated with the data obtained from the jang sung dam jsd watershed the largest hg storage in the watershed was found in soil 93 however soil erosion was not the largest hg source for the jsd reservoir due to the well established forests the model suggests the baseflow is the most dominant hg loading to the waterbody which accounts for 80 0 of the thg yields on the annual basis the model simulation was projected to the year 2040 and showed that the hg levels in various compartments of watershed have not reached to steady state hence the hg levels changed slowly over the decades even in status quo atmospheric hg deposition graphical abstract image 1 keywords mercury temperate mountainous watershed terrestrial transport swat bioaccumulation 1 introduction risks associated with mercury hg exposure via fish consumption has significantly increased over the past decades in global scale due to the combustion of fossil fuels and consequent release of hg into the atmosphere and waterbodies fitzgerald et al 2005 unep 2013 to reduce hg release to environments and to mitigate human and ecologic exposure risks of hg the minamata convention is being implemented to decrease the release of hg into the atmosphere in coal fired powerplants cement production industries and other sources various efforts are being taken to reduce atmospheric hg release worldwide however estimating the magnitude and timing of the risk reduction of hg exposure via fish consumption is still challenging due to the complex transport processes of hg in the terrestrial and aquatic environments munthe et al 2007 oswald et al 2014 in environments atmospherically released hg can be transported to pristine areas fitzgerald et al 1998 lindberg and stratton 1998 and ultimately be deposited on various land covers such as urban areas forests barren soils snow covers etc futter et al 2012 graydon et al 2006 also the atmospheric hg can be associated with plants and tree leaves which is then transported to soil by litterfall wang et al 2016a the litter can be decomposed slowly by biogeochemical reactions and hg can be released and adsorbed to soil particles pokharel 2011 the hg can be associated with plants and soils and can be transported to waters by terrestrial and hydrological processes agnan et al 2016 denkenberger et al 2014 gai et al 2016 yu et al 2014 which could be converted to methylmercury mehg and can enter the food chain base via bioconcentration and bioaccumulation fitzgerald et al 2007 lavoie et al 2013 morel et al 1998 the bioaccumulated hg can be biomagnified and predators can be exposed to high level of hg via fish consumption lavoie et al 2013 tsui et al 2009 the released atmospheric hg needs to travel through several environmental media in air soil water and biota to get to final receptors blackwell et al 2014 domagalski et al 2016 obrist et al 2016 hence linking atmospherically released hg to final receptors and quantifying risks associated with the accumulation of hg in biota is important but has been a challenge due to the uncertainties associated with the complex transport and transformation processes of hg there have been several empirical and experimental efforts that attempted to link atmospheric hg deposition to hg levels in fish and eventually to subsequent hg exposure to ultimate receptors considering watershed characteristics and terrestrial processes gerson and driscoll 2016 harris et al 2007 hurley et al 1995 oswald et al 2014 in addition simple watershed transport models and complex aquatic hg biogeochemical reactions were integrated and applied to several rivers and lakes with varying size of watersheds ambrose et al 2005 brown et al 2007 hong et al 2014 knightes 2008 knightes et al 2009 to relate the atmospheric hg deposition to hg in fish the integrated catchments model for mercury inca hg was developed and applied to two boreal forest areas with more sophisticated catchment processes and hg biogeochemical reactions futter et al 2012 however these models are generally limited to specific sites or regions thus the application of the model in other areas has been limited specifically one of the main deficiencies of the inca hg is the lack of dynamic plant growth simulation which makes it uncertain to estimate leaf fall litter production and ultimately the transport of leaf accumulated hg from canopy to the litter layer on the ground in addition the fate and transport of hg in terrestrial environments can be largely driven by hydrologic processes which are tightly coupled with land use types watershed terrain morphology and geological conditions bradley et al 2012 hurley et al 1998 which have been rarely considered in modeling hg fate and transport at the watershed scale in the present study the hg fate and transport in terrestrial and aquatic environments are integrated into the soil and water assessment tool swat https swat tamu edu last accessed on november 20 2018 swat is a gis based watershed scale model developed by the u s department of agriculture agricultural research service usda ars swat has been mainly used to predict the impact of land management practices on water sediment nutrients pesticides and heavy metal yields in large complex watersheds with varying soils land use and management conditions over long periods of time i e decades arnold et al 2012 chen et al 2017 cho et al 2012 du et al 2019 gao et al 2019 jiao et al 2014 meng et al 2018 zhang et al 2014 zhang 2018 in this study complex hg terrestrial and aquatic biogeochemical processes with distributed deposition of atmospheric hg are incorporated into swat to evaluate watershed scale hg transport processes especially this study focused on investigating the mass fluxes of hg in various environmental compartments i e atmospheric and litterfall hg deposition seasonable variation of watershed loadings to a waterbody and the importance of base flow on hg transport in a watershed scale 2 conceptual model of swat hg the swat creates a physical watershed model from a gis based digital elevation model dem land use information and soil types a stream network and subbasins are created from the delineation of the dem then the maps of land uses soils and land slopes are overlaid to the delineated subbasins to generate hydrologic response units hrus afterward daily weather information and land management practices are integrated to input data and physical processes associated with water movement soil erosion and sediment transport plant and tree growth and nutrient cycling are simulated arnold et al 2012 computational procedures for simulating terrestrial and aquatic hg processes are developed and written to the swat source code the transformation fate and transport of the mercury species are simulated within a watershed s hydrologic system which allows the calculation of hg s terrestrial and aquatic fate and transport three mercury species divalent mercury hg2 methylmercury ch3hg and elemental mercury hg0 are assumed to be the main state variables in atmosphere tree leaves leaf surfaces four soil layers groundwaters streams sediments and biota in either dissolved or particulate form other mercury compounds such as dimethylmercury are not included in the model due to their low levels in natural environments zhu et al 2018 compared to the three hg species the summation of the three species of mercury is the total mercury thg dry and wet atmospheric deposition processes are included in weather input data the atmospherically deposited hg is split between plant canopy and lands based on daily ground cover made by plants hg accumulates on leaf surfaces in forests in washable forms as a result of dry deposition when it rains trees can intercept a portion of rainfall and hg on the leaf surface if the total rainfall exceeds the canopy interception capacity raindrops falling on leaves may reach the ground surface either in stemflow or by falling through leaves the throughfall flow washes off the hg stored on tree leaves into the ground litter and to top soils leaves can also accumulate hg in the tissue from reactive gaseous hg species in the atmosphere and become litterfall during fall and winter seasons a fraction of the hg deposited to soils evaporates back into the atmosphere depending on daily solar radiation air temperature and the total hg concentrations in the soil once hg is deposited on the soil surface hg is partitioned between soil particles and porewater based on soil organic carbon fraction as surface litter is decomposed over a number of years the dissolved form of hg is mobile in saturated soils as soil water percolates to subsoil layers if excess rainfall results in overland flow to occur dissolved hg in the top soil layer may become mobile in the runoff water and transported laterally to receiving waterbodies such as streams or reservoirs in addition dissolved hg is subject to flowing laterally in the rootzone discharging to the main channel in the return flow from shallow aquifer or recharging deep aquifer particulate hg does not percolate to deep soils but can be mixed into sub soil layers as a result of soil mixing by earthworms under proper soil moisture and soil temperature conditions although the majority of particulate hg remains on the soil surface significant overland flow event with erosive force can transport particulate hg to downstream along with soil particles aquatic hg processes are modeled based on the assumption of treating reach segments or reservoir storage a well mixed tank reactor two sediment layers are defined underneath waterbodies the surficial sediment layer is biogeochemically active and interacts with the water column the deeper sediment layer represents historically deposited sediments and hg significant processes including sediment transport deposition resuspension hg methylation demethylation and sorption desorption occur both in the water column and surficial sediment the mehg in the water column is bioconcentrated at the foodweb base then biomagnified through trophic transfer which can be quantified by nitrogen stable isotope fractionation the conceptual hg model is graphically depicted in fig 1 3 swat hg model development 3 1 atmospheric hg deposition model the swat hg model considers the atmospheric deposition as a source of hg though specific point and unspecific non point hg sources can also be assigned in the model either a constant or variable rates of hg atmospheric deposition in the air at daily monthly or yearly time scale provide boundary conditions to the model for simulating hg transport processes every day the total amount of atmospheric hg deposition is split between the plant canopy and land depending on the ground cover made by plants the fraction of ground cover by plants fgrdcv is estimated as follows riaño et al 2004 1 f grdcv 1 e k lai where k is the light extinction coefficient which is available in the plant database of the swat model and lai is the leaf area index which varies by type of tree the relationship between gaseous elemental hg concentration c hg air ng m 3 in the air and hg contents in foliar tissues c hg foliar ng g 1 can be approximated to a linear function in the forest environment fay and gustin 2007 2 c hg foliar a c hg air b the slope a and the intercept b vary by different type of tree table s1 shows these parameters for different types of trees the hg in leaves will be a watershed loading when the litterfall starts from fall the atmospheric deposition of hg may occur in dry weather conditions on land surface i e direct deposition or on leaf surfaces in forests leaf fall in washable forms as follows 3 w hg dry direct w hg dry 1 f grdcv 4 w hg dry leaf w hg dry f grdcv where whg dry direct is the dry atmospheric hg deposition to soil surface ng m 2 d 1 whg dry leaf is the dry atmospheric hg deposition to leaf surface ng m 2 d 1 whg dry is total dry atmospheric hg deposition ng m 2 d 1 established plants may intercept a portion of rainfall on leaves if the total rainfall exceeds the canopy interception capacity of a tree raindrops falling on leaves may reach the ground surface either in stemflow or by falling through leaves the throughfall flow washes off hg on tree leaves into the ground and top soils the total wet deposition is estimated by 5 w hg wet total w hg throughfall f grdcv w hg wet direct 1 f grdcv where whg wet total is the wet atmospheric hg deposition ng m 2 d 1 and whg throughfall is the wet atmospheric hg deposition via throughfall ng m 2 d 1 and whg wet direct is the direct wet atmospheric hg deposition to soil surface ng m 2 d 1 in eq 4 w throughfall is the sum of hg in rainfall and the hg that is washed off from leaf surfaces it is assumed that the amount of hg wash off is proportional to the ratio of the effective rainfall total rainfall canopy interception and canopy interception if snow is present atmospheric hg deposition applies to snow cover if it exists and the hg storage in snow cover moves to top soil in proportion to snow melt amount as it occurs 3 2 fate and transport of hg in soils the dissolved form of hg can be mobile in saturated soils as soil water percolates to subsoil layers if excess rainfall results in overland flow occurring dissolved hg in the top soil layer may become mobile in the runoff water and transported laterally to the receiving waterbody in addition dissolved hg is subject to lateral flow in the rootzone return flow from shallow aquifer or deep aquifer recharge by following the soil water in dissolved form particulate hg does not percolate to deep soil layers gai et al 2016 but can be mixed in subsoil layers as biological mixing by earthworms becomes active with proper soil moisture and soil temperature significant overland flow with erosive force may detach particulate hg along from soils along with soil particles to deliver to downstream in soils hg in dissolved and particulate phase can partition to each other based on the soil organic carbon contents soc schuster 1991 yin et al 1997 equilibrium state is made between the particulate and dissolved from hg based on the linear adsorption partitioning coefficients as follows 6 c hg p soil k d hg soil c hg d soil 7 k d hg soil a hg f oc soil where kd hg soil is the linear adsorption coefficient of hg between solid and dissolved phase in soil l kg 1 foc is soc g g 1 and ahg is the linear relationship coefficient between kd hg soil and foc soil l kg 1 in soil mercury methylation demethylation and oxidation and reduction could occur qiu et al 2005 however the rates usually have a time scale of hours and the transformation between mercury species reaches quasi steady state within a day thus the swat model assumes these equilibrium conditions occur instantly while running at daily time step hence in soil the specific distribution of hg2 ch3hg and hg0 is assigned as follows 8 c hg p soil f hg soil c thg p where chg p soil is the particulate hg2 ch3hg and hg0 concentrations in soil μg g 1 fhg soil is the fraction of hg2 ch3hg and hg0 and cthg p is the total hg concentration in soil particulate μg g 1 3 3 characterizing tree growth and leaffall swat simulates plant growth based on the epic plant model g arnold et al 2012 plant growth is modeled by simulating leaf area development light interception and conversion of intercepted light into biomass assuming that a plant has a unique radiation use efficiency 9 w bio rue 0 5 h day 1 e k l lai where wbio is the potential increase in total plant biomass on a day kg ha 1 rue is radiation use efficiency 0 5hday is the incident total solar radiation mj m 2 kl is the light extinction coefficient and lai is the leaf area index rue is estimated using stockle et al 1992 10 rue 100 co 2 co 2 e r 1 r 2 co 2 where co2 is the concentration of carbon dioxide in the atmosphere ppmv and r1 and r2 are shape coefficients plant growth is simulated by calculating the leaf area development based on the daily heat unit index hui and the potential heat units phu and provides an s shape curve function shown in fig s2 even though the swat plant growth model estimates biomass in shoots and roots development it neglects calculating the amount of leaves on the aboveground biomass but only assumes that a fraction of the aboveground biomass falls to the ground during senescence or dormant periods as hg is known to accumulate on leaf surfaces and in leaf tissue it is important that leaf biomass is accurately estimated to reduce error in simulating hg deposition on or in tree leaves therefore the swat plant growth model was enhanced to estimate leaf biomass amounts by adopting the regression model proposed by baskerville 1972 11 log m l β log m t k where ml is the total leaf biomass of a tree tons mt is the total biomass of a tree tons and β and k are regression factors and shown in table s2 in supporting information three types of forest are defined in the swat plant database namely deciduous evergreen and mixed forest the growth characteristics of these forests are defined with 33 unique plant parameters the selected parameters were refined for the three types of forest in the jangsung dam watershed shown in table s3 autumn leaf fall in the jdw occurs between late september and mid november korea meteorological administration last accessed on march 7 2017 once lai reaches dlai the fraction of growing season when leaf area begins to decline senescence occurs for the remaining period of the growing season due to the biological aging of plant the amount of biomass lost in leaffall on a day is estimated as a function of the decrease in lai as leaffall occurs the hg deposited on leaf surface moves from leaf storage to litter storage or soil storage if no litter exists on the ground the non washable form hg existing in leaf tissue also moves from canopy to litter storage with senescence 3 4 hg evaporation from surficial soil the evaporation of hg from surficial soil to atmosphere can be significant in the overall hg balance in forestry watershed scale han et al 2016 schlüter 2000 research has suggested several important environmental factors that modulate hg fluxes from terrestrial surfaces which include solar radiation air and soil temperatures soil moisture etc agnan et al 2016 using the available data collected in the site the hg evaporation from soil flux hg soil ng m 2 hr 1 is modeled as follows lin et al 2010 12 flux hg soil c thg p soil β 0 β 1 t air β l 2 where β0 β1 β2 are fitting parameters that were fitted by field measured values cthg p soil is the total mercury soil concentrations μg kg 1 tair is the air temperature oc and l is the solar radiation mw cm 2 3 5 mercury process in water and sediment mercury undergoes complex biogeochemical reactions such as oxidation reduction methylation demethylation and photolytic degradation demethylation in water knightes 2008 the mercury methylation and demethylation are modeled using first order rate kinetics in the water column it is assumed that mercury methylation and demethylation occur in the dissolved phase 13 d c c h 3 h g d w c d t k m e t h y l w c c h g 2 d w c k d e m e t h l y 1 w c c c h 3 h g d w c 14 d c h g 2 d w c d t k m e t h y l w c c h g 2 d w c k d e m e t h l y 1 w c c c h 3 h g d w c where cch3hg d wc is the dissolved methylmercury concentrations in water column ng l 1 chg2 d wc is the dissolved inorganic mercury concentration in water column ng l 1 kmethyl wc is the mercury methylation rate constant in water column d 1 and kdemethyl wc is the mercury demethylation rate constant in water column d 1 similar hg processes could occur in sediments since the original swat doesn t consider the sediment as a separate model domain the sediment model is updated and described in si then mercury methylation and demthylation reactions are included in the model as follows 15 d c c h 3 h g p s e d d t k m e t h y l s e d c h g 2 p s e d k d e m e t h l y 1 s e d c c h 3 h g d s e d 16 d c h g 2 p s e d d t k m e t h y l s e d c h g 2 p s e d k d e m e t h l y 1 s e d c c h 3 h g d s e d where cch3hg p sed is the particulate methylmercury concentrations in sediment ng g 1 chg2 p sed is the particulate inorganic mercury concentration in sediment ng g 1 kmethyl sed is the mercury methylation rate constant in sediment d 1 and kdemethyl sed is the mercury demethylation rate constant in sediment d 1 after the reactions the equilibrium between the dissolved and solid phase in water column as well as between sediment porewater and sediment is calculated then by the concentration differences between dissolved hg in water and sediment porewater hg concentrations the hg flux to overlying water can be calculated the hg partitioning between solid and dissolved phase is calculated by linear adsorption as follows 17 c h g p w c k d h g w c c h g d w c 18 c h g p s e d k d h g s e d c h g d s e d where kd hg wc is the linear adsorption coefficient of hg between solid and dissolved phase in the water column l kg 1 and kd hg sed is the linear adsorption coefficient of hg between solid and dissolved phase in sediment l kg 1 the diffusive flux at the sediment water interface was calculated using the following equation 19 f l u x h g s e d w c θ s e d d h g w 1 1 n θ s e d 2 d c d x where dhg w is the diffusivity of hg2 or ch3hg in water cm2 s 1 θsed is the porosity of sediments dc is the hg2 or ch3hg concentration difference between water column chg d wc and porewater of sediment layer 1 chg d sed ng l 1 and dx is half of the sediment layer 1 depth which is fixed at 2 5 cm 3 6 hg bioaccumulation and biomagnification once dissolved thg and ch3hg concentrations in water column and sediment porewater are determined the hg concentrations in biota such as zooplankton and fish were calculated using bioconcentration factors as follows burkhard 2003 hong et al 2014 20 log c hg biota a hg biota δ 15 n biota b hg biota where chg biota is the thg or ch3hg concentration in biota ng g 1 ahg biota is an average slope of the equation referring to the biomagnification rate for thg or ch3hg δ15nbiota is the stable isotope ratio of nitrogen of organism and bhg biota is the average intercept representing the food chain specific base line value for thg and ch3hg the bhg biota is dependent upon dissolved hg levels in the water column as follows 21 b h g b i o t a b c f f b c h g d w c where bcffb is the bioconcentration factor for the food chain specific base line l kg 1 3 7 study area and model input parameters jang sung dam jsd was constructed mainly for agricultural irrigation in 1976 and has a storage capacity of 8 480 tons the jsd watershed is a headwater subbasin of the youngsan river basin located in the south western end of the korean peninsula fig 2 a the watershed is in the east asian monsoonal region having temperate climate with four distinct seasons daily temperature varies between 5 c in winter and 29 c in summer offering a warm temperature throughout the year for plant growth fig s1 of the si shows that about 50 of annual rainfall 1 432 mm occurs during the monsoon period from july to august the jsd watershed is predominantly a pristine forest watershed with no known local sources of hg therefore atmospheric deposition is assumed to be the only source of hg in the watershed the spatial map layers historical weather data and dam discharge rate were formatted as input to the current swat model swat 2012 rev 645 and arcswat 2012 10 2 16 using a 30 m digital elevation model the jsd watershed was delineated into subbasins and a stream network that best represents the tributary and main channels was developed the jsd watershed model was developed such that the final outlet of the watershed is jang sung dam raster maps representing land uses soil properties and land slopes are overlaid to define hydrological response units hrus since the swat hg simulates both hydrologic cycles and hg biogeochemical processes extensive observed data were collected from the study watershed the weather data in the jsd watershed was collected from the korea meteorological administration historical daily water storage volume and irrigational use of the water in jsd were recorded by the korea rural community corporation hg values in the jsd water sediment fish soil leaves and groundwater were measured as part of this study section 3 of the si summarizes detailed methods used for collecting these hg data for atmospheric hg deposition rate we used published data in similar forestry environments in south korea han et al 2016 for model calibration and validation input parameters were refined to best describe water storage suspended solid concentrations and hg concentrations in the water and sediment bed layers in the reservoir as well as the hg contents in soils leaves and groundwater in jsd watershed these input parameters calibrated for hg simulation are summarized in table s1 4 results and discussion 4 1 watershed characteristics and calibration results the watershed was discretized into 26 subbasins and further split into 384 hrus fig 2 b d illustrate the spatial diversity of elevation land uses and soils in the jsd watershed the watershed covers a 105 km2 area and the predominant land use type is forest which claims 84 9 of the land the forest lands are comprised of deciduous trees 36 7 that are most prevalent in the northern subwatersheds evergreen trees 22 3 in the central and southern subwatersheds and mixed trees 26 0 in the south part of the watershed other land uses include paddy fields 9 urban 2 and others 4 paddies are mostly formed along the main and tributary channels for irrigation purposes overall the soils are well draining but a large area in the northern side of the watershed is formed with highly impermeable rocks on steep slopes the soil textural composition in the rest of the watershed is mainly silty sands with the soil thickness varying between 1 4 and 2 5 m the terrain consists mostly of steep mountain slopes with narrow valleys approximately 71 of the watershed is on steep slopes greater than 40 or 0 4 m m while the mean slope is 45 6 the average channel lengths were 1 95 and 3 7 km for the main channels and tributary channels respectively the tributary channels had steeper slopes 10 7 than the main channels 1 9 as main channels are usually higher order streams in the downstream of tributary channels the swat model characterized the hydrological properties and sediment transport in the jsd watershed by calibrating the water storage volume and sediment concentration in the jsd reservoir over a 5 year period as shown in fig 3 and table s2 of si daily discharge data were incorporated into swat and simulated for the five years from 2010 to 2014 with two years of a spin up period predicted daily water storage values and sediment concentration in the reservoir were compared to observed values for calibration 2010 2012 and for validation 2013 2014 after calibration and validation the swat model for the jsd watershed was able to simulate the water balances reasonably accurately to achieve statistical correlations in central tendency as well as in temporal variability the high uniformity in land uses with distributed pristine forests is attributable to such predictable hydrologic behavior results indicate that the water balance and sediment yield are mainly driven by steep land slopes in the jsd watershed at the watershed scale hydrologic processes driven by precipitation 1 433 mm are the return flow 44 4 that is comprised of the lateral flow 27 4 and groundwater flow 16 9 evapotranspiration 38 9 and surface runoff 12 2 aquifer recharge was estimated to account for 4 6 of the precipitation steep slopes promote soil erosion and if no management is implemented for soil erosion can cause high sediment loss in the jsd watershed even though the total water yield is more than 50 of the precipitation on steep slopes the estimated sediment yield was only 19 tons ha year though unmanaged the forest system provides soil cover with its residue on the top soil layer also the steep slope promotes quick drainage of soil moisture on shallow soil layers which in turn promotes infiltration of rain water these eco system services the forest may provide offer good protection of top soils from erosion even on steep slopes fig 4 summarizes calibration results for hg biogeochemical processes and the measured thg and mehg as well as other ancillary parameters in various forms are summarized in si the observed hg soil contents varied between sampling locations up to two folds between the lowest value of 82 μg kg 1 in subbasin 24 and the highest value of 168 μg kg 1 in subbasin 5 the spatial variability of hg soil contents in the jsd area may be attributable to the heterogeneous soils and diverse vegetation types at the microscale overall the model managed to produce a reasonable magnitude of standard errors with the mean standard deviation of 21 2 μg kg 1 except for subbasin 12 where the predicted standard deviation was 76 4 μg kg 1 while the mean value was 154 1 μg kg 1 rice paddies and urban lands claiming large areas of the subbasin are attributable to the significant variance in the predicted hg distribution the measured hg levels in leaves water sediment sediment porewater and fish were also compared with values calculated by the swat hg the average standard deviation thg and mehg concentrations in the 14 unfiltered lake water samples were 1 05 0 34 and 0 093 0 058 ng l 1 respectively the average thg and mehg in 16 leaf samples was 38 3 3 50 ng g 1 and 0 14 0 13 ng g 1 respectively the average thg and mehg in 6 sediment samples was 138 8 32 5 ng g 1 and 0 71 0 63 ng g 1 respectively the average thg and mehg in 6 sediment porewater samples were 16 4 2 8 ng l 1 and 2 33 0 32 ng l 1 respectively the average thg in four groundwater samples was 2 33 1 07 ng l 1 the values were in the range of reported values obtained in other pristine forestry areas denkenberger et al 2014 kainz and lucotte 2006 luo et al 2014 noh et al 2016 obrist et al 2011 the bioconcentration and biomagnification of mehg in fish and food chain was best described by log cmehg fish 0 15 δ15n 0 72 r 0 68 the slope of the linear regression results is called the trophic magnification slope and the value was within the range of reported values of 0 24 0 08 lavoie et al 2013 the swat hg reproduced the measured values well and the model captured the major hg biogeochemical reactions in the watershed 4 2 spatial distribution of hg in surficial soil and temporal variation of hg loadings to jsd in the jsd watershed the deciduous trees are broadleaf trees such as japanese maple and chinese cork oak trees while the evergreen forest has narrowleaf trees like pine trees these trees can affect hg cycling in the watershed blackwell et al 2014 through hg leaf accumulation followed by leaf falls fig s6 and table s3 of the si show the distribution of simulated leaf storage of hg between watershed components and its levels in each forest soil that are simulated the variance of the hg storage in deciduous forest was markedly high and this has to do with the variable timing and intensity of autumn leaf falls as well as the annual growth rate of broadleaf trees demers et al 2007 the top right of fig 5 shows that the northern side headwater subbasins subbasins 1 to 5 that are predominantly covered with deciduous forest have low hg deposition rates or soil hg storages the rocky soil in these subbasins with the soil classification name of roo has rock fraction in soils greater than 90 which makes the soil environment harsh for plant growth thus the trees in these subbasins do not grow well compared to those in more fertile soils in general subbasins that have high hg storage in soils are the subbasins with high hg deposition rates by litterfall subbasins 6 9 and 10 or those having high leaf storage subbasins 16 to 22 because little hg gets transported out of the soil by runoff or erosion due to the strong association with soils no correlation was found between the hydrologic properties like the curve number or slopes and the hg soil storage to understand the impact of the hydrologic properties on the hg cycling the hg loading to the watershed is further investigated fig 6 shows monthly averaged values and indicates that the jsd watershed received an annual rainfall of 1 400 mm and the estimated surface runoff was 133 mm which accounts for only 10 of the rainfall the major hydrologic processes in this specific watershed are lateral flow and groundwater flow which account for 40 8 and 31 1 respectively of the total water yield in the jsd these hydrologic cycles are dependent upon the precipitation pattern hence show temporal behaviors high flows occur during monsoon seasons in july and august when the rainfall intensity is the highest surface runoff and sediment transport also occur actively during the monsoon period claiming 36 of the total hg loads while in low flow season they were negligible the total hg loading to the watershed was 136 g yr 1 and the loading in july and august were 33 4 and 28 4 g yr 1 respectively this corresponds to 24 5 and 21 0 of the total hg loading per year among the hg loadings that are elevated during the rainy season the surface runoff and soil erosion are particularly the main causes of the increase in hg loads the portions of hg loading by surface runoff and soil erosion are 46 1 and 24 2 of the total hg loading for july and august respectively suggesting the importance of soil erosion for hg transport uplands to waterbody during rainy seasons however for the whole year groundwater flow is the most influential media that carries hg to the waterbody accounting for 52 0 of the annual thg loads the hg loading by lateral flow is 28 4 and surface runoff and soil erosion accounts for only 19 6 of the thg loading to the waterbody on a yearly basis though the environmental and hydrologic settings are different similar results were reported in other coastal plain streams bradley et al 2012 vidon et al 2013 the hydrologic cycle is the major driving environmental process that transport various nutrients and pollutants to a water body g arnold et al 2012 and groundwater hg transport can be an important hg source to a waterbody 4 3 watershed scale mass balance of hg fig 7 summarizes the hg mass balance estimated with the swat hg for the jsd watershed overall 1 83 kg yr 1 of hg is introduced to the watershed by atmospheric hg deposition and litter fall the amount of hg that leaves the watershed is 0 92 kg yr 1 which is calculated from evaporation 0 73 g yr 1 transportation to jsd 0 136 kg yr 1 and percolation to deep aquifer 0 05 kg yr 1 ultimately the net hg accumulation in the watershed is estimated 0 91 kg yr 1 the total hg storage in the watershed is 200 kg which is estimated from storages in leaves 3 23 kg litter 0 006 kg soil 133 kg aquifer 0 43 kg water 0 091 kg and sediment 63 3 kg under the current hg deposition conditions there can be 0 46 of hg mass increase per year 0 91 kg yr 1 divided by 200 kg in the watershed most of the hg introduced to the water body is deposited in sediment as particulate form although the hg may be released back to the water by diffusive flux from sediment porewater to the waterbody the mass balance in sediment suggests that thg levels can increase over time since there is a net amount of 6 g yr 1 thg deposition to the sediment the total hg storage in sediment is 63 3 kg and the rate of hg mass increase would be 0 01 per year in the watershed the thg deposition rate was 1 8 kg yr 1 and the atmospheric hg deposition was 1 5 kg yr 1 which accounts for 81 6 of the total hg deposition to the watershed the litter fall was 0 34 kg yr 1 and also provided significant hg deposition to the soil occupying 18 4 of the thg deposition previous studies show that litterfall alone could account for 30 60 of the total atmospheric hg inputs pokharel 2011 sheehan et al 2006 the thg loading by leaf fall in the jsd watershed seemed to be lower compared to the previous observations and this could be due to the coniferous trees that occupy more than 40 of the watershed most of the hg deposition in the jsd is stored in soils 130 kg and on tree leaves 3 2 kg which could subsequently be transported to the jsd reservoir a wide range of litterfall hg inputs are reported in the literature wang et al 2016a 2016b in evergreen broadleaves forest in china 26 0 42 9 μg m 2 yr 1 of litterfall hg inputs are reported litterfall hg input observed at temperate and boreal tb forest background sites in europe and north america ranged 2 7 59 5 μg m 2 yr 1 litterfall hg in the jsd watershed forest was 4 0 μg m 2 yr 1 and this value was in the lower range of the reported values in the tb forest the hg levels in jsd watershed leaves were 38 3 11 1 ng g 1 which are similar to the reported values e g 47 23 ng g 1 at evergreen broadleaves eb forest in china and 45 11 ng g 1 at tb forest sites in addition litterfall biomass production was 1304 161 mg km 2 yr 1 for eb forest in china and 318 163 mg km 2 yr 1 in tb wang et al 2016b though the litterfall biomass in jsd watershed was 303 mg km 2 yr 1 which were similar with the average values in tb note that the litterfall biomass in similar temperature forest in korea was 338 54 mg km 2 yr and 198 123 mg km 2 yr 1 for deciduous and coniferous trees an et al 2017 respectively our hg litterfall related values are comparable to the values obtained in the tb forest sites and further support the robustness of the swat hg the thg loading into the jangsung lake was 136 g yr 1 estimated from the surface runoff 3 2 g yr 1 soil erosion 24 g yr 1 lateral flow 39 g yr 1 and groundwater return flow 70 g yr 1 a previous study in lake ontario showed that the total fluvial hg fluxes to the lake varied from 0 5 to 2 6 g km 2 yr 1 depending on watershed characteristics and in particular the hg flux in the salmon watershed which was covered with 89 of forest was 1 6 g km 2 yr 1 denkenberger et al 2014 the total hg fluvial flux in the present study was 1 3 g km 2 yr 1 0 136 kg yr 1 from 105 km2 which agrees well with these observed values the total hg atmospheric deposition in the jsd was 14 2 μg m 2 yr 1 which was also in the range of the values in lake ontario 7 0 19 4 μg m 2 yr 1 our study further showed that in forestry areas with small soil erosion the hidden hydrologic cycles such as lateral and groundwater flow could be the dominant hg loading processes andrea et al 2018 gonzález fernández et al 2014 lamborg et al 2013 the average residence time of hg in the terrestrial environments can be roughly calculated from the total hg storage 137 kg divided by the hg that leaves the environment 0 92 kg yr 1 which is 149 years this implies that the soil enriched with historic hg deposition from anthropogenic sources would reside in the area as a potential source for a very long time in global scale hg transport models the average residence time of hg in soil is assumed to be in the range 100 1000 years selin et al 2008 other modeling study showed that the mean soil hg turnover time for tropical and temperate forest ranged 126 151 years and boreal forest was 560 years our results were similar to the values used for tropical and temperate forest smith downey et al 2010 the swat hg showed a potential on simulating the basic mehg dynamics the total mehg loading to the waterbody was 1 75 g yr 1 and the net mehg flux from sediment to overlying water was 2 0 g yr 1 the mehg discharged to the downstream of the waterbody was 11 4 g yr 1 suggesting that 70 of mehg in the waterbody is produced in the water column probably mercury methylation can occur in oxic water columns in this specific jsd waterbody and this results correspond with a recent study gascón díez et al 2016 the actual mercury methylation processes are complex and affected by the dissolved organic matter temperature and concentrations of nutrients noh et al 2017 2018 which are not considered in the present study and further research is needed to better understand mehg cycling in watershed scale 4 4 predicting future hg levels in jsd watershed simple swat hg simulations were conducted for the jsd watershed to assess the impact of change in hg atmospheric deposition to terrestrial and aquatic hg fate and transport in these scenarios atmospheric hg deposition rate was reduced or increased by 20 from historical to represent a successful control of hg and an elevated hg emission due to failure in the management of hg sources respectively pacyna et al 2016 these scenarios were simulated from 2015 to 2040 using only the historical weather data of 2015 fig 8 a shows that the change of atmospheric hg deposition slowly affects the hg levels in jsd water over the decades even in status quo atmospheric hg deposition scenario the increase of hg levels in jsd water was observed suggesting that the hg levels in the watershed has not reached steady state yet lowering atmospheric hg deposition by 20 seems to keep the level of hg in water constant and the increase of hg deposition by 20 raised the level of hg in a greater proportion hence greater reduction of hg deposition i e more than 20 reduction may be needed in the area to lower the hg exposure risks to human and ecologic top predators this may be the case for most of the hg impacted inland waterbodies with forestry watershed historically deposited hg could be a long term hg sources and constantly raise hg levels in water and fish even after cutting down of atmospheric hg deposition fig 8 b provides a few more detailed hg levels in the various compartments of watershed in 2040 after the change of atmospheric hg deposition in 2015 the changes of hg values relative to the status quo were similar to the changes of atmospheric hg deposition for runoff hg loading baseflow hg loading and thg and mehg concentrations in water however the soil hg storage total hg loading and hg loading by soil erosion were resistant to the changes over the decades these modeling results further showed that the dissolved forms of hg transport by runoff and baseflow in watershed is the major hg transport process and the response of the watershed loadings by the change of atmospheric hg deposition would be faster compared to particulate phase transport and would affect hg levels in waterbodies 5 conclusions in the present study swat hg model is developed to simulate hg fate and transport in conjunction with hydrologic processes in watershed scale the model was tested in a small mountainous watershed which was predominant occupied by a temperate forest with steep slopes the atmospheric hg deposition accounted for 82 of total hg deposition and the remaining 18 was originated from litterfall the swat hg simulated seasonal dynamic growth of tree leaves and was able to explain the higher hg levels in soils covered with deciduous trees although the most of the deposited hg was stored in soil however the soil erosion was not significant due to the well developed forest hg transport by baseflow was the most significant part since at the watershed scale the baseflow occupied 44 3 of hydrologic processes driven by precipitation our study further showed that in forestry areas with small soil erosion the hidden hydrologic cycles such as lateral and groundwater flow could be the dominant hg loading processes the average residence time of hg in the terrestrial environments was estimated to be 149 years and this implies that the soil enriched with historic hg deposition from anthropogenic sources would reside in the area as a potential source for a very long time one of the advantages of swat hg is its possible application to various watersheds in many other countries that have defined land uses soil properties and land slope databases although the swat hg could have a potential for extending to other areas the model needs to be improved to better predict hg fate and transport in the watershed and biogeochemical transformation in waterbodies for example daily or monthly values of hg atmospheric deposition in each subbasin would refine the spatial variability of the atmospheric deposition rate the actual mercury methylation processes are complex and affected by dissolved organic matter temperature ph concentrations of nutrients etc which are not considered in this study an incorporation of methylmercury production considering aquatic environmental factors would be an important enhancement of the model in assessing the future behaviors of mercury in watersheds and waterbodies since the values are coupled with each other and are changing simultaneously software availability name of software swat hg developer jaehak jung contact address texas a m agrilife research blackland research and extension center temple tx 76502 usa e mail jjeong brc tamus edu year first available 2019 program language fortran availability swat hg is freely available at https bitbucket org j9849 swat hg src master the swat hg code will ultimately merged to the main swat model swat2016 swat and distributed through the swat website https swat tamu edu declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported by the basic science research program through the national research foundation of korea nrf funded by the ministry of science ict future planning 2018r1d1a1b07049757 and by the korean ministry of the environment via the environmental health action program grant no 2015001370003 authors thank kurt louis solis for creating toc art appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104644 
26050,terrestrial and aquatic mercury biogeochemical processes are integrated into the soil and water assessment tool swat to simulate the watershed scale hg behaviors the swat hg was successfully calibrated and validated with the data obtained from the jang sung dam jsd watershed the largest hg storage in the watershed was found in soil 93 however soil erosion was not the largest hg source for the jsd reservoir due to the well established forests the model suggests the baseflow is the most dominant hg loading to the waterbody which accounts for 80 0 of the thg yields on the annual basis the model simulation was projected to the year 2040 and showed that the hg levels in various compartments of watershed have not reached to steady state hence the hg levels changed slowly over the decades even in status quo atmospheric hg deposition graphical abstract image 1 keywords mercury temperate mountainous watershed terrestrial transport swat bioaccumulation 1 introduction risks associated with mercury hg exposure via fish consumption has significantly increased over the past decades in global scale due to the combustion of fossil fuels and consequent release of hg into the atmosphere and waterbodies fitzgerald et al 2005 unep 2013 to reduce hg release to environments and to mitigate human and ecologic exposure risks of hg the minamata convention is being implemented to decrease the release of hg into the atmosphere in coal fired powerplants cement production industries and other sources various efforts are being taken to reduce atmospheric hg release worldwide however estimating the magnitude and timing of the risk reduction of hg exposure via fish consumption is still challenging due to the complex transport processes of hg in the terrestrial and aquatic environments munthe et al 2007 oswald et al 2014 in environments atmospherically released hg can be transported to pristine areas fitzgerald et al 1998 lindberg and stratton 1998 and ultimately be deposited on various land covers such as urban areas forests barren soils snow covers etc futter et al 2012 graydon et al 2006 also the atmospheric hg can be associated with plants and tree leaves which is then transported to soil by litterfall wang et al 2016a the litter can be decomposed slowly by biogeochemical reactions and hg can be released and adsorbed to soil particles pokharel 2011 the hg can be associated with plants and soils and can be transported to waters by terrestrial and hydrological processes agnan et al 2016 denkenberger et al 2014 gai et al 2016 yu et al 2014 which could be converted to methylmercury mehg and can enter the food chain base via bioconcentration and bioaccumulation fitzgerald et al 2007 lavoie et al 2013 morel et al 1998 the bioaccumulated hg can be biomagnified and predators can be exposed to high level of hg via fish consumption lavoie et al 2013 tsui et al 2009 the released atmospheric hg needs to travel through several environmental media in air soil water and biota to get to final receptors blackwell et al 2014 domagalski et al 2016 obrist et al 2016 hence linking atmospherically released hg to final receptors and quantifying risks associated with the accumulation of hg in biota is important but has been a challenge due to the uncertainties associated with the complex transport and transformation processes of hg there have been several empirical and experimental efforts that attempted to link atmospheric hg deposition to hg levels in fish and eventually to subsequent hg exposure to ultimate receptors considering watershed characteristics and terrestrial processes gerson and driscoll 2016 harris et al 2007 hurley et al 1995 oswald et al 2014 in addition simple watershed transport models and complex aquatic hg biogeochemical reactions were integrated and applied to several rivers and lakes with varying size of watersheds ambrose et al 2005 brown et al 2007 hong et al 2014 knightes 2008 knightes et al 2009 to relate the atmospheric hg deposition to hg in fish the integrated catchments model for mercury inca hg was developed and applied to two boreal forest areas with more sophisticated catchment processes and hg biogeochemical reactions futter et al 2012 however these models are generally limited to specific sites or regions thus the application of the model in other areas has been limited specifically one of the main deficiencies of the inca hg is the lack of dynamic plant growth simulation which makes it uncertain to estimate leaf fall litter production and ultimately the transport of leaf accumulated hg from canopy to the litter layer on the ground in addition the fate and transport of hg in terrestrial environments can be largely driven by hydrologic processes which are tightly coupled with land use types watershed terrain morphology and geological conditions bradley et al 2012 hurley et al 1998 which have been rarely considered in modeling hg fate and transport at the watershed scale in the present study the hg fate and transport in terrestrial and aquatic environments are integrated into the soil and water assessment tool swat https swat tamu edu last accessed on november 20 2018 swat is a gis based watershed scale model developed by the u s department of agriculture agricultural research service usda ars swat has been mainly used to predict the impact of land management practices on water sediment nutrients pesticides and heavy metal yields in large complex watersheds with varying soils land use and management conditions over long periods of time i e decades arnold et al 2012 chen et al 2017 cho et al 2012 du et al 2019 gao et al 2019 jiao et al 2014 meng et al 2018 zhang et al 2014 zhang 2018 in this study complex hg terrestrial and aquatic biogeochemical processes with distributed deposition of atmospheric hg are incorporated into swat to evaluate watershed scale hg transport processes especially this study focused on investigating the mass fluxes of hg in various environmental compartments i e atmospheric and litterfall hg deposition seasonable variation of watershed loadings to a waterbody and the importance of base flow on hg transport in a watershed scale 2 conceptual model of swat hg the swat creates a physical watershed model from a gis based digital elevation model dem land use information and soil types a stream network and subbasins are created from the delineation of the dem then the maps of land uses soils and land slopes are overlaid to the delineated subbasins to generate hydrologic response units hrus afterward daily weather information and land management practices are integrated to input data and physical processes associated with water movement soil erosion and sediment transport plant and tree growth and nutrient cycling are simulated arnold et al 2012 computational procedures for simulating terrestrial and aquatic hg processes are developed and written to the swat source code the transformation fate and transport of the mercury species are simulated within a watershed s hydrologic system which allows the calculation of hg s terrestrial and aquatic fate and transport three mercury species divalent mercury hg2 methylmercury ch3hg and elemental mercury hg0 are assumed to be the main state variables in atmosphere tree leaves leaf surfaces four soil layers groundwaters streams sediments and biota in either dissolved or particulate form other mercury compounds such as dimethylmercury are not included in the model due to their low levels in natural environments zhu et al 2018 compared to the three hg species the summation of the three species of mercury is the total mercury thg dry and wet atmospheric deposition processes are included in weather input data the atmospherically deposited hg is split between plant canopy and lands based on daily ground cover made by plants hg accumulates on leaf surfaces in forests in washable forms as a result of dry deposition when it rains trees can intercept a portion of rainfall and hg on the leaf surface if the total rainfall exceeds the canopy interception capacity raindrops falling on leaves may reach the ground surface either in stemflow or by falling through leaves the throughfall flow washes off the hg stored on tree leaves into the ground litter and to top soils leaves can also accumulate hg in the tissue from reactive gaseous hg species in the atmosphere and become litterfall during fall and winter seasons a fraction of the hg deposited to soils evaporates back into the atmosphere depending on daily solar radiation air temperature and the total hg concentrations in the soil once hg is deposited on the soil surface hg is partitioned between soil particles and porewater based on soil organic carbon fraction as surface litter is decomposed over a number of years the dissolved form of hg is mobile in saturated soils as soil water percolates to subsoil layers if excess rainfall results in overland flow to occur dissolved hg in the top soil layer may become mobile in the runoff water and transported laterally to receiving waterbodies such as streams or reservoirs in addition dissolved hg is subject to flowing laterally in the rootzone discharging to the main channel in the return flow from shallow aquifer or recharging deep aquifer particulate hg does not percolate to deep soils but can be mixed into sub soil layers as a result of soil mixing by earthworms under proper soil moisture and soil temperature conditions although the majority of particulate hg remains on the soil surface significant overland flow event with erosive force can transport particulate hg to downstream along with soil particles aquatic hg processes are modeled based on the assumption of treating reach segments or reservoir storage a well mixed tank reactor two sediment layers are defined underneath waterbodies the surficial sediment layer is biogeochemically active and interacts with the water column the deeper sediment layer represents historically deposited sediments and hg significant processes including sediment transport deposition resuspension hg methylation demethylation and sorption desorption occur both in the water column and surficial sediment the mehg in the water column is bioconcentrated at the foodweb base then biomagnified through trophic transfer which can be quantified by nitrogen stable isotope fractionation the conceptual hg model is graphically depicted in fig 1 3 swat hg model development 3 1 atmospheric hg deposition model the swat hg model considers the atmospheric deposition as a source of hg though specific point and unspecific non point hg sources can also be assigned in the model either a constant or variable rates of hg atmospheric deposition in the air at daily monthly or yearly time scale provide boundary conditions to the model for simulating hg transport processes every day the total amount of atmospheric hg deposition is split between the plant canopy and land depending on the ground cover made by plants the fraction of ground cover by plants fgrdcv is estimated as follows riaño et al 2004 1 f grdcv 1 e k lai where k is the light extinction coefficient which is available in the plant database of the swat model and lai is the leaf area index which varies by type of tree the relationship between gaseous elemental hg concentration c hg air ng m 3 in the air and hg contents in foliar tissues c hg foliar ng g 1 can be approximated to a linear function in the forest environment fay and gustin 2007 2 c hg foliar a c hg air b the slope a and the intercept b vary by different type of tree table s1 shows these parameters for different types of trees the hg in leaves will be a watershed loading when the litterfall starts from fall the atmospheric deposition of hg may occur in dry weather conditions on land surface i e direct deposition or on leaf surfaces in forests leaf fall in washable forms as follows 3 w hg dry direct w hg dry 1 f grdcv 4 w hg dry leaf w hg dry f grdcv where whg dry direct is the dry atmospheric hg deposition to soil surface ng m 2 d 1 whg dry leaf is the dry atmospheric hg deposition to leaf surface ng m 2 d 1 whg dry is total dry atmospheric hg deposition ng m 2 d 1 established plants may intercept a portion of rainfall on leaves if the total rainfall exceeds the canopy interception capacity of a tree raindrops falling on leaves may reach the ground surface either in stemflow or by falling through leaves the throughfall flow washes off hg on tree leaves into the ground and top soils the total wet deposition is estimated by 5 w hg wet total w hg throughfall f grdcv w hg wet direct 1 f grdcv where whg wet total is the wet atmospheric hg deposition ng m 2 d 1 and whg throughfall is the wet atmospheric hg deposition via throughfall ng m 2 d 1 and whg wet direct is the direct wet atmospheric hg deposition to soil surface ng m 2 d 1 in eq 4 w throughfall is the sum of hg in rainfall and the hg that is washed off from leaf surfaces it is assumed that the amount of hg wash off is proportional to the ratio of the effective rainfall total rainfall canopy interception and canopy interception if snow is present atmospheric hg deposition applies to snow cover if it exists and the hg storage in snow cover moves to top soil in proportion to snow melt amount as it occurs 3 2 fate and transport of hg in soils the dissolved form of hg can be mobile in saturated soils as soil water percolates to subsoil layers if excess rainfall results in overland flow occurring dissolved hg in the top soil layer may become mobile in the runoff water and transported laterally to the receiving waterbody in addition dissolved hg is subject to lateral flow in the rootzone return flow from shallow aquifer or deep aquifer recharge by following the soil water in dissolved form particulate hg does not percolate to deep soil layers gai et al 2016 but can be mixed in subsoil layers as biological mixing by earthworms becomes active with proper soil moisture and soil temperature significant overland flow with erosive force may detach particulate hg along from soils along with soil particles to deliver to downstream in soils hg in dissolved and particulate phase can partition to each other based on the soil organic carbon contents soc schuster 1991 yin et al 1997 equilibrium state is made between the particulate and dissolved from hg based on the linear adsorption partitioning coefficients as follows 6 c hg p soil k d hg soil c hg d soil 7 k d hg soil a hg f oc soil where kd hg soil is the linear adsorption coefficient of hg between solid and dissolved phase in soil l kg 1 foc is soc g g 1 and ahg is the linear relationship coefficient between kd hg soil and foc soil l kg 1 in soil mercury methylation demethylation and oxidation and reduction could occur qiu et al 2005 however the rates usually have a time scale of hours and the transformation between mercury species reaches quasi steady state within a day thus the swat model assumes these equilibrium conditions occur instantly while running at daily time step hence in soil the specific distribution of hg2 ch3hg and hg0 is assigned as follows 8 c hg p soil f hg soil c thg p where chg p soil is the particulate hg2 ch3hg and hg0 concentrations in soil μg g 1 fhg soil is the fraction of hg2 ch3hg and hg0 and cthg p is the total hg concentration in soil particulate μg g 1 3 3 characterizing tree growth and leaffall swat simulates plant growth based on the epic plant model g arnold et al 2012 plant growth is modeled by simulating leaf area development light interception and conversion of intercepted light into biomass assuming that a plant has a unique radiation use efficiency 9 w bio rue 0 5 h day 1 e k l lai where wbio is the potential increase in total plant biomass on a day kg ha 1 rue is radiation use efficiency 0 5hday is the incident total solar radiation mj m 2 kl is the light extinction coefficient and lai is the leaf area index rue is estimated using stockle et al 1992 10 rue 100 co 2 co 2 e r 1 r 2 co 2 where co2 is the concentration of carbon dioxide in the atmosphere ppmv and r1 and r2 are shape coefficients plant growth is simulated by calculating the leaf area development based on the daily heat unit index hui and the potential heat units phu and provides an s shape curve function shown in fig s2 even though the swat plant growth model estimates biomass in shoots and roots development it neglects calculating the amount of leaves on the aboveground biomass but only assumes that a fraction of the aboveground biomass falls to the ground during senescence or dormant periods as hg is known to accumulate on leaf surfaces and in leaf tissue it is important that leaf biomass is accurately estimated to reduce error in simulating hg deposition on or in tree leaves therefore the swat plant growth model was enhanced to estimate leaf biomass amounts by adopting the regression model proposed by baskerville 1972 11 log m l β log m t k where ml is the total leaf biomass of a tree tons mt is the total biomass of a tree tons and β and k are regression factors and shown in table s2 in supporting information three types of forest are defined in the swat plant database namely deciduous evergreen and mixed forest the growth characteristics of these forests are defined with 33 unique plant parameters the selected parameters were refined for the three types of forest in the jangsung dam watershed shown in table s3 autumn leaf fall in the jdw occurs between late september and mid november korea meteorological administration last accessed on march 7 2017 once lai reaches dlai the fraction of growing season when leaf area begins to decline senescence occurs for the remaining period of the growing season due to the biological aging of plant the amount of biomass lost in leaffall on a day is estimated as a function of the decrease in lai as leaffall occurs the hg deposited on leaf surface moves from leaf storage to litter storage or soil storage if no litter exists on the ground the non washable form hg existing in leaf tissue also moves from canopy to litter storage with senescence 3 4 hg evaporation from surficial soil the evaporation of hg from surficial soil to atmosphere can be significant in the overall hg balance in forestry watershed scale han et al 2016 schlüter 2000 research has suggested several important environmental factors that modulate hg fluxes from terrestrial surfaces which include solar radiation air and soil temperatures soil moisture etc agnan et al 2016 using the available data collected in the site the hg evaporation from soil flux hg soil ng m 2 hr 1 is modeled as follows lin et al 2010 12 flux hg soil c thg p soil β 0 β 1 t air β l 2 where β0 β1 β2 are fitting parameters that were fitted by field measured values cthg p soil is the total mercury soil concentrations μg kg 1 tair is the air temperature oc and l is the solar radiation mw cm 2 3 5 mercury process in water and sediment mercury undergoes complex biogeochemical reactions such as oxidation reduction methylation demethylation and photolytic degradation demethylation in water knightes 2008 the mercury methylation and demethylation are modeled using first order rate kinetics in the water column it is assumed that mercury methylation and demethylation occur in the dissolved phase 13 d c c h 3 h g d w c d t k m e t h y l w c c h g 2 d w c k d e m e t h l y 1 w c c c h 3 h g d w c 14 d c h g 2 d w c d t k m e t h y l w c c h g 2 d w c k d e m e t h l y 1 w c c c h 3 h g d w c where cch3hg d wc is the dissolved methylmercury concentrations in water column ng l 1 chg2 d wc is the dissolved inorganic mercury concentration in water column ng l 1 kmethyl wc is the mercury methylation rate constant in water column d 1 and kdemethyl wc is the mercury demethylation rate constant in water column d 1 similar hg processes could occur in sediments since the original swat doesn t consider the sediment as a separate model domain the sediment model is updated and described in si then mercury methylation and demthylation reactions are included in the model as follows 15 d c c h 3 h g p s e d d t k m e t h y l s e d c h g 2 p s e d k d e m e t h l y 1 s e d c c h 3 h g d s e d 16 d c h g 2 p s e d d t k m e t h y l s e d c h g 2 p s e d k d e m e t h l y 1 s e d c c h 3 h g d s e d where cch3hg p sed is the particulate methylmercury concentrations in sediment ng g 1 chg2 p sed is the particulate inorganic mercury concentration in sediment ng g 1 kmethyl sed is the mercury methylation rate constant in sediment d 1 and kdemethyl sed is the mercury demethylation rate constant in sediment d 1 after the reactions the equilibrium between the dissolved and solid phase in water column as well as between sediment porewater and sediment is calculated then by the concentration differences between dissolved hg in water and sediment porewater hg concentrations the hg flux to overlying water can be calculated the hg partitioning between solid and dissolved phase is calculated by linear adsorption as follows 17 c h g p w c k d h g w c c h g d w c 18 c h g p s e d k d h g s e d c h g d s e d where kd hg wc is the linear adsorption coefficient of hg between solid and dissolved phase in the water column l kg 1 and kd hg sed is the linear adsorption coefficient of hg between solid and dissolved phase in sediment l kg 1 the diffusive flux at the sediment water interface was calculated using the following equation 19 f l u x h g s e d w c θ s e d d h g w 1 1 n θ s e d 2 d c d x where dhg w is the diffusivity of hg2 or ch3hg in water cm2 s 1 θsed is the porosity of sediments dc is the hg2 or ch3hg concentration difference between water column chg d wc and porewater of sediment layer 1 chg d sed ng l 1 and dx is half of the sediment layer 1 depth which is fixed at 2 5 cm 3 6 hg bioaccumulation and biomagnification once dissolved thg and ch3hg concentrations in water column and sediment porewater are determined the hg concentrations in biota such as zooplankton and fish were calculated using bioconcentration factors as follows burkhard 2003 hong et al 2014 20 log c hg biota a hg biota δ 15 n biota b hg biota where chg biota is the thg or ch3hg concentration in biota ng g 1 ahg biota is an average slope of the equation referring to the biomagnification rate for thg or ch3hg δ15nbiota is the stable isotope ratio of nitrogen of organism and bhg biota is the average intercept representing the food chain specific base line value for thg and ch3hg the bhg biota is dependent upon dissolved hg levels in the water column as follows 21 b h g b i o t a b c f f b c h g d w c where bcffb is the bioconcentration factor for the food chain specific base line l kg 1 3 7 study area and model input parameters jang sung dam jsd was constructed mainly for agricultural irrigation in 1976 and has a storage capacity of 8 480 tons the jsd watershed is a headwater subbasin of the youngsan river basin located in the south western end of the korean peninsula fig 2 a the watershed is in the east asian monsoonal region having temperate climate with four distinct seasons daily temperature varies between 5 c in winter and 29 c in summer offering a warm temperature throughout the year for plant growth fig s1 of the si shows that about 50 of annual rainfall 1 432 mm occurs during the monsoon period from july to august the jsd watershed is predominantly a pristine forest watershed with no known local sources of hg therefore atmospheric deposition is assumed to be the only source of hg in the watershed the spatial map layers historical weather data and dam discharge rate were formatted as input to the current swat model swat 2012 rev 645 and arcswat 2012 10 2 16 using a 30 m digital elevation model the jsd watershed was delineated into subbasins and a stream network that best represents the tributary and main channels was developed the jsd watershed model was developed such that the final outlet of the watershed is jang sung dam raster maps representing land uses soil properties and land slopes are overlaid to define hydrological response units hrus since the swat hg simulates both hydrologic cycles and hg biogeochemical processes extensive observed data were collected from the study watershed the weather data in the jsd watershed was collected from the korea meteorological administration historical daily water storage volume and irrigational use of the water in jsd were recorded by the korea rural community corporation hg values in the jsd water sediment fish soil leaves and groundwater were measured as part of this study section 3 of the si summarizes detailed methods used for collecting these hg data for atmospheric hg deposition rate we used published data in similar forestry environments in south korea han et al 2016 for model calibration and validation input parameters were refined to best describe water storage suspended solid concentrations and hg concentrations in the water and sediment bed layers in the reservoir as well as the hg contents in soils leaves and groundwater in jsd watershed these input parameters calibrated for hg simulation are summarized in table s1 4 results and discussion 4 1 watershed characteristics and calibration results the watershed was discretized into 26 subbasins and further split into 384 hrus fig 2 b d illustrate the spatial diversity of elevation land uses and soils in the jsd watershed the watershed covers a 105 km2 area and the predominant land use type is forest which claims 84 9 of the land the forest lands are comprised of deciduous trees 36 7 that are most prevalent in the northern subwatersheds evergreen trees 22 3 in the central and southern subwatersheds and mixed trees 26 0 in the south part of the watershed other land uses include paddy fields 9 urban 2 and others 4 paddies are mostly formed along the main and tributary channels for irrigation purposes overall the soils are well draining but a large area in the northern side of the watershed is formed with highly impermeable rocks on steep slopes the soil textural composition in the rest of the watershed is mainly silty sands with the soil thickness varying between 1 4 and 2 5 m the terrain consists mostly of steep mountain slopes with narrow valleys approximately 71 of the watershed is on steep slopes greater than 40 or 0 4 m m while the mean slope is 45 6 the average channel lengths were 1 95 and 3 7 km for the main channels and tributary channels respectively the tributary channels had steeper slopes 10 7 than the main channels 1 9 as main channels are usually higher order streams in the downstream of tributary channels the swat model characterized the hydrological properties and sediment transport in the jsd watershed by calibrating the water storage volume and sediment concentration in the jsd reservoir over a 5 year period as shown in fig 3 and table s2 of si daily discharge data were incorporated into swat and simulated for the five years from 2010 to 2014 with two years of a spin up period predicted daily water storage values and sediment concentration in the reservoir were compared to observed values for calibration 2010 2012 and for validation 2013 2014 after calibration and validation the swat model for the jsd watershed was able to simulate the water balances reasonably accurately to achieve statistical correlations in central tendency as well as in temporal variability the high uniformity in land uses with distributed pristine forests is attributable to such predictable hydrologic behavior results indicate that the water balance and sediment yield are mainly driven by steep land slopes in the jsd watershed at the watershed scale hydrologic processes driven by precipitation 1 433 mm are the return flow 44 4 that is comprised of the lateral flow 27 4 and groundwater flow 16 9 evapotranspiration 38 9 and surface runoff 12 2 aquifer recharge was estimated to account for 4 6 of the precipitation steep slopes promote soil erosion and if no management is implemented for soil erosion can cause high sediment loss in the jsd watershed even though the total water yield is more than 50 of the precipitation on steep slopes the estimated sediment yield was only 19 tons ha year though unmanaged the forest system provides soil cover with its residue on the top soil layer also the steep slope promotes quick drainage of soil moisture on shallow soil layers which in turn promotes infiltration of rain water these eco system services the forest may provide offer good protection of top soils from erosion even on steep slopes fig 4 summarizes calibration results for hg biogeochemical processes and the measured thg and mehg as well as other ancillary parameters in various forms are summarized in si the observed hg soil contents varied between sampling locations up to two folds between the lowest value of 82 μg kg 1 in subbasin 24 and the highest value of 168 μg kg 1 in subbasin 5 the spatial variability of hg soil contents in the jsd area may be attributable to the heterogeneous soils and diverse vegetation types at the microscale overall the model managed to produce a reasonable magnitude of standard errors with the mean standard deviation of 21 2 μg kg 1 except for subbasin 12 where the predicted standard deviation was 76 4 μg kg 1 while the mean value was 154 1 μg kg 1 rice paddies and urban lands claiming large areas of the subbasin are attributable to the significant variance in the predicted hg distribution the measured hg levels in leaves water sediment sediment porewater and fish were also compared with values calculated by the swat hg the average standard deviation thg and mehg concentrations in the 14 unfiltered lake water samples were 1 05 0 34 and 0 093 0 058 ng l 1 respectively the average thg and mehg in 16 leaf samples was 38 3 3 50 ng g 1 and 0 14 0 13 ng g 1 respectively the average thg and mehg in 6 sediment samples was 138 8 32 5 ng g 1 and 0 71 0 63 ng g 1 respectively the average thg and mehg in 6 sediment porewater samples were 16 4 2 8 ng l 1 and 2 33 0 32 ng l 1 respectively the average thg in four groundwater samples was 2 33 1 07 ng l 1 the values were in the range of reported values obtained in other pristine forestry areas denkenberger et al 2014 kainz and lucotte 2006 luo et al 2014 noh et al 2016 obrist et al 2011 the bioconcentration and biomagnification of mehg in fish and food chain was best described by log cmehg fish 0 15 δ15n 0 72 r 0 68 the slope of the linear regression results is called the trophic magnification slope and the value was within the range of reported values of 0 24 0 08 lavoie et al 2013 the swat hg reproduced the measured values well and the model captured the major hg biogeochemical reactions in the watershed 4 2 spatial distribution of hg in surficial soil and temporal variation of hg loadings to jsd in the jsd watershed the deciduous trees are broadleaf trees such as japanese maple and chinese cork oak trees while the evergreen forest has narrowleaf trees like pine trees these trees can affect hg cycling in the watershed blackwell et al 2014 through hg leaf accumulation followed by leaf falls fig s6 and table s3 of the si show the distribution of simulated leaf storage of hg between watershed components and its levels in each forest soil that are simulated the variance of the hg storage in deciduous forest was markedly high and this has to do with the variable timing and intensity of autumn leaf falls as well as the annual growth rate of broadleaf trees demers et al 2007 the top right of fig 5 shows that the northern side headwater subbasins subbasins 1 to 5 that are predominantly covered with deciduous forest have low hg deposition rates or soil hg storages the rocky soil in these subbasins with the soil classification name of roo has rock fraction in soils greater than 90 which makes the soil environment harsh for plant growth thus the trees in these subbasins do not grow well compared to those in more fertile soils in general subbasins that have high hg storage in soils are the subbasins with high hg deposition rates by litterfall subbasins 6 9 and 10 or those having high leaf storage subbasins 16 to 22 because little hg gets transported out of the soil by runoff or erosion due to the strong association with soils no correlation was found between the hydrologic properties like the curve number or slopes and the hg soil storage to understand the impact of the hydrologic properties on the hg cycling the hg loading to the watershed is further investigated fig 6 shows monthly averaged values and indicates that the jsd watershed received an annual rainfall of 1 400 mm and the estimated surface runoff was 133 mm which accounts for only 10 of the rainfall the major hydrologic processes in this specific watershed are lateral flow and groundwater flow which account for 40 8 and 31 1 respectively of the total water yield in the jsd these hydrologic cycles are dependent upon the precipitation pattern hence show temporal behaviors high flows occur during monsoon seasons in july and august when the rainfall intensity is the highest surface runoff and sediment transport also occur actively during the monsoon period claiming 36 of the total hg loads while in low flow season they were negligible the total hg loading to the watershed was 136 g yr 1 and the loading in july and august were 33 4 and 28 4 g yr 1 respectively this corresponds to 24 5 and 21 0 of the total hg loading per year among the hg loadings that are elevated during the rainy season the surface runoff and soil erosion are particularly the main causes of the increase in hg loads the portions of hg loading by surface runoff and soil erosion are 46 1 and 24 2 of the total hg loading for july and august respectively suggesting the importance of soil erosion for hg transport uplands to waterbody during rainy seasons however for the whole year groundwater flow is the most influential media that carries hg to the waterbody accounting for 52 0 of the annual thg loads the hg loading by lateral flow is 28 4 and surface runoff and soil erosion accounts for only 19 6 of the thg loading to the waterbody on a yearly basis though the environmental and hydrologic settings are different similar results were reported in other coastal plain streams bradley et al 2012 vidon et al 2013 the hydrologic cycle is the major driving environmental process that transport various nutrients and pollutants to a water body g arnold et al 2012 and groundwater hg transport can be an important hg source to a waterbody 4 3 watershed scale mass balance of hg fig 7 summarizes the hg mass balance estimated with the swat hg for the jsd watershed overall 1 83 kg yr 1 of hg is introduced to the watershed by atmospheric hg deposition and litter fall the amount of hg that leaves the watershed is 0 92 kg yr 1 which is calculated from evaporation 0 73 g yr 1 transportation to jsd 0 136 kg yr 1 and percolation to deep aquifer 0 05 kg yr 1 ultimately the net hg accumulation in the watershed is estimated 0 91 kg yr 1 the total hg storage in the watershed is 200 kg which is estimated from storages in leaves 3 23 kg litter 0 006 kg soil 133 kg aquifer 0 43 kg water 0 091 kg and sediment 63 3 kg under the current hg deposition conditions there can be 0 46 of hg mass increase per year 0 91 kg yr 1 divided by 200 kg in the watershed most of the hg introduced to the water body is deposited in sediment as particulate form although the hg may be released back to the water by diffusive flux from sediment porewater to the waterbody the mass balance in sediment suggests that thg levels can increase over time since there is a net amount of 6 g yr 1 thg deposition to the sediment the total hg storage in sediment is 63 3 kg and the rate of hg mass increase would be 0 01 per year in the watershed the thg deposition rate was 1 8 kg yr 1 and the atmospheric hg deposition was 1 5 kg yr 1 which accounts for 81 6 of the total hg deposition to the watershed the litter fall was 0 34 kg yr 1 and also provided significant hg deposition to the soil occupying 18 4 of the thg deposition previous studies show that litterfall alone could account for 30 60 of the total atmospheric hg inputs pokharel 2011 sheehan et al 2006 the thg loading by leaf fall in the jsd watershed seemed to be lower compared to the previous observations and this could be due to the coniferous trees that occupy more than 40 of the watershed most of the hg deposition in the jsd is stored in soils 130 kg and on tree leaves 3 2 kg which could subsequently be transported to the jsd reservoir a wide range of litterfall hg inputs are reported in the literature wang et al 2016a 2016b in evergreen broadleaves forest in china 26 0 42 9 μg m 2 yr 1 of litterfall hg inputs are reported litterfall hg input observed at temperate and boreal tb forest background sites in europe and north america ranged 2 7 59 5 μg m 2 yr 1 litterfall hg in the jsd watershed forest was 4 0 μg m 2 yr 1 and this value was in the lower range of the reported values in the tb forest the hg levels in jsd watershed leaves were 38 3 11 1 ng g 1 which are similar to the reported values e g 47 23 ng g 1 at evergreen broadleaves eb forest in china and 45 11 ng g 1 at tb forest sites in addition litterfall biomass production was 1304 161 mg km 2 yr 1 for eb forest in china and 318 163 mg km 2 yr 1 in tb wang et al 2016b though the litterfall biomass in jsd watershed was 303 mg km 2 yr 1 which were similar with the average values in tb note that the litterfall biomass in similar temperature forest in korea was 338 54 mg km 2 yr and 198 123 mg km 2 yr 1 for deciduous and coniferous trees an et al 2017 respectively our hg litterfall related values are comparable to the values obtained in the tb forest sites and further support the robustness of the swat hg the thg loading into the jangsung lake was 136 g yr 1 estimated from the surface runoff 3 2 g yr 1 soil erosion 24 g yr 1 lateral flow 39 g yr 1 and groundwater return flow 70 g yr 1 a previous study in lake ontario showed that the total fluvial hg fluxes to the lake varied from 0 5 to 2 6 g km 2 yr 1 depending on watershed characteristics and in particular the hg flux in the salmon watershed which was covered with 89 of forest was 1 6 g km 2 yr 1 denkenberger et al 2014 the total hg fluvial flux in the present study was 1 3 g km 2 yr 1 0 136 kg yr 1 from 105 km2 which agrees well with these observed values the total hg atmospheric deposition in the jsd was 14 2 μg m 2 yr 1 which was also in the range of the values in lake ontario 7 0 19 4 μg m 2 yr 1 our study further showed that in forestry areas with small soil erosion the hidden hydrologic cycles such as lateral and groundwater flow could be the dominant hg loading processes andrea et al 2018 gonzález fernández et al 2014 lamborg et al 2013 the average residence time of hg in the terrestrial environments can be roughly calculated from the total hg storage 137 kg divided by the hg that leaves the environment 0 92 kg yr 1 which is 149 years this implies that the soil enriched with historic hg deposition from anthropogenic sources would reside in the area as a potential source for a very long time in global scale hg transport models the average residence time of hg in soil is assumed to be in the range 100 1000 years selin et al 2008 other modeling study showed that the mean soil hg turnover time for tropical and temperate forest ranged 126 151 years and boreal forest was 560 years our results were similar to the values used for tropical and temperate forest smith downey et al 2010 the swat hg showed a potential on simulating the basic mehg dynamics the total mehg loading to the waterbody was 1 75 g yr 1 and the net mehg flux from sediment to overlying water was 2 0 g yr 1 the mehg discharged to the downstream of the waterbody was 11 4 g yr 1 suggesting that 70 of mehg in the waterbody is produced in the water column probably mercury methylation can occur in oxic water columns in this specific jsd waterbody and this results correspond with a recent study gascón díez et al 2016 the actual mercury methylation processes are complex and affected by the dissolved organic matter temperature and concentrations of nutrients noh et al 2017 2018 which are not considered in the present study and further research is needed to better understand mehg cycling in watershed scale 4 4 predicting future hg levels in jsd watershed simple swat hg simulations were conducted for the jsd watershed to assess the impact of change in hg atmospheric deposition to terrestrial and aquatic hg fate and transport in these scenarios atmospheric hg deposition rate was reduced or increased by 20 from historical to represent a successful control of hg and an elevated hg emission due to failure in the management of hg sources respectively pacyna et al 2016 these scenarios were simulated from 2015 to 2040 using only the historical weather data of 2015 fig 8 a shows that the change of atmospheric hg deposition slowly affects the hg levels in jsd water over the decades even in status quo atmospheric hg deposition scenario the increase of hg levels in jsd water was observed suggesting that the hg levels in the watershed has not reached steady state yet lowering atmospheric hg deposition by 20 seems to keep the level of hg in water constant and the increase of hg deposition by 20 raised the level of hg in a greater proportion hence greater reduction of hg deposition i e more than 20 reduction may be needed in the area to lower the hg exposure risks to human and ecologic top predators this may be the case for most of the hg impacted inland waterbodies with forestry watershed historically deposited hg could be a long term hg sources and constantly raise hg levels in water and fish even after cutting down of atmospheric hg deposition fig 8 b provides a few more detailed hg levels in the various compartments of watershed in 2040 after the change of atmospheric hg deposition in 2015 the changes of hg values relative to the status quo were similar to the changes of atmospheric hg deposition for runoff hg loading baseflow hg loading and thg and mehg concentrations in water however the soil hg storage total hg loading and hg loading by soil erosion were resistant to the changes over the decades these modeling results further showed that the dissolved forms of hg transport by runoff and baseflow in watershed is the major hg transport process and the response of the watershed loadings by the change of atmospheric hg deposition would be faster compared to particulate phase transport and would affect hg levels in waterbodies 5 conclusions in the present study swat hg model is developed to simulate hg fate and transport in conjunction with hydrologic processes in watershed scale the model was tested in a small mountainous watershed which was predominant occupied by a temperate forest with steep slopes the atmospheric hg deposition accounted for 82 of total hg deposition and the remaining 18 was originated from litterfall the swat hg simulated seasonal dynamic growth of tree leaves and was able to explain the higher hg levels in soils covered with deciduous trees although the most of the deposited hg was stored in soil however the soil erosion was not significant due to the well developed forest hg transport by baseflow was the most significant part since at the watershed scale the baseflow occupied 44 3 of hydrologic processes driven by precipitation our study further showed that in forestry areas with small soil erosion the hidden hydrologic cycles such as lateral and groundwater flow could be the dominant hg loading processes the average residence time of hg in the terrestrial environments was estimated to be 149 years and this implies that the soil enriched with historic hg deposition from anthropogenic sources would reside in the area as a potential source for a very long time one of the advantages of swat hg is its possible application to various watersheds in many other countries that have defined land uses soil properties and land slope databases although the swat hg could have a potential for extending to other areas the model needs to be improved to better predict hg fate and transport in the watershed and biogeochemical transformation in waterbodies for example daily or monthly values of hg atmospheric deposition in each subbasin would refine the spatial variability of the atmospheric deposition rate the actual mercury methylation processes are complex and affected by dissolved organic matter temperature ph concentrations of nutrients etc which are not considered in this study an incorporation of methylmercury production considering aquatic environmental factors would be an important enhancement of the model in assessing the future behaviors of mercury in watersheds and waterbodies since the values are coupled with each other and are changing simultaneously software availability name of software swat hg developer jaehak jung contact address texas a m agrilife research blackland research and extension center temple tx 76502 usa e mail jjeong brc tamus edu year first available 2019 program language fortran availability swat hg is freely available at https bitbucket org j9849 swat hg src master the swat hg code will ultimately merged to the main swat model swat2016 swat and distributed through the swat website https swat tamu edu declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported by the basic science research program through the national research foundation of korea nrf funded by the ministry of science ict future planning 2018r1d1a1b07049757 and by the korean ministry of the environment via the environmental health action program grant no 2015001370003 authors thank kurt louis solis for creating toc art appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104644 
26051,a new generalised water resource network modelling python library pywr is presented given hydrological inflows pywr simulates customisable water allocation and operation rules throughout complex multi purpose managed water systems at each user defined time step the model uses a low level interface to existing linear programming solvers for fast priority based optimisation driven simulation the library uses an object based system for users to provide input data and record simulation outputs a novel multi scenario simulation method provides an almost 4 fold improvement in model run times and supports calculating robustness metrics across scenarios a flexible interface to specify multi objective optimisation formulations as part of a model s input file is included these features enable analysts to apply advanced water planning approaches such as robust decision making and robust optimisation to real systems the library is available under the gplv3 open source licence includes several examples and a regression test suite keywords water resource simulation network optimisation open source python multi reservoir operations decision making under deep uncertainty software availability name of software pywr description pywr is a generalised network resource allocation model written in python it aims to be fast free and extendable it provides a multi scenario simulation platform for undertaking advanced decision making under deep uncertainty studies developers j h arnott j e tomlinson language python 3 6 supported systems microsoft windows gnu linux macos licence gnu general public licence v3 source code https github com pywr pywr distribution pypi https pypi org project pywr distribution conda forge https anaconda org conda forge pywr 1 introduction planning and management of water resource systems has used simulation models for decades as its core approach to evaluating impacts of changes in supply and demand maass et al 1962 loucks et al 1981 rogers and fiering 1986 water resources management simulation models loucks et al 2005 are used to aid our existing qualitative understanding with additional quantitative information many generalised simulators have been developed by numerous authors and institutions several reviews labadie 2004 wurbs 2005 loucks et al 2005 rani and moreira 2010 sulis and sechi 2013 of these efforts are available simulation models assimilate time varying boundary conditions e g river flows operational rules and a representation of the water system to produce a prediction of state in that system over a number of time steps e g daily weekly or monthly these models represent a water resources system as a network or graph of nodes and links simulation models can be classified based on their approach to resource allocation there are those models which use a rule based or ad hoc approach to allocation an example is an algorithm that traverses the system from upstream to downstream following particular rules as it goes such as incetinkaya et al 2008 andmatrosov et al 2011 rule based models can be computationally efficient fast run times and follow complex operating procedures found in particular systems but these models can be cumbersome to develop further maintain and apply to new systems a second type of simulation model are those which use mathematical programming optimisation to assist in simulating water allocation in the resource network at each time step in this case the simulation model transforms the representation of a water system to a mathematical optimisation problem which is solved for one or more time steps at a time the optimisation allows priority based simulation of water resources where the model allocates water throughout the network in each time step to minimise some expression of a penalty e g a financial or economic cost or some empirically calibrated penalty which enables the model to simulate water allocation accurately this technology came from transport system analysis schrijver 2002 and special mathematical algorithms for network flow problems were used because they were more efficient than linear programmes jensen and barnes 1980 for example the ford fulkerson algorithm ford and fulkerson 1956 and out of kilter algorithm fulkerson 1961 were developed to solve maximum flow and minimum cost problems in flow networks respectively minimum cost network flow based water resource models such as wathnet kuczera 1992 modsim labadie 1995 realm perera et al 2005 have since been used widely for river basin planning specialised network flow algorithms have been introduced e g relaxiv bertsekas et al 1994 used by realm and non network constraints can be added to network flow algorithms by iteratively solving a network formulation eventually different types of minimum cost network flow linear programming and non linear programming have been used for this task labadie 2004 network flow algorithms have been shown to be computationally more efficient than linear programme algorithms even when including iteration sun et al 1995 however ilich 2009 demonstrates that such an approach can be problematic and should be used with caution linear programmes can accommodate additional linearised non network constraints without the use of iteration althoughilich 2008 demonstrated some limitations linear programming formulations have been used by several generalised water resources management models such as aquatool andreu et al 1996 aquator oxford scientific software 2008 calsim draper et al 2004 calvin python version dogan et al 2018 oasis randall et al 1997 riverware zagona et al 2001 sisagua barros et al 2008 mike basin jha and gupta 2003 miser fowler et al 1999 weap yates et al 2005 and others they are attractive due to the availability of generic solvers typically using the revised simplex method that apply them the utility of water resources simulation stems from its use in resource assessment and decision making analyses and its compatibility with exploratory methods such as sensitivity analysis and decision making under deep uncertainty dmdu sensitivity analysis investigates how different simulation model results can be attributed to variations of its inputs pianosi et al 2016 dmdu has developed many techniques which have been widely applied to water systems seekwakkel et al 2016b for an introduction such approaches use simulators to evaluate and or discover different infrastructure environmental or institutional interventions for example robust decision making lempert et al 2006 simulates many scenarios to help understand a planned system s vulnerabilities and multi objective robust decision making kasprzyk et al 2013 uses multi objective heuristic optimisation to find candidate sets of policies further extensions such as robust optimisation search over multiple scenarios hamarat et al 2014 kwakkel et al 2015 borgomeo et al 2016 huskova et al 2016 beh et al 2017 watson and kasprzyk 2017 eker and kwakkel 2018 sub sampled from a large uncertainty space water resources simulation is a classical tool but its use is increasing to meet demand for environmental change and intervention impact assessments common to all of these approaches is performing many system simulations under different scenarios this is typically performed by simulating scenarios individually a form of task based parallelism where both the programme model and data can change this requires the entire simulation to be recreated including opening file handles and data structures and repeated for each scenario and potentially distributed to many computers however sensitivity analyses are usually a form of data based parallelism where the programme model is singular but the data e g boundary conditions policy decisions etc change also for simulation intensive dmdu methods task based execution is problematic for two reasons 1 the calculation of sensitivity or robustness metrics herman et al 2015 kwakkel et al 2016a mcphail et al 2018 must be undertaken outside of the modelling software once all the independent scenarios are completed and 2 it is computationally inefficient in cases where the majority of data in the model is identical between two scenarios therefore the current approach to many scenario water resource simulation can be improved in this work we present a new water resources simulation system written in python pywr it uses a time stepping linear programming approach the linear programme is created from the modeller s description of a network of nodes each node type contains attributes and data which alter the linear programme solved in each time step the implementation directly incorporates a novel many scenario simulation method to enable application of advanced dmdu approaches this approach is demonstrated to be computationally more efficient than repeated single scenario execution in addition the implementation incorporates a component based system for the definition of boundary conditions model parameters and calculation of metrics this system is general and extendable by users it also contains an application programming interface api for coupling with dmdu software including the updating of model variables and calculation of robustness metrics across multiple scenarios making it suitable for use in several dmdu frameworks pywr is implemented using cross platform technology and is separated from any data management or user interface software it utilises open formats e g json hdf5 the hdf group 2018 for input output aiding integration into other software packages and processes it includes a regression test suite that is tested on microsoft windows macos and gnu linux the software is released under the general public licence gpl v3 we describe the core approach to simulation and two resource allocation algorithms in section2 the implementation details including software architecture and technology are described in section3 performance benchmarks are given in section4 followed by a discussion and conclusions in sections5 and 6 respectively we demonstrate pywr features with examples and provide some additional benchmarks in appendices 2 methods pywr is a generic dynamic modelling library for network based resource allocation models the key structure of any pywr model is a network or graph of connected nodes these connections links edges are directional and the data is stored internally as a directed graph digraph in contrast to other network models pywr does not assign information to connections except the source and destination nodes all of the data required to perform the allocation procedure is stored on the nodes and determined from the connectivity implied by the connections 2 1 simulation algorithm pywr solves time dynamic problems by looping through a prescribed number of time steps during each time step iteration a reallocation of the model s available resources is performed across the network using a mass balance approach these iterations cause state e g volume in the case of water resource problems changes in nodes that store resource e g reservoirs these state changes along with time varying boundary conditions at particular input nodes cause a dynamic allocation of resources during a model run this core time stepping algorithm follows the approach in 1 pywr defines each time step to be an integer number of days in length time follows a first order finite difference procedure where state is updated by the mean resource flow per day multiplied by the time step length in days 1 1 v i 1 v i q i i n q i o u t δ t i the solution flow q i is provided by the solution of the resource allocation algorithm in each time step the net flow q i i n q i o u t to a storage node with volume v i results in an updated storage volume v i 1 for the subsequent time step the overarching simulation algorithm 1 performs a set of procedures before and after the solution of the resource allocation algorithm it is during these phases that implementation specific calculations may be performed many of these calculations will directly or indirectly influence the values used in the resource allocation algorithm further detail on these calculations and how these values can be generically defined by the modeller is described in section3 2 2 network definition it is necessary to describe the definition of a pywr network prior to a detailed description of the resource allocation algorithm pywr models use a directed graph containing the set of nodes n core node types contained within n include input i output o and storage s as well as links l the pywr implementation includes other node types which extend the basic functionality given by these nodes several of these extension nodes internally create several of the core node types for this reason we refer to these as compound nodes we refer the reader to the implementation details in section3 2 n i o s l each node is also designated a resource domain the purpose of a resource domain is to group common nodes such that the same resource flows through paths between nodes in the same domain other domains in the network will contain their own paths crucially each domain could represent a different resource e g water and energy with exchange of one resource type for another occurring at inter domain connections e g a power plant a resource enters the network at input source nodes and exits at output sink nodes from this definition we can describe all feasible routes through the network that resources could travel by we define these routes as the set of simple paths between any combination of input and output nodes as p these paths contain no repeated nodes cycles may be formed in the network but because only simple paths with no repeated nodes are found this does not present a problem for the subsequent allocation algorithm the addition of a cycle permits more complex network structures such as bi directional links between nodes the definition of paths place some restrictions on the definition of a model s network 1 every node in n must be present in at least one path and 2 only input nodes may have an indegree of zero i e all paths must start with an input node 3 only output nodes may have an outdegree of zero i e all paths must end with an output node we also define the set of edges on the graph that connect an output to input with different resource domains as c these edges enable flow from one domain to another a single output node can be connected to multiple input nodes in other domains but an input node can only contain a single edge to an output node on a different domain 2 2 1 node properties link input and output nodes share a common set of properties minimum flow maximum flow and cost the flow properties constrain the flow on all paths at or through the node the cost property does not have to be monetised it is a general penalty term used to specify the penalties or benefits negative penalties that are incurred with net flow though the node from the user s perspective this means definition of minimum or maximum flow constraints are applied regardless of how many paths traverse a particular node the detail on how the use of these properties in the allocation procedure is given in section2 3 in addition input nodes also contain a conversion factor property this property is used to factor flow between domains and can therefore be used to simulate a different resource type in each domain for example the water and electricity domains where the conversion factor would represent power generation per volume of water 2 2 2 storage nodes storage nodes s differ from the other nodes they are a compound node containing at least one input and one output node and have a minimum and maximum volume property instead of a minimum and maximum flow property the use of a compound node means that adding a storage node also adds its associated input and output nodes to the network edges on the graph to the storage node connect to one of its child output nodes and edges from the storage node connect from one of its child input nodes fig 1 shows the underlying graph structure used for the compound storage node a consequence of this implementation is that paths are broken at storage nodes i e a single path cannot traverse a storage node because the child input and output within the storage node are not connected to one another in contrast to the other nodes a storage node retains a state variable throughout a simulation this variable is the current volume of the storage node and is updated at the end of every time step as per 1 the permitted flow on the incoming and outgoing paths of each storage node is constrained by its minimum and maximum volume storage nodes also include a cost property similar to the other node types which is applied to the underlying child input and output nodes 2 2 3 virtual node types pywr models can optionally use two special virtual node types that are not directly connected to a model s network these are aggregated and virtual storage nodes respectively the purpose of these node types is to apply constraints or calculate properties across multiple nodes that may not be connected within the network both node types act like a conventional node but rather than being connected to other nodes they are assigned other nodes called child nodes in a model the aggregated node allows minimum and or maximum flow constraints to be applied across all the child nodes this does not supersede any node s individual flow constraint it is an additional constraint this aggregated flow constraint has practical application in water systems where multiple abstractions at different locations may be constrained by a single licence in addition proportional factors can be given to enforce a particular ratio of flow between the child nodes this is useful in systems where a fixed split of resource is prescribed to downstream paths e g managed river systems or where a predefined sharing of resource sources is required virtual storage nodes include a volume state similar to a conventional storage node change in volume is a function of the total flow through all child nodes virtual storage nodes must be initialised with an initial volume along with minimum and maximum volumes optional flow factors can be given for each child node to provide a multiplier of their contribution to change in the volume of a virtual storage node virtual storage nodes allow the modeller to track the usage of for example annual abstraction licence volumes the virtual storage node s state can be used to make allocation behaviour dependent on abstraction under or over use e g by restricting future abstractions for the remainder of the licensed period we define the set of aggregated nodes with flow constraints as g n and with proportional factors as g f the set of virtual storage nodes is defined as g s we use the notation n g g to define the child nodes n in a aggregated or virtual storage node g taken from the complete set of aggregated nodes g 2 3 allocation algorithms the software architecture of pywr allows multiple resource allocation algorithms to be implemented we have implemented and tested two optimisation based resource allocation algorithms we note immediately that neither of these implementations is of a classical minimum cost network flow problem for which specialised algorithms are available for example seekuczera 1989 1992 the pywr implementation is similar but practical applications require some important additional constraints these constraints primarily arise from the aforementioned aggregated and virtual storage node types and resource domains therefore pywr uses a standard linear programme lp to determine the allocation of resource across the network at every time step a linear programme in general form is shown ineq 3 3 minimise c t x subjectto a a x b l x u where x r we implement two alternative linear programmes to demonstrate the flexibility of the approach the two approaches differ in their treatment of the variables x in the linear programme the first creates a variable column for each simple path between input and output in the network following a similar approach tocheng et al 2009 the second creates a variable column for each edge arc in the network following a similar approach toerfani et al 2013 both approaches create similar constraints to accommodate the node level properties e g minimum and maximum flow a complete description of both approaches is given in appendixa a comparison of the number of columns and rows in the respective linear programmes is given in table1 a direct comparison is made difficult by the dependency of the linear programme shape on the size and connectivity of the network we note that the path based solver will create linear programmes with fewer columns in networks that are long with several link nodes however it will create a large number of columns in more connected networks with larger numbers of input and output nodes due to the larger number of paths the edge based solver will always create a linear programme with a larger number of constraints rows due to the additional mass balance constraints however it is important to note that these constraints and their bounds do not need to be updated during a simulation finally we note that the solve time of the simplex method is a function of the size number of rows and columns and sparsity of the constraint matrix however the simplex algorithm is an example of algorithm that performs well in practice but is known to have poor worst case performance spielman and teng 2004 therefore it is difficult to make general conclusions about the solve time of the two different linear programme formulations we benchmark both linear programmes in section4 by comparing performance with randomly generated models of different sizes and connection densities 2 4 updating and solving the linear programme the simulation algorithm 1 requires the linear programme to be updated and solved at each time step during a simulation boundary conditions and internal state will vary these changes must be reflected in the allocation algorithm the algorithm and linear programme defined here place no restrictions on what may or may not be updated each time step however specific implementations may trade off arbitrary updates to the linear programme for the sake of efficiency for example the current pywr implementation does not permit changes to the matrix a between time steps there are two primary methods for solving linear programmes the most widely used is the simplex method which originated fromdantzig 1951 laterkarmarkar 1984 introduced the first of a class of algorithms now known as interior point methods which were competitive with the simplex methods in practical applications vanderbei 2014 gives a complete introduction to both the simplex and interior point methods once solved the solution x of the linear programme must be applied to the internal state of the network for each storage node this requires updating the current volume v n as per eq 1 the remaining nodes simply have their properties updated with the sum of the flow through all paths in which they are present these properties can be used in the subsequent time step s to alter the linear programme bounds or saved to a storage device 3 implementation pywr is written in the open source python vanrossum and drakejr 1995 programming language the pywr library provides the necessary data structures and routines to create execute and analyse models of resource systems in this section we describe the implementation details and core functionality of the pywr library while the core approach centres around the resource allocation algorithm described in section2 it is the software library implementation that contributes an innovative approach to water resource modelling specifically this is the approach to multi scenario simulation and integration with modern dmdu methods and component based system for defining data input and output this section is organised as follows the overall architectural design and technology of the library is described in section3 1 section3 2 details the implementation of the allocation algorithms using glpk the main node types and their management as a network are described in sections3 3 and 3 4 respectively the data input output system is explained in section3 5 section3 6 details the implementation of the approach to efficiency multiple scenario simulation finally section3 7 describes extensions to the main library including links to multi objective evolutionary algorithms 3 1 software architecture and technology the pywr implementation uses a modified version of the time stepping algorithm 1 this modified algorithm is given in 2 the primary modification is the evaluation of multiple scenarios during a single time step for example if a model is used to simulate 100 plausible futures the first time step including resource allocation of all futures is performed before moving to the next time step more detail on this approach is described in section3 6 we also define api methods setup reset before after and finish on all components of a model the library is structured as a standard python package it utilises an object oriented programming approach allowing users to extend its base functionality as required the library has dependencies on several other python packages including numpy oliphant 2006 scipy jones et al 2001 pandas mckinney 2010 pytables pytables developers team 2002 matplotlib hunter 2007 and networkx hagberg et al 2008 pywr was designed to be easily integrated into dmdu analyses lempert et al 2006 hallegatte et al 2012 matrosov et al 2013 and multi objective evolutionary algorithms moea reed et al 2013 maier et al 2014 2019 analyses however the standard python interpreter typically referred to as cpython does not produce efficient code when compared with statically typed compiled languages such as c and fortran we overcome this limitation by the extensive use of cython behnel et al 2011 cython is a tool for generating python c extensions users of cython write code in a superset of python s syntax which cython interprets to generate c this c code is compiled to an extension library which can then be imported in python the benefit of cython is that a programmer can retain much of the flexibility of python while also using the additional cython syntax to make performance critical functions more efficient determining which parts of pywr constitute core functionality and therefore requiring the use of cython was discovered using benchmarking and profiling tools by using cython rather than a pure c or fortran library we remain compatible with python this provides users with significant flexibility to extend the core functionality with extensions written in python providing generalised approach to simulating resource networks with bespoke rules and behaviours specific to each network as users write more complex extensions the calculations may become a bottleneck in model run time users may then migrate their extensions into cython to improve computational performance 3 2 solving the resource allocation algorithm the pywr implementations utilises the gnu linear programming kit glpk makhorin 2018 to solve the linear programme described in section2 glpk is an open source linear programming library that implements several linear programming and related algorithms including primary and dual simplex methods and a primary dual interior point method glpk provides a library that is written in the c programming language and is available on several platforms including microsoft windows and gnu linux pywr utilises cython to directly interface with the glpk c library this approach contrasts with several approaches to linear programming in python typically an additional library such as pyomo hart et al 2017 2011 or high level modelling framework such as gams corporation 2013 is used to provide a common framework to construct and solve optimisation models these approaches provide a high level interface and choice of solver at the expense of computational efficiency and would add significant overhead to the time stepping pywr algorithm 2 we demonstrate this in our specific case by the benchmarking of a pyomo based implementation of one of the resource allocation algorithms in appendixc the direct interface to the glpk c library requires managing the creation update and solution of the linear programme data structures for efficiency purposes the pywr algorithm permits changes to the objective coefficients c p and constraints bounds a and b changes to the matrix a are not permitted between time steps therefore the matrix a can be created once at the beginning of the simulation and remains a constant this approach allows the main glpk problem data structure to be created during the setup phase at the beginning of a simulation all calls to glp create prob glp add rows glp add cols and set mat row are undertaken during this initialisation phase outside of the main time step loop during the update of the linear programme there are only calls to set obj coef and set row bounds prior to the use of the simplex method the consequence of this decision is that only model parameters that effect row bounds or objective coefficients can be altered during a simulation this has two major implications first it requires the connectivity of the network remain constant during a simulation specific routes through the network can however can be effectively enabled disabled using dynamic constraints to maximum flow second it implies conversion factors for cross domain connections and flow factors for both aggregated and virtual storage nodes remain constant during a simulation we note also that the library is structured such that different methods or algorithms can be used to solve the linear programme at every time step indeed one could implement an allocation routine that does not use a linear programme currently glpk makhorin 2018 and lpsolve berkelaar et al 2016 are implemented in the pywr library 3 3 fundamental node types the core components that a modeller works with in pywr are the various types of node object the modeller will define and assign attributes to multiple nodes of different types during the development of a pywr model instance which are then connected together to form a network the library contains a core set of node types that map directly to those described in section2 while providing the four types of standard node link input output and storage and two aggregated node types aggregated and virtual storage the library also provides a number of variations of these types for convenience to the modeller these node types are not included in the methodology section because their implementation is a subset of the general behaviour provided by the standard node types for example a generic catchment node that is a special case of an input node which uses the same value for maximum and minimum flow 3 3 1 piecewise link in certain situations it may be desirable to define a non linear cost function at a particular node for example this might represent a desire to allocate a particular minimum flow of resource through a node with additional flow above the threshold being permitted but providing no benefit a piecewiselink node is included in the library that allows a piecewise objective function minimum and or maximum flow constraint to be applied internally the piecewise link is implemented as multiple link nodes with different costs and maximum flow properties connected in parallel following the approach oflund and ferreira 1996 fig 2 shows how this is implemented for the case a of a minimum flow threshold with a lower cost flow is not explicitly forced through the path associated with the minimum flow threshold but the lower cost for this path will ensure this path is preferred when the linear programme is solved the piecewise link acts as a normal node and the internal representation is not exposed the node reports the sum of flow through all its internal link nodes and therefore its entire structure is not shown to the user 3 4 network structure the core of the pywr library depends on the networkx hagberg et al 2008 library to store and manipulate the directed network structure of a model networkx provides many advanced algorithms to create and manipulate complex networks its primary function in pywr is to manage a model s connectivity and determine the number of feasible flow routes the latter is achieved by using a simple paths algorithm derived fromsedgewick 2001 a secondary use of networkx in pywr is to record the dependency tree of model components see section3 5 the pywr library provides a convenient api for connecting the various model nodes together connection is directional requiring the user to specify the upstream and downstream nodes there is no restriction on the creation of loops but bi directional connections must be created using one connection in each direction the use of bi directional links overcomes some of the issues associated with rule based models that must progress from upstream to downstream the definition of upstream and downstream is difficult when loops are present and therefore may require iteration of rule based algorithms 3 5 model data and output components while much of the above discussion has focused about the technical details of the core parts of the pywr library a useful computational library must provide a convenient and helpful interface to create models in a pywr model the specification of data and output is defined through two types of component parameters and recorders both components share the same parent class definition complex behaviour in pywr is created through the use of multiple inter dependent components for example the summation of multiple parameters or the transformation of one parameter value to another via some user defined function first we describe the common functionality and integration into the pywr algorithm 2 in the following sections we given more detail about the specific roles of parameters and recorders themselves 3 5 1 components and dependencies the main computational algorithm 2 deployed by pywr performs an outer loop over the time steps in the simulation during each iteration of this loop component objects have the opportunity to perform any required internal calculations before and after the allocation algorithm is solved the api specifically defines before and after methods as written in algorithm 2 for all components the default implementation in the parent component from which all other components inherit performs no operations but allows specific component subclasses to define their own behaviour in general arbitrary interdependencies between components could exist the order in which the components perform the internal calculations is therefore important for example a component dependent on the value of another must be updated after its dependency pywr solves this problem using parent and child relationships between components upon creation each component must register any relationship it has with other components typically this takes place during initialisation where the dependent component s is passed to the parent s constructor method the parent must then ensure it registers the dependent component object as a child once the components interdependencies are defined they remain fixed for an entire simulation during the model setup phase the component relationships are inspected to construct a directed graph of the dependencies this graph is initialised with a root node to which all components are a child this graph encodes the component dependencies and is inspected to determine the appropriate order to perform component calculations the order is determined by performing a depth first search using the networkx dfs postorder nodes function this algorithm is based on a depth first search algorithm byeppstein 2004 if this algorithm detects any circular dependencies the model is unable to run and an exception is raised if the algorithm is successful the traversal of the dependency graph is flattened and reversed this order is cached for the entire simulation such that it does not need to be recalculated each time step during each update of the main algorithm 2 e g before and after the components methods are called in this order the major benefit of this approach is that it ensures each component is updated only once per time step while allowing multiple components to depend on the same child component 3 5 2 input data parameters the purpose of parameter objects is to provide data to the model this is achieved through a common api to which all parameters must adhere this api extends the common functionality of components with additional methods that should compute the value to be used in the model and allow retrieval of that value by parent components as part of an object oriented structure each new parameter type must implement a method value which computes the current value of that parameter instance this method is called by the overarching algorithm for every scenario see 3 6 in every time step the current time step and or scenario information can be used as part of the calculation if required the standard pywr library includes many useful parameters that can be used to provide complex data input of particular interest is aggregatedparameter which can undertaken a user defined aggregation across multiple other parameters this is achieved using the component dependency previously described many other parameter types require other values and therefore parameters for their calculations in water resource modelling it is common to implement a dynamic hedging rule on demands yeh 1985 russell and campbell 1996 oliveira and loucks 1997 there are many different forms of hedging rule but a simple example is given in 4 the demand d 1 is factored based on the volume v 1 of a reservoir the factor α is determined by a comparison of the volume v 1 against 1 or more hedging curves f i the factors α i the hedging curve functions f i and the comparison operator i e the if then else statement are all implemented as separate parameter instances for example an aggregatedparameter would perform the multiplication of α with d 1 and controlcurveparameter would implement the if then else conditions that determine whether α 1 α 2 or α 3 is used 4 d ˆ 1 t α d 1 where α α 1 if v 1 f 1 v 1 t α 2 elseif v 1 f 2 v 1 t α 3 otherwise this component based approach has two advantages 1 the components can be shared and reused by multiple components and nodes in a model and 2 individual components can be changed to different types transparently for example the hedging curve rule f 1 ineq 4 might be a constant time varying profile or some custom function based on multiple reservoirs volumes or other information the modeller can easily change parameterisation of their model by changing component types as opposed to changing node types a complex example of the general approach given ineq 4 can be found in the examples given in the appendix of this paper 3 5 3 output data recorders in contrast to parameters recorder components are used to observe or save simulation results by default a pywr model will not include any recorders and therefore not save any information recorders extend the parent component api by adding methods to return a single value per simulation see section3 6 and a single value aggregated across all simulations individual recorder types are responsible for calculating the per scenario values in the values method a common aggregation function is provided that is user configurable it includes a default set of aggregation methods sum min max mean median and product but can also apply a user defined function a range of recorder types are provided in the standard pywr library these include the ability to save timeseries from each node to hdf5 or csv format other recorders are useful for tracking certain performance metrics in a given simulation for example the total or mean flow that passes through a particular node 3 6 scenarios in many contexts there is a need to perform multiple simulations of a model with variations upon the input parameters for example when performing a sensitivity or scenario analysis pianosi et al 2016 more recently robustness metrics coupled with multi objective optimisation have become popular in water resource planning the requirements for the length of simulations the number of scenarios and number of function evaluations is increasing due to a desire to better resolve risk and uncertainty and design more complex adaptive planning policies pywr has integrated support for multiple simulations a core innovation and contribution of this simulation platform this approach shown in algorithm 2 differs from a traditional sensitivity analysis approach that would rerun the entire simulation instead pywr requires the definition of multiple sources of uncertainty each with a set of scenarios the full set of scenarios to simulate can be the product of all sets or a sub sample thereof for each scenario to be run internal state variables e g storage node volumes v n s are created the simulation proceeds by solving the resource allocation algorithm for each scenario within each time step this approach does not use multi processing or any kind of parallel computing this approach increases the memory requirement of the library by requiring state variables for each scenario to be stored simultaneously however it takes advantage of the commonalities across the scenarios typically the parameters that vary between scenarios are few compared to the complexity of an entire model therefore in all simulations the model structure and most of the model s properties are identical we have included an assessment of the additional memory consumption used when running multiple scenarios in 4 2 the parameter system employed by pywr reflects this approach some parameter types allow the modeller to specify a different value per scenario for a given uncertainty dimension others simply return the same value in all scenarios parameters that operate on other parameter values correctly operate on a per scenario basis it is up to the modeller when to use parameter types that vary their return value by scenario in appendixb we show an example of a demand model that uses multiple parameter objects one parameter is used to create different baseline demands the performance improvement of using this approach is demonstrated in section4 1 3 7 extensions and linking pywr to other libraries pywr components give a user the ability add functionality for their own purposes this might include the integration with other models to define certain values for example a crop model could be used to define the demand at an agricultural node provided such a model could be written or accessed from python it could be integrated with a custom parameter object to directly provide data to a simulation the real benefit of this approach is when such custom parameter s values are dependent on the pywr model s state for example demand that is a complex function of current reservoir storage the modeller is able to write an arbitrary relationship between the model s state and their parameter s output 3 7 1 optimisation simulation based optimisation frameworks such as mordm kasprzyk et al 2013 and moro hamarat et al 2014 have become popular in recent years these approaches couple water resource simulators global optimisation algorithms in an effort to find efficient values for key input parameters decision variables these parameters could represent control or hedging curves release rates and or investment decisions objectives and constraints are metrics computed by the simulation model repeated evaluation of different decision variable values returns the corresponding objective and constraint metrics allowing an exploration of the decision space by heuristic search algorithms the pywr component system provides an api that can be used by optimisation algorithms parameter instances can be flagged as decision variables is variable true and provide an api to determine the number of real and integer variables that parametrise that particular type set the values of the internal parameterisation and retrieve the values of the internal parameterisation this approach allows complex types requiring multiple decision variables to be incorporated easily in to an optimisation formulation in a similar manner recorder instances be flagged as an objective and or constraint is constraint true objective direction is set by applying a string e g is objective minimise this api allows an external algorithm or programme to determine the number and type of variables in any given model number of constraints and the number and direction of objectives the parameter variables can be updated as the first stage of an optimisation loop which repeatedly reruns the simulation after a simulation is complete the constraint and objective values can be determined from the existing recorder api for calculating the aggregate value over all scenarios therefore this api allows multi objective constrained robust optimisation ro ben tal et al 2009 to be undertaken a key contribution of pywr is that through the component subsystem variables constraints and or objectives can be easily altered either their representation change e g from flat to time varying hedging curves or their inclusion or not in the optimisation formulation i e enable disable decision variables or objectives optimisation wrappers have been written for four libraries borg hadka and reed 2013 platypus opt hadka 2016 inspyred garrett 2012 and pygmo biscani et al 2019 an example of coupling a simple pywr model to a two objective control curve problem is given in appendixb 3 3 7 2 data management and user interface pywr models can be defined using python via a conventional application or script however models may also be defined using a javascript object notation json based file format an entire model including nodes edges and components may be defined as a standalone file json was chosen for its common use in modern web based services and communication parsers and writers for json are commonly available in many programming languages including python the format is designed to be human readable and less verbose than xml as a python library pywr includes no user interface or data or run management sub system users are required to write and edit models using the json file format and provide large data via external files however as an open source python library pywr can be easily integrated in to external data management and user interface systems this has been successfully achieved with the hydraknox et al 2019 data management system via an interface application that supports import export and execution of pywr models model data and results are stored in a database transparent to the modeller such data management systems facilitate the use of other technologies to interact with model data for example web based user interface applications and simulation result viewers 4 benchmarking in this section we demonstrate the efficiency of the multi scenario simulation algorithm and compare two different allocation formulations the examples and benchmarks used to produce these results are available in the pywr source code repository additional benchmarks are given in appendixc these demonstrate the efficiency of the implementation by showing that the overhead of running a pywr model is small and also that significant improvements to performance have been achieved by the progressive use of cython over the development of the project continuous benchmarking 4 1 speed up of multiple scenarios in this section we demonstrate the performance characteristics of the approach to multiple scenario simulation described in section3 6 this approach enables significant efficiencies in model execution by taking advantage of the shared structure of linear programme across all of the scenarios the overhead involved in initialising and stepping through time steps is also shared by all scenarios this method means that the linear programme bounds and objective function are updated for each scenario in each time step and solved again therefore eliminating the need to store or create multiple copies of the linear programme for each scenario in addition when the simplex method is used the previous optimal basis can be used as a warm start for the new solve potentially reducing the number of iterations required to reach the new optimal solution the approach is shown in algorithm 2 we use the asv benchmarking system and 3 model benchmarks with a varying number of scenarios to show the speed up in run time the simplethames benchmark utilises a different catchment inflow timeseries in each of the scenarios the speed up is calculated by dividing the total run time of the multiple scenario simulations by the expected run time assuming separate simulations of a single scenario each of the benchmarks show similar speed up characteristics approaching 4 times the equivalent single scenario run time with 160 scenarios see fig 3 4 2 memory benchmarking in addition to the timing benchmarks we have also undertaken some analysis of memory consumption we use the same asv system described in the previous section to undertake an analysis of memory usage when running multiple scenarios these benchmarks use the same models as used for the timing benchmarks in fig 4 we show the total peak memory consumption in megabytes and peak memory consumption per scenario all models show a close to linear increase in memory consumption as the number of scenarios increases we note that the increase per scenario is small relative to the memory requirement for running a single scenario the peak memory consumption per scenario shows that running multiple scenarios in one simulation is more efficient than running each scenario in separate parallel processes that do not share memory we note that these benchmarks are relatively simple memory consumption in more complex models could be highly dependent on the type of components inputs and outputs used in the model 4 3 comparison of allocation algorithm performance two linear programme based algorithms have been implemented using glpk makhorin 2018 and are described in appendixa we use the asv benchmarking system to compare these two algorithms on the randomly generated models results are presented in fig 5 for the two algorithms with increasing size of model these models have been generated with a connection density of 5 we observe that the path based algorithm has marginally better performance than the edge based algorithm for a smaller number of nodes however it suffers a significant reduction in performance with a larger number of nodes this is due to the large number of feasible routes and hence linear programme variables when using the path based algorithm with a highly connected and large model 5 discussion 5 1 limitations we have presented a computational library for water resource systems analysis pywr does not include a graphical user interface to aid users with creating and calibrating models instead users must either utilise the python api directly or write a model definition file in json format neither of these approaches are attractive for a water resource systems analyst or stakeholder untrained in either method the creation of a graphical user interface to aid the construction of pywr models would mitigate this limitation pywr uses an optimisation based resource allocation procedure while the library can support multiple formulations the overall algorithm remains the same the use of optimisation based allocation is not unique but is also not without limitations ilich 2008 has outlined several of these limitations for similar models the analyst as with any modelling system must be aware of the overall formulation and its limitations models must be checked calibrated and tested to ensure the system parameterisation and behaviour is as expected another limitation with the pywr formulation is the fixed time step and the use of euler s method for time integration while euler s method is simple to implement it can suffer from instability and accumulate errors if the timestep is not appropriate in pywr this would manifest through errors in the volume predictions of storage nodes pywr currently has a lower limit of a 1 day for the time step which could be too large in some circumstances in addition the time step must be defined as an integer number of days therefore it is not possible to define a time step that aligns exactly with for example the weeks or months of the year however it is possible to resample the output timeseries to align with a desired frequency that is different to the pywr time step e g run a daily simulation but resample results to monthly means 5 2 benefits despite the limitations pywr has proved a useful modelling framework with several benefits the approach to multiple scenario simulation components and links to several optimisation frameworks create an efficient modelling process for both mordm and ro studies we have demonstrated a roughly 4 times speed up of the multiple scenario simulation strategy this speed up is a result of the software implementation not parallel processing taking advantage of 1 the common data e g the network structure and boundary conditions between the scenarios and 2 undertaking the time stepping procedure only once for many scenarios the speed up result is particularly important for ro studies where many function evaluations over multiple scenarios are required the component based system for specifying both model parameters and outputs is very flexible for optimisation studies the output can be reduced to only that required to compute the objective values thereby reducing the memory and computational overhead of a simulation and also removing any requirement to save data to files in addition the component system allows for easy alteration and testing of the dmdu formulation for example the addition or removal of decision variables and or objectives can be undertaken via boolean flags in the model definition file the type of decision variable and or objective can also be easily changed independently of the network definition an example of this is provided in the appendixb 3 where the parameterisation of a control curve is tested under optimisation these benefits are coupled with in built support for 3 different moea libraries which allow modellers to easily perform multi objective studies without necessarily writing any new code pywr is designed as a computational library and is consequentially decoupled from a graphical user interface or data management system this modular architecture allows the library to be deployed easily to high performance and cloud computing environments as a python library it is cross platform thereby allowing analysts to develop models on their local computers in one operating system e g microsoft windows but run large simulation studies in another e g gnu linux the modular design also allows pywr to be linked to larger applications possibly running multiple models for example a multi model simulation framework such as pynsim knox et al 2018 pywr is available under the general public licence gpl and continues to be developed free and open source software foss licences allow the creation and shared contributions of a community of modellers historically few water resource models are available under foss licences many have been available under academic use only licences restricting the commercial use and adoption of such models pywr is managed via github by the original developers and welcomes contributions from researchers and users 5 3 future work development of pywr is ongoing and driven by its application to specific case study applications further research into the development of water resource system modelling and pywr itself can focus on its application to large scale systems and multi objective optimisation problems the library can expand its features in this area by providing modellers with additional tools to work on such problems in addition more complex parameter types can be envisaged that incorporate more advanced operating rules e g multi reservoir radial basis functions foresight and adaptive planning more complex recorder types could also compute more sophisticated objectives including different robustness metrics e g those discussed byherman et al 2015 kwakkel et al 2016a mcphail et al 2018 some water management models particularly in situations with water markets require tracking transactions water exchanges in water resource networks erfani et al 2013 2014 although not yet attempted with pywr this could be potentially be achieved by creating a trade parameter that is shared by two nodes representing different water users depending on the formulation trades could be optimised via the allocation algorithm within time step or by global optimisation we have benchmarked pywr against itself but recognise that we have not undertaken a comprehensive comparison of this tool against existing similar tools in future work we aim to provide a complete comparison of these tools in terms of their computational efficiency features and accuracy additional computational efficiencies may be possible through further optimisation and exploitation of multi scenario simulation such simulations are trivially parallel and can exploit parallel computing by dividing the scenario space between individual i e multiple instruction multiple data mimd with the existing implementation however further efficiencies may be possible by exploiting single instruction multiple data simd approaches such as those found in modern graphics processing units gpu when running a very large number of scenarios pywr is a computational library and subsequently does incorporate a graphical user interface gui this is a conscious design decision separating the computational model from the interface user interfaces could become available to assist with constructing maintaining and sharing water resource network models efforts are in process pywr s modular design means diverse technology and development choices will be available for user interaction 6 conclusion in this work we contribute to the field of water resource systems analysis and planning by providing a modern and open source computational library pywr implements a time stepping optimisation based resource allocation procedure using a direct interface to linear programming solvers this approach coupled with the use of python and cython provides a fast yet extendable software library in addition pywr supports multiple scenario simulation directly and exploits the identical network structure and shared data that typically exists when undertaking multiple simulations this efficiency is demonstrated through benchmarking more generally pywr provides a component based and extendable system of parameters and recorders that allow users to define complex boundary conditions and performance metrics respectively the pywr library includes many useful components as standard but users can easily develop their own using python s class system we also provide api support for integration of pywr models into robust optimisation studies where an external algorithm can update parameter values perform a simulation and gather objective values from recorder outputs examples of these approaches are provided in appendixb of this paper declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements uk research and innovation ukri provided support through the global challenge research fund gcrf futuredams project es p011373 1 and the university of manchester united kingdom funded james tomlinson s doctoral studies any opinions findings and conclusions or recommendations expressed in this material are those of the authors but not necessarily the funders both joshua arnott and james tomlinson were employees at atkins when the open source project began appendix a allocation algorithms a 1 path based allocation algorithm in this formulation variables or columns in the linear programme x are created for each path p p these variables represent the flow along each path a 1 x x p p p a 1 1 node constraints constraints are added to the linear programme for each of the nodes in the network a 2 a n a f a d a s a n a f 0 a s b n b f 0 b s where a f a d and a s are sets of constraints associated with minimum and maximum flow flow between domains and minimum and maximum storage respectively each node can be present in multiple paths and therefore the total flow at a given node is the sum of all paths containing that node a 3 a f n p p n p x p a f n a f n b f n forall n n where a f n and b f n are the minimum and maximum flow for the node n these values are determined by the properties of each node the set of edges c that define the inter domain connections is used to define a constraint for each output node a connection links one output node to one input node a 4 a d p p o c p x p p p i c p x p f i c 0 0 forall o c i c c where o c and i c are the output and input node for each of the inter domain connections respectively for each input f i c is the conversion factor the equality bounds on these constraints ensure mass conservation between domains subject to any unit conversion storage nodes cannot release more resource than the incoming flow plus the available volume and cannot store more resource than the current spare capacity less any outgoing flow i e the net flow to a storage node cannot violate the minimum and maximum volume constraints a 5 a s n p p n p forall n o n x p p p n p forall n i n x p a s n v m i n n v n δ t b s n v m a x n v n δ t a s n a s n b s n forall n s where o n and i n are the set of output and input nodes associated with the storage node n a s n is the net inflow to the storage node n net inflow is constrained by minimum and maximum volume v m i n n and v m a x n respectively determined from the node s properties these limits combined with the current volume v n determine the lower and upper bounds a s n and b s n respectively a 1 2 virtual node constraints each of the aggregated nodes contributes constraints to the linear programme as follows a 6 a a a a f a a z a v s a a a a f 0 a v s b a b a f 0 b v s aggregated nodes with minimum and or maximum flow limits apply constraints to the linear programme the constraint is applied to the sum of flow across all paths in which the child nodes n of the aggregated node g are present a 7 a a f g p p n p forall n g x p a a f g a a f g b a f g forall g g n aggregated nodes with optional factors apply constraints to the linear programme for each child node n beyond the first a new constraint is added that enforces the ratio of flow between n and the first child node n 0 the factor is normalised by the first child node s factor a 8 p p n 0 p x p f 0 f i p p n i p x p a a z 0 for n i g g f and i 0 virtual storage nodes with minimum and or maximum volume limits apply constraints to the linear programme the constraint is applied to the net change in volume of the virtual storage node based on the flow through all child nodes n the convention here is that all child nodes contribute to net flow out of the virtual storage node the contribution is factored by f n for each node a 9 a v s g p p n p forall n g f n x p a v s g v m i n v g t δ t b v s g v m a x v g t δ t a v s g a v s g b v s g forall g g s a 1 3 objective function the linear programme is defined to minimise the cost of flow through the network costs are accrued to routes by summing up all the penalty values cost properties for each node c n in each path p input and output nodes associated with storage nodes defer their cost property to that of the parent storage node in this situation input nodes return the negative of their parent storage node cost the purpose of this is to have the storage node cost applied to the net inflow a s n therefore the cost applied to the storage node can be used as a penalty or benefit if negative to increasing the volume of stored resource a 10 c p n p c n forall p p minimise p p c p x p the choice of penalty values to use in any implementation is entirely up to the user we make no a priori assumptions regarding the cost values and their magnitude the value of c n for each node is determined from the node s properties a 2 edge based allocation algorithm a 2 1 node constraints constraints are added to the linear programme for each of the nodes in the network a 11 a n a m a f a d a s a n 0 a f 0 a s b n 0 b f 0 b s where a m a f a d and a s are sets of constraints associated with mass balance minimum and maximum flow flow between domains and minimum and maximum storage respectively mass balance constraints ensure that the flow entering and leaving link nodes is identical this constraint is based in the entering edges e i n and leaving edges e o n for each node n a 12 a m n e e i n x e e e o n x e forall n n the minimum and maximum flow constraints are applied to the set of edges e n associated with node n for output nodes e n is the set of edges terminating at node n for all other nodes e n is the set of edges starting at node n a 13 a f n e e n x e a f n a f n b f n forall n n where a f n and b f n are the minimum and maximum flow for the node n these values are determined by the properties of each node the set of edges c that define the inter domain connections is used to define a constraint for each output node a connection links one output node to one input node a 14 a d e e i o c x e e e o i c x e f i c 0 0 forall o c i c c where o c and i c are the output and input node for each of the inter domain connections respectively for each input f i c is the conversion factor the equality bounds on these constraints ensure mass conservation between domains subject to any unit conversion storage nodes cannot release more resource than the incoming flow plus the available volume and cannot store more resource than the current spare capacity less any outgoing flow i e the net flow to a storage node cannot violate the minimum and maximum volume constraints a 15 a s n e e i n forall n o n x e e e o n forall n i n x e a s n v m i n n v n δ t b s n v m a x n v n δ t a s n a s n b s n forall n s where o n and i n are the set of output and input nodes associated with the storage node n a s n is the net inflow to the storage node n net inflow is constrained by minimum and maximum volume v m i n n and v m a x n respectively determined from the node s properties these limits combined with the current volume v n determine the lower and upper bounds a s n and b s n respectively a 2 2 virtual node constraints each of the aggregated nodes contributes constraints to the linear programme as follows a 16 a a a a f a a z a v s a a a a f 0 a v s b a b a f 0 b v s aggregated nodes with minimum and or maximum flow limits apply constraints to the linear programme the constraint is applied to the sum of flow across all paths in which the child nodes n of the aggregated node g are present a 17 a a f g e e n forall n g x e a a f g a a f g b a f g forall g g n aggregated nodes with optional factors apply constraints to the linear programme for each child node n beyond the first a new constraint is added that enforces the ratio of flow between n and the first child node n 0 the factor is normalised by the first child node s factor a 18 p p n 0 p x p f 0 f i p p n i p x p a a z 0 for n i g g f and i 0 virtual storage nodes with minimum and or maximum volume limits apply constraints to the linear programme the constraint is applied to the net change in volume of the virtual storage node based on the flow through all child nodes n the convention here is that all child nodes contribute to net flow out of the virtual storage node the contribution is factored by f n for each node a 19 a v s g e e n forall n g f n x e a v s g v m i n v g t δ t b v s g v m a x v g t δ t a v s g a v s g b v s g forall g g s a 2 3 objective function the linear programme is defined to minimise the cost of flow through the network costs are accrued to edges by summing up all the penalty values cost properties associated with node c n edges associated with link nodes have their costs factored to avoid double counting the indegree and outdegree edges input and output nodes associated with storage nodes defer their cost property to that of the parent storage node in this situation input nodes return the negative of their parent storage node cost the purpose of this is to have the storage node cost applied to the net inflow a s n therefore the cost applied to the storage node can be used as a penalty or benefit if negative to increasing the volume of stored resource a 20 c e n e f n c n forall e e where f n 0 5 if n is link 1 0 otherwise minimise e e c e x e the choice of penalty values to use in any implementation is entirely up to the user we make no a priori assumptions regarding the cost values and their magnitude the value of c n for each node is determined from the node s properties appendix b examples the pywr software repository contains an extensive regression test suite with over 300 unit tests these tests are run automatically by the continuous integration ci system upon changes to the software new features should be accompanied by appropriate tests ensuring the correct behaviour now and in the future while unit tests are good practice in any software project they do not easily demonstrate the complete functionality of a project to this end we have included a number of example models that are distributed with the source code in this section we use those examples to demonstrate pywr s core functionality and outputs we note that while these models are more complex than typical unit tests they remain relatively simple when compared with real systems b 1 system with analytical solution the most basic example model demonstrates the ability of pywr to compute a numerical approximation to a simple system for which an analytical solution is known eq b 1 defines a simple storage system in which there is a time varying inflow q i n and constant outflow q o u t this system can be modelled using three nodes in pywr an input and output node are connected upstream and downstream of a storage node respectively the output node is given a constant maximum flow constraint equal to q o u t and a negative cost the input node has minimum and maximum i e an equality constraint flow constraint to equal to q i n b 1 δ v δ t q i n q o u t q i n s 1 cos ω t q o u t s the frequency ω of the inflow is set to 2 π 366 with the unit of t in days the example system uses s 100 mm 3 day the storage node has a minimum capacity of zero maximum capacity of 1 e 6 mm 3 the initial volume v 0 is defined as 2 s ω these values ensure the storage node does not reach either minimum or maximum capacity during the simulation the solution to b 1 is given in b 2 b 2 v s sin ω t ω v 0 a comparison of the analytical result b 1 with the numerical pywr prediction is given in fig b 6 the simulation is performed for a single year a comparison is made between the simulated and analytical volumes absolute volumetric error is displayed in the lower panel the numerical results from pywr match very closely with the analytical solution b 2 simple system including storage the following example water system is based on a single catchment with a pumped storage reservoir it is loosely based on the supply system for london and utilises synthetic flow data derived from measured flow of the river thames the centre for ecology hydrology 2018 a schematic of the model is given in fig b 7 the model includes multiple demand scenarios and a dynamic demand calculation that imposes restrictions during poor reservoir states this dynamic behaviour is accomplished through the use of multiple parameter objects that implement the reservoir control curves the link to current reservoir volume and the look up of the current restriction factor to apply this approach is a common way of modelling demand side drought restrictions in the united kingdom b 3 d t α β 1 7 where β istakenfromtableb 2foreachscenario α 0 9 γ w ifwinter 1 2 γ s otherwise and γ w 1 00 if v t v m a x 0 8 0 95 elseif v t v m a x 0 5 0 80 otherwise γ s 1 00 if v t v m a x 0 8 0 90 elseif v t v m a x 0 5 0 75 otherwise the model includes a single catchment boundary condition where inflow is specified a downstream minimum flow requirement of 0 6 mm 3 day controls the pumped abstraction to the reservoir the maximum abstraction rate is 3 5 mm 3 day a baseline demand of 1 7 mm 3 day is combined with a monthly profile that applies 90 of baseline demand in winter september april and 120 of baseline demand in summer may august two flat control curves are defined as 80 and 50 of relative storage in the reservoir respectively the demand model applies 100 of demand when above both curves 95 winter or 90 summer of demand when between the two curves and 80 winter or 75 summer of demand when below both curves the variation of demand restrictions with time of year reflects the higher efficacy of such measures e g hose pipe bans during summer periods the complete demand model is given in b 3 the model is also setup to execute 5 scenarios simultaneously to for example reflect uncertainty the scenarios are defined tableb 2 during simulation all 5 scenarios are executed with the value of β in b 3 set appropriately for each scenario from tableb 2 the remainder of the model definition is unchanged and unaware of the scenarios effecting other parameters in the model creating this single model but with a single input value i e parameter object varying per scenario is a parsimonious way to perform scenario analysis without full copies of the entire model definition the model is executed for a 100 year period using a 7 day time step the stochastic flow data has been given fictional dates starting from the year 2100 the performance of the reservoir across the simulation is shown in fig b 8 the left panel shows a timeseries of the first 25years of simulation the right panel shows the storage duration curve across the full simulation for all demand scenarios this example quickly shows the effect of varying demand on the reservoir performance due to it being an illustrative example of pywr s functionality we do not explore the results in any further detail here this example includes dynamic behaviour through the use of pywr s parameters the model includes 7 nodes connected using 6 edges and 11 parameters a model simulation of 100years using a 7 day time step requires 5218 timesteps across 5 scenarios totalling 25 640 linear programme updates and solves fig b 9 shows the key performance statistics of a simulation without the writing of a full hdf5 output file the timing of which can be difficult to measure due to disk buffering we note however that writing a complete output file of all timeseries does have an adverse effect on overall execution time all results are based on a single run on the author s workstation intel i5 3570k the purely computational elements shown in fig b 9 demonstrate that not a single part of simulation algorithm is dominating execution time total simulation time is 0 82s giving a simulation speed of 31889 timesteps per second the calculation of the dynamic demand model b 3 via parameters is less than 7 time taken before of the simulation time solving the linear programme i e glpk s simplex routine is less than 20 solver lp solve of the simulation time the complete interaction with glpk i e updating row bounds updating objective coefficients solving and applying the results takes less than 48 of the simulation time other solver overhead involves iterating through the scenarios and does not directly interact with glpk s routines b 3 two reservoir system with optimisation the following example see fig b 10 for a schematic demonstrates the coupling of a pywr to a multi objective evolutionary algorithm moea the moea is used to optimise two competing performance objectives and discover an approximate pareto frontier the two objectives are defined in tableb 3 the model exposes a single pywr parameter as the variables in the optimisation problem this parameter represents a reservoir control curve which controls the operation of a transfer by changing the shape of the control curve the operation of the transfer pump is changed and thus the performance metrics are altered this example includes no constraints the users has the choice of using two different types of parameter a monthly profile or an annual harmonic profile each parameter type exposes a different number of variables to the moea the monthly profile parameter is defined by a single value for each month when used as a variable during an optimisation this parameter requires 12 separate floating point variables in contrast the number of variables required by the annual harmonic parameter is dependent on the number of harmonic frequencies n included the annual harmonic function is given in b 4 the parameter exposes the variables a a n and ϕ n to the moea therefore the total number variables is 1 2 n in this example two harmonics are used i e n 2 b 4 f t a n 1 n a n cos 2 π n t 365 ϕ n this example demonstrate the use of pywr s optimisation functionality by allowing the creation of two models identical except for the control curve parameter used the moea wrappers provided in pywr allow fast testing and modification of the optimisation formulation by dynamically reading the problem size number of variables constraints and objectives from the model itself this example presents a comparison of the two different control curve types solved using the same simulation model and optimisation algorithm we use the platypus hadka 2016 optimisation library to perform 10 000 function evaluations using the nsga ii algorithm a comparison of the approximate pareto frontiers produced is shown in b 11 see fig b 11 b 4 extension potential water energy model and a user interface pywr can be extended to simulate other networked resource systems such an energy or multi resource systems pywr is currently being used to undertake water energy system simulations the pywr model simulates the allocation of two resources water and energy in a combined system coupled via hydropower schemes that relate the flow of water to the generation of power using pywr a combined allocation procedure linear programme is created which ensures system constraints from both resources are maintained future work will improve the energy system simulation by including the power flow equations into a new linear programme based allocation algorithm the pywr library has been coupled with an online web based user interface see fig b 12 this interface allows users to construct and simulate pywr models with a graphical user interface internally this process generates a pywr input file json which is executed by a worker process in a remote server this web based system allows users to collaborate and share models easily between different locations and exploit cloud based compute resources for model simulations further details of the water energy simulation and web based user interface will be given in future publications appendix c benchmarking c 1 theoretical throughput the pywr algorithm 2 contains two major outer loops during an simulation the outer most loop iterates through each time step in a simulation an inner loop iterates through each scenario within each time step parameter values are updated once per time step i e inside the outer loop but a linear programme resolve is required for each iteration of the inner loop i e there is one linear programme solved for each time step for each scenario in addition before the resolve is executed the linear programme must be updated to reflect the current time step and scenario finally after the solve is complete the solution must be applied to the pywr data structures for the remainder of this section we shall refer to this complete process update solve and apply as a resolve critical to the performance of this algorithm is the overhead involved in executing each resolve the software architecture and technology used by pywr have been designed to remove as much of this overhead as possible the use of cython for the resolve process removes the overhead of pure python function calls and the direct interface to the glpk shared library removes as much overhead as possible in modifying and solving the linear programme itself we show that this has been achieved by comparing the throughput of a trivial pywr model with different linear programme solvers tablec 4 compares function call time of one solve of the allocation algorithm using pywr s default cython implementation with a pyomo implementation both using glpk for reference we also include a null implementation which does nothing and the equivalent call times for a standard python and cython function we show that the pywr s cython glpk implementation does not add significant overhead compared with the null implementation and is considerably more efficient than the utilising pyomo c 2 continuous benchmarking as part of the continuous development of pywr a set of standard benchmarks have been created the benchmarks comprise two models from the regression test suite a basic river thames and london model included as an example and a set of random networks the predefined models can be found in the pywr source code repository an algorithm is used to generate the random models for a given number of water resource zones and connection densities a water resource zone is described as a simple system comprising of an input link and output node the maximum flow of the input and output nodes are also randomly determined from normal distributions a number of connections or transfers between the link nodes of the water resources zones are also randomly created the algorithm creates connections up to a target density where 100 density would make a connection from every link to every other link during benchmarking the algorithm is seeded so that the random models are deterministically generated and therefore reproducible the benchmarks are utilised in a continuous benchmarking system airspeed velocity asv droettboom and virtanen 2019 the purposes of this system is to track the performance of the benchmarks through the development of the project using a repeatable process in a similar way to a regression test suite that provides protection against feature and behaviour regression the continuous benchmarking provides view of performance changes this is particularly important for tracking performance regressions and comparing different allocation algorithms and or component implementations here we do not present the complete asv results for all the commits in the project interested users can access the benchmarks and associated configuration from pywr s github project website however the system was used to create the benchmarks provided in section4 
26051,a new generalised water resource network modelling python library pywr is presented given hydrological inflows pywr simulates customisable water allocation and operation rules throughout complex multi purpose managed water systems at each user defined time step the model uses a low level interface to existing linear programming solvers for fast priority based optimisation driven simulation the library uses an object based system for users to provide input data and record simulation outputs a novel multi scenario simulation method provides an almost 4 fold improvement in model run times and supports calculating robustness metrics across scenarios a flexible interface to specify multi objective optimisation formulations as part of a model s input file is included these features enable analysts to apply advanced water planning approaches such as robust decision making and robust optimisation to real systems the library is available under the gplv3 open source licence includes several examples and a regression test suite keywords water resource simulation network optimisation open source python multi reservoir operations decision making under deep uncertainty software availability name of software pywr description pywr is a generalised network resource allocation model written in python it aims to be fast free and extendable it provides a multi scenario simulation platform for undertaking advanced decision making under deep uncertainty studies developers j h arnott j e tomlinson language python 3 6 supported systems microsoft windows gnu linux macos licence gnu general public licence v3 source code https github com pywr pywr distribution pypi https pypi org project pywr distribution conda forge https anaconda org conda forge pywr 1 introduction planning and management of water resource systems has used simulation models for decades as its core approach to evaluating impacts of changes in supply and demand maass et al 1962 loucks et al 1981 rogers and fiering 1986 water resources management simulation models loucks et al 2005 are used to aid our existing qualitative understanding with additional quantitative information many generalised simulators have been developed by numerous authors and institutions several reviews labadie 2004 wurbs 2005 loucks et al 2005 rani and moreira 2010 sulis and sechi 2013 of these efforts are available simulation models assimilate time varying boundary conditions e g river flows operational rules and a representation of the water system to produce a prediction of state in that system over a number of time steps e g daily weekly or monthly these models represent a water resources system as a network or graph of nodes and links simulation models can be classified based on their approach to resource allocation there are those models which use a rule based or ad hoc approach to allocation an example is an algorithm that traverses the system from upstream to downstream following particular rules as it goes such as incetinkaya et al 2008 andmatrosov et al 2011 rule based models can be computationally efficient fast run times and follow complex operating procedures found in particular systems but these models can be cumbersome to develop further maintain and apply to new systems a second type of simulation model are those which use mathematical programming optimisation to assist in simulating water allocation in the resource network at each time step in this case the simulation model transforms the representation of a water system to a mathematical optimisation problem which is solved for one or more time steps at a time the optimisation allows priority based simulation of water resources where the model allocates water throughout the network in each time step to minimise some expression of a penalty e g a financial or economic cost or some empirically calibrated penalty which enables the model to simulate water allocation accurately this technology came from transport system analysis schrijver 2002 and special mathematical algorithms for network flow problems were used because they were more efficient than linear programmes jensen and barnes 1980 for example the ford fulkerson algorithm ford and fulkerson 1956 and out of kilter algorithm fulkerson 1961 were developed to solve maximum flow and minimum cost problems in flow networks respectively minimum cost network flow based water resource models such as wathnet kuczera 1992 modsim labadie 1995 realm perera et al 2005 have since been used widely for river basin planning specialised network flow algorithms have been introduced e g relaxiv bertsekas et al 1994 used by realm and non network constraints can be added to network flow algorithms by iteratively solving a network formulation eventually different types of minimum cost network flow linear programming and non linear programming have been used for this task labadie 2004 network flow algorithms have been shown to be computationally more efficient than linear programme algorithms even when including iteration sun et al 1995 however ilich 2009 demonstrates that such an approach can be problematic and should be used with caution linear programmes can accommodate additional linearised non network constraints without the use of iteration althoughilich 2008 demonstrated some limitations linear programming formulations have been used by several generalised water resources management models such as aquatool andreu et al 1996 aquator oxford scientific software 2008 calsim draper et al 2004 calvin python version dogan et al 2018 oasis randall et al 1997 riverware zagona et al 2001 sisagua barros et al 2008 mike basin jha and gupta 2003 miser fowler et al 1999 weap yates et al 2005 and others they are attractive due to the availability of generic solvers typically using the revised simplex method that apply them the utility of water resources simulation stems from its use in resource assessment and decision making analyses and its compatibility with exploratory methods such as sensitivity analysis and decision making under deep uncertainty dmdu sensitivity analysis investigates how different simulation model results can be attributed to variations of its inputs pianosi et al 2016 dmdu has developed many techniques which have been widely applied to water systems seekwakkel et al 2016b for an introduction such approaches use simulators to evaluate and or discover different infrastructure environmental or institutional interventions for example robust decision making lempert et al 2006 simulates many scenarios to help understand a planned system s vulnerabilities and multi objective robust decision making kasprzyk et al 2013 uses multi objective heuristic optimisation to find candidate sets of policies further extensions such as robust optimisation search over multiple scenarios hamarat et al 2014 kwakkel et al 2015 borgomeo et al 2016 huskova et al 2016 beh et al 2017 watson and kasprzyk 2017 eker and kwakkel 2018 sub sampled from a large uncertainty space water resources simulation is a classical tool but its use is increasing to meet demand for environmental change and intervention impact assessments common to all of these approaches is performing many system simulations under different scenarios this is typically performed by simulating scenarios individually a form of task based parallelism where both the programme model and data can change this requires the entire simulation to be recreated including opening file handles and data structures and repeated for each scenario and potentially distributed to many computers however sensitivity analyses are usually a form of data based parallelism where the programme model is singular but the data e g boundary conditions policy decisions etc change also for simulation intensive dmdu methods task based execution is problematic for two reasons 1 the calculation of sensitivity or robustness metrics herman et al 2015 kwakkel et al 2016a mcphail et al 2018 must be undertaken outside of the modelling software once all the independent scenarios are completed and 2 it is computationally inefficient in cases where the majority of data in the model is identical between two scenarios therefore the current approach to many scenario water resource simulation can be improved in this work we present a new water resources simulation system written in python pywr it uses a time stepping linear programming approach the linear programme is created from the modeller s description of a network of nodes each node type contains attributes and data which alter the linear programme solved in each time step the implementation directly incorporates a novel many scenario simulation method to enable application of advanced dmdu approaches this approach is demonstrated to be computationally more efficient than repeated single scenario execution in addition the implementation incorporates a component based system for the definition of boundary conditions model parameters and calculation of metrics this system is general and extendable by users it also contains an application programming interface api for coupling with dmdu software including the updating of model variables and calculation of robustness metrics across multiple scenarios making it suitable for use in several dmdu frameworks pywr is implemented using cross platform technology and is separated from any data management or user interface software it utilises open formats e g json hdf5 the hdf group 2018 for input output aiding integration into other software packages and processes it includes a regression test suite that is tested on microsoft windows macos and gnu linux the software is released under the general public licence gpl v3 we describe the core approach to simulation and two resource allocation algorithms in section2 the implementation details including software architecture and technology are described in section3 performance benchmarks are given in section4 followed by a discussion and conclusions in sections5 and 6 respectively we demonstrate pywr features with examples and provide some additional benchmarks in appendices 2 methods pywr is a generic dynamic modelling library for network based resource allocation models the key structure of any pywr model is a network or graph of connected nodes these connections links edges are directional and the data is stored internally as a directed graph digraph in contrast to other network models pywr does not assign information to connections except the source and destination nodes all of the data required to perform the allocation procedure is stored on the nodes and determined from the connectivity implied by the connections 2 1 simulation algorithm pywr solves time dynamic problems by looping through a prescribed number of time steps during each time step iteration a reallocation of the model s available resources is performed across the network using a mass balance approach these iterations cause state e g volume in the case of water resource problems changes in nodes that store resource e g reservoirs these state changes along with time varying boundary conditions at particular input nodes cause a dynamic allocation of resources during a model run this core time stepping algorithm follows the approach in 1 pywr defines each time step to be an integer number of days in length time follows a first order finite difference procedure where state is updated by the mean resource flow per day multiplied by the time step length in days 1 1 v i 1 v i q i i n q i o u t δ t i the solution flow q i is provided by the solution of the resource allocation algorithm in each time step the net flow q i i n q i o u t to a storage node with volume v i results in an updated storage volume v i 1 for the subsequent time step the overarching simulation algorithm 1 performs a set of procedures before and after the solution of the resource allocation algorithm it is during these phases that implementation specific calculations may be performed many of these calculations will directly or indirectly influence the values used in the resource allocation algorithm further detail on these calculations and how these values can be generically defined by the modeller is described in section3 2 2 network definition it is necessary to describe the definition of a pywr network prior to a detailed description of the resource allocation algorithm pywr models use a directed graph containing the set of nodes n core node types contained within n include input i output o and storage s as well as links l the pywr implementation includes other node types which extend the basic functionality given by these nodes several of these extension nodes internally create several of the core node types for this reason we refer to these as compound nodes we refer the reader to the implementation details in section3 2 n i o s l each node is also designated a resource domain the purpose of a resource domain is to group common nodes such that the same resource flows through paths between nodes in the same domain other domains in the network will contain their own paths crucially each domain could represent a different resource e g water and energy with exchange of one resource type for another occurring at inter domain connections e g a power plant a resource enters the network at input source nodes and exits at output sink nodes from this definition we can describe all feasible routes through the network that resources could travel by we define these routes as the set of simple paths between any combination of input and output nodes as p these paths contain no repeated nodes cycles may be formed in the network but because only simple paths with no repeated nodes are found this does not present a problem for the subsequent allocation algorithm the addition of a cycle permits more complex network structures such as bi directional links between nodes the definition of paths place some restrictions on the definition of a model s network 1 every node in n must be present in at least one path and 2 only input nodes may have an indegree of zero i e all paths must start with an input node 3 only output nodes may have an outdegree of zero i e all paths must end with an output node we also define the set of edges on the graph that connect an output to input with different resource domains as c these edges enable flow from one domain to another a single output node can be connected to multiple input nodes in other domains but an input node can only contain a single edge to an output node on a different domain 2 2 1 node properties link input and output nodes share a common set of properties minimum flow maximum flow and cost the flow properties constrain the flow on all paths at or through the node the cost property does not have to be monetised it is a general penalty term used to specify the penalties or benefits negative penalties that are incurred with net flow though the node from the user s perspective this means definition of minimum or maximum flow constraints are applied regardless of how many paths traverse a particular node the detail on how the use of these properties in the allocation procedure is given in section2 3 in addition input nodes also contain a conversion factor property this property is used to factor flow between domains and can therefore be used to simulate a different resource type in each domain for example the water and electricity domains where the conversion factor would represent power generation per volume of water 2 2 2 storage nodes storage nodes s differ from the other nodes they are a compound node containing at least one input and one output node and have a minimum and maximum volume property instead of a minimum and maximum flow property the use of a compound node means that adding a storage node also adds its associated input and output nodes to the network edges on the graph to the storage node connect to one of its child output nodes and edges from the storage node connect from one of its child input nodes fig 1 shows the underlying graph structure used for the compound storage node a consequence of this implementation is that paths are broken at storage nodes i e a single path cannot traverse a storage node because the child input and output within the storage node are not connected to one another in contrast to the other nodes a storage node retains a state variable throughout a simulation this variable is the current volume of the storage node and is updated at the end of every time step as per 1 the permitted flow on the incoming and outgoing paths of each storage node is constrained by its minimum and maximum volume storage nodes also include a cost property similar to the other node types which is applied to the underlying child input and output nodes 2 2 3 virtual node types pywr models can optionally use two special virtual node types that are not directly connected to a model s network these are aggregated and virtual storage nodes respectively the purpose of these node types is to apply constraints or calculate properties across multiple nodes that may not be connected within the network both node types act like a conventional node but rather than being connected to other nodes they are assigned other nodes called child nodes in a model the aggregated node allows minimum and or maximum flow constraints to be applied across all the child nodes this does not supersede any node s individual flow constraint it is an additional constraint this aggregated flow constraint has practical application in water systems where multiple abstractions at different locations may be constrained by a single licence in addition proportional factors can be given to enforce a particular ratio of flow between the child nodes this is useful in systems where a fixed split of resource is prescribed to downstream paths e g managed river systems or where a predefined sharing of resource sources is required virtual storage nodes include a volume state similar to a conventional storage node change in volume is a function of the total flow through all child nodes virtual storage nodes must be initialised with an initial volume along with minimum and maximum volumes optional flow factors can be given for each child node to provide a multiplier of their contribution to change in the volume of a virtual storage node virtual storage nodes allow the modeller to track the usage of for example annual abstraction licence volumes the virtual storage node s state can be used to make allocation behaviour dependent on abstraction under or over use e g by restricting future abstractions for the remainder of the licensed period we define the set of aggregated nodes with flow constraints as g n and with proportional factors as g f the set of virtual storage nodes is defined as g s we use the notation n g g to define the child nodes n in a aggregated or virtual storage node g taken from the complete set of aggregated nodes g 2 3 allocation algorithms the software architecture of pywr allows multiple resource allocation algorithms to be implemented we have implemented and tested two optimisation based resource allocation algorithms we note immediately that neither of these implementations is of a classical minimum cost network flow problem for which specialised algorithms are available for example seekuczera 1989 1992 the pywr implementation is similar but practical applications require some important additional constraints these constraints primarily arise from the aforementioned aggregated and virtual storage node types and resource domains therefore pywr uses a standard linear programme lp to determine the allocation of resource across the network at every time step a linear programme in general form is shown ineq 3 3 minimise c t x subjectto a a x b l x u where x r we implement two alternative linear programmes to demonstrate the flexibility of the approach the two approaches differ in their treatment of the variables x in the linear programme the first creates a variable column for each simple path between input and output in the network following a similar approach tocheng et al 2009 the second creates a variable column for each edge arc in the network following a similar approach toerfani et al 2013 both approaches create similar constraints to accommodate the node level properties e g minimum and maximum flow a complete description of both approaches is given in appendixa a comparison of the number of columns and rows in the respective linear programmes is given in table1 a direct comparison is made difficult by the dependency of the linear programme shape on the size and connectivity of the network we note that the path based solver will create linear programmes with fewer columns in networks that are long with several link nodes however it will create a large number of columns in more connected networks with larger numbers of input and output nodes due to the larger number of paths the edge based solver will always create a linear programme with a larger number of constraints rows due to the additional mass balance constraints however it is important to note that these constraints and their bounds do not need to be updated during a simulation finally we note that the solve time of the simplex method is a function of the size number of rows and columns and sparsity of the constraint matrix however the simplex algorithm is an example of algorithm that performs well in practice but is known to have poor worst case performance spielman and teng 2004 therefore it is difficult to make general conclusions about the solve time of the two different linear programme formulations we benchmark both linear programmes in section4 by comparing performance with randomly generated models of different sizes and connection densities 2 4 updating and solving the linear programme the simulation algorithm 1 requires the linear programme to be updated and solved at each time step during a simulation boundary conditions and internal state will vary these changes must be reflected in the allocation algorithm the algorithm and linear programme defined here place no restrictions on what may or may not be updated each time step however specific implementations may trade off arbitrary updates to the linear programme for the sake of efficiency for example the current pywr implementation does not permit changes to the matrix a between time steps there are two primary methods for solving linear programmes the most widely used is the simplex method which originated fromdantzig 1951 laterkarmarkar 1984 introduced the first of a class of algorithms now known as interior point methods which were competitive with the simplex methods in practical applications vanderbei 2014 gives a complete introduction to both the simplex and interior point methods once solved the solution x of the linear programme must be applied to the internal state of the network for each storage node this requires updating the current volume v n as per eq 1 the remaining nodes simply have their properties updated with the sum of the flow through all paths in which they are present these properties can be used in the subsequent time step s to alter the linear programme bounds or saved to a storage device 3 implementation pywr is written in the open source python vanrossum and drakejr 1995 programming language the pywr library provides the necessary data structures and routines to create execute and analyse models of resource systems in this section we describe the implementation details and core functionality of the pywr library while the core approach centres around the resource allocation algorithm described in section2 it is the software library implementation that contributes an innovative approach to water resource modelling specifically this is the approach to multi scenario simulation and integration with modern dmdu methods and component based system for defining data input and output this section is organised as follows the overall architectural design and technology of the library is described in section3 1 section3 2 details the implementation of the allocation algorithms using glpk the main node types and their management as a network are described in sections3 3 and 3 4 respectively the data input output system is explained in section3 5 section3 6 details the implementation of the approach to efficiency multiple scenario simulation finally section3 7 describes extensions to the main library including links to multi objective evolutionary algorithms 3 1 software architecture and technology the pywr implementation uses a modified version of the time stepping algorithm 1 this modified algorithm is given in 2 the primary modification is the evaluation of multiple scenarios during a single time step for example if a model is used to simulate 100 plausible futures the first time step including resource allocation of all futures is performed before moving to the next time step more detail on this approach is described in section3 6 we also define api methods setup reset before after and finish on all components of a model the library is structured as a standard python package it utilises an object oriented programming approach allowing users to extend its base functionality as required the library has dependencies on several other python packages including numpy oliphant 2006 scipy jones et al 2001 pandas mckinney 2010 pytables pytables developers team 2002 matplotlib hunter 2007 and networkx hagberg et al 2008 pywr was designed to be easily integrated into dmdu analyses lempert et al 2006 hallegatte et al 2012 matrosov et al 2013 and multi objective evolutionary algorithms moea reed et al 2013 maier et al 2014 2019 analyses however the standard python interpreter typically referred to as cpython does not produce efficient code when compared with statically typed compiled languages such as c and fortran we overcome this limitation by the extensive use of cython behnel et al 2011 cython is a tool for generating python c extensions users of cython write code in a superset of python s syntax which cython interprets to generate c this c code is compiled to an extension library which can then be imported in python the benefit of cython is that a programmer can retain much of the flexibility of python while also using the additional cython syntax to make performance critical functions more efficient determining which parts of pywr constitute core functionality and therefore requiring the use of cython was discovered using benchmarking and profiling tools by using cython rather than a pure c or fortran library we remain compatible with python this provides users with significant flexibility to extend the core functionality with extensions written in python providing generalised approach to simulating resource networks with bespoke rules and behaviours specific to each network as users write more complex extensions the calculations may become a bottleneck in model run time users may then migrate their extensions into cython to improve computational performance 3 2 solving the resource allocation algorithm the pywr implementations utilises the gnu linear programming kit glpk makhorin 2018 to solve the linear programme described in section2 glpk is an open source linear programming library that implements several linear programming and related algorithms including primary and dual simplex methods and a primary dual interior point method glpk provides a library that is written in the c programming language and is available on several platforms including microsoft windows and gnu linux pywr utilises cython to directly interface with the glpk c library this approach contrasts with several approaches to linear programming in python typically an additional library such as pyomo hart et al 2017 2011 or high level modelling framework such as gams corporation 2013 is used to provide a common framework to construct and solve optimisation models these approaches provide a high level interface and choice of solver at the expense of computational efficiency and would add significant overhead to the time stepping pywr algorithm 2 we demonstrate this in our specific case by the benchmarking of a pyomo based implementation of one of the resource allocation algorithms in appendixc the direct interface to the glpk c library requires managing the creation update and solution of the linear programme data structures for efficiency purposes the pywr algorithm permits changes to the objective coefficients c p and constraints bounds a and b changes to the matrix a are not permitted between time steps therefore the matrix a can be created once at the beginning of the simulation and remains a constant this approach allows the main glpk problem data structure to be created during the setup phase at the beginning of a simulation all calls to glp create prob glp add rows glp add cols and set mat row are undertaken during this initialisation phase outside of the main time step loop during the update of the linear programme there are only calls to set obj coef and set row bounds prior to the use of the simplex method the consequence of this decision is that only model parameters that effect row bounds or objective coefficients can be altered during a simulation this has two major implications first it requires the connectivity of the network remain constant during a simulation specific routes through the network can however can be effectively enabled disabled using dynamic constraints to maximum flow second it implies conversion factors for cross domain connections and flow factors for both aggregated and virtual storage nodes remain constant during a simulation we note also that the library is structured such that different methods or algorithms can be used to solve the linear programme at every time step indeed one could implement an allocation routine that does not use a linear programme currently glpk makhorin 2018 and lpsolve berkelaar et al 2016 are implemented in the pywr library 3 3 fundamental node types the core components that a modeller works with in pywr are the various types of node object the modeller will define and assign attributes to multiple nodes of different types during the development of a pywr model instance which are then connected together to form a network the library contains a core set of node types that map directly to those described in section2 while providing the four types of standard node link input output and storage and two aggregated node types aggregated and virtual storage the library also provides a number of variations of these types for convenience to the modeller these node types are not included in the methodology section because their implementation is a subset of the general behaviour provided by the standard node types for example a generic catchment node that is a special case of an input node which uses the same value for maximum and minimum flow 3 3 1 piecewise link in certain situations it may be desirable to define a non linear cost function at a particular node for example this might represent a desire to allocate a particular minimum flow of resource through a node with additional flow above the threshold being permitted but providing no benefit a piecewiselink node is included in the library that allows a piecewise objective function minimum and or maximum flow constraint to be applied internally the piecewise link is implemented as multiple link nodes with different costs and maximum flow properties connected in parallel following the approach oflund and ferreira 1996 fig 2 shows how this is implemented for the case a of a minimum flow threshold with a lower cost flow is not explicitly forced through the path associated with the minimum flow threshold but the lower cost for this path will ensure this path is preferred when the linear programme is solved the piecewise link acts as a normal node and the internal representation is not exposed the node reports the sum of flow through all its internal link nodes and therefore its entire structure is not shown to the user 3 4 network structure the core of the pywr library depends on the networkx hagberg et al 2008 library to store and manipulate the directed network structure of a model networkx provides many advanced algorithms to create and manipulate complex networks its primary function in pywr is to manage a model s connectivity and determine the number of feasible flow routes the latter is achieved by using a simple paths algorithm derived fromsedgewick 2001 a secondary use of networkx in pywr is to record the dependency tree of model components see section3 5 the pywr library provides a convenient api for connecting the various model nodes together connection is directional requiring the user to specify the upstream and downstream nodes there is no restriction on the creation of loops but bi directional connections must be created using one connection in each direction the use of bi directional links overcomes some of the issues associated with rule based models that must progress from upstream to downstream the definition of upstream and downstream is difficult when loops are present and therefore may require iteration of rule based algorithms 3 5 model data and output components while much of the above discussion has focused about the technical details of the core parts of the pywr library a useful computational library must provide a convenient and helpful interface to create models in a pywr model the specification of data and output is defined through two types of component parameters and recorders both components share the same parent class definition complex behaviour in pywr is created through the use of multiple inter dependent components for example the summation of multiple parameters or the transformation of one parameter value to another via some user defined function first we describe the common functionality and integration into the pywr algorithm 2 in the following sections we given more detail about the specific roles of parameters and recorders themselves 3 5 1 components and dependencies the main computational algorithm 2 deployed by pywr performs an outer loop over the time steps in the simulation during each iteration of this loop component objects have the opportunity to perform any required internal calculations before and after the allocation algorithm is solved the api specifically defines before and after methods as written in algorithm 2 for all components the default implementation in the parent component from which all other components inherit performs no operations but allows specific component subclasses to define their own behaviour in general arbitrary interdependencies between components could exist the order in which the components perform the internal calculations is therefore important for example a component dependent on the value of another must be updated after its dependency pywr solves this problem using parent and child relationships between components upon creation each component must register any relationship it has with other components typically this takes place during initialisation where the dependent component s is passed to the parent s constructor method the parent must then ensure it registers the dependent component object as a child once the components interdependencies are defined they remain fixed for an entire simulation during the model setup phase the component relationships are inspected to construct a directed graph of the dependencies this graph is initialised with a root node to which all components are a child this graph encodes the component dependencies and is inspected to determine the appropriate order to perform component calculations the order is determined by performing a depth first search using the networkx dfs postorder nodes function this algorithm is based on a depth first search algorithm byeppstein 2004 if this algorithm detects any circular dependencies the model is unable to run and an exception is raised if the algorithm is successful the traversal of the dependency graph is flattened and reversed this order is cached for the entire simulation such that it does not need to be recalculated each time step during each update of the main algorithm 2 e g before and after the components methods are called in this order the major benefit of this approach is that it ensures each component is updated only once per time step while allowing multiple components to depend on the same child component 3 5 2 input data parameters the purpose of parameter objects is to provide data to the model this is achieved through a common api to which all parameters must adhere this api extends the common functionality of components with additional methods that should compute the value to be used in the model and allow retrieval of that value by parent components as part of an object oriented structure each new parameter type must implement a method value which computes the current value of that parameter instance this method is called by the overarching algorithm for every scenario see 3 6 in every time step the current time step and or scenario information can be used as part of the calculation if required the standard pywr library includes many useful parameters that can be used to provide complex data input of particular interest is aggregatedparameter which can undertaken a user defined aggregation across multiple other parameters this is achieved using the component dependency previously described many other parameter types require other values and therefore parameters for their calculations in water resource modelling it is common to implement a dynamic hedging rule on demands yeh 1985 russell and campbell 1996 oliveira and loucks 1997 there are many different forms of hedging rule but a simple example is given in 4 the demand d 1 is factored based on the volume v 1 of a reservoir the factor α is determined by a comparison of the volume v 1 against 1 or more hedging curves f i the factors α i the hedging curve functions f i and the comparison operator i e the if then else statement are all implemented as separate parameter instances for example an aggregatedparameter would perform the multiplication of α with d 1 and controlcurveparameter would implement the if then else conditions that determine whether α 1 α 2 or α 3 is used 4 d ˆ 1 t α d 1 where α α 1 if v 1 f 1 v 1 t α 2 elseif v 1 f 2 v 1 t α 3 otherwise this component based approach has two advantages 1 the components can be shared and reused by multiple components and nodes in a model and 2 individual components can be changed to different types transparently for example the hedging curve rule f 1 ineq 4 might be a constant time varying profile or some custom function based on multiple reservoirs volumes or other information the modeller can easily change parameterisation of their model by changing component types as opposed to changing node types a complex example of the general approach given ineq 4 can be found in the examples given in the appendix of this paper 3 5 3 output data recorders in contrast to parameters recorder components are used to observe or save simulation results by default a pywr model will not include any recorders and therefore not save any information recorders extend the parent component api by adding methods to return a single value per simulation see section3 6 and a single value aggregated across all simulations individual recorder types are responsible for calculating the per scenario values in the values method a common aggregation function is provided that is user configurable it includes a default set of aggregation methods sum min max mean median and product but can also apply a user defined function a range of recorder types are provided in the standard pywr library these include the ability to save timeseries from each node to hdf5 or csv format other recorders are useful for tracking certain performance metrics in a given simulation for example the total or mean flow that passes through a particular node 3 6 scenarios in many contexts there is a need to perform multiple simulations of a model with variations upon the input parameters for example when performing a sensitivity or scenario analysis pianosi et al 2016 more recently robustness metrics coupled with multi objective optimisation have become popular in water resource planning the requirements for the length of simulations the number of scenarios and number of function evaluations is increasing due to a desire to better resolve risk and uncertainty and design more complex adaptive planning policies pywr has integrated support for multiple simulations a core innovation and contribution of this simulation platform this approach shown in algorithm 2 differs from a traditional sensitivity analysis approach that would rerun the entire simulation instead pywr requires the definition of multiple sources of uncertainty each with a set of scenarios the full set of scenarios to simulate can be the product of all sets or a sub sample thereof for each scenario to be run internal state variables e g storage node volumes v n s are created the simulation proceeds by solving the resource allocation algorithm for each scenario within each time step this approach does not use multi processing or any kind of parallel computing this approach increases the memory requirement of the library by requiring state variables for each scenario to be stored simultaneously however it takes advantage of the commonalities across the scenarios typically the parameters that vary between scenarios are few compared to the complexity of an entire model therefore in all simulations the model structure and most of the model s properties are identical we have included an assessment of the additional memory consumption used when running multiple scenarios in 4 2 the parameter system employed by pywr reflects this approach some parameter types allow the modeller to specify a different value per scenario for a given uncertainty dimension others simply return the same value in all scenarios parameters that operate on other parameter values correctly operate on a per scenario basis it is up to the modeller when to use parameter types that vary their return value by scenario in appendixb we show an example of a demand model that uses multiple parameter objects one parameter is used to create different baseline demands the performance improvement of using this approach is demonstrated in section4 1 3 7 extensions and linking pywr to other libraries pywr components give a user the ability add functionality for their own purposes this might include the integration with other models to define certain values for example a crop model could be used to define the demand at an agricultural node provided such a model could be written or accessed from python it could be integrated with a custom parameter object to directly provide data to a simulation the real benefit of this approach is when such custom parameter s values are dependent on the pywr model s state for example demand that is a complex function of current reservoir storage the modeller is able to write an arbitrary relationship between the model s state and their parameter s output 3 7 1 optimisation simulation based optimisation frameworks such as mordm kasprzyk et al 2013 and moro hamarat et al 2014 have become popular in recent years these approaches couple water resource simulators global optimisation algorithms in an effort to find efficient values for key input parameters decision variables these parameters could represent control or hedging curves release rates and or investment decisions objectives and constraints are metrics computed by the simulation model repeated evaluation of different decision variable values returns the corresponding objective and constraint metrics allowing an exploration of the decision space by heuristic search algorithms the pywr component system provides an api that can be used by optimisation algorithms parameter instances can be flagged as decision variables is variable true and provide an api to determine the number of real and integer variables that parametrise that particular type set the values of the internal parameterisation and retrieve the values of the internal parameterisation this approach allows complex types requiring multiple decision variables to be incorporated easily in to an optimisation formulation in a similar manner recorder instances be flagged as an objective and or constraint is constraint true objective direction is set by applying a string e g is objective minimise this api allows an external algorithm or programme to determine the number and type of variables in any given model number of constraints and the number and direction of objectives the parameter variables can be updated as the first stage of an optimisation loop which repeatedly reruns the simulation after a simulation is complete the constraint and objective values can be determined from the existing recorder api for calculating the aggregate value over all scenarios therefore this api allows multi objective constrained robust optimisation ro ben tal et al 2009 to be undertaken a key contribution of pywr is that through the component subsystem variables constraints and or objectives can be easily altered either their representation change e g from flat to time varying hedging curves or their inclusion or not in the optimisation formulation i e enable disable decision variables or objectives optimisation wrappers have been written for four libraries borg hadka and reed 2013 platypus opt hadka 2016 inspyred garrett 2012 and pygmo biscani et al 2019 an example of coupling a simple pywr model to a two objective control curve problem is given in appendixb 3 3 7 2 data management and user interface pywr models can be defined using python via a conventional application or script however models may also be defined using a javascript object notation json based file format an entire model including nodes edges and components may be defined as a standalone file json was chosen for its common use in modern web based services and communication parsers and writers for json are commonly available in many programming languages including python the format is designed to be human readable and less verbose than xml as a python library pywr includes no user interface or data or run management sub system users are required to write and edit models using the json file format and provide large data via external files however as an open source python library pywr can be easily integrated in to external data management and user interface systems this has been successfully achieved with the hydraknox et al 2019 data management system via an interface application that supports import export and execution of pywr models model data and results are stored in a database transparent to the modeller such data management systems facilitate the use of other technologies to interact with model data for example web based user interface applications and simulation result viewers 4 benchmarking in this section we demonstrate the efficiency of the multi scenario simulation algorithm and compare two different allocation formulations the examples and benchmarks used to produce these results are available in the pywr source code repository additional benchmarks are given in appendixc these demonstrate the efficiency of the implementation by showing that the overhead of running a pywr model is small and also that significant improvements to performance have been achieved by the progressive use of cython over the development of the project continuous benchmarking 4 1 speed up of multiple scenarios in this section we demonstrate the performance characteristics of the approach to multiple scenario simulation described in section3 6 this approach enables significant efficiencies in model execution by taking advantage of the shared structure of linear programme across all of the scenarios the overhead involved in initialising and stepping through time steps is also shared by all scenarios this method means that the linear programme bounds and objective function are updated for each scenario in each time step and solved again therefore eliminating the need to store or create multiple copies of the linear programme for each scenario in addition when the simplex method is used the previous optimal basis can be used as a warm start for the new solve potentially reducing the number of iterations required to reach the new optimal solution the approach is shown in algorithm 2 we use the asv benchmarking system and 3 model benchmarks with a varying number of scenarios to show the speed up in run time the simplethames benchmark utilises a different catchment inflow timeseries in each of the scenarios the speed up is calculated by dividing the total run time of the multiple scenario simulations by the expected run time assuming separate simulations of a single scenario each of the benchmarks show similar speed up characteristics approaching 4 times the equivalent single scenario run time with 160 scenarios see fig 3 4 2 memory benchmarking in addition to the timing benchmarks we have also undertaken some analysis of memory consumption we use the same asv system described in the previous section to undertake an analysis of memory usage when running multiple scenarios these benchmarks use the same models as used for the timing benchmarks in fig 4 we show the total peak memory consumption in megabytes and peak memory consumption per scenario all models show a close to linear increase in memory consumption as the number of scenarios increases we note that the increase per scenario is small relative to the memory requirement for running a single scenario the peak memory consumption per scenario shows that running multiple scenarios in one simulation is more efficient than running each scenario in separate parallel processes that do not share memory we note that these benchmarks are relatively simple memory consumption in more complex models could be highly dependent on the type of components inputs and outputs used in the model 4 3 comparison of allocation algorithm performance two linear programme based algorithms have been implemented using glpk makhorin 2018 and are described in appendixa we use the asv benchmarking system to compare these two algorithms on the randomly generated models results are presented in fig 5 for the two algorithms with increasing size of model these models have been generated with a connection density of 5 we observe that the path based algorithm has marginally better performance than the edge based algorithm for a smaller number of nodes however it suffers a significant reduction in performance with a larger number of nodes this is due to the large number of feasible routes and hence linear programme variables when using the path based algorithm with a highly connected and large model 5 discussion 5 1 limitations we have presented a computational library for water resource systems analysis pywr does not include a graphical user interface to aid users with creating and calibrating models instead users must either utilise the python api directly or write a model definition file in json format neither of these approaches are attractive for a water resource systems analyst or stakeholder untrained in either method the creation of a graphical user interface to aid the construction of pywr models would mitigate this limitation pywr uses an optimisation based resource allocation procedure while the library can support multiple formulations the overall algorithm remains the same the use of optimisation based allocation is not unique but is also not without limitations ilich 2008 has outlined several of these limitations for similar models the analyst as with any modelling system must be aware of the overall formulation and its limitations models must be checked calibrated and tested to ensure the system parameterisation and behaviour is as expected another limitation with the pywr formulation is the fixed time step and the use of euler s method for time integration while euler s method is simple to implement it can suffer from instability and accumulate errors if the timestep is not appropriate in pywr this would manifest through errors in the volume predictions of storage nodes pywr currently has a lower limit of a 1 day for the time step which could be too large in some circumstances in addition the time step must be defined as an integer number of days therefore it is not possible to define a time step that aligns exactly with for example the weeks or months of the year however it is possible to resample the output timeseries to align with a desired frequency that is different to the pywr time step e g run a daily simulation but resample results to monthly means 5 2 benefits despite the limitations pywr has proved a useful modelling framework with several benefits the approach to multiple scenario simulation components and links to several optimisation frameworks create an efficient modelling process for both mordm and ro studies we have demonstrated a roughly 4 times speed up of the multiple scenario simulation strategy this speed up is a result of the software implementation not parallel processing taking advantage of 1 the common data e g the network structure and boundary conditions between the scenarios and 2 undertaking the time stepping procedure only once for many scenarios the speed up result is particularly important for ro studies where many function evaluations over multiple scenarios are required the component based system for specifying both model parameters and outputs is very flexible for optimisation studies the output can be reduced to only that required to compute the objective values thereby reducing the memory and computational overhead of a simulation and also removing any requirement to save data to files in addition the component system allows for easy alteration and testing of the dmdu formulation for example the addition or removal of decision variables and or objectives can be undertaken via boolean flags in the model definition file the type of decision variable and or objective can also be easily changed independently of the network definition an example of this is provided in the appendixb 3 where the parameterisation of a control curve is tested under optimisation these benefits are coupled with in built support for 3 different moea libraries which allow modellers to easily perform multi objective studies without necessarily writing any new code pywr is designed as a computational library and is consequentially decoupled from a graphical user interface or data management system this modular architecture allows the library to be deployed easily to high performance and cloud computing environments as a python library it is cross platform thereby allowing analysts to develop models on their local computers in one operating system e g microsoft windows but run large simulation studies in another e g gnu linux the modular design also allows pywr to be linked to larger applications possibly running multiple models for example a multi model simulation framework such as pynsim knox et al 2018 pywr is available under the general public licence gpl and continues to be developed free and open source software foss licences allow the creation and shared contributions of a community of modellers historically few water resource models are available under foss licences many have been available under academic use only licences restricting the commercial use and adoption of such models pywr is managed via github by the original developers and welcomes contributions from researchers and users 5 3 future work development of pywr is ongoing and driven by its application to specific case study applications further research into the development of water resource system modelling and pywr itself can focus on its application to large scale systems and multi objective optimisation problems the library can expand its features in this area by providing modellers with additional tools to work on such problems in addition more complex parameter types can be envisaged that incorporate more advanced operating rules e g multi reservoir radial basis functions foresight and adaptive planning more complex recorder types could also compute more sophisticated objectives including different robustness metrics e g those discussed byherman et al 2015 kwakkel et al 2016a mcphail et al 2018 some water management models particularly in situations with water markets require tracking transactions water exchanges in water resource networks erfani et al 2013 2014 although not yet attempted with pywr this could be potentially be achieved by creating a trade parameter that is shared by two nodes representing different water users depending on the formulation trades could be optimised via the allocation algorithm within time step or by global optimisation we have benchmarked pywr against itself but recognise that we have not undertaken a comprehensive comparison of this tool against existing similar tools in future work we aim to provide a complete comparison of these tools in terms of their computational efficiency features and accuracy additional computational efficiencies may be possible through further optimisation and exploitation of multi scenario simulation such simulations are trivially parallel and can exploit parallel computing by dividing the scenario space between individual i e multiple instruction multiple data mimd with the existing implementation however further efficiencies may be possible by exploiting single instruction multiple data simd approaches such as those found in modern graphics processing units gpu when running a very large number of scenarios pywr is a computational library and subsequently does incorporate a graphical user interface gui this is a conscious design decision separating the computational model from the interface user interfaces could become available to assist with constructing maintaining and sharing water resource network models efforts are in process pywr s modular design means diverse technology and development choices will be available for user interaction 6 conclusion in this work we contribute to the field of water resource systems analysis and planning by providing a modern and open source computational library pywr implements a time stepping optimisation based resource allocation procedure using a direct interface to linear programming solvers this approach coupled with the use of python and cython provides a fast yet extendable software library in addition pywr supports multiple scenario simulation directly and exploits the identical network structure and shared data that typically exists when undertaking multiple simulations this efficiency is demonstrated through benchmarking more generally pywr provides a component based and extendable system of parameters and recorders that allow users to define complex boundary conditions and performance metrics respectively the pywr library includes many useful components as standard but users can easily develop their own using python s class system we also provide api support for integration of pywr models into robust optimisation studies where an external algorithm can update parameter values perform a simulation and gather objective values from recorder outputs examples of these approaches are provided in appendixb of this paper declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements uk research and innovation ukri provided support through the global challenge research fund gcrf futuredams project es p011373 1 and the university of manchester united kingdom funded james tomlinson s doctoral studies any opinions findings and conclusions or recommendations expressed in this material are those of the authors but not necessarily the funders both joshua arnott and james tomlinson were employees at atkins when the open source project began appendix a allocation algorithms a 1 path based allocation algorithm in this formulation variables or columns in the linear programme x are created for each path p p these variables represent the flow along each path a 1 x x p p p a 1 1 node constraints constraints are added to the linear programme for each of the nodes in the network a 2 a n a f a d a s a n a f 0 a s b n b f 0 b s where a f a d and a s are sets of constraints associated with minimum and maximum flow flow between domains and minimum and maximum storage respectively each node can be present in multiple paths and therefore the total flow at a given node is the sum of all paths containing that node a 3 a f n p p n p x p a f n a f n b f n forall n n where a f n and b f n are the minimum and maximum flow for the node n these values are determined by the properties of each node the set of edges c that define the inter domain connections is used to define a constraint for each output node a connection links one output node to one input node a 4 a d p p o c p x p p p i c p x p f i c 0 0 forall o c i c c where o c and i c are the output and input node for each of the inter domain connections respectively for each input f i c is the conversion factor the equality bounds on these constraints ensure mass conservation between domains subject to any unit conversion storage nodes cannot release more resource than the incoming flow plus the available volume and cannot store more resource than the current spare capacity less any outgoing flow i e the net flow to a storage node cannot violate the minimum and maximum volume constraints a 5 a s n p p n p forall n o n x p p p n p forall n i n x p a s n v m i n n v n δ t b s n v m a x n v n δ t a s n a s n b s n forall n s where o n and i n are the set of output and input nodes associated with the storage node n a s n is the net inflow to the storage node n net inflow is constrained by minimum and maximum volume v m i n n and v m a x n respectively determined from the node s properties these limits combined with the current volume v n determine the lower and upper bounds a s n and b s n respectively a 1 2 virtual node constraints each of the aggregated nodes contributes constraints to the linear programme as follows a 6 a a a a f a a z a v s a a a a f 0 a v s b a b a f 0 b v s aggregated nodes with minimum and or maximum flow limits apply constraints to the linear programme the constraint is applied to the sum of flow across all paths in which the child nodes n of the aggregated node g are present a 7 a a f g p p n p forall n g x p a a f g a a f g b a f g forall g g n aggregated nodes with optional factors apply constraints to the linear programme for each child node n beyond the first a new constraint is added that enforces the ratio of flow between n and the first child node n 0 the factor is normalised by the first child node s factor a 8 p p n 0 p x p f 0 f i p p n i p x p a a z 0 for n i g g f and i 0 virtual storage nodes with minimum and or maximum volume limits apply constraints to the linear programme the constraint is applied to the net change in volume of the virtual storage node based on the flow through all child nodes n the convention here is that all child nodes contribute to net flow out of the virtual storage node the contribution is factored by f n for each node a 9 a v s g p p n p forall n g f n x p a v s g v m i n v g t δ t b v s g v m a x v g t δ t a v s g a v s g b v s g forall g g s a 1 3 objective function the linear programme is defined to minimise the cost of flow through the network costs are accrued to routes by summing up all the penalty values cost properties for each node c n in each path p input and output nodes associated with storage nodes defer their cost property to that of the parent storage node in this situation input nodes return the negative of their parent storage node cost the purpose of this is to have the storage node cost applied to the net inflow a s n therefore the cost applied to the storage node can be used as a penalty or benefit if negative to increasing the volume of stored resource a 10 c p n p c n forall p p minimise p p c p x p the choice of penalty values to use in any implementation is entirely up to the user we make no a priori assumptions regarding the cost values and their magnitude the value of c n for each node is determined from the node s properties a 2 edge based allocation algorithm a 2 1 node constraints constraints are added to the linear programme for each of the nodes in the network a 11 a n a m a f a d a s a n 0 a f 0 a s b n 0 b f 0 b s where a m a f a d and a s are sets of constraints associated with mass balance minimum and maximum flow flow between domains and minimum and maximum storage respectively mass balance constraints ensure that the flow entering and leaving link nodes is identical this constraint is based in the entering edges e i n and leaving edges e o n for each node n a 12 a m n e e i n x e e e o n x e forall n n the minimum and maximum flow constraints are applied to the set of edges e n associated with node n for output nodes e n is the set of edges terminating at node n for all other nodes e n is the set of edges starting at node n a 13 a f n e e n x e a f n a f n b f n forall n n where a f n and b f n are the minimum and maximum flow for the node n these values are determined by the properties of each node the set of edges c that define the inter domain connections is used to define a constraint for each output node a connection links one output node to one input node a 14 a d e e i o c x e e e o i c x e f i c 0 0 forall o c i c c where o c and i c are the output and input node for each of the inter domain connections respectively for each input f i c is the conversion factor the equality bounds on these constraints ensure mass conservation between domains subject to any unit conversion storage nodes cannot release more resource than the incoming flow plus the available volume and cannot store more resource than the current spare capacity less any outgoing flow i e the net flow to a storage node cannot violate the minimum and maximum volume constraints a 15 a s n e e i n forall n o n x e e e o n forall n i n x e a s n v m i n n v n δ t b s n v m a x n v n δ t a s n a s n b s n forall n s where o n and i n are the set of output and input nodes associated with the storage node n a s n is the net inflow to the storage node n net inflow is constrained by minimum and maximum volume v m i n n and v m a x n respectively determined from the node s properties these limits combined with the current volume v n determine the lower and upper bounds a s n and b s n respectively a 2 2 virtual node constraints each of the aggregated nodes contributes constraints to the linear programme as follows a 16 a a a a f a a z a v s a a a a f 0 a v s b a b a f 0 b v s aggregated nodes with minimum and or maximum flow limits apply constraints to the linear programme the constraint is applied to the sum of flow across all paths in which the child nodes n of the aggregated node g are present a 17 a a f g e e n forall n g x e a a f g a a f g b a f g forall g g n aggregated nodes with optional factors apply constraints to the linear programme for each child node n beyond the first a new constraint is added that enforces the ratio of flow between n and the first child node n 0 the factor is normalised by the first child node s factor a 18 p p n 0 p x p f 0 f i p p n i p x p a a z 0 for n i g g f and i 0 virtual storage nodes with minimum and or maximum volume limits apply constraints to the linear programme the constraint is applied to the net change in volume of the virtual storage node based on the flow through all child nodes n the convention here is that all child nodes contribute to net flow out of the virtual storage node the contribution is factored by f n for each node a 19 a v s g e e n forall n g f n x e a v s g v m i n v g t δ t b v s g v m a x v g t δ t a v s g a v s g b v s g forall g g s a 2 3 objective function the linear programme is defined to minimise the cost of flow through the network costs are accrued to edges by summing up all the penalty values cost properties associated with node c n edges associated with link nodes have their costs factored to avoid double counting the indegree and outdegree edges input and output nodes associated with storage nodes defer their cost property to that of the parent storage node in this situation input nodes return the negative of their parent storage node cost the purpose of this is to have the storage node cost applied to the net inflow a s n therefore the cost applied to the storage node can be used as a penalty or benefit if negative to increasing the volume of stored resource a 20 c e n e f n c n forall e e where f n 0 5 if n is link 1 0 otherwise minimise e e c e x e the choice of penalty values to use in any implementation is entirely up to the user we make no a priori assumptions regarding the cost values and their magnitude the value of c n for each node is determined from the node s properties appendix b examples the pywr software repository contains an extensive regression test suite with over 300 unit tests these tests are run automatically by the continuous integration ci system upon changes to the software new features should be accompanied by appropriate tests ensuring the correct behaviour now and in the future while unit tests are good practice in any software project they do not easily demonstrate the complete functionality of a project to this end we have included a number of example models that are distributed with the source code in this section we use those examples to demonstrate pywr s core functionality and outputs we note that while these models are more complex than typical unit tests they remain relatively simple when compared with real systems b 1 system with analytical solution the most basic example model demonstrates the ability of pywr to compute a numerical approximation to a simple system for which an analytical solution is known eq b 1 defines a simple storage system in which there is a time varying inflow q i n and constant outflow q o u t this system can be modelled using three nodes in pywr an input and output node are connected upstream and downstream of a storage node respectively the output node is given a constant maximum flow constraint equal to q o u t and a negative cost the input node has minimum and maximum i e an equality constraint flow constraint to equal to q i n b 1 δ v δ t q i n q o u t q i n s 1 cos ω t q o u t s the frequency ω of the inflow is set to 2 π 366 with the unit of t in days the example system uses s 100 mm 3 day the storage node has a minimum capacity of zero maximum capacity of 1 e 6 mm 3 the initial volume v 0 is defined as 2 s ω these values ensure the storage node does not reach either minimum or maximum capacity during the simulation the solution to b 1 is given in b 2 b 2 v s sin ω t ω v 0 a comparison of the analytical result b 1 with the numerical pywr prediction is given in fig b 6 the simulation is performed for a single year a comparison is made between the simulated and analytical volumes absolute volumetric error is displayed in the lower panel the numerical results from pywr match very closely with the analytical solution b 2 simple system including storage the following example water system is based on a single catchment with a pumped storage reservoir it is loosely based on the supply system for london and utilises synthetic flow data derived from measured flow of the river thames the centre for ecology hydrology 2018 a schematic of the model is given in fig b 7 the model includes multiple demand scenarios and a dynamic demand calculation that imposes restrictions during poor reservoir states this dynamic behaviour is accomplished through the use of multiple parameter objects that implement the reservoir control curves the link to current reservoir volume and the look up of the current restriction factor to apply this approach is a common way of modelling demand side drought restrictions in the united kingdom b 3 d t α β 1 7 where β istakenfromtableb 2foreachscenario α 0 9 γ w ifwinter 1 2 γ s otherwise and γ w 1 00 if v t v m a x 0 8 0 95 elseif v t v m a x 0 5 0 80 otherwise γ s 1 00 if v t v m a x 0 8 0 90 elseif v t v m a x 0 5 0 75 otherwise the model includes a single catchment boundary condition where inflow is specified a downstream minimum flow requirement of 0 6 mm 3 day controls the pumped abstraction to the reservoir the maximum abstraction rate is 3 5 mm 3 day a baseline demand of 1 7 mm 3 day is combined with a monthly profile that applies 90 of baseline demand in winter september april and 120 of baseline demand in summer may august two flat control curves are defined as 80 and 50 of relative storage in the reservoir respectively the demand model applies 100 of demand when above both curves 95 winter or 90 summer of demand when between the two curves and 80 winter or 75 summer of demand when below both curves the variation of demand restrictions with time of year reflects the higher efficacy of such measures e g hose pipe bans during summer periods the complete demand model is given in b 3 the model is also setup to execute 5 scenarios simultaneously to for example reflect uncertainty the scenarios are defined tableb 2 during simulation all 5 scenarios are executed with the value of β in b 3 set appropriately for each scenario from tableb 2 the remainder of the model definition is unchanged and unaware of the scenarios effecting other parameters in the model creating this single model but with a single input value i e parameter object varying per scenario is a parsimonious way to perform scenario analysis without full copies of the entire model definition the model is executed for a 100 year period using a 7 day time step the stochastic flow data has been given fictional dates starting from the year 2100 the performance of the reservoir across the simulation is shown in fig b 8 the left panel shows a timeseries of the first 25years of simulation the right panel shows the storage duration curve across the full simulation for all demand scenarios this example quickly shows the effect of varying demand on the reservoir performance due to it being an illustrative example of pywr s functionality we do not explore the results in any further detail here this example includes dynamic behaviour through the use of pywr s parameters the model includes 7 nodes connected using 6 edges and 11 parameters a model simulation of 100years using a 7 day time step requires 5218 timesteps across 5 scenarios totalling 25 640 linear programme updates and solves fig b 9 shows the key performance statistics of a simulation without the writing of a full hdf5 output file the timing of which can be difficult to measure due to disk buffering we note however that writing a complete output file of all timeseries does have an adverse effect on overall execution time all results are based on a single run on the author s workstation intel i5 3570k the purely computational elements shown in fig b 9 demonstrate that not a single part of simulation algorithm is dominating execution time total simulation time is 0 82s giving a simulation speed of 31889 timesteps per second the calculation of the dynamic demand model b 3 via parameters is less than 7 time taken before of the simulation time solving the linear programme i e glpk s simplex routine is less than 20 solver lp solve of the simulation time the complete interaction with glpk i e updating row bounds updating objective coefficients solving and applying the results takes less than 48 of the simulation time other solver overhead involves iterating through the scenarios and does not directly interact with glpk s routines b 3 two reservoir system with optimisation the following example see fig b 10 for a schematic demonstrates the coupling of a pywr to a multi objective evolutionary algorithm moea the moea is used to optimise two competing performance objectives and discover an approximate pareto frontier the two objectives are defined in tableb 3 the model exposes a single pywr parameter as the variables in the optimisation problem this parameter represents a reservoir control curve which controls the operation of a transfer by changing the shape of the control curve the operation of the transfer pump is changed and thus the performance metrics are altered this example includes no constraints the users has the choice of using two different types of parameter a monthly profile or an annual harmonic profile each parameter type exposes a different number of variables to the moea the monthly profile parameter is defined by a single value for each month when used as a variable during an optimisation this parameter requires 12 separate floating point variables in contrast the number of variables required by the annual harmonic parameter is dependent on the number of harmonic frequencies n included the annual harmonic function is given in b 4 the parameter exposes the variables a a n and ϕ n to the moea therefore the total number variables is 1 2 n in this example two harmonics are used i e n 2 b 4 f t a n 1 n a n cos 2 π n t 365 ϕ n this example demonstrate the use of pywr s optimisation functionality by allowing the creation of two models identical except for the control curve parameter used the moea wrappers provided in pywr allow fast testing and modification of the optimisation formulation by dynamically reading the problem size number of variables constraints and objectives from the model itself this example presents a comparison of the two different control curve types solved using the same simulation model and optimisation algorithm we use the platypus hadka 2016 optimisation library to perform 10 000 function evaluations using the nsga ii algorithm a comparison of the approximate pareto frontiers produced is shown in b 11 see fig b 11 b 4 extension potential water energy model and a user interface pywr can be extended to simulate other networked resource systems such an energy or multi resource systems pywr is currently being used to undertake water energy system simulations the pywr model simulates the allocation of two resources water and energy in a combined system coupled via hydropower schemes that relate the flow of water to the generation of power using pywr a combined allocation procedure linear programme is created which ensures system constraints from both resources are maintained future work will improve the energy system simulation by including the power flow equations into a new linear programme based allocation algorithm the pywr library has been coupled with an online web based user interface see fig b 12 this interface allows users to construct and simulate pywr models with a graphical user interface internally this process generates a pywr input file json which is executed by a worker process in a remote server this web based system allows users to collaborate and share models easily between different locations and exploit cloud based compute resources for model simulations further details of the water energy simulation and web based user interface will be given in future publications appendix c benchmarking c 1 theoretical throughput the pywr algorithm 2 contains two major outer loops during an simulation the outer most loop iterates through each time step in a simulation an inner loop iterates through each scenario within each time step parameter values are updated once per time step i e inside the outer loop but a linear programme resolve is required for each iteration of the inner loop i e there is one linear programme solved for each time step for each scenario in addition before the resolve is executed the linear programme must be updated to reflect the current time step and scenario finally after the solve is complete the solution must be applied to the pywr data structures for the remainder of this section we shall refer to this complete process update solve and apply as a resolve critical to the performance of this algorithm is the overhead involved in executing each resolve the software architecture and technology used by pywr have been designed to remove as much of this overhead as possible the use of cython for the resolve process removes the overhead of pure python function calls and the direct interface to the glpk shared library removes as much overhead as possible in modifying and solving the linear programme itself we show that this has been achieved by comparing the throughput of a trivial pywr model with different linear programme solvers tablec 4 compares function call time of one solve of the allocation algorithm using pywr s default cython implementation with a pyomo implementation both using glpk for reference we also include a null implementation which does nothing and the equivalent call times for a standard python and cython function we show that the pywr s cython glpk implementation does not add significant overhead compared with the null implementation and is considerably more efficient than the utilising pyomo c 2 continuous benchmarking as part of the continuous development of pywr a set of standard benchmarks have been created the benchmarks comprise two models from the regression test suite a basic river thames and london model included as an example and a set of random networks the predefined models can be found in the pywr source code repository an algorithm is used to generate the random models for a given number of water resource zones and connection densities a water resource zone is described as a simple system comprising of an input link and output node the maximum flow of the input and output nodes are also randomly determined from normal distributions a number of connections or transfers between the link nodes of the water resources zones are also randomly created the algorithm creates connections up to a target density where 100 density would make a connection from every link to every other link during benchmarking the algorithm is seeded so that the random models are deterministically generated and therefore reproducible the benchmarks are utilised in a continuous benchmarking system airspeed velocity asv droettboom and virtanen 2019 the purposes of this system is to track the performance of the benchmarks through the development of the project using a repeatable process in a similar way to a regression test suite that provides protection against feature and behaviour regression the continuous benchmarking provides view of performance changes this is particularly important for tracking performance regressions and comparing different allocation algorithms and or component implementations here we do not present the complete asv results for all the commits in the project interested users can access the benchmarks and associated configuration from pywr s github project website however the system was used to create the benchmarks provided in section4 
26052,the pathway of a modelling project is commonly described as an adaptively adjusted chain of steps at which various decisions are made communication and documentation about these decisions are crucial to enabling reflection and adapting the pathway to changing circumstances such that well informed planning is required project decision making however often remains a black box it is rare to find reporting of dead ends alternative decisions and changes in decisions during the project this paper uses an integrated environmental management case study in iran to demonstrate the importance of reflective documentation and communication within the pathway we show how a pathway diagram incorporating some 14 symbols depicting steps decision forks options selected and alternatives actions communication and documentation can illustrate the role of communication within the project and identify lessons learnt we also encourage further work on application of agile project management and social science techniques to improve modelling practices graphical abstract image 1 keywords pathway reflection project communication documentation adaptation decision forks 1 introduction 1 1 modelling as an adaptively constructed pathway modelling and problem solving research can be thought of as an adaptively constructed chain of steps decisions and tasks forming a path gregory et al 1997 many different and alternative paths might be possible for a specific research purpose and the outcome of the process is usually dependent on the taken path hämäläinen and lahtinen 2016 moallemi et al 2020 some examples of steps in modelling and problem solving research include forming the team problem definition and model selection badham et al 2019 jakeman et al 2006 at each step many decisions are taken such as the specific quantities and scales of interest type of model to use the specifications of the model and the type and frequency of communication steps may also be packaged into heterogeneous phases with a project potentially only planned one phase at a time the main message of the pathway concept is that history matters hämäläinen and lahtinen 2016 a degree of path dependency is unavoidable and it is not possible to go back in time instead it is common and judicious to take an iterative approach revisiting the path taken and redirecting the next steps the iteration process provides a natural opportunity to compensate for any potential decision making deficiencies and adapt to any change in goals methods and available information the project pathway can be described as an adaptive spiral process fig 1 which might need several iterations to become more mature over time and meet stakeholder needs and project goals regular reflection can help ensure that necessary and timely iteration occurs a project should not be forced into a predefined protocol or procedure and analysts should be adaptive to changes and go in circles and branch out be prepared to go back reiterate and refine voinov and bousquet 2010 literature on adaptive management holling 1978 walters 1986 learning argyris 1977 and reflexivity popa et al 2015 emphasizes the opportunity to gradually improve outcomes and understanding by keeping in mind alternative choices within the project and being ready to switch to them when needed haasnoot et al 2013 successful iteration therefore requires a certain awareness and transparency around decisions iteration is particularly important in tackling complex problems the need for integration and tackling of implementation concerns bammer 2005 means that new information emerges and projects rarely go as originally planned knowledge of policymakers and other stakeholders needs to be amalgamated structured and shared along with input from multiple scientific disciplines in the modelling world this is typically approached in terms of integrated assessment and modelling iam a notable example being iam studies of water iwam which have increased over the last few decades zare et al 2017 iam has several dimensions hamilton et al 2015 in addition to requiring transdisciplinary integration of hydrology economics social science and software engineering iwam research usually involves stakeholder engagement and often purports to contribute to decision making there are at least three different audiences in any iwam research other scientists iwam team members decision makers and the public it is a challenge to enable effective flow of information among them there is a need for shared approaches tools protocols and reporting guidelines moallemi et al 2019 to report on facts findings and progress of the project and communicate within and between all three audience groups zare et al 2019a transparent communication is required to address different audience needs in a way that is accessible for everyone in order to obtain a social license to operate badham et al 2019 1 2 the role of communication and documentation in iteration when communicating about or documenting a project the process of iteration is often skipped over and persists as a black box it tends to be regarded as the messy underbelly that distracts from a result investigating that black box is however crucial to identifying opportunities for improvement decisions are made based on the context resources and constraints to achieve a specific aim and purpose the decisions made at each decision fork justify the next step or direction of the process and shape the pathway lahtinen et al 2017 moallemi et al 2020 decisions are made under the influence of a variety of behavioural phenomena such as biases beliefs heuristics and values bbhv glynn et al 2017 resources restrictions trade offs and limitations related to both researchers and the research process hämäläinen 2015 it is normal for modellers to make decisions based on their professional opinion which is unavoidably biased krueger et al 2012 this does not mean that decisions are purely the interpretation of one person but rather that credibility comes from agreement among experts inter subjective agreement rather than objectivity as argued by voinov et al 2014 in ten commandments for a socio environmental modelling agenda modellers should stop pretending that applied science and models are always objective and value neutral therefore projects should document and communicate the implicit decisions in their modelling and discuss the assumptions and values transparently this process of establishing the path of the project bears similarities to an emerging body of work in the field of behavioural operations research which empirically studies how operations research is performed franco and hämäläinen 2016 hämäläinen et al 2013 white 2015 for example with analyses of workshops using ethnomethodology franco and greiffenhagen 2018 in the context of team based transdisciplinary research the essential importance of iteration and reflection means that communication and documentation then take centre stage transparency and reflexivity require an exchange of information to ensure the right people are aware of influential issues affecting the study at the right time for example mauthner and doucet 2003 give a practical example showing how regular meetings within a research group improve their ability to be reflexive about their research communication may be verbal or visual with face to face or virtual meetings or involve reporting by email flyers papers or technical and management reports janse 2008 studied communication between forest scientists and policy makers in europe and highlighted that policy makers asked for shorter and easy to comprehend formats and that scientists need to invest in improving their communication ability documentation is a common form of communication which can be permanent and referred to easily understandable documentation allows for reflective assessment of a process and outcomes increasing transparency recognising limitations krueger et al 2012 and gaining the stakeholders trust voinov and bousquet 2010 often documentation happens after the modelling is finished describing the model code providing an analysis report and helping the user to use the model through presentations tutorials and user guides calder et al 2018 guidelines for proper documentation exist making sure that models can be fully checked and re implemented improving communication amongst modellers and facilitating critical scientific evaluation cartwright et al 2016 approaches such as transparent and comprehensive ecological modelling trace try to standardise documentation grimm et al 2014 documentation of provenance in particular is common in the fields of art digital libraries data management and workflow systems for reproducible science moreau et al 2008 and its importance has been recognised in integrated environmental modelling zhang et al 2017 model provenance refers to the description of the origin creation propagation process and responsible person for data models zhang et al 2017 in addition to data provenance systems tools bochner et al 2008 ikeda and widom 2010 provenance harvard 2019 wikis ticket issue systems and version control are commonly used to document processes and facilitate team communication in modelling software engineering and data analytics however these efforts mostly focus on the model and its specification in bringing together multiple models disciplines sectors and stakeholders for a purpose integrated modelling research is more than a model and the model is more than software given the importance of qualitative and semi quantitative approaches involving stakeholders voinov et al 2018 therefore communication and documentation tools also need to stretch to other parts of the project such as goal setting stakeholder analysis and selection with a focus on exposing the black box of the modelling process and targeting all three audience groups scientists decision makers and stakeholders documentation should go beyond describing what has happened and speak about the reasons behind each decision in the process of modelling badham et al 2019 also it should use a common language that is easy to use and minimises misunderstanding for everyone with the ability to show failures and lessons in software engineering agile project management techniques tackle the problem of connecting users and technical requirements ambler 2002 and suggest regular reflection activities to improve efficiency but these techniques have not yet been commonly used in the integrated assessment and modelling literature current documentation style in modelling practices usually has poor documentation of the decisions and the reasoning behind them müller et al 2013 although modelling has a long history documentation is still a problematic issue to be seriously addressed by modellers rahmandad and sterman 2012 there is therefore a need for guidance about reflexive communication how can researchers document and communicate in such a way that they receive useful feedback that helps shape their next steps and achieve better project outcomes given that this communication sits in the messy underbelly of the project clear guidance is elusive given the importance of alternative pathways practitioners need to speak about why why they used this approach model method data and scenarios and why not other options what were the other options but how should that conversation be handled our suggestion here is to treat communication about the project in the same way we treat the project itself as an iterative task that requires reflection and can be improved over time 1 3 improving iteration through reflection reflection is already widely recognised as an important concept reflection on the pathway as a whole is particularly vital hämäläinen 2015 in cases where the problems are complex participatory and include multiple sources of uncertainties lahtinen et al 2017 such as typically occurs in environmental modelling post normal science suggests that quality assurance of processes and the resolution of issues be approached through debate and dialogue within the extended peer community funtowicz and ravetz 1993 several reflective approaches have been developed for quality assurance in modelling and knowledge processes such as nusap numeral unit spread assessment and pedigree van der sluijs 2006 and the uncertainty guidance of rivm mnp van der sluijs et al 2004 a theory of change approach is similar whilst typically focussing on the intended rather than past pathway taplin and clark 2012 in modelling with stakeholders reflection is often seen as crucial to avoid forcing stakeholders into a fixed protocol or procedure recognising the need to adapt to differing stakeholder motivations voinov and bousquet 2010 unexpected changes are possible in goals and priorities and consequently in the pathway obtaining reflection on the pathway helps to undertake this iteration at the right time and place and avoid the consequences of skipping reflection and taking a poor path there is substantial literature on reflection theory and methods in many disciplines such as medical practice jarvis 1992 educational philosophy reynolds 2011 and recently in interdisciplinary and transdisciplinary research roux et al 2010 wickson et al 2006 as well as design research dalsgaard and halskov 2012 the definition of reflection may differ among disciplines but commonly involves thinking about past or ongoing experience to evaluate inform and improve future choices decisions or actions reynolds 2011 within the existing body of work there is not yet however a demonstration of how reflection can be used to scrutinize project communication specifically and identify how communication could be improved to achieve better project outcomes given the recognised benefits of reflection the importance of communication and the need for practical examples of reflective communication this paper provides an analysis of communication within an integrated environmental management case study in iran we make explicit the team s actual project pathways including the role of documentation and communication as well as the alternative options pathways that were considered during the project and the other options that could have been followed in hindsight we emphasise that the approach taken is to reflect on the case study as reflective practitioners schon 1983 the analysis method relies on reflection by practitioners two of the authors were closely involved in the case study using soft systems modelling avoiding the use of formal interview or document analysis methods minimises the additional skills required for other reflective practitioners to replicate this method while recognising that the focus of the method is therefore on supporting reflection by practitioners rather than performing a rigorous social science analysis we analysed the project by using the technical reports management documents meeting notes project management documents and authors reflections there are several advantages to using a soft systems approach notably that 1 it gives ownership of the evaluation process to the practitioner as opposed to an external objective evaluation 2 it allows the practitioner to focus on problems in the process that they considered important rather than getting bogged down in the detail of providing a comprehensive assessment and 3 as a modelling based process there is less of a barrier to uptake than forcing the use of formal social science methods while we do not have formal social science training we are in fact arguing that modellers and other practitioners should be engaging in this type of work and by our example we would like to encourage other modellers to similarly reflect on how they can improve their processes this case study demonstrates two key ideas firstly reflective communication is valuable that is communication is most effective when it triggers reflection on decisions taken secondly reflecting on communication level of detail tool frequency and involved parties is also valuable examining past communication can help identify lessons learnt to improve project communication in the future fig 2 illustrates a case in which a meeting that obtained reflection from other parties in step 1 allowed switching to an alternative decision step 2 ensuring project effectiveness and efficiency by avoiding a dead end and avoiding unnecessary work step 2 if the meeting had not occurred reflecting on the pathway after the fact might have identified that such a meeting could be beneficial in future by expanding on these ideas in a real case this paper demonstrates how reflective communication about decisions along the research pathway helps make projects more adaptive and agile and helps achieve project outcomes more effectively and efficiently 2 methods 2 1 documenting pathways to investigate the role of reflective communication we adopted a path perspective lahtinen et al 2017 existing discussions of modelling processes tend to take quite an idealized view and do not look at how modelling decisions and iteration occur in practice in contrast using pathways involves essentially building a model of what happened and could have happened describing the steps taken in the project and the decisions and circumstances that led to that choice in this paper we take a soft systems conceptual modelling approach we use graphical methods to capture the team s perspective of what happened rather than objective truth this type of soft systems perspective has a recognised history in dealing with multiple perspectives and associated biases elsawah and guillaume 2016 we identified two pathways the first pathway is the actual path taken through the project including other options considered at each decision fork it also shows the changes in path due to dead ends failures and changes in project aim s the second pathway shows the lessons learnt other options that could have been taken to improve the project the pathway diagram is intended to provide a systematic setting to help identify those lessons learnt and potential improvements to identify how we could improve the project we went through a process of constructing a counterfactual pathway for achieving our desired aim if we accept that there are many roads to rome say the actual pathway is the road that practitioners took and the lesson learnt pathway is one of the roads that might have reached rome more quickly or more safely there might be multiple possible lessons learnt pathways however just one of them is identified in this paper as an example to demonstrate the importance of reflective documentation and communication we aim to describe identified points in the pathway where 1 communication successfully led to iteration or avoided the need for later iteration 2 communication would have been beneficial or could have been improved and 3 communication improved the usefulness utility and benefits of the final product report and results 2 2 the pathway diagram tool and its use although existing research describes the idea of a pathway hämäläinen and lahtinen 2016 lahtinen et al 2017 lahtinen and hämäläinen 2016 it does not propose any specific tool for visualising pathways and sharing them we developed a pathway diagram as a graphical tool to document the steps taken in a project and facilitate communication and reflection it is a thinking reflecting and documenting tool to open up space for speaking about other options potentially contributing to both project evaluation and project management documenting the pathway taken helps to prise open and expose the otherwise black box nature of the project and thereby increase adaptability transparency and trust in the project the pathway diagramming approach and its components are adapted from tools used in other disciplines particularly elements of activity diagrams the latter describes workflows by showing activities and actions with support for choice iteration and concurrent execution of activities dumas and ter hofstede 2001 including associations between activities conditions and constraints activity diagrams are classified in the behavioural category of the unified modelling language meaning that they capture the dynamic behaviour of a system over time in the pathway diagram we use the idea of showing the flow but we focus on the decisions and communications during the process leading to a different set of core components table 1 this diagram could be a useful tool to facilitate the communication between stakeholders modellers and decision makers in order to reach a shared understanding of the modelling process zare et al 2019b our starting point is that a path is a sequence of steps taken in a problem solving case hämäläinen and lahtinen 2016 and where steps encapsulate a coherent set of activities with a set goal the goal of each step should be set adaptively and flexible so that it can be revised in agile project management literature the process is split into sprints or phases which we consider to be steps here many decisions are made at each step and at each decision fork there might be more than one option to choose the analyst might need to take some actions to support decisions and might communicate and document the decisions and their results therefore we regard symbols as indispensable to portray these elements steps decision forks options selected and alternatives actions communication and documentation table 1 shows the main elements of the pathway diagram and a brief description of each element based on the iterative construction of the diagram these 14 specific elements are needed to capture the necessary aspects of the pathway in this study documentation of the pathway evolved iteratively through reflection by the authors at first we identified the steps phases of the project then we added main actions in each step next we identified the decision forks in each step in each decision fork we show the alternative choices considered after achieving an overall view of the pathway we added detail about meetings and documentation to capture the communication that led to each decision gradually refining the pathway provides the opportunity to see obvious alternatives first and stop before reaching reflection fatigue decision forks or decision points occur when people choose decide or agree on objectives targets tools resources approaches or any other activities to identify a decision fork we used an evidence based analysis grounded on text and interviews text documents from the project by definition show documented options whereas interviews usually add undocumented options to the pathway diagram the emphasis is on capturing aspects of the path that are important for reflection rather than trying to document all decision forks we focused on documenting decisions that changed the pathway or affected the efficiency or output of the project these decisions need to be documented and reflected upon to avoid relying on intuitive system 1 thinking which is reliant on mental operations and shortcuts such as heuristics and biases kahneman 2011 in addition there is a need to document and reflect on decisions whenever intermediate results are obtained any new learning is gained or new data report resources become available similarly the aim of mapping the pathway is not to show all the possible alternative options as practitioners might not be aware of them all some options are known unknowns where there is a lack of knowledge there might also be some options in other areas domains and disciplines of which practitioners are not aware unknown unknown options or that practitioners used unconsciously i e unknown known options kerwin 1993 the actual pathway will not include alternatives that were unknown to the team however practitioners should map all the known known options the ones that practitioners are aware of and that were considered and evaluated it should also be noted that if there is no direct arrow between actions events it does not mean there is no effect and connection only the main links are captured within the modelling process recognising available options and making decisions can be influenced deliberately or unintentionally by a variety of human factors originating from cognitive behavioural and mental frameworks as well as the beliefs biases heuristics experience and values of stakeholders moallemi et al 2020 identifying the influencing factors and controlling their effect is a complicated process and might need expertise that is not available in the team increasing awareness of human factors and their effects on the judgement is a critical step to reduce the risks of negative impacts and taking poor paths nickerson 1998 glynn et al 2017 suggested an adaptive framework to generate information and recognise human factors to help manage their impacts consistent documentation and immediate reflection help to capture the choices made and their rationale identify cases where a decision is not consistent with the objectives assumptions and available information and increase attention paid to human factors to reduce their negative effects glynn et al 2017 moallemi et al 2020 the pathway diagram facilitates these documentation and reflection processes along with tracking the decisions and considered options the pathway diagram could include dates of events as well but they are not always vital to the storyline of the project we consider there to be two fundamental rules in diagramming a pathway first paths are one way so it is not possible to go back in time even if a path has failed it is impossible to go back to the previous state because the pathway has reached a new state in which practitioners know one of the options will not be successful in achieving its intended aim in other words failure of an option increases knowledge about other options the second rule is that only one decision solid arrow should emerge from a decision fork with other options shown as alternatives dashed arrows the solid arrow captures the way of choosing a path forward moving into a new state by closing other options including returning to the previous state where multiple decisions are made they can either be presented as one collective decision or split into separate sequential or simultaneous decision forks if one pathway successfully reaches the expected outcome it does not mean others will not we should acknowledge the fact that any modelling path can have different outcomes and there might not be just one ideal path hämäläinen 2015 for example in fig 3 suppose path 3 thick line is the pathway that actually materialised other lines embody alternative pathways some pathways still reach the same endpoint and provide the same output some others might reach a different endpoint but still achieve the expected outcome some alternatives might lead to failure the successful pathways might be different in the character of the outcomes different level of trust outcomes reliability and others or in the path taken which might be important in terms of efficiency and resources by success we simply mean achieving the intended or ideal goals of a project in terms of both outputs of a project and the process to achieve them a holistic view of success in modelling projects means the ability to embed the modelling in a social process that connects scientists decision makers and stakeholders and achieves impact in accordance to its purpose which may vary from a shared understanding of a problem to policy analysis badham et al 2019 hamilton et al 2019 not being able to achieve this outcome is a failure situation for the modelling project failure and success are therefore not absolute terms goals might be different for each group of stakeholders and can be revised during the project the setting and revising of goals is affected by many factors such as acquisition and processing of information judgement and evaluation system 2 thinking and biases beliefs heuristics and values system 1 thinking glynn et al 2017 effective modelling involves a process of practices decisions and actions which suit the context of the problem badham et al 2019 and can take on different forms with the relevant criteria for suitability depending on the project hamilton et al 2019 various authors have described the many dimensions and perspectives on success including merritt et al 2017 who identified 33 different factors to assess the success in water resource projects mcintosh et al 2011 who defined success of environmental decision support systems in terms of best practices and hamilton et al 2019 who covered 32 criteria specifically focused on effectiveness of environmental modelling in this paper an iwam project is considered effective if it achieves the intended outcomes and efficient if it uses its resources as intended zare et al 2019b demonstrate a hypothetical pathway example to illustrate the way the explained components and rules can be used they used the pathway diagram to construct a customized participatory modelling pathway 3 application zaribar lake case study iran the zaribar project analysed changes in land use and their trends type intensity rate and location using remote sensing techniques the aim was to help identify the reasons for the changes in a lake and facilitate restoration planning build trust between stakeholders and the research team and achieve a shared understanding of land use changes this project was part of an overarching project to restore zaribar lake in kurdistan province iran in 2012 a restoration study project started following reports that showed the lake faced some serious issues such as a change in its physical shape increasing pollutants and pollution load reyahi khoram and hoshmand 2012 sharifinia et al 2013 sedimentation karbasi abd and bayati 2008 excessive algae lack of dissolved oxygen odours fish kills and increases in floating plants and other plants such as reeds typha latifolia and cyperus longus there was an element of panic among the local people who strongly believed that plants were covering the entire lake some news reports producing dire statements such as one that the lake will be dead in 5 years residents started fires and burnt vegetation as part of their effort to save the lake these actions however only worsened the lake s condition while scientific research is commonly used to back up environmental planning it is often undertaken in isolation from affected local communities and stakeholders this isolation can compromise the likelihood of success for restoration projects if the results of the research are not entirely accepted or used by decision makers and stakeholders and therefore will not change individual or community behaviour and beliefs the restoration project led by mahsab shargh consulting company therefore included stakeholder involvement intending to help the department of environment to identify the drivers and causes of the problem and design and implement a practical solution in the first meeting the restoration project team decided to start a remote sensing rs project to study land use change this paper focuses on the rs project and fig 4 gives an overview of the intended project plan the project and problem scope were defined in a meeting with the restoration project team the project plan included a provision for regular documentation meetings and progressive feedback the description of the actual pathway followed fig 5 includes other options considered at each decision fork and changes in the direction of the project pathway we focus here on the main decision forks and associated lessons learnt regarding communication and documentation the supplementary materials provide a complete analysis and description of all phases 3 1 actual pathway problem definition ultimately occurred through three separate meetings the first defining the project scope around remote sensing of land use and its change the second discussing further details especially the spatial boundary of the analysis and resulting in a project plan while the third meeting obtained approval from the steering committee fig 4 phases 1 and 2 land use change detection involved many sub steps and decision forks initial analysis specification informed selection of imagery allowing preparation of images and the actual image processing the process took six months and project status reports only resulted in approval to continue without alteration of the project pathway unsupervised classification is fast efficient in time and budget and does not require fieldwork castellana et al 2006 therefore unsupervised classification was used as a first step to obtaining a general impression of different land use classes in the basin and their changes once initial unsupervised classification results were obtained a meeting was held with the restoration team it was observed that the results could not answer all questions raised by the restoration team the previous meeting had chosen to use a spatial boundary based on surface hydrology rather than a more extensive overarching boundary including groundwater and areas with socio economic impacts see fig s13 in the supplementary materials produced in the project as part of the discussion in phase 6 use of the hydrological boundary meant it was not possible to investigate the role of urban expansion and agricultural development one of the hypotheses for the restoration plan was that agriculture development and city expansion had impacted on the lake situation so the socio economic boundary would have been vital to detect changes in these growth elements land use characteristics also affect groundwater infiltration and levels so the groundwater boundary is essential as well one of the primary water sources for the lake is from the springs in its bed which makes groundwater changes critical to consider two options presented themselves either restrict the scope of the restoration project or change the boundary which meant returning to step 2 1 resulting in three more months of work and associated costs restricting the scope of the restoration project would still have provided useful outcomes about the changes in the water body and floating plants of the lake and land use changes in the lake s hydrological basin which provides information about water quality and quantity changes however it was decided that the additional costs were worthwhile fortunately the selection of images was not affected because the landsat images used covered both boundaries however it was necessary to redo the processing phases and steps data preprocessing based on the new boundary and redoing the initial unsupervised classification remote sensing analysis after this process the meeting with the restoration team approved the results allowing further supervised classification and validation to continue the project members decided to be upfront with the steering committee about the issues with the spatial boundary even before the meeting the problem was well documented and raised as a point requiring reflection surprisingly the steering committee asked to have the results for both boundaries in the final report the hydrological boundary highlighted changes to the lake and nearby villages and it was useful in order for the environment department to develop a policy for the area the overarching boundary was felt to be useful for the agriculture department environment department and city council planning the project results were approved by the steering committee to be used as a basis for the restoration planning project and it was useful for all involved to understand the different roles of the hydrological and socio economic and groundwater boundaries overall the project took four months longer than expected with an associated cost results for the hydrological boundary could have been provided using the analysis with the overarching boundary instead of undertaking separate analysis so if the overarching boundary had been selected first the team could have avoided the extra cost and produced the same outputs 3 2 possible improvements lessons learnt while the project did successfully redirect the analysis so that it used a different spatial boundary it is worth asking whether the extra cost could have been avoided could communication and documentation have been improved reflecting on the pathway by the authors including a member of the remote sensing group resulted in four possible changes that might have avoided this redirection summarised in table 2 and highlighted red items in fig 6 3 2 1 possible improvement no 1 dummy results the redirection of the project in the meeting with the restoration team was triggered simply by presenting the results while it saved time to obtain feedback from unsupervised rather than supervised classification results it would have been possible to get feedback earlier in the process by presenting dummy results for example in phase 2 2 after critical decisions had been made but before the time consuming collection and analysis of the satellite imagery a dummy or synthetic result is a mock up of the results showing what the eventual outputs might look like dummy results would therefore have shown the difference that the options at a decision fork would make in the final output and highlight the consequences of each choice of the boundary showing dummy results fosters critical thinking and reflective practice this helps decision makers in this case other modellers make a more informed decision a closely related alternative would be to demonstrate the effect of the boundary using another case study in order to highlight the importance of this type of decision fork in general terms and the consequences of each option 3 2 2 possible improvement no 2 risk analysis given that one team member had already raised the issue of boundary selection in phase 3 another possible change could have involved emphasising the stakes involved i e the risk trade offs involved in selecting the two boundaries it was a clear risk that selecting the smaller boundary might leave out areas that could be of interest later selecting the larger boundary reduces this risk at the cost of extra time structured risk analysis is a well established approach for thinking about the uncertain consequences of decisions rotmans 1998 in this case the decision depends on how much extra time is worth committing for an uncertain benefit later discussing this trade off explicitly might have altered the decision more generally a risk analysis could in principle have been used in the initial specification of the analysis phase 2 1 risk analysis would have systematically mapped out consequences of decisions in the planned process which might have surfaced the spatial boundary problem however such risk analyses can take some time to do well such that it is understandable that most projects including this one decide to rely on the judgement of critical members of the team at the risk of groupthink or being dominated by the supervisor s opinion risk analysis can improve judgements and decision making but it is affected by many biases which need to be reduced or taken into account montibeller and von winterfeldt 2015 there are however success stories for example stacey et al 2017 assessed the use of a decision aid to support risk analysis of benefits and costs of the decisions the suggested decision aid documentation method helped people to make more informed decisions with more accurate risk perceptions while being more precise about their values and taking a more active role in decision making 3 2 3 possible improvement no 3 interactive and reflective documentation the project did have regular project management and technical reports but these did not prevent taking the wrong pathway at the right time the reports could have been prepared in a way that would have provoked more interaction and reflection the reports could have included specific questions about technical decisions and their implications inviting the readers of the reports to encourage questioning of decisions and reflection rather than merely seeking approval the form of feedback could also have been reconsidered the reflection on the reports usually was a series of comments or suggestions from the steering committee which was sent to the rs team about one month after submitting reports these comments sometimes focused on minor issues and overlooked major ones for example suggesting a different map resolution colour or name instead of approving or questioning the boundary sending a set of questions with the report might therefore have helped to focus reflection on key decisions overall the suggestion is to have regular interactive and reflective documentation going beyond common management reports to make sure that stakeholders are aware of and agree with the decisions and events burgess et al 2007 interactive communication with opportunities to question claims reflectivity is usually valued by decision makers because this type of communication can support individual learning develop shared understanding and lead to more useful outputs o connor et al 2019 as part of this interaction the timing of feedback could have been revisited given that project work continued while waiting for comments which limited the ability to influence technical decisions the reviewers schedule could have been organised to provide a shorter turn around time or a video meeting when delivering each report could have accelerated the reflection time in general a team s schedule can also be organised to accommodate feedback it can be possible to pause a project to wait for feedback or to switch to other tasks within the same project in this case it was necessary to continue working to meet project deadlines in computer science this strategy is called optimistic concurrency control it is assumed that concurrent work feedback will not change the path taken so a small risk is accepted of potentially having to repeat work this risk taking appears to have been justified in this case as no feedback from these reports resulted in work having to be repeated 3 2 4 possible improvement no 4 multimedia communication meeting the last possible change in table 2 has a more nebulous goal of improving the effectiveness of communication the issue of the spatial boundary was already raised in the problem definition meeting between the rs team and the restoration team the decision fork was obvious for the team but the importance and impact of each choice were not and so it was not considered seriously perhaps if a different means of communication had been used the outcome would have been different a multimedia communication meeting virtual or real could have used a variety of tools beyond traditional reports to communicate about the decision and ask specifically for reflection it is important to use the right tools in the right phase place and time to maximise success we cannot be sure what specific means of communication might have been effective in this case it may have been useful to use more diverse and creative media tools and forms of communication such as interpersonal discussion communication monologue verbal communication speech presentations movies games documents e g websites emails scientific articles and briefs janse 2008 2006 these give us the ability to indicate intuitions conceptual models and assumptions glynn 2015 guldin et al 2005 suggested having regular and effective communication through multiple different channels using a multimedia tool to present a dummy result might have had a greater cognitive impact though the result is likely to depend on the precise form of the multimedia and the characteristics of the audience yildiz and atkins 1996 as an example when deciding on the boundary phase 2 2 the team could have used google earth engine as a tool to facilitate communication and highlight the difference between having marivan city in the boundary or not fig 7 shows satellite images for 1992 and 2016 in the zaribar region in the top row the city of marivan is covered whereas in the second it is visible which provides a visual impression of the substantial change in the area covered by the city in a meeting this could have been made more striking by visualising the changes using the timelapse tool in google earth engine 1 1 check the following timelapse https earthengine google com timelapse v 35 54879 46 13937 11 305 latlng t 0 44 ps 50 bt 19840101 et 20181231 startdwell 0 enddwell 0 just making this change more obvious could have caught the attention of participants from other disciplines e g members of sociology hydrology and ecology teams highlighting that the increase in urban area is likely linked to increased population a social impact resulting in more sewage discharging into the lake water quality impact thereby contributing nutrients for the spread of the floating plants across the lake ecological impact this is a potentially crucial part of the story regarding the area covered by floating plants which might have captured the interests of members of the sociology hydrology and ecology teams so a larger boundary is then clearly worthwhile 3 3 lessons learnt for this paper these lessons learnt firstly demonstrate that reflective communication can play a crucial role in the success of a project while the communication used did achieve redirection of the pathway and ensured the project was effective further improvements could have increased its efficiency and reducing costs documentation might not be useful by itself if subject to insufficient reflection the documentation needs to be capable of triggering iteration in steps and redirection of project pathways secondly this paper demonstrates that reflection on communication regarding the project pathway can help identify potential improvements in project communication for the future team members who raise an issue and feel that reflection was not achieved can use a variety of communication strategies to ensure the issue is addressed thirdly then the lessons learnt highlight that reflection on documentation used within the project itself might have added value thinking critically about how decisions are shared and feedback is sought can help identify more effective communication strategies and more effective iteration 4 discussion this article has attempted to demonstrate how the concept of a pathway can be used to help assess communication within an integrated environmental management case study in iran a newly developed pathway diagram incorporating symbols that indicate steps decision forks options selected and alternatives actions communication and documentation was used to establish the intended and actual path of the project and reflect on possible improvements the case study assessment is intended to provide a proof of concept demonstrating that reflection on communication can provide useful insights for future improvement as well as a concrete example of the value of reflective communication in the next sections drawing on related work we propose three implications of our study for 1 documentation section 4 1 2 reflective practice section 4 2 and 3 effective project communication section 4 3 then we discuss the limitations of our pathway approach and requisite future work to extend its value section 4 4 4 1 implications for documentation to support reflection writing reports manuals and papers about models has always been a standard method of communication nevertheless there is still limited documented knowledge on the modelling process the role of influences in determining this process and practices to employ badham et al 2019 currently there are two common forms of documentation at two levels of detail management and technical fig 8 at the management level documentation usually contains information about the project management workflow timeline finance and events and sometimes also a summary of the results of the project according to badham et al 2019 technical reports usually describe detailed information about the model such as input output executable and source code scientific justification of assumptions calibration and validation the primary aim is to establish scientific credibility and provide support for future development a report might also include recommendations and conclusions of the research user guidance documentation or manuals can be considered a type of technical report the guidance documents focus more on helping users to run the model and interpret the model outputs they usually include a clear interface description and tutorials there is however a gap between these two levels of detail which is why modelling project decisions tend to remain nebulous and unformulated a new level of documentation that targets all stakeholders decision makers experts public etc which might be referred to as pathway documentation fig 8 is crucial if we are to expose the black box of the process and aim to make it clear and transparent pathway documentation would report the detail of decisions alternative decisions dead ends and changes in decisions during the project to support reflection documentation should be compiled whenever a decision is made which will affect the taken pathway lahtinen et al 2017 for example the boundary decision fork in the zaribar case study whenever stakeholders need more clarification or their reflection would be valuable such as in phase 6 in fig 5 and at the end of each step and phase to obtain consent and approval from stakeholders our pathway diagram can be used as a tool for this type of documentation to facilitate the process of obtaining reflection and provide a holistic appreciation of the pathway of the project and its locked in dead end and successful paths the diagram links the technical and management level of detail together by reporting both the overall process and the individual technical decisions the zaribar case study suggests that instead of emphasising well documented models badham et al 2019 we need to emphasise well documented modelling processes and a focus on decisions and pathway documentation can help achieve this documenting paths can support evaluation and help extract best practices popa et al 2015 some authors have touched on related ideas but it seems that pathway documentation has rarely been implemented to document a modelling project and there are some key differences between these related ideas table 3 shows an overview of different approaches of decision documentation with a summary of their aims and differences relative to our proposal 4 2 implications for reflective practice as described by finlay 2008 reflective practice is the process of learning through and from experience towards gaining new insights of self and practice practitioners examine the assumptions underlying their everyday practice and critically evaluate their responses and decisions in order to gain new understanding and so improve future practice reflective practice involves the interaction of three essential skills 1 critical thinking to challenge the assumptions and context 2 self awareness to evaluate the knowledge derived and 3 reflection to promote self and social awareness and social action our focus in the zaribar case study was just on reflection popa et al 2015 argued there is a need to complement classic approaches in sustainability research with more reflexive and transformative approaches which require design mechanisms to transform values practices and institutions through stakeholder participation rather than common consultative participation the zaribar case study provides an example of the role of reflection versus consultation and demonstrates that adopting a reflective communication approach can help deliver a more efficient and effective project our work also suggests that in a team setting it is useful to make explicit the role of communication and documentation in the reflective cycle for example the one suggested by gibbs 1988 which has become a standard model for reflection in education research this cycle proposed that theory and practice enrich each other in fig 9 we therefore use the gibbs cycle to help describe the idea of reflective documentation the cycle starts with the project documentation phase describing what has happened and needing critical thinking to be informative the next steps are thinking evaluating and analysing which make up the reflection phase in the conclusion phase the team chooses between changing the pathway or approving it and moving on reflective documentation is therefore the lynchpin linking previous actions to new plans differentiating it from existing forms of documentation as discussed in the previous section fig 9 shows that reflective documentation fits easily within the existing literature there are some critical points about reflection that therefore also apply to reflective documentation the reflection step might be performed by individuals or as a group and it can be done by the research team stakeholders or a third party reflection by the research team amounts to a self reflection hibbert et al 2010 examining one s own decisions and actions to control the effect of motives biases and values behind them and to evaluate those decisions based on the project aim a notable example of a third party reflection involves appointing a devil s advocate or even a parallel modelling team from the first step of the project to challenge the approaches and the decisions of the primary team see e g glynn 2015 avoiding groupthink and reducing the risk of being stuck on a poor path moallemi et al 2020 to maximise the advantages of reflection practitioners need to create an atmosphere of trust and reduce the fear of making mistakes or speaking about them this environment can promote learning creative thinking and prevent anchoring to standard methods lahtinen et al 2017 reflection is not limited to comparison with the past as in this paper reflection is more generally a comparison between the current pathway and that desired by practitioners visioning practices could therefore help in defining what is success in the project and clarify our vision of success kallis et al 2009 visioning practices generate a common goal and a clear image of the future and offer an opportunity for meaningful reflection and change in decisions throughout the pathway helping to motivate purposeful action kallis et al 2009 consequently the role of pathway diagrams in a visioning context may be a fruitful future research topic 4 3 implications for effective project communication effective communication between team members and between the team and stakeholders is a vital element to ensure the success of the project and improve its efficiency effectiveness transparency and trust making we consider communication to be effective if the decisions assumptions resources and knowledge become clear for all the team members stakeholders and decision makers the process of reflective communication and documentation of the zaribar pathway was designed to examine its impact on the effectiveness of communication in shaping project outcomes based on this case study we recommend having a concrete but iteratively updated plan for reflective documentation and communication about decisions and events in the project consistent with more general calls for communication plans and improvement in communication generally giovanni 2006 janse 2008 in particular it is likely to be useful to explicitly address resourcing and scheduling of reflective documentation while providing sufficient flexibility for team members and stakeholders to discuss the issues likely to be important to them at the time discussion is needed it should be emphasised that communication is an interactive and reciprocal activity which includes an action and a reaction janse 2008 so the receiver of the information should not be passive and should be actively engaged janse 2006 o connor et al 2019 to make sure that real feedback is obtained and the message is clearly received through two way communication the project is more likely to achieve mutual understanding and trust between stakeholders and the research team this can for example be facilitated by sending a list of reflective questions with any document to facilitate engagement and draw attention to team member concerns planning of communication is of course already common in many projects for instance alvarez et al 2010 developed participatory impact pathways analysis pipa for reflective documentation and communication they suggested regular reflection and adjustment workshops with key stakeholders during the project to promote learning and provide a framework for action research on processes of change our recommendation for active planning builds on this by emphasising the importance of iteratively striving to improve reflective documentation practice as this case study has shown the pathway diagram can provide a basis for both planning and evaluating reflective documentation and communication and potentially helping to develop mutual trust which are all key requirements for effective communication janse 2008 practitioners should be aware of the risk of over formalising the reflection process and making it a ritual dance but at the same time putting aside the required resources and time and improving awareness of its necessity to allow the reflection happens on time likewise effective communication is a critical issue in knowledge co production projects in natural resource management where the legitimacy of a decision support process depends on fairness transparency and inclusivity in all aspects of processes and not just basing decisions on science transparent documented processes of determining knowledge inputs and technical approaches are also valued o connor et al 2019 agile project management also values many of the same principles that we have argued for in this paper including the importance of interactions over processes and tools and having regular intervals when the team reflects on and adjusts the process in order to improve effectiveness compared to blindly following a plan ambler 2002 it is essential for the practitioners to have the humility to admit that you may not know everything that others have value to add to your project efforts ambler 2002 so it is necessary to have early and continual focus on users early and frequent feedback from stakeholders and to iterate to reach the best design chamberlain et al 2006 the importance of the team beck 2004 coherent teamwork chamberlain et al 2006 and effective communication within the development team ambler 2002 beck 2004 are emphasised as well despite all valuable recommendations in agile research projects have mixed implementation results and tend to pick and choose the agile principles they implement sletholt et al 2012 and not all scientists who develop and use scientific models have a high level of knowledge about standard software engineering concepts hannay et al 2009 in some agile projects pressure on iteration may also mean that reflection opportunities are missed necessitating the integration of reflective practices schon 1983 into agile methods in more practical terms babb et al 2014 in this paper we further emphasise that care needs to be taken to apply these ideas to the project as a whole e g problem definition step stakeholder involvement translating the model result into practical policies properly integrating modelling and software into the big picture rather than treating it as a separate part we also emphasise that encouraging reflective communication provides a solid foundation to work towards planned projects that are flexible so as to allow adaptation and adjustment to the new situation realising improvements through iteration 4 4 limitations and future work as the first study examining the role of iterative communication and documentation in the context of integrated environmental modelling our experience suggests that further studies are worthwhile and other existing techniques may find a use a soft systems conceptual modelling approach to documenting project pathways as used here is a conceptually simple approach focussing on identifying steps decision forks options selected and their alternatives although the process of drawing the diagrams required a substantial amount of iteration this was for the most part considered beneficial to reflection and we would recommend this approach in future in particular efforts to improve decision making support would benefit from broader testing and deployment of this type of technique to establish a body of knowledge on existing techniques for effective documentation communication reflection and iteration and to test their effectiveness in different contexts here the pathway diagram has been used as an ex post analysis tool i e after the project was finished its effectiveness at the start of a project ex ante or within a project still needs to be determined here diagrams were created using microsoft visio and static images were produced figs 4 figs 5 and 6 it may be useful to explore the use of interactive interfaces both to allow interaction with the pathway itself e g with links to documents and to further facilitate its construction indeed the usefulness of interactive tools to facilitate the communication for knowledge transfer and adoption has been demonstrated in other research robins 2006 while this paper has focused on encouraging reflection emphasising in particular the importance of reflective documentation and communication we acknowledge that further guidance may be needed to help put these ideas into practice this may come in the form of demonstration of and recipes for agile project management in environmental modelling but also in the form of more prescriptive guidance such guidance could primarily include what modellers may want to reflect on with what communication and documentation techniques e g related to the role of beliefs biases heuristics and values and differentiating between model failures involving occurrence and non occurrence of desired outcomes unanticipated consequences and decisions not to make a decision reflective communication is also expected to have advantages other than enabling redirection of the project path to improve effectiveness or efficiency in particular it is expected to improve soft outcomes of the project such as achieving trust learning outcomes and stakeholder involvement popa et al 2015 analysis and realisation of these additional benefits provide an interesting challenge for future work examining project management literature e g from agile software engineering practices is also expected to provide useful insights and tools to improve modelling practice and achieve these benefits 5 conclusions a modelling process such as the zaribar case study covered here is not just about the final model and the results but also about the decisions which are made at each step leading to that model and those results decisions are a key element of the process decisions are subjective and often uncertain biased or just imperfect even visual and verbal communication strategies used in modelling projects can be a source of biases we the practitioners should accept the uncertainty around our decisions and be adaptive in our problem solving and multidisciplinary projects and we should avoid committing and becoming locked into one pathway which consequently increases the risk of failure in the project this article has therefore argued for the need to adopt a reflexive approach which requires evaluating adapting and delivering the best decisions e g about tools methods assumptions data within a reflective process of documentation and communication adopting this approach helps to be transparent gain agreement and improve the efficiency and effectiveness of the project we used a new pathway diagram describing the path taken in the case study to demonstrate the role of communication and documentation within this reflexive approach and to identify opportunities for improvement reflection on documentation can add value to it noting that documentation might not be useful if there is no reflection on it the zaribar case study therefore provides an example of reflective practice showcasing reflection during the pathway examination of assumptions underlying existing practice self awareness and critical self evaluation to enable the action oriented transformation of practice a focus on reflective communication can be relevant to anyone working with problem solving research and policy decision making and in particular to environmental modellers and interdisciplinary researchers acknowledgment fateme zare is supported by the australian government s endeavour scholarships and fellowships program the work was carried out as part of her phd based on the zaribar lake project for which she was a project manager in mahsab shargh company joseph guillaume received funding from academy of finland funded project wasco grant no 305471 and emil aaltonen foundation funded project eat less water in the context of research on improving modelling practice and by an australian research council discovery early career award project no de190100317 the authors would like to thank hamid taheri shahraiyni for his supervision on the remote sensing project and raimo hämäläinen and sondoss elsawah for valuable feedback during the conceptualisation of the pathway diagram and takuya iwanaga for his useful feedback on agile project management elements of the article build on ideas in a conference paper zare et al 2019b the authors would like to thank the three anonymous reviewers for their valuable comments and suggestions to improve the quality of the paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104645 
26052,the pathway of a modelling project is commonly described as an adaptively adjusted chain of steps at which various decisions are made communication and documentation about these decisions are crucial to enabling reflection and adapting the pathway to changing circumstances such that well informed planning is required project decision making however often remains a black box it is rare to find reporting of dead ends alternative decisions and changes in decisions during the project this paper uses an integrated environmental management case study in iran to demonstrate the importance of reflective documentation and communication within the pathway we show how a pathway diagram incorporating some 14 symbols depicting steps decision forks options selected and alternatives actions communication and documentation can illustrate the role of communication within the project and identify lessons learnt we also encourage further work on application of agile project management and social science techniques to improve modelling practices graphical abstract image 1 keywords pathway reflection project communication documentation adaptation decision forks 1 introduction 1 1 modelling as an adaptively constructed pathway modelling and problem solving research can be thought of as an adaptively constructed chain of steps decisions and tasks forming a path gregory et al 1997 many different and alternative paths might be possible for a specific research purpose and the outcome of the process is usually dependent on the taken path hämäläinen and lahtinen 2016 moallemi et al 2020 some examples of steps in modelling and problem solving research include forming the team problem definition and model selection badham et al 2019 jakeman et al 2006 at each step many decisions are taken such as the specific quantities and scales of interest type of model to use the specifications of the model and the type and frequency of communication steps may also be packaged into heterogeneous phases with a project potentially only planned one phase at a time the main message of the pathway concept is that history matters hämäläinen and lahtinen 2016 a degree of path dependency is unavoidable and it is not possible to go back in time instead it is common and judicious to take an iterative approach revisiting the path taken and redirecting the next steps the iteration process provides a natural opportunity to compensate for any potential decision making deficiencies and adapt to any change in goals methods and available information the project pathway can be described as an adaptive spiral process fig 1 which might need several iterations to become more mature over time and meet stakeholder needs and project goals regular reflection can help ensure that necessary and timely iteration occurs a project should not be forced into a predefined protocol or procedure and analysts should be adaptive to changes and go in circles and branch out be prepared to go back reiterate and refine voinov and bousquet 2010 literature on adaptive management holling 1978 walters 1986 learning argyris 1977 and reflexivity popa et al 2015 emphasizes the opportunity to gradually improve outcomes and understanding by keeping in mind alternative choices within the project and being ready to switch to them when needed haasnoot et al 2013 successful iteration therefore requires a certain awareness and transparency around decisions iteration is particularly important in tackling complex problems the need for integration and tackling of implementation concerns bammer 2005 means that new information emerges and projects rarely go as originally planned knowledge of policymakers and other stakeholders needs to be amalgamated structured and shared along with input from multiple scientific disciplines in the modelling world this is typically approached in terms of integrated assessment and modelling iam a notable example being iam studies of water iwam which have increased over the last few decades zare et al 2017 iam has several dimensions hamilton et al 2015 in addition to requiring transdisciplinary integration of hydrology economics social science and software engineering iwam research usually involves stakeholder engagement and often purports to contribute to decision making there are at least three different audiences in any iwam research other scientists iwam team members decision makers and the public it is a challenge to enable effective flow of information among them there is a need for shared approaches tools protocols and reporting guidelines moallemi et al 2019 to report on facts findings and progress of the project and communicate within and between all three audience groups zare et al 2019a transparent communication is required to address different audience needs in a way that is accessible for everyone in order to obtain a social license to operate badham et al 2019 1 2 the role of communication and documentation in iteration when communicating about or documenting a project the process of iteration is often skipped over and persists as a black box it tends to be regarded as the messy underbelly that distracts from a result investigating that black box is however crucial to identifying opportunities for improvement decisions are made based on the context resources and constraints to achieve a specific aim and purpose the decisions made at each decision fork justify the next step or direction of the process and shape the pathway lahtinen et al 2017 moallemi et al 2020 decisions are made under the influence of a variety of behavioural phenomena such as biases beliefs heuristics and values bbhv glynn et al 2017 resources restrictions trade offs and limitations related to both researchers and the research process hämäläinen 2015 it is normal for modellers to make decisions based on their professional opinion which is unavoidably biased krueger et al 2012 this does not mean that decisions are purely the interpretation of one person but rather that credibility comes from agreement among experts inter subjective agreement rather than objectivity as argued by voinov et al 2014 in ten commandments for a socio environmental modelling agenda modellers should stop pretending that applied science and models are always objective and value neutral therefore projects should document and communicate the implicit decisions in their modelling and discuss the assumptions and values transparently this process of establishing the path of the project bears similarities to an emerging body of work in the field of behavioural operations research which empirically studies how operations research is performed franco and hämäläinen 2016 hämäläinen et al 2013 white 2015 for example with analyses of workshops using ethnomethodology franco and greiffenhagen 2018 in the context of team based transdisciplinary research the essential importance of iteration and reflection means that communication and documentation then take centre stage transparency and reflexivity require an exchange of information to ensure the right people are aware of influential issues affecting the study at the right time for example mauthner and doucet 2003 give a practical example showing how regular meetings within a research group improve their ability to be reflexive about their research communication may be verbal or visual with face to face or virtual meetings or involve reporting by email flyers papers or technical and management reports janse 2008 studied communication between forest scientists and policy makers in europe and highlighted that policy makers asked for shorter and easy to comprehend formats and that scientists need to invest in improving their communication ability documentation is a common form of communication which can be permanent and referred to easily understandable documentation allows for reflective assessment of a process and outcomes increasing transparency recognising limitations krueger et al 2012 and gaining the stakeholders trust voinov and bousquet 2010 often documentation happens after the modelling is finished describing the model code providing an analysis report and helping the user to use the model through presentations tutorials and user guides calder et al 2018 guidelines for proper documentation exist making sure that models can be fully checked and re implemented improving communication amongst modellers and facilitating critical scientific evaluation cartwright et al 2016 approaches such as transparent and comprehensive ecological modelling trace try to standardise documentation grimm et al 2014 documentation of provenance in particular is common in the fields of art digital libraries data management and workflow systems for reproducible science moreau et al 2008 and its importance has been recognised in integrated environmental modelling zhang et al 2017 model provenance refers to the description of the origin creation propagation process and responsible person for data models zhang et al 2017 in addition to data provenance systems tools bochner et al 2008 ikeda and widom 2010 provenance harvard 2019 wikis ticket issue systems and version control are commonly used to document processes and facilitate team communication in modelling software engineering and data analytics however these efforts mostly focus on the model and its specification in bringing together multiple models disciplines sectors and stakeholders for a purpose integrated modelling research is more than a model and the model is more than software given the importance of qualitative and semi quantitative approaches involving stakeholders voinov et al 2018 therefore communication and documentation tools also need to stretch to other parts of the project such as goal setting stakeholder analysis and selection with a focus on exposing the black box of the modelling process and targeting all three audience groups scientists decision makers and stakeholders documentation should go beyond describing what has happened and speak about the reasons behind each decision in the process of modelling badham et al 2019 also it should use a common language that is easy to use and minimises misunderstanding for everyone with the ability to show failures and lessons in software engineering agile project management techniques tackle the problem of connecting users and technical requirements ambler 2002 and suggest regular reflection activities to improve efficiency but these techniques have not yet been commonly used in the integrated assessment and modelling literature current documentation style in modelling practices usually has poor documentation of the decisions and the reasoning behind them müller et al 2013 although modelling has a long history documentation is still a problematic issue to be seriously addressed by modellers rahmandad and sterman 2012 there is therefore a need for guidance about reflexive communication how can researchers document and communicate in such a way that they receive useful feedback that helps shape their next steps and achieve better project outcomes given that this communication sits in the messy underbelly of the project clear guidance is elusive given the importance of alternative pathways practitioners need to speak about why why they used this approach model method data and scenarios and why not other options what were the other options but how should that conversation be handled our suggestion here is to treat communication about the project in the same way we treat the project itself as an iterative task that requires reflection and can be improved over time 1 3 improving iteration through reflection reflection is already widely recognised as an important concept reflection on the pathway as a whole is particularly vital hämäläinen 2015 in cases where the problems are complex participatory and include multiple sources of uncertainties lahtinen et al 2017 such as typically occurs in environmental modelling post normal science suggests that quality assurance of processes and the resolution of issues be approached through debate and dialogue within the extended peer community funtowicz and ravetz 1993 several reflective approaches have been developed for quality assurance in modelling and knowledge processes such as nusap numeral unit spread assessment and pedigree van der sluijs 2006 and the uncertainty guidance of rivm mnp van der sluijs et al 2004 a theory of change approach is similar whilst typically focussing on the intended rather than past pathway taplin and clark 2012 in modelling with stakeholders reflection is often seen as crucial to avoid forcing stakeholders into a fixed protocol or procedure recognising the need to adapt to differing stakeholder motivations voinov and bousquet 2010 unexpected changes are possible in goals and priorities and consequently in the pathway obtaining reflection on the pathway helps to undertake this iteration at the right time and place and avoid the consequences of skipping reflection and taking a poor path there is substantial literature on reflection theory and methods in many disciplines such as medical practice jarvis 1992 educational philosophy reynolds 2011 and recently in interdisciplinary and transdisciplinary research roux et al 2010 wickson et al 2006 as well as design research dalsgaard and halskov 2012 the definition of reflection may differ among disciplines but commonly involves thinking about past or ongoing experience to evaluate inform and improve future choices decisions or actions reynolds 2011 within the existing body of work there is not yet however a demonstration of how reflection can be used to scrutinize project communication specifically and identify how communication could be improved to achieve better project outcomes given the recognised benefits of reflection the importance of communication and the need for practical examples of reflective communication this paper provides an analysis of communication within an integrated environmental management case study in iran we make explicit the team s actual project pathways including the role of documentation and communication as well as the alternative options pathways that were considered during the project and the other options that could have been followed in hindsight we emphasise that the approach taken is to reflect on the case study as reflective practitioners schon 1983 the analysis method relies on reflection by practitioners two of the authors were closely involved in the case study using soft systems modelling avoiding the use of formal interview or document analysis methods minimises the additional skills required for other reflective practitioners to replicate this method while recognising that the focus of the method is therefore on supporting reflection by practitioners rather than performing a rigorous social science analysis we analysed the project by using the technical reports management documents meeting notes project management documents and authors reflections there are several advantages to using a soft systems approach notably that 1 it gives ownership of the evaluation process to the practitioner as opposed to an external objective evaluation 2 it allows the practitioner to focus on problems in the process that they considered important rather than getting bogged down in the detail of providing a comprehensive assessment and 3 as a modelling based process there is less of a barrier to uptake than forcing the use of formal social science methods while we do not have formal social science training we are in fact arguing that modellers and other practitioners should be engaging in this type of work and by our example we would like to encourage other modellers to similarly reflect on how they can improve their processes this case study demonstrates two key ideas firstly reflective communication is valuable that is communication is most effective when it triggers reflection on decisions taken secondly reflecting on communication level of detail tool frequency and involved parties is also valuable examining past communication can help identify lessons learnt to improve project communication in the future fig 2 illustrates a case in which a meeting that obtained reflection from other parties in step 1 allowed switching to an alternative decision step 2 ensuring project effectiveness and efficiency by avoiding a dead end and avoiding unnecessary work step 2 if the meeting had not occurred reflecting on the pathway after the fact might have identified that such a meeting could be beneficial in future by expanding on these ideas in a real case this paper demonstrates how reflective communication about decisions along the research pathway helps make projects more adaptive and agile and helps achieve project outcomes more effectively and efficiently 2 methods 2 1 documenting pathways to investigate the role of reflective communication we adopted a path perspective lahtinen et al 2017 existing discussions of modelling processes tend to take quite an idealized view and do not look at how modelling decisions and iteration occur in practice in contrast using pathways involves essentially building a model of what happened and could have happened describing the steps taken in the project and the decisions and circumstances that led to that choice in this paper we take a soft systems conceptual modelling approach we use graphical methods to capture the team s perspective of what happened rather than objective truth this type of soft systems perspective has a recognised history in dealing with multiple perspectives and associated biases elsawah and guillaume 2016 we identified two pathways the first pathway is the actual path taken through the project including other options considered at each decision fork it also shows the changes in path due to dead ends failures and changes in project aim s the second pathway shows the lessons learnt other options that could have been taken to improve the project the pathway diagram is intended to provide a systematic setting to help identify those lessons learnt and potential improvements to identify how we could improve the project we went through a process of constructing a counterfactual pathway for achieving our desired aim if we accept that there are many roads to rome say the actual pathway is the road that practitioners took and the lesson learnt pathway is one of the roads that might have reached rome more quickly or more safely there might be multiple possible lessons learnt pathways however just one of them is identified in this paper as an example to demonstrate the importance of reflective documentation and communication we aim to describe identified points in the pathway where 1 communication successfully led to iteration or avoided the need for later iteration 2 communication would have been beneficial or could have been improved and 3 communication improved the usefulness utility and benefits of the final product report and results 2 2 the pathway diagram tool and its use although existing research describes the idea of a pathway hämäläinen and lahtinen 2016 lahtinen et al 2017 lahtinen and hämäläinen 2016 it does not propose any specific tool for visualising pathways and sharing them we developed a pathway diagram as a graphical tool to document the steps taken in a project and facilitate communication and reflection it is a thinking reflecting and documenting tool to open up space for speaking about other options potentially contributing to both project evaluation and project management documenting the pathway taken helps to prise open and expose the otherwise black box nature of the project and thereby increase adaptability transparency and trust in the project the pathway diagramming approach and its components are adapted from tools used in other disciplines particularly elements of activity diagrams the latter describes workflows by showing activities and actions with support for choice iteration and concurrent execution of activities dumas and ter hofstede 2001 including associations between activities conditions and constraints activity diagrams are classified in the behavioural category of the unified modelling language meaning that they capture the dynamic behaviour of a system over time in the pathway diagram we use the idea of showing the flow but we focus on the decisions and communications during the process leading to a different set of core components table 1 this diagram could be a useful tool to facilitate the communication between stakeholders modellers and decision makers in order to reach a shared understanding of the modelling process zare et al 2019b our starting point is that a path is a sequence of steps taken in a problem solving case hämäläinen and lahtinen 2016 and where steps encapsulate a coherent set of activities with a set goal the goal of each step should be set adaptively and flexible so that it can be revised in agile project management literature the process is split into sprints or phases which we consider to be steps here many decisions are made at each step and at each decision fork there might be more than one option to choose the analyst might need to take some actions to support decisions and might communicate and document the decisions and their results therefore we regard symbols as indispensable to portray these elements steps decision forks options selected and alternatives actions communication and documentation table 1 shows the main elements of the pathway diagram and a brief description of each element based on the iterative construction of the diagram these 14 specific elements are needed to capture the necessary aspects of the pathway in this study documentation of the pathway evolved iteratively through reflection by the authors at first we identified the steps phases of the project then we added main actions in each step next we identified the decision forks in each step in each decision fork we show the alternative choices considered after achieving an overall view of the pathway we added detail about meetings and documentation to capture the communication that led to each decision gradually refining the pathway provides the opportunity to see obvious alternatives first and stop before reaching reflection fatigue decision forks or decision points occur when people choose decide or agree on objectives targets tools resources approaches or any other activities to identify a decision fork we used an evidence based analysis grounded on text and interviews text documents from the project by definition show documented options whereas interviews usually add undocumented options to the pathway diagram the emphasis is on capturing aspects of the path that are important for reflection rather than trying to document all decision forks we focused on documenting decisions that changed the pathway or affected the efficiency or output of the project these decisions need to be documented and reflected upon to avoid relying on intuitive system 1 thinking which is reliant on mental operations and shortcuts such as heuristics and biases kahneman 2011 in addition there is a need to document and reflect on decisions whenever intermediate results are obtained any new learning is gained or new data report resources become available similarly the aim of mapping the pathway is not to show all the possible alternative options as practitioners might not be aware of them all some options are known unknowns where there is a lack of knowledge there might also be some options in other areas domains and disciplines of which practitioners are not aware unknown unknown options or that practitioners used unconsciously i e unknown known options kerwin 1993 the actual pathway will not include alternatives that were unknown to the team however practitioners should map all the known known options the ones that practitioners are aware of and that were considered and evaluated it should also be noted that if there is no direct arrow between actions events it does not mean there is no effect and connection only the main links are captured within the modelling process recognising available options and making decisions can be influenced deliberately or unintentionally by a variety of human factors originating from cognitive behavioural and mental frameworks as well as the beliefs biases heuristics experience and values of stakeholders moallemi et al 2020 identifying the influencing factors and controlling their effect is a complicated process and might need expertise that is not available in the team increasing awareness of human factors and their effects on the judgement is a critical step to reduce the risks of negative impacts and taking poor paths nickerson 1998 glynn et al 2017 suggested an adaptive framework to generate information and recognise human factors to help manage their impacts consistent documentation and immediate reflection help to capture the choices made and their rationale identify cases where a decision is not consistent with the objectives assumptions and available information and increase attention paid to human factors to reduce their negative effects glynn et al 2017 moallemi et al 2020 the pathway diagram facilitates these documentation and reflection processes along with tracking the decisions and considered options the pathway diagram could include dates of events as well but they are not always vital to the storyline of the project we consider there to be two fundamental rules in diagramming a pathway first paths are one way so it is not possible to go back in time even if a path has failed it is impossible to go back to the previous state because the pathway has reached a new state in which practitioners know one of the options will not be successful in achieving its intended aim in other words failure of an option increases knowledge about other options the second rule is that only one decision solid arrow should emerge from a decision fork with other options shown as alternatives dashed arrows the solid arrow captures the way of choosing a path forward moving into a new state by closing other options including returning to the previous state where multiple decisions are made they can either be presented as one collective decision or split into separate sequential or simultaneous decision forks if one pathway successfully reaches the expected outcome it does not mean others will not we should acknowledge the fact that any modelling path can have different outcomes and there might not be just one ideal path hämäläinen 2015 for example in fig 3 suppose path 3 thick line is the pathway that actually materialised other lines embody alternative pathways some pathways still reach the same endpoint and provide the same output some others might reach a different endpoint but still achieve the expected outcome some alternatives might lead to failure the successful pathways might be different in the character of the outcomes different level of trust outcomes reliability and others or in the path taken which might be important in terms of efficiency and resources by success we simply mean achieving the intended or ideal goals of a project in terms of both outputs of a project and the process to achieve them a holistic view of success in modelling projects means the ability to embed the modelling in a social process that connects scientists decision makers and stakeholders and achieves impact in accordance to its purpose which may vary from a shared understanding of a problem to policy analysis badham et al 2019 hamilton et al 2019 not being able to achieve this outcome is a failure situation for the modelling project failure and success are therefore not absolute terms goals might be different for each group of stakeholders and can be revised during the project the setting and revising of goals is affected by many factors such as acquisition and processing of information judgement and evaluation system 2 thinking and biases beliefs heuristics and values system 1 thinking glynn et al 2017 effective modelling involves a process of practices decisions and actions which suit the context of the problem badham et al 2019 and can take on different forms with the relevant criteria for suitability depending on the project hamilton et al 2019 various authors have described the many dimensions and perspectives on success including merritt et al 2017 who identified 33 different factors to assess the success in water resource projects mcintosh et al 2011 who defined success of environmental decision support systems in terms of best practices and hamilton et al 2019 who covered 32 criteria specifically focused on effectiveness of environmental modelling in this paper an iwam project is considered effective if it achieves the intended outcomes and efficient if it uses its resources as intended zare et al 2019b demonstrate a hypothetical pathway example to illustrate the way the explained components and rules can be used they used the pathway diagram to construct a customized participatory modelling pathway 3 application zaribar lake case study iran the zaribar project analysed changes in land use and their trends type intensity rate and location using remote sensing techniques the aim was to help identify the reasons for the changes in a lake and facilitate restoration planning build trust between stakeholders and the research team and achieve a shared understanding of land use changes this project was part of an overarching project to restore zaribar lake in kurdistan province iran in 2012 a restoration study project started following reports that showed the lake faced some serious issues such as a change in its physical shape increasing pollutants and pollution load reyahi khoram and hoshmand 2012 sharifinia et al 2013 sedimentation karbasi abd and bayati 2008 excessive algae lack of dissolved oxygen odours fish kills and increases in floating plants and other plants such as reeds typha latifolia and cyperus longus there was an element of panic among the local people who strongly believed that plants were covering the entire lake some news reports producing dire statements such as one that the lake will be dead in 5 years residents started fires and burnt vegetation as part of their effort to save the lake these actions however only worsened the lake s condition while scientific research is commonly used to back up environmental planning it is often undertaken in isolation from affected local communities and stakeholders this isolation can compromise the likelihood of success for restoration projects if the results of the research are not entirely accepted or used by decision makers and stakeholders and therefore will not change individual or community behaviour and beliefs the restoration project led by mahsab shargh consulting company therefore included stakeholder involvement intending to help the department of environment to identify the drivers and causes of the problem and design and implement a practical solution in the first meeting the restoration project team decided to start a remote sensing rs project to study land use change this paper focuses on the rs project and fig 4 gives an overview of the intended project plan the project and problem scope were defined in a meeting with the restoration project team the project plan included a provision for regular documentation meetings and progressive feedback the description of the actual pathway followed fig 5 includes other options considered at each decision fork and changes in the direction of the project pathway we focus here on the main decision forks and associated lessons learnt regarding communication and documentation the supplementary materials provide a complete analysis and description of all phases 3 1 actual pathway problem definition ultimately occurred through three separate meetings the first defining the project scope around remote sensing of land use and its change the second discussing further details especially the spatial boundary of the analysis and resulting in a project plan while the third meeting obtained approval from the steering committee fig 4 phases 1 and 2 land use change detection involved many sub steps and decision forks initial analysis specification informed selection of imagery allowing preparation of images and the actual image processing the process took six months and project status reports only resulted in approval to continue without alteration of the project pathway unsupervised classification is fast efficient in time and budget and does not require fieldwork castellana et al 2006 therefore unsupervised classification was used as a first step to obtaining a general impression of different land use classes in the basin and their changes once initial unsupervised classification results were obtained a meeting was held with the restoration team it was observed that the results could not answer all questions raised by the restoration team the previous meeting had chosen to use a spatial boundary based on surface hydrology rather than a more extensive overarching boundary including groundwater and areas with socio economic impacts see fig s13 in the supplementary materials produced in the project as part of the discussion in phase 6 use of the hydrological boundary meant it was not possible to investigate the role of urban expansion and agricultural development one of the hypotheses for the restoration plan was that agriculture development and city expansion had impacted on the lake situation so the socio economic boundary would have been vital to detect changes in these growth elements land use characteristics also affect groundwater infiltration and levels so the groundwater boundary is essential as well one of the primary water sources for the lake is from the springs in its bed which makes groundwater changes critical to consider two options presented themselves either restrict the scope of the restoration project or change the boundary which meant returning to step 2 1 resulting in three more months of work and associated costs restricting the scope of the restoration project would still have provided useful outcomes about the changes in the water body and floating plants of the lake and land use changes in the lake s hydrological basin which provides information about water quality and quantity changes however it was decided that the additional costs were worthwhile fortunately the selection of images was not affected because the landsat images used covered both boundaries however it was necessary to redo the processing phases and steps data preprocessing based on the new boundary and redoing the initial unsupervised classification remote sensing analysis after this process the meeting with the restoration team approved the results allowing further supervised classification and validation to continue the project members decided to be upfront with the steering committee about the issues with the spatial boundary even before the meeting the problem was well documented and raised as a point requiring reflection surprisingly the steering committee asked to have the results for both boundaries in the final report the hydrological boundary highlighted changes to the lake and nearby villages and it was useful in order for the environment department to develop a policy for the area the overarching boundary was felt to be useful for the agriculture department environment department and city council planning the project results were approved by the steering committee to be used as a basis for the restoration planning project and it was useful for all involved to understand the different roles of the hydrological and socio economic and groundwater boundaries overall the project took four months longer than expected with an associated cost results for the hydrological boundary could have been provided using the analysis with the overarching boundary instead of undertaking separate analysis so if the overarching boundary had been selected first the team could have avoided the extra cost and produced the same outputs 3 2 possible improvements lessons learnt while the project did successfully redirect the analysis so that it used a different spatial boundary it is worth asking whether the extra cost could have been avoided could communication and documentation have been improved reflecting on the pathway by the authors including a member of the remote sensing group resulted in four possible changes that might have avoided this redirection summarised in table 2 and highlighted red items in fig 6 3 2 1 possible improvement no 1 dummy results the redirection of the project in the meeting with the restoration team was triggered simply by presenting the results while it saved time to obtain feedback from unsupervised rather than supervised classification results it would have been possible to get feedback earlier in the process by presenting dummy results for example in phase 2 2 after critical decisions had been made but before the time consuming collection and analysis of the satellite imagery a dummy or synthetic result is a mock up of the results showing what the eventual outputs might look like dummy results would therefore have shown the difference that the options at a decision fork would make in the final output and highlight the consequences of each choice of the boundary showing dummy results fosters critical thinking and reflective practice this helps decision makers in this case other modellers make a more informed decision a closely related alternative would be to demonstrate the effect of the boundary using another case study in order to highlight the importance of this type of decision fork in general terms and the consequences of each option 3 2 2 possible improvement no 2 risk analysis given that one team member had already raised the issue of boundary selection in phase 3 another possible change could have involved emphasising the stakes involved i e the risk trade offs involved in selecting the two boundaries it was a clear risk that selecting the smaller boundary might leave out areas that could be of interest later selecting the larger boundary reduces this risk at the cost of extra time structured risk analysis is a well established approach for thinking about the uncertain consequences of decisions rotmans 1998 in this case the decision depends on how much extra time is worth committing for an uncertain benefit later discussing this trade off explicitly might have altered the decision more generally a risk analysis could in principle have been used in the initial specification of the analysis phase 2 1 risk analysis would have systematically mapped out consequences of decisions in the planned process which might have surfaced the spatial boundary problem however such risk analyses can take some time to do well such that it is understandable that most projects including this one decide to rely on the judgement of critical members of the team at the risk of groupthink or being dominated by the supervisor s opinion risk analysis can improve judgements and decision making but it is affected by many biases which need to be reduced or taken into account montibeller and von winterfeldt 2015 there are however success stories for example stacey et al 2017 assessed the use of a decision aid to support risk analysis of benefits and costs of the decisions the suggested decision aid documentation method helped people to make more informed decisions with more accurate risk perceptions while being more precise about their values and taking a more active role in decision making 3 2 3 possible improvement no 3 interactive and reflective documentation the project did have regular project management and technical reports but these did not prevent taking the wrong pathway at the right time the reports could have been prepared in a way that would have provoked more interaction and reflection the reports could have included specific questions about technical decisions and their implications inviting the readers of the reports to encourage questioning of decisions and reflection rather than merely seeking approval the form of feedback could also have been reconsidered the reflection on the reports usually was a series of comments or suggestions from the steering committee which was sent to the rs team about one month after submitting reports these comments sometimes focused on minor issues and overlooked major ones for example suggesting a different map resolution colour or name instead of approving or questioning the boundary sending a set of questions with the report might therefore have helped to focus reflection on key decisions overall the suggestion is to have regular interactive and reflective documentation going beyond common management reports to make sure that stakeholders are aware of and agree with the decisions and events burgess et al 2007 interactive communication with opportunities to question claims reflectivity is usually valued by decision makers because this type of communication can support individual learning develop shared understanding and lead to more useful outputs o connor et al 2019 as part of this interaction the timing of feedback could have been revisited given that project work continued while waiting for comments which limited the ability to influence technical decisions the reviewers schedule could have been organised to provide a shorter turn around time or a video meeting when delivering each report could have accelerated the reflection time in general a team s schedule can also be organised to accommodate feedback it can be possible to pause a project to wait for feedback or to switch to other tasks within the same project in this case it was necessary to continue working to meet project deadlines in computer science this strategy is called optimistic concurrency control it is assumed that concurrent work feedback will not change the path taken so a small risk is accepted of potentially having to repeat work this risk taking appears to have been justified in this case as no feedback from these reports resulted in work having to be repeated 3 2 4 possible improvement no 4 multimedia communication meeting the last possible change in table 2 has a more nebulous goal of improving the effectiveness of communication the issue of the spatial boundary was already raised in the problem definition meeting between the rs team and the restoration team the decision fork was obvious for the team but the importance and impact of each choice were not and so it was not considered seriously perhaps if a different means of communication had been used the outcome would have been different a multimedia communication meeting virtual or real could have used a variety of tools beyond traditional reports to communicate about the decision and ask specifically for reflection it is important to use the right tools in the right phase place and time to maximise success we cannot be sure what specific means of communication might have been effective in this case it may have been useful to use more diverse and creative media tools and forms of communication such as interpersonal discussion communication monologue verbal communication speech presentations movies games documents e g websites emails scientific articles and briefs janse 2008 2006 these give us the ability to indicate intuitions conceptual models and assumptions glynn 2015 guldin et al 2005 suggested having regular and effective communication through multiple different channels using a multimedia tool to present a dummy result might have had a greater cognitive impact though the result is likely to depend on the precise form of the multimedia and the characteristics of the audience yildiz and atkins 1996 as an example when deciding on the boundary phase 2 2 the team could have used google earth engine as a tool to facilitate communication and highlight the difference between having marivan city in the boundary or not fig 7 shows satellite images for 1992 and 2016 in the zaribar region in the top row the city of marivan is covered whereas in the second it is visible which provides a visual impression of the substantial change in the area covered by the city in a meeting this could have been made more striking by visualising the changes using the timelapse tool in google earth engine 1 1 check the following timelapse https earthengine google com timelapse v 35 54879 46 13937 11 305 latlng t 0 44 ps 50 bt 19840101 et 20181231 startdwell 0 enddwell 0 just making this change more obvious could have caught the attention of participants from other disciplines e g members of sociology hydrology and ecology teams highlighting that the increase in urban area is likely linked to increased population a social impact resulting in more sewage discharging into the lake water quality impact thereby contributing nutrients for the spread of the floating plants across the lake ecological impact this is a potentially crucial part of the story regarding the area covered by floating plants which might have captured the interests of members of the sociology hydrology and ecology teams so a larger boundary is then clearly worthwhile 3 3 lessons learnt for this paper these lessons learnt firstly demonstrate that reflective communication can play a crucial role in the success of a project while the communication used did achieve redirection of the pathway and ensured the project was effective further improvements could have increased its efficiency and reducing costs documentation might not be useful by itself if subject to insufficient reflection the documentation needs to be capable of triggering iteration in steps and redirection of project pathways secondly this paper demonstrates that reflection on communication regarding the project pathway can help identify potential improvements in project communication for the future team members who raise an issue and feel that reflection was not achieved can use a variety of communication strategies to ensure the issue is addressed thirdly then the lessons learnt highlight that reflection on documentation used within the project itself might have added value thinking critically about how decisions are shared and feedback is sought can help identify more effective communication strategies and more effective iteration 4 discussion this article has attempted to demonstrate how the concept of a pathway can be used to help assess communication within an integrated environmental management case study in iran a newly developed pathway diagram incorporating symbols that indicate steps decision forks options selected and alternatives actions communication and documentation was used to establish the intended and actual path of the project and reflect on possible improvements the case study assessment is intended to provide a proof of concept demonstrating that reflection on communication can provide useful insights for future improvement as well as a concrete example of the value of reflective communication in the next sections drawing on related work we propose three implications of our study for 1 documentation section 4 1 2 reflective practice section 4 2 and 3 effective project communication section 4 3 then we discuss the limitations of our pathway approach and requisite future work to extend its value section 4 4 4 1 implications for documentation to support reflection writing reports manuals and papers about models has always been a standard method of communication nevertheless there is still limited documented knowledge on the modelling process the role of influences in determining this process and practices to employ badham et al 2019 currently there are two common forms of documentation at two levels of detail management and technical fig 8 at the management level documentation usually contains information about the project management workflow timeline finance and events and sometimes also a summary of the results of the project according to badham et al 2019 technical reports usually describe detailed information about the model such as input output executable and source code scientific justification of assumptions calibration and validation the primary aim is to establish scientific credibility and provide support for future development a report might also include recommendations and conclusions of the research user guidance documentation or manuals can be considered a type of technical report the guidance documents focus more on helping users to run the model and interpret the model outputs they usually include a clear interface description and tutorials there is however a gap between these two levels of detail which is why modelling project decisions tend to remain nebulous and unformulated a new level of documentation that targets all stakeholders decision makers experts public etc which might be referred to as pathway documentation fig 8 is crucial if we are to expose the black box of the process and aim to make it clear and transparent pathway documentation would report the detail of decisions alternative decisions dead ends and changes in decisions during the project to support reflection documentation should be compiled whenever a decision is made which will affect the taken pathway lahtinen et al 2017 for example the boundary decision fork in the zaribar case study whenever stakeholders need more clarification or their reflection would be valuable such as in phase 6 in fig 5 and at the end of each step and phase to obtain consent and approval from stakeholders our pathway diagram can be used as a tool for this type of documentation to facilitate the process of obtaining reflection and provide a holistic appreciation of the pathway of the project and its locked in dead end and successful paths the diagram links the technical and management level of detail together by reporting both the overall process and the individual technical decisions the zaribar case study suggests that instead of emphasising well documented models badham et al 2019 we need to emphasise well documented modelling processes and a focus on decisions and pathway documentation can help achieve this documenting paths can support evaluation and help extract best practices popa et al 2015 some authors have touched on related ideas but it seems that pathway documentation has rarely been implemented to document a modelling project and there are some key differences between these related ideas table 3 shows an overview of different approaches of decision documentation with a summary of their aims and differences relative to our proposal 4 2 implications for reflective practice as described by finlay 2008 reflective practice is the process of learning through and from experience towards gaining new insights of self and practice practitioners examine the assumptions underlying their everyday practice and critically evaluate their responses and decisions in order to gain new understanding and so improve future practice reflective practice involves the interaction of three essential skills 1 critical thinking to challenge the assumptions and context 2 self awareness to evaluate the knowledge derived and 3 reflection to promote self and social awareness and social action our focus in the zaribar case study was just on reflection popa et al 2015 argued there is a need to complement classic approaches in sustainability research with more reflexive and transformative approaches which require design mechanisms to transform values practices and institutions through stakeholder participation rather than common consultative participation the zaribar case study provides an example of the role of reflection versus consultation and demonstrates that adopting a reflective communication approach can help deliver a more efficient and effective project our work also suggests that in a team setting it is useful to make explicit the role of communication and documentation in the reflective cycle for example the one suggested by gibbs 1988 which has become a standard model for reflection in education research this cycle proposed that theory and practice enrich each other in fig 9 we therefore use the gibbs cycle to help describe the idea of reflective documentation the cycle starts with the project documentation phase describing what has happened and needing critical thinking to be informative the next steps are thinking evaluating and analysing which make up the reflection phase in the conclusion phase the team chooses between changing the pathway or approving it and moving on reflective documentation is therefore the lynchpin linking previous actions to new plans differentiating it from existing forms of documentation as discussed in the previous section fig 9 shows that reflective documentation fits easily within the existing literature there are some critical points about reflection that therefore also apply to reflective documentation the reflection step might be performed by individuals or as a group and it can be done by the research team stakeholders or a third party reflection by the research team amounts to a self reflection hibbert et al 2010 examining one s own decisions and actions to control the effect of motives biases and values behind them and to evaluate those decisions based on the project aim a notable example of a third party reflection involves appointing a devil s advocate or even a parallel modelling team from the first step of the project to challenge the approaches and the decisions of the primary team see e g glynn 2015 avoiding groupthink and reducing the risk of being stuck on a poor path moallemi et al 2020 to maximise the advantages of reflection practitioners need to create an atmosphere of trust and reduce the fear of making mistakes or speaking about them this environment can promote learning creative thinking and prevent anchoring to standard methods lahtinen et al 2017 reflection is not limited to comparison with the past as in this paper reflection is more generally a comparison between the current pathway and that desired by practitioners visioning practices could therefore help in defining what is success in the project and clarify our vision of success kallis et al 2009 visioning practices generate a common goal and a clear image of the future and offer an opportunity for meaningful reflection and change in decisions throughout the pathway helping to motivate purposeful action kallis et al 2009 consequently the role of pathway diagrams in a visioning context may be a fruitful future research topic 4 3 implications for effective project communication effective communication between team members and between the team and stakeholders is a vital element to ensure the success of the project and improve its efficiency effectiveness transparency and trust making we consider communication to be effective if the decisions assumptions resources and knowledge become clear for all the team members stakeholders and decision makers the process of reflective communication and documentation of the zaribar pathway was designed to examine its impact on the effectiveness of communication in shaping project outcomes based on this case study we recommend having a concrete but iteratively updated plan for reflective documentation and communication about decisions and events in the project consistent with more general calls for communication plans and improvement in communication generally giovanni 2006 janse 2008 in particular it is likely to be useful to explicitly address resourcing and scheduling of reflective documentation while providing sufficient flexibility for team members and stakeholders to discuss the issues likely to be important to them at the time discussion is needed it should be emphasised that communication is an interactive and reciprocal activity which includes an action and a reaction janse 2008 so the receiver of the information should not be passive and should be actively engaged janse 2006 o connor et al 2019 to make sure that real feedback is obtained and the message is clearly received through two way communication the project is more likely to achieve mutual understanding and trust between stakeholders and the research team this can for example be facilitated by sending a list of reflective questions with any document to facilitate engagement and draw attention to team member concerns planning of communication is of course already common in many projects for instance alvarez et al 2010 developed participatory impact pathways analysis pipa for reflective documentation and communication they suggested regular reflection and adjustment workshops with key stakeholders during the project to promote learning and provide a framework for action research on processes of change our recommendation for active planning builds on this by emphasising the importance of iteratively striving to improve reflective documentation practice as this case study has shown the pathway diagram can provide a basis for both planning and evaluating reflective documentation and communication and potentially helping to develop mutual trust which are all key requirements for effective communication janse 2008 practitioners should be aware of the risk of over formalising the reflection process and making it a ritual dance but at the same time putting aside the required resources and time and improving awareness of its necessity to allow the reflection happens on time likewise effective communication is a critical issue in knowledge co production projects in natural resource management where the legitimacy of a decision support process depends on fairness transparency and inclusivity in all aspects of processes and not just basing decisions on science transparent documented processes of determining knowledge inputs and technical approaches are also valued o connor et al 2019 agile project management also values many of the same principles that we have argued for in this paper including the importance of interactions over processes and tools and having regular intervals when the team reflects on and adjusts the process in order to improve effectiveness compared to blindly following a plan ambler 2002 it is essential for the practitioners to have the humility to admit that you may not know everything that others have value to add to your project efforts ambler 2002 so it is necessary to have early and continual focus on users early and frequent feedback from stakeholders and to iterate to reach the best design chamberlain et al 2006 the importance of the team beck 2004 coherent teamwork chamberlain et al 2006 and effective communication within the development team ambler 2002 beck 2004 are emphasised as well despite all valuable recommendations in agile research projects have mixed implementation results and tend to pick and choose the agile principles they implement sletholt et al 2012 and not all scientists who develop and use scientific models have a high level of knowledge about standard software engineering concepts hannay et al 2009 in some agile projects pressure on iteration may also mean that reflection opportunities are missed necessitating the integration of reflective practices schon 1983 into agile methods in more practical terms babb et al 2014 in this paper we further emphasise that care needs to be taken to apply these ideas to the project as a whole e g problem definition step stakeholder involvement translating the model result into practical policies properly integrating modelling and software into the big picture rather than treating it as a separate part we also emphasise that encouraging reflective communication provides a solid foundation to work towards planned projects that are flexible so as to allow adaptation and adjustment to the new situation realising improvements through iteration 4 4 limitations and future work as the first study examining the role of iterative communication and documentation in the context of integrated environmental modelling our experience suggests that further studies are worthwhile and other existing techniques may find a use a soft systems conceptual modelling approach to documenting project pathways as used here is a conceptually simple approach focussing on identifying steps decision forks options selected and their alternatives although the process of drawing the diagrams required a substantial amount of iteration this was for the most part considered beneficial to reflection and we would recommend this approach in future in particular efforts to improve decision making support would benefit from broader testing and deployment of this type of technique to establish a body of knowledge on existing techniques for effective documentation communication reflection and iteration and to test their effectiveness in different contexts here the pathway diagram has been used as an ex post analysis tool i e after the project was finished its effectiveness at the start of a project ex ante or within a project still needs to be determined here diagrams were created using microsoft visio and static images were produced figs 4 figs 5 and 6 it may be useful to explore the use of interactive interfaces both to allow interaction with the pathway itself e g with links to documents and to further facilitate its construction indeed the usefulness of interactive tools to facilitate the communication for knowledge transfer and adoption has been demonstrated in other research robins 2006 while this paper has focused on encouraging reflection emphasising in particular the importance of reflective documentation and communication we acknowledge that further guidance may be needed to help put these ideas into practice this may come in the form of demonstration of and recipes for agile project management in environmental modelling but also in the form of more prescriptive guidance such guidance could primarily include what modellers may want to reflect on with what communication and documentation techniques e g related to the role of beliefs biases heuristics and values and differentiating between model failures involving occurrence and non occurrence of desired outcomes unanticipated consequences and decisions not to make a decision reflective communication is also expected to have advantages other than enabling redirection of the project path to improve effectiveness or efficiency in particular it is expected to improve soft outcomes of the project such as achieving trust learning outcomes and stakeholder involvement popa et al 2015 analysis and realisation of these additional benefits provide an interesting challenge for future work examining project management literature e g from agile software engineering practices is also expected to provide useful insights and tools to improve modelling practice and achieve these benefits 5 conclusions a modelling process such as the zaribar case study covered here is not just about the final model and the results but also about the decisions which are made at each step leading to that model and those results decisions are a key element of the process decisions are subjective and often uncertain biased or just imperfect even visual and verbal communication strategies used in modelling projects can be a source of biases we the practitioners should accept the uncertainty around our decisions and be adaptive in our problem solving and multidisciplinary projects and we should avoid committing and becoming locked into one pathway which consequently increases the risk of failure in the project this article has therefore argued for the need to adopt a reflexive approach which requires evaluating adapting and delivering the best decisions e g about tools methods assumptions data within a reflective process of documentation and communication adopting this approach helps to be transparent gain agreement and improve the efficiency and effectiveness of the project we used a new pathway diagram describing the path taken in the case study to demonstrate the role of communication and documentation within this reflexive approach and to identify opportunities for improvement reflection on documentation can add value to it noting that documentation might not be useful if there is no reflection on it the zaribar case study therefore provides an example of reflective practice showcasing reflection during the pathway examination of assumptions underlying existing practice self awareness and critical self evaluation to enable the action oriented transformation of practice a focus on reflective communication can be relevant to anyone working with problem solving research and policy decision making and in particular to environmental modellers and interdisciplinary researchers acknowledgment fateme zare is supported by the australian government s endeavour scholarships and fellowships program the work was carried out as part of her phd based on the zaribar lake project for which she was a project manager in mahsab shargh company joseph guillaume received funding from academy of finland funded project wasco grant no 305471 and emil aaltonen foundation funded project eat less water in the context of research on improving modelling practice and by an australian research council discovery early career award project no de190100317 the authors would like to thank hamid taheri shahraiyni for his supervision on the remote sensing project and raimo hämäläinen and sondoss elsawah for valuable feedback during the conceptualisation of the pathway diagram and takuya iwanaga for his useful feedback on agile project management elements of the article build on ideas in a conference paper zare et al 2019b the authors would like to thank the three anonymous reviewers for their valuable comments and suggestions to improve the quality of the paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104645 
26053,coastal shallow basins are often heavily anthropized and greatly exposed to environmental risk areas thus requiring strict monitoring action by local authorities and stakeholders preventive measures against environmental degradation and early warning to hazards have been proved to benefit from the combined use of numerical models and field measurements the present work sets out to show the potential of a meteorological hydrodynamic model system validated with field data to identify the main physical processes characterizing a semi enclosed basin located in the inner part of the ionian sea in southern italy furthermore based on the model results we adopted some convenient indicators especially related to flow exchanges in order to identify and characterize the area in the basin most sensitive to environmental problems the results highlight the retentive feature of the inner part of the basin and different times necessary for the water renewal in both the surface and bottom layers keywords field measurements meteorological model hydrodynamic model semi enclosed basin sea currents 1 introduction a common challenge in coastal management is to balance environmental conservation priorities human activities and economic development an essential tool in the decision support system is the possibility to detect and identify the most sensitive coastal areas i e areas exposed to strong human and industrial pressure where environmental problems especially linked to transport and diffusion of pollutants assume relevant importance there are many pollutant sources in heavily anthropized coastal sites unfortunately available data to identify them and to establish their greater or lesser danger are often scarce widespread pollutant inputs can even be associated with inland sources and can be transported underground in the aquifer and discharged into the sea consequently knowledge of possible paths and fate of polluting tracers sediments trapping water renewal times is of paramount importance for coastal management plans and in situ decision making moreover the mapping of the most critical and sensitive areas within a coastal basin based on some key factors could be of great benefit for this scope guiding possible monitoring actions and rehabilitation interventions armenio et al 2018 2019 de serio and mossa 2016 2018 de padova et al 2017a b de carolis et al 2013 kjerfve and magill 1989 this is the main reason for the present study nowadays predictive operational oceanography takes into account regional sub regional and shelf coastal scales based on coupled models of wave current and tracer dynamics the interaction between sea and atmosphere is often limitedly modelled by means of empirical bulk formulas providing wind stress as a function of wind velocity and depending on empirical drag coefficients samaras et al 2016 gaeta et al 2016 wróbel niedźwiecka et al 2019 this approach must face some limitations to provide reliable results on the sea circulation in the basin in fact understanding the physical processes involved at the atmosphere sea interface is complex as well as essential especially in terms of exchange of momentum between wind and sea surface di bernardino et al 2016 thus it is desirable to address numerical meteorological models simulating the overlying atmospheric flow in this way meteorological input data such as wind speed and direction air temperature and atmospheric pressure among others can be used as boundary conditions at the atmosphere sea interface furthermore hydrodynamic numerical models need a setup a calibration and a validation so that their integration with waves current and tide data is fundamental umgiesser et al 2004 armenio et al 2016a 2017 cannata et al 2018 it is also worth noting that the accuracy of model outputs relies on the quantity quality and duration of the available observations therefore extensive field measurements are of primary importance and monitoring actions should be judiciously programmed samaras et al 2016 trotta et al 2017 in this study we aim to examine the main hydrodynamic processes in a coastal basin with lagoon features which is assumed as a target case by using a meteorological and a hydrodynamic model a semi enclosed basin located in southern italy exposed to heavy urban industrial and military activities was used for this scope de padova et al 2017a b the effectiveness of the proposed model chain was provided by comparing the model results and the field data which were acquired onsite by means of two fixed monitoring stations and operational surveys the runs covered an annual period and the model results were provided onto a mesh with a high spatial resolution in this way we could identify some typical trends in the water circulation and exchanges which makes it possible to detect the most vulnerable locations in the domain examined based on some suitable indicators following cucco and umgiesser 2015 and de pascalis et al 2016a the water transit time the water renewal time and the trapping index were defined and computed for this scope the added value of this study with respect to previous numerical works applied to the same area de pascalis et al 2016a b is in i the adoption of a meteorological model to provide time varying and space varying meteorological data as realistic input for the implementation of the hydrodynamic model ii the time period used for the simulation that is the whole year 2014 while previous simulations referred to 2013 for which proper field measurements were available iii the computation of all the three indicators related to the basin vulnerability iv this computation was based on the entire annual period rather than on a short time window such as couple of months thus resulting more reliable the paper is structured in the following way sections 2 and 3 respectively describe the area studied and the monitoring activity section 4 presents the numerical models used section 5 describes the validation of the model outputs with field data and the main hydrodynamic features section 6 identifies the key indicators used to assess the basin vulnerability the discussion on these findings is in section 7 2 study area the proposed approach was applied to a target area located in the inner part of the gulf of taranto in the ionian sea along the coast of southern italy fig 1 the study area is composed of the mar grande and the inner basin of mar piccolo which in turn consists of two bays respectively named bay i and bay ii the mar grande and mar piccolo basins are joined by means of an artificial channel namely the navigable channel 58m wide and 12m deep and a natural one namely the porta napoli channel 150 m wide and 2 5 m deep the total area of the mar piccolo is about 21 7 km2 and the average depth is about 7 m the total area of the mar grande is about 35 km2 while its maximum depth is about 35 m in its central area along the outer perimeter of the mar grande there are two small islands called cheradi isles joined by a long breakwater in the mar piccolo there are about 34 freshwater submarine sources of karst origin called citri lisco et al 2016 they can be temporary or permanent during the year depending on the rainfall and the dynamics of the submarine water intrusion in the springs the presence of these freshwater inputs influences the salinity gradients of the basin giving it the typical characteristics of a transitional environment umgiesser et al 2014 de pascalis et al 2016b in addition a submarine freshwater source called citro san cataldo with an average flow rate of 2 m3 s is present at the mouth of the porta napoli channel cardellicchio et al 2016 the environmental problems of this domain are related to the degraded conditions of the air soil water sediments and biota environmental matrices the basins represent ecosystems potentially influenced by different anthropogenic pressures urbanization industrial waste agriculture commercial fishing and aquaculture in particular the basins include one of the most important steel production plants in europe ilva an oil refinery a cement plant two thermoelectric power plants and three waste incinerators in addition the city of taranto hosts an important commercial port and the main naval base of the italian navy located in bay i of the mar piccolo and in the southern area of the mar grande as a result both basins are affected by pollutant loads due to numerous outflows from civil military and industrial origins which are authorized and monitored only in some cases in the mar grande these discharges are found mainly along its northern and northeastern coasts on the other hand superficial hydrography in the mar piccolo is extremely scarce due to the nature of the rocks and the subtropical climate furthermore the mar piccolo has hosted several human activities for a long time and is home to the largest mussel farm in italy which covers 65 of the total surface of both the bays with a mussel production estimated to be approximately 64 000 t year cardellicchio et al 2016 fig 1 the need for an assessment of models able to predict hydrodynamics and pollutant dispersion in the two water basins is imperative this is further enhanced by the presence of two main rivers which provide a limited contribution to the water inflow i e the galeso 0 8 m3 s and the canale d aiedda 1 m3 s rivers where the latter also carries wastewater from sewage plants de pascalis et al 2016b 3 monitoring activity 3 1 fixed monitoring stations in december 2013 a meteo oceanographic station station o was installed in the mar grande basin at the geographical coordinates 40 27 6 n and 17 12 9 e fig 1 the local depth in this station is on average equal to 23 5m the station is provided with many instruments including a bottom mounted acoustic doppler current profile adcp a multidirectional wave array a weather station and a ctd measuring water salinity water temperature and depth in detail the weather system combines an ultrasonic wind speed and direction sensor accuracy 2 and 3 for velocity and direction respectively providing hourly averaged values velocity and direction of sea currents are measured by means of the adcp using a janus configuration which samples them along the water column with 0 50m vertical bin resolution and a 1 60m blanking distance mean current velocity profiles are collected continuously at hourly intervals using an average of 60 measurements acquired every 10s in this way hourly averaged velocity components along the water column are available for analysis in may 2014 a second monitoring station station n was installed in the navigable channel fig 1 equipped with a second bottom mounted adcp and a wave array also completed with the deployment of an ultrasonic tide gauge it is located at the geographical coordinates 40 473 n and 17 235 e fig 1 the local depth in this station is on average 13 7m also in this case taking into account the adcp size and its blanking distance the current velocities are assessed along the vertical from the seabed at constant intervals equally spaced every 0 5m the acoustic frequency of both the adcps is 600 khz and their velocity accuracy is 0 3 of the water velocity more details on the stations can be found in armenio et al 2016b and de serio and mossa 2018 the analysis presented in this study is based on the data collected in the target area including wind waves and currents from january 2014 to december 2014 for station o and from june 2014 to december 2014 for station n in particular data recorded by station o were used as input in the modelling phase as described in sect 4 2 1 while data recorded by station n were used to validate the hydrodynamic model sect 4 2 2 3 2 ad hoc monitoring surveys the institute of marine sciences of venice of the national research council cnr ismar and the dicatech of the polytechnic university of bari carried out a measurement field campaign on november 26 27 2014 during this campaign data of water current temperature and salinity were measured in the mar piccolo the three velocity components of the sea current were acquired by means of a vessel mounted acoustic doppler current profiler vm adcp while ctds were used to record water temperature and salinity fig 2 shows the locations of the measurement standing stations current velocities and directions were assessed in six stationing points bs along the vertical equally spaced every 0 50 m five of them are located in bay i of the mar piccolo and one in bay ii table 1 4 numerical models based on the field data available both models run to simulate the behavior of the two basins during the year 2014 first the meteorological model was used to estimate the atmospheric flow in the area during the whole year then these data were used as boundary conditions at the sea surface to force the hydrodynamic model together with other input data supplied by various national agencies in charge of monitoring them as will be specified in the following the results in terms of water velocity temperature and salinity were then compared with the field data to i validate the models examined ii assess the water circulation pattern in the domain and iii detect the associated temporal scales 4 1 meteorological model the meteorological simulations were performed using the calmet model scire et al 2000 it is a mass consistent diagnostic meteorological model developed by the california air resource board and successively improved by the u s environmental protection agency calmet allows the reconstruction of the three dimensional wind and air temperature fields as well as the two dimensional gridded fields of the main turbulence parameters such as the pasquill gifford stability classes the obukhov length and the mixing height in the present work calmet was used for the numerical simulation of the wind field in the study area and in particular to calculate the shear stress at the atmosphere sea interface it is worth noting that in simple applications and for small domains the value of the tangential stress inferred from wind measurements carried out in one point can be assigned as boundary condition on the entire sea surface when referring to wider coastal domains global forecasts and specific datasets provided by the european centre for medium range weather forecasts ecmwf are generally used to force the hydrodynamics nevertheless they have a spatial resolution of 10 km contrariwise in our case the availability of a high resolution wind field 300m makes it possible to consider the spatial inhomogeneities due to variations in wind intensity and direction and consequently to improve the modelling of the marine circulation recent studies on the joint use of meteorological and sea or lake models have highlighted a significant enhancement in the representation of the main basin scale motions if the shear stress description at the atmosphere sea surface interface is improved e g amadori et al 2018 the meteorological field in the studied area is significantly influenced by the sea breeze regime gariazzo et al 2007 di bernardino et al 2016 as is known when the synoptic wind is weak or absent the wind field in coastal areas is governed for most of the year by the breeze regime which typically shows spatial scales of tens of kilometers petenko et al 2011 pelliccioni et al 2015 this fact together with the presence of geographical features such as hills or mountains can make the wind field quite inhomogeneous the considerable differences in intensity and direction between the two wind roses depicted in fig 3 clearly demonstrate this fact the two wind roses were obtained based on the velocity data collected at the meteorological stations of grottaglie and marina di ginosa during the year 2014 in grottaglie fig 3a the wind is mainly from nne and sww i e perpendicular to the coastline and coincident with the main direction of the sea and land breezes similarly in marina di ginosa fig 3b the wind is aligned with nnw and sse this is further evidence that the hypothesis of wind with constant intensity and direction throughout the domain is far from being verified this assumption could therefore cause significant errors in the simulations of the anemological field and consequently of the marine circulation particularly when focusing on small regions as in the present case calmet uses a two step approach to compute the wind field the first step consists of the computation of the initial wind field on the basis of i measurements taken at the upper air stations ii dimension of the chosen grid and iii topography and land use in this way kinematic effects of terrain slopes and obstacles are considered in the second step calmet adopts an inverse distance method to perform the horizontal spatial interpolation of the wind measured at the surface meteorological stations to each grid point of the computational grid and extrapolates the velocity to higher layers using different parametric and similarity laws the velocity divergence referred to all three dimensional velocity components carried out for each grid point is minimized by calculating the mass unbalance and correcting the horizontal velocity components by means of an iterative process the final velocity field is obtained when the divergence in each grid node is smaller than a user specified threshold value for more details please refer to scire et al 2000 4 1 1 boundary conditions for the meteorological runs calmet needs a series of input data such as digital elevation model and land use category in situ measurements from surface meteorological stations and at least two vertical profiles of meteorological variables per day in this study we used data input related to the surface elevation from the shuttle radar topography mission srtm provided by nasa with spatial resolution of 1 arc sec 30 m while data provided by the corine land cover clc2000 level 2 with a spatial resolution of 100 m were employed for land use as is known to evaluate reliable mass consistent flow fields in complex terrain it is fundamental to conduct the analysis using as many surface stations as possible for this reason the study domain investigated by calmet included that of the hydrodynamic model and covered an area of approximately 3800 km2 from 636 km to 709 km utm east and from 4517 km to 4465 km utm north fig 4 the shape size and position of the numerical domain used for the simulation was chosen based on terrain characteristics i e shape and position of the coastline and terrain altitude and wind regimes of the area of interest prevailing and dominant winds come from ne and se respectively the domain was discretized horizontally by a 243 173 regular grid along the x longitude and y latitude axis with a spatial resolution of 300 m along the vertical nine levels with spacing increasing with altitude were fixed to ensure an accurate analysis close to the ground where larger velocity and temperature gradient are expected to occur in particular the lowest model level height was at 10 m above the ground i e the typical elevation of the surface meteorological stations as previously mentioned the simulations were carried out for the whole year 2014 calmet requires hourly data referred to the surface level while it needs at least a vertical profile every 12 h surface stations must be located within the domain while the upper air data may also be external to it in this study ten surface meteorological stations belonging to the networks managed by the italian company for flight assistance the regional agency for environmental protection arpa the italian tidal network the matera space geodesy center and by the port authority of taranto were taken into consideration the surface stations were selected paying attention to their distance from the target basin and the availability of data for the whole simulated period most of them were located close to the basins of interest to ensure that the wind field simulations were as accurate as possible the vertical profiles of the meteorological variables were taken from the upper air station launched every 12 h from brindisi airport icao code libr not included in fig 4 table 2 displays both the surface and upper air stations used for the numerical runs 4 2 hydrodynamic model shyfem is the hydrodynamic numerical model adopted in the present study it is an open source code developed at the national research council institute of marine sciences of venice http www ve ismar cnr it shyfem it consists of a finite element 3 d hydrodynamic model integrated by several modules for waves sediment transport eulerian and lagrangian tracer dispersion the model solves the 3 d primitive hydrodynamic equations vertically integrated on each layer using a semi implicit time stepping scheme that contributes to a stable solution of the equations details on shyfem can be found in bellafiore and umgiesser 2010 to compute water temperature and salinity field evolution the model solves the transport and diffusion equations using a first order explicit scheme corrected by a total variation diminishing method the model uses a k ε turbulence closure scheme to compute vertical turbulent viscosities and diffusivities for this aim the module of gotm general ocean turbulence model described in burchard and baumert 1995 was adapted in the shyfem model the finite element method for the horizontal discretization with its unstructured grid approach makes it possible to represent the complexity of coastal morphologies in fact it makes it possible to move from the lowest spatial resolution open sea to the higher one ports canals bays lagoons gradually without nesting different grids and has already been successfully applied to different types of coastal ecosystems ferrarin et al 2014 4 2 1 input and boundary conditions for hydrodynamic runs the bathymetry of the basin fig 5 up to a maximum depth of 200m was obtained by merging different data sets subsequently interpolated on the numerical grid in particular we used the bathymetry of the mar grande and mar piccolo digitalized by the navy hydrographic institute iim nautical charts the multibeam bathymetric data of the mar piccolo supplied by arpa puglia apulian regional agency for environmental protection the bathymetry of the gulf of taranto provided by the puglia geographic information system www sit puglia it the computation grid with triangular elements of the domain fig 5b was created by using the open source software gmsh geuzaine and remacle 2009 and consists of 15617 nodes and 29159 elements the vertical discretization is in 21 z levels and the spatial resolution ranges from 20 to 900 m the numerical grid was improved with respect to previous studies de pascalis et al 2016a b by adding the pylons of the punta penna bridge shown in the enlargement of fig 5b the annual simulation was performed with reference to the year 2014 the model was forced at the sea open boundary with temperature and salinity from e u copernicus marine service information and water level measured at s eligio pier table 3 by the national institute for environmental protection and research ispra for freshwater inputs the data adjusted after the calibration by de pascalis et al 2016b were used considering the yearly flow discharge of the sources quite constant table 3 the heat fluxes were computed by shyfem based on data i e solar radiation air temperature and humidity measured by station o fig 1 rain data were taken from the database of the puglia region agency for irrigation and forestry activities http www agrometeopuglia it the wind data variable in space and time throughout the entire domain resulting from the calmet simulations were used to force the shyfem model the data concerning salinity s and temperature t supplied by copernicus service products http marine copernicus eu and set as boundary condition at the open sea were reprocessed using vertical interpolation programs in order to be compatible with the 21 z levels of the shyfem model the initial conditions for t and s was set constant in all the basin i e t 20 c and s 37 psu the time necessary to reach the dynamic equilibrium spin up time for temperature and salinity is usually higher than that related to hydrodynamic velocities and water level therefore a spin up time of one month february 2014 was considered and the analysis of the 2014 results was then performed starting from march 5 modelling validation and results as an example fig 6 a illustrates the annual averaged wind velocity field computed by calmet for the whole domain at the lowest model level 10 m above the ground the annual averaged wind stress components τx along w e and τy along s n are also shown in fig 6b and c respectively as previously mentioned the shape of the coastline makes the target zone prone to high spatial variability of the wind the noticeable inhomogeneity of the wind field calculated by calmet with dominant winds coming from nw and sinuously extending towards e confirms that fact in particular a significant variability in wind direction and intensity is evident in both the mar grande and mar piccolo basins this fact makes the need to use a meteorological model even more evident given the strong relationship between wind velocity and surface stress hence simplified approaches based on the use of meteorological data collected from a single measuring station to force the hydrodynamic model could be inadequate in situations such as the one considered in the present work it is worth noting that calmet and shyfem models were both calibrated in previous research di bernardino et al 2016 de pascalis et al 2016a b thus in this study the validation was operated at the end of the complete modelling procedure that is by referring to the final results of the shyfem model in turn forced with the results of the calmet model the data acquired in station o fig 1 and during the survey in bss stations fig 2 were used for the comparison with the shyfem model outputs specifically based on the dates of the survey we decided to carry out the validation referring to the data acquired in november 2014 5 1 validation procedure with data of station n the shyfem model current velocity outputs were compared with data measured in station n of navigable channel fig 1 during november 2014 fig 7 shows the rose plots of the measured and modelled horizontal velocity for two representative depths i e near surface 1 5 m from the sea surface and near bottom 8 5 m from the sea surface a double circulation in the navigable channel is evident with prevailing outflowing currents near the surface and inflowing ones near the bottom this aspect is well reproduced by the model the surface current roses measured by the adcp shows a higher variability compared to their simulated counterparts nevertheless the predominant direction sw i e exiting the mar piccolo towards the mar grande and the velocities magnitude agree with regard to the near bottom currents the model is again able to reproduce the predominant direction ne i e towards the mar piccolo and the current intensity specifically the model does not precisely reproduce some peaks observed in the ssw direction at the bottom as well as the surface variability around sw in any case considering the orientation of the navigable channel the main flow along the ne sw axis is correctly simulated this can be detected from fig 8 where the dispersion plots of the meridional velocity component i e sn positive northward near the surface and near the bottom respectively are displayed the correlation between the two series of data is around 0 5 near the surface and around 0 8 near the bottom thus meaning a better capability of the model to reproduce the near bottom dominant flow 5 2 validation with data of bs stations in this case to perform the comparison between the measured and the modelled profiles the latter were extracted at the grid point closest to the actual measuring station points furthermore both the zonal and the meridional velocity components were extracted for the corresponding acquisition time period of the field data as an example fig 9 a shows the comparison between the vertical profiles of the horizontal velocity vectors both modelled and measured in station bs2 fig 2 which are in the range 0 1 0 4 m s fig 9b displays the deviation between the modelled and measured velocity magnitudes at the corresponding depths the numerical model is generally able to reproduce the main current and with greater accuracy in the middle part of the water column in fact it tends to slightly overestimate the velocity current in the upper layers while it underestimates it near the bottom the anticyclonic rotation of the vectors while deepening is also reproduced by the model but it is slightly over shifted for all the bs stations examined the deviations in terms of magnitudes between the simulated and measured current velocity is 0 10 m s indicating with negative values an underestimation of the model with respect to the measurement this result is acceptable and satisfactory considering that the discrepancies could be ascribed to several reasons first the non perfect adherence of the measuring station point to the grid node or the possible time shift between the measured value and the modelling output to validate the temperature t and salinity s variables the shyfem results were compared with the observations sampled during the campaign on 26 27 november 2014 with the ctd fig 10 shows these comparisons as well as the differences between simulated and observed data residuals referring to stations bs1 bs3 bs4 only fig 2 for the sake of brevity quite a good matching is noted referring to the vertical trends which highlight for both t and s increasing values from the surface towards the bottom for all the stations investigated low rmse values are computed max rmse for s is 0 45 and for t is 0 6 moreover the residual values of t and s are always in the range 1 1 thus demonstrating a good agreement between simulated and measured data 5 3 modelling results for current temperature and salinity as is known temperature salinity and density gradients wind action non linear tidal interactions with basin morphology strongly influence the long term circulation and pollutant transport therefore the patterns of the surface and bottom annual averaged currents are here examined the residual circulation referred to the year 2014 depicted in fig 10 shows a structure very similar to that reproduced by de pascalis et al 2016b for the year 2013 in fact the surface current tends to outflow from the mar piccolo towards the mar grande and the open sea fig 11 a while there is an opposite behavior at the bottom fig 11b in general the velocities on the entire water column are rather small falling in the range of 0 02 0 10 m s with peaks at the bottom 0 2 m s near the navigable channel the data of november 2014 fig 8 also confirm this double bottom surface circulation to detect the annual averaged vertical stratification of salinity and temperature a surface to bottom difference was computed for t and s in each node of the domain fig 12 negative values of the difference mean higher values of these scalar variables at the bottom while positive values mean higher values at the surface in general a pronounced stratification for both salinity and temperature is observed near the industrial discharges outside the mar grande eni ilva1 ilva2 as well as near galeso and citrello submarine springs in bay i of the mar piccolo table 3 in the mar piccolo the salinity at the surface is lower than at the bottom by about 2 2 psu in bay i and 1 5 psu in bay ii in the mar grande the salinity gradient is lower than 0 8 psu fig 12a the presence of citri and urban discharges especially in the mar piccolo induces a salinity gradient decreasing from its bays towards the mar grande and the open sea vertical temperature gradient values fall in the range of 0 2 1 c range fig 12b specifically in the mar grande the annual averaged temperature at the surface exceeds the near bottom one by about 0 5 1 c while in the mar piccolo the opposite occurs in fact due to its lower depth the mar piccolo responds much more rapidly to the atmospheric forcing consequently regulating its temperature while the mar grande is more influenced by the vertical stratification this leads to a stable stratification with saltier and colder water near the bottom coming from the open sea and less saline and warmer water i e less dense water near the surface this peculiar horizontal and vertical density distribution gives rise to a long term estuarine circulation confirming what found by de pascalis et al 2016b with reference to 2013 6 time scale indicators the knowledge of the main features of the water fluxes in the basin allows us to obtain a clear picture of the transport and spreading processes of polluting substances and sediments a vigorous water exchange between a semi enclosed basin and the open sea could guarantee a rapid replacement of its water mass and some protection from pollution events depending on the ability to renew water masses the basin is more or less vulnerable to pollution hazards moreover it could also define the role of the basin in terms of nutrient distribution and trophic net ecosystem productivity nursery and connectivity propensity guélorget and perthuisot 1992 ghezzo et al 2015 as described in the following relating the basin hydrodynamics to such phenomena is possible if we focus on some specific indicators specifically linked to two different time scales these indexes are not easily measurable in the field and numerical modelling tools must be used to quantify them in the present study a lagrangian module was added to the main shyfem code to carry out particle tracking applications de pascalis et al 2016a the lagrangian module was tightly integrated into the hydrodynamic framework and run on the same unstructured grid as shyfem itself to avoid the numerical diffusion due to the interpolation of velocities onto different grids this module provides the particle trajectory i e the position of the particle at the new time step as a function of the position at the previous time step and the displacement due to both the advective velocities given by the current field and the ones given by the turbulent diffusion this computation was done in all three directions so the module differs from previous research where it was limited to the 2d case cucco and umgiesser 2006 bellafiore et al 2008 cucco et al 2009 thus referring to the time scale indicators the added value of the present study lies in the fact that they were calculated all i e wrt wtt and ti and specifically the ti was calculated in its 3d version further they are all based on an annual time scale i e 2014 6 1 water renewal time and water transit time over the years many studies have dealt with the concept of water renewal time wrt and water transit time wtt takeoka 1984 sanford and boicourt 1992 luketina 1998 wang et al 2004 the wrt is a time scale based on a eulerian reference system and is defined as the time needed to replace with clean water the mass of a conservative tracer originally released in the domain in each element and layer of the computational grid cucco et al 2009 cucco and umgiesser 2015 in our study the water coming from the open sea and from the submarine and surface springs was fixed with tracer concentration equal to zero this useful factor characterizes the system in terms of renewal capacity and resilience especially with reference to potential pollution phenomena kjerfve and magill 1989 umgiesser et al 2004 ferrarin et a 2014 furthermore being related to the exchange of water masses it could be a vector for different types of materials oil debris etc jordi et al 2006 davidson et al 2009 the knowledge of this transport time scale can also help to understand the complex behavior associated with bio geo chemical phenomena ghezzo et al 2010 the wtt is a time scale based on a lagrangian reference system it is defined as the time required by a particle seeded in each grid element and vertical layer to exit from the domain reaching the open sea bolin and rodhe 1973 dronkers and zimmerman 1982 de kreeke 1983 prandle 1984 cucco and umgiesser 2015 this value is typical of the point where the particle is initially released hence conceptually the two transport time scales are different in fact the wrt is a feature of the area under examination and of the main hydrodynamic patterns indicating how long it takes to exchange water masses with new water coming from both the sea and the rivers discharging into the basin instead the wtt is a characteristic of the single particle flowing into the basin until it leaves it and therefore it essentially depends on the number and initial positions of seeded particles in this study about 107 of passive particles were released on march 1 2014 homogeneously vertically and horizontally inside the mar grande and mar piccolo basins in order to have statistical robustness for the computed trajectories details on the lagrangian method used are in de pascalis et al 2016a 6 2 trapping index although significant the wrt and wtt time scales represent two different aspects of the transport dynamics of the system thus high values of wrt and wtt do not always identify the areas where the water masses remain trapped and the exchange is poor to better characterize areas of stagnation cucco and umgiesser 2015 proposed a new dimensionless index in a first 2d version called trapping index ti which is defined as the normalized product between the water renewal and transit times this allows the identification of areas where both these time scales are high as well as hot spots where water masses might be especially vulnerable to pollution a first attempt to define a 3d ti was made by de pascalis et al 2016a nevertheless it provided vague results being the computation applied to a very short temporal period of investigation that is two months in this work ti is extended to the 3d case on an annual scale and calculated at each point of the domain for each vertical level according to the formula 1 t i w r t w t t w r t max w t t max where the subscript max indicates the overall maximum values of wrt and wtt in the basin as a result ti ranges from 0 to 1 meaning increasing water stagnation the trapping index is expected to be large only in areas where both residence time and transit time are large in areas where one of these time scales is large and the other is small the trapping index will also be small it will be negligible in areas where both time scales are small it is therefore possible to easily identify areas that show high residence and transit times e g areas that have a small tendency to exchange water with the surrounding areas and that can be identified with water masses trapped in the basin fig 13 displays the wrt mapping at surface and near bottom levels for the year 2014 analogously to the salinity and temperature distributions also in this case we can observe the different behavior of the three sub basins i e bay ii bay i and the mar grande the longest renewal time characterizes bay ii with values around 40 days the wrt decreases to around 32 days in bay i and finally to 12 days in the mar grande the vertical distribution corroborates the estimates computed for the year 2013 by de pascalis et al 2016b showing surface waters that have the tendency to stagnate more with respect to the bottom ones in fact due to the intense inflow from the open sea bottom waters are replaced even 10 days faster than the surface waters fig 14 shows the distribution of the wtt near the surface and the bottom layer as a general consideration the wtt map shows a higher spatial variability with respect to that experienced by wrt which is due to the chaotic lagrangian effect and to the initial seeding of particles not comparable to the eulerian continuum of tracer concentration cucco et al 2006 the wtt distribution highlights that the lagrangian particles released in the surface layer of the mar grande are transported offshore faster than the ones released at the bottom fig 14 because of the intense outflowing at the surface fig 11 this is especially true in the mar grande where this time difference reaches even twenty days in bays ii and i we note a similar behavior but the difference between surface and bottom wtt values is halved based on current intensities more comparable along the depth fig 15 a and b shows the ti index plotted on the surface and bottom layers respectively in both cases its distribution confirms the intuitive characterization for the three sub basins from the most retentive bay ii to the one with highest rate of water exchange with the open sea mar grande moreover some hot spots are evident in the three sub basins at the surface are to be found fig 15a i in the mar grande the port area due to the presence of docks and piers which are obstacles to water flowing and the south eastern inlet where the southern shore at the navy base is located ii in bay i the large retentive zone that extends from the navy arsenal fig 2 to the ilva water pump serving the industrial refrigeration system iii in bay ii characterized by an even more retentive central area the small inlet neighbouring the d ajedda channel table 3 near the bottom fig 15b we generally observe lower values of ti with respect to the surface consistently with both the wtt and the wrt distributions the highest ti values of the basin are concentrated in bay ii in particular along the shoreline as expected considering that these are the shallowest areas of the domain analogously to the surface also at the bottom the hot spots with the largest values of ti are the inlet close to the d ajedda channel and the one of the navy base 7 discussion the hydrodynamic and thermohaline results described in the previous sections confirm an established baroclinic estuarine circulation through the three sub basins as already found by de pascalis et al 2016b in any case it is worth noting that de pascalis et al 2016b reached this conclusion by forcing their numerical model with the data of 2013 while in our modelling we used the data of 2014 accomplishing an analogous result is thus meaningful because it highlights that on the annual temporal average the hydrodynamic and thermohaline patterns in the basin have a recurrent established and typical behavior in fact the annual averaged trends of surface fig 11a and bottom currents fig 11b highlight the presence of a dual opposite circulation with different patterns in fact two different water masses can be recognized the salty and cold water entering the system on the bottom layer and the less salty and warmer water outward directed on to the surface fig 12 in this case the role of the estuarine circulation engine generally played by the river in a typical estuary is attributable to submarine and subaerial freshwater springs while the bottom current represents the salt wedge entering the system from the open sea in particular in the present study these hydrodynamic patterns are validated based on the velocities measured by both the fixed station n and during the survey obtaining a satisfactory matching previous researches used salinity and temperature data to validate indirectly their numerical velocity fields de pascalis et al 2016b it is also important to point out that the hydrodynamic model has been forced at the sea surface by using the wind stress components coming from the meteorological model calmet such a choice of boundary conditions is motived by the fact that wind speed and direction measured by a single meteorological station cannot be considered representative of the wind field present on the whole area of interest despite the limited size of the basin see fig 3 it is worthwhile mentioning that one of the strong points of diagnostic models like calmet is that of being able to perform simulations with relatively small computational costs even in the case of high spatial resolution in the present work the latter is 300 m which is much higher than that generally adopted in numerical weather prediction nwp models which represent an alternative to diagnostic models see e g zannetti 1990 nwps are generally used to forecast the time evolution of the atmosphere through the numerical integration of the equations of fluid mechanics that describe the dynamics and thermodynamics of gases however nwp models require much larger computational costs and their use in complex situations is not straightforward see e g valerio et al 2017 these findings allowed us to evaluate some proper time scales strongly related to the hydrodynamics of the system i e the wrt and the wtt times especially the wtt time is a novel parameter computed for an annual temporal period for the whole domain in fact umgiesser et al 2004 examined only the taranto seas wrt time for the year 2005 limited to a 2d approach de pascalis et al 2016b described only the annual averaged wrt distribution for the year 2013 in both the mar piccolo and the mar grande basins but did not analyze the wtt time nor the ti index the trend of the wrt time obtained in the previous research is confirmed by our study the effect of the bottom marine water intrusion is to accelerate the bottom layer water exchange on average by ten days nevertheless by now analyzing also the wtt time we observed that the surface lagrangian transport directed outside the domain is much faster than the bottom one with a remarkable time difference 10 20 days therefore it is worth noting that a single time scale i e only the renewal time or only the transit time could offer a misleading perception of the real situation in fact finding high values of one of these time scales might not necessarily mean a slow water exchange indeed a high renewal time but a low transit time might be caused by water masses that pass through the area but come from other zones cucco and umgiesser 2015 for this reason the ti trapping index which is the simple normalized product between the residence and the transit time was introduced it is also worth observing that this index is only based on the hydrodynamics of the basin and not on chemical or biological parameters directly related to the water quality based on the values assumed by the ti index and comparing them along the vertical we mapped the areas of potential trapped water masses and consequently the hot spots as being very exposed to pollution and scarce water quality on annual average they are displayed in fig 16 with increasing numbering as their sensitivity increases and result from the comparison between the superficial and near bottom distributions of ti based on the highest values observed the procedure of the macro zonation consists in the following phases we divided the total basin in subareas of order of magnitude o 104 m2 for each subarea the spatial average of the ti values was calculated this operation applied to both surface and bottom ti distributions after this for each subarea we compared the averaged superficial and bottom tis and assumed the maximum of the two as the ti data to be plotted in the sensitivity map the largest part of the mar grande area 1 in fig 15 positively affected by the exchange flows with the open sea is the least exposed to possible retention with ti in the range of 0 0 1 two more sensitive areas in the mar grande with ti in the 0 1 0 3 range are the port area where docks and piers intercept and slow the flow and the south eastern inlet hosting the navy base due to its topographic configuration the same level of sensitivity is identified in bay i along the northern coast and in the southern part near the connection with the two channels they are all marked as areas 2 the central part of bay i as well as that of bay ii are characterized by higher sensitivity to stagnation with ti in the 0 3 0 4 range areas 3 a higher sensitivity is noted in areas 4 ti in the 0 4 0 5 range along the whole shoreline of bay ii and in two hot spots of bay i i e near the ilva pumping system and the naval shipyard the inlet neighbouring the d ajedda channel is the most sensitive area area 5 due to its ti values greater than 0 5 because of its topographic shape and depth based on fig 16 a sort of hierarchy in the zones most exposed to a potential degradation is thus established 8 conclusion in heavily anthropized and confined coastal basins the identification of areas most exposed to human activities and pollution events is a priority aspect in decision planning and coastal management for this purpose in the present study the meteorological calmet model and the hydrodynamic shyfem model were used in conjunction to i deduce the main physical processes typical of the mar grande and mar piccolo system ii identify the time scales related to these processes such as the wrt and the wtt times iii detect the areas most exposed to potential pollution based on the ti index the validation of the modelling procedure was carried out referring to the results of the shyfem model in turn forced with the results of the calmet one for this scope the available field data of current temperature and salinity measured in station n fig 1 and in stations bss fig 2 for the month of november 2014 were used the main results of the validation highlight that referring to velocity the correlation values are around 0 5 0 8 indicating that the numerical model is generally able to reproduce the main current along depth as for t and s their residuals are in the range 1 1 thus demonstrating a good agreement between simulated and measured data the results of the validated model demonstrated that the system has a typical horizontal and vertical density distribution giving rise to a long term estuarine circulation confirming previous research de pascalis et al 2016a b the key parameters assessed in order to identify possible sensitive areas subject to stagnation were the water renewal time and the water transit time which are strictly related to the hydrodynamic patterns in the basin they showed that for a complete water renewal in bay ii forty days are needed only twelve are needed for the mar grande moreover the flowing of lagrangian particles released in the surface layer is generally faster than near the bottom to have more complete information a trapping index ti based on both wrt and wtt was identified this index allowed us to map areas with different levels of sensitivity in the system and thus to recognize a sort of hierarchy specifically in the mar grande two hot spots are observed the port area and the south eastern inlet hosting the naval base in bay i we detect two other hot spots near the ilva pumping system and the naval shipyard bay ii is characterized by high sensitivity along the whole shoreline with the inlet neighbouring the d ajedda channel found to be the area most exposed to a potential degradation based on these results it is evident that particular care should be paid to the planning of periodic monitoring actions to safeguard the system author contribution vc gc rv and mm devised and funded the research mm supervised the research gl pm and mm conceived and coordinated the activities ea fdp fds adb and ap performed the numerical modelling and analyzed the results ea and fds wrote the paper ea mbm ddp fdp fds adb gl pm mm and ap discussed and revised the paper declaration of competing interest all the authors of the paper detecting sensitive areas in confined shallow basins i e elvira armenio mouldi ben meftah gennaro capasso vera corbelli diana de padova francesca de pascalis francesca de serio annalisa di bernardino giovanni leuzzi paolo monti michele mossa agnese pini and raffaele velardo declare that there is no conflict of interest regarding the publication of this article acknowledgments the monitoring stations were settled in the frame of the italian flagship project ritmare and with funds from pon r c 2007 13 project chief scientist of both michele mossa the present work was partially funded by an agreement between the polytechnic university of bari and the special commissioner for urgent measures of reclamation environmental improvements and redevelopment of taranto gratefully acknowledged 
26053,coastal shallow basins are often heavily anthropized and greatly exposed to environmental risk areas thus requiring strict monitoring action by local authorities and stakeholders preventive measures against environmental degradation and early warning to hazards have been proved to benefit from the combined use of numerical models and field measurements the present work sets out to show the potential of a meteorological hydrodynamic model system validated with field data to identify the main physical processes characterizing a semi enclosed basin located in the inner part of the ionian sea in southern italy furthermore based on the model results we adopted some convenient indicators especially related to flow exchanges in order to identify and characterize the area in the basin most sensitive to environmental problems the results highlight the retentive feature of the inner part of the basin and different times necessary for the water renewal in both the surface and bottom layers keywords field measurements meteorological model hydrodynamic model semi enclosed basin sea currents 1 introduction a common challenge in coastal management is to balance environmental conservation priorities human activities and economic development an essential tool in the decision support system is the possibility to detect and identify the most sensitive coastal areas i e areas exposed to strong human and industrial pressure where environmental problems especially linked to transport and diffusion of pollutants assume relevant importance there are many pollutant sources in heavily anthropized coastal sites unfortunately available data to identify them and to establish their greater or lesser danger are often scarce widespread pollutant inputs can even be associated with inland sources and can be transported underground in the aquifer and discharged into the sea consequently knowledge of possible paths and fate of polluting tracers sediments trapping water renewal times is of paramount importance for coastal management plans and in situ decision making moreover the mapping of the most critical and sensitive areas within a coastal basin based on some key factors could be of great benefit for this scope guiding possible monitoring actions and rehabilitation interventions armenio et al 2018 2019 de serio and mossa 2016 2018 de padova et al 2017a b de carolis et al 2013 kjerfve and magill 1989 this is the main reason for the present study nowadays predictive operational oceanography takes into account regional sub regional and shelf coastal scales based on coupled models of wave current and tracer dynamics the interaction between sea and atmosphere is often limitedly modelled by means of empirical bulk formulas providing wind stress as a function of wind velocity and depending on empirical drag coefficients samaras et al 2016 gaeta et al 2016 wróbel niedźwiecka et al 2019 this approach must face some limitations to provide reliable results on the sea circulation in the basin in fact understanding the physical processes involved at the atmosphere sea interface is complex as well as essential especially in terms of exchange of momentum between wind and sea surface di bernardino et al 2016 thus it is desirable to address numerical meteorological models simulating the overlying atmospheric flow in this way meteorological input data such as wind speed and direction air temperature and atmospheric pressure among others can be used as boundary conditions at the atmosphere sea interface furthermore hydrodynamic numerical models need a setup a calibration and a validation so that their integration with waves current and tide data is fundamental umgiesser et al 2004 armenio et al 2016a 2017 cannata et al 2018 it is also worth noting that the accuracy of model outputs relies on the quantity quality and duration of the available observations therefore extensive field measurements are of primary importance and monitoring actions should be judiciously programmed samaras et al 2016 trotta et al 2017 in this study we aim to examine the main hydrodynamic processes in a coastal basin with lagoon features which is assumed as a target case by using a meteorological and a hydrodynamic model a semi enclosed basin located in southern italy exposed to heavy urban industrial and military activities was used for this scope de padova et al 2017a b the effectiveness of the proposed model chain was provided by comparing the model results and the field data which were acquired onsite by means of two fixed monitoring stations and operational surveys the runs covered an annual period and the model results were provided onto a mesh with a high spatial resolution in this way we could identify some typical trends in the water circulation and exchanges which makes it possible to detect the most vulnerable locations in the domain examined based on some suitable indicators following cucco and umgiesser 2015 and de pascalis et al 2016a the water transit time the water renewal time and the trapping index were defined and computed for this scope the added value of this study with respect to previous numerical works applied to the same area de pascalis et al 2016a b is in i the adoption of a meteorological model to provide time varying and space varying meteorological data as realistic input for the implementation of the hydrodynamic model ii the time period used for the simulation that is the whole year 2014 while previous simulations referred to 2013 for which proper field measurements were available iii the computation of all the three indicators related to the basin vulnerability iv this computation was based on the entire annual period rather than on a short time window such as couple of months thus resulting more reliable the paper is structured in the following way sections 2 and 3 respectively describe the area studied and the monitoring activity section 4 presents the numerical models used section 5 describes the validation of the model outputs with field data and the main hydrodynamic features section 6 identifies the key indicators used to assess the basin vulnerability the discussion on these findings is in section 7 2 study area the proposed approach was applied to a target area located in the inner part of the gulf of taranto in the ionian sea along the coast of southern italy fig 1 the study area is composed of the mar grande and the inner basin of mar piccolo which in turn consists of two bays respectively named bay i and bay ii the mar grande and mar piccolo basins are joined by means of an artificial channel namely the navigable channel 58m wide and 12m deep and a natural one namely the porta napoli channel 150 m wide and 2 5 m deep the total area of the mar piccolo is about 21 7 km2 and the average depth is about 7 m the total area of the mar grande is about 35 km2 while its maximum depth is about 35 m in its central area along the outer perimeter of the mar grande there are two small islands called cheradi isles joined by a long breakwater in the mar piccolo there are about 34 freshwater submarine sources of karst origin called citri lisco et al 2016 they can be temporary or permanent during the year depending on the rainfall and the dynamics of the submarine water intrusion in the springs the presence of these freshwater inputs influences the salinity gradients of the basin giving it the typical characteristics of a transitional environment umgiesser et al 2014 de pascalis et al 2016b in addition a submarine freshwater source called citro san cataldo with an average flow rate of 2 m3 s is present at the mouth of the porta napoli channel cardellicchio et al 2016 the environmental problems of this domain are related to the degraded conditions of the air soil water sediments and biota environmental matrices the basins represent ecosystems potentially influenced by different anthropogenic pressures urbanization industrial waste agriculture commercial fishing and aquaculture in particular the basins include one of the most important steel production plants in europe ilva an oil refinery a cement plant two thermoelectric power plants and three waste incinerators in addition the city of taranto hosts an important commercial port and the main naval base of the italian navy located in bay i of the mar piccolo and in the southern area of the mar grande as a result both basins are affected by pollutant loads due to numerous outflows from civil military and industrial origins which are authorized and monitored only in some cases in the mar grande these discharges are found mainly along its northern and northeastern coasts on the other hand superficial hydrography in the mar piccolo is extremely scarce due to the nature of the rocks and the subtropical climate furthermore the mar piccolo has hosted several human activities for a long time and is home to the largest mussel farm in italy which covers 65 of the total surface of both the bays with a mussel production estimated to be approximately 64 000 t year cardellicchio et al 2016 fig 1 the need for an assessment of models able to predict hydrodynamics and pollutant dispersion in the two water basins is imperative this is further enhanced by the presence of two main rivers which provide a limited contribution to the water inflow i e the galeso 0 8 m3 s and the canale d aiedda 1 m3 s rivers where the latter also carries wastewater from sewage plants de pascalis et al 2016b 3 monitoring activity 3 1 fixed monitoring stations in december 2013 a meteo oceanographic station station o was installed in the mar grande basin at the geographical coordinates 40 27 6 n and 17 12 9 e fig 1 the local depth in this station is on average equal to 23 5m the station is provided with many instruments including a bottom mounted acoustic doppler current profile adcp a multidirectional wave array a weather station and a ctd measuring water salinity water temperature and depth in detail the weather system combines an ultrasonic wind speed and direction sensor accuracy 2 and 3 for velocity and direction respectively providing hourly averaged values velocity and direction of sea currents are measured by means of the adcp using a janus configuration which samples them along the water column with 0 50m vertical bin resolution and a 1 60m blanking distance mean current velocity profiles are collected continuously at hourly intervals using an average of 60 measurements acquired every 10s in this way hourly averaged velocity components along the water column are available for analysis in may 2014 a second monitoring station station n was installed in the navigable channel fig 1 equipped with a second bottom mounted adcp and a wave array also completed with the deployment of an ultrasonic tide gauge it is located at the geographical coordinates 40 473 n and 17 235 e fig 1 the local depth in this station is on average 13 7m also in this case taking into account the adcp size and its blanking distance the current velocities are assessed along the vertical from the seabed at constant intervals equally spaced every 0 5m the acoustic frequency of both the adcps is 600 khz and their velocity accuracy is 0 3 of the water velocity more details on the stations can be found in armenio et al 2016b and de serio and mossa 2018 the analysis presented in this study is based on the data collected in the target area including wind waves and currents from january 2014 to december 2014 for station o and from june 2014 to december 2014 for station n in particular data recorded by station o were used as input in the modelling phase as described in sect 4 2 1 while data recorded by station n were used to validate the hydrodynamic model sect 4 2 2 3 2 ad hoc monitoring surveys the institute of marine sciences of venice of the national research council cnr ismar and the dicatech of the polytechnic university of bari carried out a measurement field campaign on november 26 27 2014 during this campaign data of water current temperature and salinity were measured in the mar piccolo the three velocity components of the sea current were acquired by means of a vessel mounted acoustic doppler current profiler vm adcp while ctds were used to record water temperature and salinity fig 2 shows the locations of the measurement standing stations current velocities and directions were assessed in six stationing points bs along the vertical equally spaced every 0 50 m five of them are located in bay i of the mar piccolo and one in bay ii table 1 4 numerical models based on the field data available both models run to simulate the behavior of the two basins during the year 2014 first the meteorological model was used to estimate the atmospheric flow in the area during the whole year then these data were used as boundary conditions at the sea surface to force the hydrodynamic model together with other input data supplied by various national agencies in charge of monitoring them as will be specified in the following the results in terms of water velocity temperature and salinity were then compared with the field data to i validate the models examined ii assess the water circulation pattern in the domain and iii detect the associated temporal scales 4 1 meteorological model the meteorological simulations were performed using the calmet model scire et al 2000 it is a mass consistent diagnostic meteorological model developed by the california air resource board and successively improved by the u s environmental protection agency calmet allows the reconstruction of the three dimensional wind and air temperature fields as well as the two dimensional gridded fields of the main turbulence parameters such as the pasquill gifford stability classes the obukhov length and the mixing height in the present work calmet was used for the numerical simulation of the wind field in the study area and in particular to calculate the shear stress at the atmosphere sea interface it is worth noting that in simple applications and for small domains the value of the tangential stress inferred from wind measurements carried out in one point can be assigned as boundary condition on the entire sea surface when referring to wider coastal domains global forecasts and specific datasets provided by the european centre for medium range weather forecasts ecmwf are generally used to force the hydrodynamics nevertheless they have a spatial resolution of 10 km contrariwise in our case the availability of a high resolution wind field 300m makes it possible to consider the spatial inhomogeneities due to variations in wind intensity and direction and consequently to improve the modelling of the marine circulation recent studies on the joint use of meteorological and sea or lake models have highlighted a significant enhancement in the representation of the main basin scale motions if the shear stress description at the atmosphere sea surface interface is improved e g amadori et al 2018 the meteorological field in the studied area is significantly influenced by the sea breeze regime gariazzo et al 2007 di bernardino et al 2016 as is known when the synoptic wind is weak or absent the wind field in coastal areas is governed for most of the year by the breeze regime which typically shows spatial scales of tens of kilometers petenko et al 2011 pelliccioni et al 2015 this fact together with the presence of geographical features such as hills or mountains can make the wind field quite inhomogeneous the considerable differences in intensity and direction between the two wind roses depicted in fig 3 clearly demonstrate this fact the two wind roses were obtained based on the velocity data collected at the meteorological stations of grottaglie and marina di ginosa during the year 2014 in grottaglie fig 3a the wind is mainly from nne and sww i e perpendicular to the coastline and coincident with the main direction of the sea and land breezes similarly in marina di ginosa fig 3b the wind is aligned with nnw and sse this is further evidence that the hypothesis of wind with constant intensity and direction throughout the domain is far from being verified this assumption could therefore cause significant errors in the simulations of the anemological field and consequently of the marine circulation particularly when focusing on small regions as in the present case calmet uses a two step approach to compute the wind field the first step consists of the computation of the initial wind field on the basis of i measurements taken at the upper air stations ii dimension of the chosen grid and iii topography and land use in this way kinematic effects of terrain slopes and obstacles are considered in the second step calmet adopts an inverse distance method to perform the horizontal spatial interpolation of the wind measured at the surface meteorological stations to each grid point of the computational grid and extrapolates the velocity to higher layers using different parametric and similarity laws the velocity divergence referred to all three dimensional velocity components carried out for each grid point is minimized by calculating the mass unbalance and correcting the horizontal velocity components by means of an iterative process the final velocity field is obtained when the divergence in each grid node is smaller than a user specified threshold value for more details please refer to scire et al 2000 4 1 1 boundary conditions for the meteorological runs calmet needs a series of input data such as digital elevation model and land use category in situ measurements from surface meteorological stations and at least two vertical profiles of meteorological variables per day in this study we used data input related to the surface elevation from the shuttle radar topography mission srtm provided by nasa with spatial resolution of 1 arc sec 30 m while data provided by the corine land cover clc2000 level 2 with a spatial resolution of 100 m were employed for land use as is known to evaluate reliable mass consistent flow fields in complex terrain it is fundamental to conduct the analysis using as many surface stations as possible for this reason the study domain investigated by calmet included that of the hydrodynamic model and covered an area of approximately 3800 km2 from 636 km to 709 km utm east and from 4517 km to 4465 km utm north fig 4 the shape size and position of the numerical domain used for the simulation was chosen based on terrain characteristics i e shape and position of the coastline and terrain altitude and wind regimes of the area of interest prevailing and dominant winds come from ne and se respectively the domain was discretized horizontally by a 243 173 regular grid along the x longitude and y latitude axis with a spatial resolution of 300 m along the vertical nine levels with spacing increasing with altitude were fixed to ensure an accurate analysis close to the ground where larger velocity and temperature gradient are expected to occur in particular the lowest model level height was at 10 m above the ground i e the typical elevation of the surface meteorological stations as previously mentioned the simulations were carried out for the whole year 2014 calmet requires hourly data referred to the surface level while it needs at least a vertical profile every 12 h surface stations must be located within the domain while the upper air data may also be external to it in this study ten surface meteorological stations belonging to the networks managed by the italian company for flight assistance the regional agency for environmental protection arpa the italian tidal network the matera space geodesy center and by the port authority of taranto were taken into consideration the surface stations were selected paying attention to their distance from the target basin and the availability of data for the whole simulated period most of them were located close to the basins of interest to ensure that the wind field simulations were as accurate as possible the vertical profiles of the meteorological variables were taken from the upper air station launched every 12 h from brindisi airport icao code libr not included in fig 4 table 2 displays both the surface and upper air stations used for the numerical runs 4 2 hydrodynamic model shyfem is the hydrodynamic numerical model adopted in the present study it is an open source code developed at the national research council institute of marine sciences of venice http www ve ismar cnr it shyfem it consists of a finite element 3 d hydrodynamic model integrated by several modules for waves sediment transport eulerian and lagrangian tracer dispersion the model solves the 3 d primitive hydrodynamic equations vertically integrated on each layer using a semi implicit time stepping scheme that contributes to a stable solution of the equations details on shyfem can be found in bellafiore and umgiesser 2010 to compute water temperature and salinity field evolution the model solves the transport and diffusion equations using a first order explicit scheme corrected by a total variation diminishing method the model uses a k ε turbulence closure scheme to compute vertical turbulent viscosities and diffusivities for this aim the module of gotm general ocean turbulence model described in burchard and baumert 1995 was adapted in the shyfem model the finite element method for the horizontal discretization with its unstructured grid approach makes it possible to represent the complexity of coastal morphologies in fact it makes it possible to move from the lowest spatial resolution open sea to the higher one ports canals bays lagoons gradually without nesting different grids and has already been successfully applied to different types of coastal ecosystems ferrarin et al 2014 4 2 1 input and boundary conditions for hydrodynamic runs the bathymetry of the basin fig 5 up to a maximum depth of 200m was obtained by merging different data sets subsequently interpolated on the numerical grid in particular we used the bathymetry of the mar grande and mar piccolo digitalized by the navy hydrographic institute iim nautical charts the multibeam bathymetric data of the mar piccolo supplied by arpa puglia apulian regional agency for environmental protection the bathymetry of the gulf of taranto provided by the puglia geographic information system www sit puglia it the computation grid with triangular elements of the domain fig 5b was created by using the open source software gmsh geuzaine and remacle 2009 and consists of 15617 nodes and 29159 elements the vertical discretization is in 21 z levels and the spatial resolution ranges from 20 to 900 m the numerical grid was improved with respect to previous studies de pascalis et al 2016a b by adding the pylons of the punta penna bridge shown in the enlargement of fig 5b the annual simulation was performed with reference to the year 2014 the model was forced at the sea open boundary with temperature and salinity from e u copernicus marine service information and water level measured at s eligio pier table 3 by the national institute for environmental protection and research ispra for freshwater inputs the data adjusted after the calibration by de pascalis et al 2016b were used considering the yearly flow discharge of the sources quite constant table 3 the heat fluxes were computed by shyfem based on data i e solar radiation air temperature and humidity measured by station o fig 1 rain data were taken from the database of the puglia region agency for irrigation and forestry activities http www agrometeopuglia it the wind data variable in space and time throughout the entire domain resulting from the calmet simulations were used to force the shyfem model the data concerning salinity s and temperature t supplied by copernicus service products http marine copernicus eu and set as boundary condition at the open sea were reprocessed using vertical interpolation programs in order to be compatible with the 21 z levels of the shyfem model the initial conditions for t and s was set constant in all the basin i e t 20 c and s 37 psu the time necessary to reach the dynamic equilibrium spin up time for temperature and salinity is usually higher than that related to hydrodynamic velocities and water level therefore a spin up time of one month february 2014 was considered and the analysis of the 2014 results was then performed starting from march 5 modelling validation and results as an example fig 6 a illustrates the annual averaged wind velocity field computed by calmet for the whole domain at the lowest model level 10 m above the ground the annual averaged wind stress components τx along w e and τy along s n are also shown in fig 6b and c respectively as previously mentioned the shape of the coastline makes the target zone prone to high spatial variability of the wind the noticeable inhomogeneity of the wind field calculated by calmet with dominant winds coming from nw and sinuously extending towards e confirms that fact in particular a significant variability in wind direction and intensity is evident in both the mar grande and mar piccolo basins this fact makes the need to use a meteorological model even more evident given the strong relationship between wind velocity and surface stress hence simplified approaches based on the use of meteorological data collected from a single measuring station to force the hydrodynamic model could be inadequate in situations such as the one considered in the present work it is worth noting that calmet and shyfem models were both calibrated in previous research di bernardino et al 2016 de pascalis et al 2016a b thus in this study the validation was operated at the end of the complete modelling procedure that is by referring to the final results of the shyfem model in turn forced with the results of the calmet model the data acquired in station o fig 1 and during the survey in bss stations fig 2 were used for the comparison with the shyfem model outputs specifically based on the dates of the survey we decided to carry out the validation referring to the data acquired in november 2014 5 1 validation procedure with data of station n the shyfem model current velocity outputs were compared with data measured in station n of navigable channel fig 1 during november 2014 fig 7 shows the rose plots of the measured and modelled horizontal velocity for two representative depths i e near surface 1 5 m from the sea surface and near bottom 8 5 m from the sea surface a double circulation in the navigable channel is evident with prevailing outflowing currents near the surface and inflowing ones near the bottom this aspect is well reproduced by the model the surface current roses measured by the adcp shows a higher variability compared to their simulated counterparts nevertheless the predominant direction sw i e exiting the mar piccolo towards the mar grande and the velocities magnitude agree with regard to the near bottom currents the model is again able to reproduce the predominant direction ne i e towards the mar piccolo and the current intensity specifically the model does not precisely reproduce some peaks observed in the ssw direction at the bottom as well as the surface variability around sw in any case considering the orientation of the navigable channel the main flow along the ne sw axis is correctly simulated this can be detected from fig 8 where the dispersion plots of the meridional velocity component i e sn positive northward near the surface and near the bottom respectively are displayed the correlation between the two series of data is around 0 5 near the surface and around 0 8 near the bottom thus meaning a better capability of the model to reproduce the near bottom dominant flow 5 2 validation with data of bs stations in this case to perform the comparison between the measured and the modelled profiles the latter were extracted at the grid point closest to the actual measuring station points furthermore both the zonal and the meridional velocity components were extracted for the corresponding acquisition time period of the field data as an example fig 9 a shows the comparison between the vertical profiles of the horizontal velocity vectors both modelled and measured in station bs2 fig 2 which are in the range 0 1 0 4 m s fig 9b displays the deviation between the modelled and measured velocity magnitudes at the corresponding depths the numerical model is generally able to reproduce the main current and with greater accuracy in the middle part of the water column in fact it tends to slightly overestimate the velocity current in the upper layers while it underestimates it near the bottom the anticyclonic rotation of the vectors while deepening is also reproduced by the model but it is slightly over shifted for all the bs stations examined the deviations in terms of magnitudes between the simulated and measured current velocity is 0 10 m s indicating with negative values an underestimation of the model with respect to the measurement this result is acceptable and satisfactory considering that the discrepancies could be ascribed to several reasons first the non perfect adherence of the measuring station point to the grid node or the possible time shift between the measured value and the modelling output to validate the temperature t and salinity s variables the shyfem results were compared with the observations sampled during the campaign on 26 27 november 2014 with the ctd fig 10 shows these comparisons as well as the differences between simulated and observed data residuals referring to stations bs1 bs3 bs4 only fig 2 for the sake of brevity quite a good matching is noted referring to the vertical trends which highlight for both t and s increasing values from the surface towards the bottom for all the stations investigated low rmse values are computed max rmse for s is 0 45 and for t is 0 6 moreover the residual values of t and s are always in the range 1 1 thus demonstrating a good agreement between simulated and measured data 5 3 modelling results for current temperature and salinity as is known temperature salinity and density gradients wind action non linear tidal interactions with basin morphology strongly influence the long term circulation and pollutant transport therefore the patterns of the surface and bottom annual averaged currents are here examined the residual circulation referred to the year 2014 depicted in fig 10 shows a structure very similar to that reproduced by de pascalis et al 2016b for the year 2013 in fact the surface current tends to outflow from the mar piccolo towards the mar grande and the open sea fig 11 a while there is an opposite behavior at the bottom fig 11b in general the velocities on the entire water column are rather small falling in the range of 0 02 0 10 m s with peaks at the bottom 0 2 m s near the navigable channel the data of november 2014 fig 8 also confirm this double bottom surface circulation to detect the annual averaged vertical stratification of salinity and temperature a surface to bottom difference was computed for t and s in each node of the domain fig 12 negative values of the difference mean higher values of these scalar variables at the bottom while positive values mean higher values at the surface in general a pronounced stratification for both salinity and temperature is observed near the industrial discharges outside the mar grande eni ilva1 ilva2 as well as near galeso and citrello submarine springs in bay i of the mar piccolo table 3 in the mar piccolo the salinity at the surface is lower than at the bottom by about 2 2 psu in bay i and 1 5 psu in bay ii in the mar grande the salinity gradient is lower than 0 8 psu fig 12a the presence of citri and urban discharges especially in the mar piccolo induces a salinity gradient decreasing from its bays towards the mar grande and the open sea vertical temperature gradient values fall in the range of 0 2 1 c range fig 12b specifically in the mar grande the annual averaged temperature at the surface exceeds the near bottom one by about 0 5 1 c while in the mar piccolo the opposite occurs in fact due to its lower depth the mar piccolo responds much more rapidly to the atmospheric forcing consequently regulating its temperature while the mar grande is more influenced by the vertical stratification this leads to a stable stratification with saltier and colder water near the bottom coming from the open sea and less saline and warmer water i e less dense water near the surface this peculiar horizontal and vertical density distribution gives rise to a long term estuarine circulation confirming what found by de pascalis et al 2016b with reference to 2013 6 time scale indicators the knowledge of the main features of the water fluxes in the basin allows us to obtain a clear picture of the transport and spreading processes of polluting substances and sediments a vigorous water exchange between a semi enclosed basin and the open sea could guarantee a rapid replacement of its water mass and some protection from pollution events depending on the ability to renew water masses the basin is more or less vulnerable to pollution hazards moreover it could also define the role of the basin in terms of nutrient distribution and trophic net ecosystem productivity nursery and connectivity propensity guélorget and perthuisot 1992 ghezzo et al 2015 as described in the following relating the basin hydrodynamics to such phenomena is possible if we focus on some specific indicators specifically linked to two different time scales these indexes are not easily measurable in the field and numerical modelling tools must be used to quantify them in the present study a lagrangian module was added to the main shyfem code to carry out particle tracking applications de pascalis et al 2016a the lagrangian module was tightly integrated into the hydrodynamic framework and run on the same unstructured grid as shyfem itself to avoid the numerical diffusion due to the interpolation of velocities onto different grids this module provides the particle trajectory i e the position of the particle at the new time step as a function of the position at the previous time step and the displacement due to both the advective velocities given by the current field and the ones given by the turbulent diffusion this computation was done in all three directions so the module differs from previous research where it was limited to the 2d case cucco and umgiesser 2006 bellafiore et al 2008 cucco et al 2009 thus referring to the time scale indicators the added value of the present study lies in the fact that they were calculated all i e wrt wtt and ti and specifically the ti was calculated in its 3d version further they are all based on an annual time scale i e 2014 6 1 water renewal time and water transit time over the years many studies have dealt with the concept of water renewal time wrt and water transit time wtt takeoka 1984 sanford and boicourt 1992 luketina 1998 wang et al 2004 the wrt is a time scale based on a eulerian reference system and is defined as the time needed to replace with clean water the mass of a conservative tracer originally released in the domain in each element and layer of the computational grid cucco et al 2009 cucco and umgiesser 2015 in our study the water coming from the open sea and from the submarine and surface springs was fixed with tracer concentration equal to zero this useful factor characterizes the system in terms of renewal capacity and resilience especially with reference to potential pollution phenomena kjerfve and magill 1989 umgiesser et al 2004 ferrarin et a 2014 furthermore being related to the exchange of water masses it could be a vector for different types of materials oil debris etc jordi et al 2006 davidson et al 2009 the knowledge of this transport time scale can also help to understand the complex behavior associated with bio geo chemical phenomena ghezzo et al 2010 the wtt is a time scale based on a lagrangian reference system it is defined as the time required by a particle seeded in each grid element and vertical layer to exit from the domain reaching the open sea bolin and rodhe 1973 dronkers and zimmerman 1982 de kreeke 1983 prandle 1984 cucco and umgiesser 2015 this value is typical of the point where the particle is initially released hence conceptually the two transport time scales are different in fact the wrt is a feature of the area under examination and of the main hydrodynamic patterns indicating how long it takes to exchange water masses with new water coming from both the sea and the rivers discharging into the basin instead the wtt is a characteristic of the single particle flowing into the basin until it leaves it and therefore it essentially depends on the number and initial positions of seeded particles in this study about 107 of passive particles were released on march 1 2014 homogeneously vertically and horizontally inside the mar grande and mar piccolo basins in order to have statistical robustness for the computed trajectories details on the lagrangian method used are in de pascalis et al 2016a 6 2 trapping index although significant the wrt and wtt time scales represent two different aspects of the transport dynamics of the system thus high values of wrt and wtt do not always identify the areas where the water masses remain trapped and the exchange is poor to better characterize areas of stagnation cucco and umgiesser 2015 proposed a new dimensionless index in a first 2d version called trapping index ti which is defined as the normalized product between the water renewal and transit times this allows the identification of areas where both these time scales are high as well as hot spots where water masses might be especially vulnerable to pollution a first attempt to define a 3d ti was made by de pascalis et al 2016a nevertheless it provided vague results being the computation applied to a very short temporal period of investigation that is two months in this work ti is extended to the 3d case on an annual scale and calculated at each point of the domain for each vertical level according to the formula 1 t i w r t w t t w r t max w t t max where the subscript max indicates the overall maximum values of wrt and wtt in the basin as a result ti ranges from 0 to 1 meaning increasing water stagnation the trapping index is expected to be large only in areas where both residence time and transit time are large in areas where one of these time scales is large and the other is small the trapping index will also be small it will be negligible in areas where both time scales are small it is therefore possible to easily identify areas that show high residence and transit times e g areas that have a small tendency to exchange water with the surrounding areas and that can be identified with water masses trapped in the basin fig 13 displays the wrt mapping at surface and near bottom levels for the year 2014 analogously to the salinity and temperature distributions also in this case we can observe the different behavior of the three sub basins i e bay ii bay i and the mar grande the longest renewal time characterizes bay ii with values around 40 days the wrt decreases to around 32 days in bay i and finally to 12 days in the mar grande the vertical distribution corroborates the estimates computed for the year 2013 by de pascalis et al 2016b showing surface waters that have the tendency to stagnate more with respect to the bottom ones in fact due to the intense inflow from the open sea bottom waters are replaced even 10 days faster than the surface waters fig 14 shows the distribution of the wtt near the surface and the bottom layer as a general consideration the wtt map shows a higher spatial variability with respect to that experienced by wrt which is due to the chaotic lagrangian effect and to the initial seeding of particles not comparable to the eulerian continuum of tracer concentration cucco et al 2006 the wtt distribution highlights that the lagrangian particles released in the surface layer of the mar grande are transported offshore faster than the ones released at the bottom fig 14 because of the intense outflowing at the surface fig 11 this is especially true in the mar grande where this time difference reaches even twenty days in bays ii and i we note a similar behavior but the difference between surface and bottom wtt values is halved based on current intensities more comparable along the depth fig 15 a and b shows the ti index plotted on the surface and bottom layers respectively in both cases its distribution confirms the intuitive characterization for the three sub basins from the most retentive bay ii to the one with highest rate of water exchange with the open sea mar grande moreover some hot spots are evident in the three sub basins at the surface are to be found fig 15a i in the mar grande the port area due to the presence of docks and piers which are obstacles to water flowing and the south eastern inlet where the southern shore at the navy base is located ii in bay i the large retentive zone that extends from the navy arsenal fig 2 to the ilva water pump serving the industrial refrigeration system iii in bay ii characterized by an even more retentive central area the small inlet neighbouring the d ajedda channel table 3 near the bottom fig 15b we generally observe lower values of ti with respect to the surface consistently with both the wtt and the wrt distributions the highest ti values of the basin are concentrated in bay ii in particular along the shoreline as expected considering that these are the shallowest areas of the domain analogously to the surface also at the bottom the hot spots with the largest values of ti are the inlet close to the d ajedda channel and the one of the navy base 7 discussion the hydrodynamic and thermohaline results described in the previous sections confirm an established baroclinic estuarine circulation through the three sub basins as already found by de pascalis et al 2016b in any case it is worth noting that de pascalis et al 2016b reached this conclusion by forcing their numerical model with the data of 2013 while in our modelling we used the data of 2014 accomplishing an analogous result is thus meaningful because it highlights that on the annual temporal average the hydrodynamic and thermohaline patterns in the basin have a recurrent established and typical behavior in fact the annual averaged trends of surface fig 11a and bottom currents fig 11b highlight the presence of a dual opposite circulation with different patterns in fact two different water masses can be recognized the salty and cold water entering the system on the bottom layer and the less salty and warmer water outward directed on to the surface fig 12 in this case the role of the estuarine circulation engine generally played by the river in a typical estuary is attributable to submarine and subaerial freshwater springs while the bottom current represents the salt wedge entering the system from the open sea in particular in the present study these hydrodynamic patterns are validated based on the velocities measured by both the fixed station n and during the survey obtaining a satisfactory matching previous researches used salinity and temperature data to validate indirectly their numerical velocity fields de pascalis et al 2016b it is also important to point out that the hydrodynamic model has been forced at the sea surface by using the wind stress components coming from the meteorological model calmet such a choice of boundary conditions is motived by the fact that wind speed and direction measured by a single meteorological station cannot be considered representative of the wind field present on the whole area of interest despite the limited size of the basin see fig 3 it is worthwhile mentioning that one of the strong points of diagnostic models like calmet is that of being able to perform simulations with relatively small computational costs even in the case of high spatial resolution in the present work the latter is 300 m which is much higher than that generally adopted in numerical weather prediction nwp models which represent an alternative to diagnostic models see e g zannetti 1990 nwps are generally used to forecast the time evolution of the atmosphere through the numerical integration of the equations of fluid mechanics that describe the dynamics and thermodynamics of gases however nwp models require much larger computational costs and their use in complex situations is not straightforward see e g valerio et al 2017 these findings allowed us to evaluate some proper time scales strongly related to the hydrodynamics of the system i e the wrt and the wtt times especially the wtt time is a novel parameter computed for an annual temporal period for the whole domain in fact umgiesser et al 2004 examined only the taranto seas wrt time for the year 2005 limited to a 2d approach de pascalis et al 2016b described only the annual averaged wrt distribution for the year 2013 in both the mar piccolo and the mar grande basins but did not analyze the wtt time nor the ti index the trend of the wrt time obtained in the previous research is confirmed by our study the effect of the bottom marine water intrusion is to accelerate the bottom layer water exchange on average by ten days nevertheless by now analyzing also the wtt time we observed that the surface lagrangian transport directed outside the domain is much faster than the bottom one with a remarkable time difference 10 20 days therefore it is worth noting that a single time scale i e only the renewal time or only the transit time could offer a misleading perception of the real situation in fact finding high values of one of these time scales might not necessarily mean a slow water exchange indeed a high renewal time but a low transit time might be caused by water masses that pass through the area but come from other zones cucco and umgiesser 2015 for this reason the ti trapping index which is the simple normalized product between the residence and the transit time was introduced it is also worth observing that this index is only based on the hydrodynamics of the basin and not on chemical or biological parameters directly related to the water quality based on the values assumed by the ti index and comparing them along the vertical we mapped the areas of potential trapped water masses and consequently the hot spots as being very exposed to pollution and scarce water quality on annual average they are displayed in fig 16 with increasing numbering as their sensitivity increases and result from the comparison between the superficial and near bottom distributions of ti based on the highest values observed the procedure of the macro zonation consists in the following phases we divided the total basin in subareas of order of magnitude o 104 m2 for each subarea the spatial average of the ti values was calculated this operation applied to both surface and bottom ti distributions after this for each subarea we compared the averaged superficial and bottom tis and assumed the maximum of the two as the ti data to be plotted in the sensitivity map the largest part of the mar grande area 1 in fig 15 positively affected by the exchange flows with the open sea is the least exposed to possible retention with ti in the range of 0 0 1 two more sensitive areas in the mar grande with ti in the 0 1 0 3 range are the port area where docks and piers intercept and slow the flow and the south eastern inlet hosting the navy base due to its topographic configuration the same level of sensitivity is identified in bay i along the northern coast and in the southern part near the connection with the two channels they are all marked as areas 2 the central part of bay i as well as that of bay ii are characterized by higher sensitivity to stagnation with ti in the 0 3 0 4 range areas 3 a higher sensitivity is noted in areas 4 ti in the 0 4 0 5 range along the whole shoreline of bay ii and in two hot spots of bay i i e near the ilva pumping system and the naval shipyard the inlet neighbouring the d ajedda channel is the most sensitive area area 5 due to its ti values greater than 0 5 because of its topographic shape and depth based on fig 16 a sort of hierarchy in the zones most exposed to a potential degradation is thus established 8 conclusion in heavily anthropized and confined coastal basins the identification of areas most exposed to human activities and pollution events is a priority aspect in decision planning and coastal management for this purpose in the present study the meteorological calmet model and the hydrodynamic shyfem model were used in conjunction to i deduce the main physical processes typical of the mar grande and mar piccolo system ii identify the time scales related to these processes such as the wrt and the wtt times iii detect the areas most exposed to potential pollution based on the ti index the validation of the modelling procedure was carried out referring to the results of the shyfem model in turn forced with the results of the calmet one for this scope the available field data of current temperature and salinity measured in station n fig 1 and in stations bss fig 2 for the month of november 2014 were used the main results of the validation highlight that referring to velocity the correlation values are around 0 5 0 8 indicating that the numerical model is generally able to reproduce the main current along depth as for t and s their residuals are in the range 1 1 thus demonstrating a good agreement between simulated and measured data the results of the validated model demonstrated that the system has a typical horizontal and vertical density distribution giving rise to a long term estuarine circulation confirming previous research de pascalis et al 2016a b the key parameters assessed in order to identify possible sensitive areas subject to stagnation were the water renewal time and the water transit time which are strictly related to the hydrodynamic patterns in the basin they showed that for a complete water renewal in bay ii forty days are needed only twelve are needed for the mar grande moreover the flowing of lagrangian particles released in the surface layer is generally faster than near the bottom to have more complete information a trapping index ti based on both wrt and wtt was identified this index allowed us to map areas with different levels of sensitivity in the system and thus to recognize a sort of hierarchy specifically in the mar grande two hot spots are observed the port area and the south eastern inlet hosting the naval base in bay i we detect two other hot spots near the ilva pumping system and the naval shipyard bay ii is characterized by high sensitivity along the whole shoreline with the inlet neighbouring the d ajedda channel found to be the area most exposed to a potential degradation based on these results it is evident that particular care should be paid to the planning of periodic monitoring actions to safeguard the system author contribution vc gc rv and mm devised and funded the research mm supervised the research gl pm and mm conceived and coordinated the activities ea fdp fds adb and ap performed the numerical modelling and analyzed the results ea and fds wrote the paper ea mbm ddp fdp fds adb gl pm mm and ap discussed and revised the paper declaration of competing interest all the authors of the paper detecting sensitive areas in confined shallow basins i e elvira armenio mouldi ben meftah gennaro capasso vera corbelli diana de padova francesca de pascalis francesca de serio annalisa di bernardino giovanni leuzzi paolo monti michele mossa agnese pini and raffaele velardo declare that there is no conflict of interest regarding the publication of this article acknowledgments the monitoring stations were settled in the frame of the italian flagship project ritmare and with funds from pon r c 2007 13 project chief scientist of both michele mossa the present work was partially funded by an agreement between the polytechnic university of bari and the special commissioner for urgent measures of reclamation environmental improvements and redevelopment of taranto gratefully acknowledged 
26054,variability and extremes in streamflow wind speeds temperatures and solar irradiance influence supply and demand for electricity however previous research falls short in addressing the risks that joint uncertainties in these processes pose in power systems and wholesale electricity markets limiting challenges have included the large areal extents of power systems high temporal resolutions hourly or sub hourly and the data volumes and computational intensities required this paper introduces an open source modeling framework for evaluating risks from correlated hydrometeorological processes in electricity markets at decision relevant scales the framework is able to reproduce historical price dynamics in high profile systems while also offering unique capabilities for stochastic simulation synthetic generation of weather and hydrologic variables is coupled with simulation models of relevant infrastructure dams power plants our model will allow the role of hydrometeorological uncertainty including compound extreme events on electricity market outcomes to be explored using publicly available models keywords stochastic hydrology weather electricity markets prices 1 introduction in recent years interest has grown in exploring the effects of hydrometeorological variability and especially extreme events on the operations of bulk power systems large interconnected systems of generation transmission and load demand collins et al 2018 forster and lilliestam 2011 franco and sanstad 2008 kern and characklis 2017 staffell and pfenninger 2018 tarroja et al 2016 turner et al 2019 van vliet et al 2016 2012 voisin et al 2018 both droughts and floods compromise the operations of hydroelectric dams gleick 2017 su et al 2017 tarroja et al 2016 while droughts in particular can also impact thermal power plants that are dependent on cooling water van vliet et al 2016 2012 air temperatures influence a range of system components most notably electricity demand for heating and cooling franco and sanstad 2008 in addition as variable energy resources like wind and solar expand their share of the power mix the grid is becoming more sensitive to fluctuations in wind speeds and solar irradiance collins et al 2018 staffell and pfenninger 2018 by influencing supply and demand for electricity hydrometeorological processes have direct impacts on pollution e g increased greenhouse gas emissions collins et al 2018 hardin et al 2017 tarroja et al 2016 wholesale electricity prices boogert and dupont 2005 collins et al 2018 seel et al 2018 and the financial standing of suppliers of electricity e g retail utilities renewable energy producers and consumers bain and acker 2018 boogert and dupont 2005 foster et al 2015 kern and characklis 2017 kern et al 2015 however with few exceptions turner et al 2019 previous investigations fall short in assessing the holistic influence of hydrometeorological variability on bulk power systems past research efforts assess operational and financial risks from exposure to variability in a more limited set of hydrometeorological processes collins et al 2018 kern et al 2015 e g streamflow and temperatures or wind speeds and solar irradiance do not consider these effects within the context of large interconnected power systems kern and characklis 2017 and or do not assess impacts probabilistically hardin et al 2017 these shortcomings may be partly attributable to the challenges of modeling bulk electric power systems at sufficient scale and resolution to simulate system operations in a realistic way and over sufficient time horizons to explore joint uncertainty in multiple correlated input variables interconnected power systems span areas so large that system operators often have some ability to deal with spatially heterogeneous stressors for example a localized power supply shortfall caused by drought in one area might be managed by importing power from other areas where water and thus electricity from hydropower production and water cooled generators is more abundant from a modeling perspective this necessitates adopting system topologies that extend beyond a single watershed state and region hydrometeorological uncertainty and power system risks can also manifest on different time scales extreme meteorological and hydrological conditions can have durations on the order of days floods najibi and devineni 2017 heat waves weeks to months wind droughts and years hydrological droughts andreadis et al 2005 whereas power system modeling requires an hourly or sub hourly time step pandzžić et al 2014 although stochastic modeling approaches can be used to create large synthetic records of hydrometeorological processes in order to explore risks from extreme events brown et al 2015 reed et al 2013 this poses a direct challenge to the use of computationally expensive integer programming within power system models pandzžić et al 2014 making large ensemble monte carlo simulations less tractable adding to these challenges is the potential presence of significant spatial and temporal covariance among key hydrometeorological processes jimenez et al 2011 woodhouse et al 2016 if significant correlations exist an increased number of model runs may be required to characterize the probability of coincident extremes e g widespread simultaneous hydrological drought a wind drought and a heat wave that may be of particular concern to power system operators mazdiyasni and aghakouchak 2015 turner et al 2019 the modeling scales resolutions and ensemble sizes required in exploring the risks to bulk electric systems from hydrometeorological variability present a challenge and few if any models capable of performing this type of analysis are publically available given recent increased interest among the research community in modeling interconnected systems e g food energy water logan 2015 a generalizable and open source modeling framework for simulating the influence of correlated hydrometeorological processes on power system dynamics at decision relevant scales would be a valuable addition the goal of this paper is to present such a framework the newly developed california and west coast power capow systems model capow was designed by the authors to explore a high profile test bed the west coast of the conterminous united states u s the bulk electric power systems covering most of the states of california oregon and washington are included as well as the two major wholesale electricity markets active across these states current gaps in coverage are the pacificorp west sacramento municipal utility district los angeles department of water and power balancing authorities capow is comprehensive in its treatment of stochastic weather and streamflow simulation of relevant infrastructure reservoir networks power systems and evaluation of outcomes system costs prices etc while focused on the u s west coast the steps required in building and executing the capow model as well as much of the code are fairly generalizable and can be transferred to other systems and interconnections of interest chowdhury et al 2019 most grid specific information used in the model is publically available anywhere in the u s generator size location fuel type prime mover type average heat rate etc hydrometeorological data used to simulate electricity demand wind solar and hydropower production are also available throughout the u s as well as hourly records of renewable energy production in each balancing authority through the eia analogous transmission grid information bi directional capacities is publically available for all wecc areas and for many if not all sub regions in the eastern interconnection note that to transfer the model to other regions additional capabilities that are not currently in capow may be required e g representing impacts of extreme cold air temperatures henry and pratson 2016 and a lack of cooling water availability due to low streamflow and temperatures miara et al 2017 van vliet et al 2016 2012 on thermal power plant functionality the model is python based all code and data required to run the capow model as well as some documentation of the model is available at https github com romulus97 capow py36 under the mit free software license 2 methods our description of methods parallels the capow model s work flow fig 1 beginning with a discussion of surface water and electric power system topologies including key physical assets e g power plants dams reservoirs and their connections i e water routing between reservoirs high voltage transmission pathways this is followed by a description of capow s unit commitment and economic dispatch uc ed model which is used to simulate actual power system operations the methods section ends with a description of our approach for stochastically generating model inputs from historical weather and streamflow data 2 1 system topology 2 1 1 electric power in order to model the west coast grid the case study explored here we first adopt a 21 zone topology of the western electricity coordinating council wecc a regulatory body charged with reducing risks to the western grid by enforcing standards and assessing reliability fig 2 this topology which has been used in the past by wecc and other researchers to assist in long term planning exercises ho et al 2016 mkarov et al 2010 groups balancing authorities utility footprints into multiple zones that are connected via aggregated transmission pathways throughout the region each zone to zone transmission pathway is associated with bi directional capacities i e maximum limits on zone to zone transfers of electricity estimated from publically available data western electricity coordinating council 2016 each zone in the network consists of 1 the load electricity demands of its member balancing authorities which fluctuate on hourly daily seasonal and annual time scales and 2 a portfolio of co located generation resources with which to meet those demands comprehensive databases of generators located in each node of the 21 zone wecc topology are publically available from multiple sources us environmental protection agency 2018 western electricity coordinating council system adequacy planning department 2015 these also contain information on relevant operating characteristics for each generator e g fuel type capacity average heat rate that are used to formulate the uc ed simulation model there are two major trading hubs for wholesale electricity on the u s west coast 1 the mid columbia mid c market that serves as a hub for much of the pacific northwest region and 2 the california independent system operator caiso a competitive wholesale market that manages approximately 80 of california s electricity flow the 21 zone wecc topology shown in fig 2 includes five nodes red numbered that directly correspond to these markets node 1 pacific northwest corresponds to the mid c market and nodes 2 5 correspond to the caiso market nodes 2 5 also represent the service areas of three major utilities pacific gas and electric pg e southern california edison sce and san diego gas and electric sdg e currently only these five zones and power flows among them are modeled mechanistically using a uc ed model no uc ed models exist outside these five zones neighboring zones are considered only in terms of their exchanges of electricity with the core uc ed zones and these exchanges are modeled statistically see supplemental material 2 1 2 dams and reservoirs recent analyses of the impacts of drought on power generation in the western u s harto et al 2011 suggest that cooling water issues from low streamflow and high water temperatures pose a minor threat to thermal power plants in the region rather the primary mechanism through which hydrologic extremes can impact power system operations is through variability in hydropower generation within the wecc topology shown in fig 2 hydropower capacity makes up 58 of installed generating capacity in zone 1 pacific northwest 18 of generating capacity in zone 2 pg e valley and 4 of capacity in zone 3 sce us environmental protection agency 2018 fig s2 in the supplemental material section maps major 5 mw hydroelectric dams that participate in balancing authorities located within the five numbered zones that make up the uc ed model these dams primarily fall within the columbia river basin which spans several northwestern u s states and canada as well as the sacramento river san joaquin river and tulare lake basins in california publically available hydrologic mass balance models exist for 85 of the hydropower capacity in the pacific northwest versions of hyssr developed by the u s army corps of engineers to simulate the federal columbia river power system and a ressim model that simulates the operations of federal dams in the willamette river basin models exist for only 12 of the hydropower capacity in california the orca model herman and cohen 2019 which simulates the operations of major storage flood control dams in california much of the state s hydropower capacity is privately owned and located in high altitude areas of the sierra nevada mountains little information about the operation of these dams is publically available so hydropower production at these projects is simulated via an alternative approach in which hydropower production at upstream dams is predicted using observed streamflow downstream first for major high altitude hydroelectric dam in the sierra nevada mountains a corresponding downstream storage reservoir or stream gauge on the same river is identified in order to predict upstream hydropower generation at a given dam using observed streamflow downstream the calendar year is broken into four seasons winter spring summer and fall each season is assumed to follow a different set of operating rules that translate observed downstream flows into estimates of upstream hydropower production rules are fitted using the differential evolution algorithm in the scipy library of python based on root mean squared error rmse between observed and simulated hydropower production for each upstream dam about 15 of hydropower capacity in the pacific northwest and 20 of hydropower capacity in california are within the five core wecc zones that make up the uc ed model but fall outside the four river basins mentioned above and are not associated with publically available models these projects are modeled by scaling hydropower generation from nearby dams a more detailed description of how hydropower production is simulated on a daily basis can be found in the supplemental material 2 2 unit commitment and economic dispatch model the power system and reservoir network topologies described above form the basis of a unit commitment economic dispatch uc ed model that we use to simulate the operation of the five numbered wecc zones in fig 2 which include the mid c and caiso markets simulating the uc ed model for a single year at an hourly time step takes approximately 6 h using the cplex solver on a 16 core machine with 2 5 ghz processors using a linux operating system what follows is a general overview of the model s structure and functionality a mathematical formulation of the uc ed model can be found in the supplemental material we coded the uc ed model in python using the pyomo mathematical optimization package structuring it as an iterative mixed integer linear program over a user defined operating horizon e g 48 h deterministic optimization is used to minimize the cost of meeting demand for electricity and operating reserves including unit start costs no load costs fuel costs and penalties associated with transferring electricity between zones subject to constraints on individual generators and transmission paths costs are minimized by strategically dispatching scheduling generation from flexible generation resources natural gas power plants hydroelectric dams and system imports on an hourly basis variable renewable energy wind and solar are not dispatchable they can be consumed only when available as such they are typically treated as electricity demand reduction within a zone but can be also curtailed during periods of oversupply a single iteration of the uc ed model yields system costs and the least cost generating schedule over the operating horizon e g hours 1 48 however only the first 24 h of the solution is stored the remaining solution hours 25 48 is discarded and the whole process shifts one day into the future the next iteration of the model identifies a solution for the hours 25 48 while again looking 48 h into the future i e at hours 25 72 this ensures that the model does not have perfect foresight over unreasonably long time horizons when making decisions with path dependency e g turning on baseload power plants with high minimum up times simulation of the uc ed model creates hourly time series outputs that track provision of electricity and operating reserves by each generator the flow of electricity among zones plant specific and system wide emissions of co2 total operating costs and wholesale electricity prices co2 emissions from each power plant are calculated using historical epa egrid data that are used to estimate the kg co2 per mwh emissions for each plant note that total operating costs essentially refers to the value of the objective function in each hour the cumulative start no load and fuel costs across every power plant in every hour on the other hand wholesale electricity prices mwh are dynamic measures of the marginal value of electricity in each market i e how much generators would be paid to sell their electricity in each hour within the optimization wholesale prices are estimated for each zone as the shadow cost of an energy balance constraint at each zone i e the change in objective function value associated with a 1 mwh increase in demand at each zone calculating the shadow costs requires the uc ed model to first be solved in mixed integer form and then resolved as a linear program keeping all binary variables fixed from the integer solution in order to access dual values for relevant constraints in pyomo this yields a separate time series of wholesale electricity prices for each of the five wecc zones represented in the core uc ed model prices in the mid c market are assumed to be equivalent to prices for the pacific northwest zone to represent the caiso market prices for the four relevant zones in california pg e valley pg e bay sce and sdg e are weighted to determine an overall price for the market with the weights fitted via regression r2 0 75 p 1e 3 on observed values over the period 2012 2016 2 3 stochastic inputs the primary stochastic inputs to the uc ed model are electricity demand hourly wind and solar power production hourly and available hydropower production daily for each numbered zone in fig 2 several hydrometeorological processes air temperatures wind speeds solar irradiance and streamflow in turn drive these power system inputs in the following section we describe our approach for generating synthetic hydrometeorological time series 2 3 1 hydrometeorological variables 2 3 1 1 air temperatures wind speeds and solar irradiance we collect observed air temperatures wind speeds and solar irradiance data within major cities where electricity demand is highest and in areas known to have large amounts of installed wind and solar power capacity records of daily average temperature and wind speed over the period 1998 2017 come from noaa s global historical climatological network ghcn for seventeen meteorological stations distributed throughout the western u s table 1 global horizontal irradiance data come from the national renewable energy laboratory s national solar radiation database nsrdb sengupta et al 2018 both clear sky and observed irradiance data are acquired at a 30 min resolution and then aggregated to daily sums each weather station provides the data necessary to generate 365 day profiles of average temperature and wind speed for their respective locations we use solar irradiance data to created 365 day profiles of average clear sky cloudless conditions fig 3 1 t p n 1 y y 1 y t n y 2 w p n 1 y y 1 y w s n y 3 s p n 1 y y 1 y s n y where t p n average temperature on calendar day n across y years c t n y observed temperature on calendar day n in year y c w p n average wind speed on day n across y years m s w s n y observed wind speed on day n in year y m s s p n average clear sky irradiance on day n across y years w m2 s n y observed clear sky irradiance on day n in year y w m2 synthetic values of air temperatures wind speeds and solar irradiance are then generated by combining these average profiles e g blue series in panel a of fig 3 with stochastic representation of the autocorrelated residuals that deviate from these repeating signals e g the gray series in panel a of fig 3 average temperature and wind profiles are subtracted from observed temperature and wind speed values this yields a daily record of zero mean residuals i e deviations from average temperature and wind speed for each calendar day over the period 1998 2017 observed irradiance is subtracted from average clear sky irradiance yielding a daily record of losses due to cloud effects 4 r t d t d t p n 5 r w d w s d t w n 6 i l d s p n i d where r t d residual temperature on day d c r w d residual wind speed on day d m s i l d irradiance losses on day d w m2 residual temperatures and wind speeds as well as irradiance losses are then mean shifted to eliminate negative values and log transformed to approximate a gaussian distribution the residuals losses for each calendar day of the year are then divided by their respective standard deviations in order to control for seasonal heteroscedasticity 7 w r t d r t d ˆ σ t n 8 w r w d r w d ˆ σ w n 9 w i l d i l d ˆ σ i l n where w r t d whitened residual temperature on day d w r w d whitened residual wind speed on day d w i l d whitened irradiance losses on day d r t d ˆ mean shifted log transformed residual temperature on day d c r w d ˆ mean shifted log transformed residual wind speed on day d m s i l d ˆ mean shifted log transformed irradiance losses on day d w m2 σ t n standard deviation of transformed temperature residuals on calendar day n σ w n standard deviation of transformed wind speed residuals on calendar day n σ i l n standard deviation of transformed irradiance losses on calendar day n we then model the resultant whitened residuals and irradiance losses using a vector autoregressive var model in order to capture observed covariance across variables var models describe the behavior of a set of k variables over a given time period as a linear function of their past values and random samples from a multivariate normal distribution simulated values of each variable are stored in a k 1 vector y t which has as its i t h element y i t the value of the i t h variable at time t the lag of the model i e the number of previous time steps that are accounted for when estimating values in y t is denoted by the parameter p 10 y t c a 1 y t 1 a 2 y t 2 a p y t p ε t where c k x 1 vector of constants a i k x k matrix of coefficients ε t k x 1 vector of error terms t time period p model lag simulation of y t proceeds through random sampling of noise ε t from a multivariate normal distribution with a covariance matrix estimated from whitened residuals and irradiance losses for the period 1998 2017 the number of lags considered is determined via the akaike information criteria a fitted var model is used to simulate daily whitened temperature and wind speed residuals and irradiance losses for each ghcn and nsrdb site considered for as many years as desired simulated values are then un whitened by reversing equations 7 9 thus restoring heteroscedasticity and non normality they are then added back to the 365 day profiles reversing equations 4 6 yielding synthetic daily records of temperature and wind speeds 2 3 1 2 streamflow streamflow patterns on the west coast of the u s are driven by runoff from precipitation as rain and largely the melting of snow accumulated during the winter both total annual streamflow and the within year distribution of streamflow experienced in this region are known to be influenced by temperatures null et al 2010 at the same time there are significant correlations among the 85 separate spatially distributed streamflow gauges that drive capow s simulation of dam operations and hydropower production we make use of a gaussian copula to preserve the relationship between total annual streamflow and temperatures in stochastically generated samples first observed daily average temperatures 1953 2008 at the seventeen meteorological stations are converted to heating and cooling degree days which measures deviations from 18 33 degrees c 65 degrees f 11 h d d d s max 18 33 t d s 0 12 c d d d s max t d s 18 33 0 where h d d d s heating degree days on day d at station s c d d d s cooling degree days on day d at station s t d s average near surface air temperature on day d c at station s total annual hdds and cdds are calculated providing coarse measures of the hotness of a given year s summer and the coldness of a given year s winter total annual hdds and cdds and total annual streamflow are then transformed into quantile space by calculating the empirical cumulative probability distribution for each variable 13 p p q q where q total annual streamflow or degree days at a given site empirical probabilities are transformed again into a uniform distribution ranging from 1 to 1 as follows ensuring a mean of 0 across every variable 14 y 2 p 0 5 the covariance matrix c across all the variables at every site is estimated and then synthetic records of total annual streamflow and total annual hdds and cdds are generated by taking random samples from a multivariate normal distribution with mean 0 and covariance matrix c then back transforming reversing equations 13 and 14 the next step is to match total annual streamflow and total annual hdds and cdds simulated via the copula method with the synthetic daily temperatures generated in the previous section using a vector autoregressive var approach synthetic daily temperatures simulated using the var approach are converted to total annual hdds and cdds for each year of synthetic data desired we select a single year of total annual hdds and cdds generated using the var approach and then calculate the weighted average across every ghcn station weights are determined by the fraction of average annual flow across the 85 stream gauges that is contained within each ghcn station s surrounding area 15 w t s g 1 g a v f g a v t where w t s weight assigned to meteorological station s a v f g average annual flow at gauge site g closest to station s a v t average annual flow across all 85 stream gauges the weighted total annual hdds and cdds from the var model are compared alongside pairs of weighted total annual hdds and cdds generated using the copula method the smallest mean squared error difference is identified then the total annual streamflow values generated via the copula method are paired with the corresponding daily temperatures and also wind speeds and solar irradiance generated via var disaggregating total annual streamflow values down to a daily time step must be done in a manner that considers the potential influence of temperatures on the timing of streamflow throughout the year for example fig 4 shows the relationship between winter and spring temperatures and the timing of streamflow at two major reservoirs in california the top panel a shows 19 years 1997 2015 of weighted average temperatures across the ghcn stations calculated using weights from equation 15 lines are colored according to the mean temperature experienced over the first 24 weeks of the year the dark red line indicates the year with the hottest temperatures over this period 2015 and the dark blue line indicates the year with the coolest temperatures 2010 in panels b and c those same line colors are then used to plot contemporaneous full natural unregulated flows at folsom dam panel b and oroville dam panel c in california two large storage dams for which there are long historical flow records flows are shown in terms of standardized fractions that are created by dividing by total annual flows at each site at the top of panels b and c swarm plots identify the week of maximum streamflow for both dams years with higher average winter and spring temperatures red hued circles tend to be associated with earlier peak streamflow indicating earlier snowmelt and or major precipitation events in order to capture these dependencies between the timing of streamflow and temperatures we follow a nearest neighbor clustering approach similar to nowak et al 2010 the weights generated in equation 15 are used to create composite time series of temperatures across the 17 ghcn stations for both historical and simulated temperature data for each simulated year the historical record is searched for a past year that exhibited the most similar winter spring temperature profile in terms of mean squared error the identified historical year is then selected as the basis for determining daily flow fractions at each streamflow gauge site for the historical year selected daily flow fractions are calculated as follows 16 f f d g d f d g a f g where f f d s flow fraction for day d at streamflow gauge site g d f d s observed flow on day d at streamflow gauge site g a f g total annual flow observed at gauge site g flow fractions for each gauge site are then multiplied by simulated total annual flows to yield a synthetic record of daily flows across the study area 2 3 2 power system inputs the stochastic scenario generation framework permits the exploration of large ensembles of time series for temperatures wind speeds solar irradiance and streamflow these data are then converted to associated power system inputs for the uc ed model time series for each zone of hourly electricity demand wind and solar availability daily hydropower production and imports of electricity from other areas in the western u s table 2 provides an overview of the different approaches taken to translate raw hydrometeorological variables into power system inputs as well as their accuracies multi variate regression is used to simulate daily electricity demand solar and wind power production and system imports power flows along wecc paths listed in table 2 daily values are disaggregated down to an hourly time step by sampling from historical profiles daily values of available hydropower production are created by passing synthetic streamflow records through mass balance hydrologic models of dams in the columbia river basin and major storage reservoirs in california as well as through a machine learning representation of high altitude hydropower production in california detailed descriptions of all models used to translate raw hydrometeorological variables into power system inputs can be found in the supplemental material 3 results discussion 3 1 validation of uc ed formulation this paper proceeds with a validation of the uc ed model s ability to reproduce observed power system dynamics in particular wholesale electricity prices wholesale prices which are driven by changes in supply and demand can be viewed as aggregate measures of system performance high prices can indicate scarcity and low prices point to abundance we focus on an extended period of drought that occurred in california over the years 2012 2016 during this period in state hydropower generation decreased by an average of 40 gleick 2017 forcing the state to rely significantly more on electricity from natural gas power plants there has been considerable interest in exploring the impacts of this recent drought on pollutant emissions hardin et al 2017 as well as system costs and prices for retail electricity consumers gleick 2017 particularly when determining the latter an understanding of impacts on wholesale electricity prices is necessary retail distribution companies in california pg e sce and sdge all purchase electricity from the caiso market if the capow model is able simulate observed wholesale electricity prices over 2012 2016 with accuracy then the model could also be used to conduct controlled experiments designed to isolate the role of drought and or other hydrometorological extremes on wholesale prices revenues costs for utilities and ultimately retail prices for consumers natural gas price data used to validate the model i e compare historical caiso prices across the years 2012 2016 were obtained from eia s natural gas hub dataset although these data do not represent the exact price paid by power plants they do represent dynamic prices at major gas trading hubs these day to day fluctuations in gas prices are extremely important to capture eia s data on the delivered price of natural gas for power plants is typically listed on a monthly annual time step which would not allow us to capture more short term severe price spikes fig 5 compares observed daily average electricity prices in the caiso market alongside prices simulated by the uc ed model showing strong agreement r2 0 75 for the purposes of validating the uc ed model we used historical records of temperatures wind speeds solar irradiance and streamflow at the sites listed in table 2 thus discrepancies between observed and simulated prices are entirely due to the uc ed formulation itself and or discrepancies in fuel prices experienced in general the model accurately captures variation in electricity prices on daily time scales and above although model outputs include hourly prices hourly price dynamics e g peak and off peak patterns are not as well represented this is expected for a model reliant on a somewhat abstracted representation of the transmission network 3 2 validation of stochastic inputs the uc ed model s ability to capture more than 70 of daily variability in caiso electricity prices suggests that coupling it with stochastic simulations of weather and hydrology would enable probabilistic assessment of a broad set of hydrometeorological risks in wholesale electricity markets before using capow in this manner however the model s underlying stochastic engine i e the suite of approaches used to simulate weather and hydrological variables and relevant power system inputs must be validated 3 2 1 hydrometeorological variables given the large geographical extent considered as well as the highly interconnected nature of the u s west coast grid it is important that stochastically generated meteorological and hydrological inputs exhibit the same statistical dependencies as the historical record fig 6 shows correlation matrices calculated using historical data from the 17 ghcn stations and 7 nsrdb sites top left as well as historical data from the 85 stream gauges bottom left these are compared alongside correlation matrices calculated using 1000 years of corresponding stochastic data generated using the approaches described in section 2 3 lighter areas show positive correlation two locations variables that are more likely to both experience high low values simultaneously dark areas show negative correlations in general results show a high degree of fidelity between historical and simulated covariance across variables and space for example historical and simulated streamflow correlation matrices both show the same pockets of light values which are associated with highly correlated stream gauges located within the same watershed overall these results suggest that capow when run in stochastic mode is able to capture spatial heterogeneities in weather and hydrological processes e g the likelihood of experiencing high low temperatures wind speeds irradiance streamflow simultaneously at sites distributed across the entire region equally important the underlying stochastic engine of capow is able to reproduce observed statistical moments e g mean standard deviation in hydrometeorological conditions fig 7 shows close agreement between historical and simulated temperatures and wind speeds across the 17 ghcn stations in terms of percentile 1st 50th and 99th while also demonstrating the stochastic model s ability to occasionally generate more extreme min max values than the historical record in fig 8 a similar comparison is shown using streamflow data each panel includes historical blue red circles and simulated black line values for each of the 85 stream gauges considered red circles represent gauges in california mostly the sierra nevada mountains and blue circles represent gauges in the pacific northwest mostly the columbia river basin each panel represents a different percentile 1st 50th 99th as well as min max values note that in some cases negative values are shown this is an artifact of our use of bpa s modified flow dataset which consists of historical flows at gauge sites in the columbia river basin with modern human withdrawals applied at certain gauge sites this results in negative flow values water is subtracted from reservoir storage in general results suggest close agreement between the distributions of historical and stochastically generated streamflow values while also demonstrating the stochastic model s ability to occasionally generate more extreme min max values than the historical record 3 2 2 power system inputs a suite of models is used to translate raw temperatures wind speeds solar irradiance and streamflows into power system inputs including multivariate regression wind and solar power electricity demand system imports exports and hydrologic mass balance operational models of reservoirs hydropower coupled with our stochastic weather and streamflow generation techniques these models yield realistic time series of power system inputs that mimic historical data on seasonal daily and hour time scales table 2 for example fig 9 panel a shows historical blue and simulated red seasonality in wind power capacity factor a unitless number between 0 and 1 corresponding to the average hourly output of a wind farm as a fraction of installed capacity aggregated for the entire caiso system the simulated data is produced by coupling stochastically generated wind speeds at ghcn stations with a multivariate regression model of system wide wind power availability based on wind speeds table 2 and then adding in a record of synthetic residuals model errors results indicate alignment with historical data on a monthly basis with highest capacity factors occurring in the summer and lowest during winter this approach is also able to reproduce hourly and daily time series characteristics for wind power production fig 9 panel b shows close agreement between historical and simulated daily autocorrelation in wind power production suggesting the model does an adequate job preserving any statistically significant memory in daily wind power production fig 9 panel c shows historical and simulated seasonality in solar power capacity for the caiso system the simulated data is produced by coupling stochastically generated solar irradiance minus cloud effects at seven nsrdb sites with a multivariate regression model of system wide solar power availability based on site specific irradiance results indicate alignment with historical data on a monthly basis again with highest capacity factors occurring in the summer months and lowest during winter this approach is also able to reproduce hourly and daily time series characteristics for solar power production fig 9 panel d compares hourly capacity factors produced using historical irradiance data for a week in summer 2006 alongside stochastically generated solar power data for the same calendar week with differences being due to simulated cloud effects consideration was also given to volume of simulations required to achieve statistical convergence between historical and simulated power system inputs a primary motivating factor in developing the underlying framework of the capow model is to explore the impacts of hydrometeorological uncertainty especially extreme events on power systems and electricity markets to be useful in this regard the stochastic engine of capow as well as the uc ed model must be run over a sufficiently large number of years to produce the kind of low probability high magnitude tail events that are concerning to grid participants e g episodes of extreme shortfalls or overabundance in supply considering the high computational requirements of the uc ed model which relies on mixed integer programming a relevant question is how many years are enough fig 10 explores this question for the capow model each panel shows data for a different input in the caiso system hydropower production wind power production load electricity demand and net load defined here as load minus total renewable energy wind solar and hydropower and resources considered to be must run like nuclear and geothermal net demand is an important metric because it represents the amount of electricity that would need to be met by dispatchable generators coal and natural gas the colored lines measure the absolute difference between the historical record and synthetically generated values as a function of simulation volume for example in the bottom left panel load the red line tracks the difference between the historical record and stochastically simulated values in terms of the 99th percentile of hourly electricity demand at low simulation volumes this difference starts at around 280 mwh average hourly demand in the caiso market is more than 25 000 mwh indicating an error of less than 1 as the number of simulated years increases the absolute difference first increases but then stabilizes appearing to asymptotically approach a value close to 220 mwh stabilization occurs when increasing the number of simulation years has a negligible impact on the difference between historical and simulated values fig 10 shows that simulations from capow s stochastic engine tend to converge statistically after about 1000 years suggesting this would be a reasonable lower bound on simulation volume to run through the uc ed model overall our results suggest that capow s stochastic engine is able to reproduce historical statistical characteristics across multiple hydrometeorological variables and power system inputs needing approximately 1000 simulation years to achieve stable distributions a final validation step is to evaluate whether the stochastic engine creates an expanded distribution of system states in other words does simulation over 1000 years cause extreme events outside the historical record to emerge from joint uncertainties in individual system processes without directly running the uc ed model a preliminary analysis of this kind can be conducted using net load as a metric of interest since this typically correlates strongly with electricity prices and would be a key indicator of the potential for system shortfalls extremely high net demand and oversupply extremely low net load fig 11 evaluates net load in the caiso system under different scenarios the shaded areas show the distribution of net load over the period 1953 2008 simulated using historical hydrometeorological data colors correspond to different percentiles of net load ranging from 1st to 99th as well as the min max values for this time period net load simulated using hydrometeorological data from 1953 to 2008 is then compared alongside actual historical net load recorded for a recent year 2016 which is represented with a black line for the most part actual net load for 2016 is enveloped by the distribution of values simulated using 1953 2008 hydrometeorological data fig 11 also shows minimum and maximum values acquired from 1000 years of synthetic runs produced by the stochastic engine of capow blue dotted lines min max values produced by the stochastic engine suggest that the capow model by exploring joint uncertainties in hydrometeorolgical variables at sufficiently high simulation values is able to access rare extreme events outside the historical record the additional information provided by stochastic modeling appears to be especially valuable during late summer when net load is the highest and the stochastic model produces maximum values that are considerably larger than the highest values simulated using weather and hydrology from 1953 to 2008 these more extreme synthetic values are likely to include rare but plausible compound events in which combinations of high electricity demand and low renewable energy availability create extremely high net load with associated risks for reliability and high market prices 4 conclusions despite growing interest in the potential vulnerabilities of bulk electric power systems to hydrometeorological variability and extremes there are few if any open source modelling packages capable of exploring this issue in a comprehensive manner this paper presents a new model capow which we specifically designed to explore the influence of joint uncertainties in temperatures wind speeds solar irradiance and streamflow on bulk power systems and wholesale electricity markets capow couples synthetic generation of hydrometeorological variables with simulation models of relevant infrastructure dams power plants allowing for in depth exploration of the role of weather and hydrology on system outcomes the model is free and downloadable via public online repositories the capow model uses a topological representation of the conterminous u s west coast power system to form a unit commitment and economic dispatch uc ed model that simulates system operations and tracks performance system costs prices etc on an hourly basis when using historical weather and streamflow data as inputs to the model it is able to capture 75 of the variability in daily electricity prices in the caiso market although designed specifically with the u s west coast in mind the steps taken to construct capow as well as much of the code base can be extended to other systems of interest however some critical functionalities may need to be added for example capow does not currently represent thermal power plant curtailments due to inadequate cooling water supplies caused by low streamflows and high temperatures when run in stochastic mode capow couples the uc ed model with a stochastic engine that creates synthetic records of temperatures wind speeds solar irradiance and streamflow for a group of 17 meteorological stations 7 solar resource assessment sites and 85 stream gauges distributed throughout the west coast stochastically generated hydrometeorological variables are used to predict electricity demand via temperatures wind speeds wind power production via wind speeds solar power production via irradiance and hydropower availability via streamflows which then drive the uc ed model the statistical properties moments cross correlations time series characteristics of synthetic data produced mirror those of the historical record while also allowing for the generation of more extreme but plausible events exploring the joint uncertainty in relevant hydrometeorological variables is computationally tractable with the statistics of stochastic simulations converging with the historical record after approximately 1000 simulation years overall our framework which is also easily transferrable across systems and geographic areas simulates the operations of bulk electric power systems and wholesale markets at sufficient scales and resolutions to simulate system operations in a realistic way and over sufficient time horizons to explore joint uncertainty across multiple correlated variables of interest as such it should prove to be a valuable future resource for direct grid participants as well as the research community particularly in answering questions related to the vulnerability of the grid to future changes in hydroclimate as well as the sensitivity of variable renewable energy dominated grids to stationary hydrometeorological uncertainty software and data availability all code and data required to run the capow model as well as some documentation of the model is available at https github com romulus97 capow py36 under the mit free software license funding sources this research was supported by the national science foundation infews programs awards 1639268 t2 and 1700082 t1 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104667 
26054,variability and extremes in streamflow wind speeds temperatures and solar irradiance influence supply and demand for electricity however previous research falls short in addressing the risks that joint uncertainties in these processes pose in power systems and wholesale electricity markets limiting challenges have included the large areal extents of power systems high temporal resolutions hourly or sub hourly and the data volumes and computational intensities required this paper introduces an open source modeling framework for evaluating risks from correlated hydrometeorological processes in electricity markets at decision relevant scales the framework is able to reproduce historical price dynamics in high profile systems while also offering unique capabilities for stochastic simulation synthetic generation of weather and hydrologic variables is coupled with simulation models of relevant infrastructure dams power plants our model will allow the role of hydrometeorological uncertainty including compound extreme events on electricity market outcomes to be explored using publicly available models keywords stochastic hydrology weather electricity markets prices 1 introduction in recent years interest has grown in exploring the effects of hydrometeorological variability and especially extreme events on the operations of bulk power systems large interconnected systems of generation transmission and load demand collins et al 2018 forster and lilliestam 2011 franco and sanstad 2008 kern and characklis 2017 staffell and pfenninger 2018 tarroja et al 2016 turner et al 2019 van vliet et al 2016 2012 voisin et al 2018 both droughts and floods compromise the operations of hydroelectric dams gleick 2017 su et al 2017 tarroja et al 2016 while droughts in particular can also impact thermal power plants that are dependent on cooling water van vliet et al 2016 2012 air temperatures influence a range of system components most notably electricity demand for heating and cooling franco and sanstad 2008 in addition as variable energy resources like wind and solar expand their share of the power mix the grid is becoming more sensitive to fluctuations in wind speeds and solar irradiance collins et al 2018 staffell and pfenninger 2018 by influencing supply and demand for electricity hydrometeorological processes have direct impacts on pollution e g increased greenhouse gas emissions collins et al 2018 hardin et al 2017 tarroja et al 2016 wholesale electricity prices boogert and dupont 2005 collins et al 2018 seel et al 2018 and the financial standing of suppliers of electricity e g retail utilities renewable energy producers and consumers bain and acker 2018 boogert and dupont 2005 foster et al 2015 kern and characklis 2017 kern et al 2015 however with few exceptions turner et al 2019 previous investigations fall short in assessing the holistic influence of hydrometeorological variability on bulk power systems past research efforts assess operational and financial risks from exposure to variability in a more limited set of hydrometeorological processes collins et al 2018 kern et al 2015 e g streamflow and temperatures or wind speeds and solar irradiance do not consider these effects within the context of large interconnected power systems kern and characklis 2017 and or do not assess impacts probabilistically hardin et al 2017 these shortcomings may be partly attributable to the challenges of modeling bulk electric power systems at sufficient scale and resolution to simulate system operations in a realistic way and over sufficient time horizons to explore joint uncertainty in multiple correlated input variables interconnected power systems span areas so large that system operators often have some ability to deal with spatially heterogeneous stressors for example a localized power supply shortfall caused by drought in one area might be managed by importing power from other areas where water and thus electricity from hydropower production and water cooled generators is more abundant from a modeling perspective this necessitates adopting system topologies that extend beyond a single watershed state and region hydrometeorological uncertainty and power system risks can also manifest on different time scales extreme meteorological and hydrological conditions can have durations on the order of days floods najibi and devineni 2017 heat waves weeks to months wind droughts and years hydrological droughts andreadis et al 2005 whereas power system modeling requires an hourly or sub hourly time step pandzžić et al 2014 although stochastic modeling approaches can be used to create large synthetic records of hydrometeorological processes in order to explore risks from extreme events brown et al 2015 reed et al 2013 this poses a direct challenge to the use of computationally expensive integer programming within power system models pandzžić et al 2014 making large ensemble monte carlo simulations less tractable adding to these challenges is the potential presence of significant spatial and temporal covariance among key hydrometeorological processes jimenez et al 2011 woodhouse et al 2016 if significant correlations exist an increased number of model runs may be required to characterize the probability of coincident extremes e g widespread simultaneous hydrological drought a wind drought and a heat wave that may be of particular concern to power system operators mazdiyasni and aghakouchak 2015 turner et al 2019 the modeling scales resolutions and ensemble sizes required in exploring the risks to bulk electric systems from hydrometeorological variability present a challenge and few if any models capable of performing this type of analysis are publically available given recent increased interest among the research community in modeling interconnected systems e g food energy water logan 2015 a generalizable and open source modeling framework for simulating the influence of correlated hydrometeorological processes on power system dynamics at decision relevant scales would be a valuable addition the goal of this paper is to present such a framework the newly developed california and west coast power capow systems model capow was designed by the authors to explore a high profile test bed the west coast of the conterminous united states u s the bulk electric power systems covering most of the states of california oregon and washington are included as well as the two major wholesale electricity markets active across these states current gaps in coverage are the pacificorp west sacramento municipal utility district los angeles department of water and power balancing authorities capow is comprehensive in its treatment of stochastic weather and streamflow simulation of relevant infrastructure reservoir networks power systems and evaluation of outcomes system costs prices etc while focused on the u s west coast the steps required in building and executing the capow model as well as much of the code are fairly generalizable and can be transferred to other systems and interconnections of interest chowdhury et al 2019 most grid specific information used in the model is publically available anywhere in the u s generator size location fuel type prime mover type average heat rate etc hydrometeorological data used to simulate electricity demand wind solar and hydropower production are also available throughout the u s as well as hourly records of renewable energy production in each balancing authority through the eia analogous transmission grid information bi directional capacities is publically available for all wecc areas and for many if not all sub regions in the eastern interconnection note that to transfer the model to other regions additional capabilities that are not currently in capow may be required e g representing impacts of extreme cold air temperatures henry and pratson 2016 and a lack of cooling water availability due to low streamflow and temperatures miara et al 2017 van vliet et al 2016 2012 on thermal power plant functionality the model is python based all code and data required to run the capow model as well as some documentation of the model is available at https github com romulus97 capow py36 under the mit free software license 2 methods our description of methods parallels the capow model s work flow fig 1 beginning with a discussion of surface water and electric power system topologies including key physical assets e g power plants dams reservoirs and their connections i e water routing between reservoirs high voltage transmission pathways this is followed by a description of capow s unit commitment and economic dispatch uc ed model which is used to simulate actual power system operations the methods section ends with a description of our approach for stochastically generating model inputs from historical weather and streamflow data 2 1 system topology 2 1 1 electric power in order to model the west coast grid the case study explored here we first adopt a 21 zone topology of the western electricity coordinating council wecc a regulatory body charged with reducing risks to the western grid by enforcing standards and assessing reliability fig 2 this topology which has been used in the past by wecc and other researchers to assist in long term planning exercises ho et al 2016 mkarov et al 2010 groups balancing authorities utility footprints into multiple zones that are connected via aggregated transmission pathways throughout the region each zone to zone transmission pathway is associated with bi directional capacities i e maximum limits on zone to zone transfers of electricity estimated from publically available data western electricity coordinating council 2016 each zone in the network consists of 1 the load electricity demands of its member balancing authorities which fluctuate on hourly daily seasonal and annual time scales and 2 a portfolio of co located generation resources with which to meet those demands comprehensive databases of generators located in each node of the 21 zone wecc topology are publically available from multiple sources us environmental protection agency 2018 western electricity coordinating council system adequacy planning department 2015 these also contain information on relevant operating characteristics for each generator e g fuel type capacity average heat rate that are used to formulate the uc ed simulation model there are two major trading hubs for wholesale electricity on the u s west coast 1 the mid columbia mid c market that serves as a hub for much of the pacific northwest region and 2 the california independent system operator caiso a competitive wholesale market that manages approximately 80 of california s electricity flow the 21 zone wecc topology shown in fig 2 includes five nodes red numbered that directly correspond to these markets node 1 pacific northwest corresponds to the mid c market and nodes 2 5 correspond to the caiso market nodes 2 5 also represent the service areas of three major utilities pacific gas and electric pg e southern california edison sce and san diego gas and electric sdg e currently only these five zones and power flows among them are modeled mechanistically using a uc ed model no uc ed models exist outside these five zones neighboring zones are considered only in terms of their exchanges of electricity with the core uc ed zones and these exchanges are modeled statistically see supplemental material 2 1 2 dams and reservoirs recent analyses of the impacts of drought on power generation in the western u s harto et al 2011 suggest that cooling water issues from low streamflow and high water temperatures pose a minor threat to thermal power plants in the region rather the primary mechanism through which hydrologic extremes can impact power system operations is through variability in hydropower generation within the wecc topology shown in fig 2 hydropower capacity makes up 58 of installed generating capacity in zone 1 pacific northwest 18 of generating capacity in zone 2 pg e valley and 4 of capacity in zone 3 sce us environmental protection agency 2018 fig s2 in the supplemental material section maps major 5 mw hydroelectric dams that participate in balancing authorities located within the five numbered zones that make up the uc ed model these dams primarily fall within the columbia river basin which spans several northwestern u s states and canada as well as the sacramento river san joaquin river and tulare lake basins in california publically available hydrologic mass balance models exist for 85 of the hydropower capacity in the pacific northwest versions of hyssr developed by the u s army corps of engineers to simulate the federal columbia river power system and a ressim model that simulates the operations of federal dams in the willamette river basin models exist for only 12 of the hydropower capacity in california the orca model herman and cohen 2019 which simulates the operations of major storage flood control dams in california much of the state s hydropower capacity is privately owned and located in high altitude areas of the sierra nevada mountains little information about the operation of these dams is publically available so hydropower production at these projects is simulated via an alternative approach in which hydropower production at upstream dams is predicted using observed streamflow downstream first for major high altitude hydroelectric dam in the sierra nevada mountains a corresponding downstream storage reservoir or stream gauge on the same river is identified in order to predict upstream hydropower generation at a given dam using observed streamflow downstream the calendar year is broken into four seasons winter spring summer and fall each season is assumed to follow a different set of operating rules that translate observed downstream flows into estimates of upstream hydropower production rules are fitted using the differential evolution algorithm in the scipy library of python based on root mean squared error rmse between observed and simulated hydropower production for each upstream dam about 15 of hydropower capacity in the pacific northwest and 20 of hydropower capacity in california are within the five core wecc zones that make up the uc ed model but fall outside the four river basins mentioned above and are not associated with publically available models these projects are modeled by scaling hydropower generation from nearby dams a more detailed description of how hydropower production is simulated on a daily basis can be found in the supplemental material 2 2 unit commitment and economic dispatch model the power system and reservoir network topologies described above form the basis of a unit commitment economic dispatch uc ed model that we use to simulate the operation of the five numbered wecc zones in fig 2 which include the mid c and caiso markets simulating the uc ed model for a single year at an hourly time step takes approximately 6 h using the cplex solver on a 16 core machine with 2 5 ghz processors using a linux operating system what follows is a general overview of the model s structure and functionality a mathematical formulation of the uc ed model can be found in the supplemental material we coded the uc ed model in python using the pyomo mathematical optimization package structuring it as an iterative mixed integer linear program over a user defined operating horizon e g 48 h deterministic optimization is used to minimize the cost of meeting demand for electricity and operating reserves including unit start costs no load costs fuel costs and penalties associated with transferring electricity between zones subject to constraints on individual generators and transmission paths costs are minimized by strategically dispatching scheduling generation from flexible generation resources natural gas power plants hydroelectric dams and system imports on an hourly basis variable renewable energy wind and solar are not dispatchable they can be consumed only when available as such they are typically treated as electricity demand reduction within a zone but can be also curtailed during periods of oversupply a single iteration of the uc ed model yields system costs and the least cost generating schedule over the operating horizon e g hours 1 48 however only the first 24 h of the solution is stored the remaining solution hours 25 48 is discarded and the whole process shifts one day into the future the next iteration of the model identifies a solution for the hours 25 48 while again looking 48 h into the future i e at hours 25 72 this ensures that the model does not have perfect foresight over unreasonably long time horizons when making decisions with path dependency e g turning on baseload power plants with high minimum up times simulation of the uc ed model creates hourly time series outputs that track provision of electricity and operating reserves by each generator the flow of electricity among zones plant specific and system wide emissions of co2 total operating costs and wholesale electricity prices co2 emissions from each power plant are calculated using historical epa egrid data that are used to estimate the kg co2 per mwh emissions for each plant note that total operating costs essentially refers to the value of the objective function in each hour the cumulative start no load and fuel costs across every power plant in every hour on the other hand wholesale electricity prices mwh are dynamic measures of the marginal value of electricity in each market i e how much generators would be paid to sell their electricity in each hour within the optimization wholesale prices are estimated for each zone as the shadow cost of an energy balance constraint at each zone i e the change in objective function value associated with a 1 mwh increase in demand at each zone calculating the shadow costs requires the uc ed model to first be solved in mixed integer form and then resolved as a linear program keeping all binary variables fixed from the integer solution in order to access dual values for relevant constraints in pyomo this yields a separate time series of wholesale electricity prices for each of the five wecc zones represented in the core uc ed model prices in the mid c market are assumed to be equivalent to prices for the pacific northwest zone to represent the caiso market prices for the four relevant zones in california pg e valley pg e bay sce and sdg e are weighted to determine an overall price for the market with the weights fitted via regression r2 0 75 p 1e 3 on observed values over the period 2012 2016 2 3 stochastic inputs the primary stochastic inputs to the uc ed model are electricity demand hourly wind and solar power production hourly and available hydropower production daily for each numbered zone in fig 2 several hydrometeorological processes air temperatures wind speeds solar irradiance and streamflow in turn drive these power system inputs in the following section we describe our approach for generating synthetic hydrometeorological time series 2 3 1 hydrometeorological variables 2 3 1 1 air temperatures wind speeds and solar irradiance we collect observed air temperatures wind speeds and solar irradiance data within major cities where electricity demand is highest and in areas known to have large amounts of installed wind and solar power capacity records of daily average temperature and wind speed over the period 1998 2017 come from noaa s global historical climatological network ghcn for seventeen meteorological stations distributed throughout the western u s table 1 global horizontal irradiance data come from the national renewable energy laboratory s national solar radiation database nsrdb sengupta et al 2018 both clear sky and observed irradiance data are acquired at a 30 min resolution and then aggregated to daily sums each weather station provides the data necessary to generate 365 day profiles of average temperature and wind speed for their respective locations we use solar irradiance data to created 365 day profiles of average clear sky cloudless conditions fig 3 1 t p n 1 y y 1 y t n y 2 w p n 1 y y 1 y w s n y 3 s p n 1 y y 1 y s n y where t p n average temperature on calendar day n across y years c t n y observed temperature on calendar day n in year y c w p n average wind speed on day n across y years m s w s n y observed wind speed on day n in year y m s s p n average clear sky irradiance on day n across y years w m2 s n y observed clear sky irradiance on day n in year y w m2 synthetic values of air temperatures wind speeds and solar irradiance are then generated by combining these average profiles e g blue series in panel a of fig 3 with stochastic representation of the autocorrelated residuals that deviate from these repeating signals e g the gray series in panel a of fig 3 average temperature and wind profiles are subtracted from observed temperature and wind speed values this yields a daily record of zero mean residuals i e deviations from average temperature and wind speed for each calendar day over the period 1998 2017 observed irradiance is subtracted from average clear sky irradiance yielding a daily record of losses due to cloud effects 4 r t d t d t p n 5 r w d w s d t w n 6 i l d s p n i d where r t d residual temperature on day d c r w d residual wind speed on day d m s i l d irradiance losses on day d w m2 residual temperatures and wind speeds as well as irradiance losses are then mean shifted to eliminate negative values and log transformed to approximate a gaussian distribution the residuals losses for each calendar day of the year are then divided by their respective standard deviations in order to control for seasonal heteroscedasticity 7 w r t d r t d ˆ σ t n 8 w r w d r w d ˆ σ w n 9 w i l d i l d ˆ σ i l n where w r t d whitened residual temperature on day d w r w d whitened residual wind speed on day d w i l d whitened irradiance losses on day d r t d ˆ mean shifted log transformed residual temperature on day d c r w d ˆ mean shifted log transformed residual wind speed on day d m s i l d ˆ mean shifted log transformed irradiance losses on day d w m2 σ t n standard deviation of transformed temperature residuals on calendar day n σ w n standard deviation of transformed wind speed residuals on calendar day n σ i l n standard deviation of transformed irradiance losses on calendar day n we then model the resultant whitened residuals and irradiance losses using a vector autoregressive var model in order to capture observed covariance across variables var models describe the behavior of a set of k variables over a given time period as a linear function of their past values and random samples from a multivariate normal distribution simulated values of each variable are stored in a k 1 vector y t which has as its i t h element y i t the value of the i t h variable at time t the lag of the model i e the number of previous time steps that are accounted for when estimating values in y t is denoted by the parameter p 10 y t c a 1 y t 1 a 2 y t 2 a p y t p ε t where c k x 1 vector of constants a i k x k matrix of coefficients ε t k x 1 vector of error terms t time period p model lag simulation of y t proceeds through random sampling of noise ε t from a multivariate normal distribution with a covariance matrix estimated from whitened residuals and irradiance losses for the period 1998 2017 the number of lags considered is determined via the akaike information criteria a fitted var model is used to simulate daily whitened temperature and wind speed residuals and irradiance losses for each ghcn and nsrdb site considered for as many years as desired simulated values are then un whitened by reversing equations 7 9 thus restoring heteroscedasticity and non normality they are then added back to the 365 day profiles reversing equations 4 6 yielding synthetic daily records of temperature and wind speeds 2 3 1 2 streamflow streamflow patterns on the west coast of the u s are driven by runoff from precipitation as rain and largely the melting of snow accumulated during the winter both total annual streamflow and the within year distribution of streamflow experienced in this region are known to be influenced by temperatures null et al 2010 at the same time there are significant correlations among the 85 separate spatially distributed streamflow gauges that drive capow s simulation of dam operations and hydropower production we make use of a gaussian copula to preserve the relationship between total annual streamflow and temperatures in stochastically generated samples first observed daily average temperatures 1953 2008 at the seventeen meteorological stations are converted to heating and cooling degree days which measures deviations from 18 33 degrees c 65 degrees f 11 h d d d s max 18 33 t d s 0 12 c d d d s max t d s 18 33 0 where h d d d s heating degree days on day d at station s c d d d s cooling degree days on day d at station s t d s average near surface air temperature on day d c at station s total annual hdds and cdds are calculated providing coarse measures of the hotness of a given year s summer and the coldness of a given year s winter total annual hdds and cdds and total annual streamflow are then transformed into quantile space by calculating the empirical cumulative probability distribution for each variable 13 p p q q where q total annual streamflow or degree days at a given site empirical probabilities are transformed again into a uniform distribution ranging from 1 to 1 as follows ensuring a mean of 0 across every variable 14 y 2 p 0 5 the covariance matrix c across all the variables at every site is estimated and then synthetic records of total annual streamflow and total annual hdds and cdds are generated by taking random samples from a multivariate normal distribution with mean 0 and covariance matrix c then back transforming reversing equations 13 and 14 the next step is to match total annual streamflow and total annual hdds and cdds simulated via the copula method with the synthetic daily temperatures generated in the previous section using a vector autoregressive var approach synthetic daily temperatures simulated using the var approach are converted to total annual hdds and cdds for each year of synthetic data desired we select a single year of total annual hdds and cdds generated using the var approach and then calculate the weighted average across every ghcn station weights are determined by the fraction of average annual flow across the 85 stream gauges that is contained within each ghcn station s surrounding area 15 w t s g 1 g a v f g a v t where w t s weight assigned to meteorological station s a v f g average annual flow at gauge site g closest to station s a v t average annual flow across all 85 stream gauges the weighted total annual hdds and cdds from the var model are compared alongside pairs of weighted total annual hdds and cdds generated using the copula method the smallest mean squared error difference is identified then the total annual streamflow values generated via the copula method are paired with the corresponding daily temperatures and also wind speeds and solar irradiance generated via var disaggregating total annual streamflow values down to a daily time step must be done in a manner that considers the potential influence of temperatures on the timing of streamflow throughout the year for example fig 4 shows the relationship between winter and spring temperatures and the timing of streamflow at two major reservoirs in california the top panel a shows 19 years 1997 2015 of weighted average temperatures across the ghcn stations calculated using weights from equation 15 lines are colored according to the mean temperature experienced over the first 24 weeks of the year the dark red line indicates the year with the hottest temperatures over this period 2015 and the dark blue line indicates the year with the coolest temperatures 2010 in panels b and c those same line colors are then used to plot contemporaneous full natural unregulated flows at folsom dam panel b and oroville dam panel c in california two large storage dams for which there are long historical flow records flows are shown in terms of standardized fractions that are created by dividing by total annual flows at each site at the top of panels b and c swarm plots identify the week of maximum streamflow for both dams years with higher average winter and spring temperatures red hued circles tend to be associated with earlier peak streamflow indicating earlier snowmelt and or major precipitation events in order to capture these dependencies between the timing of streamflow and temperatures we follow a nearest neighbor clustering approach similar to nowak et al 2010 the weights generated in equation 15 are used to create composite time series of temperatures across the 17 ghcn stations for both historical and simulated temperature data for each simulated year the historical record is searched for a past year that exhibited the most similar winter spring temperature profile in terms of mean squared error the identified historical year is then selected as the basis for determining daily flow fractions at each streamflow gauge site for the historical year selected daily flow fractions are calculated as follows 16 f f d g d f d g a f g where f f d s flow fraction for day d at streamflow gauge site g d f d s observed flow on day d at streamflow gauge site g a f g total annual flow observed at gauge site g flow fractions for each gauge site are then multiplied by simulated total annual flows to yield a synthetic record of daily flows across the study area 2 3 2 power system inputs the stochastic scenario generation framework permits the exploration of large ensembles of time series for temperatures wind speeds solar irradiance and streamflow these data are then converted to associated power system inputs for the uc ed model time series for each zone of hourly electricity demand wind and solar availability daily hydropower production and imports of electricity from other areas in the western u s table 2 provides an overview of the different approaches taken to translate raw hydrometeorological variables into power system inputs as well as their accuracies multi variate regression is used to simulate daily electricity demand solar and wind power production and system imports power flows along wecc paths listed in table 2 daily values are disaggregated down to an hourly time step by sampling from historical profiles daily values of available hydropower production are created by passing synthetic streamflow records through mass balance hydrologic models of dams in the columbia river basin and major storage reservoirs in california as well as through a machine learning representation of high altitude hydropower production in california detailed descriptions of all models used to translate raw hydrometeorological variables into power system inputs can be found in the supplemental material 3 results discussion 3 1 validation of uc ed formulation this paper proceeds with a validation of the uc ed model s ability to reproduce observed power system dynamics in particular wholesale electricity prices wholesale prices which are driven by changes in supply and demand can be viewed as aggregate measures of system performance high prices can indicate scarcity and low prices point to abundance we focus on an extended period of drought that occurred in california over the years 2012 2016 during this period in state hydropower generation decreased by an average of 40 gleick 2017 forcing the state to rely significantly more on electricity from natural gas power plants there has been considerable interest in exploring the impacts of this recent drought on pollutant emissions hardin et al 2017 as well as system costs and prices for retail electricity consumers gleick 2017 particularly when determining the latter an understanding of impacts on wholesale electricity prices is necessary retail distribution companies in california pg e sce and sdge all purchase electricity from the caiso market if the capow model is able simulate observed wholesale electricity prices over 2012 2016 with accuracy then the model could also be used to conduct controlled experiments designed to isolate the role of drought and or other hydrometorological extremes on wholesale prices revenues costs for utilities and ultimately retail prices for consumers natural gas price data used to validate the model i e compare historical caiso prices across the years 2012 2016 were obtained from eia s natural gas hub dataset although these data do not represent the exact price paid by power plants they do represent dynamic prices at major gas trading hubs these day to day fluctuations in gas prices are extremely important to capture eia s data on the delivered price of natural gas for power plants is typically listed on a monthly annual time step which would not allow us to capture more short term severe price spikes fig 5 compares observed daily average electricity prices in the caiso market alongside prices simulated by the uc ed model showing strong agreement r2 0 75 for the purposes of validating the uc ed model we used historical records of temperatures wind speeds solar irradiance and streamflow at the sites listed in table 2 thus discrepancies between observed and simulated prices are entirely due to the uc ed formulation itself and or discrepancies in fuel prices experienced in general the model accurately captures variation in electricity prices on daily time scales and above although model outputs include hourly prices hourly price dynamics e g peak and off peak patterns are not as well represented this is expected for a model reliant on a somewhat abstracted representation of the transmission network 3 2 validation of stochastic inputs the uc ed model s ability to capture more than 70 of daily variability in caiso electricity prices suggests that coupling it with stochastic simulations of weather and hydrology would enable probabilistic assessment of a broad set of hydrometeorological risks in wholesale electricity markets before using capow in this manner however the model s underlying stochastic engine i e the suite of approaches used to simulate weather and hydrological variables and relevant power system inputs must be validated 3 2 1 hydrometeorological variables given the large geographical extent considered as well as the highly interconnected nature of the u s west coast grid it is important that stochastically generated meteorological and hydrological inputs exhibit the same statistical dependencies as the historical record fig 6 shows correlation matrices calculated using historical data from the 17 ghcn stations and 7 nsrdb sites top left as well as historical data from the 85 stream gauges bottom left these are compared alongside correlation matrices calculated using 1000 years of corresponding stochastic data generated using the approaches described in section 2 3 lighter areas show positive correlation two locations variables that are more likely to both experience high low values simultaneously dark areas show negative correlations in general results show a high degree of fidelity between historical and simulated covariance across variables and space for example historical and simulated streamflow correlation matrices both show the same pockets of light values which are associated with highly correlated stream gauges located within the same watershed overall these results suggest that capow when run in stochastic mode is able to capture spatial heterogeneities in weather and hydrological processes e g the likelihood of experiencing high low temperatures wind speeds irradiance streamflow simultaneously at sites distributed across the entire region equally important the underlying stochastic engine of capow is able to reproduce observed statistical moments e g mean standard deviation in hydrometeorological conditions fig 7 shows close agreement between historical and simulated temperatures and wind speeds across the 17 ghcn stations in terms of percentile 1st 50th and 99th while also demonstrating the stochastic model s ability to occasionally generate more extreme min max values than the historical record in fig 8 a similar comparison is shown using streamflow data each panel includes historical blue red circles and simulated black line values for each of the 85 stream gauges considered red circles represent gauges in california mostly the sierra nevada mountains and blue circles represent gauges in the pacific northwest mostly the columbia river basin each panel represents a different percentile 1st 50th 99th as well as min max values note that in some cases negative values are shown this is an artifact of our use of bpa s modified flow dataset which consists of historical flows at gauge sites in the columbia river basin with modern human withdrawals applied at certain gauge sites this results in negative flow values water is subtracted from reservoir storage in general results suggest close agreement between the distributions of historical and stochastically generated streamflow values while also demonstrating the stochastic model s ability to occasionally generate more extreme min max values than the historical record 3 2 2 power system inputs a suite of models is used to translate raw temperatures wind speeds solar irradiance and streamflows into power system inputs including multivariate regression wind and solar power electricity demand system imports exports and hydrologic mass balance operational models of reservoirs hydropower coupled with our stochastic weather and streamflow generation techniques these models yield realistic time series of power system inputs that mimic historical data on seasonal daily and hour time scales table 2 for example fig 9 panel a shows historical blue and simulated red seasonality in wind power capacity factor a unitless number between 0 and 1 corresponding to the average hourly output of a wind farm as a fraction of installed capacity aggregated for the entire caiso system the simulated data is produced by coupling stochastically generated wind speeds at ghcn stations with a multivariate regression model of system wide wind power availability based on wind speeds table 2 and then adding in a record of synthetic residuals model errors results indicate alignment with historical data on a monthly basis with highest capacity factors occurring in the summer and lowest during winter this approach is also able to reproduce hourly and daily time series characteristics for wind power production fig 9 panel b shows close agreement between historical and simulated daily autocorrelation in wind power production suggesting the model does an adequate job preserving any statistically significant memory in daily wind power production fig 9 panel c shows historical and simulated seasonality in solar power capacity for the caiso system the simulated data is produced by coupling stochastically generated solar irradiance minus cloud effects at seven nsrdb sites with a multivariate regression model of system wide solar power availability based on site specific irradiance results indicate alignment with historical data on a monthly basis again with highest capacity factors occurring in the summer months and lowest during winter this approach is also able to reproduce hourly and daily time series characteristics for solar power production fig 9 panel d compares hourly capacity factors produced using historical irradiance data for a week in summer 2006 alongside stochastically generated solar power data for the same calendar week with differences being due to simulated cloud effects consideration was also given to volume of simulations required to achieve statistical convergence between historical and simulated power system inputs a primary motivating factor in developing the underlying framework of the capow model is to explore the impacts of hydrometeorological uncertainty especially extreme events on power systems and electricity markets to be useful in this regard the stochastic engine of capow as well as the uc ed model must be run over a sufficiently large number of years to produce the kind of low probability high magnitude tail events that are concerning to grid participants e g episodes of extreme shortfalls or overabundance in supply considering the high computational requirements of the uc ed model which relies on mixed integer programming a relevant question is how many years are enough fig 10 explores this question for the capow model each panel shows data for a different input in the caiso system hydropower production wind power production load electricity demand and net load defined here as load minus total renewable energy wind solar and hydropower and resources considered to be must run like nuclear and geothermal net demand is an important metric because it represents the amount of electricity that would need to be met by dispatchable generators coal and natural gas the colored lines measure the absolute difference between the historical record and synthetically generated values as a function of simulation volume for example in the bottom left panel load the red line tracks the difference between the historical record and stochastically simulated values in terms of the 99th percentile of hourly electricity demand at low simulation volumes this difference starts at around 280 mwh average hourly demand in the caiso market is more than 25 000 mwh indicating an error of less than 1 as the number of simulated years increases the absolute difference first increases but then stabilizes appearing to asymptotically approach a value close to 220 mwh stabilization occurs when increasing the number of simulation years has a negligible impact on the difference between historical and simulated values fig 10 shows that simulations from capow s stochastic engine tend to converge statistically after about 1000 years suggesting this would be a reasonable lower bound on simulation volume to run through the uc ed model overall our results suggest that capow s stochastic engine is able to reproduce historical statistical characteristics across multiple hydrometeorological variables and power system inputs needing approximately 1000 simulation years to achieve stable distributions a final validation step is to evaluate whether the stochastic engine creates an expanded distribution of system states in other words does simulation over 1000 years cause extreme events outside the historical record to emerge from joint uncertainties in individual system processes without directly running the uc ed model a preliminary analysis of this kind can be conducted using net load as a metric of interest since this typically correlates strongly with electricity prices and would be a key indicator of the potential for system shortfalls extremely high net demand and oversupply extremely low net load fig 11 evaluates net load in the caiso system under different scenarios the shaded areas show the distribution of net load over the period 1953 2008 simulated using historical hydrometeorological data colors correspond to different percentiles of net load ranging from 1st to 99th as well as the min max values for this time period net load simulated using hydrometeorological data from 1953 to 2008 is then compared alongside actual historical net load recorded for a recent year 2016 which is represented with a black line for the most part actual net load for 2016 is enveloped by the distribution of values simulated using 1953 2008 hydrometeorological data fig 11 also shows minimum and maximum values acquired from 1000 years of synthetic runs produced by the stochastic engine of capow blue dotted lines min max values produced by the stochastic engine suggest that the capow model by exploring joint uncertainties in hydrometeorolgical variables at sufficiently high simulation values is able to access rare extreme events outside the historical record the additional information provided by stochastic modeling appears to be especially valuable during late summer when net load is the highest and the stochastic model produces maximum values that are considerably larger than the highest values simulated using weather and hydrology from 1953 to 2008 these more extreme synthetic values are likely to include rare but plausible compound events in which combinations of high electricity demand and low renewable energy availability create extremely high net load with associated risks for reliability and high market prices 4 conclusions despite growing interest in the potential vulnerabilities of bulk electric power systems to hydrometeorological variability and extremes there are few if any open source modelling packages capable of exploring this issue in a comprehensive manner this paper presents a new model capow which we specifically designed to explore the influence of joint uncertainties in temperatures wind speeds solar irradiance and streamflow on bulk power systems and wholesale electricity markets capow couples synthetic generation of hydrometeorological variables with simulation models of relevant infrastructure dams power plants allowing for in depth exploration of the role of weather and hydrology on system outcomes the model is free and downloadable via public online repositories the capow model uses a topological representation of the conterminous u s west coast power system to form a unit commitment and economic dispatch uc ed model that simulates system operations and tracks performance system costs prices etc on an hourly basis when using historical weather and streamflow data as inputs to the model it is able to capture 75 of the variability in daily electricity prices in the caiso market although designed specifically with the u s west coast in mind the steps taken to construct capow as well as much of the code base can be extended to other systems of interest however some critical functionalities may need to be added for example capow does not currently represent thermal power plant curtailments due to inadequate cooling water supplies caused by low streamflows and high temperatures when run in stochastic mode capow couples the uc ed model with a stochastic engine that creates synthetic records of temperatures wind speeds solar irradiance and streamflow for a group of 17 meteorological stations 7 solar resource assessment sites and 85 stream gauges distributed throughout the west coast stochastically generated hydrometeorological variables are used to predict electricity demand via temperatures wind speeds wind power production via wind speeds solar power production via irradiance and hydropower availability via streamflows which then drive the uc ed model the statistical properties moments cross correlations time series characteristics of synthetic data produced mirror those of the historical record while also allowing for the generation of more extreme but plausible events exploring the joint uncertainty in relevant hydrometeorological variables is computationally tractable with the statistics of stochastic simulations converging with the historical record after approximately 1000 simulation years overall our framework which is also easily transferrable across systems and geographic areas simulates the operations of bulk electric power systems and wholesale markets at sufficient scales and resolutions to simulate system operations in a realistic way and over sufficient time horizons to explore joint uncertainty across multiple correlated variables of interest as such it should prove to be a valuable future resource for direct grid participants as well as the research community particularly in answering questions related to the vulnerability of the grid to future changes in hydroclimate as well as the sensitivity of variable renewable energy dominated grids to stationary hydrometeorological uncertainty software and data availability all code and data required to run the capow model as well as some documentation of the model is available at https github com romulus97 capow py36 under the mit free software license funding sources this research was supported by the national science foundation infews programs awards 1639268 t2 and 1700082 t1 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104667 
