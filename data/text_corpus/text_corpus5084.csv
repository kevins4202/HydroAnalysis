index,text
25420,green infrastructure gi measures are increasingly used for climate adaptation in urban areas but it remains a challenge to evaluate their effectiveness and strategically allocate investment planning gi is subject to deep uncertainties and requires navigating tradeoffs between multiple objectives many objective robust decision making mordm can be useful in addressing these modeling challenges thus far mordm has been used sparsely for gi planning to help mainstream mordm applications in gi planning we developed an open source python library rhodium swmm rhodium swmm connects the usepa s stormwater management model swmm to rhodium a python library for mordm rhodium swmm provides a generalizable and flexible interface for taking swmm input files and setting up a multi objective optimization problem with the ability to define a wide range of parameters in the swmm input file as uncertainties or levers this opens opportunities to more conveniently analyze new research questions in multi scale gi placement under deep uncertainty keywords rhodium swmm robust decision making green infrastructure planning uncertainty multi objective optimization data availability data will be made available on request software availability name of software rhodium swmm developer nastaran tebyanian department of architecture the pennsylvania state university university park pa usa with contributions from george rossick zetier inc state college pa usa year first available 2022 hardware required 1 gb ram software required docker availability https github com nastarant rhodium swmm cost free program language python supported systems linux windows mac x86 machines program size 4 3 mb license rhodium swmm can be redistributed and or modified under the terms of the gnu general public license by the free software foundation version 3 or later these files are distributed in the hope that they will be useful but without any warranty without even the implied warranty of merchantability or fitness for a particular purpose see the gnu general public license for more details 1 introduction green infrastructure gi defined as stormwater management practices that mimic natural hydrological processes usepa 2020 has been increasingly used for mitigating the negative impacts of urban development liberalesso et al 2020 and climate change adaptation matthews et al 2015 although gi implementations have been primarily motivated by their stormwater flow reduction and treatment functions gi provides many other ecosystem services such as regulating local climate supporting habitat and enhancing human physical and mental health hansen and pauleit 2014 therefore effective gi planning requires harnessing multiple benefits of gi and ensuring that these benefits are robust to future changes for example climate change significantly affects the frequency intensity and or duration of extreme weather events globally revi et al 2014 willems and olsson 2012 indicated a 10 60 increase in future 2100 rainfall intensity compared to the recent past 1961 1990 at the small urban watershed scale such drastic changes in climate will most likely significantly affect gi efficacy in addition to climate change other factors such as urbanization and associated alteration of the natural hydrological cycle and aging gray stormwater infrastructure will also continue to challenge multi functional gi planning incorporating multi functionality in gi planning while considering uncertainty requires advanced decision making tools although methods for decision making under deep uncertainty such as robust decision making rdm casal campos et al 2015 mei et al 2018 and many objective robust decision making mordm fischbach et al 2017 2020 have been applied to gi planning before there is a critical lack of generalizable open source tools to support multi scale multi objective gi decision making under uncertainty to address this gap we have developed rhodium swmm an open source python library that facilitates integrating uncertainty considerations into multi scale gi decision making this paper introduces the library and showcases an example of the novel capabilities it provides we begin this article with an in depth literature review of the limitations of traditional modeling approaches and the advantages of alternative approaches sub sections 1 1 and 1 2 section 2 then introduces rhodium swmm s components and software architecture as well as the methods for quantifying responses and uncertainties section 3 showcases the tool by demonstrating analyses for an example problem of gi placement lastly we conclude by briefly summarizing the tool s capabilities and future research steps 1 1 limitations of traditional gi modeling approaches with planning objectives such as optimizing stormwater quantity and cost traditional gi modeling approaches typically use gi location type and size as the most important parameters that influence the provision and distribution of gi benefits under current conditions wang et al 2020 zhang and chui 2018 zhang and chui 2018 frame these parameters as allocation components and this kind of decision problem as a spatial allocation problem they compare two methods that provide decision support to these problems i e scenario based models and spatial allocation optimization tools concluding that the latter offers more capacity to showcase tradeoffs between different objectives multi objective gi spatial allocation is a well studied problem researchers have explored many different aspects of modeling such as integrating overland and river processes leng et al 2022 incorporating socioecological indexes and including both green and gray infrastructure in optimization liu et al 2021 and performance of different multi objective optimization algorithms xu et al 2018 recent studies also have begun to support multi functionality and multi scale approaches e g hansen and pauleit 2014 vogel et al 2015 however uncertainty considerations have only been integrated into a limited number of gi spatial allocation studies tebyanian et al 2022 uncertainty considerations can change what we consider optimal and robust gi policies jayasooriya and ng 2014 tebyanian et al 2022 wang et al 2020 uncertainties surrounding gi objectives affect the gi policies performance a fast growing body of research has considered uncertainty focused on water quality quantity and cost objectives tebyanian et al 2022 examples of the uncertainties addressed in gi spatial allocation literature belong to the categories of hydrologic modeling hydraulic modeling water quality modeling gi cost gi adoption implementation gi design stakeholder preferences and data resolution ashley et al 2018 dong et al 2021 eckart et al 2018 gu et al 2018 jayasooriya et al 2018 kazak et al 2018 leng et al 2021 lim and welty 2018 liu et al 2019 piscopo et al 2021 raei et al 2019 xu et al 2019 among these uncertainties rainfall and land use uncertainties have been the most frequently studied if we expand the objectives to include other benefits such as urban heat island uhi reduction and biodiversity the categories of relevant uncertainties could increase considerably hou et al 2013 however ecosystem services beyond hydrological benefits have rarely been integrated into multi objective gi optimization literature tebyanian et al 2022 limited scenario analysis is another major limitation in existing gi spatial allocation studies that consider uncertainty tebyanian et al 2022 a small number of planning alternatives are usually prescribed rather than searched and sampled because of this they do not support decision making under uncertainty and can prove counter productive and sometimes dangerous in a fast changing world lempert 2019 they also do not address questions raised by the deep uncertainties surrounding gi models decision levers i e actions that comprise the alternative strategies decision makers want to explore lempert 2003 and success metrics tebyanian et al 2022 1 2 robust decision making rdm and many objective robust decision making mordm when predictions are unreliable it can be beneficial to adopt alternative modeling approaches that run the traditional analysis backward lempert et al 2013 and explore many plausible futures to identify conditions under which the plans policies perform well or poorly lempert et al 2013 among these approaches is robust decision making rdm rdm is a multi scenario multi objective decision analysis approach that stress tests strategies over myriad plausible paths into the future and then identifies policy relevant scenarios and robust adaptive strategies lempert 2019 rdm embedded analytic tools also promote learning and consensus building among stakeholders lempert 2019 rdm has been applied to various environmental problems such as evaluating water fischbach et al 2015 matrosov et al 2013 and climate policies hall et al 2012 when dealing with multiple and potentially conflicting objectives multi objective optimization approaches to decision making under uncertainty such as many objective robust decision making mordm can enable the discovery of tradeoffs among planning objectives kasprzyk et al 2013 by combining many objective evolutionary optimization moea such as the widely used nsga ii algorithm and rdm mordm tests each solution under the ensemble of future extreme states of the world sow kasprzyk et al 2013 additionally the extensive use of interactive visual analytics in mordm facilitates the management of complex environmental systems kasprzyk et al 2013 the mordm process has four main steps 1 problem formulation 2 alternatives generation 3 uncertainty analysis and 4 scenario discovery and tradeoff analysis mordm has been applied to environmental problems such as water allocation yan et al 2017 ecosystem management singh et al 2015 flood risk management zarekarizi et al 2020 and agriculture decision making gonzález et al 2020 in the context of gi planning the applications of rdm and mordm have been limited especially for the latter fischbach et al 2017 2020 use rdm to examine the vulnerabilities and tradeoffs of gi policies in regional and urban watershed scales in pittsburgh pennsylvania usa the studies assess gi policies for several water metrics as well as gi co benefits and cost the levers are regions of gi policies with different gi investment intensities and types casal campos et al 2015 apply rdm to assess acceptable performances of the combination of gray and green strategies in several metrics such as flooding cost and health across different future scenarios the study focuses on regret calculation and does not form a multi objective optimization problem finally piscopo et al 2021 apply mordm to optimize gi placement for water quality quantity and cost using levers including the amount of impervious area treated with gi gi size and type facilitating the applications of rdm and mordm in gi planning offers great promises in identifying robust stormwater solutions that simultaneously address multiple objectives and deep uncertainties 2 rhodium swmm rhodium swmm is an open source python library for robust green infrastructure planning under deep uncertainty developed in the linux environment it connects two open source tools the stormwater management model swmm by the us environmental protection agency epa rossman 2015 and rhodium a python library for mordm hadjimichael et al 2020 swmm is a dynamic rainfall runoff simulation model used for the design and analysis of urban drainage systems rossman 2015 although many other urban drainage models exist such as sustain shoemaker et al 2009 mike urban mike urban documentation index n d musicx wong et al 2012 the low impact development rapid assessment lidra montalto et al 2007 and city drain achleitner et al 2007 swmm is the most widely applied urban model globally and is also the most frequently used software in studies that provide decision support for sustainable urban drainage systems ferrans et al 2022 niazi et al 2017 swmm is considered the most sophisticated for modeling stormwater quality quantity and gi performance regarding accuracy algorithms and the temporal and spatial scales covered ferrans et al 2022 jayasooriya and ng 2014 niazi et al 2017 swmm can simulate either single event or long term runoff quantity and quality its runoff component operates on sub catchments that receive precipitation and generate runoff and pollutant loads whereas the routing portion transports this runoff through a system of pipes channels storage treatment devices pumps and regulators rossman 2015 gi modeling is supported in swmm under low impact development lid a synonym for gi controls swmm s input file includes all necessary information for the stormwater model including the names and locations of sub catchments junctions outfalls and gi types it also includes simulation specifications such as duration and timesteps and environmental specifications such as rainfall and river stage information it can also include links to external data such as rainfall time series rhodium is an open source python library for rdm mordm and exploratory modeling hadjimichael et al 2020 rhodium helps identify robust strategies for managing complex environmental systems by evaluating tradeoffs among candidate strategies and characterizing their vulnerabilities the core classes in rhodium include 1 model which enables the definition of the simulation model to be analyzed 2 parameter which can be used to define model parameters that can either be constant controlled by a lever or subject to uncertainty 3 response which represents model outputs and can be set to minimize or maximize 4 constraint which is used to set hard constraints that must be satisfied for a candidate solution to be considered feasible 5 lever which defines an adjustable lever that controls a model parameter during optimization hadjimichael et al 2020 a brief guide on how to set up levers uncertainties and constraints has been included in appendix a to combine rhodium and swmm we first adapted the windows based swmm for the linux environment by compiling its source code using the open water analytics modification of the source code s build system mcdonnell et al 2021 we also utilized code from ostrich swmm an open source tool for gi multi objective optimization macro et al 2019 to translate the swmm input file to and from a python dictionary format the primary components of rhodium swmm in the mordm process for gi decision making are shown in fig 1 after defining the problem the user can access all the rhodium mordm analytic capabilities for example the user can run multi objective optimization to find the pareto optimal set which is a set of solutions that cannot be outperformed by another solution across all performance metrics as an example of uncertainty analysis capabilities the user can run robust multi objective optimization to find the pareto satisficing set the pareto satisficing set includes solutions that remain close to the pareto optimal surfaces for different states of the world sow kasprzyk et al 2013 each sow is defined through a latin hypercube sampling lhs across uncertain parameters mckay et al 1979 this sampling method helps ensure that different values of uncertain parameters are represented regardless of their dominance mckay et al 1979 the robustness is measured through a combination of satisficing and regret measures satisficing measures emphasize maximizing the number of plausible futures in which some criteria are met while regret measures emphasize minimizing deviation from performance in the expected future hadka et al 2015 savage 1951 the user can also access the scenario discovery capacities by evaluating a policy in different sows in terms of both performances across metrics and robustness here classification algorithms such as the patient rule induction method prim and classification and regression trees cart are applied to identify the ranges of uncertain factors most likely to cause system failure under specific policies hadka et al 2015 2 1 rhodium swmm software architecture we developed the rhodium swmm library in python 3 for multi thread simulation it can be used with python 3 8 and higher otherwise 3 6 the core classes of rhodium swmm are rhodiumparameter swmminputelement and rhodiumswmmmodel fig 2 shows the inheritance and composition relationships among the main classes the rhodiumswmmmodel class comprises rhodiummodel lidcontrol lidusage and subcatchment classes lidcontrol lidusage subcatchment and rhodiumparameter classes inherit from swmminputelement lidcontrol lidusage and subcatchment classes mirror the corresponding sections of the swmm input file the combination of rhodiumparameter and swmminputelement class definitions creates the possibility of treating any component of the swmm input file as a parameter that rhodium can change the user can provide any swmm input file the lidcontrol including layer information lidusage and subcatchment values will be imported into the specified rhodiumswmmmodel layer class provides the capability to change layer information for each specific lid type for example for bioretention the layer includes surface soil storage and drain subclasses lidcontrol lidusage and subcatchment classes provide the ability to add modify the whole sections of lid controls lid usage and subcatchments in a swmm input file currently we have included bioretention infiltration trench rain barrel and permeable pavement as the supported lid types future work can be done to include other types such as green roofs when all the above components are imported into the specified rhodiumswmmmoldel the user can substitute any value in those swmminputelements with a rhodiumparameter in other words any specification in the swmm input file can be set as an instance of rhodiumparameter class and thus as levers uncertainties or constraints examples of levers include gi type gi design specification in the lid controls section of the swmm input file and gi size and area in the lid usage section of the swmm input file the same parameters can be set as constraints or uncertainties examples of uncertainties and constraints include the rainfall time series in the raingages section and impervious percentage in the subcatchment section of the swmm input file once the rhodiumswmmmodel is created by adjusting the swmm input file and specifying levers and uncertainties responses for gi placement will be calculated and written as a class that inherits from rhodiumswmmresponse and ultimately rhodium response class fig 2 currently there are four responses and their detailed calculations are explained in section 2 2 below 2 2 quantifying gi placement responses and uncertainties rhodium swmm calculates four main categories of responses related to gi planning 1 stormwater runoff metrics e g runoff volume or rate 2 cost 3 co benefit and 4 spatial priorities here cost accounts for both gi installation and lifetime operation and maintenance costs co benefit refers to the ecosystem benefits expressed in monetary value brought by green space creation spatial priorities integrate three metrics related to sub catchment vacancy gi aggregation and existing stormwater problems explained in detail later fig 3 shows the input data needed for each response calculation first the stormwater metric calculation is run by the swmm engine swmm exe using the swmm input file and external data such as rainfall rrf files pyswmm a python interface to swmm mcdonnell et al 2020 is integrated into rhodium swmm to provide the ability to extract and set any node or outfall statistics as a response the node statistics provide stormwater information for each junction or outfall in the swmm model including but not limited to water depth inflow volume flooding volume and duration and surcharge duration examples of outfall statistics are peak flow rate and pollutant loading one possibility is to use maximum hydraulic grade line hgl and flooding duration as metrics to estimate peak flood depth and duration of flooding in nodes of interest the current version of rhodium swmm uses the outfall peak flow volume as the default stormwater metric any specification in the swmm input file can be set as uncertainties that affect the stormwater metric calculation examples of uncertain factors are gi design specifications such as storage and soil specifications rainfall time series and the model boundary condition second cost response parameters include gi number and area planning horizon discount rate installation year installation cost operation and maintenance cost and gi lifetime the uncertain cost parameters defined in the current version of the tool are installation cost operation and maintenance cost and gi lifetime while the flexible design of the tool allows for setting up gi type as a lever the current version uses bioretention as the default gi type and focuses on the location and size of gi measures as levers bioretention refers to a landscape depression consisting of a surface ponding layer vegetation a soil layer a storage layer overflow structures and an optional underdrain system yang and chui 2018 and is often considered the workhorse of gis due to its water quantity and quality treatment functions for characterizing bioretention cost uncertainty we sample uniformly across the ranges of uncertain parameters extracted from the literature see table a1 appendix a the cost response is thus calculated as the sum of discounted costs in installation years first year and at the end of gi lifetime for reinstallation and regular annual discounted operation and maintenance costs for the planning horizon we discount the cost by dividing it by ediscount rate year 1 for the duration of the planning horizon if in the first year or when a reinstalling is due end of the gi lifetime the cost is calculated by y e a r l y c o s t l i d n u m b e r i n s t a l l c o s t l i d n u m b e r l i d a r e a o m cos t p e r a r e a e d i s c o u n t r a t e y e a r 1 otherwise regular year y e a r l y c o s t l i d n u m b e r l i d a r e a o m cos t p e r a r e a e d i s c o u n t r a t e y e a r 1 the total discounted cost for the whole planning horizon is i 1 n p l a n n i n g h o r i z o n y e a r l y d i s c o u n t e d c o s t third two types of co benefits are quantified through a literature review 1 green areas connected to gray infrastructure benefit and 2 park benefit bockarjova et al 2020 we define that the green connected to gray benefit happens at any scale of green infrastructure and the park benefit happens when the aggregated gi area exceeds the average lot size in the study area we chose 4046 86 m2 or 1 acre to represent the average lot size in the current version for the range of economic values of the co benefits see table a2 appendix a co benefit response parameters thus include gi number and area planning horizon discount rate green connected to gray benefit per area park benefit per area and gi efficacy ranges from 0 to 1 1 being the full co benefits realized the defined co benefit uncertain parameters are green connected to gray benefit per area park benefit per area and gi efficacy for characterizing co benefit uncertainty we sample uniformly across the ranges of uncertain parameters listed in table a2 appendix a based on these parameters the co benefit is calculated with the equations below when the gi area is 4046 86 m2 y e a r l y c o b e n e f i t l i d n u m b e r l i d a r e a p a r k b e n e f i t g i e f f i c a c y e d i s c o u n t r a t e y e a r 1 when the gi area is 4046 86 m2 y e a r l y c o b e n e f i t l i d n u m b e r l i d a r e a g r e e n g r a y b e n e f i t g i e f f i c a c y e d i s c o u n t r a t e y e a r 1 the total discounted economic co benefit for the whole planning horizon is i 1 n p l a n n i n g h o r i z o n y e a r l y d i s c o u n t e d c o b e n e f i t lastly we developed three metrics as examples of spatial priority objectives 1 vacant priority 2 aggregation priority and 3 community issue priority we intend to maximize the number of gi measures in 1 sub catchments with a large percentage of vacant land vacant priority 2 sub catchments with not only large vacant areas themselves but also at least one neighboring sub catchment with a high vacancy to provide opportunities for creating neighborhood parks from the aggregation of gi measures aggregation priority and 3 sub catchments suffering from existing stormwater issues as reported by the communities issue priority for aggregation priority specifically we assign an aggregation priority score of 1 3 respectively to 1 sub catchments without high vacancy 2 sub catchments with a high vacancy but no neighboring high vacancy sub catchment and 3 sub catchments with a high vacancy and at least one neighboring high vacancy sub catchment a priority score for each metric can be calculated for each sub catchment based on the csv inputs for each category the total priority score to maximize is calculated through i 1 n n u m b e r o f a l l s u b c a t c h m e n t s w i t h g i l i d a r e a l i d n u m b e r l i d s s u b c a t c h m e n t s p r i o r i t y s c o r e the priority score was then normalized to its original range by dividing the number above by the total lid area for example for vacant priority in a three sub catchment s1 s3 model if the vacant percentages for s1 s2 and s3 are 80 20 and 40 respectively and a solution installed 40 80 and 120 ft2 of lid in s1 through s3 then the vacant priority score for this solution is calculated as 80 40 20 80 40 120 240 total lid area 40 the aggregation and issue priorities can be calculated using the same approach with all the responses defined the goal in the multi objective optimization for gi placement is to optimize for stormwater metrics e g minimize runoff volume or rate minimize cost maximize the co benefit and maximize the spatial priority score 3 example green infrastructure placement problem this section illustrates the software using an example green infrastructure planning problem we used a sample epa stormwater management model site drainage model inp included with the installation of the swmm 5 1 desktop version us epa 2014 see model details in table a3 and fig a1 appendix a it models runoff quantity and quality in a 39 acre residential subdivision the site contains seven sub catchments ranging from 0 80 to 2 75 ha in area 2 0 3 1 in slope and 0 0 95 0 in impervious percentage the sub catchments are connected to a system of swales and culverts that convey runoff to an outfall appendix a we used the xlrm problem formulation from the mordm framework to define our problem lempert et al 2003 x in the xlrm diagram stands for exogenous uncertainties that affect the system l represents policy levers or actions the decision maker can take or the alternative strategies decision makers want to explore r is the relationship between levers and uncertain factors and the outcomes and m are the measures that decision makers use to rank the solutions based on different objectives lempert et al 2003 our specific xlrm problem formulation is shown in table 1 to showcase the uncertainty analysis integration we only considered the uncertainty in the cost calculation parameters of bioretention lifetime installation cost and operation and maintenance cost the lever is the number of bioretention units 40 ft2 3 72 m2 per unit in each sub catchment we set the limit of bioretention units per sub catchment to either the number of units that fully cover the sub catchment s impervious area or 100 units to control for total cost whichever is smaller because only six of the seven sub catchments have impervious surfaces hence bioretention installation the number of levers is six while the rhodium swmm package provides quantification of other metrics such as co benefit for a simpler demonstration of the tool we focused on three objectives 1 minimize peak runoff rate at the outfall 2 minimize discounted life cycle cost and 3 maximize installation in areas with higher vacant percentage we randomly assigned the vacant percentage of sub catchment one through seven as 60 20 100 40 10 80 and 5 respectively to showcase an example problem we created an example module in the rhodium swmm repository with two main files 1 initialize py for problem formulation and initializing the data folder and parallel processing and 2 spatial visualization a folder containing a python script map viz py the initialize py file is where the user can set the specification for rhodium model response uncertainties and levers it also allows the user to collect all necessary data and create a data folder in the testing folder for a specific simulation the user can then use the command line interface to perform rhodium analysis on the specified problem the rhodium analysis py and cli py in the rhodium swmm repository are responsible for the command line interface capabilities several commands have been defined to perform analyses such as multi objective optimization uncertainty analysis scenario discovery and tradeoff analysis see tebyanian 2022a for detailed command line capabilities when the analysis is performed one or multiple outputs are written to the test folder the python script map viz py gets these outputs such as a pareto set from multi objective optimization combines them with the shapefile data of the sub catchments and creates a map based visualization of the results 3 1 examples of analyses the connected rhodium analysis capabilities help address several gi placement questions we demonstrate three examples of these questions 1 what is the optimum number of gi measures in each sub catchment when optimizing for peak runoff rate and cost reduction while maximizing investment in sub catchments with high vacancy 2 what is the tradeoff between cost runoff and vacant spatial priority 3 if we choose a solution from the optimal set and set a criterion for acceptable cost in the future what are the future scenarios to which this solution will be vulnerable the three objective optimization using the nsgaii genetic algorithm resulted in an approximate pareto optimal set we analyzed the approximate pareto set with different numbers of runs to diagnose convergence fig a2 apx a we saw no apparent evidence for lack of convergence after 30 000 number of fitness evaluation nfe or the number of runs fig 4 addresses question one by illustrating the ratio of the bioretention area to the impervious area in each sub catchment resulting from a solution with the lowest peak runoff rate in the pareto set to understand the tradeoffs among the objectives we plotted the pareto set of the three objective optimization figs 5 and 6 for the third tradeoff plot please see figs a3 appendix a fig 5 shows two 2d plots of the 3d pareto front see fig a4 appendix a there are a few very low cost high peak runoff rate solutions with varying vacant percentage scores that bend the 3d pareto front curve these solutions are located on the right side of fig 5 right the same solutions exist in fig 5 left in the bottom right corner but are overlaid with other solutions in a 2d representation of the 3d space the parallel axes plot fig 6 shows all the gi solutions in the pareto set comparing their performances in peak runoff rate cost and vacant score this plot shows that reducing the first increment of the peak runoff rate in cubic feet per second or cfs is extremely inexpensive compared to further runoff reductions more importantly the very high cost solutions minimally enhance peak runoff reduction compared to those about 500 000 cheaper additionally the solutions considered good in terms of peak runoff rate 25 cfs have a 50 75 vacant percentage range this indicates a tradeoff between prioritizing gi in sub catchments with a 75 vacancy and optimizing peak runoff reduction there are also some low cost low runoff options that perform poorly in the vacant priority score metric 25 vacant percentage this analysis highlights the frequent tradeoffs while optimizing for cost hydrological performance and spatial preference for implementation while we showcased the vacant percentage as an example a wide range of other spatial priority metrics can be included to understand the future scenarios to which a particular solution will be vulnerable question three we chose a solution in the pareto set with the least peak runoff rate that costs 2 311 900 8 this solution proposes 100 99 94 100 0 and 100 units of 40 ft2 bioretention cells in sub catchments one through six we set the criterion for acceptable future cost as 2 5 million 8 increase and ran the prim classification algorithm using the uncertain ranges in cost parameters such as gi lifetime range 20 45 years default value 25 years installation cost range 3623 9 4429 2 default value 3623 9 and annual operation and maintenance cost range 2382 89 4765 79 default value 2382 89 the results show that the solution will be vulnerable to scenarios where the lifetime is less than 34 years and the installation cost is more than 4100 see fig a5 appendix a for the prim plots in addition to this low runoff high cost solution we also performed prim on multiple other solutions of differing costs to assess if the values of vulnerable scenarios would change however no meaningful differences emerged from these comparisons likely due to the limitation of using a simple example to showcase the tool despite these limitations this simple example remains valuable for providing an easy to follow showcase of the fundamental concepts and capabilities of the tool the primary contribution of this article is to explain in detail the development of the rhodium swmm software making it challenging to accommodate a more complex example problem involving gi decision making in a real world context future publications will be necessary to further demonstrate the tool s full capabilities using advanced case studies to provide more nuanced and decision relevant insights future test cases can include additional sources of uncertainty such as rainfall and co benefit uncertainties or additional objectives such as other spatial priority metrics and gi co benefits they will also require context specific research such as future climate scenarios and community spatial priority needs to address critical gi placement questions in respective cities beyond the types of gi questions showcased in the examples above the tool facilitates addressing other research questions potentially at larger spatial scales and with a higher number of objectives future research can investigate other research questions such as 1 where are the hotspots of robust optimal gi solutions 2 how do the hotspots change when optimizing for different gi ecosystem services 3 how do uncertainty considerations change the gi investment recommendations resulting from multi objective optimization 4 what sources of uncertainty have the strongest influence on the metrics of interest it should be noted that expanding the software s capabilities will require future work to overcome certain computing barriers because of the nature of our simulations even though we ran the example problem with parallel processors on a high performance computing cluster the higher number of core processors did not improve the simulation time we further tested whether the main bottleneck is disk i o speed by adjusting the swmm engine to run the simulations in memory and not on disk the adjustments did not improve the computation time we hypothesized that the bottleneck is the amount of cache for each cpu future work is needed to address this problem and enable the software to run on cloud higher performance computing platforms lastly the current library can also be improved to support more gi types such as green roofs incorporate other ecosystem services into multi objective optimization and accommodate unit systems other than the us imperial system 4 conclusions swmm is frequently used worldwide for urban drainage system design and green infrastructure modeling rhodium swmm provides a generalizable flexible open source interface for taking any swmm input file and setting up a multi objective optimization problem with the ability to define a wide range of swmm input parameters as uncertainties or levers in doing so rhodium swmm enables decision support that informs gi robustness and multi functionality and facilitates uncertainty analysis and scenario discovery it helps to efficiently search and sample states of the world and gi decision alternatives and identify vulnerabilities in the system the tool also opens new opportunities to investigate a wider variety of research questions in multi scale gi placement under deep uncertainty global cities are increasingly challenged by water management problems while adapting to climate change constantly facing shifting biophysical and sociopolitical environments green infrastructure planning needs publicly accessible generalizable and robust decision making tools such as ours to support the identification of multi functional cost effective stormwater solutions robust to future changes future research can continue to expand rhodium swmm s capabilities by integrating additional gi types and ecosystem services for example to maximize the co benefit provision of future green infrastructure in an uncertain world declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was supported by the rand frederick s pardee center for longer range global policy and the future human condition mid atlantic regional integrated sciences and assessments marisa penn state initiative for resilient community psirc penn state center for climate risk management hamer center for community design and the thayer school of engineering at dartmouth college we thank george rossick for his invaluable contribution to software development and usability sitara baboolal for replicating the code on linux windows and mac machines and members of the keller research group for their feedback throughout research design and development n t and k k designed the research j f r l d k h w l d i and k k helped with the research development n t developed the software and performed the analysis n t wrote the paper j f r l h w l d i and k k helped with editing and revising the paper h w guided writing revisions appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105671 
25420,green infrastructure gi measures are increasingly used for climate adaptation in urban areas but it remains a challenge to evaluate their effectiveness and strategically allocate investment planning gi is subject to deep uncertainties and requires navigating tradeoffs between multiple objectives many objective robust decision making mordm can be useful in addressing these modeling challenges thus far mordm has been used sparsely for gi planning to help mainstream mordm applications in gi planning we developed an open source python library rhodium swmm rhodium swmm connects the usepa s stormwater management model swmm to rhodium a python library for mordm rhodium swmm provides a generalizable and flexible interface for taking swmm input files and setting up a multi objective optimization problem with the ability to define a wide range of parameters in the swmm input file as uncertainties or levers this opens opportunities to more conveniently analyze new research questions in multi scale gi placement under deep uncertainty keywords rhodium swmm robust decision making green infrastructure planning uncertainty multi objective optimization data availability data will be made available on request software availability name of software rhodium swmm developer nastaran tebyanian department of architecture the pennsylvania state university university park pa usa with contributions from george rossick zetier inc state college pa usa year first available 2022 hardware required 1 gb ram software required docker availability https github com nastarant rhodium swmm cost free program language python supported systems linux windows mac x86 machines program size 4 3 mb license rhodium swmm can be redistributed and or modified under the terms of the gnu general public license by the free software foundation version 3 or later these files are distributed in the hope that they will be useful but without any warranty without even the implied warranty of merchantability or fitness for a particular purpose see the gnu general public license for more details 1 introduction green infrastructure gi defined as stormwater management practices that mimic natural hydrological processes usepa 2020 has been increasingly used for mitigating the negative impacts of urban development liberalesso et al 2020 and climate change adaptation matthews et al 2015 although gi implementations have been primarily motivated by their stormwater flow reduction and treatment functions gi provides many other ecosystem services such as regulating local climate supporting habitat and enhancing human physical and mental health hansen and pauleit 2014 therefore effective gi planning requires harnessing multiple benefits of gi and ensuring that these benefits are robust to future changes for example climate change significantly affects the frequency intensity and or duration of extreme weather events globally revi et al 2014 willems and olsson 2012 indicated a 10 60 increase in future 2100 rainfall intensity compared to the recent past 1961 1990 at the small urban watershed scale such drastic changes in climate will most likely significantly affect gi efficacy in addition to climate change other factors such as urbanization and associated alteration of the natural hydrological cycle and aging gray stormwater infrastructure will also continue to challenge multi functional gi planning incorporating multi functionality in gi planning while considering uncertainty requires advanced decision making tools although methods for decision making under deep uncertainty such as robust decision making rdm casal campos et al 2015 mei et al 2018 and many objective robust decision making mordm fischbach et al 2017 2020 have been applied to gi planning before there is a critical lack of generalizable open source tools to support multi scale multi objective gi decision making under uncertainty to address this gap we have developed rhodium swmm an open source python library that facilitates integrating uncertainty considerations into multi scale gi decision making this paper introduces the library and showcases an example of the novel capabilities it provides we begin this article with an in depth literature review of the limitations of traditional modeling approaches and the advantages of alternative approaches sub sections 1 1 and 1 2 section 2 then introduces rhodium swmm s components and software architecture as well as the methods for quantifying responses and uncertainties section 3 showcases the tool by demonstrating analyses for an example problem of gi placement lastly we conclude by briefly summarizing the tool s capabilities and future research steps 1 1 limitations of traditional gi modeling approaches with planning objectives such as optimizing stormwater quantity and cost traditional gi modeling approaches typically use gi location type and size as the most important parameters that influence the provision and distribution of gi benefits under current conditions wang et al 2020 zhang and chui 2018 zhang and chui 2018 frame these parameters as allocation components and this kind of decision problem as a spatial allocation problem they compare two methods that provide decision support to these problems i e scenario based models and spatial allocation optimization tools concluding that the latter offers more capacity to showcase tradeoffs between different objectives multi objective gi spatial allocation is a well studied problem researchers have explored many different aspects of modeling such as integrating overland and river processes leng et al 2022 incorporating socioecological indexes and including both green and gray infrastructure in optimization liu et al 2021 and performance of different multi objective optimization algorithms xu et al 2018 recent studies also have begun to support multi functionality and multi scale approaches e g hansen and pauleit 2014 vogel et al 2015 however uncertainty considerations have only been integrated into a limited number of gi spatial allocation studies tebyanian et al 2022 uncertainty considerations can change what we consider optimal and robust gi policies jayasooriya and ng 2014 tebyanian et al 2022 wang et al 2020 uncertainties surrounding gi objectives affect the gi policies performance a fast growing body of research has considered uncertainty focused on water quality quantity and cost objectives tebyanian et al 2022 examples of the uncertainties addressed in gi spatial allocation literature belong to the categories of hydrologic modeling hydraulic modeling water quality modeling gi cost gi adoption implementation gi design stakeholder preferences and data resolution ashley et al 2018 dong et al 2021 eckart et al 2018 gu et al 2018 jayasooriya et al 2018 kazak et al 2018 leng et al 2021 lim and welty 2018 liu et al 2019 piscopo et al 2021 raei et al 2019 xu et al 2019 among these uncertainties rainfall and land use uncertainties have been the most frequently studied if we expand the objectives to include other benefits such as urban heat island uhi reduction and biodiversity the categories of relevant uncertainties could increase considerably hou et al 2013 however ecosystem services beyond hydrological benefits have rarely been integrated into multi objective gi optimization literature tebyanian et al 2022 limited scenario analysis is another major limitation in existing gi spatial allocation studies that consider uncertainty tebyanian et al 2022 a small number of planning alternatives are usually prescribed rather than searched and sampled because of this they do not support decision making under uncertainty and can prove counter productive and sometimes dangerous in a fast changing world lempert 2019 they also do not address questions raised by the deep uncertainties surrounding gi models decision levers i e actions that comprise the alternative strategies decision makers want to explore lempert 2003 and success metrics tebyanian et al 2022 1 2 robust decision making rdm and many objective robust decision making mordm when predictions are unreliable it can be beneficial to adopt alternative modeling approaches that run the traditional analysis backward lempert et al 2013 and explore many plausible futures to identify conditions under which the plans policies perform well or poorly lempert et al 2013 among these approaches is robust decision making rdm rdm is a multi scenario multi objective decision analysis approach that stress tests strategies over myriad plausible paths into the future and then identifies policy relevant scenarios and robust adaptive strategies lempert 2019 rdm embedded analytic tools also promote learning and consensus building among stakeholders lempert 2019 rdm has been applied to various environmental problems such as evaluating water fischbach et al 2015 matrosov et al 2013 and climate policies hall et al 2012 when dealing with multiple and potentially conflicting objectives multi objective optimization approaches to decision making under uncertainty such as many objective robust decision making mordm can enable the discovery of tradeoffs among planning objectives kasprzyk et al 2013 by combining many objective evolutionary optimization moea such as the widely used nsga ii algorithm and rdm mordm tests each solution under the ensemble of future extreme states of the world sow kasprzyk et al 2013 additionally the extensive use of interactive visual analytics in mordm facilitates the management of complex environmental systems kasprzyk et al 2013 the mordm process has four main steps 1 problem formulation 2 alternatives generation 3 uncertainty analysis and 4 scenario discovery and tradeoff analysis mordm has been applied to environmental problems such as water allocation yan et al 2017 ecosystem management singh et al 2015 flood risk management zarekarizi et al 2020 and agriculture decision making gonzález et al 2020 in the context of gi planning the applications of rdm and mordm have been limited especially for the latter fischbach et al 2017 2020 use rdm to examine the vulnerabilities and tradeoffs of gi policies in regional and urban watershed scales in pittsburgh pennsylvania usa the studies assess gi policies for several water metrics as well as gi co benefits and cost the levers are regions of gi policies with different gi investment intensities and types casal campos et al 2015 apply rdm to assess acceptable performances of the combination of gray and green strategies in several metrics such as flooding cost and health across different future scenarios the study focuses on regret calculation and does not form a multi objective optimization problem finally piscopo et al 2021 apply mordm to optimize gi placement for water quality quantity and cost using levers including the amount of impervious area treated with gi gi size and type facilitating the applications of rdm and mordm in gi planning offers great promises in identifying robust stormwater solutions that simultaneously address multiple objectives and deep uncertainties 2 rhodium swmm rhodium swmm is an open source python library for robust green infrastructure planning under deep uncertainty developed in the linux environment it connects two open source tools the stormwater management model swmm by the us environmental protection agency epa rossman 2015 and rhodium a python library for mordm hadjimichael et al 2020 swmm is a dynamic rainfall runoff simulation model used for the design and analysis of urban drainage systems rossman 2015 although many other urban drainage models exist such as sustain shoemaker et al 2009 mike urban mike urban documentation index n d musicx wong et al 2012 the low impact development rapid assessment lidra montalto et al 2007 and city drain achleitner et al 2007 swmm is the most widely applied urban model globally and is also the most frequently used software in studies that provide decision support for sustainable urban drainage systems ferrans et al 2022 niazi et al 2017 swmm is considered the most sophisticated for modeling stormwater quality quantity and gi performance regarding accuracy algorithms and the temporal and spatial scales covered ferrans et al 2022 jayasooriya and ng 2014 niazi et al 2017 swmm can simulate either single event or long term runoff quantity and quality its runoff component operates on sub catchments that receive precipitation and generate runoff and pollutant loads whereas the routing portion transports this runoff through a system of pipes channels storage treatment devices pumps and regulators rossman 2015 gi modeling is supported in swmm under low impact development lid a synonym for gi controls swmm s input file includes all necessary information for the stormwater model including the names and locations of sub catchments junctions outfalls and gi types it also includes simulation specifications such as duration and timesteps and environmental specifications such as rainfall and river stage information it can also include links to external data such as rainfall time series rhodium is an open source python library for rdm mordm and exploratory modeling hadjimichael et al 2020 rhodium helps identify robust strategies for managing complex environmental systems by evaluating tradeoffs among candidate strategies and characterizing their vulnerabilities the core classes in rhodium include 1 model which enables the definition of the simulation model to be analyzed 2 parameter which can be used to define model parameters that can either be constant controlled by a lever or subject to uncertainty 3 response which represents model outputs and can be set to minimize or maximize 4 constraint which is used to set hard constraints that must be satisfied for a candidate solution to be considered feasible 5 lever which defines an adjustable lever that controls a model parameter during optimization hadjimichael et al 2020 a brief guide on how to set up levers uncertainties and constraints has been included in appendix a to combine rhodium and swmm we first adapted the windows based swmm for the linux environment by compiling its source code using the open water analytics modification of the source code s build system mcdonnell et al 2021 we also utilized code from ostrich swmm an open source tool for gi multi objective optimization macro et al 2019 to translate the swmm input file to and from a python dictionary format the primary components of rhodium swmm in the mordm process for gi decision making are shown in fig 1 after defining the problem the user can access all the rhodium mordm analytic capabilities for example the user can run multi objective optimization to find the pareto optimal set which is a set of solutions that cannot be outperformed by another solution across all performance metrics as an example of uncertainty analysis capabilities the user can run robust multi objective optimization to find the pareto satisficing set the pareto satisficing set includes solutions that remain close to the pareto optimal surfaces for different states of the world sow kasprzyk et al 2013 each sow is defined through a latin hypercube sampling lhs across uncertain parameters mckay et al 1979 this sampling method helps ensure that different values of uncertain parameters are represented regardless of their dominance mckay et al 1979 the robustness is measured through a combination of satisficing and regret measures satisficing measures emphasize maximizing the number of plausible futures in which some criteria are met while regret measures emphasize minimizing deviation from performance in the expected future hadka et al 2015 savage 1951 the user can also access the scenario discovery capacities by evaluating a policy in different sows in terms of both performances across metrics and robustness here classification algorithms such as the patient rule induction method prim and classification and regression trees cart are applied to identify the ranges of uncertain factors most likely to cause system failure under specific policies hadka et al 2015 2 1 rhodium swmm software architecture we developed the rhodium swmm library in python 3 for multi thread simulation it can be used with python 3 8 and higher otherwise 3 6 the core classes of rhodium swmm are rhodiumparameter swmminputelement and rhodiumswmmmodel fig 2 shows the inheritance and composition relationships among the main classes the rhodiumswmmmodel class comprises rhodiummodel lidcontrol lidusage and subcatchment classes lidcontrol lidusage subcatchment and rhodiumparameter classes inherit from swmminputelement lidcontrol lidusage and subcatchment classes mirror the corresponding sections of the swmm input file the combination of rhodiumparameter and swmminputelement class definitions creates the possibility of treating any component of the swmm input file as a parameter that rhodium can change the user can provide any swmm input file the lidcontrol including layer information lidusage and subcatchment values will be imported into the specified rhodiumswmmmodel layer class provides the capability to change layer information for each specific lid type for example for bioretention the layer includes surface soil storage and drain subclasses lidcontrol lidusage and subcatchment classes provide the ability to add modify the whole sections of lid controls lid usage and subcatchments in a swmm input file currently we have included bioretention infiltration trench rain barrel and permeable pavement as the supported lid types future work can be done to include other types such as green roofs when all the above components are imported into the specified rhodiumswmmmoldel the user can substitute any value in those swmminputelements with a rhodiumparameter in other words any specification in the swmm input file can be set as an instance of rhodiumparameter class and thus as levers uncertainties or constraints examples of levers include gi type gi design specification in the lid controls section of the swmm input file and gi size and area in the lid usage section of the swmm input file the same parameters can be set as constraints or uncertainties examples of uncertainties and constraints include the rainfall time series in the raingages section and impervious percentage in the subcatchment section of the swmm input file once the rhodiumswmmmodel is created by adjusting the swmm input file and specifying levers and uncertainties responses for gi placement will be calculated and written as a class that inherits from rhodiumswmmresponse and ultimately rhodium response class fig 2 currently there are four responses and their detailed calculations are explained in section 2 2 below 2 2 quantifying gi placement responses and uncertainties rhodium swmm calculates four main categories of responses related to gi planning 1 stormwater runoff metrics e g runoff volume or rate 2 cost 3 co benefit and 4 spatial priorities here cost accounts for both gi installation and lifetime operation and maintenance costs co benefit refers to the ecosystem benefits expressed in monetary value brought by green space creation spatial priorities integrate three metrics related to sub catchment vacancy gi aggregation and existing stormwater problems explained in detail later fig 3 shows the input data needed for each response calculation first the stormwater metric calculation is run by the swmm engine swmm exe using the swmm input file and external data such as rainfall rrf files pyswmm a python interface to swmm mcdonnell et al 2020 is integrated into rhodium swmm to provide the ability to extract and set any node or outfall statistics as a response the node statistics provide stormwater information for each junction or outfall in the swmm model including but not limited to water depth inflow volume flooding volume and duration and surcharge duration examples of outfall statistics are peak flow rate and pollutant loading one possibility is to use maximum hydraulic grade line hgl and flooding duration as metrics to estimate peak flood depth and duration of flooding in nodes of interest the current version of rhodium swmm uses the outfall peak flow volume as the default stormwater metric any specification in the swmm input file can be set as uncertainties that affect the stormwater metric calculation examples of uncertain factors are gi design specifications such as storage and soil specifications rainfall time series and the model boundary condition second cost response parameters include gi number and area planning horizon discount rate installation year installation cost operation and maintenance cost and gi lifetime the uncertain cost parameters defined in the current version of the tool are installation cost operation and maintenance cost and gi lifetime while the flexible design of the tool allows for setting up gi type as a lever the current version uses bioretention as the default gi type and focuses on the location and size of gi measures as levers bioretention refers to a landscape depression consisting of a surface ponding layer vegetation a soil layer a storage layer overflow structures and an optional underdrain system yang and chui 2018 and is often considered the workhorse of gis due to its water quantity and quality treatment functions for characterizing bioretention cost uncertainty we sample uniformly across the ranges of uncertain parameters extracted from the literature see table a1 appendix a the cost response is thus calculated as the sum of discounted costs in installation years first year and at the end of gi lifetime for reinstallation and regular annual discounted operation and maintenance costs for the planning horizon we discount the cost by dividing it by ediscount rate year 1 for the duration of the planning horizon if in the first year or when a reinstalling is due end of the gi lifetime the cost is calculated by y e a r l y c o s t l i d n u m b e r i n s t a l l c o s t l i d n u m b e r l i d a r e a o m cos t p e r a r e a e d i s c o u n t r a t e y e a r 1 otherwise regular year y e a r l y c o s t l i d n u m b e r l i d a r e a o m cos t p e r a r e a e d i s c o u n t r a t e y e a r 1 the total discounted cost for the whole planning horizon is i 1 n p l a n n i n g h o r i z o n y e a r l y d i s c o u n t e d c o s t third two types of co benefits are quantified through a literature review 1 green areas connected to gray infrastructure benefit and 2 park benefit bockarjova et al 2020 we define that the green connected to gray benefit happens at any scale of green infrastructure and the park benefit happens when the aggregated gi area exceeds the average lot size in the study area we chose 4046 86 m2 or 1 acre to represent the average lot size in the current version for the range of economic values of the co benefits see table a2 appendix a co benefit response parameters thus include gi number and area planning horizon discount rate green connected to gray benefit per area park benefit per area and gi efficacy ranges from 0 to 1 1 being the full co benefits realized the defined co benefit uncertain parameters are green connected to gray benefit per area park benefit per area and gi efficacy for characterizing co benefit uncertainty we sample uniformly across the ranges of uncertain parameters listed in table a2 appendix a based on these parameters the co benefit is calculated with the equations below when the gi area is 4046 86 m2 y e a r l y c o b e n e f i t l i d n u m b e r l i d a r e a p a r k b e n e f i t g i e f f i c a c y e d i s c o u n t r a t e y e a r 1 when the gi area is 4046 86 m2 y e a r l y c o b e n e f i t l i d n u m b e r l i d a r e a g r e e n g r a y b e n e f i t g i e f f i c a c y e d i s c o u n t r a t e y e a r 1 the total discounted economic co benefit for the whole planning horizon is i 1 n p l a n n i n g h o r i z o n y e a r l y d i s c o u n t e d c o b e n e f i t lastly we developed three metrics as examples of spatial priority objectives 1 vacant priority 2 aggregation priority and 3 community issue priority we intend to maximize the number of gi measures in 1 sub catchments with a large percentage of vacant land vacant priority 2 sub catchments with not only large vacant areas themselves but also at least one neighboring sub catchment with a high vacancy to provide opportunities for creating neighborhood parks from the aggregation of gi measures aggregation priority and 3 sub catchments suffering from existing stormwater issues as reported by the communities issue priority for aggregation priority specifically we assign an aggregation priority score of 1 3 respectively to 1 sub catchments without high vacancy 2 sub catchments with a high vacancy but no neighboring high vacancy sub catchment and 3 sub catchments with a high vacancy and at least one neighboring high vacancy sub catchment a priority score for each metric can be calculated for each sub catchment based on the csv inputs for each category the total priority score to maximize is calculated through i 1 n n u m b e r o f a l l s u b c a t c h m e n t s w i t h g i l i d a r e a l i d n u m b e r l i d s s u b c a t c h m e n t s p r i o r i t y s c o r e the priority score was then normalized to its original range by dividing the number above by the total lid area for example for vacant priority in a three sub catchment s1 s3 model if the vacant percentages for s1 s2 and s3 are 80 20 and 40 respectively and a solution installed 40 80 and 120 ft2 of lid in s1 through s3 then the vacant priority score for this solution is calculated as 80 40 20 80 40 120 240 total lid area 40 the aggregation and issue priorities can be calculated using the same approach with all the responses defined the goal in the multi objective optimization for gi placement is to optimize for stormwater metrics e g minimize runoff volume or rate minimize cost maximize the co benefit and maximize the spatial priority score 3 example green infrastructure placement problem this section illustrates the software using an example green infrastructure planning problem we used a sample epa stormwater management model site drainage model inp included with the installation of the swmm 5 1 desktop version us epa 2014 see model details in table a3 and fig a1 appendix a it models runoff quantity and quality in a 39 acre residential subdivision the site contains seven sub catchments ranging from 0 80 to 2 75 ha in area 2 0 3 1 in slope and 0 0 95 0 in impervious percentage the sub catchments are connected to a system of swales and culverts that convey runoff to an outfall appendix a we used the xlrm problem formulation from the mordm framework to define our problem lempert et al 2003 x in the xlrm diagram stands for exogenous uncertainties that affect the system l represents policy levers or actions the decision maker can take or the alternative strategies decision makers want to explore r is the relationship between levers and uncertain factors and the outcomes and m are the measures that decision makers use to rank the solutions based on different objectives lempert et al 2003 our specific xlrm problem formulation is shown in table 1 to showcase the uncertainty analysis integration we only considered the uncertainty in the cost calculation parameters of bioretention lifetime installation cost and operation and maintenance cost the lever is the number of bioretention units 40 ft2 3 72 m2 per unit in each sub catchment we set the limit of bioretention units per sub catchment to either the number of units that fully cover the sub catchment s impervious area or 100 units to control for total cost whichever is smaller because only six of the seven sub catchments have impervious surfaces hence bioretention installation the number of levers is six while the rhodium swmm package provides quantification of other metrics such as co benefit for a simpler demonstration of the tool we focused on three objectives 1 minimize peak runoff rate at the outfall 2 minimize discounted life cycle cost and 3 maximize installation in areas with higher vacant percentage we randomly assigned the vacant percentage of sub catchment one through seven as 60 20 100 40 10 80 and 5 respectively to showcase an example problem we created an example module in the rhodium swmm repository with two main files 1 initialize py for problem formulation and initializing the data folder and parallel processing and 2 spatial visualization a folder containing a python script map viz py the initialize py file is where the user can set the specification for rhodium model response uncertainties and levers it also allows the user to collect all necessary data and create a data folder in the testing folder for a specific simulation the user can then use the command line interface to perform rhodium analysis on the specified problem the rhodium analysis py and cli py in the rhodium swmm repository are responsible for the command line interface capabilities several commands have been defined to perform analyses such as multi objective optimization uncertainty analysis scenario discovery and tradeoff analysis see tebyanian 2022a for detailed command line capabilities when the analysis is performed one or multiple outputs are written to the test folder the python script map viz py gets these outputs such as a pareto set from multi objective optimization combines them with the shapefile data of the sub catchments and creates a map based visualization of the results 3 1 examples of analyses the connected rhodium analysis capabilities help address several gi placement questions we demonstrate three examples of these questions 1 what is the optimum number of gi measures in each sub catchment when optimizing for peak runoff rate and cost reduction while maximizing investment in sub catchments with high vacancy 2 what is the tradeoff between cost runoff and vacant spatial priority 3 if we choose a solution from the optimal set and set a criterion for acceptable cost in the future what are the future scenarios to which this solution will be vulnerable the three objective optimization using the nsgaii genetic algorithm resulted in an approximate pareto optimal set we analyzed the approximate pareto set with different numbers of runs to diagnose convergence fig a2 apx a we saw no apparent evidence for lack of convergence after 30 000 number of fitness evaluation nfe or the number of runs fig 4 addresses question one by illustrating the ratio of the bioretention area to the impervious area in each sub catchment resulting from a solution with the lowest peak runoff rate in the pareto set to understand the tradeoffs among the objectives we plotted the pareto set of the three objective optimization figs 5 and 6 for the third tradeoff plot please see figs a3 appendix a fig 5 shows two 2d plots of the 3d pareto front see fig a4 appendix a there are a few very low cost high peak runoff rate solutions with varying vacant percentage scores that bend the 3d pareto front curve these solutions are located on the right side of fig 5 right the same solutions exist in fig 5 left in the bottom right corner but are overlaid with other solutions in a 2d representation of the 3d space the parallel axes plot fig 6 shows all the gi solutions in the pareto set comparing their performances in peak runoff rate cost and vacant score this plot shows that reducing the first increment of the peak runoff rate in cubic feet per second or cfs is extremely inexpensive compared to further runoff reductions more importantly the very high cost solutions minimally enhance peak runoff reduction compared to those about 500 000 cheaper additionally the solutions considered good in terms of peak runoff rate 25 cfs have a 50 75 vacant percentage range this indicates a tradeoff between prioritizing gi in sub catchments with a 75 vacancy and optimizing peak runoff reduction there are also some low cost low runoff options that perform poorly in the vacant priority score metric 25 vacant percentage this analysis highlights the frequent tradeoffs while optimizing for cost hydrological performance and spatial preference for implementation while we showcased the vacant percentage as an example a wide range of other spatial priority metrics can be included to understand the future scenarios to which a particular solution will be vulnerable question three we chose a solution in the pareto set with the least peak runoff rate that costs 2 311 900 8 this solution proposes 100 99 94 100 0 and 100 units of 40 ft2 bioretention cells in sub catchments one through six we set the criterion for acceptable future cost as 2 5 million 8 increase and ran the prim classification algorithm using the uncertain ranges in cost parameters such as gi lifetime range 20 45 years default value 25 years installation cost range 3623 9 4429 2 default value 3623 9 and annual operation and maintenance cost range 2382 89 4765 79 default value 2382 89 the results show that the solution will be vulnerable to scenarios where the lifetime is less than 34 years and the installation cost is more than 4100 see fig a5 appendix a for the prim plots in addition to this low runoff high cost solution we also performed prim on multiple other solutions of differing costs to assess if the values of vulnerable scenarios would change however no meaningful differences emerged from these comparisons likely due to the limitation of using a simple example to showcase the tool despite these limitations this simple example remains valuable for providing an easy to follow showcase of the fundamental concepts and capabilities of the tool the primary contribution of this article is to explain in detail the development of the rhodium swmm software making it challenging to accommodate a more complex example problem involving gi decision making in a real world context future publications will be necessary to further demonstrate the tool s full capabilities using advanced case studies to provide more nuanced and decision relevant insights future test cases can include additional sources of uncertainty such as rainfall and co benefit uncertainties or additional objectives such as other spatial priority metrics and gi co benefits they will also require context specific research such as future climate scenarios and community spatial priority needs to address critical gi placement questions in respective cities beyond the types of gi questions showcased in the examples above the tool facilitates addressing other research questions potentially at larger spatial scales and with a higher number of objectives future research can investigate other research questions such as 1 where are the hotspots of robust optimal gi solutions 2 how do the hotspots change when optimizing for different gi ecosystem services 3 how do uncertainty considerations change the gi investment recommendations resulting from multi objective optimization 4 what sources of uncertainty have the strongest influence on the metrics of interest it should be noted that expanding the software s capabilities will require future work to overcome certain computing barriers because of the nature of our simulations even though we ran the example problem with parallel processors on a high performance computing cluster the higher number of core processors did not improve the simulation time we further tested whether the main bottleneck is disk i o speed by adjusting the swmm engine to run the simulations in memory and not on disk the adjustments did not improve the computation time we hypothesized that the bottleneck is the amount of cache for each cpu future work is needed to address this problem and enable the software to run on cloud higher performance computing platforms lastly the current library can also be improved to support more gi types such as green roofs incorporate other ecosystem services into multi objective optimization and accommodate unit systems other than the us imperial system 4 conclusions swmm is frequently used worldwide for urban drainage system design and green infrastructure modeling rhodium swmm provides a generalizable flexible open source interface for taking any swmm input file and setting up a multi objective optimization problem with the ability to define a wide range of swmm input parameters as uncertainties or levers in doing so rhodium swmm enables decision support that informs gi robustness and multi functionality and facilitates uncertainty analysis and scenario discovery it helps to efficiently search and sample states of the world and gi decision alternatives and identify vulnerabilities in the system the tool also opens new opportunities to investigate a wider variety of research questions in multi scale gi placement under deep uncertainty global cities are increasingly challenged by water management problems while adapting to climate change constantly facing shifting biophysical and sociopolitical environments green infrastructure planning needs publicly accessible generalizable and robust decision making tools such as ours to support the identification of multi functional cost effective stormwater solutions robust to future changes future research can continue to expand rhodium swmm s capabilities by integrating additional gi types and ecosystem services for example to maximize the co benefit provision of future green infrastructure in an uncertain world declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was supported by the rand frederick s pardee center for longer range global policy and the future human condition mid atlantic regional integrated sciences and assessments marisa penn state initiative for resilient community psirc penn state center for climate risk management hamer center for community design and the thayer school of engineering at dartmouth college we thank george rossick for his invaluable contribution to software development and usability sitara baboolal for replicating the code on linux windows and mac machines and members of the keller research group for their feedback throughout research design and development n t and k k designed the research j f r l d k h w l d i and k k helped with the research development n t developed the software and performed the analysis n t wrote the paper j f r l h w l d i and k k helped with editing and revising the paper h w guided writing revisions appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105671 
25421,two main approaches are commonly used to map fire prone areas when designing firefighting and prevention campaigns fire spread simulators and machine learning models despite they used mainly the same environmental variables they differ in handling them thus it is worth assessing differences in results and interpretations for supporting reliable decision making process burn probabilities bp were calculated in southern italy using flammap and the random forest algorithm results showed contrasting spatial patterns with random forest projecting more smoothed results than flammap which showed medium high bp values only across some locations in addition bp from flammap and random forest differ across fuel types and environmental conditions results suggest that decisions based on fire simulators might be more tightly linked with actions preventing fire spread in contrast those based on machine learning might be more linked with fire occurrence elements not necessarily related to spreading e g socioeconomic causes keywords wildfire simulators burn probability fire occurrence fire likelihood fire susceptibility wildfire management data availability the authors do not have permission to share data 1 introduction fire is a key natural factor that shapes vegetation strategies and drives landscape dynamics he et al 2019 however because of land use and climate change large scale and intense wildfires are becoming more common and an increasing concern for fire agencies tedim et al 2018 san miguel ayanz et al 2019 viegas et al 2019 wildfires natural and socioeconomic costs led to developing of tools and approaches for anticipating decisions to reduce wildfire impacts parisien et al 2019 indeed mapping fire prone areas hereafter referred as burning probability ager et al 2010 salis et al 2015 shang et al 2020 but also commonly named as fire susceptibility e g leuenberger et al 2018 tonini et al 2020 or fire occurrence probability likelihood e g elia et al 2020 milanović et al 2021 is commonly used across many fire agencies to design firefighting and prevention campaigns miller and ager 2013 parisien et al 2019 two main approaches are commonly used to map burn probabilities fire spread simulators fs e g ager et al 2010 salis et al 2015 lozano et al 2017 jahdi et al 2020 jahdi et al 2023 and machine learning models ml e g elia et al 2020 shang et al 2020 tonini et al 2020 milanović et al 2021 fs simulators are based on fire spread algorithms that spatially simulate fire growth under specific environmental conditions the minimum travel time algorithm mtt finney 2002 is the most common it is based on the rothermel equation which depends on topography climate conditions and fuel characteristics fuel loads surface volume ratios humidity heat content etc andrews 2018 mtt is implemented in well known and commonly used software such as flammap farsite and wildfire analyst returning burn probabilities bp which is the number of times a pixel is burned regarding the total number of fires simulated locations with greater bp mean that they are more prone to be crossed by fires because of spread drivers fs simulators are spatially explicit meaning that spread depends on how locations are interconnected in topography and vegetation furthermore fs simulators can be used to model bp assuming historical fire ignition patterns to highlight the main fire pathways conditioned by historical drivers somehow including also socio cultural factors or considering random ignition distribution thus showing the fire potential due to intrinsic environmental characteristics i e topography and fuel types fs simulators were used to assess potential climate change impacts on fire regimes lozano et al 2017 wildfire risk in the urban interface ager et al 2010 the effects of fuel treatments on fire spread salis et al 2016 or to assess wildfire exposure of high value assets alcasena et al 2015 instead ml approaches are based on statistical techniques and provide information on how likely a location is fire prone because it includes factors biophysical or socioeconomic related to fire occurrence ml approaches are based on historical fire data commonly retrieved from in situ datasets e g elia et al 2020 or estimated with remote sensing e g milanović et al 2021 fire data is then empirically related to several potential explanatory factors such as topography climate vegetation and human infrastructures after calibration and cross validation with historical data the ml model is projected over the territory in this way it allows assessing the burning probability also commonly referred to as fire occurrence probability likelihood over the whole region based on observed relationships between fires and these factors different modelling techniques have been used for mapping bp e g logistic regression de bem et al 2019 elia et al 2020 random forest tonini et al 2020 milanović et al 2021 and neural networks de bem et al 2019 elia et al 2020 the ml approach is mainly used to map fire prone areas and understand which factors influence fire occurrence across different scales and bioclimatic and geographic areas de bem et al 2019 elia et al 2020 tonini et al 2020 for instance cilli et al 2022 and qiu et al 2022 assessed fire susceptibility and its drivers in mediterranean europe and the us respectively whereas shmuel and heifetz 2022 performed analysis at a global scale furthermore ml was also used for modelling fire behaviour farguell et al 2021 and impacts costa saura et al 2022 current and future weather effects on fire danger velasco hererra et al 2022 and management actions such as the potential success of initial suppression attacks rodrigues et al 2019 although both approaches integrate the same factors i e topography climate and vegetation they differ in how they combine them fs simulators are based on physical processes governing the effects of these factors on fire spread finney 2002 in contrast ml are based on statistical relationships between fire occurrence and biophysical or socioeconomic factors miller and ager 2013 thus different data handling might lead to different results and interpretations for decision making miller and ager 2013 for instance decisions based on fire simulators might be more tightly linked with actions preventing fire spread in contrast those based on ml might be more linked with fire occurrence elements that are not necessarily related to spreading e g socioeconomic causes moreira et al 2001 thus a clear results comparison between both approaches in terms of statistical and spatial characteristics and how results are distributed across environmental factors helps understand better the underlying mechanisms of both methods and how they affect the interpretation of the results furthermore fire managers and multidisciplinary researchers sometimes only focus on the outputs without acknowledging the difference between both approaches making worthy of underlining differences in results and interpretations indeed to our knowledge there are no previous studies explicitly comparing both approaches or discussing the implications of using one of those specific methods rather than the other in fire prevention and management thus a rigorous comparison of results is required to ensure and support proper decision making processes to fill this gap and under the framework of the ofidia2 interreg italy greece project aimed to improve the operational capacity of detecting and fighting https www cmcc it it projects ofidia operational fire danger prevention platform 2 we ran flammap and built a random forests model to compare the projected burn probabilities of these two approaches which are commonly used in risk management to assess these agreements differences we analysed 1 the fire probability map of every single approach and how much they are correlated 2 the difference between both approaches across fuel types topographical and climate conditions and 3 the effect of including historical ignition density when estimating bp under both approaches 2 material and methods 2 1 study area and fire data the study area covers the apulia region southern italy which comprises approximately 19 345 km2 fig 1 around 50 of the study region consists of plains and rolling hills with low mountains in the north west area the highest peak reaches 1152 m above sea level the apulian climate can be considered typically mediterranean with hot and dry summers and mild rainy winters the mean annual temperature ranges from 12 c in mountainous areas to 19 c along the southern coast while maximum temperatures are above 34 c during the summer the mean values of rainfall vary between 450 and 650 mm per year daily fire perimeters in shapefile format from 2007 to 2017 were acquired from the former corpo forestale dello stato actually carabinieri c u f a a overall 4758 fires occurred during the study period burning 71 693 ha 2 2 fire spread simulations we used flammap to calculate burn probabilities for the apulia region flammap is a freely available fire behaviour software that computes potential fire perimeters using the minimum travel time mtt algorithm developed by finney 2002 which uses rothermel 1972 equation to calculate the fire rate of spread flammap needs topographical data i e elevation slope and aspect retrieved and computed using the eu dem v1 0 data from copernicus land monitoring https land copernicus eu fig 1 flammap also needs a fuel map which was created according to lozano et al 2017 reclassification of the corine land cover 2012 into 13 fuel types fuel data characteristics were set using data from elia et al 2015 and lozano et al 2017 and the fire model was validated under the framework of ofidia2 interreg italy greece project with three recent fire perimeters in the apulia region we analysed weather data during the fire starting dates to define weather conditions for the simulations fig 2 we used the era5 land dataset at 0 1 of resolution for the 2007 2017 period acquired through the copernicus climate data store https cds climate copernicus eu the analysis showed that the most common conditions during big fires and thus the values used for simulations are 15 km h 1 winds from the southwest with air humidity values around 20 fig 2 we simulated a total of 35 989 random fires i e approximately two ignition points per km2 of burnable area following the criteria set up by lozano et al 2017 fixed fire duration is a common procedure in this kind of study in particular lozano et al 2017 suggested 10 h as common fire duration for mediterranean areas to overcome computational limitations simulations were run at 200 m resolution bp was rescaled to 0 1 by dividing by the 99th percentile to compare results between fs and ml methods best 2 3 machine learning models ml models allow estimating bp based on statistical relationships between fire occurrence and explanatory factors we selected the random forest algorithm since previous studies showed that it performs better than traditional approaches such as logistic regression leuenberger et al 2018 milanović et al 2021 indeed our preliminary analysis showed that rf performed better than logistic regression data not shown furthermore random forests might perform similar or even better than deep learning algorithms such as neural networks fernández delgado et al 2014 leuenberger et al 2018 but requires less computation resources and pre processing in addition random forest algorithm is more robust with less data and easier to set up avoiding to tune several hyperparameters such as the number of layers and neurons the activation function and the training algorithm the momentum or the batch size random forest is a nonparametric supervised approach i e without assuming specific data distribution and shape of the effects based on decision trees the algorithm fits a number of trees by randomly splitting the original sample at each iteration and using a subset of explanatory variables when creating the nodes in every single tree this procedure minimizes correlation between the trees which improves the accuracy of the ensemble predictions breiman 2001 fire perimeters were converted into grid format to create the response variable thus if the fire polygon or a portion of it fell in a 200 m cell it was considered burned whereas if it did not it was considered unburned four types of explanatory variables commonly used when building ml models were employed here de bem et al 2019 elia et al 2020 tonini et al 2020 milanović et al 2021 cilli et al 2022 1 topographic variables i e elevation slope and aspect which were derived from the eu dem v1 0 https land copernicus eu imagery in situ eu dem eu dem v1 0 and derived products eu dem v1 0 2 climate based variables i e annual mean precipitation ap annual mean temperature tm and the summer mean of the canadian fire weather index fwi for the studied period which were derived from era5 land dataset https cds climate copernicus eu cdsapp dataset reanalysis era5 land tab form 3 vegetation information i e fuel types derived from the corine land cover 2012 as in flammap simulations and 4 human related variables i e distance to roads using data from the open street map https www openstreetmap org the explanatory variables were aligned and resampled according to fire data to build the data matrix 8730 samples for subsequent analysis to avoid collinearity between variables of the same type i e topographical and climate based we performed a principal component analysis for each type for this step the variables were scaled to avoid bias towards those with high variance finally we only retained slope aspect ap and fwi fig 3 for building the random forest model to validate and assess the model s accuracy we performed a 10 fold cross validation repeated 5 times procedure using the r package caret the method randomly selects the 75 of the dataset for training in each iteration and uses the other 25 to assess accuracy by using the area under the receiver operating characteristics curve auc which is commonly applied in ml approaches dealing with classification algorithms e g elia et al 2020 furthermore we assessed the importance of each variable in the model estimating the mean decrease in accuracy across trees after permutating the target variable 2 4 analysis to assess spatial differences in bp between flammap simulations and random forest projections we evaluated the correlation of both approaches furthermore we also showed how results change across topographical conditions slope and aspect classes ap and fwi classes based on european forest fire information system i e effis fire danger categories fuel types and distance to roads despite the study focusing on both approaches raw outputs we also assess the effect of considering the fire historical distribution when mapping results to achieve that an ignition density map was calculated and latterly multiplied by the bp maps the ignition density map was calculated following lozano et al 2017 which used the inverse distance weighted algorithm with a search radius of 5 000 km all the analyses were performed using the r software 3 results 3 1 flammap simulations flammap bp show a long tail distribution fig 4 with the highest values mainly located over a few specific areas i e middle and northern mountain areas fig 5 results also show that bp is mostly equal across aspect classes and it does not increase with fwi and ap fig 6 across fuel types higher bp values are observed over herbaceous maquis and low maquis fig 6 when accounting for the historical distribution of fires in the region bp values mainly decrease over sparse flat areas along the study area in contrast bp patterns are conserved over the other areas fig 5 3 2 random forest simulations the k fold cross validation of the random forest model shows an auc value of 0 88 higher bp is predicted over the northeast and west inland mountain areas fig 5 the most important variable in the model is the fuel types in contrast distance to roads is the one with lower importance fig 7 higher probabilities are mostly equally distributed across all fuel types except for orchards and grasslands showing lower values fig 6 overall fire probability increases with slope ap and fwi and is higher in southern slopes fig 6 when accounting for the historical distribution of fires in the region the extent of areas with middle values of probability decrease especially in urban areas fig 5 3 3 agreement across approaches overall random forest shows smoother fig 4 and broader distributed results over the study area than flammap the correlation between burn and fire occurrence probability was 0 49 and 0 25 when considering or not considering the historical distribution of fires in the region respectively despite some spatial similarities several discrepancies emerge fig 8 fire simulator projects higher risk in some mountain areas in the north and centre of the inland apulia region whereas random forest emphasizes the northern inland region 4 discussion and conclusions overall our results suggest that despite a certain agreement for specific areas fire simulators and machine learning models provide different results in terms of statistical properties spatial distribution and relationships between environmental factors and wildfire probability although post processing techniques might reduce these differences e g weighting probabilities using a historical ignition density grid how both methods develop their predictions i e statistically or process based is still conditioning the results and their interpretation despite in this work we only compared a ml method with one fs simulator to underline the difference between both approaches we did not expect significant differences across ml algorithms and fs models when using the same explanatory variables previous studies comparing ml algorithms that are quite different in terms of flexibility when handling the data i e non parametric approaches such as neural networks and forcing methods such as logistic regression only found small differences in performance and somehow similar spatial patterns leuenberger et al 2018 de bem et al 2019 elia d este et al 2020 across fs simulators e g flammap burnp3 brunpro or wildfire analyst since they share similar conceptual approaches parisien et al 2019 we did not expect important differences when estimating bp at regional scales we focused on flammap since it is commonly used in research and for operational purposes parisien et al 2019 nevertheless a detailed analysis comparing different algorithms simulators will prove worthy to highlight potential advantages and drawbacks overall random forest showed more uniformly distributed results than flammap which concentrated most of its values close to zero except for some locations with medium and high probabilities in the north and centre of apulia spatially restricted results in flammap suggest that since ignition points were randomly distributed across the whole study area topographical and fuel conditions of those areas make fires regardless of where they initiate more likely to go through since results rely on spread algorithm it suggests that management actions should be oriented to reduce spread e g fuel reductions or barriers to break fires ager et al 2010 this interpretation is crucial since it should differ from machine learning models focused on factors associated with fire occurrence indeed fire simulator outputs are commonly used to assess the effects of fuel treatments on fire spread and behaviour salis et al 2016 although a greater number of ignition points and longer fire duration can be set up for simulations minor effects on main spatial patterns are expected since ignitions are distributed randomly a large enough number of ignition points ensures a good representation of bp avoiding time consuming computations indeed restricted areas for high bp values are commonly observed across different studies regardless of their scale salis et al 2015 jahdi et al 2020 proving beneficial to identify the most critical locations in terms of fire spread furthermore these results highlight two main patterns in the study area constrained and not constrained spread locations i e low and high bp respectively which are of primary importance for management however it is essential to notice that in other regions high bp values might be widespread because of land conformation e g slope and fuel loads regarding machine learning models random forest yields more smoothed results highlighting inland and mountain areas in the northeast results agreed with previous studies using other algorithms at larger scales e g artificial neural networks elia et al 2020 contrasting with fire spread simulators random forest should be interpreted as isolated pixel information since pixel probability does not depend on their surroundings thus managers should carefully review the area e g a pixel and their surroundings to assess wildfire probability better furthermore since machine learning methods rely on statistical relationships between explanatory variables and wildfires forest managers should also analyse the values of the variables within specific areas and their importance in the model e g regression coefficients to detect the primary driver of bp and thus better select specific prevention actions despite being less intuitive the approach could provide complementary information to spread models helping to address wildfire issues better e g detecting socioeconomic cultural drivers the bp output from flammap and random forest change across fuel types and environmental conditions in the study area across fuel types flammap bp was higher under herbaceous maquis and low maquis fuel types suggesting that in the study area these locations are much more likely to burn during wildfires and their spread is potentially higher than in other fuel types however other studies observed greater bp over different vegetation types jahdi et al 2020 indicating that landscape conformation i e topography and fuel distribution might drive this observed pattern furthermore those vegetation types were not the most abundant in the study area reinforcing the idea that landscape conformation is the main driver indeed the slope seemed to be associated with bp in flammap with the greater the slope greater the bp although the slope is an important driver of the rate of spread andrews 2018 other studies did not observe a strong association with bp when using fs simulators ye et al 2017 suggesting that the results are highly dependent on the different landscape components e g the spatial disbrution of cultivated areas this confirms that extrapolating results should be made with caution indeed previous studies showed how the relative importance of different factors might change across landscapes miller et al 2010 surprisingly flammap bp did not increase with summer mean fwi ap and aspect the latter expected higher in southern slopes because of greater solar radiation and thus greater fuel dryness it suggests that in the study area main fire pathways are not associated to climatological fire prone areas most likely the scale of the analysis is driving this result since other studies showed that the importance of weather conditions increases with the extent scale of the analysis liu et al 2013 it is important to notice that the spatial variability of live fuel dryness is not considered in flammap so its potential effect on making some areas more likely to be crossed by wildfires because regularly become dryer than others is not accounted however if substantial differences in dryness exist across locations differences in vegetation fuel types will be also expected which might affect in a greater degree fire spread and bp random forest also projected higher probabilities with increasing the slope and on fuel types such as herbaceous maquis and low maquis the higher bp over those conditions as suggested by flammap projections might be associated with land conformation which converts these conditions as main fire pathways interestingly random forest predicted higher bp over conifers than broadleaf forests whereas flammap did not detect this pattern it suggests that despite fires might occur more frequently in conifers forest they are not located in the main fire pathways this is an important issue for wildfire risk management in the region since it conditions the type of fire fighting actions appropriated to each location e g speeding up suppression efforts or reducing ignition factors in high susceptible areas versus constraining spread in main fire pathways however in contrast with flammap random forest probabilities increase with dryness and meteorological fire risk in terms of southern slopes and greater fwi suggesting that drivers of fire occurrence in the study area differ from those of spread this is an important consideration since it indicates that fires might occur over those locations not because factors are favouring spread but because of potential occurrence causes e g close to ignition location or fuel dryness thus fire managers should take care when interpreting results since management actions should adapt accordingly to the method used surprisingly distance to roads had a minor role in predicting bp when using random forest some previous studies found a significant influence of this variable but were focused on ignition factors rather than on the susceptibility of the territory ganteaume et al 2013 elia et al 2020b indeed studies at larger scales focused on fire occurrence did not observed such importance de bem et al 2019 elia et al 2020 suggesting that the drivers of these two features might differ substantially applying an ignition density grid to account for the historical distribution of fires proved to be useful mainly under the random forest approach for instance it helped to mask not burnable areas highlighted because most likely one or more factors related to fire occurrence were present in those locations e g high fwi or low distance to roads this issue does not happen with flammap since the spread is impossible because of the absence of fuel i e vegetation integrating an ignition density grid by default when using fire simulators is a common practice alcasena et al 2015 salis et al 2015 lozano et al 2017 jahdi et al 2020 however it might hide valuable information in landscape management contrasting results with and without integrating historical patterns might reveal potential spread areas that did not burn in the past this information might serve both 1 to not forget those locations when designing prevention risk campaigns and 2 to analyse which factors e g traditional practices make those areas not fire prone the present study shows how results might differ among methods used to map fire probability also referred in the literature as fire susceptibility or likelihood and how interpretations should change accordingly however paying attention to this both approaches should provide complementary information for fire agencies for instance bp from fire simulators can be more used explicitly for planning fuel management strategies salis et al 2016 and to assess assets exposure to wildfires alcasena et al 2015 whereas bp from random forest can be exploited to detect main human factors driving fire occurrence and thus orient socio cultural prevention actions rodrigues and de la riva 2014 indeed as in other scientific disciplines a multi model framework multi approach multi weather scenarios might be helpful to assess better uncertainties associated with fire risk declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25421,two main approaches are commonly used to map fire prone areas when designing firefighting and prevention campaigns fire spread simulators and machine learning models despite they used mainly the same environmental variables they differ in handling them thus it is worth assessing differences in results and interpretations for supporting reliable decision making process burn probabilities bp were calculated in southern italy using flammap and the random forest algorithm results showed contrasting spatial patterns with random forest projecting more smoothed results than flammap which showed medium high bp values only across some locations in addition bp from flammap and random forest differ across fuel types and environmental conditions results suggest that decisions based on fire simulators might be more tightly linked with actions preventing fire spread in contrast those based on machine learning might be more linked with fire occurrence elements not necessarily related to spreading e g socioeconomic causes keywords wildfire simulators burn probability fire occurrence fire likelihood fire susceptibility wildfire management data availability the authors do not have permission to share data 1 introduction fire is a key natural factor that shapes vegetation strategies and drives landscape dynamics he et al 2019 however because of land use and climate change large scale and intense wildfires are becoming more common and an increasing concern for fire agencies tedim et al 2018 san miguel ayanz et al 2019 viegas et al 2019 wildfires natural and socioeconomic costs led to developing of tools and approaches for anticipating decisions to reduce wildfire impacts parisien et al 2019 indeed mapping fire prone areas hereafter referred as burning probability ager et al 2010 salis et al 2015 shang et al 2020 but also commonly named as fire susceptibility e g leuenberger et al 2018 tonini et al 2020 or fire occurrence probability likelihood e g elia et al 2020 milanović et al 2021 is commonly used across many fire agencies to design firefighting and prevention campaigns miller and ager 2013 parisien et al 2019 two main approaches are commonly used to map burn probabilities fire spread simulators fs e g ager et al 2010 salis et al 2015 lozano et al 2017 jahdi et al 2020 jahdi et al 2023 and machine learning models ml e g elia et al 2020 shang et al 2020 tonini et al 2020 milanović et al 2021 fs simulators are based on fire spread algorithms that spatially simulate fire growth under specific environmental conditions the minimum travel time algorithm mtt finney 2002 is the most common it is based on the rothermel equation which depends on topography climate conditions and fuel characteristics fuel loads surface volume ratios humidity heat content etc andrews 2018 mtt is implemented in well known and commonly used software such as flammap farsite and wildfire analyst returning burn probabilities bp which is the number of times a pixel is burned regarding the total number of fires simulated locations with greater bp mean that they are more prone to be crossed by fires because of spread drivers fs simulators are spatially explicit meaning that spread depends on how locations are interconnected in topography and vegetation furthermore fs simulators can be used to model bp assuming historical fire ignition patterns to highlight the main fire pathways conditioned by historical drivers somehow including also socio cultural factors or considering random ignition distribution thus showing the fire potential due to intrinsic environmental characteristics i e topography and fuel types fs simulators were used to assess potential climate change impacts on fire regimes lozano et al 2017 wildfire risk in the urban interface ager et al 2010 the effects of fuel treatments on fire spread salis et al 2016 or to assess wildfire exposure of high value assets alcasena et al 2015 instead ml approaches are based on statistical techniques and provide information on how likely a location is fire prone because it includes factors biophysical or socioeconomic related to fire occurrence ml approaches are based on historical fire data commonly retrieved from in situ datasets e g elia et al 2020 or estimated with remote sensing e g milanović et al 2021 fire data is then empirically related to several potential explanatory factors such as topography climate vegetation and human infrastructures after calibration and cross validation with historical data the ml model is projected over the territory in this way it allows assessing the burning probability also commonly referred to as fire occurrence probability likelihood over the whole region based on observed relationships between fires and these factors different modelling techniques have been used for mapping bp e g logistic regression de bem et al 2019 elia et al 2020 random forest tonini et al 2020 milanović et al 2021 and neural networks de bem et al 2019 elia et al 2020 the ml approach is mainly used to map fire prone areas and understand which factors influence fire occurrence across different scales and bioclimatic and geographic areas de bem et al 2019 elia et al 2020 tonini et al 2020 for instance cilli et al 2022 and qiu et al 2022 assessed fire susceptibility and its drivers in mediterranean europe and the us respectively whereas shmuel and heifetz 2022 performed analysis at a global scale furthermore ml was also used for modelling fire behaviour farguell et al 2021 and impacts costa saura et al 2022 current and future weather effects on fire danger velasco hererra et al 2022 and management actions such as the potential success of initial suppression attacks rodrigues et al 2019 although both approaches integrate the same factors i e topography climate and vegetation they differ in how they combine them fs simulators are based on physical processes governing the effects of these factors on fire spread finney 2002 in contrast ml are based on statistical relationships between fire occurrence and biophysical or socioeconomic factors miller and ager 2013 thus different data handling might lead to different results and interpretations for decision making miller and ager 2013 for instance decisions based on fire simulators might be more tightly linked with actions preventing fire spread in contrast those based on ml might be more linked with fire occurrence elements that are not necessarily related to spreading e g socioeconomic causes moreira et al 2001 thus a clear results comparison between both approaches in terms of statistical and spatial characteristics and how results are distributed across environmental factors helps understand better the underlying mechanisms of both methods and how they affect the interpretation of the results furthermore fire managers and multidisciplinary researchers sometimes only focus on the outputs without acknowledging the difference between both approaches making worthy of underlining differences in results and interpretations indeed to our knowledge there are no previous studies explicitly comparing both approaches or discussing the implications of using one of those specific methods rather than the other in fire prevention and management thus a rigorous comparison of results is required to ensure and support proper decision making processes to fill this gap and under the framework of the ofidia2 interreg italy greece project aimed to improve the operational capacity of detecting and fighting https www cmcc it it projects ofidia operational fire danger prevention platform 2 we ran flammap and built a random forests model to compare the projected burn probabilities of these two approaches which are commonly used in risk management to assess these agreements differences we analysed 1 the fire probability map of every single approach and how much they are correlated 2 the difference between both approaches across fuel types topographical and climate conditions and 3 the effect of including historical ignition density when estimating bp under both approaches 2 material and methods 2 1 study area and fire data the study area covers the apulia region southern italy which comprises approximately 19 345 km2 fig 1 around 50 of the study region consists of plains and rolling hills with low mountains in the north west area the highest peak reaches 1152 m above sea level the apulian climate can be considered typically mediterranean with hot and dry summers and mild rainy winters the mean annual temperature ranges from 12 c in mountainous areas to 19 c along the southern coast while maximum temperatures are above 34 c during the summer the mean values of rainfall vary between 450 and 650 mm per year daily fire perimeters in shapefile format from 2007 to 2017 were acquired from the former corpo forestale dello stato actually carabinieri c u f a a overall 4758 fires occurred during the study period burning 71 693 ha 2 2 fire spread simulations we used flammap to calculate burn probabilities for the apulia region flammap is a freely available fire behaviour software that computes potential fire perimeters using the minimum travel time mtt algorithm developed by finney 2002 which uses rothermel 1972 equation to calculate the fire rate of spread flammap needs topographical data i e elevation slope and aspect retrieved and computed using the eu dem v1 0 data from copernicus land monitoring https land copernicus eu fig 1 flammap also needs a fuel map which was created according to lozano et al 2017 reclassification of the corine land cover 2012 into 13 fuel types fuel data characteristics were set using data from elia et al 2015 and lozano et al 2017 and the fire model was validated under the framework of ofidia2 interreg italy greece project with three recent fire perimeters in the apulia region we analysed weather data during the fire starting dates to define weather conditions for the simulations fig 2 we used the era5 land dataset at 0 1 of resolution for the 2007 2017 period acquired through the copernicus climate data store https cds climate copernicus eu the analysis showed that the most common conditions during big fires and thus the values used for simulations are 15 km h 1 winds from the southwest with air humidity values around 20 fig 2 we simulated a total of 35 989 random fires i e approximately two ignition points per km2 of burnable area following the criteria set up by lozano et al 2017 fixed fire duration is a common procedure in this kind of study in particular lozano et al 2017 suggested 10 h as common fire duration for mediterranean areas to overcome computational limitations simulations were run at 200 m resolution bp was rescaled to 0 1 by dividing by the 99th percentile to compare results between fs and ml methods best 2 3 machine learning models ml models allow estimating bp based on statistical relationships between fire occurrence and explanatory factors we selected the random forest algorithm since previous studies showed that it performs better than traditional approaches such as logistic regression leuenberger et al 2018 milanović et al 2021 indeed our preliminary analysis showed that rf performed better than logistic regression data not shown furthermore random forests might perform similar or even better than deep learning algorithms such as neural networks fernández delgado et al 2014 leuenberger et al 2018 but requires less computation resources and pre processing in addition random forest algorithm is more robust with less data and easier to set up avoiding to tune several hyperparameters such as the number of layers and neurons the activation function and the training algorithm the momentum or the batch size random forest is a nonparametric supervised approach i e without assuming specific data distribution and shape of the effects based on decision trees the algorithm fits a number of trees by randomly splitting the original sample at each iteration and using a subset of explanatory variables when creating the nodes in every single tree this procedure minimizes correlation between the trees which improves the accuracy of the ensemble predictions breiman 2001 fire perimeters were converted into grid format to create the response variable thus if the fire polygon or a portion of it fell in a 200 m cell it was considered burned whereas if it did not it was considered unburned four types of explanatory variables commonly used when building ml models were employed here de bem et al 2019 elia et al 2020 tonini et al 2020 milanović et al 2021 cilli et al 2022 1 topographic variables i e elevation slope and aspect which were derived from the eu dem v1 0 https land copernicus eu imagery in situ eu dem eu dem v1 0 and derived products eu dem v1 0 2 climate based variables i e annual mean precipitation ap annual mean temperature tm and the summer mean of the canadian fire weather index fwi for the studied period which were derived from era5 land dataset https cds climate copernicus eu cdsapp dataset reanalysis era5 land tab form 3 vegetation information i e fuel types derived from the corine land cover 2012 as in flammap simulations and 4 human related variables i e distance to roads using data from the open street map https www openstreetmap org the explanatory variables were aligned and resampled according to fire data to build the data matrix 8730 samples for subsequent analysis to avoid collinearity between variables of the same type i e topographical and climate based we performed a principal component analysis for each type for this step the variables were scaled to avoid bias towards those with high variance finally we only retained slope aspect ap and fwi fig 3 for building the random forest model to validate and assess the model s accuracy we performed a 10 fold cross validation repeated 5 times procedure using the r package caret the method randomly selects the 75 of the dataset for training in each iteration and uses the other 25 to assess accuracy by using the area under the receiver operating characteristics curve auc which is commonly applied in ml approaches dealing with classification algorithms e g elia et al 2020 furthermore we assessed the importance of each variable in the model estimating the mean decrease in accuracy across trees after permutating the target variable 2 4 analysis to assess spatial differences in bp between flammap simulations and random forest projections we evaluated the correlation of both approaches furthermore we also showed how results change across topographical conditions slope and aspect classes ap and fwi classes based on european forest fire information system i e effis fire danger categories fuel types and distance to roads despite the study focusing on both approaches raw outputs we also assess the effect of considering the fire historical distribution when mapping results to achieve that an ignition density map was calculated and latterly multiplied by the bp maps the ignition density map was calculated following lozano et al 2017 which used the inverse distance weighted algorithm with a search radius of 5 000 km all the analyses were performed using the r software 3 results 3 1 flammap simulations flammap bp show a long tail distribution fig 4 with the highest values mainly located over a few specific areas i e middle and northern mountain areas fig 5 results also show that bp is mostly equal across aspect classes and it does not increase with fwi and ap fig 6 across fuel types higher bp values are observed over herbaceous maquis and low maquis fig 6 when accounting for the historical distribution of fires in the region bp values mainly decrease over sparse flat areas along the study area in contrast bp patterns are conserved over the other areas fig 5 3 2 random forest simulations the k fold cross validation of the random forest model shows an auc value of 0 88 higher bp is predicted over the northeast and west inland mountain areas fig 5 the most important variable in the model is the fuel types in contrast distance to roads is the one with lower importance fig 7 higher probabilities are mostly equally distributed across all fuel types except for orchards and grasslands showing lower values fig 6 overall fire probability increases with slope ap and fwi and is higher in southern slopes fig 6 when accounting for the historical distribution of fires in the region the extent of areas with middle values of probability decrease especially in urban areas fig 5 3 3 agreement across approaches overall random forest shows smoother fig 4 and broader distributed results over the study area than flammap the correlation between burn and fire occurrence probability was 0 49 and 0 25 when considering or not considering the historical distribution of fires in the region respectively despite some spatial similarities several discrepancies emerge fig 8 fire simulator projects higher risk in some mountain areas in the north and centre of the inland apulia region whereas random forest emphasizes the northern inland region 4 discussion and conclusions overall our results suggest that despite a certain agreement for specific areas fire simulators and machine learning models provide different results in terms of statistical properties spatial distribution and relationships between environmental factors and wildfire probability although post processing techniques might reduce these differences e g weighting probabilities using a historical ignition density grid how both methods develop their predictions i e statistically or process based is still conditioning the results and their interpretation despite in this work we only compared a ml method with one fs simulator to underline the difference between both approaches we did not expect significant differences across ml algorithms and fs models when using the same explanatory variables previous studies comparing ml algorithms that are quite different in terms of flexibility when handling the data i e non parametric approaches such as neural networks and forcing methods such as logistic regression only found small differences in performance and somehow similar spatial patterns leuenberger et al 2018 de bem et al 2019 elia d este et al 2020 across fs simulators e g flammap burnp3 brunpro or wildfire analyst since they share similar conceptual approaches parisien et al 2019 we did not expect important differences when estimating bp at regional scales we focused on flammap since it is commonly used in research and for operational purposes parisien et al 2019 nevertheless a detailed analysis comparing different algorithms simulators will prove worthy to highlight potential advantages and drawbacks overall random forest showed more uniformly distributed results than flammap which concentrated most of its values close to zero except for some locations with medium and high probabilities in the north and centre of apulia spatially restricted results in flammap suggest that since ignition points were randomly distributed across the whole study area topographical and fuel conditions of those areas make fires regardless of where they initiate more likely to go through since results rely on spread algorithm it suggests that management actions should be oriented to reduce spread e g fuel reductions or barriers to break fires ager et al 2010 this interpretation is crucial since it should differ from machine learning models focused on factors associated with fire occurrence indeed fire simulator outputs are commonly used to assess the effects of fuel treatments on fire spread and behaviour salis et al 2016 although a greater number of ignition points and longer fire duration can be set up for simulations minor effects on main spatial patterns are expected since ignitions are distributed randomly a large enough number of ignition points ensures a good representation of bp avoiding time consuming computations indeed restricted areas for high bp values are commonly observed across different studies regardless of their scale salis et al 2015 jahdi et al 2020 proving beneficial to identify the most critical locations in terms of fire spread furthermore these results highlight two main patterns in the study area constrained and not constrained spread locations i e low and high bp respectively which are of primary importance for management however it is essential to notice that in other regions high bp values might be widespread because of land conformation e g slope and fuel loads regarding machine learning models random forest yields more smoothed results highlighting inland and mountain areas in the northeast results agreed with previous studies using other algorithms at larger scales e g artificial neural networks elia et al 2020 contrasting with fire spread simulators random forest should be interpreted as isolated pixel information since pixel probability does not depend on their surroundings thus managers should carefully review the area e g a pixel and their surroundings to assess wildfire probability better furthermore since machine learning methods rely on statistical relationships between explanatory variables and wildfires forest managers should also analyse the values of the variables within specific areas and their importance in the model e g regression coefficients to detect the primary driver of bp and thus better select specific prevention actions despite being less intuitive the approach could provide complementary information to spread models helping to address wildfire issues better e g detecting socioeconomic cultural drivers the bp output from flammap and random forest change across fuel types and environmental conditions in the study area across fuel types flammap bp was higher under herbaceous maquis and low maquis fuel types suggesting that in the study area these locations are much more likely to burn during wildfires and their spread is potentially higher than in other fuel types however other studies observed greater bp over different vegetation types jahdi et al 2020 indicating that landscape conformation i e topography and fuel distribution might drive this observed pattern furthermore those vegetation types were not the most abundant in the study area reinforcing the idea that landscape conformation is the main driver indeed the slope seemed to be associated with bp in flammap with the greater the slope greater the bp although the slope is an important driver of the rate of spread andrews 2018 other studies did not observe a strong association with bp when using fs simulators ye et al 2017 suggesting that the results are highly dependent on the different landscape components e g the spatial disbrution of cultivated areas this confirms that extrapolating results should be made with caution indeed previous studies showed how the relative importance of different factors might change across landscapes miller et al 2010 surprisingly flammap bp did not increase with summer mean fwi ap and aspect the latter expected higher in southern slopes because of greater solar radiation and thus greater fuel dryness it suggests that in the study area main fire pathways are not associated to climatological fire prone areas most likely the scale of the analysis is driving this result since other studies showed that the importance of weather conditions increases with the extent scale of the analysis liu et al 2013 it is important to notice that the spatial variability of live fuel dryness is not considered in flammap so its potential effect on making some areas more likely to be crossed by wildfires because regularly become dryer than others is not accounted however if substantial differences in dryness exist across locations differences in vegetation fuel types will be also expected which might affect in a greater degree fire spread and bp random forest also projected higher probabilities with increasing the slope and on fuel types such as herbaceous maquis and low maquis the higher bp over those conditions as suggested by flammap projections might be associated with land conformation which converts these conditions as main fire pathways interestingly random forest predicted higher bp over conifers than broadleaf forests whereas flammap did not detect this pattern it suggests that despite fires might occur more frequently in conifers forest they are not located in the main fire pathways this is an important issue for wildfire risk management in the region since it conditions the type of fire fighting actions appropriated to each location e g speeding up suppression efforts or reducing ignition factors in high susceptible areas versus constraining spread in main fire pathways however in contrast with flammap random forest probabilities increase with dryness and meteorological fire risk in terms of southern slopes and greater fwi suggesting that drivers of fire occurrence in the study area differ from those of spread this is an important consideration since it indicates that fires might occur over those locations not because factors are favouring spread but because of potential occurrence causes e g close to ignition location or fuel dryness thus fire managers should take care when interpreting results since management actions should adapt accordingly to the method used surprisingly distance to roads had a minor role in predicting bp when using random forest some previous studies found a significant influence of this variable but were focused on ignition factors rather than on the susceptibility of the territory ganteaume et al 2013 elia et al 2020b indeed studies at larger scales focused on fire occurrence did not observed such importance de bem et al 2019 elia et al 2020 suggesting that the drivers of these two features might differ substantially applying an ignition density grid to account for the historical distribution of fires proved to be useful mainly under the random forest approach for instance it helped to mask not burnable areas highlighted because most likely one or more factors related to fire occurrence were present in those locations e g high fwi or low distance to roads this issue does not happen with flammap since the spread is impossible because of the absence of fuel i e vegetation integrating an ignition density grid by default when using fire simulators is a common practice alcasena et al 2015 salis et al 2015 lozano et al 2017 jahdi et al 2020 however it might hide valuable information in landscape management contrasting results with and without integrating historical patterns might reveal potential spread areas that did not burn in the past this information might serve both 1 to not forget those locations when designing prevention risk campaigns and 2 to analyse which factors e g traditional practices make those areas not fire prone the present study shows how results might differ among methods used to map fire probability also referred in the literature as fire susceptibility or likelihood and how interpretations should change accordingly however paying attention to this both approaches should provide complementary information for fire agencies for instance bp from fire simulators can be more used explicitly for planning fuel management strategies salis et al 2016 and to assess assets exposure to wildfires alcasena et al 2015 whereas bp from random forest can be exploited to detect main human factors driving fire occurrence and thus orient socio cultural prevention actions rodrigues and de la riva 2014 indeed as in other scientific disciplines a multi model framework multi approach multi weather scenarios might be helpful to assess better uncertainties associated with fire risk declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25422,models and data play an important role in informing decision making in environmental systems providing different and complementary information multiple frameworks have been developed to address model limitations and there is a large body of research focused on improving the quality of data however when models and data disagree the focus is usually on fixing the model rather than the data in this study we introduce the framework talks trigger articulate list knowledge elicitation solve as a way of resolving model data discrepancies the framework emphasises that a mismatch between data and model outputs could be due to issues in the model the data or both through three case studies we exemplify how models can be used to identify and improve issues with the data and hence make the most out of models and data the framework can be applied more broadly to better integrate models and data in environmental decision making graphical abstract image 1 keywords environmental modelling model assessment model improvement interdisciplinary research data availability the data analysis and figures for the case studies were generated using python 3 10 with the numpy 1 21 5 pandas 1 4 1 and matplotlib 3 5 1 modules the code and data sets for case studies 1 2 and 3 can be found on github https github com fegger talks a systematic framework for resolving model data discrepancies the repository also includes an environment file talks python envrionment yaml to reproduce the authors python environment for conda based python distributions the repository was created by felix egger e mail f egger uq edu au in 2022 and includes visualisation scripts python script for case study 1 and 2 jupyter notebook for case study 3 and the python environment file 58 kb and the data visualised in this paper 2 7 kb authors experimental environment was as follows os windows 10 enterprise cpu 11th gen intel r core tm i5 1145g7 2 60ghz 1 50 ghz ram 16 0 gb the data for each case study was published and used with permission from the following sources case study 1 vilas et al 2022 https doi org 10 1016 j scitotenv 2021 150019 software used to generate the data agricultural production systems simulator apsim apsim initiative 1994 https doi org 10 1016 j envsoft 2014 07 009 case study 2 stewart and jeffrey 2021 https doi org 10 36334 modsim 2021 j13 stewart software used to generate the data source v3 5 0 ewater ltd 2013 https wiki ewater org au display sd37 source user guide case study 3 provided by jonathan ferrer mestres software used to generate the data tensorflow for python 3 8 https doi org 10 5281 zenodo 4724125 1 introduction models are developed to guide decision makers and managers by supporting sound scientific understanding of environmental systems aumann 2011 mcintosh et al 2011 pohjola et al 2013 schuwirth et al 2019 environmental systems are inherently complex and variable in space and time and models must be rigorously evaluated to ensure the relevance accuracy and precision of their outputs jakeman et al 2006 power 1993 several frameworks have been proposed to guide the development of scientifically sound models ascough et al 2008 bennett et al 2013 hipsey et al 2020 jakeman et al 2006 pohjola et al 2013 refsgaard et al 2007 walker et al 2010 these frameworks typically target one or more of the following areas 1 quality assurance quality control qa qc methods 2 uncertainty evaluation 3 technical assessment of models 4 evaluation of the effectiveness of models beyond the immediate research community and or 5 other perspectives such as evaluation of communication and participatory modelling pohjola et al 2013 in general frameworks for model development and assessment have the objective of ensuring that the model accurately describes the dynamics of a real system and or is useful in decision making pohjola et al 2013 frameworks for model development and assessment bennett et al 2013 hipsey et al 2020 jakeman et al 2006 usually present different perspectives regarding the model development and assessment process but have some commonalities models should be developed with and evaluated against independent datasets see guo et al 2020 hipsey et al 2020 zheng et al 2022 for examples and be continuously refined by exposing them to new datasets gibbs et al 2018 keating 2020 model assessment should ideally use data obtained at different levels e g process and system robson et al 2020 and draw on expert understanding holzworth et al 2011 keating 2020 thus data are key to develop and assess models the level of reliance of a model on data varies depending on the type of model used mount et al 2016 while data driven models e g machine learning are strongly structured around data theory based models e g process based are more structured around mechanistic hypotheses or theory jakeman et al 2006 thornley 1976 but always at some level incorporate empirical formulations e g photosynthesis irradiance curves that account for the effect of light on primary production and therefore require data for rigorous calibration and performance assessment mount et al 2016 robson 2014 the level of abstraction at which processes are represented varies practical models for applied science of macroscopic quantities do not for example simulate the sub atomic processes underlying physical or chemical reactions in the absence of data theory based models are still therefore hypotheses that need to be tested silberstein 2006 whether environmental models are developed using data driven or mechanistic approaches models need to be widely evaluated against data to be credible aumann 2007 2011 hamilton et al 2022 harper et al 2021 humphrey et al 2017 modellers of environmental systems thus have a multitude of frameworks that can aid them in developing credible models however most frameworks rely on the assumption that the data used for model development or assessment precisely and accurately represents the system in question offering an objective standard for testing model performance hipsey et al 2020 however just as mathematical models are only ever simplified representations of a real system data are only a snapshot of the system in time and space more colloquially just as all models are wrong but some are useful box 1979 we argue analogously that all data are incomplete but most are insightful environmental data are rarely collected on the spatial and temporal scales that match system variability oreskes et al 1994 furthermore data like models are subject to errors from various sources e g sampling procedures laboratory analyses and or data processing see batley 1999 fundamentally measurement requires the existence of a data model a conceptual understanding of the relationship between what is actually measured e g photon flux intensities at several different wavelengths and what that measurement is supposed to represent e g photosynthetically active radiation as discussed in gitelman 2013 data are anything but raw and should be thought of as a cultural resource that needs to be generated protected and interpreted hence objectively resolving the cause of discrepancies between measured data and model predictions means recognising that these differences could arise from issues with the model the data and or the underlying data model discrepancies between predictions and observations typically arise from model limitations for example there are reports of models capturing seasonal trends in state variables but failing to represent episodic events elliott et al 2000 such discrepancies are an expected part of the process of model refinement and development the typical response is either to improve the model conceptualisation structure and parameters or to use the model data comparison to qualify uncertainty or to better constrain the range of applications for which the model is considered fit for purpose hence a critical step in resolving such discrepancies to clarify the purpose of the model and to answer questions such as jakeman et al 2006 what is the purpose of the model is it to represent the mean behaviour the episodic event or both model evaluation frameworks e g bennett et al 2013 hipsey et al 2020 jakeman et al 2006 pohjola et al 2013 address limitations in models however there is a lack of specific guidance in the literature on how model predictions can offer insights into the quality of data ideally the evaluation of a model should be conducted to understand the information contained in both model and data gupta et al 2008 the data can reveal areas of improvement for the model conversely the model can draw attention to inadequacies in the data which can be addressed through further investigation the presence of limitations in the data can significantly impact the model development process leading to inaccurate depictions of the system s structure and function as well as wasted time and resources in refining a model using erroneous data fig 1 currently data quality is primarily assessed through expert judgement or anomaly detection algorithms foorthuis 2021 leigh et al 2019 that rely solely on the data however problems in data can escape the scrutiny of either experts or algorithms in this study we demonstrate the value of models in identifying data issues as a way of balancing out the two possibilities that either models or data could be responsible for model data discrepancies thus avoiding the trap of implicitly assuming that the model is wrong and the data are right we introduce the framework talks trigger articulate list knowledge solve to help modellers resolve model data discrepancies conversely to other frameworks the talks framework applies the same level of critical analysis on both models and data and highlights the importance of expert knowledge elicitation in resolving model data discrepancies because modelling frameworks tend to focus on improving models based on data and beyond e g hipsey et al 2020 we present three case studies that demonstrate how models can be used to improve the insights that can be gained from data the case studies cover the following areas 1 field modelling 2 catchment modelling and 3 conservation science 2 overview of the talks framework the talks framework that we introduce in the present work summarised in fig 2 and expanded with examples in table 1 consists of five steps 1 trigger the first step and entry point to the framework is to identify that a discrepancy exists the trigger could take many forms for example in the case of theory based models where parameters have a physical interpretation a trigger could be that satisfactory model performance can only be achieved using unrealistic parameters or an unlikely combination of parameters robson et al 2018 whether parameters have a physical interpretation or not it could be that very different parameters are needed to achieve an adequate model data fit in the presence of new data 2 articulate discrepancies the second step is to articulate how the model and the data disagree the objective of this step is to characterise the discrepancy thus a clear first step is to visualise the discrepancy and estimate the overall model performance further analyses can give insight into the next step list potential causes of the discrepancies for example model structural issues and issues with model parameterisation inputs and initial conditions can be detected using techniques such as plotting the residuals and looking at cross correlations between model residuals and inputs components of the system 3 list potential causes the third step is to consider the potential causes for the discrepancy it is important to distinguish between discrepancies arising from potential problems in models from potential problems in data 4 knowledge elicitation the fourth step is to consult with experts and critically evaluate the potential causes of the discrepancies this step involves tapping into on ground knowledge and consulting with data providers and or experts in the measurement technology in use e g sensor developers as well as applying available expertise within the team this is an iterative step process in which expertise is applied and consulted until the causes of the discrepancy are identified 5 solve the last step is to develop strategies to resolve the discrepancy this could involve updating the model structure applying different models or updating discarding data table 1 3 application of the talks framework to three case studies in this section we apply the talks framework to three different case studies to demonstrate how model data discrepancies can be used to improve data quality in some cases these case studies are intended to be illustrative rather than exhaustive 3 1 case study 1 field model identifies errors when importing data in the first case study a discrepancy between the data and the predictions of a theory based field model led to the identification of a runoff event missing from the dataset this discrepancy was identified when validating a new sub routine in the field model to predict dissolved inorganic nitrogen din in runoff losses from sugarcane land uses the data to validate the new algorithm was collected at the field scale in the mackay whitsunday region queensland australia vilas et al 2022 runoff volume was measured using runoff flumes with water height through the flume measured using a pressure transducer din in runoff from an event was calculated from composite water samples collected during each event the field experiment consisted of two treatments with more than three times as much nitrogen fertiliser applied per hectare in the high nitrogen treatment compared to the low nitrogen treatment table 2 cumulative runoff and cumulative din losses in runoff between the start of the experiment and the time of plant harvest were reported the field experiment was simulated using the agricultural production systems simulator apsim version 7 10 holzworth et al 2014 a full description of the field experiment and model set up is described in vilas et al 2022 the trigger step 1 was a difference between the model and data in din in runoff losses specifically step 2 articulate the discrepancies the model predicted higher din loads in runoff for the high nitrogen treatment than the low nitrogen treatment but similar din loads were reported in runoff from the different treatments table 2 because the goal of the modelling was to evaluate the effect of farm management e g fertiliser application on din in runoff losses from sugarcane fields it was determined that the observed model data mismatch had consequences for the modelling goal the following model causes of the discrepancy were considered step 3a list model causes 1 errors in the initial conditions and 2 errors in the model structure regarding the initial conditions antecedent soil conditions e g soil water nitrogen in the soil were assumed to be the same for both treatments however the low nitrogen treatment could have had more nitrogen in the soil at the start of the experiment leading to the unexpected system behaviour in which both treatments had similar din losses regarding model structure nitrogen in the high nitrogen treatment could have been lost through a loss pathway that was not included in the model structure e g ammonia volatilisation leading to nitrogen being lost to the atmosphere we also identified issues with the data that could have led to the discrepancies step 3b list data causes 1 errors when importing the data 2 errors in data collection measurement transport storage and analyses and 3 data processing and handling errors when checking the data e g step 2 in bennett et al 2013 we noticed that din in runoff measurements in the high and low nitrogen treatments had different start dates the period of records provided was not the same for both treatments with the high nitrogen treatment started to be recorded later than the low nitrogen treatment which could have yielded an underestimation of total din in runoff losses in the high nitrogen treatment errors in the pressure sensors or in the din laboratory analyses could also have led to an underestimation of total runoff in the high nitrogen fertiliser treatment regarding data processing and handling errors inaccuracies in the calculation of din in runoff loads from din concentrations and runoff volumes ferrant et al 2013 could have led to the observed discrepancies to narrow down the causes of the discrepancy available expertise was applied step 4 knowledge elicitation consultation with the monitoring team confirmed that the soil in which the experiment was conducted was homogenised before the start of the experiment thus validating the assumption of similar initial conditions across both treatments any model structural errors were also discarded after discussion with monitoring teams however while analysing the data the monitoring teams identified that there was one runoff event missing in the high nitrogen treatment this error occurred during the data handling process the final step was to resolve the discrepancy step 5 solve in this case study the monitoring team was able to locate the missing runoff event when the missing runoff event was included in the analysis the model predictions and observations of din in runoff finally showed sensible agreement table 2 3 2 case study 2 hydrology model reveals changes in model input data in this case study a hydrology model of the tully river located within the wet tropics region queensland australia was calibrated against flow data stewart and jeffrey 2021 the wet tropics basin hydrology was modelled using the ewater source ewater ltd 2013 catchments modelling framework mccloskey et al 2021a 2021b the model is driven by long term climate data sourced from the silo climate data archive silo 2020 the hydrology was calibrated against measured flow data using an independent parameter estimation tool pest doherty et al 2005 for the calibration daily and monthly flow timeseries as well as the frequency of measured daily flows were used moriasi et al 2007 a full description of the model set up and calibration can be found in mccloskey et al 2021a 2021b the trigger step 1 was a change in the direction of the deviation between model and data fig 3 specifically step 2 articulate discrepancies the modelled annual flow was consistently higher than the measured annual flow before year 2000 2001 july 2000 to june 2001 with an average discrepancy of 7 however after 2000 2001 this pattern reversed so that annual mean flow was consistently lower than the measured annual flow average discrepancy of 16 because the model aimed at predicting flows and the discrepancy meant that the flow would have been largely underpredicted post 2000 2001 it was established that this discrepancy was consequential to the modelling goal the sudden change in the pattern and quality of the model data fit post 2000 2001 most likely suggests a problem with either the measured discharge or the input data to the model step 3b list data causes rather than with the model although this could also occur in the case of a sudden change in conditions not reflected in the model structure step 3a list model causes because the model fit changed after a specific point in time it was identified that changes in the methods and spatial temporal frequency used to measure data could have led to the discrepancy specifically the methods to estimate flow could have changed after 2000 2001 leading to an overestimation of the measured flow in addition model inputs e g rainfall could have changed post 2000 2001 leading to an underestimation of the modelled flow available expertise was applied to narrow down the causes of the discrepancy step 4 knowledge elicitation concentrating attention on either 1 possible changes in measurement methods after the year 2000 or 2 changes to model inputs e g rainfall after the year 2000 2001 after consultation with monitoring teams it was confirmed that there had been no changes to the way flow data were collected and analysed after the year 2000 2001 in addition it was confirmed that there had been no changes to the discharge curve used to estimate flow from measured water level next the inputs to the model were investigated the model was driven with gridded rainfall data obtained from the silo database silo gridded rainfall data are produced by interpolating between point rainfall measurements after discussion with silo teams it was found that the number of rainfall stations used in the model changed from 2000 2001 specifically a rainfall station in the upper tully catchment was added to the rainfall interpolation in 2001 two methods were employed to establish the impact of the station addition on the silo gridded rainfall data specific details of these methods are provided in stewart and jeffrey 2021 both methods showed a substantial difference between the time periods before and after the station addition in the estimated rainfall in the grids that were adjacent to the added station results of the analysis showed that post 2000 2001 the frequency of low volume rainfall events increased and the frequency of larger runoff producing rainfalls decreased in addition analysis of the trend of cumulative estimated rainfall pre and post rainfall station addition showed a reduction in annual average rainfall it was therefore concluded that the addition of the station in the upper tully impacted the long term characteristics of the interpolated rainfall data which in turn reduced the model estimates of overall flow post 2000 2001 the final step was to resolve this discrepancy step 5 solve in this case study the rainfall measurement density was increased by adding data from additional rainfall stations into the silo interpolation this change led to an improvement of the representation of tully river modelled flow by reducing the pre to post 2000 2001 flow discrepancy by 31 percent 3 3 case study 3 a machine learning model highlights issues with data export in this case study a machine learning model was used to predict the cost of management actions as a function of management area to assist in the prioritisation of actions to preserve the maximum number of species classified as endangered by the commonwealth conservation status nsw office of environment and heritage https www environment gov au the study site was the whole of new south wales australia the model was developed using historical data two datasets were used 1 a training dataset consisting of multiple features such as size of management area ha previously applied management actions e g fire grazing weed control flora monitoring and management cost aud the value to predict and 2 an application dataset consisting of the same features excluding management cost the data was obtained from multiple sources e g https www data gov au bionet atlas https www environment gov au the training dataset was split into training 75 and test 25 data the model was then used on an application dataset to predict the cost of the management actions the trigger step 1 was a discrepancy between the expected cost and the cost predicted by the machine learning model specifically the predicted cost of some management areas was substantially lower than the cost of management areas with similar characteristics which caught the attention of the modellers step 2 articulate discrepancies because accurately predicting the management cost is important for optimal allocation of resources it was established that this discrepancy was consequential to the modelling goal the discrepancy between the predicted and expected cost could indicate issues with the model see step 3a list model causes the following issues with the model conceptualisation structure and parameters were considered 1 the chosen machine learning algorithm catboost gradient boosting https catboost ai was not fit for purpose 2 the features used to train the model were not informative for predicting the management cost see galelli et al 2014 and 3 the range of hyperparameters used to train the machine learning model e g learning rate size of the training dataset training time was limited the discrepancy between the predicted and expected costs could also indicate issues with the data step 3b list data causes a potential problem arising from the data was also considered specifically the possibility that there may have been an error when exporting the data used for training and or application from the database to narrow down the causes of the discrepancy available expertise was applied step 4 knowledge elicitation feature selection analysis methodically selecting and combining features that significantly influence the variable being predicted indicated that modifying input features did not significantly improve the accuracy of the model in addition the use of other machine learning algorithms to build the model e g artificial neural networks random forest did not significantly improve the accuracy of the model suggesting that the issue was not in the selected algorithm catboost furthermore analysis of the hyperparameters used to train the machine learning model showed high model accuracy regardless of the algorithm used to check for errors in the data the features of the 4 management areas with substantially lower cost than expected were compared with another dataset spatial maps obtained from https www environment gov au when comparing the areas it was found that the areas did not match in 49 sites with 12 sites having an absolute proportional difference greater than 100 fig 4 after consultation with the data providers it was found that an error occurred when exporting the training dataset from a database the final step was to resolve this discrepancy step 5 solve in this case study the data providers re extracted the training dataset from the database and ensured that no errors occurred in the data extraction process when the re extracted dataset was used to train the model the model prediction on the application dataset matched the expectation and the issue was resolved 4 discussion effective integration of information from models and data is the holy grail of information quality for decision makers and those seeking to understand environmental systems however discrepancies between model predictions and measured data can make this integration challenging in this study we introduced the talks framework to deal with model data discrepancies we applied the framework to three case studies which show that even basic data issues such as errors during data import export can be consequential to the modelling goal therefore to address model data discrepancies it is crucial to consider the possibility of basic issues in both data and models additionally our study highlights a critical challenge in hydrological modelling namely issues with rainfall density which has been well documented in the literature jeffrey et al 2001 stewart and jeffrey 2021 while we provide an extensive set of examples for each step of the talks framework we acknowledge that inexperienced modellers may require additional guidance to effectively implement these examples the talks framework complements existing frameworks for model development and evaluation as it provides clear guidelines on how to resolve model data discrepancies which can arise during model calibration and technical assessment e g step 3 in bennett et al 2013 steps 7 10 in jakeman et al 2006 and steps 4 5 in refsgaard et al 2007 furthermore the talks framework expands on existing frameworks because it encourages the same level of evaluation to both models and data the possibility that some aspect of the data may be invalid is usually not the first avenue of interrogation when models are assessed hipsey et al 2020 however as the three case studies presented in this paper demonstrate the assumption of data validity should definitely be evaluated alongside the assumption of model validity if efficient resolution of model data discrepancies is desired the talks framework consists of four steps step 1 of the framework is to identify that a discrepancy exists trigger step 2 of the framework articulate discrepancies is standard modelling practice and can be achieved using a variety of methods such as visualising model performance or calculating performance metrics bennett et al 2013 if the discrepancies were found to be due to model causes step 3a list model causes the modeller applies their expertise and acts to resolve the discrepancies by following a model assessment framework such as concept state process system hipsey et al 2020 robson et al 2020 incorporating uncertainty ascough et al 2008 or highlighting the limitations of the model guo et al 2019 jones et al 2020 these processes are thus already well established as good practice in the field of environmental modelling however if model data discrepancies are due to data issues step 3b list data causes the expertise of data experts should be consulted step 4 knowledge elicitation and both the modelling and data teams should aim to resolve the issue together step 5 solve unfortunately often there is no direct contact between people who collect data and people who model it silberstein 2006 there are many reasons why this can happen for example data may be collected before any modelling endeavour or modellers may not have an existing relationship with data providers which makes the collaboration difficult in addition it is not common for modellers to work with people who collect data in designing data collection programs and modellers are often not equipped to identify data quality issues sambasivan et al 2021 due to a lack of practical or on ground knowledge of the environmental system being modelled keating 2020 the sensor technology in use or the measurement protocols employed thus model data integration requires more than automated tools to facilitate the use of data in models e g chen et al 2022 but also investment in the relationships between the different actors who contribute to the modelling process while modellers clearly play a pivotal role in the process of model development and assessment here we highlight that different disciplines may need to be integrated in the process of resolving model data discrepancies better model data integration can be attained through collaboration between modellers and monitoring teams gupta et al 2008 effective collaboration across the disciplinary divide is often not trivial and it takes time skill and goodwill to develop the relationships required for designing data collection to best inform models flynn 2005 querying monitoring teams about the quality of their data is even more fraught monitoring teams have high quality control standards for their specific data use purposes and often collect data without any intention that they be used for modelling furthermore monitoring teams may not be aware of the type of data needed to obtain reliable estimates of parameters or model outputs guillaume et al 2019 hence any conversation between modellers and monitoring teams need to start early in the modelling process hamilton et al 2019 and involve goodwill on both sides as well as great tact and emotional intelligence on the side of those initiating the conversation it could be argued therefore that the greatest opportunity for improving model data integration may not be new software or systems but simply investing in the relationships between those who model and those who measure developing strong collaborative relationships between modellers and monitoring teams requires dedication and effort modellers need to invest time to familiarise monitoring teams with the model as a prior understanding of the model can enhance collaboration between the two teams kotir et al under review furthermore modellers need to venture out into the field to gain a better understanding of the monitoring data and the system they are attempting to model by doing so modellers can gain firsthand experience and insights that can inform their modelling efforts and ultimately lead to more accurate models for an example see o brien et al 2022 which includes a video animation on how to improve relationships between modellers and monitoring teams despite evidence that trust in models is linked to trust in the modeller harper et al 2021 many modelling projects fail to recognize the importance of interpersonal relationships to improve the quality of science produced by modelling projects it is essential to allocate funding towards facilitating interactions between modellers and monitoring teams this can create opportunities for modellers to reflect on their modelling practices share their conceptual understanding with data providers and collaborate on sense checking this understanding espig et al 2020 goodwill and connections built up in this way will make it easier to have constructive dialogue that integrates the expertise of both modellers and data providers in resolving model data discrepancies 5 conclusion differences between model predictions and system observations are often seen as a barrier to model and data integration but here we show that an objective assessment of model data discrepancies can be valuable for enhancing the quality of information available from both models and data we recognize that despite being wrong models can be useful in better understanding and contextualising data to better manage environmental systems we need good quality information which can only be obtained when we acknowledge that neither model nor data are right but can be more useful when used together achieving this goal will require respectful and constructive relationships between those who build and use models and those who provide the data which feeds them software agricultural production systems simulator apsim apsim initiative 1994 holzworth et al 2014 https doi org 10 1016 j envsoft 2014 07 009 source v3 5 0 ewater ltd 2013 https wiki ewater org au display sd37 source user guide operating systems windows declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work received funding from the queensland government through the queensland water modelling network rd i project a framework to marry models and data tessa chamberlain stephen donaldson puwasala gamakumara ken rohde mel shaw and cassady swinbourne are thanked for helpful discussions during manuscript development stephen donaldson tony howes and ken rohde are thanked for reviewing the manuscript hayley langsdorf is acknowledged for developing the drawn visual material mpa s contribution was funded by an arc discovery early career researcher award de200100683 
25422,models and data play an important role in informing decision making in environmental systems providing different and complementary information multiple frameworks have been developed to address model limitations and there is a large body of research focused on improving the quality of data however when models and data disagree the focus is usually on fixing the model rather than the data in this study we introduce the framework talks trigger articulate list knowledge elicitation solve as a way of resolving model data discrepancies the framework emphasises that a mismatch between data and model outputs could be due to issues in the model the data or both through three case studies we exemplify how models can be used to identify and improve issues with the data and hence make the most out of models and data the framework can be applied more broadly to better integrate models and data in environmental decision making graphical abstract image 1 keywords environmental modelling model assessment model improvement interdisciplinary research data availability the data analysis and figures for the case studies were generated using python 3 10 with the numpy 1 21 5 pandas 1 4 1 and matplotlib 3 5 1 modules the code and data sets for case studies 1 2 and 3 can be found on github https github com fegger talks a systematic framework for resolving model data discrepancies the repository also includes an environment file talks python envrionment yaml to reproduce the authors python environment for conda based python distributions the repository was created by felix egger e mail f egger uq edu au in 2022 and includes visualisation scripts python script for case study 1 and 2 jupyter notebook for case study 3 and the python environment file 58 kb and the data visualised in this paper 2 7 kb authors experimental environment was as follows os windows 10 enterprise cpu 11th gen intel r core tm i5 1145g7 2 60ghz 1 50 ghz ram 16 0 gb the data for each case study was published and used with permission from the following sources case study 1 vilas et al 2022 https doi org 10 1016 j scitotenv 2021 150019 software used to generate the data agricultural production systems simulator apsim apsim initiative 1994 https doi org 10 1016 j envsoft 2014 07 009 case study 2 stewart and jeffrey 2021 https doi org 10 36334 modsim 2021 j13 stewart software used to generate the data source v3 5 0 ewater ltd 2013 https wiki ewater org au display sd37 source user guide case study 3 provided by jonathan ferrer mestres software used to generate the data tensorflow for python 3 8 https doi org 10 5281 zenodo 4724125 1 introduction models are developed to guide decision makers and managers by supporting sound scientific understanding of environmental systems aumann 2011 mcintosh et al 2011 pohjola et al 2013 schuwirth et al 2019 environmental systems are inherently complex and variable in space and time and models must be rigorously evaluated to ensure the relevance accuracy and precision of their outputs jakeman et al 2006 power 1993 several frameworks have been proposed to guide the development of scientifically sound models ascough et al 2008 bennett et al 2013 hipsey et al 2020 jakeman et al 2006 pohjola et al 2013 refsgaard et al 2007 walker et al 2010 these frameworks typically target one or more of the following areas 1 quality assurance quality control qa qc methods 2 uncertainty evaluation 3 technical assessment of models 4 evaluation of the effectiveness of models beyond the immediate research community and or 5 other perspectives such as evaluation of communication and participatory modelling pohjola et al 2013 in general frameworks for model development and assessment have the objective of ensuring that the model accurately describes the dynamics of a real system and or is useful in decision making pohjola et al 2013 frameworks for model development and assessment bennett et al 2013 hipsey et al 2020 jakeman et al 2006 usually present different perspectives regarding the model development and assessment process but have some commonalities models should be developed with and evaluated against independent datasets see guo et al 2020 hipsey et al 2020 zheng et al 2022 for examples and be continuously refined by exposing them to new datasets gibbs et al 2018 keating 2020 model assessment should ideally use data obtained at different levels e g process and system robson et al 2020 and draw on expert understanding holzworth et al 2011 keating 2020 thus data are key to develop and assess models the level of reliance of a model on data varies depending on the type of model used mount et al 2016 while data driven models e g machine learning are strongly structured around data theory based models e g process based are more structured around mechanistic hypotheses or theory jakeman et al 2006 thornley 1976 but always at some level incorporate empirical formulations e g photosynthesis irradiance curves that account for the effect of light on primary production and therefore require data for rigorous calibration and performance assessment mount et al 2016 robson 2014 the level of abstraction at which processes are represented varies practical models for applied science of macroscopic quantities do not for example simulate the sub atomic processes underlying physical or chemical reactions in the absence of data theory based models are still therefore hypotheses that need to be tested silberstein 2006 whether environmental models are developed using data driven or mechanistic approaches models need to be widely evaluated against data to be credible aumann 2007 2011 hamilton et al 2022 harper et al 2021 humphrey et al 2017 modellers of environmental systems thus have a multitude of frameworks that can aid them in developing credible models however most frameworks rely on the assumption that the data used for model development or assessment precisely and accurately represents the system in question offering an objective standard for testing model performance hipsey et al 2020 however just as mathematical models are only ever simplified representations of a real system data are only a snapshot of the system in time and space more colloquially just as all models are wrong but some are useful box 1979 we argue analogously that all data are incomplete but most are insightful environmental data are rarely collected on the spatial and temporal scales that match system variability oreskes et al 1994 furthermore data like models are subject to errors from various sources e g sampling procedures laboratory analyses and or data processing see batley 1999 fundamentally measurement requires the existence of a data model a conceptual understanding of the relationship between what is actually measured e g photon flux intensities at several different wavelengths and what that measurement is supposed to represent e g photosynthetically active radiation as discussed in gitelman 2013 data are anything but raw and should be thought of as a cultural resource that needs to be generated protected and interpreted hence objectively resolving the cause of discrepancies between measured data and model predictions means recognising that these differences could arise from issues with the model the data and or the underlying data model discrepancies between predictions and observations typically arise from model limitations for example there are reports of models capturing seasonal trends in state variables but failing to represent episodic events elliott et al 2000 such discrepancies are an expected part of the process of model refinement and development the typical response is either to improve the model conceptualisation structure and parameters or to use the model data comparison to qualify uncertainty or to better constrain the range of applications for which the model is considered fit for purpose hence a critical step in resolving such discrepancies to clarify the purpose of the model and to answer questions such as jakeman et al 2006 what is the purpose of the model is it to represent the mean behaviour the episodic event or both model evaluation frameworks e g bennett et al 2013 hipsey et al 2020 jakeman et al 2006 pohjola et al 2013 address limitations in models however there is a lack of specific guidance in the literature on how model predictions can offer insights into the quality of data ideally the evaluation of a model should be conducted to understand the information contained in both model and data gupta et al 2008 the data can reveal areas of improvement for the model conversely the model can draw attention to inadequacies in the data which can be addressed through further investigation the presence of limitations in the data can significantly impact the model development process leading to inaccurate depictions of the system s structure and function as well as wasted time and resources in refining a model using erroneous data fig 1 currently data quality is primarily assessed through expert judgement or anomaly detection algorithms foorthuis 2021 leigh et al 2019 that rely solely on the data however problems in data can escape the scrutiny of either experts or algorithms in this study we demonstrate the value of models in identifying data issues as a way of balancing out the two possibilities that either models or data could be responsible for model data discrepancies thus avoiding the trap of implicitly assuming that the model is wrong and the data are right we introduce the framework talks trigger articulate list knowledge solve to help modellers resolve model data discrepancies conversely to other frameworks the talks framework applies the same level of critical analysis on both models and data and highlights the importance of expert knowledge elicitation in resolving model data discrepancies because modelling frameworks tend to focus on improving models based on data and beyond e g hipsey et al 2020 we present three case studies that demonstrate how models can be used to improve the insights that can be gained from data the case studies cover the following areas 1 field modelling 2 catchment modelling and 3 conservation science 2 overview of the talks framework the talks framework that we introduce in the present work summarised in fig 2 and expanded with examples in table 1 consists of five steps 1 trigger the first step and entry point to the framework is to identify that a discrepancy exists the trigger could take many forms for example in the case of theory based models where parameters have a physical interpretation a trigger could be that satisfactory model performance can only be achieved using unrealistic parameters or an unlikely combination of parameters robson et al 2018 whether parameters have a physical interpretation or not it could be that very different parameters are needed to achieve an adequate model data fit in the presence of new data 2 articulate discrepancies the second step is to articulate how the model and the data disagree the objective of this step is to characterise the discrepancy thus a clear first step is to visualise the discrepancy and estimate the overall model performance further analyses can give insight into the next step list potential causes of the discrepancies for example model structural issues and issues with model parameterisation inputs and initial conditions can be detected using techniques such as plotting the residuals and looking at cross correlations between model residuals and inputs components of the system 3 list potential causes the third step is to consider the potential causes for the discrepancy it is important to distinguish between discrepancies arising from potential problems in models from potential problems in data 4 knowledge elicitation the fourth step is to consult with experts and critically evaluate the potential causes of the discrepancies this step involves tapping into on ground knowledge and consulting with data providers and or experts in the measurement technology in use e g sensor developers as well as applying available expertise within the team this is an iterative step process in which expertise is applied and consulted until the causes of the discrepancy are identified 5 solve the last step is to develop strategies to resolve the discrepancy this could involve updating the model structure applying different models or updating discarding data table 1 3 application of the talks framework to three case studies in this section we apply the talks framework to three different case studies to demonstrate how model data discrepancies can be used to improve data quality in some cases these case studies are intended to be illustrative rather than exhaustive 3 1 case study 1 field model identifies errors when importing data in the first case study a discrepancy between the data and the predictions of a theory based field model led to the identification of a runoff event missing from the dataset this discrepancy was identified when validating a new sub routine in the field model to predict dissolved inorganic nitrogen din in runoff losses from sugarcane land uses the data to validate the new algorithm was collected at the field scale in the mackay whitsunday region queensland australia vilas et al 2022 runoff volume was measured using runoff flumes with water height through the flume measured using a pressure transducer din in runoff from an event was calculated from composite water samples collected during each event the field experiment consisted of two treatments with more than three times as much nitrogen fertiliser applied per hectare in the high nitrogen treatment compared to the low nitrogen treatment table 2 cumulative runoff and cumulative din losses in runoff between the start of the experiment and the time of plant harvest were reported the field experiment was simulated using the agricultural production systems simulator apsim version 7 10 holzworth et al 2014 a full description of the field experiment and model set up is described in vilas et al 2022 the trigger step 1 was a difference between the model and data in din in runoff losses specifically step 2 articulate the discrepancies the model predicted higher din loads in runoff for the high nitrogen treatment than the low nitrogen treatment but similar din loads were reported in runoff from the different treatments table 2 because the goal of the modelling was to evaluate the effect of farm management e g fertiliser application on din in runoff losses from sugarcane fields it was determined that the observed model data mismatch had consequences for the modelling goal the following model causes of the discrepancy were considered step 3a list model causes 1 errors in the initial conditions and 2 errors in the model structure regarding the initial conditions antecedent soil conditions e g soil water nitrogen in the soil were assumed to be the same for both treatments however the low nitrogen treatment could have had more nitrogen in the soil at the start of the experiment leading to the unexpected system behaviour in which both treatments had similar din losses regarding model structure nitrogen in the high nitrogen treatment could have been lost through a loss pathway that was not included in the model structure e g ammonia volatilisation leading to nitrogen being lost to the atmosphere we also identified issues with the data that could have led to the discrepancies step 3b list data causes 1 errors when importing the data 2 errors in data collection measurement transport storage and analyses and 3 data processing and handling errors when checking the data e g step 2 in bennett et al 2013 we noticed that din in runoff measurements in the high and low nitrogen treatments had different start dates the period of records provided was not the same for both treatments with the high nitrogen treatment started to be recorded later than the low nitrogen treatment which could have yielded an underestimation of total din in runoff losses in the high nitrogen treatment errors in the pressure sensors or in the din laboratory analyses could also have led to an underestimation of total runoff in the high nitrogen fertiliser treatment regarding data processing and handling errors inaccuracies in the calculation of din in runoff loads from din concentrations and runoff volumes ferrant et al 2013 could have led to the observed discrepancies to narrow down the causes of the discrepancy available expertise was applied step 4 knowledge elicitation consultation with the monitoring team confirmed that the soil in which the experiment was conducted was homogenised before the start of the experiment thus validating the assumption of similar initial conditions across both treatments any model structural errors were also discarded after discussion with monitoring teams however while analysing the data the monitoring teams identified that there was one runoff event missing in the high nitrogen treatment this error occurred during the data handling process the final step was to resolve the discrepancy step 5 solve in this case study the monitoring team was able to locate the missing runoff event when the missing runoff event was included in the analysis the model predictions and observations of din in runoff finally showed sensible agreement table 2 3 2 case study 2 hydrology model reveals changes in model input data in this case study a hydrology model of the tully river located within the wet tropics region queensland australia was calibrated against flow data stewart and jeffrey 2021 the wet tropics basin hydrology was modelled using the ewater source ewater ltd 2013 catchments modelling framework mccloskey et al 2021a 2021b the model is driven by long term climate data sourced from the silo climate data archive silo 2020 the hydrology was calibrated against measured flow data using an independent parameter estimation tool pest doherty et al 2005 for the calibration daily and monthly flow timeseries as well as the frequency of measured daily flows were used moriasi et al 2007 a full description of the model set up and calibration can be found in mccloskey et al 2021a 2021b the trigger step 1 was a change in the direction of the deviation between model and data fig 3 specifically step 2 articulate discrepancies the modelled annual flow was consistently higher than the measured annual flow before year 2000 2001 july 2000 to june 2001 with an average discrepancy of 7 however after 2000 2001 this pattern reversed so that annual mean flow was consistently lower than the measured annual flow average discrepancy of 16 because the model aimed at predicting flows and the discrepancy meant that the flow would have been largely underpredicted post 2000 2001 it was established that this discrepancy was consequential to the modelling goal the sudden change in the pattern and quality of the model data fit post 2000 2001 most likely suggests a problem with either the measured discharge or the input data to the model step 3b list data causes rather than with the model although this could also occur in the case of a sudden change in conditions not reflected in the model structure step 3a list model causes because the model fit changed after a specific point in time it was identified that changes in the methods and spatial temporal frequency used to measure data could have led to the discrepancy specifically the methods to estimate flow could have changed after 2000 2001 leading to an overestimation of the measured flow in addition model inputs e g rainfall could have changed post 2000 2001 leading to an underestimation of the modelled flow available expertise was applied to narrow down the causes of the discrepancy step 4 knowledge elicitation concentrating attention on either 1 possible changes in measurement methods after the year 2000 or 2 changes to model inputs e g rainfall after the year 2000 2001 after consultation with monitoring teams it was confirmed that there had been no changes to the way flow data were collected and analysed after the year 2000 2001 in addition it was confirmed that there had been no changes to the discharge curve used to estimate flow from measured water level next the inputs to the model were investigated the model was driven with gridded rainfall data obtained from the silo database silo gridded rainfall data are produced by interpolating between point rainfall measurements after discussion with silo teams it was found that the number of rainfall stations used in the model changed from 2000 2001 specifically a rainfall station in the upper tully catchment was added to the rainfall interpolation in 2001 two methods were employed to establish the impact of the station addition on the silo gridded rainfall data specific details of these methods are provided in stewart and jeffrey 2021 both methods showed a substantial difference between the time periods before and after the station addition in the estimated rainfall in the grids that were adjacent to the added station results of the analysis showed that post 2000 2001 the frequency of low volume rainfall events increased and the frequency of larger runoff producing rainfalls decreased in addition analysis of the trend of cumulative estimated rainfall pre and post rainfall station addition showed a reduction in annual average rainfall it was therefore concluded that the addition of the station in the upper tully impacted the long term characteristics of the interpolated rainfall data which in turn reduced the model estimates of overall flow post 2000 2001 the final step was to resolve this discrepancy step 5 solve in this case study the rainfall measurement density was increased by adding data from additional rainfall stations into the silo interpolation this change led to an improvement of the representation of tully river modelled flow by reducing the pre to post 2000 2001 flow discrepancy by 31 percent 3 3 case study 3 a machine learning model highlights issues with data export in this case study a machine learning model was used to predict the cost of management actions as a function of management area to assist in the prioritisation of actions to preserve the maximum number of species classified as endangered by the commonwealth conservation status nsw office of environment and heritage https www environment gov au the study site was the whole of new south wales australia the model was developed using historical data two datasets were used 1 a training dataset consisting of multiple features such as size of management area ha previously applied management actions e g fire grazing weed control flora monitoring and management cost aud the value to predict and 2 an application dataset consisting of the same features excluding management cost the data was obtained from multiple sources e g https www data gov au bionet atlas https www environment gov au the training dataset was split into training 75 and test 25 data the model was then used on an application dataset to predict the cost of the management actions the trigger step 1 was a discrepancy between the expected cost and the cost predicted by the machine learning model specifically the predicted cost of some management areas was substantially lower than the cost of management areas with similar characteristics which caught the attention of the modellers step 2 articulate discrepancies because accurately predicting the management cost is important for optimal allocation of resources it was established that this discrepancy was consequential to the modelling goal the discrepancy between the predicted and expected cost could indicate issues with the model see step 3a list model causes the following issues with the model conceptualisation structure and parameters were considered 1 the chosen machine learning algorithm catboost gradient boosting https catboost ai was not fit for purpose 2 the features used to train the model were not informative for predicting the management cost see galelli et al 2014 and 3 the range of hyperparameters used to train the machine learning model e g learning rate size of the training dataset training time was limited the discrepancy between the predicted and expected costs could also indicate issues with the data step 3b list data causes a potential problem arising from the data was also considered specifically the possibility that there may have been an error when exporting the data used for training and or application from the database to narrow down the causes of the discrepancy available expertise was applied step 4 knowledge elicitation feature selection analysis methodically selecting and combining features that significantly influence the variable being predicted indicated that modifying input features did not significantly improve the accuracy of the model in addition the use of other machine learning algorithms to build the model e g artificial neural networks random forest did not significantly improve the accuracy of the model suggesting that the issue was not in the selected algorithm catboost furthermore analysis of the hyperparameters used to train the machine learning model showed high model accuracy regardless of the algorithm used to check for errors in the data the features of the 4 management areas with substantially lower cost than expected were compared with another dataset spatial maps obtained from https www environment gov au when comparing the areas it was found that the areas did not match in 49 sites with 12 sites having an absolute proportional difference greater than 100 fig 4 after consultation with the data providers it was found that an error occurred when exporting the training dataset from a database the final step was to resolve this discrepancy step 5 solve in this case study the data providers re extracted the training dataset from the database and ensured that no errors occurred in the data extraction process when the re extracted dataset was used to train the model the model prediction on the application dataset matched the expectation and the issue was resolved 4 discussion effective integration of information from models and data is the holy grail of information quality for decision makers and those seeking to understand environmental systems however discrepancies between model predictions and measured data can make this integration challenging in this study we introduced the talks framework to deal with model data discrepancies we applied the framework to three case studies which show that even basic data issues such as errors during data import export can be consequential to the modelling goal therefore to address model data discrepancies it is crucial to consider the possibility of basic issues in both data and models additionally our study highlights a critical challenge in hydrological modelling namely issues with rainfall density which has been well documented in the literature jeffrey et al 2001 stewart and jeffrey 2021 while we provide an extensive set of examples for each step of the talks framework we acknowledge that inexperienced modellers may require additional guidance to effectively implement these examples the talks framework complements existing frameworks for model development and evaluation as it provides clear guidelines on how to resolve model data discrepancies which can arise during model calibration and technical assessment e g step 3 in bennett et al 2013 steps 7 10 in jakeman et al 2006 and steps 4 5 in refsgaard et al 2007 furthermore the talks framework expands on existing frameworks because it encourages the same level of evaluation to both models and data the possibility that some aspect of the data may be invalid is usually not the first avenue of interrogation when models are assessed hipsey et al 2020 however as the three case studies presented in this paper demonstrate the assumption of data validity should definitely be evaluated alongside the assumption of model validity if efficient resolution of model data discrepancies is desired the talks framework consists of four steps step 1 of the framework is to identify that a discrepancy exists trigger step 2 of the framework articulate discrepancies is standard modelling practice and can be achieved using a variety of methods such as visualising model performance or calculating performance metrics bennett et al 2013 if the discrepancies were found to be due to model causes step 3a list model causes the modeller applies their expertise and acts to resolve the discrepancies by following a model assessment framework such as concept state process system hipsey et al 2020 robson et al 2020 incorporating uncertainty ascough et al 2008 or highlighting the limitations of the model guo et al 2019 jones et al 2020 these processes are thus already well established as good practice in the field of environmental modelling however if model data discrepancies are due to data issues step 3b list data causes the expertise of data experts should be consulted step 4 knowledge elicitation and both the modelling and data teams should aim to resolve the issue together step 5 solve unfortunately often there is no direct contact between people who collect data and people who model it silberstein 2006 there are many reasons why this can happen for example data may be collected before any modelling endeavour or modellers may not have an existing relationship with data providers which makes the collaboration difficult in addition it is not common for modellers to work with people who collect data in designing data collection programs and modellers are often not equipped to identify data quality issues sambasivan et al 2021 due to a lack of practical or on ground knowledge of the environmental system being modelled keating 2020 the sensor technology in use or the measurement protocols employed thus model data integration requires more than automated tools to facilitate the use of data in models e g chen et al 2022 but also investment in the relationships between the different actors who contribute to the modelling process while modellers clearly play a pivotal role in the process of model development and assessment here we highlight that different disciplines may need to be integrated in the process of resolving model data discrepancies better model data integration can be attained through collaboration between modellers and monitoring teams gupta et al 2008 effective collaboration across the disciplinary divide is often not trivial and it takes time skill and goodwill to develop the relationships required for designing data collection to best inform models flynn 2005 querying monitoring teams about the quality of their data is even more fraught monitoring teams have high quality control standards for their specific data use purposes and often collect data without any intention that they be used for modelling furthermore monitoring teams may not be aware of the type of data needed to obtain reliable estimates of parameters or model outputs guillaume et al 2019 hence any conversation between modellers and monitoring teams need to start early in the modelling process hamilton et al 2019 and involve goodwill on both sides as well as great tact and emotional intelligence on the side of those initiating the conversation it could be argued therefore that the greatest opportunity for improving model data integration may not be new software or systems but simply investing in the relationships between those who model and those who measure developing strong collaborative relationships between modellers and monitoring teams requires dedication and effort modellers need to invest time to familiarise monitoring teams with the model as a prior understanding of the model can enhance collaboration between the two teams kotir et al under review furthermore modellers need to venture out into the field to gain a better understanding of the monitoring data and the system they are attempting to model by doing so modellers can gain firsthand experience and insights that can inform their modelling efforts and ultimately lead to more accurate models for an example see o brien et al 2022 which includes a video animation on how to improve relationships between modellers and monitoring teams despite evidence that trust in models is linked to trust in the modeller harper et al 2021 many modelling projects fail to recognize the importance of interpersonal relationships to improve the quality of science produced by modelling projects it is essential to allocate funding towards facilitating interactions between modellers and monitoring teams this can create opportunities for modellers to reflect on their modelling practices share their conceptual understanding with data providers and collaborate on sense checking this understanding espig et al 2020 goodwill and connections built up in this way will make it easier to have constructive dialogue that integrates the expertise of both modellers and data providers in resolving model data discrepancies 5 conclusion differences between model predictions and system observations are often seen as a barrier to model and data integration but here we show that an objective assessment of model data discrepancies can be valuable for enhancing the quality of information available from both models and data we recognize that despite being wrong models can be useful in better understanding and contextualising data to better manage environmental systems we need good quality information which can only be obtained when we acknowledge that neither model nor data are right but can be more useful when used together achieving this goal will require respectful and constructive relationships between those who build and use models and those who provide the data which feeds them software agricultural production systems simulator apsim apsim initiative 1994 holzworth et al 2014 https doi org 10 1016 j envsoft 2014 07 009 source v3 5 0 ewater ltd 2013 https wiki ewater org au display sd37 source user guide operating systems windows declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work received funding from the queensland government through the queensland water modelling network rd i project a framework to marry models and data tessa chamberlain stephen donaldson puwasala gamakumara ken rohde mel shaw and cassady swinbourne are thanked for helpful discussions during manuscript development stephen donaldson tony howes and ken rohde are thanked for reviewing the manuscript hayley langsdorf is acknowledged for developing the drawn visual material mpa s contribution was funded by an arc discovery early career researcher award de200100683 
25423,reproducibility is important to verify the credibility and reliability of scientific research geoscientists increasingly focus on the reproducibility of geo simulation experiments gses however geoscientists propensity to record in different styles and extensive speculation about the essential information required make it difficult to reproduce a gse from a notebook to facilitate reproducibility of gses this article proposes documentation strategy based on a conceptual model designed to identify and abstract the gse lifecycle to demonstrate different stages of gses there are three main parts in this strategy recommendations of reference descriptions construction of a gse s document gsedocument and processes for evaluating the reproducibility of gses thus a reproducible gsedocument can be constructed to document and share the gse lifecycle moreover to verify the effectiveness of documentation strategy a case study from an online competition is conducted through this strategy transparency utility and credibility of published research related to gses can be improved keywords reproducibility geo simulation experiment lifecycle experimental documentation data availability data will be made available on request 1 introduction currently concerns about scientific credit are steadily rising easterbrook 2014 goodman et al 2016 ioannidis 2005 with more than 70 of scientists failing to reproduce others experiments baker 2016 open science collaboration 2015 movements to address the credibility crisis of published computational results are arising in fields as disparate as computational science neuroscience and bioinformatics in particular begley and ioannidis 2015 committee on reproducibility and replicability in science et al 2019 peng et al 2006 poldrack 2019 reproducibility is the precondition and substantial ability to prove research credibility as reproducible research enables scientists to reimplement experiments reported by other scientists barba 2018 konkol et al 2019 mcnutt 2014 2014 2014 generally reproducibility can be cited as the ability to obtain consistent results to answer the same question as well as the usage of the same materials and methods barba 2018 committee on reproducibility and replicability in science et al 2019 wolke et al 2016 a geo simulation experiment gse can be regarded as a series of processes carried out to answer a real world question utilizing computational geographic analysis models and tools balci 1990 sacks et al 1989 it is a practical vehicle for geographic simulation which is an application step of geographic modeling aiming to reflect and predict specific geographic patterns and processes batty 2011 chen et al 2020 ruscheinski et al 2020 there are several reasons why improving the reproducibility of gses is important first complex and uncontrolled natural phenomena and geographic events such as earthquakes and debris flows are impossible to fully reproduce since no two phenomena or events are the same committee on reproducibility and replicability in science et al 2019 however to prove a model s usability as well as the credibility of findings reproducing natural environments and phenomena can be attempted to some extent by means of reproducing gses second with the emergence of quantitative research in geoscience computers have become an indispensable component of gses fotheringham and sachdeva 2022 qian et al 2022a manifesting as a series of computational models and tools the evolution of the computational mode in gses has created a demand for its reproducibility in order to meet the requirement of reproducibility of computational experiments gil et al 2016 goodchild et al 2021 li et al 2021a piccolo and frampton 2016 stodden et al 2018 third given the complexity and uncertainty of geoscience processes of world exploration tend to be iterative in light of previous gses chen et al 2021 gil et al 2016 based on reproducing gses the geo community could understand the specific approaches to geographic problems correct previous errors and even disclose some truths of geo phenomena barba 2018 goodchild and li 2021 according to the definition of reproducibility above a gse is reproducible when the same resources i e data resources and model resources and processes i e a series of activities that interact to generate a numerical output are used to provide consistent results in resolving the same geographic question making a gse reproducible is challenging because intrinsic processes of the given gse are often not fully available to reviewers konkol et al 2019 novère et al 2005 vasilevsky et al 2013 it is routine to use experiment notebooks and documentation to write a narrative about what exactly these analytical steps are in an experiment lifecycle from generating a geo problem to obtaining a satisfactory conclusion dirnagl and przesdzing 2016 kluyver et al 2016 piccolo and frampton 2016 grimm et al have proposed transparent and comprehensive ecological modelling documentation trace to provide better modeling support grimm et al 2014 the jupyter notebook has now been widely used by scientists from different fields to save their ideas and computational analyses kluyver et al 2016 however there are still some challenges to keeping a notebook the first challenge involves the preparation process for recording in the notebook which requires considerable effort gil et al 2016 sandve et al 2013 this is mainly because keeping a notebook is quite tedious work requiring geoscientists to choose and rearrange related information in a structured way while they may have no idea which information is core essawy et al 2020 konkol et al 2019 stodden et al 2018 if geoscientists have not divulged enough information from a complicated experiment it is probable that independent variables were not effectively controlled rendering a gse irreproducible when reviewers attempt to recreate it the second challenge is that many geoscientists keep notebooks at their own favor and discretion in different forms konkol et al 2019 pimentel et al 2019 which is casual and often incomplete as a result a notebook may lack crucial information and include an excessive amount of useless information ayllón et al 2021 this will not only lead to difficulty in migration between different platforms but also to confusion in the understanding of experiments by fellow scientists dirnagl and przesdzing 2016 after some time these notebooks may not be understood even by the authors themselves in this context this study aims to provide the documentation strategy to facilitate the reproducibility of a gse the gse lifecycle is identified as a framework of gses containing several activities actions and tasks that are required to solve a geo problem with acceptable quality characteristics balci 1994 2012 meanwhile the gse lifecycle can be used to express different and iterative stages of gses and a cmor model is designed to generalize and abstract it moreover the recommendation of reference descriptions the construction of a gse s document gsedocument and the processes for evaluating the reproducibility of gses based on a gsedocument are three main parts of the proposed strategy using the proposed strategy a gsedocument can be easily created by geoscientists making a gse understood and reproduced step by step in a clearer and more standard form by fellow scientists furthermore this study implements the proposed strategy into practice provides a gsedocument example for an online competition involving geographic analysis modeling and simulation and achieves some substantial results finally there is also the issue of scope when applying documentation strategy to reality above all this strategy applies only to the computational experiments that are accessible in most domains of geographic simulation such as hydrology soil atmosphere biology and human civilization moreover reference descriptions provided in this study only refer to the essential information that should be included when publishing a gse without specifying the technical form in which the information should be represented such as text graph or video form it is important to note that this study is not concerned with verifying the correctness or rationality of the purpose resources steps or results however fellow scientists are encouraged to verify the adequacy of information provided to obtain comprehensive results finally this study is not intended to criticize the quality of any existing work or notebook in other words it is possible for many gses to be reproducible without complying with the documentation strategy we anticipate that through the proposed strategy other scientists will be able to contribute more effectively to related future research the remainder of this article is structured as follows in section 2 the expression of the gse lifecycle and basic idea for constructing documentation strategy are explained section 3 describes the recommendations of reference descriptions the detailed design to construct a gsedocument and the processes for evaluating reproducibility section 4 verifies the documentation strategy with a case study and section 5 provides the discussion and conclusion of the study 2 basic design the documentation strategy depends on the description of a gse which can be articulated by the gse lifecycle this section first outlines gse lifecycle and evaluation for its reproducibility then integrality effectiveness and objectivity are identified to guide the construction of this strategy finally a conceptual model is developed to express the abstraction of the gse lifecycle as shown in fig 1 2 1 gse lifecycle currently there are two primary forms of gses geoscientists discover real world phenomena and patterns by using geographic analysis models for instance some scientists utilize spatial analysis models to simulate urban layouts and environments li et al 2021b qian et al 2020 zhang et al 2022 additionally geoscientists investigate the robustness and universality of models with a comparison of several models by simulating the target region qian et al 2022b tsai et al 2021 gse lifecycle is concluded as a series of ordered tasks completed in gses by geoscientists beginning with 1 scientific geo problem identification 2 purpose definition for determination of a gse 3 experimental scheme design 4 data and model resource preparation 5 model calculation 6 output validation 7 output visualization and 8 result analysis and answers for a geo problem balci 2012 ma et al 2021 2022 ruscheinski et al 2020 2 2 evaluation of a gse s reproducibility the ultimate purpose of this study is to increase the reproducibility of gses once a gse based paper is published geoscientists are expected to reproduce the involved experiment for this blueprint evaluation of the reproducibility of gse is essential meaning to what extent a gse can be considered to be reproducible three levels of reproducibility were defined to generalize the reproducibility of gse first complete reproducibility means that a gse can be reimplemented to yield the consistent outputs and answer the same geo problem second partial reproducibility means that one or some procedures can be reproduced and yield the consistent intermediate output while answering the same geo problem third irreproducibility refers to any procedures that cannot be re executed to answer the same geo problem in addition consistent outputs refer to the procedure outputs that are equal to the original outputs which can be examined by statistical indicators such as the mean square error mse mean absolute error mae and r2 2 3 fundamental principles to guide the construction of documentation strategy three main goals have been identified integrality this indicates whether the entire information needed to reproduce a gse s results is provided publicly chirigati et al 2013 to ensure integrality documentation strategy considers the usefulness and reliability of information which could influence the difference in results for future reviewers effectiveness this refers to whether the details are essentially provided in general some but not all details are essential for documenting the gse lifecycle to ensure its reproducibility davison 2012 thus careful consideration is given to which information that should be turned into entries in a public document to avoid redundancy objectivity this means that the result of a gse should not be influenced by any experimenter ferro et al 2016 to guarantee objectivity this study assumes certain information for steps with identical inputs and outputs although some analyses may involve randomness the use of constant random numbers given the same initial seed can ensure consistency 2 4 conceptual model following the three principles and in the interest of understandability the cmor model is designed to generalize and abstract series tasks of the gse lifecycle as shown in equation 1 and fig 1 1 a gse lifecycle c m o r r is the r ole item individuals responsible for the gse lifecycle as well as individuals and agencies in the cited information are referred to the role item this is used to indicate the provenance of responsible individuals to enhance the credibility and traceability of this gse c refers to the c ontext item it is a background summary aiming to gain awareness about a geo problem the sheer intellectual diversity of geoscience requires precise cognition to allow other geoscientists to comprehend purposes and demands as well as sufficiently understand the geo problems badham et al 2019 ma et al 2021 m is the m ethod item it is the collection of computational processes that includes tools techniques and models that can be used to address a problem or problem situation mingers 2000 method item is the most important part during a gse lifecycle designed to understand how to conduct a precise implementation strategy o refers to the o utput item it is a set of analyses and interpretations of outputs made to generate final results sandve et al 2013 without any numerical change in raw output the analysis and evaluation of gse results play a vital role in decision support chen et al 2021 lü et al 2019 at the end of the gse lifecycle raw outputs from the method item tend to be analyzed as well as the textual interpretations aiming to answer the geo problem mentioned in the context item this study also constructed a submodel of the iea item element attribute triple with equation 2 to represent different tasks in the gse lifecycle according to the iea triple this paper abstracts the gse lifecycle to the attribute level which can demonstrate the gse lifecycle in a structured and comprehensive manner here item is used to describe different phases during the gse lifecycle divided into role item context item method item and output item it also consists of different elements and attributes the relationships of the iea triple are shown in fig 2 an element can be cited as a subitem in an item which is primarily the union of task detailed information in addition an attribute is a specification that defines a property of an element at the attribute level key value pairs are designed to guide geoscientists to fill out each attribute of the gses aspects where key refers to the author defined attribute and value means the detailed information that needs to be described for the key 2 item element attribute 3 design of the documentation strategy 3 1 reference descriptions for the gse lifecycle reference descriptions serve as a standard format on the basis of the cmor model which recommends that core information needs to be documented in a notebook before examining specific ways to record the gse lifecycle it will be useful to develop a vocabulary of reference descriptions this will assist authors in considering what they can explain in an item and what they may do in each element table 1 presents the applicability of specific elements in each item in addition details and reasons for the recommended information that should be logged in the attributes of a gse are shown in table 2 3 1 1 applicability of elements in each item author and cited authorship element every gse should have at least one author who serves as a submitter chirigati et al 2013 there are also some stakeholders who make various contributions from developing technical routes to analyzing raw outputs such as developers teammates and decision makers chirigati et al 2013 additionally some information and resources generated are initially produced or declared by other scientists or agencies which need to be clarified thus one element in the role item is the author element which refers to someone who serves as a submitter and stakeholder in a gse the cited authorship element is the other element of role item related to authorships of any cited information in a gse key contributors of any cited information and resources who are not authors of a gse should also be assigned gil et al 2016 this element is used to indicate the provenance in order to enhance the credibility and traceability of a gse p roblem and solution element as defined by booth et al and badham et al the common structure of a context item consists of two elements including a statement of the problem and response to the problem asdal and moser 2012 badham et al 2019 booth et al 2016 consequently the problem element is the first element in the context item stating the common ground of a shared understanding about a geo problem song et al 2020 the solution element is designed to present a formulated plan in order to gather the desired information for solving the problem above at a minimal cost dellavigna et al 2019 ferro et al 2016 resource and procedure element as defined by mingers this paper considers the compatibility of various models a method item can be divided into two aspects the resource element and procedure element mingers 2000 data resources and model resources constituting all resources in a gse are used and generated in the process of specific implementation procedures chen et al 2020 zhang et al 2021 data resources are data used and produced in a method when performing a gse including input data parameters intermediate data i e the data are produced in one step and used in the next step and final outputs gil et al 2016 model resources are functioning quantitative models and tools involving a set of equations or rules to simulate the earth badham et al 2019 chen et al 2020 2021 the procedure element is a crucial concern for recording the implementation procedures in a gse as seemingly insignificant experimental details can drastically influence the reproduced outputs freire and silva 2012 to document the specific implementation of how routines of the solution item mentioned above are executed the procedure element is designed as a collection of ordered steps causing a numerical change in the outputs with the application of different resources freire et al 2012 validation and representation element as mentioned above the output item includes the validation of data to process and the analysis of desired numerical results first the validation element is not necessary but should not be ignored determining how well a gse s outputs compare with desired patterns balci 1998 sargent 2010 second the representation element is an interpretation and documentation of simulation results rabe et al 2009 sargent 2010 due to the complexity of some simulation results failing to provide textual interpretations of these results may lead to incorrect geo problem decisions although these results are sufficiently credible balci 2012 3 1 2 recommendations for attributes in each element a total of 29 recommendations regarding the essential information of each element in a gse lifecycle are given as shown in table 2 1 attributes in a role item attributes of an author element r1 authors and stakeholders should be correctly contacted if there are problems when reviewing or reproducing a gse thus name e mail professional title and affiliated institution should be specified which is the basic contact information derisi et al 2003 r2 the author s type should be specified such as submitters developers teammates and decision makers as a result geo scientists can contact the corresponding author rapidly when encountering troubles in one procedure of a gse orchard et al 2011 attributes of a cited authorship element r3 a cited authorship element should specify literature references including the authors publication date and reference citation for example gb t 7774 mla and apa are common standards used for reference citation r4 any relevant agent information that comes with resources e g where exactly the data was found in a publication or report should be cited such as the publishers or repository s name if it exists contact information affiliated institutions and other valuable information should also be noted novère et al 2005 2 attributes in a context item attributes of a problem element c1 when a geo problem is recognized a scientist or team initiates an experiment by communicating the geo problem to an analysis thus precise and applied questions should be outlined first balci 1998 c2 the purpose can be seen as the significance of doing this experiment by noting the purpose authors can create a stronger relationship with reviewers because they connect the extension of a gse to reviewers via their interested fields booth et al 2016 c3 geographic characteristics refer to the spatial temporal scale of this experiment e g spatial temporal boundaries dimensions and scales ma et al 2021 phillips 2022 in doing so a gse can be generalized on a similar spatial temporal scale for further study attributes of a solution element c4 a hypothesis is one or several conditions that can be provisionally accepted as the foundation for further experimentation wikipedia https en wikipedia org wiki hypothesis working hypothesis c5 experimental design can be shown with a flowchart in the form of a workflow or a summarized graph to respond to the problem above 3 attributes in a method item attributes of a data resource element above all the description of metadata for resource element is recommended which documents the integrity of each resource resources can be structured in a public machine readable format as either model standards such as systems biology markup language sbml overview design concepts and details odd protocol and open geographic modeling and simulation opengms standards chen et al 2019 grimm et al 2020 hucka et al 2003 or data standards such as the world wide web consortium provenance w3c prov and iso 19115 family and can be supported by specific software applications closa et al 2017 groth and frew 2012 zhang et al 2020 if not other information need to be provided are mentioned below m1 general information is the overview of data resources including name and type in this article the data resource type ranges from the initial analysis input data parameters and intermediate data i e the data are produced in one step and used in the next step to the final output contributing to the construction of the dataflow of procedures m2 a publicly accessible location where data resources are posted or where exactly authors can find them in a publication or report should be cited ideally data resources can be cited with a permanent unique identifier enabling other scientists to retrieve the data with directed access such as digital object unique identifiers dois archival resource keys arks identifiers org and personalized uniform resource locator purl derisi et al 2003 juty et al 2020 wilkinson et al 2016 in addition if there are some restrictions access information should be provided m3 a license specifies the limitations for data resources reuse including whether resources can be modified before redistribution whether the creator should be acknowledged and whether the resources can be used for commercial purposes for example the open data commons attribution license odc by is offered by creative commons allowing re users to share create and adapt the data resources in any medium or format https opendatacommons org index html m4 for parameters the precise value when implementing the gse needs to be fixed and specified the same process will typically yield different outputs every time it is executed when the parameters are not the same attributes of a model resource element like data resources model resources need to be documented and shared with a similar set of attributes m5 the general information of model resources should be noted including name type and version there are three formats of model resources including module e g buffer analysis in arcmap service e g soil water assessment tool swat service in opengms and code e g random forest source code stored on github org which should be specified as the type of model resource m6 all formats of model resources should be published in an accessible location ideally with a permanent unique identifier and m7 a license should be specified to define their acceptable use for example the open source initiative osi provides open source license options for model resources in geoscience such as the mit license the apache public license apl and the gnu general public license gpl fitzgerald 2006 m8 model resources must be correctly used in the relevant application software platform thus the application software platform dependency information e g what the dependent application software platform is and where the application software platform is stored as well as the dependency library e g r package and python package operating system e g windows macos and linux and dependency techniques e g cpu and gpu requirements should be explicitly documented m9 model resources should specify the required necessary input output information e g input data parameter and output data in several model resources with reference to the initial values of state variables if they exist attributes of a procedure element m10 before obtaining details each procedure should start with general information this is a description of what has been done in a procedure as well as what has been accomplished it should consist of i a title ii an overview of what the procedure is and why the procedure must be done iii the order of different procedures iv the version and v the type of procedure e g data evaluation data preprocessing simulation analysis data postprocessing uncertainty and sensitivity analysis m11 the configuration requirements should be documented these specify what exactly the model resource used in a procedure is and which data resource is applied as the input parameter or output of this model resource m12 the procedure should document the dataflow the dataflow indicates how the intermediate data are generated and traded as input data in the next procedure m13 if they exist the model settings of boundary conditions should be documented such as stop conditions number of replicates and limitation scope at runtime 4 attributes in an output item attributes of a validation item similar to a procedure element a validation element needs to be documented as a series of procedures o1 general information on what has been done should be provided o2 configuration requirements for analyzing the raw output should be documented o3 the dataflow of the analysis procedure should be specified and o4 the validation method applied in this procedure should not be ignored such as related indicators e g mse rmse and mae similar to data resources o5 the location of raw output from validation needs to be specified showing the credibility of validation and o6 results produced through the validation method should be noted attributes of a representation item o7 the particular analysis of final outputs should be specified this is textual information regarding the analysis and interpretation of raw figures 3 2 gsedocument construction normally reference descriptions should be consulted as a document in any form when authors want to publish gse related papers or reports every form such as free text or an extensible markup language xml schema should be accepted both online and offline thus this study proposes pathways for creating a gsedocument on the basis of reference descriptions on the basis of reference descriptions a gsedocument is a comprehensive document built from a formal collection of entries about the gse lifecycle that should be described by authors the key steps for compiling a gsedocument are shown in fig 3 1 extract the element attribute list in accordance with reference descriptions and append any additional and necessary information 2 construct a template for the gsedocument upon the extracted lists each entry in this template employs key value pairs where key means relying on the attributes contained in the above reference and value is the real life experimental information 3 retrieve corresponding information from a gse including text graphs tables and videos after this step the gsedocument is structured in accordance with the template 4 check the completeness of the gsedocument with these steps geoscientists can define any gsedocument that best meets its requirements for simulation characteristics 3 3 processes for evaluating the reproducibility based on gsedocument one of the most important parts of the documentation strategy is processes conducted to evaluate the gse s reproducibility which refers to how reviewers evaluate the gse s reproducibility based on a gsedocument according to the three levels of reproducibility definition above the processes conducted are shown in fig 4 it is carried out in six phases and on account of gsedocument 1 reviewing a gsedocument to understand a geo problem 2 locating any related resources 3 attempting to obtain resources and related software and applications as well as configuring the environment of related resources 4 rerunning models with related data in each procedure recording the intermediate data and output and comparing reproduced intermediate data and output with the initial ones 5 validating raw output and 6 checking the analysis and interpretation of the results 4 case study to gradually enable the documentation strategy to be accepted and disseminated by the geo community this study provides a gsedocument example for an online competition involving geographic analysis modeling and simulation this section introduces the content of the gsedocument example first and then analyzes the results uploaded by competitors to demonstrate the efficacy of the proposed strategy 4 1 a gsedocument example for an online competition the first national geographic analysis model development and application competition was launched by nanjing normal university from 7th october to 7th december 2022 this competition is established to improve the capabilities of development and application of geographic analysis models for undergraduates and graduates more than 700 participants from universities across china participated in the competition a gsedocument template the content of which is shown in table 3 and is based on reference descriptions was created for this competition at the deadline of this competition we received 72 gsedocuments according to these three levels of reproducibility above we found that 30 gses could be completely reproduced yielding identical answers and responses to the same question while 33 gses were partially reproducible and 9 of them could not be reproduced there were four reasons why a complete gse could not be reproduced or could be only partially reproduced as shown in fig 5 the first challenge is exacerbated by the incompleteness of the gsedocument accounting for 57 14 of incompletely reproducible gses second nonstandard description detail is another cause accounting for 47 62 third 7 14 of these gses were not completely reproducible because reviewers were not able to achieve consistent results while they tried to reproduce gses in accordance with the information shown in the gsedocument they obtained different results causing irreproducibility of the procedures finally 2 38 of these gses models could not be rerun due to the runtime error when we tried to execute models mentioned in the gsedocument with the usage of initial data resources the models were always run crash resulting in the runtime error 4 2 a typical gse in the competition taking one typical case in the competition as an example this study illustrates how to evaluate a gse s reproducibility based on the documentation strategy a brief presentation of this case is shown in fig 6 first the gse aims to quantitatively examine the impact of the built environment on crime occurrence from the perspective of multi scale analysis utilizing big data of streetscape images and deep learning technology second this gse has several models including fcn 8s fully convolutional networks 8s buffer gwr geographically weighted regression and mgwr multi scale geography weighted regression these models also correspond to some software such as arcmap 10 5 gpu cuda enabled semantic segmentation app mgwr 2 2 office 2019 and r studio 4 2 1 additionally there are numerous data resources such as google street view image road network buffer distance environmental characteristic indicator and manhattan crime datasets used as input parameters or intermediate data in a procedure these data resources are uploaded into the opengms data container yue et al 2015 which can be downloaded by other reviewers all the initial data resources and model software could be obtained from the provided locations third the environment was configured in accordance with the description of the model s resources in particular the semantic segmentation application requires hardware that is gpus with nvdia cuda 9 x fourth based on the information of each procedure this study reran the models and recorded all the used and produced data there were thirteen procedures in this gse and the dataflow shown in fig 6 was established to represent all the procedures in a brief way fifth all the intermediate data output and analysis and interpretation of results are checked therefore this study uses mse mae and r2 to evaluate the consistency between the reproduced data and original data provided by the authors mse and mae are two typical indicators to evaluate the difference between reproduced and initial outputs hodson 2022 r2 is used to access the goodness of fit between reproduced outputs and the original outputs chicco et al 2021 we found that these procedural intermediate data and outputs are exactly constant with the original ones some substantial outputs are shown in fig 7 and table 4 finally this gse analyzes the raw outputs in table 4 and the author reported conclusions in three aspects that is comparison of grw and mgrw scale analysis and spatial pattern analysis of regression coefficients in summary through the development of the experimental case on the one hand the multiscale descriptive quantitative framework of crime scene environments is constructed moreover the results revealed the multiscale relationship between environmental characteristics and crime answering the geo problem mentioned above 5 conclusion and outlook keeping a notebook or document of the gse lifecycle is a routine way to make a gse reproducible as mentioned earlier major challenges to keeping a notebook include extensive speculation about essential information for keeping a notebook and the preference to record experiments in the geoscientist s own style in this paper we propose documentation strategy to help address these barriers through a concise and practical articulation of gsedocument construction despite some benefits there is still considerable room for improvement of gses reproducibility this study next presents three recommendations for the future development of gses reproducibility to make the proposed strategy more acceptable useable and comprehensive first the reproducibility of a gse is closely tied to the evaluation criteria there are still many questions about how to evaluate an experiment is reproducible barba 2018 essawy et al 2020 goodman et al 2016 when conforming to randomness of method accuracy of data manual error and some other factors may influence the consistency of results definition and evaluation criteria may have a deeper and more complex meaning second from our perspective there are still some shortcomings of the proposed strategy as with the odd protocol trace notebook and minimum information required in the annotation of biochemical models miriam ayllón et al 2021 evans 1969 grimm et al 2014 2020 novère et al 2005 orchard et al 2011 our study aims to learn from the experience of existing guidelines as well as to reference and welcome geo communities to provide feedback by contacting the lead author we also hope that documentation strategy can be widely adopted by other scientists and the geo community and ultimately galvanize discussion because each constituent entry in reference descriptions may change over time finally one important concern in implementing a documentation strategy is a lack of tools to facilitate the recording processes numerous tools already exist for problem formulation resource distribution metadata documentation computational process provenance capture and many other features described in this study cerutti et al 2021 freire and silva 2012 pimentel et al 2019 steeves et al 2020 these tools can be implemented as part of the proposed strategy to save time and effort another important concern is the demand for an online gse supportive database that allows geoscientists to query evaluate re execute and make new modifications to the original experiment although these technologies are not yet integrated effortlessly into gsedocument practices their integration should be considered in future research declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we appreciate the detailed suggestions and comments from the editor and anonymous reviewers and express heartfelt thanks to the other members of the opengms team this work was supported by the national natural science foundation of china grant no 42071363 and postgraduate research practice innovation program of jiangsu province grant no kycx22 1566 
25423,reproducibility is important to verify the credibility and reliability of scientific research geoscientists increasingly focus on the reproducibility of geo simulation experiments gses however geoscientists propensity to record in different styles and extensive speculation about the essential information required make it difficult to reproduce a gse from a notebook to facilitate reproducibility of gses this article proposes documentation strategy based on a conceptual model designed to identify and abstract the gse lifecycle to demonstrate different stages of gses there are three main parts in this strategy recommendations of reference descriptions construction of a gse s document gsedocument and processes for evaluating the reproducibility of gses thus a reproducible gsedocument can be constructed to document and share the gse lifecycle moreover to verify the effectiveness of documentation strategy a case study from an online competition is conducted through this strategy transparency utility and credibility of published research related to gses can be improved keywords reproducibility geo simulation experiment lifecycle experimental documentation data availability data will be made available on request 1 introduction currently concerns about scientific credit are steadily rising easterbrook 2014 goodman et al 2016 ioannidis 2005 with more than 70 of scientists failing to reproduce others experiments baker 2016 open science collaboration 2015 movements to address the credibility crisis of published computational results are arising in fields as disparate as computational science neuroscience and bioinformatics in particular begley and ioannidis 2015 committee on reproducibility and replicability in science et al 2019 peng et al 2006 poldrack 2019 reproducibility is the precondition and substantial ability to prove research credibility as reproducible research enables scientists to reimplement experiments reported by other scientists barba 2018 konkol et al 2019 mcnutt 2014 2014 2014 generally reproducibility can be cited as the ability to obtain consistent results to answer the same question as well as the usage of the same materials and methods barba 2018 committee on reproducibility and replicability in science et al 2019 wolke et al 2016 a geo simulation experiment gse can be regarded as a series of processes carried out to answer a real world question utilizing computational geographic analysis models and tools balci 1990 sacks et al 1989 it is a practical vehicle for geographic simulation which is an application step of geographic modeling aiming to reflect and predict specific geographic patterns and processes batty 2011 chen et al 2020 ruscheinski et al 2020 there are several reasons why improving the reproducibility of gses is important first complex and uncontrolled natural phenomena and geographic events such as earthquakes and debris flows are impossible to fully reproduce since no two phenomena or events are the same committee on reproducibility and replicability in science et al 2019 however to prove a model s usability as well as the credibility of findings reproducing natural environments and phenomena can be attempted to some extent by means of reproducing gses second with the emergence of quantitative research in geoscience computers have become an indispensable component of gses fotheringham and sachdeva 2022 qian et al 2022a manifesting as a series of computational models and tools the evolution of the computational mode in gses has created a demand for its reproducibility in order to meet the requirement of reproducibility of computational experiments gil et al 2016 goodchild et al 2021 li et al 2021a piccolo and frampton 2016 stodden et al 2018 third given the complexity and uncertainty of geoscience processes of world exploration tend to be iterative in light of previous gses chen et al 2021 gil et al 2016 based on reproducing gses the geo community could understand the specific approaches to geographic problems correct previous errors and even disclose some truths of geo phenomena barba 2018 goodchild and li 2021 according to the definition of reproducibility above a gse is reproducible when the same resources i e data resources and model resources and processes i e a series of activities that interact to generate a numerical output are used to provide consistent results in resolving the same geographic question making a gse reproducible is challenging because intrinsic processes of the given gse are often not fully available to reviewers konkol et al 2019 novère et al 2005 vasilevsky et al 2013 it is routine to use experiment notebooks and documentation to write a narrative about what exactly these analytical steps are in an experiment lifecycle from generating a geo problem to obtaining a satisfactory conclusion dirnagl and przesdzing 2016 kluyver et al 2016 piccolo and frampton 2016 grimm et al have proposed transparent and comprehensive ecological modelling documentation trace to provide better modeling support grimm et al 2014 the jupyter notebook has now been widely used by scientists from different fields to save their ideas and computational analyses kluyver et al 2016 however there are still some challenges to keeping a notebook the first challenge involves the preparation process for recording in the notebook which requires considerable effort gil et al 2016 sandve et al 2013 this is mainly because keeping a notebook is quite tedious work requiring geoscientists to choose and rearrange related information in a structured way while they may have no idea which information is core essawy et al 2020 konkol et al 2019 stodden et al 2018 if geoscientists have not divulged enough information from a complicated experiment it is probable that independent variables were not effectively controlled rendering a gse irreproducible when reviewers attempt to recreate it the second challenge is that many geoscientists keep notebooks at their own favor and discretion in different forms konkol et al 2019 pimentel et al 2019 which is casual and often incomplete as a result a notebook may lack crucial information and include an excessive amount of useless information ayllón et al 2021 this will not only lead to difficulty in migration between different platforms but also to confusion in the understanding of experiments by fellow scientists dirnagl and przesdzing 2016 after some time these notebooks may not be understood even by the authors themselves in this context this study aims to provide the documentation strategy to facilitate the reproducibility of a gse the gse lifecycle is identified as a framework of gses containing several activities actions and tasks that are required to solve a geo problem with acceptable quality characteristics balci 1994 2012 meanwhile the gse lifecycle can be used to express different and iterative stages of gses and a cmor model is designed to generalize and abstract it moreover the recommendation of reference descriptions the construction of a gse s document gsedocument and the processes for evaluating the reproducibility of gses based on a gsedocument are three main parts of the proposed strategy using the proposed strategy a gsedocument can be easily created by geoscientists making a gse understood and reproduced step by step in a clearer and more standard form by fellow scientists furthermore this study implements the proposed strategy into practice provides a gsedocument example for an online competition involving geographic analysis modeling and simulation and achieves some substantial results finally there is also the issue of scope when applying documentation strategy to reality above all this strategy applies only to the computational experiments that are accessible in most domains of geographic simulation such as hydrology soil atmosphere biology and human civilization moreover reference descriptions provided in this study only refer to the essential information that should be included when publishing a gse without specifying the technical form in which the information should be represented such as text graph or video form it is important to note that this study is not concerned with verifying the correctness or rationality of the purpose resources steps or results however fellow scientists are encouraged to verify the adequacy of information provided to obtain comprehensive results finally this study is not intended to criticize the quality of any existing work or notebook in other words it is possible for many gses to be reproducible without complying with the documentation strategy we anticipate that through the proposed strategy other scientists will be able to contribute more effectively to related future research the remainder of this article is structured as follows in section 2 the expression of the gse lifecycle and basic idea for constructing documentation strategy are explained section 3 describes the recommendations of reference descriptions the detailed design to construct a gsedocument and the processes for evaluating reproducibility section 4 verifies the documentation strategy with a case study and section 5 provides the discussion and conclusion of the study 2 basic design the documentation strategy depends on the description of a gse which can be articulated by the gse lifecycle this section first outlines gse lifecycle and evaluation for its reproducibility then integrality effectiveness and objectivity are identified to guide the construction of this strategy finally a conceptual model is developed to express the abstraction of the gse lifecycle as shown in fig 1 2 1 gse lifecycle currently there are two primary forms of gses geoscientists discover real world phenomena and patterns by using geographic analysis models for instance some scientists utilize spatial analysis models to simulate urban layouts and environments li et al 2021b qian et al 2020 zhang et al 2022 additionally geoscientists investigate the robustness and universality of models with a comparison of several models by simulating the target region qian et al 2022b tsai et al 2021 gse lifecycle is concluded as a series of ordered tasks completed in gses by geoscientists beginning with 1 scientific geo problem identification 2 purpose definition for determination of a gse 3 experimental scheme design 4 data and model resource preparation 5 model calculation 6 output validation 7 output visualization and 8 result analysis and answers for a geo problem balci 2012 ma et al 2021 2022 ruscheinski et al 2020 2 2 evaluation of a gse s reproducibility the ultimate purpose of this study is to increase the reproducibility of gses once a gse based paper is published geoscientists are expected to reproduce the involved experiment for this blueprint evaluation of the reproducibility of gse is essential meaning to what extent a gse can be considered to be reproducible three levels of reproducibility were defined to generalize the reproducibility of gse first complete reproducibility means that a gse can be reimplemented to yield the consistent outputs and answer the same geo problem second partial reproducibility means that one or some procedures can be reproduced and yield the consistent intermediate output while answering the same geo problem third irreproducibility refers to any procedures that cannot be re executed to answer the same geo problem in addition consistent outputs refer to the procedure outputs that are equal to the original outputs which can be examined by statistical indicators such as the mean square error mse mean absolute error mae and r2 2 3 fundamental principles to guide the construction of documentation strategy three main goals have been identified integrality this indicates whether the entire information needed to reproduce a gse s results is provided publicly chirigati et al 2013 to ensure integrality documentation strategy considers the usefulness and reliability of information which could influence the difference in results for future reviewers effectiveness this refers to whether the details are essentially provided in general some but not all details are essential for documenting the gse lifecycle to ensure its reproducibility davison 2012 thus careful consideration is given to which information that should be turned into entries in a public document to avoid redundancy objectivity this means that the result of a gse should not be influenced by any experimenter ferro et al 2016 to guarantee objectivity this study assumes certain information for steps with identical inputs and outputs although some analyses may involve randomness the use of constant random numbers given the same initial seed can ensure consistency 2 4 conceptual model following the three principles and in the interest of understandability the cmor model is designed to generalize and abstract series tasks of the gse lifecycle as shown in equation 1 and fig 1 1 a gse lifecycle c m o r r is the r ole item individuals responsible for the gse lifecycle as well as individuals and agencies in the cited information are referred to the role item this is used to indicate the provenance of responsible individuals to enhance the credibility and traceability of this gse c refers to the c ontext item it is a background summary aiming to gain awareness about a geo problem the sheer intellectual diversity of geoscience requires precise cognition to allow other geoscientists to comprehend purposes and demands as well as sufficiently understand the geo problems badham et al 2019 ma et al 2021 m is the m ethod item it is the collection of computational processes that includes tools techniques and models that can be used to address a problem or problem situation mingers 2000 method item is the most important part during a gse lifecycle designed to understand how to conduct a precise implementation strategy o refers to the o utput item it is a set of analyses and interpretations of outputs made to generate final results sandve et al 2013 without any numerical change in raw output the analysis and evaluation of gse results play a vital role in decision support chen et al 2021 lü et al 2019 at the end of the gse lifecycle raw outputs from the method item tend to be analyzed as well as the textual interpretations aiming to answer the geo problem mentioned in the context item this study also constructed a submodel of the iea item element attribute triple with equation 2 to represent different tasks in the gse lifecycle according to the iea triple this paper abstracts the gse lifecycle to the attribute level which can demonstrate the gse lifecycle in a structured and comprehensive manner here item is used to describe different phases during the gse lifecycle divided into role item context item method item and output item it also consists of different elements and attributes the relationships of the iea triple are shown in fig 2 an element can be cited as a subitem in an item which is primarily the union of task detailed information in addition an attribute is a specification that defines a property of an element at the attribute level key value pairs are designed to guide geoscientists to fill out each attribute of the gses aspects where key refers to the author defined attribute and value means the detailed information that needs to be described for the key 2 item element attribute 3 design of the documentation strategy 3 1 reference descriptions for the gse lifecycle reference descriptions serve as a standard format on the basis of the cmor model which recommends that core information needs to be documented in a notebook before examining specific ways to record the gse lifecycle it will be useful to develop a vocabulary of reference descriptions this will assist authors in considering what they can explain in an item and what they may do in each element table 1 presents the applicability of specific elements in each item in addition details and reasons for the recommended information that should be logged in the attributes of a gse are shown in table 2 3 1 1 applicability of elements in each item author and cited authorship element every gse should have at least one author who serves as a submitter chirigati et al 2013 there are also some stakeholders who make various contributions from developing technical routes to analyzing raw outputs such as developers teammates and decision makers chirigati et al 2013 additionally some information and resources generated are initially produced or declared by other scientists or agencies which need to be clarified thus one element in the role item is the author element which refers to someone who serves as a submitter and stakeholder in a gse the cited authorship element is the other element of role item related to authorships of any cited information in a gse key contributors of any cited information and resources who are not authors of a gse should also be assigned gil et al 2016 this element is used to indicate the provenance in order to enhance the credibility and traceability of a gse p roblem and solution element as defined by booth et al and badham et al the common structure of a context item consists of two elements including a statement of the problem and response to the problem asdal and moser 2012 badham et al 2019 booth et al 2016 consequently the problem element is the first element in the context item stating the common ground of a shared understanding about a geo problem song et al 2020 the solution element is designed to present a formulated plan in order to gather the desired information for solving the problem above at a minimal cost dellavigna et al 2019 ferro et al 2016 resource and procedure element as defined by mingers this paper considers the compatibility of various models a method item can be divided into two aspects the resource element and procedure element mingers 2000 data resources and model resources constituting all resources in a gse are used and generated in the process of specific implementation procedures chen et al 2020 zhang et al 2021 data resources are data used and produced in a method when performing a gse including input data parameters intermediate data i e the data are produced in one step and used in the next step and final outputs gil et al 2016 model resources are functioning quantitative models and tools involving a set of equations or rules to simulate the earth badham et al 2019 chen et al 2020 2021 the procedure element is a crucial concern for recording the implementation procedures in a gse as seemingly insignificant experimental details can drastically influence the reproduced outputs freire and silva 2012 to document the specific implementation of how routines of the solution item mentioned above are executed the procedure element is designed as a collection of ordered steps causing a numerical change in the outputs with the application of different resources freire et al 2012 validation and representation element as mentioned above the output item includes the validation of data to process and the analysis of desired numerical results first the validation element is not necessary but should not be ignored determining how well a gse s outputs compare with desired patterns balci 1998 sargent 2010 second the representation element is an interpretation and documentation of simulation results rabe et al 2009 sargent 2010 due to the complexity of some simulation results failing to provide textual interpretations of these results may lead to incorrect geo problem decisions although these results are sufficiently credible balci 2012 3 1 2 recommendations for attributes in each element a total of 29 recommendations regarding the essential information of each element in a gse lifecycle are given as shown in table 2 1 attributes in a role item attributes of an author element r1 authors and stakeholders should be correctly contacted if there are problems when reviewing or reproducing a gse thus name e mail professional title and affiliated institution should be specified which is the basic contact information derisi et al 2003 r2 the author s type should be specified such as submitters developers teammates and decision makers as a result geo scientists can contact the corresponding author rapidly when encountering troubles in one procedure of a gse orchard et al 2011 attributes of a cited authorship element r3 a cited authorship element should specify literature references including the authors publication date and reference citation for example gb t 7774 mla and apa are common standards used for reference citation r4 any relevant agent information that comes with resources e g where exactly the data was found in a publication or report should be cited such as the publishers or repository s name if it exists contact information affiliated institutions and other valuable information should also be noted novère et al 2005 2 attributes in a context item attributes of a problem element c1 when a geo problem is recognized a scientist or team initiates an experiment by communicating the geo problem to an analysis thus precise and applied questions should be outlined first balci 1998 c2 the purpose can be seen as the significance of doing this experiment by noting the purpose authors can create a stronger relationship with reviewers because they connect the extension of a gse to reviewers via their interested fields booth et al 2016 c3 geographic characteristics refer to the spatial temporal scale of this experiment e g spatial temporal boundaries dimensions and scales ma et al 2021 phillips 2022 in doing so a gse can be generalized on a similar spatial temporal scale for further study attributes of a solution element c4 a hypothesis is one or several conditions that can be provisionally accepted as the foundation for further experimentation wikipedia https en wikipedia org wiki hypothesis working hypothesis c5 experimental design can be shown with a flowchart in the form of a workflow or a summarized graph to respond to the problem above 3 attributes in a method item attributes of a data resource element above all the description of metadata for resource element is recommended which documents the integrity of each resource resources can be structured in a public machine readable format as either model standards such as systems biology markup language sbml overview design concepts and details odd protocol and open geographic modeling and simulation opengms standards chen et al 2019 grimm et al 2020 hucka et al 2003 or data standards such as the world wide web consortium provenance w3c prov and iso 19115 family and can be supported by specific software applications closa et al 2017 groth and frew 2012 zhang et al 2020 if not other information need to be provided are mentioned below m1 general information is the overview of data resources including name and type in this article the data resource type ranges from the initial analysis input data parameters and intermediate data i e the data are produced in one step and used in the next step to the final output contributing to the construction of the dataflow of procedures m2 a publicly accessible location where data resources are posted or where exactly authors can find them in a publication or report should be cited ideally data resources can be cited with a permanent unique identifier enabling other scientists to retrieve the data with directed access such as digital object unique identifiers dois archival resource keys arks identifiers org and personalized uniform resource locator purl derisi et al 2003 juty et al 2020 wilkinson et al 2016 in addition if there are some restrictions access information should be provided m3 a license specifies the limitations for data resources reuse including whether resources can be modified before redistribution whether the creator should be acknowledged and whether the resources can be used for commercial purposes for example the open data commons attribution license odc by is offered by creative commons allowing re users to share create and adapt the data resources in any medium or format https opendatacommons org index html m4 for parameters the precise value when implementing the gse needs to be fixed and specified the same process will typically yield different outputs every time it is executed when the parameters are not the same attributes of a model resource element like data resources model resources need to be documented and shared with a similar set of attributes m5 the general information of model resources should be noted including name type and version there are three formats of model resources including module e g buffer analysis in arcmap service e g soil water assessment tool swat service in opengms and code e g random forest source code stored on github org which should be specified as the type of model resource m6 all formats of model resources should be published in an accessible location ideally with a permanent unique identifier and m7 a license should be specified to define their acceptable use for example the open source initiative osi provides open source license options for model resources in geoscience such as the mit license the apache public license apl and the gnu general public license gpl fitzgerald 2006 m8 model resources must be correctly used in the relevant application software platform thus the application software platform dependency information e g what the dependent application software platform is and where the application software platform is stored as well as the dependency library e g r package and python package operating system e g windows macos and linux and dependency techniques e g cpu and gpu requirements should be explicitly documented m9 model resources should specify the required necessary input output information e g input data parameter and output data in several model resources with reference to the initial values of state variables if they exist attributes of a procedure element m10 before obtaining details each procedure should start with general information this is a description of what has been done in a procedure as well as what has been accomplished it should consist of i a title ii an overview of what the procedure is and why the procedure must be done iii the order of different procedures iv the version and v the type of procedure e g data evaluation data preprocessing simulation analysis data postprocessing uncertainty and sensitivity analysis m11 the configuration requirements should be documented these specify what exactly the model resource used in a procedure is and which data resource is applied as the input parameter or output of this model resource m12 the procedure should document the dataflow the dataflow indicates how the intermediate data are generated and traded as input data in the next procedure m13 if they exist the model settings of boundary conditions should be documented such as stop conditions number of replicates and limitation scope at runtime 4 attributes in an output item attributes of a validation item similar to a procedure element a validation element needs to be documented as a series of procedures o1 general information on what has been done should be provided o2 configuration requirements for analyzing the raw output should be documented o3 the dataflow of the analysis procedure should be specified and o4 the validation method applied in this procedure should not be ignored such as related indicators e g mse rmse and mae similar to data resources o5 the location of raw output from validation needs to be specified showing the credibility of validation and o6 results produced through the validation method should be noted attributes of a representation item o7 the particular analysis of final outputs should be specified this is textual information regarding the analysis and interpretation of raw figures 3 2 gsedocument construction normally reference descriptions should be consulted as a document in any form when authors want to publish gse related papers or reports every form such as free text or an extensible markup language xml schema should be accepted both online and offline thus this study proposes pathways for creating a gsedocument on the basis of reference descriptions on the basis of reference descriptions a gsedocument is a comprehensive document built from a formal collection of entries about the gse lifecycle that should be described by authors the key steps for compiling a gsedocument are shown in fig 3 1 extract the element attribute list in accordance with reference descriptions and append any additional and necessary information 2 construct a template for the gsedocument upon the extracted lists each entry in this template employs key value pairs where key means relying on the attributes contained in the above reference and value is the real life experimental information 3 retrieve corresponding information from a gse including text graphs tables and videos after this step the gsedocument is structured in accordance with the template 4 check the completeness of the gsedocument with these steps geoscientists can define any gsedocument that best meets its requirements for simulation characteristics 3 3 processes for evaluating the reproducibility based on gsedocument one of the most important parts of the documentation strategy is processes conducted to evaluate the gse s reproducibility which refers to how reviewers evaluate the gse s reproducibility based on a gsedocument according to the three levels of reproducibility definition above the processes conducted are shown in fig 4 it is carried out in six phases and on account of gsedocument 1 reviewing a gsedocument to understand a geo problem 2 locating any related resources 3 attempting to obtain resources and related software and applications as well as configuring the environment of related resources 4 rerunning models with related data in each procedure recording the intermediate data and output and comparing reproduced intermediate data and output with the initial ones 5 validating raw output and 6 checking the analysis and interpretation of the results 4 case study to gradually enable the documentation strategy to be accepted and disseminated by the geo community this study provides a gsedocument example for an online competition involving geographic analysis modeling and simulation this section introduces the content of the gsedocument example first and then analyzes the results uploaded by competitors to demonstrate the efficacy of the proposed strategy 4 1 a gsedocument example for an online competition the first national geographic analysis model development and application competition was launched by nanjing normal university from 7th october to 7th december 2022 this competition is established to improve the capabilities of development and application of geographic analysis models for undergraduates and graduates more than 700 participants from universities across china participated in the competition a gsedocument template the content of which is shown in table 3 and is based on reference descriptions was created for this competition at the deadline of this competition we received 72 gsedocuments according to these three levels of reproducibility above we found that 30 gses could be completely reproduced yielding identical answers and responses to the same question while 33 gses were partially reproducible and 9 of them could not be reproduced there were four reasons why a complete gse could not be reproduced or could be only partially reproduced as shown in fig 5 the first challenge is exacerbated by the incompleteness of the gsedocument accounting for 57 14 of incompletely reproducible gses second nonstandard description detail is another cause accounting for 47 62 third 7 14 of these gses were not completely reproducible because reviewers were not able to achieve consistent results while they tried to reproduce gses in accordance with the information shown in the gsedocument they obtained different results causing irreproducibility of the procedures finally 2 38 of these gses models could not be rerun due to the runtime error when we tried to execute models mentioned in the gsedocument with the usage of initial data resources the models were always run crash resulting in the runtime error 4 2 a typical gse in the competition taking one typical case in the competition as an example this study illustrates how to evaluate a gse s reproducibility based on the documentation strategy a brief presentation of this case is shown in fig 6 first the gse aims to quantitatively examine the impact of the built environment on crime occurrence from the perspective of multi scale analysis utilizing big data of streetscape images and deep learning technology second this gse has several models including fcn 8s fully convolutional networks 8s buffer gwr geographically weighted regression and mgwr multi scale geography weighted regression these models also correspond to some software such as arcmap 10 5 gpu cuda enabled semantic segmentation app mgwr 2 2 office 2019 and r studio 4 2 1 additionally there are numerous data resources such as google street view image road network buffer distance environmental characteristic indicator and manhattan crime datasets used as input parameters or intermediate data in a procedure these data resources are uploaded into the opengms data container yue et al 2015 which can be downloaded by other reviewers all the initial data resources and model software could be obtained from the provided locations third the environment was configured in accordance with the description of the model s resources in particular the semantic segmentation application requires hardware that is gpus with nvdia cuda 9 x fourth based on the information of each procedure this study reran the models and recorded all the used and produced data there were thirteen procedures in this gse and the dataflow shown in fig 6 was established to represent all the procedures in a brief way fifth all the intermediate data output and analysis and interpretation of results are checked therefore this study uses mse mae and r2 to evaluate the consistency between the reproduced data and original data provided by the authors mse and mae are two typical indicators to evaluate the difference between reproduced and initial outputs hodson 2022 r2 is used to access the goodness of fit between reproduced outputs and the original outputs chicco et al 2021 we found that these procedural intermediate data and outputs are exactly constant with the original ones some substantial outputs are shown in fig 7 and table 4 finally this gse analyzes the raw outputs in table 4 and the author reported conclusions in three aspects that is comparison of grw and mgrw scale analysis and spatial pattern analysis of regression coefficients in summary through the development of the experimental case on the one hand the multiscale descriptive quantitative framework of crime scene environments is constructed moreover the results revealed the multiscale relationship between environmental characteristics and crime answering the geo problem mentioned above 5 conclusion and outlook keeping a notebook or document of the gse lifecycle is a routine way to make a gse reproducible as mentioned earlier major challenges to keeping a notebook include extensive speculation about essential information for keeping a notebook and the preference to record experiments in the geoscientist s own style in this paper we propose documentation strategy to help address these barriers through a concise and practical articulation of gsedocument construction despite some benefits there is still considerable room for improvement of gses reproducibility this study next presents three recommendations for the future development of gses reproducibility to make the proposed strategy more acceptable useable and comprehensive first the reproducibility of a gse is closely tied to the evaluation criteria there are still many questions about how to evaluate an experiment is reproducible barba 2018 essawy et al 2020 goodman et al 2016 when conforming to randomness of method accuracy of data manual error and some other factors may influence the consistency of results definition and evaluation criteria may have a deeper and more complex meaning second from our perspective there are still some shortcomings of the proposed strategy as with the odd protocol trace notebook and minimum information required in the annotation of biochemical models miriam ayllón et al 2021 evans 1969 grimm et al 2014 2020 novère et al 2005 orchard et al 2011 our study aims to learn from the experience of existing guidelines as well as to reference and welcome geo communities to provide feedback by contacting the lead author we also hope that documentation strategy can be widely adopted by other scientists and the geo community and ultimately galvanize discussion because each constituent entry in reference descriptions may change over time finally one important concern in implementing a documentation strategy is a lack of tools to facilitate the recording processes numerous tools already exist for problem formulation resource distribution metadata documentation computational process provenance capture and many other features described in this study cerutti et al 2021 freire and silva 2012 pimentel et al 2019 steeves et al 2020 these tools can be implemented as part of the proposed strategy to save time and effort another important concern is the demand for an online gse supportive database that allows geoscientists to query evaluate re execute and make new modifications to the original experiment although these technologies are not yet integrated effortlessly into gsedocument practices their integration should be considered in future research declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we appreciate the detailed suggestions and comments from the editor and anonymous reviewers and express heartfelt thanks to the other members of the opengms team this work was supported by the national natural science foundation of china grant no 42071363 and postgraduate research practice innovation program of jiangsu province grant no kycx22 1566 
25424,the aim of the research was to implement the swan wave model powered by meteorological data from the cosmo model in the baltic area with particular emphasis on the southern regions where the swan model is operational in the validation of the model the predictive performance of the formulas was assessed westhuysen komen janssen st6 the verification of wave forecast quality obtained with the spectral model was carried out in two stages the first step involved selecting the best formula using the adopted parameterizations based on short term measurement data after choosing the best physics formula st6 the model results were verified with multi stream measurement and satellite data at two points representing the shallow and deepwater zones in the open sea zone the model calibrated based on st6 physics obtained high correlation coefficients above 0 95 only for saral they were smaller and in the coastal zone the correlation coefficient was above 0 75 keywords swan wave conditions numerical model satellite data baltic sea data availability the authors do not have permission to share data 1 introduction wave models reflecting the state of the sea is of fundamental importance for forecasting the phenomenon with particular utility on identifying extreme situations it is important for forecasts in warning systems especially in the currently observed intensification of extreme phenomena both their frequency and intensity wolski and wisniewski 2021 the purpose of forecasting sea states and warning against storm phenomena is to ensure the safety of the population in coastal areas and sea shipping the verified hydrodynamic models are the basis of reliable forecasts enabling spatial forecasting of waves covering large sea areas thus constituting an alternative to point measurements the application of which taking into account the dynamic nature of the phenomenon has a limited range wave models are also an important tool used to analyse the wave climate of a given area which is an important factor that often determines the design and operation of venturesin the offshore and nearshore zones spectral wave models can also be the basis for long term forecasts of the wave regime associated with climate change mäll et al 2020 which can be the basis for designing adaptation measures one of the generally available models is swan simulating waves nearshore used in many sea basins in the world this applies to semi closed ones the mediterranean sea amarouche et al 2019 the yellow sea wang et al 2018 the black sea myslenkov 2016 the sea of marmara kutupoğlu 2018 the sea of beaufort hoque et al 2019 this also applies to open areas such as the north sea de león et al 2018 this model has also found application in the baltic sea björkqvist et al 2020a b medvedeva et al 2015 reda and paplinska 2002 a correctly calibrated and verified swan model can be a tool for calculating the energy obtained from sea waves chen et al 2021 and with empirical corrections swan can help simulate overtopping lashley et al 2020 the swan model is also used in forecasts and warnings of floods and surge waves khalid 2020 both the research conducted so far and the forecasts of changes in the wave regime indicate the need to extend the study and deepen the knowledge in the field of waves with particular emphasis on proper forecasting with reliable models a numbers of factors determine the modelled forecasting results the key issue is implementing a wave model of reliable meteorological data as an element driving the phenomenon soomere 2011 but also other input data such as bathymetric coverage sea level and initial and boundary conditions introduced into the model are also crucial björkqvist 2018 räämet et al 2010 adjusting these parameters at the calibration stage generates simulations most consistent with the actual conditions amarouche et al 2019 each model especially the one that is to be used for operational purposes must be validated and constantly monitored validation using all the formulas of the model s third generation allows for the qualitative selection of the most compatible wave properties for the selected body of water hoque 2019 vieira et al 2020 xu 2020 björkqvist et al 2018 dreier et al 2021 and many others variable conditions of the atmosphere and hydrosphere which are the result of among other things climate changes affect the variability and the possibility of underestimating the results of the wave model in most cases simulations of the wave model forced by the corrected wind geostrophic capture all important wave events and their duration räämet et al 2010 however the maximum wave heights are slightly underestimated during some storms and in highly variable wind conditions such mismatches in the time series of the measured and modelled wave height are found in the modelling of wave conditions in the baltic sea and force the continuous verification of the quality of the generated forecasts and in the case of an identified non compliance re validation of the model the analysis of the literature shows that in the baltic sea areas there is significant spatial variability of sea states associated with seasonality e g miętus and storch von 1997 cieślikiewicz and paplinska swerpel 2008 kriezi and broman 2008 räämet and soomere 2010 the average monthly wave heights and their extremes in the fall winter period may even be twice as high as spring summer months tuomi et al 2011 the reconstructed long term wave conditions show that the highest values 99th percentile of the significant wave height occur in the baltic proper in particular in the gotland basin over 4 m and the lowest 1 75 2 m in the gulf of bothnia the gulf of riga and the gulf of finland räämet et al 2010 björkqvist et al 2018 as in the case of the significant wave height the highest mean values and the 99th percentile for the peak wave period were found in the baltic proper where the mean values are 5 6 s and the maximum value is 10 s the average peak period in the pomerania bay and gulf of gdańsk is 4 5 s while in the gulf of riga bothnia and finland bays 5 6 s all the storms where significant wave height exceeds 7 m occurred from september to april and the highest were found in the southern part of the baltic proper björkqvist et al 2018 the fetch size is an important factor in shaping the wave conditions in a given area the occurrence of above average high waves in the southern baltic region is associated with relatively frequent strong winds gale and a large fetch which results from the longitudinal extent of the baltic sea over 600 km in the baltic proper and the latitudinal extent approx 350 km of unlimited span in the south baltic area the ice cover is an obstacle that suppresses waves and limits the fetch vihma and haapala 2008 still recent years indicate a significant reduction in the extent of the ice schwegmann 2021 and it has been invisible in the southern baltic region for several years the main factor determining the wave regime is wind conditions more than half of all winds over the baltic sea area come from sectors 180 270 deg jaagus and kull 2011 the wind speed has a distinct annual cycle with values above the average occurring in the autumn and winter the average yearly wind speed ranges from 6 to 8 m s 1 and the monthly values vary by a maximum of 1 5 m s 1 alari 2013 in the baltic wind speed extremes are caused by non tropical cyclones where winds can reach over 30 m s 1 suursaar and kullas 2009a these winds are usually associated with high storm hazards storm surges and coastal erosion despite numerous studies of the wave regime of the baltic sea including the application of various models swan gröger et al 2021 myslenkov et al 2016 kumara 2020 wam kanarik et al 2021 mike sokolov and chubarenko 2020 wwiii björkqvist et al 2020 the southern baltic area remains relatively poorly covered in terms of verified spatial wave forecasts although the wave properties of the baltic sea and their temporal changes have been the subject of research based on both measurements and numerical simulations conducted by several authors e g sokolov and chubarenko 2020 kanarik et al 2021 bonaduce et al 2019 räämet et al 2010 wrang et al 2021 responding to the need to implement a reliable forecasting system based on the coupling of the wave model with the best suited meteorological model we researched to implement the swan wave model in the southern baltic area to work in an operational mode institute of meteorology and water management national research institute imwm nri acts as a hydrological and meteorological service in marine areas among the many forecasts wave forecasting is crucial for many recipients including the population of coastal towns shipping tourists water sports enthusiasts coastal protection organizations rescue teams and investors working in coastal or offshore zones 2 wave model description and setup 2 1 wave model description due to the lack of currently operating wave models in the south baltic sea and the role of imwm nri we undertook research to develop a verified model for operational work that will forecast wave conditions for 72 h the maximum time of a reliable cosmo forecast the model configuration takes into account various formulas including st6 physics which was only released from swan 41 31 a swan the climate change initiative coastal sea level team 2020 and has already been used for analyses for closed seas such as the black sea aydoğan and ayat 2021 and open water bodies north sea fernández et al 2021 data from the cosmo model consortium for small scale modeling were used as the wind field data sources model validation was carried out in two stages first selecting the most optimal formula from among westhuysen komen janssen st6 and then a multi stream comparison of the model results using the formula chosen based on measurement data satellite data and other models used in this area the swan simulating waves nearshore numerical model developed by delft university of technology was used to simulate wave conditions in the south baltic area from 2018 to 2020 resulting from the availability of archived input data the model is based on a fully spectral representation of equilibrium formulas considering all physical processes no restrictions are placed on the spectral evolution which makes the model a third generation model booij et al 1999 the swan model solves the equation of the equilibrium density of action in time space geographic space and spectral space the action density is n e σ where e represents the energy density and σ is the relative frequency the superior equation is described by the formula 1 1 1 1 t n x c x n y c y n σ c σ n θ c θ n s σ where x y are horizontal cartesian coordinates t is time θ is the direction of propagation of each wave component cx cy cσ cθ are the speed of propagation in the spaces x y σ and θ respectively s is a source term for energy density which includes generation scattering and nonlinear wave interactions the first term on the left side of the equation 1 1 is an expression of the change in the density of action over time the second and third parts are the propagation across the action of physical space the fourth and fifth terms show the relative frequency shift and wave refraction caused by changes in depth and currents liang et al 2019 reda and paplińska 2002 qin et al 2005 spectral models need to be calibrated to compensate for the difference between the actual measurement and the simulated value suursaar and kullas 2009a 2009b suursaar 2010 by calibration the model reproduces not only basic wave statistics but also time series of wave properties including distant rise and extreme wave conditions in verified zones zaitseva pärnaste et al 2009 the computational domain of the model is determined by the range of input meteorological data from the cosmo model which are generated in the form of operational forecasts developed for the hydrological and meteorological service carried out by the institute of meteorology and water management national research institute the domain covers the area of the baltic proper the south baltic and partially the danish straits kattegat fig 1 a the computational grid was orthogonal curvilinear where the grid size varied from 0 1 km near shore hel peninsula to 7 km in the central baltic proper a curvilinear orthogonal mesh with a lower resolution in coastal zones and higher resolution in deepwater zones enables accurate and optimally fast wave forecast simulations whitney 2003 to verify the model data with measurement and satellite data simulations were carried out for selected points of location of measuring devices and common points in the orbits of satellite passages model simulations were also carried out for eight points located in the southern baltic and partially in the baltic proper with a time step resolution of 30 min to develop the wave characteristics in the baltic area covered by the model domain the main grids of the model are computational wind and bathymetric fig 1b 2 2 model settings and boundary conditions the computational domain of the model is the area close to the baltic proper this is due to the range of the wind field data of the cosmo model which at that time covered the analysed area cosmo 2018 the computational grid is orthogonal curvilinear grid size 0 1 7 km frequency discretisation is constant at 36 frequencies between 0 05 and 1 hz and for the directional discretisation 36 directions were considered in all runs the directional discretisation is equal to 36 directions considered in all runs the wind scaling for the formulas of komen janssen and westhuysen is described by the formula u 28u and in the formula st6 u swsu where sws is a free parameter swan team recommends using sws 28 in the case of st6 a it is 32 and in the case of st6 b it is 28 moreover according to the calibration variants swan team 2020 the st6 a formula used the counter bias factor in the input wind field debias 0 89 boundary conditions at the open borders the northern and western boundaries are created in the initial simulations using commercial database emcwf and the mars data archive ecmwf 2022 with a spatial resolution of 0 1 0 1 lat long in subsequent simulations conditions from model excitations using the hotstart function are used where the initial wave field is read from a file if the previous run were nonstationary the time found on the file would be assumed to be the initial computation time it can also be used for stationary computation as a first guess the computational grid both in geographical and spectral space must be identical to the one in the run in which the initial wave field was computed swan the climate change initiative coastal sea level team 2020 2 3 wave wave interactions depth limited breaking and bottom friction the quadruplet are modelled by the discrete interaction approximation dia swan team 2020 fully explicit computation of the nonlinear transfer with dia per sweep the coefficient for quadruplet configuration is equal to 0 25 proportionality coefficient for quadruplet interactions equal to 3 107 and coefficients for shallow water scaling csh1 csh2 csh3 respectively are 5 5 0 8333 1 25 nonlinear triad interaction is turned off since their effect is minor christakos et al 2021 in the analysed area the battjes and janssen 1978 formula represents the depth limited wave breaking the bottom friction is modelled using the jonswap formula and as recommended by swan team 2020 is 0 038 2 4 whitecapping 2 4 1 komen and janssen the values of the tunable coefficients cds and exponent p in this model have been obtained by komen et al 1984 and janssen 1991 by closing the energy balance of the waves in idealized wave growth conditions both for growing and fully developed wind seas for deep water the default whitecapping setting is the komen formula proposed by komen et al in 1984 this formula was also later presented by among others the swan team 1 2 s s d σ θ c d s 1 δ δ k k s s p m p σ k k e σ θ where cds 2 36 10 5 δ 1 and p 4 for komen formula and cds 4 10 10 5 δ 0 5 and p 4 for janssen formula the mentioned parameters are calibration and the remaining ones mean k is the wavenumber e is the energy of the wave spectrum and is a mean spectral steepness and are the mean wavenumber and the mean circular frequency this corresponds to the mean spectral steepness of a pierson moskowitz spectrum the calibration parameters were adopted according to the default ones proposed by the swan team 2020 2 4 2 westhuysen an alternative formula to komen is westhuysen et al 2007 which expresses whitecapping without the dependencies which are problematic in mixed wind sea and swell conditions this method is based on experimental results showing that whitecapping is associated with the nonlinear hydrodynamics within wave groups the formula is as follows 1 3 s b r σ θ c d s b k b r p 2 tanh k d 2 p 0 4 g k e σ θ where d is the water depth g is the acceleration due to gravity b k is the azimuthally integrated spectrum saturation and is referred to as a formula e σ k 3 c g where the last parameter is the wave group velocity br 1 75 10 3 is a threshold saturation level the proportionality coefficient is set to cds 5 10 5 when b k br waves break and the exponent p is equal to a calibration parameter p0 for b k br there is no breaking but some residual dissipation proved necessary this is obtained by setting p 0 a smooth transition between these two situations is achieved by alves and banner 2003 2 4 3 st6 the whitecapping is the sum of two dissipation components t1 1 5 which is the inherent break related to instabilities of wave and t2 1 6 which describes the dissipation of shorter waves triggered by longer breaking waves 1 4 s d s σ θ t 1 σ t 2 σ e σ θ 1 5 t 1 σ a 1 a σ σ 2 π e σ e t σ e t σ p 1 1 6 t 2 σ a 2 σ 1 σ a σ 2 π e σ e t σ e t σ p 2 d σ the calibration parameters are the a1 a2 p1 p2 coefficients rogers et al 2012 these four factors determine the different scattering shapes of t1 and t2 the relative size of a1 and a2 is set to achieve the desired ratio of t1 and t2 at large wave age values at moderate wind speeds the power coefficients p1 and p2 control how strongly the dissipation term reacts to the threshold exceedance the st6 does not have a default parameterisation and the swan team recommends using five selected examples variants of settings swan the climate change initiative coastal sea level team 2020 among the variants the settings containing the fan wind resistance formula which uses an iterative procedure for estimating based on the actual seas state were eliminated the fan formula is a function of wave age and wind speed and may cause a decrease in the obtained significant wave height values swan the climate change initiative coastal sea level team 2020 from the remaining variants among others a set named in the work st6 a which is commonly used and recommended aydoğan and ayat 2021 day et al 2021 and comparative st6 b in which the values of a1sds and a2sds differ significantly and in which wscaling is 28 according to with the recommendations of the swan team 2020 table 1 2 5 wind data the key issue in modelling wind waves in sea areas especially those characterized by complex geometry and high coastal cliffs is the correct selection of atmospheric circulation data soomere et al 2011 model wind input data even those covering a widely known area mainly represent the properties of offshore winds and may reveal mismatch compared to measurement data broman et al 2006 soomere 2008 and may deviate from other modelled predictions keevallik and soomere 2010 this inconsistency also occurs in reproducing wave fields using wave models based on scaled wind data räämet et al 2010 geostrophic wind field data tends to underestimate the actual wind effect on sea surfaces soomere et al 2011 the sensitivity analysis carried out by nikishova et al 2017 indicates that the wind direction by which the model is assimilated to the model significantly affects the simulation results we chose the cosmo model cosmo 2018 to power the swan model because the data from this model is also generated by imwm nri imwm nri department of numerical forecasts cosmo which guarantees constant access to verified and reliable data enabling the operational use of the model cosmo is a non hydrostatic atmosphere forecasting model designed for operational work on an available resolution scale of 7 km this model s initial and boundary conditions are implemented using the gme generic modeling environment and ifs internal family systems model processes cosmo 2011 the implemented data relates to data with a time step of 1 h and a spatial resolution of 7 km 160 120 knots and is in the form of a regular orthogonally grid the range of the wind fields covers the southern baltic and the baltic proper the implemented data is a forecast of wind direction and speed thus containing u v longitudinal and latitudinal components and show high compliance of the estimates with real conditions linkowskawyszogrodzki 2014 2 6 bathymetry and disturbances the basic input data in hydrodynamic modeling are bathymetric and orographic data this type of fed data to the model plays a key role in forecasting wave growth and breaking by taking into account the presence of wind and other forms of seabed relief in the bathymetric coverage the model does not take into account the ice cover because the southern baltic is rarely covered with ice sztobryn et al 2012 as in 2018 2020 it did not occur in the area covered by the modeling stanisławczyk 2020 2021 moreover the bottom cover with phytobenthic organisms and hydrotechnical objects was not taken into account as their influence is insignificant in relation to the resolution in which the simulations were carried out the bathymetric grid was created by combining two data sets the first with a resolution of 500 500 m baltic sea hydrographic commission 2013 covers the entire computing domain the second data set polish eez obtained from hydrographic office of the polish navy was filling the first area due to its resolution which is differentiated but not greater than 100 m detailed bathymetry especially in coastal areas is crucial for correct wave simulation nikishova et al 2017 3 measurement and satellite data 3 1 wave measurements the measurement network of the south baltic sea is underdeveloped as there are currently only a few measurement points approx 3 two of which have been included in the analysis fig 1a data from measurements carried out with three devices located in both nearshore and offshore areas were used to validate the model and characterize the wave conditions fig 1a in the pomeranian bay measurements are carried out using the nemo wpa waves processing array nemo teledyne 2008 device belonging to the maritime office in szczecin and is installed at a depth of approximately 7 m in the open sea area two devices owned by the institute of meteorology and water management national research institute are installed on the petrobaltic platform one of the devices awac acoustic wave and currents profiler nortek 2017 is located on the bottom 80 m and performs measurements using the adcp technique with a frequency of 400 khz and 1 5 hz sampling of the surface elevation the second measuring device is a waveguide radac 2020 installed at 30 m on the petribaltic platform waveguide is carried out by the fmcw frequency modulated continuous wave radar sensor with a frequency of 10 hz all devices allow the measurement of parameters significant wave height hs maximum wave height hmax mean wave period tm peak wave period tp and wave direction the analysis used measurements from the 2018 2020 period with a record time step resolution of 30 min to unify and synchronize the databases 3 2 model calibration the model was verified in two stages the first stage was to select the model s optimal parameterisation to obtain the estimated parameters hs significant wave height tm mean wave period that are closest to the real conditions the second stage is to carry out multi stream verification using the selected formula from the initial validation stage the initial validation of the model was based on calculations for four formulas with default parameterisation komena komen et al 1994 janssen 1991 westhuysen westhuysen et al 2007 and st6 physics rogers and in 2012 in the case of the st6 formula for which there are no default parameterisation settings the variant was analysed taking into account wind and debias scaling and different values of the a1sds a2sds p1sds and p2sds parameters eq 1 4 1 6 as the swan model is intended to be used in the operational mode the calculation time of the generated wave forecasts is essential based on the internal tests and conclusions of numerous authors a o aydoğan and ayat 2021 amarouche et al 2019 the simulations were performed with a time step of 30 min and three iterations the default settings were used for other variables including quadruplets triads depth induced wave breaking and bottom dissipation the model results were compared with the observations using time series fig 6 7 and statistical analysis table 3 the obtained values were compared statistically using the mean bias eq 3 1 which is a representation of the model s mean long term error where its value either indicates average overestimation positive or underestimation negative compared to the measurements bryant et al 2016 mean square error rmse eq 3 2 which depends on the measurement accuracy scale and is used to assess the ability of various models to predict a single variable the lower the rmse value the more accurate the model fit chai and draxler 2014 the scatter index si eq 3 3 which is a normalised measure of error lower values of the si are an indication of better model performance it is defined as the root mean square deviation divided by the mean of the observations mentaschi et al 2013 and the pearson correlation coefficient r eq 3 4 which is a measure of the degree of linear dependence between the model and the observations rogers et al 2012 a perfect positive linear relationship i e as the value of one variable increases the value of the other variable increases has a value of 1 0 while no linear relationship is indicated by a value of 0 0 bryant et al 2016 the purpose of interpreting the correlation results was the rule of thumb hinkle et al 2003 where very strong correlations are in the range 0 9 1 0 strong 0 7 0 9 moderate 0 5 0 7 low 0 3 0 5 negligible 0 0 0 3 positive or negative in the formulas below xm is the calculated value of the model xo is the actual measurement value and n is the number of data 3 1 b i a s x m x o 3 2 r m s e 1 n i 1 n x m x o 2 n 3 3 s i r m s e x m x o 3 4 r i 1 n x m x m x o x o i 1 n x m x m 2 i 1 n x o x o 2 3 3 verification and crosscheking of measurement data the key method of verifying the correctness of model forecasts compares their results with measurement data therefore the measurement data must be of high documented quality in our study we used data on the height of the significant wave from three measurement points two of them awac and waveguide located in one location in the deep water zone and nemo wpa located in the pomeranian bay fig 1a this allowed for data verification using cross checking however the first step was verifying each data series from three measurements based on internal procedures error codes a comparison of measurements from awac with measurements from the waveguide device was based on data from 2019 to 2020 with a harmonized measurement time step of 30 min data obtained from both devices under non storm conditions show a high agreement fig 2 the most considerable deviations were observed at a significant wave height above 4 7 m in which conditions awac did not register the measurement or recorded it incorrectly fig 2 specialists correctly calibrate both devices their complementary measures enable the subsequent use of the data for model verification this is due to an error that occurs when the projected matrix is too large for the peak wavelength nortek 2022 the comparison of data from awac and waveguide devices also included a compilation of statistical data for the significant wave height am median me lower quartile lq upper quartile uq minimum min and maximum max the mean and median values of the significant wave height determined for the years 2019 2020 were very similar fig 3 in 2019 the average values were identical 1 94 m in the median values the difference was only 0 01 m the range of hs values was 0 15 5 21 m for awac and 0 16 5 14 m for waveguide respectively in 2020 the differences between the individual parameters were only slightly greater even though awac did not register some high waves significant wave height varied from 0 11 m to 5 75 m for awac and 0 12 5 25 m for waveguide the average values were 1 99 m and 2 01 m respectively the comparison of the statistically significant wave height parameters determined for the months shows a very good agreement of the data from awac and waveguide fig 4 the largest differences between measurements of both devices occurred in june august and december for which the differences in the mean values were 0 28 and 0 22 respectively but a different period of data collection should be taken into account 3 years n 52560 awac and 2 years n 35040 waveguide monthly averaged significant wave heights equal to or greater than 2 m characterized the months from january to march and september to december the maximum average significant wave height values were recorded in both case in december 2 22 m awac and 2 24 m waveguide the lowest values were recorded in june 1 39 m awac and 1 11 m waveguide the highest values of the standard deviation marked with a frame were found in the autumn winter months i iii x and the lowest values in may and july fig 4 the obtained results clearly indicate a very good compatibility of the data from both devices confirming the very high quality of the data that can be used to verify the applied wave model the analyses showed very strong statistically significant relationships between the series of data from different sources the correlation coefficient eg 3 4 between the awac and waveguide measurement data was 0 986 p 0 005 as in the case of measurement data from two sources statistical parameters were determined on the basis of model data from the period 2018 2020 the obtained results show a very good agreement the mean annual hs values from measurements and model measurements were practically identical the differences between them did not exceed 0 05 m fig 3 the maximum values were slightly more differentiated and 0 5 m higher in the case of model data in each year of the study a similar agreement between the parameters describing the measurement and model data was obtained by comparing the hs values averaged for individual months again the differences between the mean and median values did not exceed 0 05 m as in the case of the measurement data the averaged hs values ranging from 2 00 m to 2 27 m characterized the months from january to march and september to december the minimum average value of hs was 1 43 m the range of maximum hs values determined with the use of the model was shifted towards higher values by approx 0 5 m in all months except january and august where the differences were lower 3 4 satellite data all satellite data used in this study were generated by altimeters installed onboard jason 2 cnes 2011 jason 3 cnes 2016 saral cnes 2021 sentinel 3a and 3 b satellites eumetsat 2017 in the period of three years 2018 2020 the jason 2 satellite launched in 2008 and scheduled for five years of duty finished mission on 1st october 2019 in june 2017 the degradation of critical onboard components and control systems required that jason 2 move from its original orbit with 127 revolutions within the 10 days cycle maneuvered into a slightly lower orbit away from functioning satellites jpl nasa 2017 in this new orbit jason 2 was collecting data along a series of very closely spaced ground tracks just 8 km apart to complete one cycle of these new ground tracks jason 2 required one year in analysed period as a result the ground track of its orbits was very dense compared to jason 3 launched in 2016 which has stable 127 revolutions within the 10 days cycle fig 5 the saral satellite with ka band altika instruments launched in 2013 has a repetition cycle of 35 days with 501 revolutions within the cycle with a drift of 1 2 km fig 5 the altika instrument working in ka band has a much smaller footprint than a jason poseidon instrument being more useful in the coastal regions finally sentinel 3a launched in 2016 and sentinel 3b launched in 2018 which have a repetition cycle of 27 days with 385 orbits per cycle fig 5 a comparison of the main features of used instruments is presented in table 2 only gdr geophysical data record data were used representing the best quality altimeter products in comparison to ogdr operational geophysical data records and igdr interim geophysical data records the following products were used jason 2 and jason 3 swh ku product ku band corrected significant wave height saral altica swh product ka band significant wave height sentinel 3 sr 2 lan dataset swh ocean 01 ku products corrected ocean significant wave height 1 hz ku band two ground measurements points over south baltic sea were used petro batlic 18 183 e 55 481 n and buoy at pomeranian bay 14 253 e 54 016 n the first one located at open sea is very useful for comparison with satellite altimeters due to better spatial homogeneity of swh the second one located about 7 km from the coast on relatively shallow water is more problematic the beam width of the antenna is 1 28 ku band so the electromagnetic pulse irradiates a disk approximately 20 km in radius for jason and 8 km for saral however for a flat marine surface the first returns will come from the sub satellite point the satellite altimeter measurements taken at high frequency 20 hz corresponds to a ground resolution of 300 m along track measurements but measurements located less than 20 km from the cost has reduced quality due to limitations with atmospheric correction climate change initiative coastal sea level the climate change initiative coastal sea level team 2020 the difference between ku band jason 2 and 3 sentinel 3a and 3 b and ka band saral altica measurements must be mentioned due to the higher frequency measurements in ka band are more sensitive to rainfall and thick clouds with large droplets which could be a source of the limited accuracy of altimeter measurements the significant wave height measurements are based on the slope of reflected echo at the leading edge robinson 2004 steeper slope smaller swh the difference in delay for reflections from wave crests and wave troughs affects the slope of the leading edge of the waveform σc in comparison to the original pulse width σp ray et al 2015 this measure includes both the width of the original pulse and the standard deviation of the delays associated with reflecting facets 2σh c where c is the speed of em waves as the standard deviation of surface elevation is equivalent to one quarter of the swh the wave height can be determined 2 s w h 2 4 c 2 s c 2 s p 2 for altimeters operating in sar mode sentinel 3 both the leading edge and the trailing edge of the waveform are affected by changes in swh all mentioned limitations have to be taken into account in comparing three datasets satellite altimetry measurements buoy measurements and model results 4 results and discussion 4 1 selecting the most optimal physics formula for the wave model the parameters of the significant wave height and the mean wave period were compared in a location representing deep water petrobaltic and shallow water pomeranian bay conditions for four formulas eq 1 2 1 4 of which two variants were taken into account for the st6 physics at the petrobaltic point the strongest correlation coefficients between the model and measure values of significant wave height were obtained for the formulas st6 a and st6 b table 3 and the weakest were recorded for the westhuysen formula for the significant wave height r 0 842 and for komen s in the case of the mean wave period r 0 744 bias for both hs and tm was the highest for westhuysen the lowest rmse values were shown by the formula st6 a and st6 b table 3 i e hs 0 108 st6 a hs 0 132 st6 b and tm 0 228 st6 a and tm 0 376 st6 b the calculations using the westhuysen formula hs 0 207 tm 0 199 were characterized by the highest value of the scatter index si while the lowest value was st6 a hs 0 032 tm 0 031 table 3 in the pomeranian bay the mean wave period shows a slightly lower degree of real wave reconstruction for all analysed formulas the lowest pearson correlation coefficient for the significant wave height was r 0 864 westhuysen and for the mean wave period r 0 652 westhuysen rmse ranged from 0 038 to 0 208 for hs and 0 059 0 197 for tm as in the petrobaltic point the highest values of bias and si both for hs and tm are shown by the westhuysen formula and the lowest values were recorded for st6 a and st6 b a comparative analysis was also carried out based on the reconstruction of the storm situation from 2 january 2019 taking into account the height of the significant wave the mean wave period and the direction of wave inflow figs 6 and 7 at the petobaltic point the highest difference with the measurement data for the significant wave height fig 6a was shown by simulations using the westhuysen formula where the maximum bias value exceeded 0 5 m the minor differences were shown by the formula st6 a st6 b for which the difference did not exceed 0 3 m in the case of the mean wave period fig 6b and the direction of undulations fig 6c the simulations using the st6 parameterisation a showed the most negligible differences at the point in the pomeranian bay as in petrobaltic the minor differences were shown by simulations using the st6 a formulas where the difference with the measurement data did not exceed 0 25 m for the significant wave height 0 4 s for the mean wave period and 12 for the wave direction both in the petrobaltic point fig 6 and in the pomeranian bay fig 7 the highest agreement of the analysed parameters was demonstrated by the st6 a formula and the lowest for westhuysen the results of both st6 physics formulas are similar and the debias parameter used in st6 a and the wind scaling difference variant a 32 0 variant b 28 0 play a minor role in the reliable reproduction of the wave conditions further multi stream verification included a model based on the best in terms of recreating wave conditions in the southern baltic selected st6 formula with the set windscaling equal to 32 0 a1sds 2 8e 6 a2sds 3 5e 5 p1sds and p2sds 4 and multiplier on drag coefficient of 0 89 4 2 multi stream verification of the model based on the st6 formula the multi stream model validation determines the quality of the forecasts and warnings issued in the south baltic area there are only a few devices that enable the measurement of selected wave parameters the advantage of which is the time resolution of the measurements while their disadvantage is their point character in our research we verified the swan model based on data from two points petrobaltic pomeranin bay which are characterized by different hydrodynamic and morphological conditions for these points measurement data awac waveguide nemo wpa and satellite data jason 2 jason 3 sentinel 3 a and b saral were obtained to compare the data generated by the swan model with measurement and satellite data the model point data for the location of measuring devices was archived satellite data was collected for the same locations model measurement and satellite data series were time integrated at the point representing deep water conditions table 4 fig 3 4 very strong above 0 9 correlations with the measurement data awac waveguide showed all data sources except for saral which showed a strong correlation 0 786 in shallow water conditions table 5 the saral satellite 0 93 showed a very strong correlation with the measurement and model data data from jason 3 above 0 7 showed a strong correlation while data from jason 2 and sentinel 3 showed low correlations 0 12 0 17 and were statistically insignificant satellite data is an important tool in the provision of spatial wave height information in the analysis with the use of satellite data jason 2 jason 3 saral and sentinel 3 a and 3 b the time approximation the time of the flight of the satellite in relation to the measurement time was approximated to 30 min and the spatial approximation approx 0 1 were used the results of the comparison in the open sea area indicate a very good agreement of the simulation results of the swan model and measurements with satellite data fig 8 the plotts fig 8a and b show the scattering of the significant wave height values obtained from the satellite data measurement data and the results of the swan model the completed circle marks the comparison of satellite and measurement data in the pomeranian bay from the nemo wpa device in petrobaltic from the awac device a non filled circle marks the comparison of all satellite missions with the results of the swan model both the correlations table 4 table 5 and the scattering fig 8 show a very good agreement between the satellite data and the results of the swan model in deep water conditions petrobaltic fig 8a fig 9 a in shallow water conditions pomeranian bay the correlations were slightly weaker table 5 compared to those obtained at the petrobaltic point and the dispersion was less concentrated see fig 10 the best compliance of the measurement data awac with the satellite data was obtained for sentinel 3 r 0 999 and jason 2 r 0 992 and the worst for saral r 0 786 table 4 the satellite data also showed a high correlation of the significant wave height with the values obtained as a result of the simulations the best agreement was obtained for the data from sentinel r 0 988 the worst for saral r 0 764 the largest hs difference of 0 5 m was identified in the case of jason 3 the maximum differences for the other satellites did not exceed 0 4 m jason 2 0 2 m sentinel 3 0 1 m saral 0 34 m the comparison with the use of satellite data was also made for the point in the pomeranian bay figs 8b and 9b 4 3 characteristics of wave conditions in the period 2018 2020 the conducted studies aimed at verifying the results of simulations obtained with the use of the swan model in the areas of the southern baltic and its application in operational mode was supplemented by the reconstruction of wave conditions in the southern baltic area for the period 2018 2020 fig 11 table 6 the simulations were performed with a time step of 30 min and they included key wave parameters such as significant wave height mean wave period mean wave direction mean wave height mean wave crest length and mean wave steepness table 6 the reconstruction was carried out for eight points located in areas with different hydrodynamic characteristics p1 arkona deep p2 pomeranian bay p3 p4 bornholm basin p5 gdańsk bay p6 gotland basin petrobaltic p7 near the curonian spit p8 the gotland deep fig 11 for individual points the mean values of all parameters were determined for the entire period covered by the calculations and divided into calm iv viii and stormy ix iii periods the measured significant wave height is the average height of one third of all waves in modelling the significant wave height can be calculated as four times the zeroth moment of the process bai and bai 2014 the lowest average significant wave height hs values were characteristic for points p2 pomeranian bay and p5 gulf of gdańsk which are bay areas with an extensive coastline and reduced exposure to wave run moreover these points are characterised by the smallest depth among all analysed points i e p2 7 m below sea level p5 41 m below sea level the highest mean value of the significant wave height was recorded in the gotland deep p8 1 04 m and the lowest for the pomeranian bay p2 0 53 m the average values in these areas in the non storm period were 0 74 m and 0 44 m respectively and in the storm period 1 28 m and 0 60 m table 6 fig 11 the mean wave period which expresses the time between two wave crests or troughs is one of the main parameters describing the wave phenomenon the size of the wave period is mainly determined by the effects of wind and pressure on the sea surface the run up and depth of the seabed and geomorphological structures in the coastal zone that determine wave breakage the highest and the lowest mean values of the wave period as in the case of the significant wave height occurred in the same locations and amounted to 3 76 s at p8 and 2 60 s at p2 in the no storm season the highest average value was recorded in the gotland basin p6 3 23 s and the lowest in the pomeranian bay p2 2 43 s in the stormy season the highest values were determined for p6 4 00 s and p8 4 17 s and the lowest for p2 2 74 s low values of the mean wave period occur mainly in shallow water areas pomeranian bay gulf of gdańsk central coast of poland and also in the vicinity of the curonian spit in the non storm season table 6 these areas are characterised by relatively shallow depths and reduced exposure to winds especially from the south east in calculating the direction of wave propagation the energy spectrum plays a critical role which is a component of frequency and direction kuik et al 1998 westerly winds prevail on the polish coast richling et al 2005 in 2018 the winds blowing from the southern sector prevailed in the open sea zone biernacik zalewska ed 2020 the transfer of wind energy to the water surface determines the similar direction of the wind wave in the open parts of the analysed area the dominant direction from which the waves came in all locations was se and sw in the no storm period it was in the range of 184 8 225 4 and in the storm period 161 0 198 2 the pomeranian bay p2 the gulf of gdańsk p5 and the bornholm basin p3 p4 are characterised by the presence of dominant south west and west winds table 6 fig 11 this direction is the same as the wave propagation direction zeidler 1992 a similar wave propagation direction distribution occurred from 1970 to 2007 where the se and s directions dominated räämet et al 2010 the height of the swell wave is related to the low frequency partition of the spectrum and occurs mainly as a result of long lasting wind from one direction and as a post storm extinction wave the direction of the dead wave propagation is constant and unlike the wind wave it is not related to the wind direction these waves are characterised by an above average length and a short period of time therefore they carry less energy making them disperse relatively quickly druet 2000 the size of the swell wave height is mainly due to the depth and run up i e the continuous distance of open water on which the wind blows without a significant change of direction hence the highest values of the mean wave height were recorded in the gotland basin p6 and the gotland deep p8 the wave height values were negligible and did not exceed 0 08 in annual terms in the non storm season they remained in the range of 0 01 0 03 and in the storm free season the highest average value was 0 12 m table 4 the wavelength is the distance between the two crests or troughs of the wave which is closely related to the calculated mean wavelength parameter using the linear dispersion relationship the average length of the wave crest ranged from 8 74 to 20 4 m in the stormy period the maximum value was 24 7 m table 6 the constant north east movement of areas with increased storm activity increased the number and size of storms in the baltic sea especially in winter and spring bacc author team 2008 the steepness of a wave describes the ratio of the wave s height to its crest s length wave steepness is an important wave parameter that influences whitecapping i e wave breakage caused by steepness in deep water regions where the wave height then becomes too high compared to the wavelength and in shallow water conditions plays a vital role in the three wave interaction the model used the non breaking dissipation formula of ardhuin ardhuin et al 2010 the steepness values in the whole studied area are even and amount to 0 5 0 6 table 6 this may be due to the even ratio of the significant wave height and the wave crest length 5 conclusions the swan model verified with various tools in different depth zones both during storms and stable sea states allows with the combination of cosmo data a qualitative wave forecast of 72 h calculated in a short time approx 35 min the key element of the model is its operational mode and thanks to the cosmo data processed and automatic transfer data inside the institution a continuous forecast is provided the conducted studies aimed at implementing the swan model for operational work confirmed its applicability in the areas of the baltic sea designated by the domain of meteorological data generated by the cosmo model the initial validation of the models made it possible to select a formula that enables the best representation of the real wave conditions in different sea depth zones in a detailed multi stream verification the simulation results of the swan model were subjected to the use of physics st6 which unlike the other formulas showed the lowest values of bias rmse and si and one of the highest values of the correlation coefficient the verification of the model simulations included the comparison of the significant wave height in the period from 2018 to 2020 the use of measurement data was preceded by data quality control through internal algorithms and the comparison of data measured at one point using two different measurement systems a statistically significant correlation was demonstrated for the measurement data from awac and waveguide devices characterized by a very high correlation coefficient r 0 986 p 0 005 the correlation of the model data generated for the measurement location point in the open sea area showed a very good agreement which was reflected in a statistically significant correlation confirmed by the correlation coefficient at the level of 0 973 only a slightly worse fit r 0 968 p 0 05 was shown by the model data concerning the measurement data in the coastal areas pomeranian bay the use of satellite data for model verification can be a very good alternative to measurement data the resources of which are limited in the open sea areas correlation coefficients above 0 96 were determined for jason 2 and 3 and sentinel data in the case of saral the relationship between the model and satellite data was slightly weaker r 0 764 in coastal areas the relationships were weaker and statistically significant correlations were found only for saral r 0 938 and jason 3 r 0 704 the poorer compatibility of the model data with satellite data in coastal areas is due to the finite resolution which may cause a difference in the observation location and the closest grid point for which wave properties are calculated soomere et al 2011 the observed more significant discrepancies between the model measurement and satellite data indicate the influence of a much larger number of factors such as bathymetry shape and influence of the shoreline on the wave conditions in coastal areas this points to the need to develop models with better resolution in the edge areas based on the data generated using the verified swan model for selected points a multi point reconstruction of the wave conditions in the baltic area was performed the wave parameters were determined i e significant wave height mean wave period wave propagation direction mean wave height mean wave crest length and mean wave steepness the obtained reconstruction values agree with the results of other studies lit they indicate both the potential for renewable energy dominance of the wave direction ssw and se and the potential threats to coastal areas and shipping the method of reconstructing wave conditions can be an essential tool in planning investments and works in the offshore and nearshore zones the areas with the lowest wave dynamics are zatoka pomorska and zatoka gdanska where the average annual significant wave height does not exceed 0 67 m the wavelength is 12 06 m and the average wave period is in the range of 2 60 2 98 s gotland deep is characterized by the highest annual mean wave height 1 04 m implementation of the swan model fed with meteorological data from the cosmo model and multi stage verified with data from different sources delivers the possibility for the swan model application in other waters bodies with similar geomorphology and meteorological conditions creating a verification system based on both measurement and satellite data which should be a permanent and integral part of forecasting wave conditions in sea areas seems to be the optimal system for generating reliable wave forecasts which can be the basis of warning systems 6 software and data availability the third generation wind wave model swan is available freely at http swanmodel sourceforge net download download htm in this study we performed the multi stream analysis of the quality of forecasts of the model version 41 31 2019 functioning in the operational mode in various sea states and different depth conditions examples of wind data 1 02 mb and and selected sections of code not covered in detail in the article can be found at https github com patryksapiega swan in addition there are programming rules scientific and technical documentation and user manual with code all 1 77 mb the other model settings are described in the article this distribution may be implemented on microsoft windows linux unix and macos provided a fortran90 compiler is available the 64 bit version requires an x64 edition of windows 7 8 1 or 10 the 32 bit executable can be run on both 32 bit and 64 bit operating systems declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like to thank the team of the meteorological modeling center cosmo numerical forecasting department for the preparation of chip field data and the maritime office in szczecin for providing data from the pomeranian bay all data used in the analysis and supporting the conclusions in this manuscript can be obtained from the institute of meteorology and water management poland e mail patryk sapiega imgw pl any trade firm or product name is for descriptive purposes only and does not imply endorsement by the imwm nri 
25424,the aim of the research was to implement the swan wave model powered by meteorological data from the cosmo model in the baltic area with particular emphasis on the southern regions where the swan model is operational in the validation of the model the predictive performance of the formulas was assessed westhuysen komen janssen st6 the verification of wave forecast quality obtained with the spectral model was carried out in two stages the first step involved selecting the best formula using the adopted parameterizations based on short term measurement data after choosing the best physics formula st6 the model results were verified with multi stream measurement and satellite data at two points representing the shallow and deepwater zones in the open sea zone the model calibrated based on st6 physics obtained high correlation coefficients above 0 95 only for saral they were smaller and in the coastal zone the correlation coefficient was above 0 75 keywords swan wave conditions numerical model satellite data baltic sea data availability the authors do not have permission to share data 1 introduction wave models reflecting the state of the sea is of fundamental importance for forecasting the phenomenon with particular utility on identifying extreme situations it is important for forecasts in warning systems especially in the currently observed intensification of extreme phenomena both their frequency and intensity wolski and wisniewski 2021 the purpose of forecasting sea states and warning against storm phenomena is to ensure the safety of the population in coastal areas and sea shipping the verified hydrodynamic models are the basis of reliable forecasts enabling spatial forecasting of waves covering large sea areas thus constituting an alternative to point measurements the application of which taking into account the dynamic nature of the phenomenon has a limited range wave models are also an important tool used to analyse the wave climate of a given area which is an important factor that often determines the design and operation of venturesin the offshore and nearshore zones spectral wave models can also be the basis for long term forecasts of the wave regime associated with climate change mäll et al 2020 which can be the basis for designing adaptation measures one of the generally available models is swan simulating waves nearshore used in many sea basins in the world this applies to semi closed ones the mediterranean sea amarouche et al 2019 the yellow sea wang et al 2018 the black sea myslenkov 2016 the sea of marmara kutupoğlu 2018 the sea of beaufort hoque et al 2019 this also applies to open areas such as the north sea de león et al 2018 this model has also found application in the baltic sea björkqvist et al 2020a b medvedeva et al 2015 reda and paplinska 2002 a correctly calibrated and verified swan model can be a tool for calculating the energy obtained from sea waves chen et al 2021 and with empirical corrections swan can help simulate overtopping lashley et al 2020 the swan model is also used in forecasts and warnings of floods and surge waves khalid 2020 both the research conducted so far and the forecasts of changes in the wave regime indicate the need to extend the study and deepen the knowledge in the field of waves with particular emphasis on proper forecasting with reliable models a numbers of factors determine the modelled forecasting results the key issue is implementing a wave model of reliable meteorological data as an element driving the phenomenon soomere 2011 but also other input data such as bathymetric coverage sea level and initial and boundary conditions introduced into the model are also crucial björkqvist 2018 räämet et al 2010 adjusting these parameters at the calibration stage generates simulations most consistent with the actual conditions amarouche et al 2019 each model especially the one that is to be used for operational purposes must be validated and constantly monitored validation using all the formulas of the model s third generation allows for the qualitative selection of the most compatible wave properties for the selected body of water hoque 2019 vieira et al 2020 xu 2020 björkqvist et al 2018 dreier et al 2021 and many others variable conditions of the atmosphere and hydrosphere which are the result of among other things climate changes affect the variability and the possibility of underestimating the results of the wave model in most cases simulations of the wave model forced by the corrected wind geostrophic capture all important wave events and their duration räämet et al 2010 however the maximum wave heights are slightly underestimated during some storms and in highly variable wind conditions such mismatches in the time series of the measured and modelled wave height are found in the modelling of wave conditions in the baltic sea and force the continuous verification of the quality of the generated forecasts and in the case of an identified non compliance re validation of the model the analysis of the literature shows that in the baltic sea areas there is significant spatial variability of sea states associated with seasonality e g miętus and storch von 1997 cieślikiewicz and paplinska swerpel 2008 kriezi and broman 2008 räämet and soomere 2010 the average monthly wave heights and their extremes in the fall winter period may even be twice as high as spring summer months tuomi et al 2011 the reconstructed long term wave conditions show that the highest values 99th percentile of the significant wave height occur in the baltic proper in particular in the gotland basin over 4 m and the lowest 1 75 2 m in the gulf of bothnia the gulf of riga and the gulf of finland räämet et al 2010 björkqvist et al 2018 as in the case of the significant wave height the highest mean values and the 99th percentile for the peak wave period were found in the baltic proper where the mean values are 5 6 s and the maximum value is 10 s the average peak period in the pomerania bay and gulf of gdańsk is 4 5 s while in the gulf of riga bothnia and finland bays 5 6 s all the storms where significant wave height exceeds 7 m occurred from september to april and the highest were found in the southern part of the baltic proper björkqvist et al 2018 the fetch size is an important factor in shaping the wave conditions in a given area the occurrence of above average high waves in the southern baltic region is associated with relatively frequent strong winds gale and a large fetch which results from the longitudinal extent of the baltic sea over 600 km in the baltic proper and the latitudinal extent approx 350 km of unlimited span in the south baltic area the ice cover is an obstacle that suppresses waves and limits the fetch vihma and haapala 2008 still recent years indicate a significant reduction in the extent of the ice schwegmann 2021 and it has been invisible in the southern baltic region for several years the main factor determining the wave regime is wind conditions more than half of all winds over the baltic sea area come from sectors 180 270 deg jaagus and kull 2011 the wind speed has a distinct annual cycle with values above the average occurring in the autumn and winter the average yearly wind speed ranges from 6 to 8 m s 1 and the monthly values vary by a maximum of 1 5 m s 1 alari 2013 in the baltic wind speed extremes are caused by non tropical cyclones where winds can reach over 30 m s 1 suursaar and kullas 2009a these winds are usually associated with high storm hazards storm surges and coastal erosion despite numerous studies of the wave regime of the baltic sea including the application of various models swan gröger et al 2021 myslenkov et al 2016 kumara 2020 wam kanarik et al 2021 mike sokolov and chubarenko 2020 wwiii björkqvist et al 2020 the southern baltic area remains relatively poorly covered in terms of verified spatial wave forecasts although the wave properties of the baltic sea and their temporal changes have been the subject of research based on both measurements and numerical simulations conducted by several authors e g sokolov and chubarenko 2020 kanarik et al 2021 bonaduce et al 2019 räämet et al 2010 wrang et al 2021 responding to the need to implement a reliable forecasting system based on the coupling of the wave model with the best suited meteorological model we researched to implement the swan wave model in the southern baltic area to work in an operational mode institute of meteorology and water management national research institute imwm nri acts as a hydrological and meteorological service in marine areas among the many forecasts wave forecasting is crucial for many recipients including the population of coastal towns shipping tourists water sports enthusiasts coastal protection organizations rescue teams and investors working in coastal or offshore zones 2 wave model description and setup 2 1 wave model description due to the lack of currently operating wave models in the south baltic sea and the role of imwm nri we undertook research to develop a verified model for operational work that will forecast wave conditions for 72 h the maximum time of a reliable cosmo forecast the model configuration takes into account various formulas including st6 physics which was only released from swan 41 31 a swan the climate change initiative coastal sea level team 2020 and has already been used for analyses for closed seas such as the black sea aydoğan and ayat 2021 and open water bodies north sea fernández et al 2021 data from the cosmo model consortium for small scale modeling were used as the wind field data sources model validation was carried out in two stages first selecting the most optimal formula from among westhuysen komen janssen st6 and then a multi stream comparison of the model results using the formula chosen based on measurement data satellite data and other models used in this area the swan simulating waves nearshore numerical model developed by delft university of technology was used to simulate wave conditions in the south baltic area from 2018 to 2020 resulting from the availability of archived input data the model is based on a fully spectral representation of equilibrium formulas considering all physical processes no restrictions are placed on the spectral evolution which makes the model a third generation model booij et al 1999 the swan model solves the equation of the equilibrium density of action in time space geographic space and spectral space the action density is n e σ where e represents the energy density and σ is the relative frequency the superior equation is described by the formula 1 1 1 1 t n x c x n y c y n σ c σ n θ c θ n s σ where x y are horizontal cartesian coordinates t is time θ is the direction of propagation of each wave component cx cy cσ cθ are the speed of propagation in the spaces x y σ and θ respectively s is a source term for energy density which includes generation scattering and nonlinear wave interactions the first term on the left side of the equation 1 1 is an expression of the change in the density of action over time the second and third parts are the propagation across the action of physical space the fourth and fifth terms show the relative frequency shift and wave refraction caused by changes in depth and currents liang et al 2019 reda and paplińska 2002 qin et al 2005 spectral models need to be calibrated to compensate for the difference between the actual measurement and the simulated value suursaar and kullas 2009a 2009b suursaar 2010 by calibration the model reproduces not only basic wave statistics but also time series of wave properties including distant rise and extreme wave conditions in verified zones zaitseva pärnaste et al 2009 the computational domain of the model is determined by the range of input meteorological data from the cosmo model which are generated in the form of operational forecasts developed for the hydrological and meteorological service carried out by the institute of meteorology and water management national research institute the domain covers the area of the baltic proper the south baltic and partially the danish straits kattegat fig 1 a the computational grid was orthogonal curvilinear where the grid size varied from 0 1 km near shore hel peninsula to 7 km in the central baltic proper a curvilinear orthogonal mesh with a lower resolution in coastal zones and higher resolution in deepwater zones enables accurate and optimally fast wave forecast simulations whitney 2003 to verify the model data with measurement and satellite data simulations were carried out for selected points of location of measuring devices and common points in the orbits of satellite passages model simulations were also carried out for eight points located in the southern baltic and partially in the baltic proper with a time step resolution of 30 min to develop the wave characteristics in the baltic area covered by the model domain the main grids of the model are computational wind and bathymetric fig 1b 2 2 model settings and boundary conditions the computational domain of the model is the area close to the baltic proper this is due to the range of the wind field data of the cosmo model which at that time covered the analysed area cosmo 2018 the computational grid is orthogonal curvilinear grid size 0 1 7 km frequency discretisation is constant at 36 frequencies between 0 05 and 1 hz and for the directional discretisation 36 directions were considered in all runs the directional discretisation is equal to 36 directions considered in all runs the wind scaling for the formulas of komen janssen and westhuysen is described by the formula u 28u and in the formula st6 u swsu where sws is a free parameter swan team recommends using sws 28 in the case of st6 a it is 32 and in the case of st6 b it is 28 moreover according to the calibration variants swan team 2020 the st6 a formula used the counter bias factor in the input wind field debias 0 89 boundary conditions at the open borders the northern and western boundaries are created in the initial simulations using commercial database emcwf and the mars data archive ecmwf 2022 with a spatial resolution of 0 1 0 1 lat long in subsequent simulations conditions from model excitations using the hotstart function are used where the initial wave field is read from a file if the previous run were nonstationary the time found on the file would be assumed to be the initial computation time it can also be used for stationary computation as a first guess the computational grid both in geographical and spectral space must be identical to the one in the run in which the initial wave field was computed swan the climate change initiative coastal sea level team 2020 2 3 wave wave interactions depth limited breaking and bottom friction the quadruplet are modelled by the discrete interaction approximation dia swan team 2020 fully explicit computation of the nonlinear transfer with dia per sweep the coefficient for quadruplet configuration is equal to 0 25 proportionality coefficient for quadruplet interactions equal to 3 107 and coefficients for shallow water scaling csh1 csh2 csh3 respectively are 5 5 0 8333 1 25 nonlinear triad interaction is turned off since their effect is minor christakos et al 2021 in the analysed area the battjes and janssen 1978 formula represents the depth limited wave breaking the bottom friction is modelled using the jonswap formula and as recommended by swan team 2020 is 0 038 2 4 whitecapping 2 4 1 komen and janssen the values of the tunable coefficients cds and exponent p in this model have been obtained by komen et al 1984 and janssen 1991 by closing the energy balance of the waves in idealized wave growth conditions both for growing and fully developed wind seas for deep water the default whitecapping setting is the komen formula proposed by komen et al in 1984 this formula was also later presented by among others the swan team 1 2 s s d σ θ c d s 1 δ δ k k s s p m p σ k k e σ θ where cds 2 36 10 5 δ 1 and p 4 for komen formula and cds 4 10 10 5 δ 0 5 and p 4 for janssen formula the mentioned parameters are calibration and the remaining ones mean k is the wavenumber e is the energy of the wave spectrum and is a mean spectral steepness and are the mean wavenumber and the mean circular frequency this corresponds to the mean spectral steepness of a pierson moskowitz spectrum the calibration parameters were adopted according to the default ones proposed by the swan team 2020 2 4 2 westhuysen an alternative formula to komen is westhuysen et al 2007 which expresses whitecapping without the dependencies which are problematic in mixed wind sea and swell conditions this method is based on experimental results showing that whitecapping is associated with the nonlinear hydrodynamics within wave groups the formula is as follows 1 3 s b r σ θ c d s b k b r p 2 tanh k d 2 p 0 4 g k e σ θ where d is the water depth g is the acceleration due to gravity b k is the azimuthally integrated spectrum saturation and is referred to as a formula e σ k 3 c g where the last parameter is the wave group velocity br 1 75 10 3 is a threshold saturation level the proportionality coefficient is set to cds 5 10 5 when b k br waves break and the exponent p is equal to a calibration parameter p0 for b k br there is no breaking but some residual dissipation proved necessary this is obtained by setting p 0 a smooth transition between these two situations is achieved by alves and banner 2003 2 4 3 st6 the whitecapping is the sum of two dissipation components t1 1 5 which is the inherent break related to instabilities of wave and t2 1 6 which describes the dissipation of shorter waves triggered by longer breaking waves 1 4 s d s σ θ t 1 σ t 2 σ e σ θ 1 5 t 1 σ a 1 a σ σ 2 π e σ e t σ e t σ p 1 1 6 t 2 σ a 2 σ 1 σ a σ 2 π e σ e t σ e t σ p 2 d σ the calibration parameters are the a1 a2 p1 p2 coefficients rogers et al 2012 these four factors determine the different scattering shapes of t1 and t2 the relative size of a1 and a2 is set to achieve the desired ratio of t1 and t2 at large wave age values at moderate wind speeds the power coefficients p1 and p2 control how strongly the dissipation term reacts to the threshold exceedance the st6 does not have a default parameterisation and the swan team recommends using five selected examples variants of settings swan the climate change initiative coastal sea level team 2020 among the variants the settings containing the fan wind resistance formula which uses an iterative procedure for estimating based on the actual seas state were eliminated the fan formula is a function of wave age and wind speed and may cause a decrease in the obtained significant wave height values swan the climate change initiative coastal sea level team 2020 from the remaining variants among others a set named in the work st6 a which is commonly used and recommended aydoğan and ayat 2021 day et al 2021 and comparative st6 b in which the values of a1sds and a2sds differ significantly and in which wscaling is 28 according to with the recommendations of the swan team 2020 table 1 2 5 wind data the key issue in modelling wind waves in sea areas especially those characterized by complex geometry and high coastal cliffs is the correct selection of atmospheric circulation data soomere et al 2011 model wind input data even those covering a widely known area mainly represent the properties of offshore winds and may reveal mismatch compared to measurement data broman et al 2006 soomere 2008 and may deviate from other modelled predictions keevallik and soomere 2010 this inconsistency also occurs in reproducing wave fields using wave models based on scaled wind data räämet et al 2010 geostrophic wind field data tends to underestimate the actual wind effect on sea surfaces soomere et al 2011 the sensitivity analysis carried out by nikishova et al 2017 indicates that the wind direction by which the model is assimilated to the model significantly affects the simulation results we chose the cosmo model cosmo 2018 to power the swan model because the data from this model is also generated by imwm nri imwm nri department of numerical forecasts cosmo which guarantees constant access to verified and reliable data enabling the operational use of the model cosmo is a non hydrostatic atmosphere forecasting model designed for operational work on an available resolution scale of 7 km this model s initial and boundary conditions are implemented using the gme generic modeling environment and ifs internal family systems model processes cosmo 2011 the implemented data relates to data with a time step of 1 h and a spatial resolution of 7 km 160 120 knots and is in the form of a regular orthogonally grid the range of the wind fields covers the southern baltic and the baltic proper the implemented data is a forecast of wind direction and speed thus containing u v longitudinal and latitudinal components and show high compliance of the estimates with real conditions linkowskawyszogrodzki 2014 2 6 bathymetry and disturbances the basic input data in hydrodynamic modeling are bathymetric and orographic data this type of fed data to the model plays a key role in forecasting wave growth and breaking by taking into account the presence of wind and other forms of seabed relief in the bathymetric coverage the model does not take into account the ice cover because the southern baltic is rarely covered with ice sztobryn et al 2012 as in 2018 2020 it did not occur in the area covered by the modeling stanisławczyk 2020 2021 moreover the bottom cover with phytobenthic organisms and hydrotechnical objects was not taken into account as their influence is insignificant in relation to the resolution in which the simulations were carried out the bathymetric grid was created by combining two data sets the first with a resolution of 500 500 m baltic sea hydrographic commission 2013 covers the entire computing domain the second data set polish eez obtained from hydrographic office of the polish navy was filling the first area due to its resolution which is differentiated but not greater than 100 m detailed bathymetry especially in coastal areas is crucial for correct wave simulation nikishova et al 2017 3 measurement and satellite data 3 1 wave measurements the measurement network of the south baltic sea is underdeveloped as there are currently only a few measurement points approx 3 two of which have been included in the analysis fig 1a data from measurements carried out with three devices located in both nearshore and offshore areas were used to validate the model and characterize the wave conditions fig 1a in the pomeranian bay measurements are carried out using the nemo wpa waves processing array nemo teledyne 2008 device belonging to the maritime office in szczecin and is installed at a depth of approximately 7 m in the open sea area two devices owned by the institute of meteorology and water management national research institute are installed on the petrobaltic platform one of the devices awac acoustic wave and currents profiler nortek 2017 is located on the bottom 80 m and performs measurements using the adcp technique with a frequency of 400 khz and 1 5 hz sampling of the surface elevation the second measuring device is a waveguide radac 2020 installed at 30 m on the petribaltic platform waveguide is carried out by the fmcw frequency modulated continuous wave radar sensor with a frequency of 10 hz all devices allow the measurement of parameters significant wave height hs maximum wave height hmax mean wave period tm peak wave period tp and wave direction the analysis used measurements from the 2018 2020 period with a record time step resolution of 30 min to unify and synchronize the databases 3 2 model calibration the model was verified in two stages the first stage was to select the model s optimal parameterisation to obtain the estimated parameters hs significant wave height tm mean wave period that are closest to the real conditions the second stage is to carry out multi stream verification using the selected formula from the initial validation stage the initial validation of the model was based on calculations for four formulas with default parameterisation komena komen et al 1994 janssen 1991 westhuysen westhuysen et al 2007 and st6 physics rogers and in 2012 in the case of the st6 formula for which there are no default parameterisation settings the variant was analysed taking into account wind and debias scaling and different values of the a1sds a2sds p1sds and p2sds parameters eq 1 4 1 6 as the swan model is intended to be used in the operational mode the calculation time of the generated wave forecasts is essential based on the internal tests and conclusions of numerous authors a o aydoğan and ayat 2021 amarouche et al 2019 the simulations were performed with a time step of 30 min and three iterations the default settings were used for other variables including quadruplets triads depth induced wave breaking and bottom dissipation the model results were compared with the observations using time series fig 6 7 and statistical analysis table 3 the obtained values were compared statistically using the mean bias eq 3 1 which is a representation of the model s mean long term error where its value either indicates average overestimation positive or underestimation negative compared to the measurements bryant et al 2016 mean square error rmse eq 3 2 which depends on the measurement accuracy scale and is used to assess the ability of various models to predict a single variable the lower the rmse value the more accurate the model fit chai and draxler 2014 the scatter index si eq 3 3 which is a normalised measure of error lower values of the si are an indication of better model performance it is defined as the root mean square deviation divided by the mean of the observations mentaschi et al 2013 and the pearson correlation coefficient r eq 3 4 which is a measure of the degree of linear dependence between the model and the observations rogers et al 2012 a perfect positive linear relationship i e as the value of one variable increases the value of the other variable increases has a value of 1 0 while no linear relationship is indicated by a value of 0 0 bryant et al 2016 the purpose of interpreting the correlation results was the rule of thumb hinkle et al 2003 where very strong correlations are in the range 0 9 1 0 strong 0 7 0 9 moderate 0 5 0 7 low 0 3 0 5 negligible 0 0 0 3 positive or negative in the formulas below xm is the calculated value of the model xo is the actual measurement value and n is the number of data 3 1 b i a s x m x o 3 2 r m s e 1 n i 1 n x m x o 2 n 3 3 s i r m s e x m x o 3 4 r i 1 n x m x m x o x o i 1 n x m x m 2 i 1 n x o x o 2 3 3 verification and crosscheking of measurement data the key method of verifying the correctness of model forecasts compares their results with measurement data therefore the measurement data must be of high documented quality in our study we used data on the height of the significant wave from three measurement points two of them awac and waveguide located in one location in the deep water zone and nemo wpa located in the pomeranian bay fig 1a this allowed for data verification using cross checking however the first step was verifying each data series from three measurements based on internal procedures error codes a comparison of measurements from awac with measurements from the waveguide device was based on data from 2019 to 2020 with a harmonized measurement time step of 30 min data obtained from both devices under non storm conditions show a high agreement fig 2 the most considerable deviations were observed at a significant wave height above 4 7 m in which conditions awac did not register the measurement or recorded it incorrectly fig 2 specialists correctly calibrate both devices their complementary measures enable the subsequent use of the data for model verification this is due to an error that occurs when the projected matrix is too large for the peak wavelength nortek 2022 the comparison of data from awac and waveguide devices also included a compilation of statistical data for the significant wave height am median me lower quartile lq upper quartile uq minimum min and maximum max the mean and median values of the significant wave height determined for the years 2019 2020 were very similar fig 3 in 2019 the average values were identical 1 94 m in the median values the difference was only 0 01 m the range of hs values was 0 15 5 21 m for awac and 0 16 5 14 m for waveguide respectively in 2020 the differences between the individual parameters were only slightly greater even though awac did not register some high waves significant wave height varied from 0 11 m to 5 75 m for awac and 0 12 5 25 m for waveguide the average values were 1 99 m and 2 01 m respectively the comparison of the statistically significant wave height parameters determined for the months shows a very good agreement of the data from awac and waveguide fig 4 the largest differences between measurements of both devices occurred in june august and december for which the differences in the mean values were 0 28 and 0 22 respectively but a different period of data collection should be taken into account 3 years n 52560 awac and 2 years n 35040 waveguide monthly averaged significant wave heights equal to or greater than 2 m characterized the months from january to march and september to december the maximum average significant wave height values were recorded in both case in december 2 22 m awac and 2 24 m waveguide the lowest values were recorded in june 1 39 m awac and 1 11 m waveguide the highest values of the standard deviation marked with a frame were found in the autumn winter months i iii x and the lowest values in may and july fig 4 the obtained results clearly indicate a very good compatibility of the data from both devices confirming the very high quality of the data that can be used to verify the applied wave model the analyses showed very strong statistically significant relationships between the series of data from different sources the correlation coefficient eg 3 4 between the awac and waveguide measurement data was 0 986 p 0 005 as in the case of measurement data from two sources statistical parameters were determined on the basis of model data from the period 2018 2020 the obtained results show a very good agreement the mean annual hs values from measurements and model measurements were practically identical the differences between them did not exceed 0 05 m fig 3 the maximum values were slightly more differentiated and 0 5 m higher in the case of model data in each year of the study a similar agreement between the parameters describing the measurement and model data was obtained by comparing the hs values averaged for individual months again the differences between the mean and median values did not exceed 0 05 m as in the case of the measurement data the averaged hs values ranging from 2 00 m to 2 27 m characterized the months from january to march and september to december the minimum average value of hs was 1 43 m the range of maximum hs values determined with the use of the model was shifted towards higher values by approx 0 5 m in all months except january and august where the differences were lower 3 4 satellite data all satellite data used in this study were generated by altimeters installed onboard jason 2 cnes 2011 jason 3 cnes 2016 saral cnes 2021 sentinel 3a and 3 b satellites eumetsat 2017 in the period of three years 2018 2020 the jason 2 satellite launched in 2008 and scheduled for five years of duty finished mission on 1st october 2019 in june 2017 the degradation of critical onboard components and control systems required that jason 2 move from its original orbit with 127 revolutions within the 10 days cycle maneuvered into a slightly lower orbit away from functioning satellites jpl nasa 2017 in this new orbit jason 2 was collecting data along a series of very closely spaced ground tracks just 8 km apart to complete one cycle of these new ground tracks jason 2 required one year in analysed period as a result the ground track of its orbits was very dense compared to jason 3 launched in 2016 which has stable 127 revolutions within the 10 days cycle fig 5 the saral satellite with ka band altika instruments launched in 2013 has a repetition cycle of 35 days with 501 revolutions within the cycle with a drift of 1 2 km fig 5 the altika instrument working in ka band has a much smaller footprint than a jason poseidon instrument being more useful in the coastal regions finally sentinel 3a launched in 2016 and sentinel 3b launched in 2018 which have a repetition cycle of 27 days with 385 orbits per cycle fig 5 a comparison of the main features of used instruments is presented in table 2 only gdr geophysical data record data were used representing the best quality altimeter products in comparison to ogdr operational geophysical data records and igdr interim geophysical data records the following products were used jason 2 and jason 3 swh ku product ku band corrected significant wave height saral altica swh product ka band significant wave height sentinel 3 sr 2 lan dataset swh ocean 01 ku products corrected ocean significant wave height 1 hz ku band two ground measurements points over south baltic sea were used petro batlic 18 183 e 55 481 n and buoy at pomeranian bay 14 253 e 54 016 n the first one located at open sea is very useful for comparison with satellite altimeters due to better spatial homogeneity of swh the second one located about 7 km from the coast on relatively shallow water is more problematic the beam width of the antenna is 1 28 ku band so the electromagnetic pulse irradiates a disk approximately 20 km in radius for jason and 8 km for saral however for a flat marine surface the first returns will come from the sub satellite point the satellite altimeter measurements taken at high frequency 20 hz corresponds to a ground resolution of 300 m along track measurements but measurements located less than 20 km from the cost has reduced quality due to limitations with atmospheric correction climate change initiative coastal sea level the climate change initiative coastal sea level team 2020 the difference between ku band jason 2 and 3 sentinel 3a and 3 b and ka band saral altica measurements must be mentioned due to the higher frequency measurements in ka band are more sensitive to rainfall and thick clouds with large droplets which could be a source of the limited accuracy of altimeter measurements the significant wave height measurements are based on the slope of reflected echo at the leading edge robinson 2004 steeper slope smaller swh the difference in delay for reflections from wave crests and wave troughs affects the slope of the leading edge of the waveform σc in comparison to the original pulse width σp ray et al 2015 this measure includes both the width of the original pulse and the standard deviation of the delays associated with reflecting facets 2σh c where c is the speed of em waves as the standard deviation of surface elevation is equivalent to one quarter of the swh the wave height can be determined 2 s w h 2 4 c 2 s c 2 s p 2 for altimeters operating in sar mode sentinel 3 both the leading edge and the trailing edge of the waveform are affected by changes in swh all mentioned limitations have to be taken into account in comparing three datasets satellite altimetry measurements buoy measurements and model results 4 results and discussion 4 1 selecting the most optimal physics formula for the wave model the parameters of the significant wave height and the mean wave period were compared in a location representing deep water petrobaltic and shallow water pomeranian bay conditions for four formulas eq 1 2 1 4 of which two variants were taken into account for the st6 physics at the petrobaltic point the strongest correlation coefficients between the model and measure values of significant wave height were obtained for the formulas st6 a and st6 b table 3 and the weakest were recorded for the westhuysen formula for the significant wave height r 0 842 and for komen s in the case of the mean wave period r 0 744 bias for both hs and tm was the highest for westhuysen the lowest rmse values were shown by the formula st6 a and st6 b table 3 i e hs 0 108 st6 a hs 0 132 st6 b and tm 0 228 st6 a and tm 0 376 st6 b the calculations using the westhuysen formula hs 0 207 tm 0 199 were characterized by the highest value of the scatter index si while the lowest value was st6 a hs 0 032 tm 0 031 table 3 in the pomeranian bay the mean wave period shows a slightly lower degree of real wave reconstruction for all analysed formulas the lowest pearson correlation coefficient for the significant wave height was r 0 864 westhuysen and for the mean wave period r 0 652 westhuysen rmse ranged from 0 038 to 0 208 for hs and 0 059 0 197 for tm as in the petrobaltic point the highest values of bias and si both for hs and tm are shown by the westhuysen formula and the lowest values were recorded for st6 a and st6 b a comparative analysis was also carried out based on the reconstruction of the storm situation from 2 january 2019 taking into account the height of the significant wave the mean wave period and the direction of wave inflow figs 6 and 7 at the petobaltic point the highest difference with the measurement data for the significant wave height fig 6a was shown by simulations using the westhuysen formula where the maximum bias value exceeded 0 5 m the minor differences were shown by the formula st6 a st6 b for which the difference did not exceed 0 3 m in the case of the mean wave period fig 6b and the direction of undulations fig 6c the simulations using the st6 parameterisation a showed the most negligible differences at the point in the pomeranian bay as in petrobaltic the minor differences were shown by simulations using the st6 a formulas where the difference with the measurement data did not exceed 0 25 m for the significant wave height 0 4 s for the mean wave period and 12 for the wave direction both in the petrobaltic point fig 6 and in the pomeranian bay fig 7 the highest agreement of the analysed parameters was demonstrated by the st6 a formula and the lowest for westhuysen the results of both st6 physics formulas are similar and the debias parameter used in st6 a and the wind scaling difference variant a 32 0 variant b 28 0 play a minor role in the reliable reproduction of the wave conditions further multi stream verification included a model based on the best in terms of recreating wave conditions in the southern baltic selected st6 formula with the set windscaling equal to 32 0 a1sds 2 8e 6 a2sds 3 5e 5 p1sds and p2sds 4 and multiplier on drag coefficient of 0 89 4 2 multi stream verification of the model based on the st6 formula the multi stream model validation determines the quality of the forecasts and warnings issued in the south baltic area there are only a few devices that enable the measurement of selected wave parameters the advantage of which is the time resolution of the measurements while their disadvantage is their point character in our research we verified the swan model based on data from two points petrobaltic pomeranin bay which are characterized by different hydrodynamic and morphological conditions for these points measurement data awac waveguide nemo wpa and satellite data jason 2 jason 3 sentinel 3 a and b saral were obtained to compare the data generated by the swan model with measurement and satellite data the model point data for the location of measuring devices was archived satellite data was collected for the same locations model measurement and satellite data series were time integrated at the point representing deep water conditions table 4 fig 3 4 very strong above 0 9 correlations with the measurement data awac waveguide showed all data sources except for saral which showed a strong correlation 0 786 in shallow water conditions table 5 the saral satellite 0 93 showed a very strong correlation with the measurement and model data data from jason 3 above 0 7 showed a strong correlation while data from jason 2 and sentinel 3 showed low correlations 0 12 0 17 and were statistically insignificant satellite data is an important tool in the provision of spatial wave height information in the analysis with the use of satellite data jason 2 jason 3 saral and sentinel 3 a and 3 b the time approximation the time of the flight of the satellite in relation to the measurement time was approximated to 30 min and the spatial approximation approx 0 1 were used the results of the comparison in the open sea area indicate a very good agreement of the simulation results of the swan model and measurements with satellite data fig 8 the plotts fig 8a and b show the scattering of the significant wave height values obtained from the satellite data measurement data and the results of the swan model the completed circle marks the comparison of satellite and measurement data in the pomeranian bay from the nemo wpa device in petrobaltic from the awac device a non filled circle marks the comparison of all satellite missions with the results of the swan model both the correlations table 4 table 5 and the scattering fig 8 show a very good agreement between the satellite data and the results of the swan model in deep water conditions petrobaltic fig 8a fig 9 a in shallow water conditions pomeranian bay the correlations were slightly weaker table 5 compared to those obtained at the petrobaltic point and the dispersion was less concentrated see fig 10 the best compliance of the measurement data awac with the satellite data was obtained for sentinel 3 r 0 999 and jason 2 r 0 992 and the worst for saral r 0 786 table 4 the satellite data also showed a high correlation of the significant wave height with the values obtained as a result of the simulations the best agreement was obtained for the data from sentinel r 0 988 the worst for saral r 0 764 the largest hs difference of 0 5 m was identified in the case of jason 3 the maximum differences for the other satellites did not exceed 0 4 m jason 2 0 2 m sentinel 3 0 1 m saral 0 34 m the comparison with the use of satellite data was also made for the point in the pomeranian bay figs 8b and 9b 4 3 characteristics of wave conditions in the period 2018 2020 the conducted studies aimed at verifying the results of simulations obtained with the use of the swan model in the areas of the southern baltic and its application in operational mode was supplemented by the reconstruction of wave conditions in the southern baltic area for the period 2018 2020 fig 11 table 6 the simulations were performed with a time step of 30 min and they included key wave parameters such as significant wave height mean wave period mean wave direction mean wave height mean wave crest length and mean wave steepness table 6 the reconstruction was carried out for eight points located in areas with different hydrodynamic characteristics p1 arkona deep p2 pomeranian bay p3 p4 bornholm basin p5 gdańsk bay p6 gotland basin petrobaltic p7 near the curonian spit p8 the gotland deep fig 11 for individual points the mean values of all parameters were determined for the entire period covered by the calculations and divided into calm iv viii and stormy ix iii periods the measured significant wave height is the average height of one third of all waves in modelling the significant wave height can be calculated as four times the zeroth moment of the process bai and bai 2014 the lowest average significant wave height hs values were characteristic for points p2 pomeranian bay and p5 gulf of gdańsk which are bay areas with an extensive coastline and reduced exposure to wave run moreover these points are characterised by the smallest depth among all analysed points i e p2 7 m below sea level p5 41 m below sea level the highest mean value of the significant wave height was recorded in the gotland deep p8 1 04 m and the lowest for the pomeranian bay p2 0 53 m the average values in these areas in the non storm period were 0 74 m and 0 44 m respectively and in the storm period 1 28 m and 0 60 m table 6 fig 11 the mean wave period which expresses the time between two wave crests or troughs is one of the main parameters describing the wave phenomenon the size of the wave period is mainly determined by the effects of wind and pressure on the sea surface the run up and depth of the seabed and geomorphological structures in the coastal zone that determine wave breakage the highest and the lowest mean values of the wave period as in the case of the significant wave height occurred in the same locations and amounted to 3 76 s at p8 and 2 60 s at p2 in the no storm season the highest average value was recorded in the gotland basin p6 3 23 s and the lowest in the pomeranian bay p2 2 43 s in the stormy season the highest values were determined for p6 4 00 s and p8 4 17 s and the lowest for p2 2 74 s low values of the mean wave period occur mainly in shallow water areas pomeranian bay gulf of gdańsk central coast of poland and also in the vicinity of the curonian spit in the non storm season table 6 these areas are characterised by relatively shallow depths and reduced exposure to winds especially from the south east in calculating the direction of wave propagation the energy spectrum plays a critical role which is a component of frequency and direction kuik et al 1998 westerly winds prevail on the polish coast richling et al 2005 in 2018 the winds blowing from the southern sector prevailed in the open sea zone biernacik zalewska ed 2020 the transfer of wind energy to the water surface determines the similar direction of the wind wave in the open parts of the analysed area the dominant direction from which the waves came in all locations was se and sw in the no storm period it was in the range of 184 8 225 4 and in the storm period 161 0 198 2 the pomeranian bay p2 the gulf of gdańsk p5 and the bornholm basin p3 p4 are characterised by the presence of dominant south west and west winds table 6 fig 11 this direction is the same as the wave propagation direction zeidler 1992 a similar wave propagation direction distribution occurred from 1970 to 2007 where the se and s directions dominated räämet et al 2010 the height of the swell wave is related to the low frequency partition of the spectrum and occurs mainly as a result of long lasting wind from one direction and as a post storm extinction wave the direction of the dead wave propagation is constant and unlike the wind wave it is not related to the wind direction these waves are characterised by an above average length and a short period of time therefore they carry less energy making them disperse relatively quickly druet 2000 the size of the swell wave height is mainly due to the depth and run up i e the continuous distance of open water on which the wind blows without a significant change of direction hence the highest values of the mean wave height were recorded in the gotland basin p6 and the gotland deep p8 the wave height values were negligible and did not exceed 0 08 in annual terms in the non storm season they remained in the range of 0 01 0 03 and in the storm free season the highest average value was 0 12 m table 4 the wavelength is the distance between the two crests or troughs of the wave which is closely related to the calculated mean wavelength parameter using the linear dispersion relationship the average length of the wave crest ranged from 8 74 to 20 4 m in the stormy period the maximum value was 24 7 m table 6 the constant north east movement of areas with increased storm activity increased the number and size of storms in the baltic sea especially in winter and spring bacc author team 2008 the steepness of a wave describes the ratio of the wave s height to its crest s length wave steepness is an important wave parameter that influences whitecapping i e wave breakage caused by steepness in deep water regions where the wave height then becomes too high compared to the wavelength and in shallow water conditions plays a vital role in the three wave interaction the model used the non breaking dissipation formula of ardhuin ardhuin et al 2010 the steepness values in the whole studied area are even and amount to 0 5 0 6 table 6 this may be due to the even ratio of the significant wave height and the wave crest length 5 conclusions the swan model verified with various tools in different depth zones both during storms and stable sea states allows with the combination of cosmo data a qualitative wave forecast of 72 h calculated in a short time approx 35 min the key element of the model is its operational mode and thanks to the cosmo data processed and automatic transfer data inside the institution a continuous forecast is provided the conducted studies aimed at implementing the swan model for operational work confirmed its applicability in the areas of the baltic sea designated by the domain of meteorological data generated by the cosmo model the initial validation of the models made it possible to select a formula that enables the best representation of the real wave conditions in different sea depth zones in a detailed multi stream verification the simulation results of the swan model were subjected to the use of physics st6 which unlike the other formulas showed the lowest values of bias rmse and si and one of the highest values of the correlation coefficient the verification of the model simulations included the comparison of the significant wave height in the period from 2018 to 2020 the use of measurement data was preceded by data quality control through internal algorithms and the comparison of data measured at one point using two different measurement systems a statistically significant correlation was demonstrated for the measurement data from awac and waveguide devices characterized by a very high correlation coefficient r 0 986 p 0 005 the correlation of the model data generated for the measurement location point in the open sea area showed a very good agreement which was reflected in a statistically significant correlation confirmed by the correlation coefficient at the level of 0 973 only a slightly worse fit r 0 968 p 0 05 was shown by the model data concerning the measurement data in the coastal areas pomeranian bay the use of satellite data for model verification can be a very good alternative to measurement data the resources of which are limited in the open sea areas correlation coefficients above 0 96 were determined for jason 2 and 3 and sentinel data in the case of saral the relationship between the model and satellite data was slightly weaker r 0 764 in coastal areas the relationships were weaker and statistically significant correlations were found only for saral r 0 938 and jason 3 r 0 704 the poorer compatibility of the model data with satellite data in coastal areas is due to the finite resolution which may cause a difference in the observation location and the closest grid point for which wave properties are calculated soomere et al 2011 the observed more significant discrepancies between the model measurement and satellite data indicate the influence of a much larger number of factors such as bathymetry shape and influence of the shoreline on the wave conditions in coastal areas this points to the need to develop models with better resolution in the edge areas based on the data generated using the verified swan model for selected points a multi point reconstruction of the wave conditions in the baltic area was performed the wave parameters were determined i e significant wave height mean wave period wave propagation direction mean wave height mean wave crest length and mean wave steepness the obtained reconstruction values agree with the results of other studies lit they indicate both the potential for renewable energy dominance of the wave direction ssw and se and the potential threats to coastal areas and shipping the method of reconstructing wave conditions can be an essential tool in planning investments and works in the offshore and nearshore zones the areas with the lowest wave dynamics are zatoka pomorska and zatoka gdanska where the average annual significant wave height does not exceed 0 67 m the wavelength is 12 06 m and the average wave period is in the range of 2 60 2 98 s gotland deep is characterized by the highest annual mean wave height 1 04 m implementation of the swan model fed with meteorological data from the cosmo model and multi stage verified with data from different sources delivers the possibility for the swan model application in other waters bodies with similar geomorphology and meteorological conditions creating a verification system based on both measurement and satellite data which should be a permanent and integral part of forecasting wave conditions in sea areas seems to be the optimal system for generating reliable wave forecasts which can be the basis of warning systems 6 software and data availability the third generation wind wave model swan is available freely at http swanmodel sourceforge net download download htm in this study we performed the multi stream analysis of the quality of forecasts of the model version 41 31 2019 functioning in the operational mode in various sea states and different depth conditions examples of wind data 1 02 mb and and selected sections of code not covered in detail in the article can be found at https github com patryksapiega swan in addition there are programming rules scientific and technical documentation and user manual with code all 1 77 mb the other model settings are described in the article this distribution may be implemented on microsoft windows linux unix and macos provided a fortran90 compiler is available the 64 bit version requires an x64 edition of windows 7 8 1 or 10 the 32 bit executable can be run on both 32 bit and 64 bit operating systems declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like to thank the team of the meteorological modeling center cosmo numerical forecasting department for the preparation of chip field data and the maritime office in szczecin for providing data from the pomeranian bay all data used in the analysis and supporting the conclusions in this manuscript can be obtained from the institute of meteorology and water management poland e mail patryk sapiega imgw pl any trade firm or product name is for descriptive purposes only and does not imply endorsement by the imwm nri 
