index,text
26350,the use of existing component based modeling frameworks for integrated water resources modeling is currently hampered for some important use cases because they lack support for commonly used topology aware spatiotemporal data structures additionally existing frameworks are often accompanied by large software stacks with steep learning curves others lack specifications for deploying them on high performance heterogeneous computing hpc infrastructure this puts their use beyond the reach of many water resources modelers in this paper we describe new advances in component based modeling using a framework called hydrocouple this framework largely adopts the open modeling interface openmi 2 0 interface definitions but demonstrates important advances for water resources modeling hydrocouple explicitly defines standard and widely used geospatial data formats and provides interface definitions to support simulations on hpc infrastructure in this paper we illustrate how these advances can be used to develop efficient model components through a coupled urban stormwater modeling exercise keywords component based modeling model coupling openmi integrated water resources modeling high performance heterogeneous computing software availability name of software the software described in this paper includes the hydrocouple component based modeling interface definitions the hydrocouple software development kit sdk and the hydrocouplecomposer component coupling graphical user interface gui and command line interface cli the two hydrocouple compliant components we developed for this study include a two dimensional hydraulic model component called the finite volume hydrologic model fvhm and a component developed from the environmental protection agency s stormwater management model swmm code developer caleb a buahin contact caleb a buahin address 8200 old main hill logan ut 84322 8200 usa email caleb buahin usu edu required hardware and software any computer that runs the windows linux or macintosh operating system the performance of the swmm and fvhm components benefit from compilation with a compiler that supports openmp http www openmp org fvhm can be compiled with the message passing interface mpi enabled compilation of all the software codes enumerated requires the qt 5 7 framework http www qt io cost the hydrocouple interface definitions software stack and model components in this manuscript are freely available under the gnu lesser general public license lgpl and can be downloaded from the hydrocouple github repository https github com hydrocouple 1 introduction the goal of integrated assessment in environmental and natural resources management is to provide information within a decision making context that brings together a broader set of domains methods styles of study and or degrees of certainty than would typically characterize a study of the same issue within the bounds of a single research discipline parson 1995 laniak et al 2013 in order to make the complexity surrounding integrated assessment studies more tractable a need has also arisen to integrate computer models from diverse fields so that scientists can conduct more holistic assessments in particular for water resources specialists the need for model integration arises frequently because although many individual hydrologic processes have existing mathematical models that are able to simulate them under a given set of circumstances there is rarely a single model that can simulate all of them at the different scales and complexities desired while accounting for feedbacks between the various sub processes for integrated assessment studies beven et al 1980 argent et al 1999 selecting a particular hydrologic model requires a consideration of the specific management challenges of concern the spatial and temporal scales of interest model input data availability and computing requirements among other considerations beven et al 1980 leavesley et al 2002 argent 2004 voinov and shugart 2013 chowdhury and eslamian 2015 clark et al 2015 when a single model cannot meet the needs of a modeling study it is common for modelers to couple elements of multiple models together to form a more holistic or accurate representation of a water system this coupling must not only be considered as a technical exercise of stitching models together but must also include semantic and conceptual harmonization between coupled models janssen et al 2011 voinov and shugart 2013 although model developers in the earth systems and environmental modeling field have used several approaches to couple models for their integrated modeling efforts the component based modeling approach is increasingly receiving more attention this is because in contrast to monolithic approaches where models are compiled into a single code base or executable unit component based model development promises improved flexibility in the selection of the best available modeling approach for each application domain and re use of different models and more maintainable and extensible models fröhlich and franz 1999 szyperski 2002 the component based modeling paradigm involves the provision of interface definitions describing standard data structures and functions that models must implement so that they can be deployed independently to exchange information at runtime with other models model developers across diverse fields who adopt these interface definitions can develop models that can be coupled with other models to simulate complex earth and environmental systems component based modeling provides a natural avenue for experimenting with different model formulations since model components can be removed and added to a composition in a plug and play fashion several component based modeling frameworks and interface standards with varying degrees of complexity and application domains have been developed over the years these include the earth systems modeling framework esmf hill et al 2004 community surface dynamics modeling system csdms peckham et al 2013 the object modeling system oms david et al 2002 and others in the water resources modeling arena the open modeling interface openmi moore and tindall 2005 definitions have been tested and used extensively e g smolders et al 2008 castronova and goodall 2009 goodall et al 2011 buahin and horsburgh 2015 the appeal of openmi revolves around the fact that instead of providing a framework that has an accompanying large and complicated software stack that is beyond the expertise of many water resources modelers the openmi developers provide a set of standardized programming language agnostic interface definitions that can be adopted to develop model components that can communicate with each other directly at runtime the openmi interface definitions are object oriented with clear and well defined inheritance relationships another major advantage is that the latest openmi 2 0 version has been adopted as an open geospatial consortium ogc standard vanecek and moore 2014 which means that it has been reviewed and vetted by a large community of modelers despite these attractive features our experience using openmi revealed a few areas where advancements to the interface definitions would be useful especially for water resources modeling applications first while the openmi interface definitions are programming language agnostic the example interface definitions as well as software development kits sdk provided by the openmi developers use the c and java programming languages these languages are compiled into an intermediate bytecode before being translated into the native instructions of a target machine to be executed using a virtual machine software infrastructure i e common language runtime clr for c and java virtual machine jvm for java additionally c has traditionally been restricted to computers that run windows operating systems conversely many legacy model codes used in the water resources modeling field and in the earth systems and environmental modeling field more generally have been developed using programming languages like fortran c and c because of the mature set of tools scientific libraries and support for hpc simulations that are available with these languages additionally these programming languages are employed for computational models because they are compiled directly into native instructions for a target machine i e natively compiled and therefore generally have lower memory footprints faster performance and can be compiled on many operating systems while the advent of just in time jit compilers has significantly improved the performance of languages like c and java natively compiled languages remain faster for many applications and remain the benchmark for evaluating performance taboada et al 2013 to convert legacy codes written using natively compiled languages into components that can be coupled loosely to other models one needs to resolve the programming language mismatch between the interface definitions and the computational codes of these legacy models though there are ways to bridge this programming language mismatch e g using platform invocation service for c and java native interface for java the costs of marshalling data across this language divide can lead to increased memory usage and increased time for individual simulations we encountered and quantified these costs in a previous study where we converted the environmental protection agency s stormwater management model swmm which is written using the c programming language into an openmi compliant component using the c openmi interface definitions for a spatial domain decomposition urban stormwater coupling exercise buahin and horsburgh 2015 second while the openmi specification provides interface definitions for representing geometric primitives e g points lines and polygons and their associated time varying data these definitions lack some of the more common geospatial dataset formats used by water resources modelers e g meshes vector datasets rasters etc also the geospatial interface definitions provided by the openmi specification lack the topological relationship information that is important for many water resources modeling applications for instance a hydrologist simulating flows in a river network will need to know which upstream tributaries flow into any selected river reach these types of topological relationships are not explicitly supported by the openmi 2 0 standard but can be implemented by extending the openmi standard finally like other earth systems and environmental modelers hydrologic modelers often embark on experimental simulations where the same model is executed multiple times with varied inputs e g optimization uncertainty assessment calibration etc these types of simulations fall into the so called embarrassingly parallel class of simulations and benefit from using high performance computing hpc resources many research institutions provide access to computing clusters comprised of heterogeneous hardware configurations of multi core central processing units cpu as well as graphical processing units gpu and many integrated cores mic architecture accelerators that can be used for more efficient computations however the current openmi standard provides little direction on how to take advantage of these increasingly ubiquitous hpc infrastructures for more efficient simulations the contribution of this paper is the presentation of a set of component based modeling interface definitions called hydrocouple and its associated sdk and coupled model composition tools hydrocouple uses the openmi 2 0 interface definitions as its foundation but advances new interface definitions to address the challenges enumerated in the preceding paragraphs and others we chose to build from openmi because it has been tested and used within the water resources modeling domain and because it has recently been advanced as an ogc standard this manuscript represents a maturation of the core concepts and interfaces presented in an earlier conference proceedings paper buahin and horsburgh 2016 where we first described new spatiotemporal data structures and interfaces for supporting parallelized experimental simulations like calibration optimization and uncertainty assessment within component based modeling frameworks in this paper we extend the previous work by providing a more complete description of the spatiotemporal data structures and topological relationships supported by hydrocouple by describing implementation of hydrocouple s interface definitions and software development kit using c by introducing hydrocouple s gui for composing coupled models and by providing a detailed case study that demonstrates how hydrocouple s new capabilities can be used in complex model coupling scenarios we also illustrate how these advancements can be used to convert existing legacy codes into model components and develop new ones from scratch through a detailed modeling case study involving parallel execution of coupled models having different dimensionality we coupled a one dimensional 1d hydraulic model developed from the environmental protection agency epa stormwater management model swmm and a two dimensional 2d hydraulic model that we developed called the finite volume hydrologic model fvhm this coupled urban stormwater modeling example illustrates how these advancements can help usher more water resources modelers into the hpc realm so that they can embark on more efficient simulations the remaining chapters of this manuscript are organized as follows in section 2 we provide a review of existing component based frameworks with respect to their support for spatiotemporal data structures data exchange workflows and support for simulations on hpc infrastructure in section 3 we provide a discussion on the design of the hydrocouple framework with respect to the three areas identified in section 2 we also describe tools provided to simplify the process of developing new components and creating coupled model compositions in section 4 we present the case study involving the coupling of 1d hydraulic model component and a 2d component developed to represent an urban stormwater conveyance system through this application we illustrate how the advancements we have implemented are used 2 design of component based modeling frameworks the component based modeling approach and its precursor the component based software development approach have their origins in the object oriented programming approach with its notions of re use through encapsulation inheritance and polymorphism components are typically paired with software frameworks and they serve to extend the capabilities of frameworks while frameworks provide an environment for executing components fröhlich and franz 1999 this concept extends to model components and modeling frameworks in earth systems and environmental modeling for instance csdms components are executed within the common component architecture fast framework example in need of everything ccaffeine allan et al 2002 the definition of standard interfaces forms the basis of interaction between components and frameworks and between components themselves by describing the assumptions they make about each other fröhlich and franz 1999 the design of openmi deviates from other approaches for developing component based models by forgoing the pairing of interface definitions for components with a software framework instead it prescribes interfaces that let components communicate directly with each other independent of a framework this design choice was made to give more flexibility to component developers to optimize the data exchange process between model components the interface definitions for components are specified in a number of ways for different frameworks depending on the programming languages supported by the framework programming languages that were developed primarily for object oriented programming like c c and java have formal ways of specifying interfaces so that they can be inherited and implemented with details of their functioning to create framework compliant components for example c interfaces are specified in header files as classes with only pure virtual functions the openmi interfaces for c and java are specified this way on the other hand for component based modeling frameworks that support languages like c and fortran e g esmf and csdms models are required to register pointers to their standard functions with the framework these standard function pointers are then stored in virtual function tables that can be accessed by other components to achieve the interfacing functionality while the oms framework was written using java which is an object oriented programming language the oms interfaces are specified by marking classes functions and fields of a component with standardized java annotations e g in out execute etc which serve as a form of syntactic metadata through java s reflection capabilities annotated classes fields and functions can be accessed and invoked at runtime the typical core interfaces defined for models in component based modeling frameworks are the initialize run and finalize irf functions syvitski et al 2011 the initialize interface function is implemented to instantiate the resources and inputs needed by a model for a simulation the run interface is responsible for performing the underlying computations of a model e g performing a time step the finalize interface is implemented to dispose of the computational resources used by the model e g closing output file streams de allocating memory etc in addition to these core interfaces models also specify interfaces for the types of inputs and outputs that can be consumed and shared with other components respectively csdms attempts to standardize these elements that are shared by various component based modeling frameworks by providing a core set of interface definitions called the basic modeling interface bmi definitions peckham et al 2013 table 1 summarizes the design considerations we identified for component based modeling frameworks and standards used in the earth systems and environmental modeling arena in the sections that follow we discuss three of these design considerations that we believe are important areas where we identified opportunities for improvements to be made we focus on their support for spatiotemporal datasets options for data exchange workflows between components at runtime and support for simulations on hpc infrastructure given that openmi was the base from which we built hydrocouple we discuss how openmi is designed with respect to these three areas and contrast it with the design of other component based modeling frameworks limitations in these three areas are impediments to using openmi and more generally component based modeling in practice because important geospatial data structures used by many models are not currently supported data exchange workflows are restrictive or require re compilation for each coupled model composition and hpc simulations are either complex or not supported at all by existing frameworks 2 1 definition of spatiotemporal data structures the types of inputs that can be consumed and the outputs that can be supplied by models in a component based modeling framework are typically organized as multi dimensional arrays of data that can be accessed using indexes along their respective dimensions e g esmf csdms openmi these inputs and outputs are often further abstracted into domain specific types for many component based modeling frameworks for instance openmi provides a time space input output specialization that associates geometric primitives including points polylines polygons and polyhedra with time varying data for hydrologists this could be used to a represent hydrologic feature like a river network and its associated time varying flows in water resources modeling applications these features are often used for delineating a model s spatial domain and prescribing boundary or input data for models e g polygons for watershed boundaries polylines for alignments of rivers river cross sections etc missing from the openmi definitions is the topological information that provides the spatial relationships between adjacent geometries yet this information is critical for many applications for example although the individual cells of a two dimensional computational grid used for a hydrodynamic model of a reservoir may be represented by a list of polygons the adjacency information between cells that is needed to numerically approximate spatial gradients of variables are missing on the other hand the csdms suite of tools for component based modeling which were developed for ice terrestrial coastal and marine applications as well as esmf which was developed for global weather and climate predictions focus on providing interfaces for gridded datasets including logically rectangular unstructured and curvilinear grids in one two and three dimensions explicit support for geospatial data types like those provided in openmi are however not supported 2 2 data exchange workflows the model execution and data exchange workflow between components in a component based model composition is handled in a variety of ways for different frameworks esmf requires a modeler to write a driver program called an appdriver that contains the main routine for an esmf application collins et al 2005 this appdriver needs to be compiled for each application and is responsible for directing the time stepping and the data exchange between components the drawback with this approach is that a new driver needs to be written and compiled for each composition therefore a modeler will have to be a programmer with the know how and compilation tools to compile the application putting it out of reach for many water resources modelers who are typically not expert programmers in contrast to esmf oms provides a way for modelers to direct model execution and data exchange between components externally this is accomplished by letting users write the business rules for a simulation using a oms prescribed domain specific language dsl specified in an external file david et al 2013 in contrast to a general purpose programming language a dsl is a relatively simple specification language dedicated to a particular problem domain a particular problem representation approach and or a particular solution technique deursen and financial 1997 deursen et al 2000 david et al 2013 the benefit of this setup is that model users are able to direct the data exchange between components without recompiling the entire code or being restricted to the in built workflows provided by a component based modeling framework the primary model execution and data exchange mechanism in openmi is based on the pull based pipe and filter architecture buschmann 1996 with this method components exchange memory based data directly without an external data exchange workflow controller using a request and reply mechanism gregersen et al 2007 the most downstream component in a composition is designated as the controller trigger for an entire simulation where it requests the data it needs from upstream components that it is coupled to and waits for the data it requests to be returned before proceeding with its computations upstream components issue their own requests to their respective upstream components in a cascading fashion and wait to receive the data they requested before performing their computations as illustrated in fig 1 data exchange between an input defined by the ibaseinput interface and an output defined by the ibaseoutput interface can be mediated by an adapter defined by the ibaseadaptedoutput interface that performs the necessary data transformations e g temporal interpolation spatial interpolation unit conversion etc that are needed to supply the correct requested data from one component to another contextual adaptors are generated by a factory interface definition called an ibaseadaptedoutputfactory that uses the input and output that are to be mediated as query variables to generate appropriate adaptors while the request and reply data exchange approach requires no external data exchange controller and works for many applications it does not work for those compositions that have two or more downstream components as illustrated in fig 2 in fig 2 components d and e are not executed since they are not involved in the request and reply chain of the trigger component a yet there are circumstances where they may be needed as part of a coupled model composition for example component a might be a stream temperature model b might be a streamflow model and e might be a contaminant transport model both e and a need streamflow data from b but e would never get executed if a is the triggering component the openmi developers recognized that the pull driven data exchange approach might be too restrictive for certain applications therefore they suggested a loop driven data exchange workflow where the coupled system will loop over all components to let them check if they need to take action before proceeding moore 2010 the loop approach has been included in the interface standard of openmi 2 0 but the implementation for this loop driven approach is not provided as part of the openmi sdk in contrast to esmf and oms openmi may be easier to use for non expert programmers however the fact that the request and reply mechanism used by openmi may not work for all model compositions and the fact that the loop approach has not been implemented in openmi s sdk means that openmi could be further improved to meet additional use cases 2 3 support for simulations on hpc infrastructure increasingly the complexity of the hydrologic modeling applications that modelers embark on requires that model execution be conducted using distributed computing techniques on hpc infrastructure hpc infrastructure typically involves pooling of computational resources from many computers with multi core processors that are networked together esmf was designed to support both data and task parallelism on hpc infrastructure by making esmf components the very units of parallel execution to aid model developers in writing highly efficient and scalable codes collins et al 2005 a virtual machine vm approach is adopted to abstract away the details underlying model execution in hpc environments using the message passing interface mpi standard each component in an esmf composition is assigned a single vm comprised of one or more persistent execution threads pet where each pet is equivalent to a single mpi process a component may allocate its pets to child components additionally a developer can employ fine grained shared memory parallelism within a pet using openmp or other multi threading approaches inter and intra vm communications are handled by the framework in a fashion similar to mpi where data must be provided as raw language specific one dimensional contiguous arrays oms adopts a similar approach to esmf to achieve parallelism by making components themselves the units of parallel execution however unlike esmf oms only supports shared memory parallelism applications using multi threading where each model component is executed in its own separate thread in contrast openmi provides little direction on how to take advantage of hpc resources for more efficient simulations for scenarios where a downstream component requests input data from one or more upstream components an openmi component may make these requests using parallel threads in a multi threading environment for efficiency however this approach only scales up to the number of upstream components that supply data to a single component additionally in a bidirectional data exchange scenario between two coupled model components a and b openmi s request reply data exchange mechanism forces a sequential execution of the components involved component a requests the data it needs from b however component b needs data from component a before it can execute to avoid an infinite recursion loop component a provides an estimate of the data requested by component b so that component b can advance the computation from a performance perspective it may be desirable to execute both model components concurrently using estimates of data provided from the component that is coupled to them to the best of our knowledge existing component based modeling frameworks lack support for automated parallel simulations for those experimental evaluations that are embarrassingly parallel involving executing the same coupled model instances repeatedly with varied input parameters e g automated parameter estimation and uncertainty assessment e g blasone et al 2008 optimization e g zhang 2009 ensemble simulations e g komma et al 2007 etc while these types of applications can be currently undertaken within existing component modeling frameworks they can either only be configured to run sequentially require manual intervention or require writing code outside of the scope of a component based modeling framework 3 design of the hydrocouple framework to address the challenges discussed in the preceding sections we developed the hydrocouple component based modeling interface specifications and associated software hydrocouple builds on the strengths of openmi while advancing new interface definitions to deliver standard spatiotemporal dataset interfaces with their associated topological information and providing support for more efficient simulations on hpc infrastructure like openmi the hydrocouple interface definitions are language agnostic however we implemented the hydrocouple interface definitions using c because in addition to the benefits enumerated in the previous sections i e cross platform support relatively lower memory footprints mature set of tools and scientific libraries etc it also provides bindings with many programming languages and serves as a good foundation for a framework that seeks to integrate legacy model codes and support models written using different programming languages the interfaces for hydrocouple have been provided in four c header files with about 1500 lines of code that provide the core interface definitions a spatial extension a temporal extension and a spatio temporal extension in the following section we discuss the key choices that were made in the design of hydrocouple and the motivation behind them 3 1 types of hydrocouple components the core interface definition of the openmi standard is the ibaselinkablecomponent interface this interface encapsulates the computational engine of a model and defines the irf interfaces it is also responsible for defining the types of inputs a component can consume and the type of outputs a component can supply to other components in hydrocouple the ibaselinkablecomponent interface has been superseded by the icomponentinfo interface definition as the core interface definition this interface definition allows model developers to provide details about a component including documentation about the formulations employed in the component limitations of the component coupling configurations that can be employed data transformation adaptors that can be employed as well as details about the coupling configurations and application scenarios for which the use of the component is appropriate other details that can be specified through this interface include the name of the developer of the component contact information for the developer classification of the component to properly categorize component libraries in a gui and component version these details which can be used by modelers to seek further assistance are currently missing for components developed for many frameworks leading to poor understanding about their capabilities and their proper usage in component based modeling applications preferably these details should be specified in an external file and read by the component at runtime for display purposes in contrast to openmi which only allows a single component type instances of classes based on the icomponentinfo interface create three types of specialized components these specializations include classes that are based on the imodelcomponentinfo the iworkflowcomponentinfo and the iadaptedoutputfactorycomponentinfo interfaces as shown in fig 3 instances of classes based on the imodelcomponentinfo are responsible for creating instances of classes based on the imodelcomponent fig 4 interface which is equivalent to the openmi specification s ibaselinkablecomponent instances of classes based on the iadaptedoutputfactorycomponentinfo interface are responsible for creating instances of a new component type based on the iadaptedoutputfactorycomponent interface fig 4 this new interface definition can be implemented as an independently deployable component that is equivalent to openmi s iadaptedoutputfactory interface definition however unlike the openmi definition it is not bound to any particular component and can be reused between different models to generate adaptors for data exchange mediation finally instances of classes based on the iworkflowcomponentinfo interface are responsible for generating another new type of component based on the iworkflowcomponent interface fig 4 that is responsible for managing simulations and the data exchange workflow between components these new component types have been added to hydrocouple to avoid the duplicative work of developing new data exchange workflows and data adaptors for each component or coupled modeling application when using openmi and component based modeling frameworks in general for example a workflow component could be developed to automatically discover the dependencies that exist between model components and execute them in a sequence that ensures that all components are able to obtain the data they require a time series interpolation adapter could be developed as an independently deployed component that can be reused by different model component compositions this contrasts with the current openmi setup where an adaptor is bound to a component and needs to be implemented for every model component that needs to use it this generalizes and formalizes this process so that adaptors can be developed into components in their own right 3 2 hydrocouple spatiotemporal data structures interface definitions in order to reduce the cost of converting legacy models into components it is important that the low level geospatial data structures that hydrologists widely use are made available in component based modeling frameworks to achieve this goal hydrocouple provides an explicit implementation of the open geospatial consortium s ogc simple feature access sfa specification herring 2011 in addition to providing interfaces to describe various types of geometries the advantage of implementing the ogc sfa is that it defines functions for performing topological queries e g checking for intersection between features checking if one feature touches another etc and geospatial operations e g unions buffering symmetric difference etc that are useful in water resources modeling applications for instance an intersection operation can be used between gridded precipitation output from a weather forecast model and the boundary of a watershed in a hydrologic model to estimate the fraction of each grid cell that contributes precipitation flux to that watershed hydrocouple additionally prescribes interface definitions for various gridded dataset types for hydrodynamic and hydraulic modeling applications as well as representing some other gridded dataset types often utilized in the water resources area for example hydrocouple explicitly defines an interface for multi banded raster datasets that includes the number of raster grid cells cell size data type no data value etc this interface can be employed to represent various types of datasets in models including digital terrain models dtm aerial imagery time varying land use data etc in addition to this definition hydrocouple defines interfaces for cartesian rectilinear and curvilinear grids in two and three dimensions for hydraulic and hydrodynamics modeling applications time varying data for these grids may be associated with the nodes faces edges and volume area of their individual cells for networks unstructured meshes and polyhedra hydrocouple adopts the quad edge data structure proposed by guibas and stolfi 1985 with this data structure a directed edge stores the complete topological information about the resulting polyhedra network by storing pointers to its left and right face polygons and its origin and destination vertices these spatiotemporal interface definitions have been provided to abstract away the details of the various types of the same data structures provided by various software libraries for example a raster dataset can be stored as a geotiff file or a netcdf file which are accessed using different software libraries this was done so that data can be accessed within hydrocouple using common interfaces regardless of the underlying file format of the data a companion sdk was also developed to enable reading and writing of spatiotemporal data to and from files on disk using multiple file formats details of the implementation of these interfaces within the sdk using existing libraries are provided in subsequent sections 3 3 hydrocouple data exchange workflow interface definitions following the oms approach where the data exchange workflow can be specified externally the iworkflowcomponentinfo interface definition was introduced in hydrocouple as a way to provide modelers with flexibility in prescribing the data exchange workflow in a component based application for example openmi currently supports only the request and reply data exchange approach whereas hydrocouple adds the ability to create independent workflow components that can execute using a loop driven data exchange approach as well as others this interface creates an instance of an iworkflowcomponent that is responsible for keeping track of all components involved in a simulation and coordinating their computations and data exchange until the completion of a simulation just as the imodelcomponent interface has the irf functions we defined the iworkflowcomponent with these same functions to enable initializing the workflow component updating components associated with the workflow by asking them to perform their computations and finalizing the components upon completion of a simulation if the iworkflowcomponent interface is not instantiated for a simulation all components must default to the original openmi request and reply approach to data exchange this new interface was added so that modelers can directly control data exchange in a component based modeling simulation beyond the request and reply data exchange mechanism provided by openmi like model components a suite of independently developed and deployed workflow components may be developed so that model users are able to select and integrate the workflows that are most suitable for their simulations 3 4 hydrocouple on hpc infrastructure hydrocouple supports parallelized experimental simulations by introducing a new interface definition called the icloneablemodelcomponent interface that inherits from the imodelcomponent interface this new interface introduces a clone function that must be implemented for a component to make a copy of itself the clone function has been added so that independent copies of a model instance can be made for parallelized simulations details of the cloning approach are left up to the model component developer a parent model component keeps track of all of its child clones which can be accessed using the children function linkages with other model components are left up to the caller of the clone function this cloning process may involve making a copy of the parent icloneablemodelcomponent class and initializing it with the same arguments as the parent while making sure that outputs from the parent and child do not conflict an example optimization application of this cloning feature is described in buahin and horsburgh 2016 hydrocouple was designed to support simulations on hpc infrastructure by supporting parallel simulations on both shared and distributed memory systems as well as providing support for simulations that use general purpose computing on graphics processing units gpgpu support for distributed memory parallel computing was designed using the message passing interface mpi standard while the gpgpu parallelism was designed to support the open computing language opencl and nvidia s cuda framework the clone function described is only one of the ways to enable parallelism in hydrocouple like openmi hydrocouple provides interface definitions that must be implemented by a model developer component developers must implement all the mpi openmp or gpgpu functions that enable parallel computations the sdk we developed section 3 5 provides some resources that can be used to help in the implementation of these functionalities in designing hydrocouple s support for simulations on hpc platforms we followed the original openmi design choice of limiting the role of the framework in mediating the data exchange between components along this line hydrocouple applications adopt mpi for coarse grained parallelism where all components involved in a coupled modeling application are initialized and coupled directly on the mpi task process with rank 0 i e the master mpi process 1 as illustrated in fig 5 for components that support mpi a user may partition the remaining mpi processes to them to ensure the computational load is balanced fig 5 this is done by specifying the mpi ranks available to each component in a coupled model application configuration file frameworks like esmf automatically map computational resources into virtual machines that can be allocated to components for model execution on hpc resources while convenient in abstracting away the complexity of computational resources available it elevates the role the framework plays in mediating the data exchange between components additionally getting up to speed with the large software code stack and its accompanying complexity is beyond the expertise of many water resources model developers hydrocouple uses relatively few lines of code and distributes hpc resources directly by dividing available mpi ranks among components each component can group its allocated mpi ranks into an mpi communicator that can be shared among the component and its children this approach can be more transparent for users of shared hpc infrastructure that use job scheduling software like slurm and reduces the role of the framework developers of components need not necessarily conduct their computations on the master mpi process components initiated on the master mpi process could be developed as proxy stubs that serve as pathways for communicating results computed on child components on worker mpi processors for example in fig 5 the role of component a which is initialized on the master mpi process to communicate directly with components b and c may only involve collating computed results from its worker components initialized on mpi processes 2 3 and 4 and sharing with other components once initialized data sharing between components on different mpi processes can be conducted using standard mpi calls components on the master mpi processes are responsible for issuing messages to components on the worker processes to dispose themselves upon completion of a simulation fine grained parallelism may be employed over the cpu cores allocated to each mpi process using shared memory parallelism application programming interfaces apis like openmp like mpi gpgpu frameworks abstract away the complexity underlying computing resources from users and present a single virtual interface for accessing the hardware to ensure that gpgpu resources are distributed efficiently between components hydrocouple lets users prescribe the gpgpu device as well as the maximum number of cuda blocks or opencl work groups each component can use on each mpi process this gives users flexibility to partition jobs to gpgpu devices in a way that is tailored to the particular hardware layout on an hpc system this partitioning of computing resources can be accomplished using job scheduling software like the slurm workload manager that is used on many high performance computing centers systems 3 5 hydrocouple composer and software development kit in order to facilitate the development of components for hydrocouple we developed a sdk that implements many of the core interfaces needed to develop a component as a software class that has initialization arguments exchangeable inputs exchangeable outputs the sdk also implements the functions required to initialize a component by reading and validating initialization arguments and functions that describe the variables exchanged by component inputs and outputs this enables model developers to focus on the computational portions of their code which are placed within the correct locations inside the hydrocouple sdk classes the core interfaces that have been implemented as classes include those for describing and identifying components and their inputs and outputs the spatiotemporal domains of these inputs and outputs the variable types consumed and supplied by these input and outputs and the units of these variables in hpc environments the sdk automatically groups processors allocated to a component into a single private mpi communicator context so that communication between the component and its workers do not conflict with other components the sdk uses the geospatial data abstraction library gdal warmerdam 2008 extensively to provide support for reading various geospatial vector data formats e g shapefiles geojson gml cad kml etc into objects defined by ogc s sfa that can then be accessed programmatically by model components the sdk also enables writing these objects to files on disk e g for exporting model output the hydrocouple sdk also uses the gdal library to provide similar support for various raster dataset formats including the geotiff grib sqlite jpeg and hdf5 formats work is also underway to implement the ugrid version 1 0 http ugrid conventions github io ugrid conventions convention which is a proposal for storing unstructured mesh data in standard format using the unidata network common data form netcdf file using the climate and forecast cf metadata convention eaton et al 2011 as a starting point in addition to the sdk we developed the hydrocouple composer graphical user interface gui to provide model developers with a visual environment for composing coupled model configurations fig 6 the hydrocouple composer software displays all available model component libraries and allows users to drag and drop them onto to a graphical palette coupled model compositions are specified through a configuration file that lists the model components and their connection nodes as well as the workflow and adaptor components that are utilized each model component can be initialized using an initialization file that contains parameters specific to the component alternatively components can be initialized using instances of a class based on the iargument interface that are created in memory after a component is initialized compatible outputs and inputs from other components can be coupled interactively compatibility for coupling is determined by a function called canconsume on an instance of the input class that is to be coupled that is called when a model user tries to create a link between and input and an output this function returns true or false indicating whether the coupling is valid based on business rules defined by the component developers selecting an existing connection between an input and output displays the available contextually relevant adaptors that can be inserted to mediate data exchange for example when a connection between time series input and output is selected only time series related adaptors will be available for selection in the hydrocouple composer gui hydrocouple composer is responsible for partitioning computing resources to model components based on a user s specifications in the configuration file and launches simulations example configuration files can be accessed in the hydrocouple github repository see software availability section the hydrocouple composer also monitors the progress of simulations and displays them to a user logs messages from components and provides rudimentary visualization capabilities fig 6 for the standardized spatiotemporal inputs and outputs of model simulations finally when the executable of the hydrocouple composer is launched using predefined command line arguments it doubles as a command line interface for launching simulations on hpc resources in fig 6 model components are shown as squares connections between components are shown as directed arrows inputs are shown as blue circles and outputs are depicted as red circles 4 case study coupling a 1d and a 2d hydraulic model using hydrocouple in order to illustrate how hydrocouple s new interfaces facilitate the development of components and more efficient simulations we applied them to couple a 1d hydraulic model component that simulates flow through pipes culverts inlets outfalls and other urban stormwater infrastructure with a 2d hydraulic model component that simulates flow in rivers canals and overland areas the 1d hydraulic model component was developed using the epa s swmm model while the 2d model component is a new formulation we developed called the finite volume hydrologic model fvhm these two models were specifically chosen to illustrate 1 how the hydrocouple interface definitions and sdk can be used to convert legacy models into components 2 how to develop new components from scratch 3 how to handle the potential coupling configurations across models having differing dimensionality i e 1d versus 2d and 4 how coupled model components can be executed in parallel on hpc environments for more efficient simulations this particular type of 1d 2d hydraulic model coupling is widely implemented in the water resources modeling field because the tradeoffs between 1d and 2d hydraulic models complement each other 1d models are appropriate for simulating flows accurately and efficiently in channels pipes and other conduits with well defined shapes although 1d hydraulic models are generally more computationally efficient they are unable to accurately simulate lateral movement of flood waves into the floodplain and they incorporate topography and bathymetry using cross section profiles at various sections along the length of a river pipe whose placements are relatively subjective samuels 1990 hunter et al 2007 on the other hand 2d hydraulic models are more suitable for simulating landscape processes and overland flows albeit at a generally higher computational expense many urban hydrologic modeling scenarios e g stormwater runoff flooding design of green infrastructure and assessment of stormwater best management practices require accurate representations of both the drainage network and the urban landscape making a 1d 2d hydraulic model coupling ideal for simulating these scenarios the 1d 2d coupled model discussed in this manuscript was developed for a sub catchment in the city of logan utah to illustrate the benefits of using hpc resources we evaluated the simulation time speed up as more computational cores were allocated to the coupled model 4 1 study area the city of logan s stormwater conveyance system has its foundations in agricultural canals developed at the founding of the city to divert irrigation water from the logan river for farming as the logan river flows westwards through cache valley it is diverted at various locations along its length into these unlined canals that flow northward these same canals serve as the primary conduits for stormwater conveyance with many of the city s stormwater outfalls piped directly into them the boundary for the 5 81 km2 area we used for our modeling study encompasses the area that drains into one such canal called the northwest field canal fig 7 4 2 swmm model development the swmm code solves the 1d dynamic diffusive or kinematic wave equations for flows and water surface elevations over a network of conduits i e pipes canals and rivers and sub catchments connected together at their endpoints by nodes i e junctions outfalls storage units and flow dividers rossman 2006 we developed the swmm hydrocouple component from the epa swmm source code by modifying the code to expose those boundary data that are needed for coupling including the inflows outflows and water surface elevations at the inlets and outlets of the stormwater conveyance system in developing the swmm hydrocouple component we employed the hydrocouple network interface definition to represent the network of swmm nodes connected together by conduits the benefit of this is that the inflows and outflows as well as water surface elevation data that the swmm component supplies to components coupled to it provide the topological information that is needed to traverse the entire stormwater conveyance system developing this component required knowledge of c c so that we could modify the swmm code which was written in c and then recompile it into a compliant component this process would be similar for other models but in general the level of effort required to develop a component is subjective it depends on the programming skill of the developer the degree of understanding about the model being wrapped and the complexity of the model component being developed the swmm model code has been parallelized in many sections to improve performance using openmp as described by burger et al 2014 the parallelism introduced in swmm is a shared memory type of parallelism i e it uses a single mpi process but scales with number of cores allocated to the mpi process this feature served as the basis for our investigation of the performance of the coupled model as more cpu resources were added we developed the swmm model instance using detailed survey data of sizes for stormwater pipes inlets and outfalls provided in shapefile format by logan city the conduit diameters ranged from 0 30 to 1 38 m with lengths ranging from 0 5 to 390 m this dataset resulted in a swmm model with 1769 conduits 2093 junction inlets and 138 outfalls we executed the swmm model using swmm s adaptive timestep option with a minimum timestep of 0 01 s and a maximum time step of 5 s a maximum number of 20 iterations was selected for each time step 4 3 fvhm model development fvhm was developed to solve the shallow water equations over an unstructured triangular irregular network tin mesh using an implicit finite volume method details for the formulations and hydrologic process representations used are provided in appendix a in the context of this case study fvhm was developed to simply route flows in riverine and overland areas without many of the hydrologic processes that are typically represented in hydrologic models e g infiltration evapotranspiration etc however fvhm fully exposes the geospatial data structures needed to demonstrate how different configurations can be used to couple 1d and 2d models and run them on hpc we purposefully developed fvhm for this case study to focus on the model coupling data structures and configurations without the complexity introduced by representing many hydrologic processes in the 2d model component because the hydrologic processes represented in the 2d model are independent of the data structures and coupling configuration the techniques we demonstrate here are generalizable to 2d models that include more detailed hydrologic process representations given that our 2d model does not represent processes like infiltration the model configuration presented here represents a worst case scenario simulation and is a useful exercise for evaluating the performance of the stormwater infrastructure under a high intensity rainfall event in developing the fvhm model we directly utilized the data structures provided as part of the hydrocouple interface definitions for example the computational mesh used in fvhm adopts a tin interface definition with its associated quad edge data structure for creating boundary conditions the geometry interfaces prescribed by the ogc sfa specification were adopted for instance polygons were used to demarcate the area where precipitation inputs apply polylines were used to define the mesh edge boundaries where inflows and outflow apply the implementations of these interfaces and the file input and output implementations provided by the hydrocouple sdk simplified the model development process by allowing us to focus on the computational parts of the code we were actually interested in we were also able to use the capabilities of hydrocouple to enable parallelism in executing the model fvhm uses both fine and coarse grained parallelism in its code fine grained parallelism using openmp is employed in several areas in looping over each of the computational cells to calculate spatial gradients of water surface elevations and velocities friction and to apply boundary conditions since fvhm uses an implicit time marching scheme systems of linear equations need to be solved at each time step to compute velocities and water surface elevations for each cell fvhm uses the hypre software library falgout and yang 2002 which solves large sparse linear systems of equations on massively parallel computers fvhm partitions the system of equations it solves at every time step into smaller chunks to be solved in different mpi processes using the hypre library for the fvhm model instance we developed its computational mesh using sub meter high resolution light detection and ranging lidar data collected in 2005 this dataset was supplemented with the 10 m elevation data from the united states geological survey s usgs 3d elevation program the mesh contained 44861 cells with sizes ranging from 0 1 to 18900 m2 this range of cell areas was the result of refining the model mesh along the canal where we were interested in evaluating in more detail and coarsening the mesh in the upstream overland areas where we were only interested in estimating runoff for the boundary conditions a 30 min resolution time varying precipitation time series for the nearest rain gauge was developed using the 25 year 24 h design storm totaling 61 2 mm and the natural resources conservation service nrcs type ii cronshey 1986 rainfall distribution curve this storm is the prescribed storm for designing urban stormwater infrastructure in logan thus is a useful test for the coupled model for diversion flow from the logan river into the northwest field canal we applied the maximum legally allowable irrigation diversion of 1 351 m3 s for the entire duration of the storm to evaluate worst case inundation conditions e g an intense storm during a time where the canal was full of irrigation water we executed the fvhm model using the adaptive time step option with a minimum timestep of 0 01 s and a maximum time step of 5 s a maximum number of iterations of 200 was specified for the for each time step 4 4 1d 2d model coupling configurations in coupling swmm and fvhm we adopted different coupling configurations depending on the relationship between the water levels in the coupling cell of the fvhm model water level in the coupling inlet or outfall outlet of the swmm model ground surface elevation at the coupling interface and the invert elevations of coupling inlets or outfalls outlets in the swmm model fig 8 for all cases we adopted a bi directional exchange of boundary condition data at the coupling node interfaces between the two models this bi directional data exchange proceeds by passing the water surface elevation from the fvhm model to the swmm model and then passing outflows from the swmm model to the fvhm model for the case where the water surface elevation z m of an inlet in the 1d model is less than the bottom elevation of its overlying cell in the 2d model z s and the water depth in the 2d cell is greater than a i n w fig 10a the free weir equation equation 1 was used to estimate discharge into the inlet 1 q c w w 2 g z w z s 3 2 where q is the discharge into the inlet c w is the weir discharge coefficient w is the weir crest width g is acceleration due to gravity z w is the water surface elevation in the overlying 2d cell and a i n is area of the inlet opening if the water depth in the 2d cell is greater than a i n w fig 10b the orifice equation equation 2 is used 2 q c o a i n 2 g z w z s 1 2 where c o is the orifice discharge coefficient the discharges are computed in the 1d model and added as sinks in the continuity equation of the 2d cell and as sources to the inlet of the 1d model for the case where an inlet node of the 1d model is completely submerged by the water in the 2d model fig 10c the water level in the corresponding 2d cell is set as the water level in the receiving inlet of the swmm model the resulting surcharge or inflow values for the inlet are then applied as a sink source term in the continuity equation of the 2d cell 4 5 performance of various hpc configurations we executed the coupled model on up to 10 hpc nodes mpi processes to evaluate the benefits of devoting more computing resources fig 9 shows maximum inundation depths for each cell computed for the entire duration of the simulation the results of the experiment comparing the relative speed up as the number of mpi processes are increased to the optimal linear speed up desired are shown in fig 10 despite the rudimentary parallelism introduced into the fvhm model the results show up to a 5 times speedup with 10 hpc nodes different models and different coupling configurations may achieve different levels of speedup but this demonstration illustrates the capabilities of hydrocouple to enable simulations on hpc 5 discussion in our search of the literature related to openmi we found numerous papers examining the mechanics for developing openmi components and applying them to hypothetical or real world model coupling scenarios assessing the performance models that are coupled using openmi and the interoperability of openmi with other modeling frameworks hydrologic information systems and web services technologies for example elag et al 2011 assessed the mass balance errors of a hypothetical coupling scenario involving the coupling of simplified solute transport models including a surface and a sediment media model that were temporally misaligned bulatewicz et al 2010 successfully coupled agricultural groundwater and economic models to evaluate the impacts of alternative water use policies for a major aquifer in the us goodall et al 2011 and castronova et al 2013 demonstrated the service oriented modeling paradigm where remote models were coupled using web services technologies like the ogc s web processing service wps protocol we found it encouraging that openmi has been used for a variety of modeling use cases which further supports our decision to do this work in the context of openmi however we found relatively few studies that specifically focused on the architectural design of openmi and its advantages and drawbacks lloyd et al 2011 assessed the degree of framework invasiveness of openmi and other component based modeling frameworks using a single modeling use case across multiple frameworks they defined framework invasiveness as the degree of dependency between a modeling framework implementation and its associated models openmi showed a moderate degree of framework invasiveness when compared to oms which was lower and esmf which was higher however there was some subjectivity in the implementation of the model components that were coupled across the various frameworks and their study used the older openmi 1 4 version knapen et al 2013 examined the suitability of using openmi as a model integration platform across disciplines through a workshop that was organized to solicit feedback from both software developers and model application specialists in addition to identifying all the benefits of using openmi we enumerated above they identified the following as areas where improvements could be made 1 provision of data analysis and visualization tools 2 support for multi threaded executions 3 programming language bridges 4 support for standard spatiotemporal data structures 5 semantic integration and 6 support for models outside of the hydrologic modeling realm it must be noted that this assessment was also conducted using the older openmi version 1 4 the newer openmi version 2 0 addresses some of these challenges including the provision of interfaces that make it easier to integrate semantics and support for models outside of the hydrological modeling realm the work presented in this paper addresses some of the remaining challenges highlighted in the workshop discussed by knapen et al 2013 as well as those we encountered in our use of openmi hydrocouple s interface definitions and associated software tools build on the strengths of the openmi 2 0 standard by advancing interface definitions to better facilitate water resources modeling applications and data structures that have heretofore been missing in existing component based frameworks these interfaces include new topologically aware geospatial data structures based on widely accepted standards like the ogc sfa specification customizable data exchange workflows and support for parallelized simulations on hpc infrastructure while the hydrocouple interface definitions are programming language agnostic we implemented them using c to ensure that the code can be compiled on most operating systems another benefit of c is that unlike c it has no dependency on a framework like net which can play a role when installing model compositions on computational resources or on different operating systems compositions can be compiled in such a way that installation of the software involves copying the executable dynamic link libraries and additional files involved in a modeling application which does not require administrative privileges additionally the c hydrocouple implementation provides a way to avoid the data marshalling costs that often arise when legacy models are wrapped for component based modeling frameworks using interpreted languages c provides direct memory access to data for models developed using languages like c and fortran which have traditionally been used for model development because of their efficiency this helps address the design considerations 1 2 and 3 identified in table 1 for component based modeling frameworks existing openmi components developed by wrapping legacy models can be ported to hydrocouple components with minimal effort since hydrocouple adopts most of the openmi data structures and concepts additionally hydrocouple provides explicit implementations of standard geospatial dataset formats and associated topological information that are missing from existing component based modeling frameworks these data structures are widely employed for delineating model domains and prescribing boundary conditions in many water resources modeling applications i e design consideration 5 in table 1 in addition to better facilitating embarrassingly parallel experimental simulations within component based modeling frameworks through the addition of functions that create independent copies of model instances hydrocouple allows users to partition available hpc computing resources in a way that is transparent to model components i e design consideration 4 in table 1 the 1d 2d coupled hydraulic modeling example described illustrates the performance benefits to be gained especially in the hydrologic modeling community by moving into the hpc arena this application was developed to test the model code and also serve as an example of how to develop new model components for hydrocouple the model is shared in the github repository https github com hydrocouple fvhmcomponent examples the sdk and hydrocouple composer model coupling gui environment facilitate the development of components that use hydrocouple i e design consideration 7 in table 1 the sdk implements the core hydrocouple interfaces as well as spatiotemporal datasets and their file input and output operations so that the costs of converting existing models into components and in developing new models are reduced the hydrocouple composer gui allows users to interactively select and couple models launch simulations monitor simulations and visualize results of simulations on a single desktop or on hpc systems 6 conclusions we advanced these new hydrocouple interface definitions in hopes that they can be considered for inclusion in future versions of the openmi standard while we developed hydrocouple based on the lessons we learned using openmi so that it addresses the drawbacks we encountered we understand that it may not completely support the wide array of simulation types undertaken by water resources modelers and the wider earth systems modeling community in general while we have used the name hydrocouple to describe the software presented in this paper and have described it in terms of water resources modeling applications the advancements introduced in this manuscript are also applicable to other earth systems modeling fields we envision the continued improvement of the interfaces prescribed through hydrocouple as a community effort and have therefore shared the interface definitions hydrocouple composer gui as well as all the components we have developed so far in a transparent manner in a publicly accessible source code repository https github com hydrocouple through this repository users can contribute new hydrocouple components and improvements to the hydrocouple interfaces definitions its associated software and available components back to the repository for the benefit of the larger component based modeling community we have also communicated the advancements we have implemented to the openmi association so that they can be considered for inclusion in future versions of the openmi standard unlike their atmospheric modeling counterparts who have had a long history of executing their models on hpc infrastructure most water resources modeling practitioners have traditionally executed their models on single desktop machines there is however an increasing recognition that hpc is needed to tackle challenging problems such as simulating the interaction of land surface hydrologic processes with the atmosphere at the global scale evaluation of different model structures and uncertainty assessment hydrocouple supports this direction by prescribing interface definitions that allow users to partition cpu and gpgpu computing resources among components for more efficient simulations the proliferation of many component based modeling frameworks has made it difficult to fully achieve their promises of reusability and ability to conduct more holistic modeling evaluations while hydrocouple does not fully address this challenge it is not a completely new effort developed from scratch as so many other proposed component based modeling frameworks as much as was possible it builds on the core concepts and interface definitions used in the widely cited openmi standard and therefore makes translating the many existing openmi components into hydrocouple components and vice versa relatively inexpensive for example the swmm component described in this manuscript was adapted from one originally developed for our previous openmi application described in buahin and horsburgh 2015 it is clear that in order to avoid the challenge of over proliferation of component based modeling frameworks framework interoperability needs to be a priority among component based modeling practitioners many component based modeling frameworks share common characteristics e g the specification of irf interfaces these common characteristics can be used as leverage to create components that are able to use different frameworks hydrocouple supports this goal by providing framework and programming language independent interface definitions furthermore work is ongoing to create a wrapper component that automatically wraps components that implement the csdms bmi interface definitions so that they can be coupled to other models within the hydrocouple framework acknowledgements this research was supported by national science foundation epscor grant iia 1208732 awarded to utah state university as part of the state of utah epscor research infrastructure improvement award any opinions findings and conclusions or recommendations expressed are those of the author s and do not necessarily reflect the views of the national science foundation appendix a fvhm hydrocouple component model formulation the fvhm hydrocouple model solves the shallow water equations swe equations a 1 and a 2 using the finite volume approximation over a triangular irregular network mesh a 1 ζ t h v q a 2 h v t h v v g h ζ τ b ρ γ h v f h where ζ is the water surface elevation t is time h is the water depth v is the velocity vector q is the sum of external fluxes g is the acceleration due to gravity τ b is the bed shear stress friction vector ρ is water density γ is the sum of the kinematic ν and eddy viscosities ν t and f is the vector sum of external forces the bed shear stress is calculated using the manning s roughness equation shown in equation 3 a 3 τ b x τ b y ρ g n 2 h 3 u v u 2 v 2 where n is the manning s roughness coefficient and u and v are the velocities in the x and y direction respectively the fvhm component was developed to especially handle hydrologic modeling applications that often involve prolonged periods of dry spells with non existent or small water depths which is necessary to simulate areas with climate comparable utah and the intermountain western u s the finite volume approximation estimates the average value of a conserved quantity in an arbitrarily shaped control volume using an integral version of partial differential shallow water equations the theorem underlying the finite volume method is gauss s divergence theorem equation 4 which may be interpreted physically as the integral of the divergence of a vector i e a in a control volume i e cv is equal to the sum of the components of the vector normal i e n to surfaces of area of the control volume i e a versteeg and malalasekera 2007 a 4 cv a d v a n a d a to illustrate how the gauss theorem is used in the derivation of the finite volume numerical approximations for the shallow water equations in the fvhm component we apply it to the transport of velocity v in the control volume p surrounded by neighboring control volumes n1 n2 and n3 as depicted in fig a1 in fig a1 a b and c represent the nodes of the triangle for the control volume p η e η e n c and represent the length unit vector unit normal vector and centroid of the common edge between control volume p and its neighboring control volume n1 respectively ξ and e ξ represent the length and unit vector for the distance between the centroids of the control volumes p and n1 respectively and r pc and r cn represent the vector distances between the centroids of the control volumes p and n1 with the centroid c of their common edge respectively fig a 1 control volume p surrounded by neighboring control volumes n1 n2 and n3 fig a 1 in fvhm the collocated grid arrangement used by several investigators including peric 1985 and lai 2009 is adopted this grid arrangement involves calculating the control volume velocities depths and water surface elevations at the centroid of each control volume applying the gauss theorem to the momentum conservation equations yields equation 5 a 5 a h v t d a η e n h v v d η a g h ζ d a a τ b ρ d a η e n γ h v d η a f d a where a is the area of the control volume the numerical approximations for the terms in equation 5 are as follows a 6 a h v t d a h n 1 v n 1 h n v n a δ t a 7 a τ b ρ d a τ b ρ a a 8 a g h ζ d a g h ζ x ζ y a a 9 a f h d a f h a where the superscript n 1 and n represents the current timestep and the previous timestep respectively and δt is the current timestep the water surface elevation spatial gradients ζ x ζ y and all spatial gradients in fhvm are estimated using the least squares gradient reconstruction approach from cell centered values of neighboring cells in the previous time step previous iteration referring to fig a1 the water surface elevation of a neighboring cell n i e ζ n surrounding cell p can be estimated from the water surface gradient at p using equation 10 a 10 ζ n ζ p ζ x p δ x p n ζ y p δ y p n where δ x p n and δ y p n represent the distances in the x and y directions from the centroid of the control volume p to the centroid of the neighboring control volume n assembling equation 10 for all neighboring cells into a linear system of equations yields equation 11 which is solved using the qr decomposition method in fvhm a 11 δ x p n 1 δ y p n 1 δ x p n 2 δ y p n 2 δ x p n 3 δ y p n 3 ζ x p ζ y p ζ 1 ζ p ζ 2 ζ p ζ 3 ζ p the derivation of the numerical approximations of the more complex diffusion and advection terms in the momentum equation are provided in the following sections a 1 discretization of the diffusion term the diffusion term in the momentum equation is discretized as follows a 12 η e n γ h v d η a l l s i d e s γ c h c e n v n e n v η e η δ η where γ c and h c are the viscosity and the water depth at the centroid of the common edge between the control volume p and it neighboring cell these values are estimated using gradients calculated from the gradients derived from the least squares gradients reconstruction method described earlier it can be shown from trigonometry as detailed in versteeg and malalasekera 2007 that the direct gradient and cross diffusion terms of equation a 12 for each neighboring cell can be represented by equation a 13 a 13 e n v n e n v η e η e n e n e n e ξ v n v p δ ξ d i r e c t g r a d i e n t e ξ e η e n e ξ v b v a δ η c r o s s d i f f u s i o n where v p and v n are the cell velocities for p and n respectively and v b and v a are cell p s interpolated nodal velocities for the shared edge between cells p and n two approaches are available in fvhm for computing the cell turbulent eddy viscosity the first is the parabolic eddy viscosity model equation a 14 a 14 ν t c t u h where c t is theoretically equal to κ 6 with κ being the von kármán constant wu et al 2014 the second eddy viscosity model is the smagorinsky lilly model smagorinsky 1963 shown in equation a 15 a 15 ν t c s a u x 2 v x 2 1 2 u y v x 2 where c s is the smagorinsky constant with values that are usually between 0 1 and 0 2 a 2 discretization of the advection term following lai 2009 the advection term in the momentum equation is discretized as follows a 16 η e n h v v d η a l l s i d e s h c v c n 1 v c n 1 δ η where the superscript n 1 refers to the previous iteration for the current timestep v c is the normal velocity to the current edge at its centroid where a positive value indicates an outward flow from the control volume and a negative value indicates inflow into the control volume and v c is the velocity vector at the centroid of the current edge that is to be calculated at the current timestep v c is estimated as a function of the velocity of the current cell p and its neighboring upstream and downstream cell velocities using equation a 17 a 17 v c v p 1 2 ψ r v d v p where v d is the velocity of the control volume downstream of the current cell p and ψ r is a flux limiting function of r the variable r is the upwind ratio of consecutive gradients of velocity defined using equation a 18 a 18 r v p v u v d v p where v u is the velocity of the control volume upstream of the current cell p equation a 17 assumes that the centroid of the edge c is equidistant from the centroids of the bounding control volumes hence the multiplication factor 0 5 however for unstructured grids this may not be true therefore an inverse distance interpolation weighting factor l is applied in equation a 19 instead of the 0 5 as recommended by denner and van wachem 2015 a 19 l r p c r p c r c n assuming the direction of flow is from p to n1 it is easy to locate the downstream control volume with velocity v d in figure a1 that is to be used in calculating r however finding the upstream control volume v u is not straightforward to overcome this challenge darwish and moukalled 2003 derived equation a 20 for calculating r a 20 r 2 v p r p n v d v u where r p n is the vector distance between the centroids of the control volumes p and n using ψ r 0 leads to the edge velocity being the same as the cell velocity representing the upwind differencing scheme while the upwind differencing scheme is stable and results in smooth solutions it is only first order accurate ψ r 1 represents the central differencing scheme which while second order accurate can lead to spurious oscillations with problems that exhibit sharp discontinuities in velocities as is common with higher order schemes to obtain stable and non oscillatory solutions for higher order schemes the function ψ r must be monotonicity preserving monotonicity preserving schemes ensure that solutions do not create local extrema additionally the value of a local minimum must be non decreasing and the value of a local maximum must be non increasing versteeg and malalasekera 2007 monotonicity preserving schemes have a property that the total variation tv i e equation a 21 of the discrete solutions should be total variation diminishing tvd tvd schemes are characterized with tv values that decrease with time as shown in equation a 22 a 21 t v u x d x a 22 t v u n 1 t v u n sweby 1984 provides the necessary and sufficient conditions for ψ r to be tvd in terms of a relationship between r and ψ r several ψ r functions that meet these conditions are provided in fvhm including those shown in table a 1 table a 1 tvd flux limiter functions table a 1 name limiter function ψ r source van leer r r 1 r van leer 1974 van albada r r 2 1 r 2 van albada et al 1982 umist max 0 m i n 2 r 1 3 r 4 3 r 4 2 lien and leschziner 1994 quick max 0 m i n 2 r 3 r 4 2 leonard 1988 min mod min r 1 0 i f r 0 i f r 0 roe 1985 a 3 pressure velocity coupling the discretization for the momentum equations provided can be organized into a linearized system of equations for all control volumes based on the control volume center values and can be solved implicitly for new velocities using equation a 23 a 23 a p v p n 1 3 a n v n g h a ζ s h a where a p is the coefficient of the velocity of the current cell p a n are the coefficients of the neighboring cells and s is the sum of the external forces and constants acting on the control volume we seek to solve equation a 23 for control volume velocities as well the water surface elevations the momentum equation is non linear because it involves the multiplication of two velocity terms additionally for incompressible flows cell velocities and pressures are coupled in a non linear fashion through the momentum and continuity equations in fvhm the semi implicit method for pressure linked equations simple patankar and spalding 1972 or alternatively simple consistent simplec doormaal and raithby 1984 iterative solution procedures are adopted to deal with these nonlinearities first it is important to note that in the collocated grid arrangement where velocities and water surface elevation values are estimated for the centroid of each control volume a highly non uniform water surface elevation field can act like a uniform field when the gradients of the water surface elevation fields are calculated numerically this may lead to the well known checker board pressure field effect which in turn leads to non physical results versteeg and malalasekera 2007 to overcome this problem rhie and chow 1983 proposed the momentum interpolation equation shown in equation a 24 to calculate the edge normal velocity v c a 24 v c 1 0 l v p l v n e n 1 0 r ξ e n 1 0 l a p a p l a n a n g h c ζ n ζ p g h p 1 0 l ζ p g h n l ζ n r ξ the simple and simplec iterative solution procedure begins by using the initial or previous iteration water surface elevation values ζ to solve the momentum equation i e equation a 23 for an intermediate velocity field v as shown in equation a 25 a 25 a p v p n 1 3 a n v n g h a ζ s since the initial water surface elevation used is a guess from the previous iteration or time step the computed velocities are likely not correct a water surface elevation ζ that corrects the water surface elevation ζ is therefore defined as shown in equation a 26 similarly a new velocity v that corrects the calculated intermediate velocity v is also defined a 26 ζ p ζ p ζ p a 27 v p v p v p subtracting equation a 25 from 23 yields a 28 a p v p n 1 3 a n v n g h a ζ the velocity correction is then calculated from equation a 28 as a 29 v p n 1 3 a n v n g h ζ a p ignoring the minor terms in equation a 29 and inserting it into equation a 27 yields a 30 v p v p g h a ζ a p the velocities estimated in equation a 25 do not satisfy the continuity equation therefore the water surface elevation correction equation i e equation a 26 and the correction velocity equation i e equation a 30 are used in the continuity equation to derive the water surface elevation correction values ζ these computations form the basis for the simple method to illustrate it we derive the finite volume approximation of the continuity equation as a 31 ζ p n 1 ζ p n δ t a p n 1 3 δ η h c v c q a p where q represents external inflows inserting the water surface elevation correction equation and the rhie chow interpolated control volume edge velocities from the velocity correction equation into the continuity equation yields a 32 ζ p ζ p n 1 ζ p n δ t a p n 1 3 δ η h c n 1 v c 1 0 r ξ e n 1 0 l a p a p l a n a n g h c ζ n ζ p n 1 q a p in the simplec method the minor terms ignored in equation a 30 are included in the continuity equation to estimate that water surface elevation correction values to recap for each time step the solution process begins by using the initial or previous iteration values of water surface elevations and velocities to calculate intermediate velocity values v for each control volume the water surface correction equation equation a 32 is then solved to obtain correction values ζ for each control volume the water surface values and velocities are then corrected using equations 26 and 27 respectively if convergence is not achieved the whole process is repeated all the systems of equations generated by fvhm are solved using the algebraic multigrid preconditioned generalized minimal residual method gmres from the hypre software library falgout and yang 2002 which solves large sparse linear systems of equations on massively parallel computers even though the courant friedrichs lewy cfl condition for stability does not apply for the implicit time marching approach adopted in fvhm the use of an excessively large time step can lead to inaccurate estimates durran 2013 an adaptive time step approach was therefore adopted using a user specified maximum courant number as a controlling variable the time step at the beginning of each iteration is estimated by dividing the maximum user specified courant number c o by the maximum courant number for the control volumes at the current time step as shown in equation a 33 a 33 δ t c o o u t g o i n g f l u x e s v o l u m e m a x a 4 wetting and drying tracking of the wetting front at the boundaries between wet and dry cells in hydraulic models is important because of the numerical instabilities that would arise from the unrealistically high velocities that would be calculated by dividing volumetric fluxes by the small water depths in dry cells kim et al 2012 many approaches have been proposed for the proper treatment of wetting and drying cells in hydraulic models including the thin film element removal depth extrapolation and negative depth algorithms as discussed by medeiros and hagen 2013 a common feature for the treatment of wetting and drying cells in many hydraulic models involves first classifying cells as wet partially wetted or dry depending on the number of cell nodes that are submerged a wet cell has a water surface elevation that submerges all the nodes of the cell by a certain small threshold value e g 1e 7 m a partially wetted cell has a water surface elevation that submerges at least one node of a cell by a certain small threshold value a dry cell has a water surface elevation that does not submerge any of the nodes of a cell for each time step the momentum equations are only solved for wet cells and velocities for dry cells are set to zero velocities and water surface elevations for partially submerged cells are then extrapolated from neighboring wet cells the application of this approach to hydrological simulations that solve the full dynamic wave model however introduces some challenges hydrological simulations often involve long periods with small or no runoff generation additionally hydrological models often involve large areas coarse and often steeply sloped computational cells are often employed for computational efficiency this leads to the frequent occurrence of partially wetted cells that cause a no flow phenomenon where water is unable to leave a cell because of ponding in the lowest corner of a cell where the water surface elevation is below the two nearest cell edge midpoints begnudelli et al 2006 kim et al 2012 warnock et al 2014 also the typical assumption made in many hydraulic models that the water surface elevation at the centroid is equal to the water depth plus the elevation of the centroid of the cell does not hold for partially wetted cells to address this challenge we adopted the volume free surface relationship vfr proposed by begnudelli and sanders 2006 in fvhm to deal with partially wetted cells the vfr relationship makes a distinction between the free water surface elevation at the centroid and the depth at the centroid by assuming sheet flow for partially wetted cells this is done by calculating the flow depth as a ratio between the fluid volume in the cell and the area of the cell the vfr approach provides equations to quickly transform water surface elevation to depths and vice versa for triangular cells to support modeling in fvhm the momentum equations are solved for wet cells and partially wetted cells with depths above a specified threshold while setting velocities for dry cells to zero the mass balance equations are then solved for all cells in the model domain a 5 fvhm component verification the fvhm component was verified using test problems 1 and 6 from macdonald 1996 problem 1 involves subcritical flow through a channel with a rectangular cross section problem 6 involves flow through a channel with a rectangular cross section where there is a transition from subcritical flow to critical flow and then a hydraulic jump i e transcritical near the end of a channel the attributes for the two test problems are shown in table a 2 table a 2 properties for macdonald 1996 test problems table a 2 problem channel width m channel length m manning s roughness n inlet flow m3 s outlet water surface elevation m 1 10 1000 0 03 20 0 800054 6 10 150 0 03 20 1 700225 fig a 2 comparison of analytical results from macdonald 1996 test problems with fvhm fig a 2 
26350,the use of existing component based modeling frameworks for integrated water resources modeling is currently hampered for some important use cases because they lack support for commonly used topology aware spatiotemporal data structures additionally existing frameworks are often accompanied by large software stacks with steep learning curves others lack specifications for deploying them on high performance heterogeneous computing hpc infrastructure this puts their use beyond the reach of many water resources modelers in this paper we describe new advances in component based modeling using a framework called hydrocouple this framework largely adopts the open modeling interface openmi 2 0 interface definitions but demonstrates important advances for water resources modeling hydrocouple explicitly defines standard and widely used geospatial data formats and provides interface definitions to support simulations on hpc infrastructure in this paper we illustrate how these advances can be used to develop efficient model components through a coupled urban stormwater modeling exercise keywords component based modeling model coupling openmi integrated water resources modeling high performance heterogeneous computing software availability name of software the software described in this paper includes the hydrocouple component based modeling interface definitions the hydrocouple software development kit sdk and the hydrocouplecomposer component coupling graphical user interface gui and command line interface cli the two hydrocouple compliant components we developed for this study include a two dimensional hydraulic model component called the finite volume hydrologic model fvhm and a component developed from the environmental protection agency s stormwater management model swmm code developer caleb a buahin contact caleb a buahin address 8200 old main hill logan ut 84322 8200 usa email caleb buahin usu edu required hardware and software any computer that runs the windows linux or macintosh operating system the performance of the swmm and fvhm components benefit from compilation with a compiler that supports openmp http www openmp org fvhm can be compiled with the message passing interface mpi enabled compilation of all the software codes enumerated requires the qt 5 7 framework http www qt io cost the hydrocouple interface definitions software stack and model components in this manuscript are freely available under the gnu lesser general public license lgpl and can be downloaded from the hydrocouple github repository https github com hydrocouple 1 introduction the goal of integrated assessment in environmental and natural resources management is to provide information within a decision making context that brings together a broader set of domains methods styles of study and or degrees of certainty than would typically characterize a study of the same issue within the bounds of a single research discipline parson 1995 laniak et al 2013 in order to make the complexity surrounding integrated assessment studies more tractable a need has also arisen to integrate computer models from diverse fields so that scientists can conduct more holistic assessments in particular for water resources specialists the need for model integration arises frequently because although many individual hydrologic processes have existing mathematical models that are able to simulate them under a given set of circumstances there is rarely a single model that can simulate all of them at the different scales and complexities desired while accounting for feedbacks between the various sub processes for integrated assessment studies beven et al 1980 argent et al 1999 selecting a particular hydrologic model requires a consideration of the specific management challenges of concern the spatial and temporal scales of interest model input data availability and computing requirements among other considerations beven et al 1980 leavesley et al 2002 argent 2004 voinov and shugart 2013 chowdhury and eslamian 2015 clark et al 2015 when a single model cannot meet the needs of a modeling study it is common for modelers to couple elements of multiple models together to form a more holistic or accurate representation of a water system this coupling must not only be considered as a technical exercise of stitching models together but must also include semantic and conceptual harmonization between coupled models janssen et al 2011 voinov and shugart 2013 although model developers in the earth systems and environmental modeling field have used several approaches to couple models for their integrated modeling efforts the component based modeling approach is increasingly receiving more attention this is because in contrast to monolithic approaches where models are compiled into a single code base or executable unit component based model development promises improved flexibility in the selection of the best available modeling approach for each application domain and re use of different models and more maintainable and extensible models fröhlich and franz 1999 szyperski 2002 the component based modeling paradigm involves the provision of interface definitions describing standard data structures and functions that models must implement so that they can be deployed independently to exchange information at runtime with other models model developers across diverse fields who adopt these interface definitions can develop models that can be coupled with other models to simulate complex earth and environmental systems component based modeling provides a natural avenue for experimenting with different model formulations since model components can be removed and added to a composition in a plug and play fashion several component based modeling frameworks and interface standards with varying degrees of complexity and application domains have been developed over the years these include the earth systems modeling framework esmf hill et al 2004 community surface dynamics modeling system csdms peckham et al 2013 the object modeling system oms david et al 2002 and others in the water resources modeling arena the open modeling interface openmi moore and tindall 2005 definitions have been tested and used extensively e g smolders et al 2008 castronova and goodall 2009 goodall et al 2011 buahin and horsburgh 2015 the appeal of openmi revolves around the fact that instead of providing a framework that has an accompanying large and complicated software stack that is beyond the expertise of many water resources modelers the openmi developers provide a set of standardized programming language agnostic interface definitions that can be adopted to develop model components that can communicate with each other directly at runtime the openmi interface definitions are object oriented with clear and well defined inheritance relationships another major advantage is that the latest openmi 2 0 version has been adopted as an open geospatial consortium ogc standard vanecek and moore 2014 which means that it has been reviewed and vetted by a large community of modelers despite these attractive features our experience using openmi revealed a few areas where advancements to the interface definitions would be useful especially for water resources modeling applications first while the openmi interface definitions are programming language agnostic the example interface definitions as well as software development kits sdk provided by the openmi developers use the c and java programming languages these languages are compiled into an intermediate bytecode before being translated into the native instructions of a target machine to be executed using a virtual machine software infrastructure i e common language runtime clr for c and java virtual machine jvm for java additionally c has traditionally been restricted to computers that run windows operating systems conversely many legacy model codes used in the water resources modeling field and in the earth systems and environmental modeling field more generally have been developed using programming languages like fortran c and c because of the mature set of tools scientific libraries and support for hpc simulations that are available with these languages additionally these programming languages are employed for computational models because they are compiled directly into native instructions for a target machine i e natively compiled and therefore generally have lower memory footprints faster performance and can be compiled on many operating systems while the advent of just in time jit compilers has significantly improved the performance of languages like c and java natively compiled languages remain faster for many applications and remain the benchmark for evaluating performance taboada et al 2013 to convert legacy codes written using natively compiled languages into components that can be coupled loosely to other models one needs to resolve the programming language mismatch between the interface definitions and the computational codes of these legacy models though there are ways to bridge this programming language mismatch e g using platform invocation service for c and java native interface for java the costs of marshalling data across this language divide can lead to increased memory usage and increased time for individual simulations we encountered and quantified these costs in a previous study where we converted the environmental protection agency s stormwater management model swmm which is written using the c programming language into an openmi compliant component using the c openmi interface definitions for a spatial domain decomposition urban stormwater coupling exercise buahin and horsburgh 2015 second while the openmi specification provides interface definitions for representing geometric primitives e g points lines and polygons and their associated time varying data these definitions lack some of the more common geospatial dataset formats used by water resources modelers e g meshes vector datasets rasters etc also the geospatial interface definitions provided by the openmi specification lack the topological relationship information that is important for many water resources modeling applications for instance a hydrologist simulating flows in a river network will need to know which upstream tributaries flow into any selected river reach these types of topological relationships are not explicitly supported by the openmi 2 0 standard but can be implemented by extending the openmi standard finally like other earth systems and environmental modelers hydrologic modelers often embark on experimental simulations where the same model is executed multiple times with varied inputs e g optimization uncertainty assessment calibration etc these types of simulations fall into the so called embarrassingly parallel class of simulations and benefit from using high performance computing hpc resources many research institutions provide access to computing clusters comprised of heterogeneous hardware configurations of multi core central processing units cpu as well as graphical processing units gpu and many integrated cores mic architecture accelerators that can be used for more efficient computations however the current openmi standard provides little direction on how to take advantage of these increasingly ubiquitous hpc infrastructures for more efficient simulations the contribution of this paper is the presentation of a set of component based modeling interface definitions called hydrocouple and its associated sdk and coupled model composition tools hydrocouple uses the openmi 2 0 interface definitions as its foundation but advances new interface definitions to address the challenges enumerated in the preceding paragraphs and others we chose to build from openmi because it has been tested and used within the water resources modeling domain and because it has recently been advanced as an ogc standard this manuscript represents a maturation of the core concepts and interfaces presented in an earlier conference proceedings paper buahin and horsburgh 2016 where we first described new spatiotemporal data structures and interfaces for supporting parallelized experimental simulations like calibration optimization and uncertainty assessment within component based modeling frameworks in this paper we extend the previous work by providing a more complete description of the spatiotemporal data structures and topological relationships supported by hydrocouple by describing implementation of hydrocouple s interface definitions and software development kit using c by introducing hydrocouple s gui for composing coupled models and by providing a detailed case study that demonstrates how hydrocouple s new capabilities can be used in complex model coupling scenarios we also illustrate how these advancements can be used to convert existing legacy codes into model components and develop new ones from scratch through a detailed modeling case study involving parallel execution of coupled models having different dimensionality we coupled a one dimensional 1d hydraulic model developed from the environmental protection agency epa stormwater management model swmm and a two dimensional 2d hydraulic model that we developed called the finite volume hydrologic model fvhm this coupled urban stormwater modeling example illustrates how these advancements can help usher more water resources modelers into the hpc realm so that they can embark on more efficient simulations the remaining chapters of this manuscript are organized as follows in section 2 we provide a review of existing component based frameworks with respect to their support for spatiotemporal data structures data exchange workflows and support for simulations on hpc infrastructure in section 3 we provide a discussion on the design of the hydrocouple framework with respect to the three areas identified in section 2 we also describe tools provided to simplify the process of developing new components and creating coupled model compositions in section 4 we present the case study involving the coupling of 1d hydraulic model component and a 2d component developed to represent an urban stormwater conveyance system through this application we illustrate how the advancements we have implemented are used 2 design of component based modeling frameworks the component based modeling approach and its precursor the component based software development approach have their origins in the object oriented programming approach with its notions of re use through encapsulation inheritance and polymorphism components are typically paired with software frameworks and they serve to extend the capabilities of frameworks while frameworks provide an environment for executing components fröhlich and franz 1999 this concept extends to model components and modeling frameworks in earth systems and environmental modeling for instance csdms components are executed within the common component architecture fast framework example in need of everything ccaffeine allan et al 2002 the definition of standard interfaces forms the basis of interaction between components and frameworks and between components themselves by describing the assumptions they make about each other fröhlich and franz 1999 the design of openmi deviates from other approaches for developing component based models by forgoing the pairing of interface definitions for components with a software framework instead it prescribes interfaces that let components communicate directly with each other independent of a framework this design choice was made to give more flexibility to component developers to optimize the data exchange process between model components the interface definitions for components are specified in a number of ways for different frameworks depending on the programming languages supported by the framework programming languages that were developed primarily for object oriented programming like c c and java have formal ways of specifying interfaces so that they can be inherited and implemented with details of their functioning to create framework compliant components for example c interfaces are specified in header files as classes with only pure virtual functions the openmi interfaces for c and java are specified this way on the other hand for component based modeling frameworks that support languages like c and fortran e g esmf and csdms models are required to register pointers to their standard functions with the framework these standard function pointers are then stored in virtual function tables that can be accessed by other components to achieve the interfacing functionality while the oms framework was written using java which is an object oriented programming language the oms interfaces are specified by marking classes functions and fields of a component with standardized java annotations e g in out execute etc which serve as a form of syntactic metadata through java s reflection capabilities annotated classes fields and functions can be accessed and invoked at runtime the typical core interfaces defined for models in component based modeling frameworks are the initialize run and finalize irf functions syvitski et al 2011 the initialize interface function is implemented to instantiate the resources and inputs needed by a model for a simulation the run interface is responsible for performing the underlying computations of a model e g performing a time step the finalize interface is implemented to dispose of the computational resources used by the model e g closing output file streams de allocating memory etc in addition to these core interfaces models also specify interfaces for the types of inputs and outputs that can be consumed and shared with other components respectively csdms attempts to standardize these elements that are shared by various component based modeling frameworks by providing a core set of interface definitions called the basic modeling interface bmi definitions peckham et al 2013 table 1 summarizes the design considerations we identified for component based modeling frameworks and standards used in the earth systems and environmental modeling arena in the sections that follow we discuss three of these design considerations that we believe are important areas where we identified opportunities for improvements to be made we focus on their support for spatiotemporal datasets options for data exchange workflows between components at runtime and support for simulations on hpc infrastructure given that openmi was the base from which we built hydrocouple we discuss how openmi is designed with respect to these three areas and contrast it with the design of other component based modeling frameworks limitations in these three areas are impediments to using openmi and more generally component based modeling in practice because important geospatial data structures used by many models are not currently supported data exchange workflows are restrictive or require re compilation for each coupled model composition and hpc simulations are either complex or not supported at all by existing frameworks 2 1 definition of spatiotemporal data structures the types of inputs that can be consumed and the outputs that can be supplied by models in a component based modeling framework are typically organized as multi dimensional arrays of data that can be accessed using indexes along their respective dimensions e g esmf csdms openmi these inputs and outputs are often further abstracted into domain specific types for many component based modeling frameworks for instance openmi provides a time space input output specialization that associates geometric primitives including points polylines polygons and polyhedra with time varying data for hydrologists this could be used to a represent hydrologic feature like a river network and its associated time varying flows in water resources modeling applications these features are often used for delineating a model s spatial domain and prescribing boundary or input data for models e g polygons for watershed boundaries polylines for alignments of rivers river cross sections etc missing from the openmi definitions is the topological information that provides the spatial relationships between adjacent geometries yet this information is critical for many applications for example although the individual cells of a two dimensional computational grid used for a hydrodynamic model of a reservoir may be represented by a list of polygons the adjacency information between cells that is needed to numerically approximate spatial gradients of variables are missing on the other hand the csdms suite of tools for component based modeling which were developed for ice terrestrial coastal and marine applications as well as esmf which was developed for global weather and climate predictions focus on providing interfaces for gridded datasets including logically rectangular unstructured and curvilinear grids in one two and three dimensions explicit support for geospatial data types like those provided in openmi are however not supported 2 2 data exchange workflows the model execution and data exchange workflow between components in a component based model composition is handled in a variety of ways for different frameworks esmf requires a modeler to write a driver program called an appdriver that contains the main routine for an esmf application collins et al 2005 this appdriver needs to be compiled for each application and is responsible for directing the time stepping and the data exchange between components the drawback with this approach is that a new driver needs to be written and compiled for each composition therefore a modeler will have to be a programmer with the know how and compilation tools to compile the application putting it out of reach for many water resources modelers who are typically not expert programmers in contrast to esmf oms provides a way for modelers to direct model execution and data exchange between components externally this is accomplished by letting users write the business rules for a simulation using a oms prescribed domain specific language dsl specified in an external file david et al 2013 in contrast to a general purpose programming language a dsl is a relatively simple specification language dedicated to a particular problem domain a particular problem representation approach and or a particular solution technique deursen and financial 1997 deursen et al 2000 david et al 2013 the benefit of this setup is that model users are able to direct the data exchange between components without recompiling the entire code or being restricted to the in built workflows provided by a component based modeling framework the primary model execution and data exchange mechanism in openmi is based on the pull based pipe and filter architecture buschmann 1996 with this method components exchange memory based data directly without an external data exchange workflow controller using a request and reply mechanism gregersen et al 2007 the most downstream component in a composition is designated as the controller trigger for an entire simulation where it requests the data it needs from upstream components that it is coupled to and waits for the data it requests to be returned before proceeding with its computations upstream components issue their own requests to their respective upstream components in a cascading fashion and wait to receive the data they requested before performing their computations as illustrated in fig 1 data exchange between an input defined by the ibaseinput interface and an output defined by the ibaseoutput interface can be mediated by an adapter defined by the ibaseadaptedoutput interface that performs the necessary data transformations e g temporal interpolation spatial interpolation unit conversion etc that are needed to supply the correct requested data from one component to another contextual adaptors are generated by a factory interface definition called an ibaseadaptedoutputfactory that uses the input and output that are to be mediated as query variables to generate appropriate adaptors while the request and reply data exchange approach requires no external data exchange controller and works for many applications it does not work for those compositions that have two or more downstream components as illustrated in fig 2 in fig 2 components d and e are not executed since they are not involved in the request and reply chain of the trigger component a yet there are circumstances where they may be needed as part of a coupled model composition for example component a might be a stream temperature model b might be a streamflow model and e might be a contaminant transport model both e and a need streamflow data from b but e would never get executed if a is the triggering component the openmi developers recognized that the pull driven data exchange approach might be too restrictive for certain applications therefore they suggested a loop driven data exchange workflow where the coupled system will loop over all components to let them check if they need to take action before proceeding moore 2010 the loop approach has been included in the interface standard of openmi 2 0 but the implementation for this loop driven approach is not provided as part of the openmi sdk in contrast to esmf and oms openmi may be easier to use for non expert programmers however the fact that the request and reply mechanism used by openmi may not work for all model compositions and the fact that the loop approach has not been implemented in openmi s sdk means that openmi could be further improved to meet additional use cases 2 3 support for simulations on hpc infrastructure increasingly the complexity of the hydrologic modeling applications that modelers embark on requires that model execution be conducted using distributed computing techniques on hpc infrastructure hpc infrastructure typically involves pooling of computational resources from many computers with multi core processors that are networked together esmf was designed to support both data and task parallelism on hpc infrastructure by making esmf components the very units of parallel execution to aid model developers in writing highly efficient and scalable codes collins et al 2005 a virtual machine vm approach is adopted to abstract away the details underlying model execution in hpc environments using the message passing interface mpi standard each component in an esmf composition is assigned a single vm comprised of one or more persistent execution threads pet where each pet is equivalent to a single mpi process a component may allocate its pets to child components additionally a developer can employ fine grained shared memory parallelism within a pet using openmp or other multi threading approaches inter and intra vm communications are handled by the framework in a fashion similar to mpi where data must be provided as raw language specific one dimensional contiguous arrays oms adopts a similar approach to esmf to achieve parallelism by making components themselves the units of parallel execution however unlike esmf oms only supports shared memory parallelism applications using multi threading where each model component is executed in its own separate thread in contrast openmi provides little direction on how to take advantage of hpc resources for more efficient simulations for scenarios where a downstream component requests input data from one or more upstream components an openmi component may make these requests using parallel threads in a multi threading environment for efficiency however this approach only scales up to the number of upstream components that supply data to a single component additionally in a bidirectional data exchange scenario between two coupled model components a and b openmi s request reply data exchange mechanism forces a sequential execution of the components involved component a requests the data it needs from b however component b needs data from component a before it can execute to avoid an infinite recursion loop component a provides an estimate of the data requested by component b so that component b can advance the computation from a performance perspective it may be desirable to execute both model components concurrently using estimates of data provided from the component that is coupled to them to the best of our knowledge existing component based modeling frameworks lack support for automated parallel simulations for those experimental evaluations that are embarrassingly parallel involving executing the same coupled model instances repeatedly with varied input parameters e g automated parameter estimation and uncertainty assessment e g blasone et al 2008 optimization e g zhang 2009 ensemble simulations e g komma et al 2007 etc while these types of applications can be currently undertaken within existing component modeling frameworks they can either only be configured to run sequentially require manual intervention or require writing code outside of the scope of a component based modeling framework 3 design of the hydrocouple framework to address the challenges discussed in the preceding sections we developed the hydrocouple component based modeling interface specifications and associated software hydrocouple builds on the strengths of openmi while advancing new interface definitions to deliver standard spatiotemporal dataset interfaces with their associated topological information and providing support for more efficient simulations on hpc infrastructure like openmi the hydrocouple interface definitions are language agnostic however we implemented the hydrocouple interface definitions using c because in addition to the benefits enumerated in the previous sections i e cross platform support relatively lower memory footprints mature set of tools and scientific libraries etc it also provides bindings with many programming languages and serves as a good foundation for a framework that seeks to integrate legacy model codes and support models written using different programming languages the interfaces for hydrocouple have been provided in four c header files with about 1500 lines of code that provide the core interface definitions a spatial extension a temporal extension and a spatio temporal extension in the following section we discuss the key choices that were made in the design of hydrocouple and the motivation behind them 3 1 types of hydrocouple components the core interface definition of the openmi standard is the ibaselinkablecomponent interface this interface encapsulates the computational engine of a model and defines the irf interfaces it is also responsible for defining the types of inputs a component can consume and the type of outputs a component can supply to other components in hydrocouple the ibaselinkablecomponent interface has been superseded by the icomponentinfo interface definition as the core interface definition this interface definition allows model developers to provide details about a component including documentation about the formulations employed in the component limitations of the component coupling configurations that can be employed data transformation adaptors that can be employed as well as details about the coupling configurations and application scenarios for which the use of the component is appropriate other details that can be specified through this interface include the name of the developer of the component contact information for the developer classification of the component to properly categorize component libraries in a gui and component version these details which can be used by modelers to seek further assistance are currently missing for components developed for many frameworks leading to poor understanding about their capabilities and their proper usage in component based modeling applications preferably these details should be specified in an external file and read by the component at runtime for display purposes in contrast to openmi which only allows a single component type instances of classes based on the icomponentinfo interface create three types of specialized components these specializations include classes that are based on the imodelcomponentinfo the iworkflowcomponentinfo and the iadaptedoutputfactorycomponentinfo interfaces as shown in fig 3 instances of classes based on the imodelcomponentinfo are responsible for creating instances of classes based on the imodelcomponent fig 4 interface which is equivalent to the openmi specification s ibaselinkablecomponent instances of classes based on the iadaptedoutputfactorycomponentinfo interface are responsible for creating instances of a new component type based on the iadaptedoutputfactorycomponent interface fig 4 this new interface definition can be implemented as an independently deployable component that is equivalent to openmi s iadaptedoutputfactory interface definition however unlike the openmi definition it is not bound to any particular component and can be reused between different models to generate adaptors for data exchange mediation finally instances of classes based on the iworkflowcomponentinfo interface are responsible for generating another new type of component based on the iworkflowcomponent interface fig 4 that is responsible for managing simulations and the data exchange workflow between components these new component types have been added to hydrocouple to avoid the duplicative work of developing new data exchange workflows and data adaptors for each component or coupled modeling application when using openmi and component based modeling frameworks in general for example a workflow component could be developed to automatically discover the dependencies that exist between model components and execute them in a sequence that ensures that all components are able to obtain the data they require a time series interpolation adapter could be developed as an independently deployed component that can be reused by different model component compositions this contrasts with the current openmi setup where an adaptor is bound to a component and needs to be implemented for every model component that needs to use it this generalizes and formalizes this process so that adaptors can be developed into components in their own right 3 2 hydrocouple spatiotemporal data structures interface definitions in order to reduce the cost of converting legacy models into components it is important that the low level geospatial data structures that hydrologists widely use are made available in component based modeling frameworks to achieve this goal hydrocouple provides an explicit implementation of the open geospatial consortium s ogc simple feature access sfa specification herring 2011 in addition to providing interfaces to describe various types of geometries the advantage of implementing the ogc sfa is that it defines functions for performing topological queries e g checking for intersection between features checking if one feature touches another etc and geospatial operations e g unions buffering symmetric difference etc that are useful in water resources modeling applications for instance an intersection operation can be used between gridded precipitation output from a weather forecast model and the boundary of a watershed in a hydrologic model to estimate the fraction of each grid cell that contributes precipitation flux to that watershed hydrocouple additionally prescribes interface definitions for various gridded dataset types for hydrodynamic and hydraulic modeling applications as well as representing some other gridded dataset types often utilized in the water resources area for example hydrocouple explicitly defines an interface for multi banded raster datasets that includes the number of raster grid cells cell size data type no data value etc this interface can be employed to represent various types of datasets in models including digital terrain models dtm aerial imagery time varying land use data etc in addition to this definition hydrocouple defines interfaces for cartesian rectilinear and curvilinear grids in two and three dimensions for hydraulic and hydrodynamics modeling applications time varying data for these grids may be associated with the nodes faces edges and volume area of their individual cells for networks unstructured meshes and polyhedra hydrocouple adopts the quad edge data structure proposed by guibas and stolfi 1985 with this data structure a directed edge stores the complete topological information about the resulting polyhedra network by storing pointers to its left and right face polygons and its origin and destination vertices these spatiotemporal interface definitions have been provided to abstract away the details of the various types of the same data structures provided by various software libraries for example a raster dataset can be stored as a geotiff file or a netcdf file which are accessed using different software libraries this was done so that data can be accessed within hydrocouple using common interfaces regardless of the underlying file format of the data a companion sdk was also developed to enable reading and writing of spatiotemporal data to and from files on disk using multiple file formats details of the implementation of these interfaces within the sdk using existing libraries are provided in subsequent sections 3 3 hydrocouple data exchange workflow interface definitions following the oms approach where the data exchange workflow can be specified externally the iworkflowcomponentinfo interface definition was introduced in hydrocouple as a way to provide modelers with flexibility in prescribing the data exchange workflow in a component based application for example openmi currently supports only the request and reply data exchange approach whereas hydrocouple adds the ability to create independent workflow components that can execute using a loop driven data exchange approach as well as others this interface creates an instance of an iworkflowcomponent that is responsible for keeping track of all components involved in a simulation and coordinating their computations and data exchange until the completion of a simulation just as the imodelcomponent interface has the irf functions we defined the iworkflowcomponent with these same functions to enable initializing the workflow component updating components associated with the workflow by asking them to perform their computations and finalizing the components upon completion of a simulation if the iworkflowcomponent interface is not instantiated for a simulation all components must default to the original openmi request and reply approach to data exchange this new interface was added so that modelers can directly control data exchange in a component based modeling simulation beyond the request and reply data exchange mechanism provided by openmi like model components a suite of independently developed and deployed workflow components may be developed so that model users are able to select and integrate the workflows that are most suitable for their simulations 3 4 hydrocouple on hpc infrastructure hydrocouple supports parallelized experimental simulations by introducing a new interface definition called the icloneablemodelcomponent interface that inherits from the imodelcomponent interface this new interface introduces a clone function that must be implemented for a component to make a copy of itself the clone function has been added so that independent copies of a model instance can be made for parallelized simulations details of the cloning approach are left up to the model component developer a parent model component keeps track of all of its child clones which can be accessed using the children function linkages with other model components are left up to the caller of the clone function this cloning process may involve making a copy of the parent icloneablemodelcomponent class and initializing it with the same arguments as the parent while making sure that outputs from the parent and child do not conflict an example optimization application of this cloning feature is described in buahin and horsburgh 2016 hydrocouple was designed to support simulations on hpc infrastructure by supporting parallel simulations on both shared and distributed memory systems as well as providing support for simulations that use general purpose computing on graphics processing units gpgpu support for distributed memory parallel computing was designed using the message passing interface mpi standard while the gpgpu parallelism was designed to support the open computing language opencl and nvidia s cuda framework the clone function described is only one of the ways to enable parallelism in hydrocouple like openmi hydrocouple provides interface definitions that must be implemented by a model developer component developers must implement all the mpi openmp or gpgpu functions that enable parallel computations the sdk we developed section 3 5 provides some resources that can be used to help in the implementation of these functionalities in designing hydrocouple s support for simulations on hpc platforms we followed the original openmi design choice of limiting the role of the framework in mediating the data exchange between components along this line hydrocouple applications adopt mpi for coarse grained parallelism where all components involved in a coupled modeling application are initialized and coupled directly on the mpi task process with rank 0 i e the master mpi process 1 as illustrated in fig 5 for components that support mpi a user may partition the remaining mpi processes to them to ensure the computational load is balanced fig 5 this is done by specifying the mpi ranks available to each component in a coupled model application configuration file frameworks like esmf automatically map computational resources into virtual machines that can be allocated to components for model execution on hpc resources while convenient in abstracting away the complexity of computational resources available it elevates the role the framework plays in mediating the data exchange between components additionally getting up to speed with the large software code stack and its accompanying complexity is beyond the expertise of many water resources model developers hydrocouple uses relatively few lines of code and distributes hpc resources directly by dividing available mpi ranks among components each component can group its allocated mpi ranks into an mpi communicator that can be shared among the component and its children this approach can be more transparent for users of shared hpc infrastructure that use job scheduling software like slurm and reduces the role of the framework developers of components need not necessarily conduct their computations on the master mpi process components initiated on the master mpi process could be developed as proxy stubs that serve as pathways for communicating results computed on child components on worker mpi processors for example in fig 5 the role of component a which is initialized on the master mpi process to communicate directly with components b and c may only involve collating computed results from its worker components initialized on mpi processes 2 3 and 4 and sharing with other components once initialized data sharing between components on different mpi processes can be conducted using standard mpi calls components on the master mpi processes are responsible for issuing messages to components on the worker processes to dispose themselves upon completion of a simulation fine grained parallelism may be employed over the cpu cores allocated to each mpi process using shared memory parallelism application programming interfaces apis like openmp like mpi gpgpu frameworks abstract away the complexity underlying computing resources from users and present a single virtual interface for accessing the hardware to ensure that gpgpu resources are distributed efficiently between components hydrocouple lets users prescribe the gpgpu device as well as the maximum number of cuda blocks or opencl work groups each component can use on each mpi process this gives users flexibility to partition jobs to gpgpu devices in a way that is tailored to the particular hardware layout on an hpc system this partitioning of computing resources can be accomplished using job scheduling software like the slurm workload manager that is used on many high performance computing centers systems 3 5 hydrocouple composer and software development kit in order to facilitate the development of components for hydrocouple we developed a sdk that implements many of the core interfaces needed to develop a component as a software class that has initialization arguments exchangeable inputs exchangeable outputs the sdk also implements the functions required to initialize a component by reading and validating initialization arguments and functions that describe the variables exchanged by component inputs and outputs this enables model developers to focus on the computational portions of their code which are placed within the correct locations inside the hydrocouple sdk classes the core interfaces that have been implemented as classes include those for describing and identifying components and their inputs and outputs the spatiotemporal domains of these inputs and outputs the variable types consumed and supplied by these input and outputs and the units of these variables in hpc environments the sdk automatically groups processors allocated to a component into a single private mpi communicator context so that communication between the component and its workers do not conflict with other components the sdk uses the geospatial data abstraction library gdal warmerdam 2008 extensively to provide support for reading various geospatial vector data formats e g shapefiles geojson gml cad kml etc into objects defined by ogc s sfa that can then be accessed programmatically by model components the sdk also enables writing these objects to files on disk e g for exporting model output the hydrocouple sdk also uses the gdal library to provide similar support for various raster dataset formats including the geotiff grib sqlite jpeg and hdf5 formats work is also underway to implement the ugrid version 1 0 http ugrid conventions github io ugrid conventions convention which is a proposal for storing unstructured mesh data in standard format using the unidata network common data form netcdf file using the climate and forecast cf metadata convention eaton et al 2011 as a starting point in addition to the sdk we developed the hydrocouple composer graphical user interface gui to provide model developers with a visual environment for composing coupled model configurations fig 6 the hydrocouple composer software displays all available model component libraries and allows users to drag and drop them onto to a graphical palette coupled model compositions are specified through a configuration file that lists the model components and their connection nodes as well as the workflow and adaptor components that are utilized each model component can be initialized using an initialization file that contains parameters specific to the component alternatively components can be initialized using instances of a class based on the iargument interface that are created in memory after a component is initialized compatible outputs and inputs from other components can be coupled interactively compatibility for coupling is determined by a function called canconsume on an instance of the input class that is to be coupled that is called when a model user tries to create a link between and input and an output this function returns true or false indicating whether the coupling is valid based on business rules defined by the component developers selecting an existing connection between an input and output displays the available contextually relevant adaptors that can be inserted to mediate data exchange for example when a connection between time series input and output is selected only time series related adaptors will be available for selection in the hydrocouple composer gui hydrocouple composer is responsible for partitioning computing resources to model components based on a user s specifications in the configuration file and launches simulations example configuration files can be accessed in the hydrocouple github repository see software availability section the hydrocouple composer also monitors the progress of simulations and displays them to a user logs messages from components and provides rudimentary visualization capabilities fig 6 for the standardized spatiotemporal inputs and outputs of model simulations finally when the executable of the hydrocouple composer is launched using predefined command line arguments it doubles as a command line interface for launching simulations on hpc resources in fig 6 model components are shown as squares connections between components are shown as directed arrows inputs are shown as blue circles and outputs are depicted as red circles 4 case study coupling a 1d and a 2d hydraulic model using hydrocouple in order to illustrate how hydrocouple s new interfaces facilitate the development of components and more efficient simulations we applied them to couple a 1d hydraulic model component that simulates flow through pipes culverts inlets outfalls and other urban stormwater infrastructure with a 2d hydraulic model component that simulates flow in rivers canals and overland areas the 1d hydraulic model component was developed using the epa s swmm model while the 2d model component is a new formulation we developed called the finite volume hydrologic model fvhm these two models were specifically chosen to illustrate 1 how the hydrocouple interface definitions and sdk can be used to convert legacy models into components 2 how to develop new components from scratch 3 how to handle the potential coupling configurations across models having differing dimensionality i e 1d versus 2d and 4 how coupled model components can be executed in parallel on hpc environments for more efficient simulations this particular type of 1d 2d hydraulic model coupling is widely implemented in the water resources modeling field because the tradeoffs between 1d and 2d hydraulic models complement each other 1d models are appropriate for simulating flows accurately and efficiently in channels pipes and other conduits with well defined shapes although 1d hydraulic models are generally more computationally efficient they are unable to accurately simulate lateral movement of flood waves into the floodplain and they incorporate topography and bathymetry using cross section profiles at various sections along the length of a river pipe whose placements are relatively subjective samuels 1990 hunter et al 2007 on the other hand 2d hydraulic models are more suitable for simulating landscape processes and overland flows albeit at a generally higher computational expense many urban hydrologic modeling scenarios e g stormwater runoff flooding design of green infrastructure and assessment of stormwater best management practices require accurate representations of both the drainage network and the urban landscape making a 1d 2d hydraulic model coupling ideal for simulating these scenarios the 1d 2d coupled model discussed in this manuscript was developed for a sub catchment in the city of logan utah to illustrate the benefits of using hpc resources we evaluated the simulation time speed up as more computational cores were allocated to the coupled model 4 1 study area the city of logan s stormwater conveyance system has its foundations in agricultural canals developed at the founding of the city to divert irrigation water from the logan river for farming as the logan river flows westwards through cache valley it is diverted at various locations along its length into these unlined canals that flow northward these same canals serve as the primary conduits for stormwater conveyance with many of the city s stormwater outfalls piped directly into them the boundary for the 5 81 km2 area we used for our modeling study encompasses the area that drains into one such canal called the northwest field canal fig 7 4 2 swmm model development the swmm code solves the 1d dynamic diffusive or kinematic wave equations for flows and water surface elevations over a network of conduits i e pipes canals and rivers and sub catchments connected together at their endpoints by nodes i e junctions outfalls storage units and flow dividers rossman 2006 we developed the swmm hydrocouple component from the epa swmm source code by modifying the code to expose those boundary data that are needed for coupling including the inflows outflows and water surface elevations at the inlets and outlets of the stormwater conveyance system in developing the swmm hydrocouple component we employed the hydrocouple network interface definition to represent the network of swmm nodes connected together by conduits the benefit of this is that the inflows and outflows as well as water surface elevation data that the swmm component supplies to components coupled to it provide the topological information that is needed to traverse the entire stormwater conveyance system developing this component required knowledge of c c so that we could modify the swmm code which was written in c and then recompile it into a compliant component this process would be similar for other models but in general the level of effort required to develop a component is subjective it depends on the programming skill of the developer the degree of understanding about the model being wrapped and the complexity of the model component being developed the swmm model code has been parallelized in many sections to improve performance using openmp as described by burger et al 2014 the parallelism introduced in swmm is a shared memory type of parallelism i e it uses a single mpi process but scales with number of cores allocated to the mpi process this feature served as the basis for our investigation of the performance of the coupled model as more cpu resources were added we developed the swmm model instance using detailed survey data of sizes for stormwater pipes inlets and outfalls provided in shapefile format by logan city the conduit diameters ranged from 0 30 to 1 38 m with lengths ranging from 0 5 to 390 m this dataset resulted in a swmm model with 1769 conduits 2093 junction inlets and 138 outfalls we executed the swmm model using swmm s adaptive timestep option with a minimum timestep of 0 01 s and a maximum time step of 5 s a maximum number of 20 iterations was selected for each time step 4 3 fvhm model development fvhm was developed to solve the shallow water equations over an unstructured triangular irregular network tin mesh using an implicit finite volume method details for the formulations and hydrologic process representations used are provided in appendix a in the context of this case study fvhm was developed to simply route flows in riverine and overland areas without many of the hydrologic processes that are typically represented in hydrologic models e g infiltration evapotranspiration etc however fvhm fully exposes the geospatial data structures needed to demonstrate how different configurations can be used to couple 1d and 2d models and run them on hpc we purposefully developed fvhm for this case study to focus on the model coupling data structures and configurations without the complexity introduced by representing many hydrologic processes in the 2d model component because the hydrologic processes represented in the 2d model are independent of the data structures and coupling configuration the techniques we demonstrate here are generalizable to 2d models that include more detailed hydrologic process representations given that our 2d model does not represent processes like infiltration the model configuration presented here represents a worst case scenario simulation and is a useful exercise for evaluating the performance of the stormwater infrastructure under a high intensity rainfall event in developing the fvhm model we directly utilized the data structures provided as part of the hydrocouple interface definitions for example the computational mesh used in fvhm adopts a tin interface definition with its associated quad edge data structure for creating boundary conditions the geometry interfaces prescribed by the ogc sfa specification were adopted for instance polygons were used to demarcate the area where precipitation inputs apply polylines were used to define the mesh edge boundaries where inflows and outflow apply the implementations of these interfaces and the file input and output implementations provided by the hydrocouple sdk simplified the model development process by allowing us to focus on the computational parts of the code we were actually interested in we were also able to use the capabilities of hydrocouple to enable parallelism in executing the model fvhm uses both fine and coarse grained parallelism in its code fine grained parallelism using openmp is employed in several areas in looping over each of the computational cells to calculate spatial gradients of water surface elevations and velocities friction and to apply boundary conditions since fvhm uses an implicit time marching scheme systems of linear equations need to be solved at each time step to compute velocities and water surface elevations for each cell fvhm uses the hypre software library falgout and yang 2002 which solves large sparse linear systems of equations on massively parallel computers fvhm partitions the system of equations it solves at every time step into smaller chunks to be solved in different mpi processes using the hypre library for the fvhm model instance we developed its computational mesh using sub meter high resolution light detection and ranging lidar data collected in 2005 this dataset was supplemented with the 10 m elevation data from the united states geological survey s usgs 3d elevation program the mesh contained 44861 cells with sizes ranging from 0 1 to 18900 m2 this range of cell areas was the result of refining the model mesh along the canal where we were interested in evaluating in more detail and coarsening the mesh in the upstream overland areas where we were only interested in estimating runoff for the boundary conditions a 30 min resolution time varying precipitation time series for the nearest rain gauge was developed using the 25 year 24 h design storm totaling 61 2 mm and the natural resources conservation service nrcs type ii cronshey 1986 rainfall distribution curve this storm is the prescribed storm for designing urban stormwater infrastructure in logan thus is a useful test for the coupled model for diversion flow from the logan river into the northwest field canal we applied the maximum legally allowable irrigation diversion of 1 351 m3 s for the entire duration of the storm to evaluate worst case inundation conditions e g an intense storm during a time where the canal was full of irrigation water we executed the fvhm model using the adaptive time step option with a minimum timestep of 0 01 s and a maximum time step of 5 s a maximum number of iterations of 200 was specified for the for each time step 4 4 1d 2d model coupling configurations in coupling swmm and fvhm we adopted different coupling configurations depending on the relationship between the water levels in the coupling cell of the fvhm model water level in the coupling inlet or outfall outlet of the swmm model ground surface elevation at the coupling interface and the invert elevations of coupling inlets or outfalls outlets in the swmm model fig 8 for all cases we adopted a bi directional exchange of boundary condition data at the coupling node interfaces between the two models this bi directional data exchange proceeds by passing the water surface elevation from the fvhm model to the swmm model and then passing outflows from the swmm model to the fvhm model for the case where the water surface elevation z m of an inlet in the 1d model is less than the bottom elevation of its overlying cell in the 2d model z s and the water depth in the 2d cell is greater than a i n w fig 10a the free weir equation equation 1 was used to estimate discharge into the inlet 1 q c w w 2 g z w z s 3 2 where q is the discharge into the inlet c w is the weir discharge coefficient w is the weir crest width g is acceleration due to gravity z w is the water surface elevation in the overlying 2d cell and a i n is area of the inlet opening if the water depth in the 2d cell is greater than a i n w fig 10b the orifice equation equation 2 is used 2 q c o a i n 2 g z w z s 1 2 where c o is the orifice discharge coefficient the discharges are computed in the 1d model and added as sinks in the continuity equation of the 2d cell and as sources to the inlet of the 1d model for the case where an inlet node of the 1d model is completely submerged by the water in the 2d model fig 10c the water level in the corresponding 2d cell is set as the water level in the receiving inlet of the swmm model the resulting surcharge or inflow values for the inlet are then applied as a sink source term in the continuity equation of the 2d cell 4 5 performance of various hpc configurations we executed the coupled model on up to 10 hpc nodes mpi processes to evaluate the benefits of devoting more computing resources fig 9 shows maximum inundation depths for each cell computed for the entire duration of the simulation the results of the experiment comparing the relative speed up as the number of mpi processes are increased to the optimal linear speed up desired are shown in fig 10 despite the rudimentary parallelism introduced into the fvhm model the results show up to a 5 times speedup with 10 hpc nodes different models and different coupling configurations may achieve different levels of speedup but this demonstration illustrates the capabilities of hydrocouple to enable simulations on hpc 5 discussion in our search of the literature related to openmi we found numerous papers examining the mechanics for developing openmi components and applying them to hypothetical or real world model coupling scenarios assessing the performance models that are coupled using openmi and the interoperability of openmi with other modeling frameworks hydrologic information systems and web services technologies for example elag et al 2011 assessed the mass balance errors of a hypothetical coupling scenario involving the coupling of simplified solute transport models including a surface and a sediment media model that were temporally misaligned bulatewicz et al 2010 successfully coupled agricultural groundwater and economic models to evaluate the impacts of alternative water use policies for a major aquifer in the us goodall et al 2011 and castronova et al 2013 demonstrated the service oriented modeling paradigm where remote models were coupled using web services technologies like the ogc s web processing service wps protocol we found it encouraging that openmi has been used for a variety of modeling use cases which further supports our decision to do this work in the context of openmi however we found relatively few studies that specifically focused on the architectural design of openmi and its advantages and drawbacks lloyd et al 2011 assessed the degree of framework invasiveness of openmi and other component based modeling frameworks using a single modeling use case across multiple frameworks they defined framework invasiveness as the degree of dependency between a modeling framework implementation and its associated models openmi showed a moderate degree of framework invasiveness when compared to oms which was lower and esmf which was higher however there was some subjectivity in the implementation of the model components that were coupled across the various frameworks and their study used the older openmi 1 4 version knapen et al 2013 examined the suitability of using openmi as a model integration platform across disciplines through a workshop that was organized to solicit feedback from both software developers and model application specialists in addition to identifying all the benefits of using openmi we enumerated above they identified the following as areas where improvements could be made 1 provision of data analysis and visualization tools 2 support for multi threaded executions 3 programming language bridges 4 support for standard spatiotemporal data structures 5 semantic integration and 6 support for models outside of the hydrologic modeling realm it must be noted that this assessment was also conducted using the older openmi version 1 4 the newer openmi version 2 0 addresses some of these challenges including the provision of interfaces that make it easier to integrate semantics and support for models outside of the hydrological modeling realm the work presented in this paper addresses some of the remaining challenges highlighted in the workshop discussed by knapen et al 2013 as well as those we encountered in our use of openmi hydrocouple s interface definitions and associated software tools build on the strengths of the openmi 2 0 standard by advancing interface definitions to better facilitate water resources modeling applications and data structures that have heretofore been missing in existing component based frameworks these interfaces include new topologically aware geospatial data structures based on widely accepted standards like the ogc sfa specification customizable data exchange workflows and support for parallelized simulations on hpc infrastructure while the hydrocouple interface definitions are programming language agnostic we implemented them using c to ensure that the code can be compiled on most operating systems another benefit of c is that unlike c it has no dependency on a framework like net which can play a role when installing model compositions on computational resources or on different operating systems compositions can be compiled in such a way that installation of the software involves copying the executable dynamic link libraries and additional files involved in a modeling application which does not require administrative privileges additionally the c hydrocouple implementation provides a way to avoid the data marshalling costs that often arise when legacy models are wrapped for component based modeling frameworks using interpreted languages c provides direct memory access to data for models developed using languages like c and fortran which have traditionally been used for model development because of their efficiency this helps address the design considerations 1 2 and 3 identified in table 1 for component based modeling frameworks existing openmi components developed by wrapping legacy models can be ported to hydrocouple components with minimal effort since hydrocouple adopts most of the openmi data structures and concepts additionally hydrocouple provides explicit implementations of standard geospatial dataset formats and associated topological information that are missing from existing component based modeling frameworks these data structures are widely employed for delineating model domains and prescribing boundary conditions in many water resources modeling applications i e design consideration 5 in table 1 in addition to better facilitating embarrassingly parallel experimental simulations within component based modeling frameworks through the addition of functions that create independent copies of model instances hydrocouple allows users to partition available hpc computing resources in a way that is transparent to model components i e design consideration 4 in table 1 the 1d 2d coupled hydraulic modeling example described illustrates the performance benefits to be gained especially in the hydrologic modeling community by moving into the hpc arena this application was developed to test the model code and also serve as an example of how to develop new model components for hydrocouple the model is shared in the github repository https github com hydrocouple fvhmcomponent examples the sdk and hydrocouple composer model coupling gui environment facilitate the development of components that use hydrocouple i e design consideration 7 in table 1 the sdk implements the core hydrocouple interfaces as well as spatiotemporal datasets and their file input and output operations so that the costs of converting existing models into components and in developing new models are reduced the hydrocouple composer gui allows users to interactively select and couple models launch simulations monitor simulations and visualize results of simulations on a single desktop or on hpc systems 6 conclusions we advanced these new hydrocouple interface definitions in hopes that they can be considered for inclusion in future versions of the openmi standard while we developed hydrocouple based on the lessons we learned using openmi so that it addresses the drawbacks we encountered we understand that it may not completely support the wide array of simulation types undertaken by water resources modelers and the wider earth systems modeling community in general while we have used the name hydrocouple to describe the software presented in this paper and have described it in terms of water resources modeling applications the advancements introduced in this manuscript are also applicable to other earth systems modeling fields we envision the continued improvement of the interfaces prescribed through hydrocouple as a community effort and have therefore shared the interface definitions hydrocouple composer gui as well as all the components we have developed so far in a transparent manner in a publicly accessible source code repository https github com hydrocouple through this repository users can contribute new hydrocouple components and improvements to the hydrocouple interfaces definitions its associated software and available components back to the repository for the benefit of the larger component based modeling community we have also communicated the advancements we have implemented to the openmi association so that they can be considered for inclusion in future versions of the openmi standard unlike their atmospheric modeling counterparts who have had a long history of executing their models on hpc infrastructure most water resources modeling practitioners have traditionally executed their models on single desktop machines there is however an increasing recognition that hpc is needed to tackle challenging problems such as simulating the interaction of land surface hydrologic processes with the atmosphere at the global scale evaluation of different model structures and uncertainty assessment hydrocouple supports this direction by prescribing interface definitions that allow users to partition cpu and gpgpu computing resources among components for more efficient simulations the proliferation of many component based modeling frameworks has made it difficult to fully achieve their promises of reusability and ability to conduct more holistic modeling evaluations while hydrocouple does not fully address this challenge it is not a completely new effort developed from scratch as so many other proposed component based modeling frameworks as much as was possible it builds on the core concepts and interface definitions used in the widely cited openmi standard and therefore makes translating the many existing openmi components into hydrocouple components and vice versa relatively inexpensive for example the swmm component described in this manuscript was adapted from one originally developed for our previous openmi application described in buahin and horsburgh 2015 it is clear that in order to avoid the challenge of over proliferation of component based modeling frameworks framework interoperability needs to be a priority among component based modeling practitioners many component based modeling frameworks share common characteristics e g the specification of irf interfaces these common characteristics can be used as leverage to create components that are able to use different frameworks hydrocouple supports this goal by providing framework and programming language independent interface definitions furthermore work is ongoing to create a wrapper component that automatically wraps components that implement the csdms bmi interface definitions so that they can be coupled to other models within the hydrocouple framework acknowledgements this research was supported by national science foundation epscor grant iia 1208732 awarded to utah state university as part of the state of utah epscor research infrastructure improvement award any opinions findings and conclusions or recommendations expressed are those of the author s and do not necessarily reflect the views of the national science foundation appendix a fvhm hydrocouple component model formulation the fvhm hydrocouple model solves the shallow water equations swe equations a 1 and a 2 using the finite volume approximation over a triangular irregular network mesh a 1 ζ t h v q a 2 h v t h v v g h ζ τ b ρ γ h v f h where ζ is the water surface elevation t is time h is the water depth v is the velocity vector q is the sum of external fluxes g is the acceleration due to gravity τ b is the bed shear stress friction vector ρ is water density γ is the sum of the kinematic ν and eddy viscosities ν t and f is the vector sum of external forces the bed shear stress is calculated using the manning s roughness equation shown in equation 3 a 3 τ b x τ b y ρ g n 2 h 3 u v u 2 v 2 where n is the manning s roughness coefficient and u and v are the velocities in the x and y direction respectively the fvhm component was developed to especially handle hydrologic modeling applications that often involve prolonged periods of dry spells with non existent or small water depths which is necessary to simulate areas with climate comparable utah and the intermountain western u s the finite volume approximation estimates the average value of a conserved quantity in an arbitrarily shaped control volume using an integral version of partial differential shallow water equations the theorem underlying the finite volume method is gauss s divergence theorem equation 4 which may be interpreted physically as the integral of the divergence of a vector i e a in a control volume i e cv is equal to the sum of the components of the vector normal i e n to surfaces of area of the control volume i e a versteeg and malalasekera 2007 a 4 cv a d v a n a d a to illustrate how the gauss theorem is used in the derivation of the finite volume numerical approximations for the shallow water equations in the fvhm component we apply it to the transport of velocity v in the control volume p surrounded by neighboring control volumes n1 n2 and n3 as depicted in fig a1 in fig a1 a b and c represent the nodes of the triangle for the control volume p η e η e n c and represent the length unit vector unit normal vector and centroid of the common edge between control volume p and its neighboring control volume n1 respectively ξ and e ξ represent the length and unit vector for the distance between the centroids of the control volumes p and n1 respectively and r pc and r cn represent the vector distances between the centroids of the control volumes p and n1 with the centroid c of their common edge respectively fig a 1 control volume p surrounded by neighboring control volumes n1 n2 and n3 fig a 1 in fvhm the collocated grid arrangement used by several investigators including peric 1985 and lai 2009 is adopted this grid arrangement involves calculating the control volume velocities depths and water surface elevations at the centroid of each control volume applying the gauss theorem to the momentum conservation equations yields equation 5 a 5 a h v t d a η e n h v v d η a g h ζ d a a τ b ρ d a η e n γ h v d η a f d a where a is the area of the control volume the numerical approximations for the terms in equation 5 are as follows a 6 a h v t d a h n 1 v n 1 h n v n a δ t a 7 a τ b ρ d a τ b ρ a a 8 a g h ζ d a g h ζ x ζ y a a 9 a f h d a f h a where the superscript n 1 and n represents the current timestep and the previous timestep respectively and δt is the current timestep the water surface elevation spatial gradients ζ x ζ y and all spatial gradients in fhvm are estimated using the least squares gradient reconstruction approach from cell centered values of neighboring cells in the previous time step previous iteration referring to fig a1 the water surface elevation of a neighboring cell n i e ζ n surrounding cell p can be estimated from the water surface gradient at p using equation 10 a 10 ζ n ζ p ζ x p δ x p n ζ y p δ y p n where δ x p n and δ y p n represent the distances in the x and y directions from the centroid of the control volume p to the centroid of the neighboring control volume n assembling equation 10 for all neighboring cells into a linear system of equations yields equation 11 which is solved using the qr decomposition method in fvhm a 11 δ x p n 1 δ y p n 1 δ x p n 2 δ y p n 2 δ x p n 3 δ y p n 3 ζ x p ζ y p ζ 1 ζ p ζ 2 ζ p ζ 3 ζ p the derivation of the numerical approximations of the more complex diffusion and advection terms in the momentum equation are provided in the following sections a 1 discretization of the diffusion term the diffusion term in the momentum equation is discretized as follows a 12 η e n γ h v d η a l l s i d e s γ c h c e n v n e n v η e η δ η where γ c and h c are the viscosity and the water depth at the centroid of the common edge between the control volume p and it neighboring cell these values are estimated using gradients calculated from the gradients derived from the least squares gradients reconstruction method described earlier it can be shown from trigonometry as detailed in versteeg and malalasekera 2007 that the direct gradient and cross diffusion terms of equation a 12 for each neighboring cell can be represented by equation a 13 a 13 e n v n e n v η e η e n e n e n e ξ v n v p δ ξ d i r e c t g r a d i e n t e ξ e η e n e ξ v b v a δ η c r o s s d i f f u s i o n where v p and v n are the cell velocities for p and n respectively and v b and v a are cell p s interpolated nodal velocities for the shared edge between cells p and n two approaches are available in fvhm for computing the cell turbulent eddy viscosity the first is the parabolic eddy viscosity model equation a 14 a 14 ν t c t u h where c t is theoretically equal to κ 6 with κ being the von kármán constant wu et al 2014 the second eddy viscosity model is the smagorinsky lilly model smagorinsky 1963 shown in equation a 15 a 15 ν t c s a u x 2 v x 2 1 2 u y v x 2 where c s is the smagorinsky constant with values that are usually between 0 1 and 0 2 a 2 discretization of the advection term following lai 2009 the advection term in the momentum equation is discretized as follows a 16 η e n h v v d η a l l s i d e s h c v c n 1 v c n 1 δ η where the superscript n 1 refers to the previous iteration for the current timestep v c is the normal velocity to the current edge at its centroid where a positive value indicates an outward flow from the control volume and a negative value indicates inflow into the control volume and v c is the velocity vector at the centroid of the current edge that is to be calculated at the current timestep v c is estimated as a function of the velocity of the current cell p and its neighboring upstream and downstream cell velocities using equation a 17 a 17 v c v p 1 2 ψ r v d v p where v d is the velocity of the control volume downstream of the current cell p and ψ r is a flux limiting function of r the variable r is the upwind ratio of consecutive gradients of velocity defined using equation a 18 a 18 r v p v u v d v p where v u is the velocity of the control volume upstream of the current cell p equation a 17 assumes that the centroid of the edge c is equidistant from the centroids of the bounding control volumes hence the multiplication factor 0 5 however for unstructured grids this may not be true therefore an inverse distance interpolation weighting factor l is applied in equation a 19 instead of the 0 5 as recommended by denner and van wachem 2015 a 19 l r p c r p c r c n assuming the direction of flow is from p to n1 it is easy to locate the downstream control volume with velocity v d in figure a1 that is to be used in calculating r however finding the upstream control volume v u is not straightforward to overcome this challenge darwish and moukalled 2003 derived equation a 20 for calculating r a 20 r 2 v p r p n v d v u where r p n is the vector distance between the centroids of the control volumes p and n using ψ r 0 leads to the edge velocity being the same as the cell velocity representing the upwind differencing scheme while the upwind differencing scheme is stable and results in smooth solutions it is only first order accurate ψ r 1 represents the central differencing scheme which while second order accurate can lead to spurious oscillations with problems that exhibit sharp discontinuities in velocities as is common with higher order schemes to obtain stable and non oscillatory solutions for higher order schemes the function ψ r must be monotonicity preserving monotonicity preserving schemes ensure that solutions do not create local extrema additionally the value of a local minimum must be non decreasing and the value of a local maximum must be non increasing versteeg and malalasekera 2007 monotonicity preserving schemes have a property that the total variation tv i e equation a 21 of the discrete solutions should be total variation diminishing tvd tvd schemes are characterized with tv values that decrease with time as shown in equation a 22 a 21 t v u x d x a 22 t v u n 1 t v u n sweby 1984 provides the necessary and sufficient conditions for ψ r to be tvd in terms of a relationship between r and ψ r several ψ r functions that meet these conditions are provided in fvhm including those shown in table a 1 table a 1 tvd flux limiter functions table a 1 name limiter function ψ r source van leer r r 1 r van leer 1974 van albada r r 2 1 r 2 van albada et al 1982 umist max 0 m i n 2 r 1 3 r 4 3 r 4 2 lien and leschziner 1994 quick max 0 m i n 2 r 3 r 4 2 leonard 1988 min mod min r 1 0 i f r 0 i f r 0 roe 1985 a 3 pressure velocity coupling the discretization for the momentum equations provided can be organized into a linearized system of equations for all control volumes based on the control volume center values and can be solved implicitly for new velocities using equation a 23 a 23 a p v p n 1 3 a n v n g h a ζ s h a where a p is the coefficient of the velocity of the current cell p a n are the coefficients of the neighboring cells and s is the sum of the external forces and constants acting on the control volume we seek to solve equation a 23 for control volume velocities as well the water surface elevations the momentum equation is non linear because it involves the multiplication of two velocity terms additionally for incompressible flows cell velocities and pressures are coupled in a non linear fashion through the momentum and continuity equations in fvhm the semi implicit method for pressure linked equations simple patankar and spalding 1972 or alternatively simple consistent simplec doormaal and raithby 1984 iterative solution procedures are adopted to deal with these nonlinearities first it is important to note that in the collocated grid arrangement where velocities and water surface elevation values are estimated for the centroid of each control volume a highly non uniform water surface elevation field can act like a uniform field when the gradients of the water surface elevation fields are calculated numerically this may lead to the well known checker board pressure field effect which in turn leads to non physical results versteeg and malalasekera 2007 to overcome this problem rhie and chow 1983 proposed the momentum interpolation equation shown in equation a 24 to calculate the edge normal velocity v c a 24 v c 1 0 l v p l v n e n 1 0 r ξ e n 1 0 l a p a p l a n a n g h c ζ n ζ p g h p 1 0 l ζ p g h n l ζ n r ξ the simple and simplec iterative solution procedure begins by using the initial or previous iteration water surface elevation values ζ to solve the momentum equation i e equation a 23 for an intermediate velocity field v as shown in equation a 25 a 25 a p v p n 1 3 a n v n g h a ζ s since the initial water surface elevation used is a guess from the previous iteration or time step the computed velocities are likely not correct a water surface elevation ζ that corrects the water surface elevation ζ is therefore defined as shown in equation a 26 similarly a new velocity v that corrects the calculated intermediate velocity v is also defined a 26 ζ p ζ p ζ p a 27 v p v p v p subtracting equation a 25 from 23 yields a 28 a p v p n 1 3 a n v n g h a ζ the velocity correction is then calculated from equation a 28 as a 29 v p n 1 3 a n v n g h ζ a p ignoring the minor terms in equation a 29 and inserting it into equation a 27 yields a 30 v p v p g h a ζ a p the velocities estimated in equation a 25 do not satisfy the continuity equation therefore the water surface elevation correction equation i e equation a 26 and the correction velocity equation i e equation a 30 are used in the continuity equation to derive the water surface elevation correction values ζ these computations form the basis for the simple method to illustrate it we derive the finite volume approximation of the continuity equation as a 31 ζ p n 1 ζ p n δ t a p n 1 3 δ η h c v c q a p where q represents external inflows inserting the water surface elevation correction equation and the rhie chow interpolated control volume edge velocities from the velocity correction equation into the continuity equation yields a 32 ζ p ζ p n 1 ζ p n δ t a p n 1 3 δ η h c n 1 v c 1 0 r ξ e n 1 0 l a p a p l a n a n g h c ζ n ζ p n 1 q a p in the simplec method the minor terms ignored in equation a 30 are included in the continuity equation to estimate that water surface elevation correction values to recap for each time step the solution process begins by using the initial or previous iteration values of water surface elevations and velocities to calculate intermediate velocity values v for each control volume the water surface correction equation equation a 32 is then solved to obtain correction values ζ for each control volume the water surface values and velocities are then corrected using equations 26 and 27 respectively if convergence is not achieved the whole process is repeated all the systems of equations generated by fvhm are solved using the algebraic multigrid preconditioned generalized minimal residual method gmres from the hypre software library falgout and yang 2002 which solves large sparse linear systems of equations on massively parallel computers even though the courant friedrichs lewy cfl condition for stability does not apply for the implicit time marching approach adopted in fvhm the use of an excessively large time step can lead to inaccurate estimates durran 2013 an adaptive time step approach was therefore adopted using a user specified maximum courant number as a controlling variable the time step at the beginning of each iteration is estimated by dividing the maximum user specified courant number c o by the maximum courant number for the control volumes at the current time step as shown in equation a 33 a 33 δ t c o o u t g o i n g f l u x e s v o l u m e m a x a 4 wetting and drying tracking of the wetting front at the boundaries between wet and dry cells in hydraulic models is important because of the numerical instabilities that would arise from the unrealistically high velocities that would be calculated by dividing volumetric fluxes by the small water depths in dry cells kim et al 2012 many approaches have been proposed for the proper treatment of wetting and drying cells in hydraulic models including the thin film element removal depth extrapolation and negative depth algorithms as discussed by medeiros and hagen 2013 a common feature for the treatment of wetting and drying cells in many hydraulic models involves first classifying cells as wet partially wetted or dry depending on the number of cell nodes that are submerged a wet cell has a water surface elevation that submerges all the nodes of the cell by a certain small threshold value e g 1e 7 m a partially wetted cell has a water surface elevation that submerges at least one node of a cell by a certain small threshold value a dry cell has a water surface elevation that does not submerge any of the nodes of a cell for each time step the momentum equations are only solved for wet cells and velocities for dry cells are set to zero velocities and water surface elevations for partially submerged cells are then extrapolated from neighboring wet cells the application of this approach to hydrological simulations that solve the full dynamic wave model however introduces some challenges hydrological simulations often involve long periods with small or no runoff generation additionally hydrological models often involve large areas coarse and often steeply sloped computational cells are often employed for computational efficiency this leads to the frequent occurrence of partially wetted cells that cause a no flow phenomenon where water is unable to leave a cell because of ponding in the lowest corner of a cell where the water surface elevation is below the two nearest cell edge midpoints begnudelli et al 2006 kim et al 2012 warnock et al 2014 also the typical assumption made in many hydraulic models that the water surface elevation at the centroid is equal to the water depth plus the elevation of the centroid of the cell does not hold for partially wetted cells to address this challenge we adopted the volume free surface relationship vfr proposed by begnudelli and sanders 2006 in fvhm to deal with partially wetted cells the vfr relationship makes a distinction between the free water surface elevation at the centroid and the depth at the centroid by assuming sheet flow for partially wetted cells this is done by calculating the flow depth as a ratio between the fluid volume in the cell and the area of the cell the vfr approach provides equations to quickly transform water surface elevation to depths and vice versa for triangular cells to support modeling in fvhm the momentum equations are solved for wet cells and partially wetted cells with depths above a specified threshold while setting velocities for dry cells to zero the mass balance equations are then solved for all cells in the model domain a 5 fvhm component verification the fvhm component was verified using test problems 1 and 6 from macdonald 1996 problem 1 involves subcritical flow through a channel with a rectangular cross section problem 6 involves flow through a channel with a rectangular cross section where there is a transition from subcritical flow to critical flow and then a hydraulic jump i e transcritical near the end of a channel the attributes for the two test problems are shown in table a 2 table a 2 properties for macdonald 1996 test problems table a 2 problem channel width m channel length m manning s roughness n inlet flow m3 s outlet water surface elevation m 1 10 1000 0 03 20 0 800054 6 10 150 0 03 20 1 700225 fig a 2 comparison of analytical results from macdonald 1996 test problems with fvhm fig a 2 
26351,this study developed a comprehensive graphical data processing and modeling system named visual heiflow vhf for integrated surface water groundwater modeling its distinctive features include the following first vhf uses a generic multivariable space time data cube model which enables the system to efficiently handle large time series datasets over a large spatial domain second vhf streamlines the entire modeling procedure from data preparation at the very beginning to visualization and analysis of modeling results in a uniform environment third vhf allows updating the land use input at user specified time points without manual intervention and therefore allows a direct simulation of the hydrological effects of changing land use the applicability and versatility of vhf were demonstrated in a case study in the heihe river basin the second largest inland river basin in china the case study also demonstrated that vfh facilitates process understanding and supports management decision making keywords integrated hydrological modeling visual heiflow geographic information system heihe river basin visualization groundwater 1 introduction physically based integrated hydrological models ihms can depict hydrological processes both at the surface and in the subsurface with sufficient spatial and temporal details in the last decade a number of ihms have been developed such as mike she dhi 2005 parflow kollet and maxwell 2006 penn state integrated hydrologic model pihm qu and duffy 2007 gsflow markstrom et al 2008 hydrogeosphere brunner and simmons 2012 and liquid branger et al 2010 these models have been used to study various water resources issues including estimation of terrestrial water storage seyoum and milewski 2016 surface groundwater interaction hassan et al 2014 impact of climate change huntington and niswonger 2012 responses to land use change nie et al 2011 and optimization of water resource management condon and maxwell 2013 wu et al 2016 a variety of data are needed to establish calibrate validate and apply such complex models and these datasets include topography river network hydrogeological data meteorological data land use land cover lulc maps soil properties and hydrological observations e g streamflow groundwater level soil moisture and evapotranspiration these data have different structures and are often temporally and spatially distributed thus the data processing itself requires tremendous effort and a comprehensive understanding of the complex model which seriously hinders the wider application of ihms in scientific research and management practice to the best of the authors knowledge one stop systems that can handle the entire procedure of integrated hydrological modeling from data preparation at the very beginning to analysis and visualization of simulation results are not available in practice geographic information systems giss e g arcgis and qgis and generic data processing tools e g excel are often used in combination to handle geospatial inputs and outputs of the models in an ad hoc manner as these tools were developed for general purposes they are not able to support many specific operations of integrated hydrological modeling such as watershed delineation generation of a digitized stream network discretization of the model grid assignment of spatiotemporal data to grid cells adjustment of model parameters replacement of model inputs batch simulations and assessment and visualization of model outputs one solution is to couple a gis system with a hydrological model for example arcswat winchell et al 2013 adopts arcmap as the graphical user interface gui of soil and water assessment tool swat however arcmap is a closed source commercial software and has rigid limitations for developing sophisticated applications thus open source gis libraries such as mapwindow dotspatial and qgis are more often used to develop guis for hydrological modeling for example the current version of the usepa s better assessment science incorporating point nonpoint sources basins system kittle et al 2006 was developed based on mapwindow ames et al 2007 2008 hydrodesktop ames et al 2012 and hydromodeler castronova et al 2013 were developed by cuahsi based on dotspatial pihmgis for the pihm model bhatt et al 2014 and qswat for the swat model dile et al 2016 are both based on qgis many problems remain to be addressed in current gis model coupling practices first current giss cannot efficiently handle time series data ihms typically accept and produce time series data at a number of locations when the modeling domain is large and the model grid is of high spatial resolution the size of the time series data can vary from megabytes to gigabytes efficiently presenting and analyzing such large datasets in a spatial setting remain a great challenge for most coupled gis model systems second ihms are often employed to investigate impacts of land use land cover lulc changes on hydrological processes cuo et al 2013 karlsson et al 2016 koch et al 2016 however existing gis model systems are not able to update land use data in a continuous simulation to represent the lulc changes in modeling one has to divide the entire simulation period into multiple periods via manual intervention third the existing systems lack a general design for integrated surface water groundwater sw gw modeling this type of modeling typically requires a large amount of data in different formats users of the existing systems must spend considerable time on data conversion using a set of third party tools and on data transfer between individual tools in addition existing systems are not sufficiently flexible for accommodating new functionalities of the models to address the above problems this study designed and developed visual heiflow hereafter referred to as vhf a comprehensive graphical data processing and modeling system for integrated hydrological simulations the current version of vhf has been structured to accommodate an ihm named heiflow hydrological ecological integrated watershed scale flow model heiflow is an ihm developed by the authors its precursor is gsflow coupled ground water and surface water flow model a representative integrated surface water groundwater model developed by the usgs markstrom et al 2008 compared with existing graphical modeling tools the unique features of vhf mainly include the following first the multivariate space time data cube model in vhf provides a generic and flexible solution to store manipulate and analyze all data types possibly involved in the integrated modeling the data cube model also enables extending the analyzing capabilities of vhf without modifying the core codes second vhf has many useful designs for assisting water resources management and decision making such as updating the land use input at user specified time points without manual intervention and comprehensive water budget analysis capabilities finally with its open architecture vhf can be extended to accommodate different ihms the above system characteristics as well as some others can significantly reduce the difficulty in and time cost of building and using a complex ihm to solve real world water management problems in the remainder of this paper section 2 introduces the framework of vhf including the design concepts system architecture and implementation section 3 provides an application case in the heihe river basin hrb through which the key steps of setting up and running a model in vhf are illustrated the hrb is the second largest endorheic river basin in china and the water and ecological issues in the middle and lower hrb are very typical of china s western inland integrated sw gw modeling has been implemented for the basin tian et al 2015b section 4 demonstrates how the simulation results can be viewed and analyzed from different aspects in vhf section 5 concludes this study 2 framework of the system 2 1 integrated hydrological model although designed to accommodate typical ihms the current version of vhf has been tailored for heiflow the precursor of which is gsflow gsflow integrates prms markstrom et al 2015 with modflow harbaugh 2005 which perform surface hydrology simulation top of plant canopy to soil zone base and 3d groundwater simulation the base of soil zone to the base of aquifers respectively prms and modflow are composed of a set of components called modules packages each of which performs a specific task gsflow simulates all major processes of the hydrologic cycle compared with gsflow heiflow explicitly accounts for and flexibly handles agricultural water use activities i e surface water diversion groundwater pumping and irrigation tian et al 2015a and adequately represents ecohydrological processes of typical land covers in inland river basins such as phreatophytes li et al 2017 a full description of heiflow will be reported in a separate article heiflow delineates surface and subsurface domains using the same horizontal grid in the surface domain each grid cell represents a hydrological response unit hru with a vertical range from the land surface to the bottom of the root zone the subsurface domain which is conceptually the space below the root zone is discretized into finite difference girds heiflow takes daily meteorological data and water use data i e diversion pumping and irrigation as the main forcing functions distinct from existing hydrological models model parameters in heiflow can have time variant values and the model is therefore capable of accepting multiperiod land use data as the model inputs and automatically updating the hru parameters accordingly this unique functionality is important in regions with rapid land use changes 2 2 design concepts two primary challenges have been addressed in developing vhf first how can data be efficiently exchanged between gis and ihm second what is the best method to accommodate various types of model inputs outputs and other relevant datasets to resolve the first challenge a tight gis model coupling methodology was employed the featureset a general abstract object in most existing gis libraries was used as a shared data model between ihm and gis each feature in the featureset has a unique id and contains a geometry i e point polyline and polygon and attributes each model entity e g grid stream and well in vhf is represented by a featureset object each element in the model entity e g an hru or a stream reach exhibits a one to one relationship with the associated feature in the featureset by leveraging the power of existing gis libraries the visualization and manipulation of model features as well as conceptual modeling can be realized in addition the tight coupling strategy has the advantage of independent development of both the gis and the ihm bhatt et al 2014 to address the second challenge we used a multivariate space time data cube model to represent both static and time variant datasets in the ihm the data model is based on the idea of the space time variable cube proposed by maidment 2002 this data cube concept was initially designed for integrating time series from heterogeneous data providers within a relational database the model was also used as a means to reference measurements according to the locations times and types of the measurements in the national water information system goodall et al 2008 previous applications of the data cube were focused on integrating time series of point observations we extended this concept to represent more complicated data types associated with ihms the data cube fig 1 has three dimensions space time and variables therefore each cell in it corresponds to a specific attribute e g precipitation of a spatial object e g an hru at a specific time point e g january 1 2000 the data model has the flexibility to present different data types associated with the ihm for example it can represent time series data fig 1b at a specific location e g a gaging station for a target variable e g streamflow as well as time series data in a 2d fig 1c domain e g basin surface or 3d domain e g multilayered aquifers on the other hand the data model can also represent static but spatially distributed data e g soil parameters of hrus by setting the time dimension to one in vhf the data cube model is represented by a generic class the data cube class not only holds a 3d numeric data array but also provides many generic operations on the 3d array li et al 2014 such as memory management initialization altering dimensional size and serialization the multivariate space time data model provides a standard but yet flexible solution to store and manipulate all data types possibly involved in the integrated modeling it enables us to develop a variety of tools to perform spatiotemporal analyses of the data furthermore a set of standard interfaces tian et al 2016 has been designed to capture common properties and behaviors of ihms packages parameters and other necessary objects in the ihm the interfaces together with the multivariate space time model ensure the flexibility and extensibility of vhf 2 3 system architecture maintaining a clean separation between application logic and the user interface ui can resolve numerous development and design difficulties and make the system much easier to test maintain and improve numerous design patterns are applicable to achieve the separation in this work we adopted model view viewmodel mvvm throughout the development life cycle because it improves code reusability and flexibility and increases application testability to better implement the mvvm pattern an appropriate layered architecture is required fig 2 presents the layered architecture the architecture is divided into five layers the top layer namely the presentation layer is responsible for the look and feel of the application the application layer is responsible for the application logic the application logic can be grouped into four major groups model setup model postprocessing geo processing and spatiotemporal processing the foundation layer provides reusable libraries and modeling framework this layer contains core data structure definitions and provides functionalities to access manipulate and visualize data from the underlying sources the ihms layer contains executable programs of the ihm or ihms which are standalone executable files for the numerical computation of the model or models currently vhf accommodates heiflow but it can be extended to accommodate other ihms by coding corresponding model libraries the foundation layer can accept new customized libraries without affecting the remaining parts of the architecture in the architecture the bottom layer includes gis data files i e raster and vector files the relation database that stores point time series and model input output files storage of the point time series in the relation database follows a standard observational data template called observations data model odm odm provides a generic database template to store hydrologic observations and metadata about the data values and has been widely used within the hydrology community horsburgh and reeder 2014 huang and tian 2013 peckham and goodall 2013 in the odm database the controlled vocabularies method is used to alleviate the semantic heterogeneity problem 2 4 system implementation similar to its precursor gsflow heiflow was coded using fortran and c languages and was compiled into a standalone executional file vhf was developed using the c language based on microsoft net framework version 4 5 vhf communicates with heiflow through input output files and the information of model running e g running progress and any warnings or exceptions during the running the net framework has several advantages for developing vhf first the managed extensibility framework mef embedded in net provides a mechanism through which developers can easily develop lightweight extensible applications it also allows developers to encapsulate code easily and avoid fragile hard dependencies the mef also facilitates implementation of the mvvm pattern the dotspatial library was employed to build the gis framework dotspatial was selected because it supports a wide range of data formats provides comprehensive geo processing functionalities and has an active developer community to provide state of the art software developments furthermore because dotspatial is fully open source and is completely written in the c language dotspatial gis capabilities can be readily incorporated into vhf dotspatial has a simple plugin mechanism with which new functionalities can be added without modifying the core source code vhf extends this mechanism by enabling users to add additional functionalities related to ihm i e access to and manipulation of model packages model input output etc the gui of vhf see fig 3 has a similar style as many popular desktop gis applications the main window contains a ribbon style main menu a map view a legend a project explorer a toolbox and other controls that can be displayed in a tabbed interface with movable dockable panels the map view and 3d view are the main visualization elements vhf uses projects to organize different models when a project is opened the model structure appears in the project explorer and the gis layers are displayed in the legend as shown in fig 3 the structure of an ihm is presented as a hierarchy of submodels packages modules and parameters variables the parameters variables can be further grouped into different categories users can navigate through the hierarchy to see a graphical spatial distribution in the map view or the numerical values of a parameter in the table view the project explorer is the major means to manage an ihm including adding or removing a package many commands can be executed on the packages by right click menus in the project explorer fig 4 illustrates several important tools in vhf fig 4a displays the dem in a three dimensional space using the 3d view tool fig 4b presents the odm database manager which provides an interface to manage records in the odm database i e to explore or edit records import data from external sources such as excel spreadsheets and export records following use specified file formats users can also check the imported time series through plot tools in vhf if any error values are identified users can directly modify the values in the interface fig 4c presents the spatial toolbox which provides a collection of tools to develop and manage vector raster data and to perform various spatial analyses the appearance of the toolbox follows the style of arcgis making it friendly to users who are familiar with arcgis fig 4d presents the model toolbox which is a unique feature of vhf the model toolbox provides a set of tools to perform analyses and processes on the multivariable space time data cube table 1 lists some common tools in the model toolbox vhf can also accept user developed tools to extend the model toolbox capabilities by leveraging the mef 3 system application 3 1 application case this section presents an example of setting up a new ihm and running it in vhf the study area is the heihe river basin hrb the second largest endorheic river basin in china the modeling domain see fig 3 covers the middle and lower hrb plus a portion of the badain jaran desert the domain has an area of 90 589 km2 in our previous work tian et al 2015b a gsflow model was established and calibrated for the same domain using several individual tools including arcgis modelmuse matlab and excel the previous modeling process was extremely complicated and time consuming first no integrated platform is available to prepare the required input files in a single environment and a set of third party tools must be used considerable time was spent on data conversion and transfer between individual tools second the sizes of the input and output files for long term large scale simulations are extremely large for example the modflow cell by cell cbc output file generated by a 13 year run can be up to 100 gigabytes to efficiently manage and visualize such large datasets we designed a strategy to flexibly load and manage values from input output files in vhf input output files of ihms typically contain multiple variables e g the modflow cbc output file may contain as many as 10 variables our strategy allows loading values of one user specified variable at a time from the target file rather than loading values of all variables simultaneously the loaded values are represented by a data cube users are able to manipulate the data cube with great flexibility e g deleting it or releasing memories occupied by a specified dimension of the data cube the data cube can be visualized either by the 2d map view tool or the 3d view tool users are able to visualize any data cube of interest through the tools in this application case a new heiflow model that allows time variant land use inputs was built in vhf table 2 summarizes the data used in this case study which are grouped into four categories the first category is used for model setup and initial parameterization including the dem stream network land use soil type hydrogeology map and borehole data in particular three land cover datasets for the years 2000 2007 and 2011 were collected liu et al 2002 hu et al 2015 as shown in table 3 considerable land use changes occurred during the period from 2000 to 2011 the areas of farmland urban water body and forest expanded significantly the total areas of urban water body and forest increased by 47 86 19 92 and 28 49 respectively the expansion of water body area was mainly due to the recovery of the east juyan lake the terminal lake of the heihe river the total area of farmlands expanded from 5422 km2 in 2000 to 6320 km2 in 2011 representing a 16 56 increase the second category is driving force data we used the same datasets as in our previous study tian et al 2015b the third category is hydrological observations for model calibration and validation including daily streamflow observations at four gaging stations and groundwater level observations at 47 monitoring wells all these observations were imported into the odm database the fourth category is independent information for further analyses of the modeling results e g crosscheck and process interpretation in this case an et dataset monthly et in 2000 2012 with 1 km spatial resolution produced by the etwatch model wu et al 2012 and a leaf area index lai dataset every 8 days in 2000 2012 with 1 km spatial resolution produced by a canopy bidirectional reflectance distribution function brdf model liao et al 2013 were considered these two datasets were originally encoded in the tif format and were converted to the data cube format in vhf all the data used in this study were obtained from the heihe program data management center http www heihedata org unless otherwise noted 3 2 overall workflow the workflow of establishing an ihm and running it in vhf is illustrated in fig 5 the first step fig 5a is to create a new project the user needs to define the starting and ending time points of the entire simulation specify the number of stress periods and then define the starting and ending time points of each stress period if time variant land uses are considered the user also needs to specify the time points to load new land use data the second step fig 5b involves configuring the modeling domain the system has tools for automated watershed delineation stream definition and geometric parameter calculations which makes this step easy the next step fig 5c is to construct the surface model in which the main task is to parameterize hrus initially vfh sets default values for all hru parameters based on the prms technical document markstrom et al 2015 the user can further modify the parameter values based on the geospatial data of the study area such as dem land use land cover soil maps and others a set of spatial analysis tools is provided in vfh to aid users in adjusting parameter values the fourth step fig 5d is to build the subsurface model vhf uses a conceptual model approach to help the user complete aquifer parameterization define sources sinks and set boundary conditions the conceptual model approach used in vhf is similar to groundwater modeling system gms software aquaveo 2017 it produces a high level representation of the groundwater model using common gis layers i e points polylines and polygons cascade parameters can also be calculated in this step after the construction of surface and subsurface models the user can generate driving forces including meteorological data and irrigation data fig 5e at this point all the model input files are ready the next step fig 5f is optional to perform model calibration and validation the user needs to collect observations of key hydrological variables such as river flow rates at gaging stations and groundwater levels at monitoring wells the user can import the collected point time series into the odm database using the tool shown in fig 4b raster time series from independent remote sensing products or other modeling results can also be imported into vhf the imported data are stored using the data cube and can be used to crosscheck the model results once all the required and optional data files are prepared the user can run the model view the model running state and check model outputs in real time fig 5g finally data visualization and analysis can be performed on the model outputs via the model toolbox fig 5h 3 3 model setup and running 3 3 1 domain decomposition vhf has an automated watershed delineation tool to generate subwatersheds and stream networks based on the dem fig 6 a presents the dialog of the tool it was developed using the analysis using digital elevation models taudem library yildirim et al 2015 once the delineation is completed all the generated layers i e watershed boundary subwatershed boundaries and stream network can be displayed in the map view a delineated watershed is illustrated in fig 6 note that vhf can also import these layers from outside and skip the delineation step after the subwatershed delineation the user needs to further discretize both surface and subsurface domains into basic computing units heiflow adopts a uniform grid for both surface and subsurface discretization such that hrus in the surface domain have the same rectangular shapes as the respective finite difference grid cells in the subsurface domain once the user specifies the size of the grid cell i e width and height and the number of subsurface layers the discretization can be automatically completed in addition topographic and geometry characteristics e g elevation latitude longitude of the centroid and area of each grid cell in the top layer can also be automatically computed the generated grid is stored in a polygon shapefile and its attribute table stores index information related to the row column and cell the grid shapefile only stores active grid cells within the model boundary and the cells outside of the model boundary are omitted 3 3 2 surface model construction after the delineation the hrus are created and parameterization of the hrus is necessary the prms hru involves greater than 100 parameters some parameters are spatially distributed and can be specified based on gis data layers such as the dem land use land cover and soil data layers vhf uses a lookup table and gis overlay operations to automatically assign parameter values to hrus for parameters that are not distributed the user can directly input modify the parameter values in a table editor the parameter values are stored in a parameter file in the modular modeling system mms format leavesley et al 1996 the procedure of assigning hru parameter values based on gis data layers is illustrated in fig 7 using the soil module soilzone module as an example after loading a digital soil map the user can select the parameters to assign values in the coverage setup dialog then a lookup table that relates coverage classification to parameter values is defined in the lookup table dialog finally the lookup table is used to assign parameter values to each hru in an automated manner 3 3 3 time variant land use vhf can accommodate the input of time variant land use to heiflow heiflow can reload surface model parameters at user specified time points without interrupting the model run in vhf a set of surface model parameter files corresponding to land uses in different time periods can be generated in advance based on the method introduced in the previous section the user can then specify the time points for heiflow to load the parameter files as shown in fig 8 three parameter files corresponding to the land uses in 2000 2006 2007 2010 and 2011 2012 were supplied to the model 3 3 4 subsurface model construction the key operations of subsurface model construction include selecting packages developing the conceptual model and generating input files first the user needs to select the packages to be used in the model simulations currently the modflow packages supported by vhf include bas6 dis lpf upw gmg pcg nwt uzf1 sfr2 ghb1 chd1 lak3 wel1 hfb1 gage and oc some of the packages are required for all simulations and some are optional for example one of the flow model packages and one of the solver packages must be selected whereas both the boundary condition and observations packages are optional inputs for each package are contained in a separate file vhf aids users in generating the input files through a conceptual model approach this approach uses gis functionalities to develop a conceptual model of the modeling domain the conceptual model contains a set of shapefiles that define parameter zones as well as local sources sinks such as wells streams wells and general head flow boundaries once appropriate packages are chosen for a simulation the parameters and all other data required by the packages are defined at the conceptual model level then the conceptual model is used to assign parameter and input values to the packages in an automated manner fig 9 presents an example of assigning parameter values to the lpf package the first step is to create a polygon shapefile to define hydrogeological zones the zoning was based on the hydrogeology map of the study area as shown in fig 9 the modeling domain is divided into 75 hydrogeological zones for the top layer each zone has a unique id the lookup table in fig 9 allows the user to enter parameter values e g hydraulic conductivities specific storage and specific yield for each zone then cell by cell value assignment is automatically performed prms uses the concept of cascade to route overland flow and interflow among hrus streams and lakes the calculation of cascade requires information contained in the input files of dis sfr2 and lak3 packages with the generation of those input files vhf is able to calculate the cascade parameters in an automated manner using an embedded tool called the cascade routing tool crt henson et al 2013 3 3 5 preparation of driving forces and calibration datasets heiflow requires daily meteorological and irrigation data as driving forces the meteorological parameters include precipitation temperature maximum minimum and average air pressure relative humidity wind speed and direction and sunshine hours the irrigation data include daily flow volumes of surface water diversion and groundwater pumping heiflow requires time series of meteorological data for each hru if spatially distributed data are available e g data generated by a regional climate model or derived from radar datasets the user can prepare the input files in vhf through resampling if the data are only available at a limited number of hrus e g data recorded at weather stations vhf can perform data interpolation and extrapolation to prepare the input files using the inverse distance weighting idw method or the gradient plus inverse distance weighting gidw method nalder and wein 1998 fig 10 presents the spatial distributions of annual average precipitations over the modeling domain using two different data sources the precipitation data in fig 10a were derived from a regional climate model xiong and yan 2013 namely the regional integrated environmental model system riems 2 0 calibrated for hrb the precipitation data in fig 10b were generated based on the 19 weather stations indicated in the map using the gidw interpolation method the spatial patterns of the two datasets are quite different irrigation is explicitly represented in heiflow irrigation practices mainly include water withdrawal from sources e g rivers lakes and wells water conveyance and distribution of water in the field tian et al 2015a users are required to prepare an input file that contains information of diverting surface water from stream reaches to their linked hrus the spatial linkage between a stream reach and its linked hrus can be defined using the spatial analysis tool provided by vhf the user must specify the daily volume of diverted water from each stream reach the user should also determine the pumping rate at each well fig 11 presents the spatial distribution of annual average water volume diverted from the midstream of the heihe river warmer colors indicate larger diverted water volumes in practice the formats of original driving forces and calibration data sets are quite different the current version of vhf has utilities to convert a variety of commonly used data formats e g excel netcdf and tif into what heiflow accepts to prepare the calibration datasets the user can either use the odm database manager fig 4b or conversion tools in the model toolbox the conversion tools enable the user to convert spatially distributed data e g et encoded in netcdf or popular raster formats e g tif into the data cube format of vhf all other analysis provided by the model toolbox can be performed on the converted data 3 3 6 model running once all necessary input files are ready a model run can be initiated a window will synchronously present the running information see fig 12 the real time budget percent discrepancy is plotted in a chart and running information e g simulation time period execution time and iteration statistics for each time step is displayed in the textbox below the chart any errors or warnings will also appear in the textbox and the user can abort the simulation at any time during the simulation the user can analyze the model outputs at any time before the simulation is complete for example real time water budget results can be monitored this functionality is extremely useful when a model simulation is time consuming e g hours or even days to finish because it enables the user to identify problems and start a new simulation in a timely manner 4 analysis of the application results 4 1 comparison of modeling and observed results vhf can compare simulated time series at any user specified location against the respective observations if the point observations are ready in the odm database fig 13 presents the streamflow comparison at the zhengyixia gaging station after loading the outputs generated by the sfr2 package simulated streamflow in any reach can be plotted in the same manner by selecting a segment id and a corresponding reach id the sfr2 package divides streams into reaches and segments a stream reach is a section of a stream overlaying a particular modflow finite difference cell whereas a segment is a sequence of reaches with shared properties after selecting a gaging station name and choosing streamflow as the variable to display the observed streamflow will be automatically fetched from the database and plotted in the figure the goodness of fit measures in terms of mean squared error mse root mean squared error rmse correlation efficient correlation and nash sutcliffe coefficient karlsson et al can be calculated and presented in a table in fig 13 the mse rmse correlation and nse are 59 096 7 687 0 951 and 0 904 respectively indicating a good match this comparison functionality greatly facilitates model calibration and validation 4 2 detailed water budget analysis a useful feature of heiflow is the calculation of overall water budget and water budgets of vertical conceptual model zones including the surface zone soil zone unsaturated zone and saturated zone the water balance of the entire region can be expressed as follows 1 p sw i n gw i n et sw o u t gw o u t δ s where p denotes precipitation sw i n and gw i n represent the boundary surface water inflow and groundwater inflow respectively et is actual evapotranspiration sw o u t and gw o u t represent surface water outflow and groundwater outflow across the model boundary respectively and δ s is the total storage change et can be further decomposed as follows 2 et et s f et s et u z et s a t et l a k et r i v et c a n a l where et s f is the evapotranspiration from the surface zone including evaporation of intercepted precipitation evaporation from impervious areas and snow sublimation et s et u z and et s a t are subsurface evapotranspiration components of the soil zone unsaturated zone and saturated zone respectively and et r i v et l a k and et c a n a l represent the evapotranspiration from streams lakes and canals respectively the total storage change δ s can be expressed as follows 3 δ s δ s s f δ s l a k δ s r i v δ s c a n a l δ s s δ s u z δ s s a t where δ s s f is land surface storage which comprises snowpack intercepted precipitation and impervious surface storage δ s l a k δ s r i v and δ s c a n a l are storage changes in lakes streams and canals respectively and δ s s δ s u z and δ s s a t are storage changes in the soil zone unsaturated zone and saturated zone respectively the water balance of all hrus including surface zone and soil zone can be expressed as follows 4 p div pr g e s a t et s f et s si r i v si l a k dr r i v dr l a k ur s δ s s f δ s s where div is diversion from streams pr is pumpage from the saturated zone g e s a t is groundwater discharge from the saturated zone to the soil zone si r i v and si l a k are interflow to streams and lakes respectively dr r i v and dr l a k are dunnian and hortonian runoff to streams and lakes respectively and ur s is percolation beneath the soil zone the water balance of the unsaturated zone can be expressed as follows 5 ur s et u z gr u z δ s u z where et u z is evapotranspiration from the unsaturated zone and gr u z is groundwater recharge from the unsaturated zone to the saturated zone the water balance of the saturated zone can be expressed as follows 6 gr u z sg s a t gw i n et s a t g e s a t gs s a t gw o u t p r δ s s a t where sg s a t is groundwater recharge from streams et s a t is evapotranspiration from the saturated zone and gs s a t is groundwater discharge to streams vhf provides a tool to perform detailed water budget analyses following eqs 1 6 as shown in fig 14 a all the budget terms are organized in a tree like hierarchy left column for example the terms associated with the basin water budget are grouped into three categories 1 the input terms include precipitation boundary surface water and groundwater inflows 2 the output terms include evapotranspiration surface water and groundwater outflows and 3 the storage change terms include individual storages in different zones the user can navigate through the hierarchy to see the temporal variation in each budget component over the simulation period for example the chart view in fig 14a presents the seasonal variation in the cumulative storage change the budget analysis tool is able to compute the water balance of the entire modeling domain for user specified time periods fig 14b presents the 12 year average water budget of the entire domain in the application case it can be seen that local precipitation is the largest input item approximately 67 followed by surface inflow from the upper hrb and groundwater inflow both groundwater outflow and surface outflow are zero which indicates that all the water inputs eventually become et and storage change the water budget can also be analyzed zone by zone vertically fig 14c graphically presents the water balance in vertical conceptual model zones such graphs are very informative because the values of all major fluxes and state variables as well as their relationships with the water balances are clearly illustrated this graphical presentation helps the user easily understand the complicated hydrological processes and quickly identify any budget errors vhf enables users to explore details of the sw gw interaction for example fig 14d displays the results of stream aquifer water exchange calculated by the sfr package positive values indicate stream leakage to the aquifer whereas negative values indicate groundwater discharge to streams the spatial variability of the water exchange along the main heihe river is clearly demonstrated three segments with distinctive exchange patterns can be identified the first 35 km segment extending from yingluoxia to the 312 bridge is a losing segment the second segment extending from the 312 bridge to zhengyixia is a gaining segment below zhengyixia the river again loses water to the aquifer other stream related variables e g stream head and groundwater head can also be visualized in a similar manner 4 3 spatio temporal analysis of ecohydrological processes the model toolbox of vhf enables users to analyze spatiotemporal features of ecohydrological processes from different angles in the following examples precipitation p potential et pet et soil moisture sm and lai are considered and the data period is from 2001 to 2012 precipitation is a key model input extracted from a regional climate model xiong and yan 2013 pet et and sm are model outputs and lai uses a remote sensing based product liao et al 2013 the lai data have an eight day time interval and all other data have a daily temporal resolution to analyze long term trends all the data were summarized at an annual interval in the following analyses the summarization can be readily completed using the aggregate tool in the model toolbox of vhf the aggregate tool can summarize time series of targeted variables for each grid cell with regard to a given time interval e g days weeks months and years 4 3 1 spatial pattern analysis the spatial distribution of annual p lai and et were first derived using the temporal mean tool of vhf then a trend analysis was performed on the three variables using the trend tool the trend tool employs a linear regression method to calculate the change rate of a variable in each grid cell based on the following equation stow et al 2003 zhang et al 2016 7 t r e n d n t 1 n t y t t 1 n t t 1 n y t n t 1 n t 2 t 1 n t 2 where n is the number of time steps and y t is the value of a selected variable at time t if t r e n d 0 the variable exhibits an increasing tendency and vice versa fig 15 a b and 15c display the average annual p lai and et respectively and fig 15d e and 15f present their change rates between 2001 and 2012 the average annual precipitation ranges from 30 to 373 mm yr fig 15a and high values occur in the southeast part of the modeling domain where the elevation is relatively high the heterogeneity of the precipitation change is presented in fig 15d although most of the domain has an increasing tendency the southeastern corner where the elevation is greater than 2700 m exhibits a strong decreasing tendency the irrigated farmlands in the middle hrb exhibit high lai values whereas the vast vacant lands and desert areas outside of the oases have near zero lai values fig 15b fig 15e demonstrates that lai has increased in the midstream farmlands and the ejina delta near the end of the heihe river the latter reflects the positive impact of the environmental flow regulation in the middle hrb on the vegetation recovery in the lower hrb tian et al 2015b the spatial distribution of et fig 15c is more complicated because it is significantly influenced by meteorological conditions human activities and sw gw interaction the highest et values approximately 1200 mm yr are observed over reservoirs and lakes farmlands and places where groundwater discharges to the surface e g wetlands and springs also have high annual et values in the gobi desert with sparse vegetation the et is mainly controlled by precipitation the change in et is also complicated fig 15f in areas without irrigation the trend is similar to that of the precipitation in the irrigated farmlands the et exhibits an increasing trend the most notable increase in et is discerned at the terminal lake east juyan lake because the lake has been recovering from dried up conditions in the 1990s due to environmental flow regulation 4 3 2 temporal variability analysis annual values of p lai and et were first calculated for the entire modeling domain using the spatial mean tool and then calculated for three subbasins including the middle hrb mhrb the lower hrb lhrb and the western part of the badain jaran desert bjd using the zonal statistics tool the boundaries of the three subbasins are the same as those in our previous studies tian et al 2015b the annual precipitation ranges from 5 70 billion m3 yr to 13 33 billion m3 yr during 2001 2012 fig 16 a for the entire domain with a slightly decreasing tendency nonetheless the change varies in different subbasins fig 16d for example mhrb exhibits a significant increase whereas bjd demonstrates a significant decrease during this period for lai an increasing trend is observed for the entire domain fig 16b and the increase mainly reflects the notable vegetation recovery in the lhrb fig 16e spatial details of the lai variation in the mhrb and the ejina delta in the lhrb are further displayed in fig 17 a and b respectively although the mhrb exhibits an overall increasing lai trend an area in gaotai county experienced an lai decrease fig 17a this area has many wetlands fed by direct groundwater discharge due to rapidly increasing groundwater pumping during the data period the discharge significantly declined which affected the plant growth in the wetlands in contrast the increase in lai in the ejina delta fig 17b is largely due to the recovery of groundwater as a direct outcome of the environmental flow regulation in the mhrb as discussed in previous studies tian et al 2015b wu et al 2015 2016 under the existing water allocation regulation the recovery of the groundwater in the lhrb occurs at the cost of groundwater depletion in the mhrb because groundwater pumping in the mhrb has not been well regulated in addition the increase in annual et is notable throughout the entire domain fig 16c and each subbasin fig 16f 4 3 3 correlation analysis the model toolbox in vhf also provides a tool named correlation to calculate the correlation coefficients of targeted variables in each grid cell fig 18 provides an example of et pet correlation at an annual time scale the spatial pattern demonstrated in fig 18 reflects the underlying processes for instance the et pet correlation is significantly positive in farmlands because farmlands are well irrigated and the annual et is energy limited rather than water limited in contrast the et pet correlation is significantly negative in the gobi deserts and many vacant lands because the annual et there is water limited 4 4 impacts of land use change as introduced in section 3 3 3 vhf can handle changing land use in a continuous simulation without any manual intervention this section exemplifies the importance of this functionality in the original model run three periods of land use including 2000 2007 and 2011 were used for comparison an additional model run with the fixed land use of 2000 was performed the two model runs led to the same simulation results for the period 2000 2006 but different results thereafter i e 2007 2012 for example fig 19 maps the difference in average annual et between the two models positive differences indicate that the time variant land use led to more et than the fixed land use and negative differences indicate the opposite the positive differences are mainly due to changes from vacant land gobi desert areas to farmlands grasslands these changes commonly occurred at the edges of artificial oases where farmlands expanded rapidly or along the western boundary of badain jaran desert where wetlands have been recovering in recent years the total amount of the positive differences is 0 262 billion m3 yr which is a significant number with regard to the total water budget in contrast the negative differences are mainly associated with some unchanged farmland hrus or hrus that changed from farmland grassland to vacant land because the irrigation of these hrus has been reduced over time the total amount of the negative differences is 0 205 billion m3 yr which is also a significant number this et example indicates that ignoring significant land use changes in the integrated modeling will introduce large modeling uncertainty 5 conclusions this study developed a comprehensive graphical data processing and modeling system for integrated hydrological simulation named visual heiflow vhf vhf provides a gis based environment to establish and run integrated hydrological models and to visualize and analyze the model inputs outputs its distinctive features in comparison with existing hydrological modeling systems include the following first vhf uses a generic multivariable space time data cube model which enables the system to efficiently handle huge time series datasets over large spatial domains second vhf streamlines the entire integrated modeling procedure from data preparation at the very beginning to visualization and analysis of modeling results in a uniform environment third vhf allows updating of the land use input at user specified time points without manual intervention thereby enabling the ihm to directly simulate the hydrological effects of changing land use the case study in the heihe river basin effectively demonstrated the applicability and versatility of vhf for integrated sw gw modeling in large river basins the study also demonstrated that the visualization and spatial temporal analysis in vfh can greatly facilitate process understanding and support management decision making although the current version of vhf was specially developed for heiflow and gsflow it can be easily extended to accommodate other ihms as increasing hydrologic and geoscience data become available on the internet future work should improve vhf s capability of retrieving online data from the internet via standard web services software availability name visual heiflow program language c developers dr yong tian tiany sustc edu cn and dr yi zheng zhengy sustc edu cn school of environmental science technology southern university of science technology shenzhen 518055 china availability https github com deephydro visual heiflow acknowledgements this work was supported by the national natural science foundation of china no 41501024 no 41622111 no 91225301 no 91425303 additional support was provided by the southern university of science and technology no g01296001 and the shenzhen municipal science and technology innovation committee jcyj20160530190547253 if not collected by the authors or acknowledged in the text the data used in this study were provided by the heihe program data management center http www heihedata org 
26351,this study developed a comprehensive graphical data processing and modeling system named visual heiflow vhf for integrated surface water groundwater modeling its distinctive features include the following first vhf uses a generic multivariable space time data cube model which enables the system to efficiently handle large time series datasets over a large spatial domain second vhf streamlines the entire modeling procedure from data preparation at the very beginning to visualization and analysis of modeling results in a uniform environment third vhf allows updating the land use input at user specified time points without manual intervention and therefore allows a direct simulation of the hydrological effects of changing land use the applicability and versatility of vhf were demonstrated in a case study in the heihe river basin the second largest inland river basin in china the case study also demonstrated that vfh facilitates process understanding and supports management decision making keywords integrated hydrological modeling visual heiflow geographic information system heihe river basin visualization groundwater 1 introduction physically based integrated hydrological models ihms can depict hydrological processes both at the surface and in the subsurface with sufficient spatial and temporal details in the last decade a number of ihms have been developed such as mike she dhi 2005 parflow kollet and maxwell 2006 penn state integrated hydrologic model pihm qu and duffy 2007 gsflow markstrom et al 2008 hydrogeosphere brunner and simmons 2012 and liquid branger et al 2010 these models have been used to study various water resources issues including estimation of terrestrial water storage seyoum and milewski 2016 surface groundwater interaction hassan et al 2014 impact of climate change huntington and niswonger 2012 responses to land use change nie et al 2011 and optimization of water resource management condon and maxwell 2013 wu et al 2016 a variety of data are needed to establish calibrate validate and apply such complex models and these datasets include topography river network hydrogeological data meteorological data land use land cover lulc maps soil properties and hydrological observations e g streamflow groundwater level soil moisture and evapotranspiration these data have different structures and are often temporally and spatially distributed thus the data processing itself requires tremendous effort and a comprehensive understanding of the complex model which seriously hinders the wider application of ihms in scientific research and management practice to the best of the authors knowledge one stop systems that can handle the entire procedure of integrated hydrological modeling from data preparation at the very beginning to analysis and visualization of simulation results are not available in practice geographic information systems giss e g arcgis and qgis and generic data processing tools e g excel are often used in combination to handle geospatial inputs and outputs of the models in an ad hoc manner as these tools were developed for general purposes they are not able to support many specific operations of integrated hydrological modeling such as watershed delineation generation of a digitized stream network discretization of the model grid assignment of spatiotemporal data to grid cells adjustment of model parameters replacement of model inputs batch simulations and assessment and visualization of model outputs one solution is to couple a gis system with a hydrological model for example arcswat winchell et al 2013 adopts arcmap as the graphical user interface gui of soil and water assessment tool swat however arcmap is a closed source commercial software and has rigid limitations for developing sophisticated applications thus open source gis libraries such as mapwindow dotspatial and qgis are more often used to develop guis for hydrological modeling for example the current version of the usepa s better assessment science incorporating point nonpoint sources basins system kittle et al 2006 was developed based on mapwindow ames et al 2007 2008 hydrodesktop ames et al 2012 and hydromodeler castronova et al 2013 were developed by cuahsi based on dotspatial pihmgis for the pihm model bhatt et al 2014 and qswat for the swat model dile et al 2016 are both based on qgis many problems remain to be addressed in current gis model coupling practices first current giss cannot efficiently handle time series data ihms typically accept and produce time series data at a number of locations when the modeling domain is large and the model grid is of high spatial resolution the size of the time series data can vary from megabytes to gigabytes efficiently presenting and analyzing such large datasets in a spatial setting remain a great challenge for most coupled gis model systems second ihms are often employed to investigate impacts of land use land cover lulc changes on hydrological processes cuo et al 2013 karlsson et al 2016 koch et al 2016 however existing gis model systems are not able to update land use data in a continuous simulation to represent the lulc changes in modeling one has to divide the entire simulation period into multiple periods via manual intervention third the existing systems lack a general design for integrated surface water groundwater sw gw modeling this type of modeling typically requires a large amount of data in different formats users of the existing systems must spend considerable time on data conversion using a set of third party tools and on data transfer between individual tools in addition existing systems are not sufficiently flexible for accommodating new functionalities of the models to address the above problems this study designed and developed visual heiflow hereafter referred to as vhf a comprehensive graphical data processing and modeling system for integrated hydrological simulations the current version of vhf has been structured to accommodate an ihm named heiflow hydrological ecological integrated watershed scale flow model heiflow is an ihm developed by the authors its precursor is gsflow coupled ground water and surface water flow model a representative integrated surface water groundwater model developed by the usgs markstrom et al 2008 compared with existing graphical modeling tools the unique features of vhf mainly include the following first the multivariate space time data cube model in vhf provides a generic and flexible solution to store manipulate and analyze all data types possibly involved in the integrated modeling the data cube model also enables extending the analyzing capabilities of vhf without modifying the core codes second vhf has many useful designs for assisting water resources management and decision making such as updating the land use input at user specified time points without manual intervention and comprehensive water budget analysis capabilities finally with its open architecture vhf can be extended to accommodate different ihms the above system characteristics as well as some others can significantly reduce the difficulty in and time cost of building and using a complex ihm to solve real world water management problems in the remainder of this paper section 2 introduces the framework of vhf including the design concepts system architecture and implementation section 3 provides an application case in the heihe river basin hrb through which the key steps of setting up and running a model in vhf are illustrated the hrb is the second largest endorheic river basin in china and the water and ecological issues in the middle and lower hrb are very typical of china s western inland integrated sw gw modeling has been implemented for the basin tian et al 2015b section 4 demonstrates how the simulation results can be viewed and analyzed from different aspects in vhf section 5 concludes this study 2 framework of the system 2 1 integrated hydrological model although designed to accommodate typical ihms the current version of vhf has been tailored for heiflow the precursor of which is gsflow gsflow integrates prms markstrom et al 2015 with modflow harbaugh 2005 which perform surface hydrology simulation top of plant canopy to soil zone base and 3d groundwater simulation the base of soil zone to the base of aquifers respectively prms and modflow are composed of a set of components called modules packages each of which performs a specific task gsflow simulates all major processes of the hydrologic cycle compared with gsflow heiflow explicitly accounts for and flexibly handles agricultural water use activities i e surface water diversion groundwater pumping and irrigation tian et al 2015a and adequately represents ecohydrological processes of typical land covers in inland river basins such as phreatophytes li et al 2017 a full description of heiflow will be reported in a separate article heiflow delineates surface and subsurface domains using the same horizontal grid in the surface domain each grid cell represents a hydrological response unit hru with a vertical range from the land surface to the bottom of the root zone the subsurface domain which is conceptually the space below the root zone is discretized into finite difference girds heiflow takes daily meteorological data and water use data i e diversion pumping and irrigation as the main forcing functions distinct from existing hydrological models model parameters in heiflow can have time variant values and the model is therefore capable of accepting multiperiod land use data as the model inputs and automatically updating the hru parameters accordingly this unique functionality is important in regions with rapid land use changes 2 2 design concepts two primary challenges have been addressed in developing vhf first how can data be efficiently exchanged between gis and ihm second what is the best method to accommodate various types of model inputs outputs and other relevant datasets to resolve the first challenge a tight gis model coupling methodology was employed the featureset a general abstract object in most existing gis libraries was used as a shared data model between ihm and gis each feature in the featureset has a unique id and contains a geometry i e point polyline and polygon and attributes each model entity e g grid stream and well in vhf is represented by a featureset object each element in the model entity e g an hru or a stream reach exhibits a one to one relationship with the associated feature in the featureset by leveraging the power of existing gis libraries the visualization and manipulation of model features as well as conceptual modeling can be realized in addition the tight coupling strategy has the advantage of independent development of both the gis and the ihm bhatt et al 2014 to address the second challenge we used a multivariate space time data cube model to represent both static and time variant datasets in the ihm the data model is based on the idea of the space time variable cube proposed by maidment 2002 this data cube concept was initially designed for integrating time series from heterogeneous data providers within a relational database the model was also used as a means to reference measurements according to the locations times and types of the measurements in the national water information system goodall et al 2008 previous applications of the data cube were focused on integrating time series of point observations we extended this concept to represent more complicated data types associated with ihms the data cube fig 1 has three dimensions space time and variables therefore each cell in it corresponds to a specific attribute e g precipitation of a spatial object e g an hru at a specific time point e g january 1 2000 the data model has the flexibility to present different data types associated with the ihm for example it can represent time series data fig 1b at a specific location e g a gaging station for a target variable e g streamflow as well as time series data in a 2d fig 1c domain e g basin surface or 3d domain e g multilayered aquifers on the other hand the data model can also represent static but spatially distributed data e g soil parameters of hrus by setting the time dimension to one in vhf the data cube model is represented by a generic class the data cube class not only holds a 3d numeric data array but also provides many generic operations on the 3d array li et al 2014 such as memory management initialization altering dimensional size and serialization the multivariate space time data model provides a standard but yet flexible solution to store and manipulate all data types possibly involved in the integrated modeling it enables us to develop a variety of tools to perform spatiotemporal analyses of the data furthermore a set of standard interfaces tian et al 2016 has been designed to capture common properties and behaviors of ihms packages parameters and other necessary objects in the ihm the interfaces together with the multivariate space time model ensure the flexibility and extensibility of vhf 2 3 system architecture maintaining a clean separation between application logic and the user interface ui can resolve numerous development and design difficulties and make the system much easier to test maintain and improve numerous design patterns are applicable to achieve the separation in this work we adopted model view viewmodel mvvm throughout the development life cycle because it improves code reusability and flexibility and increases application testability to better implement the mvvm pattern an appropriate layered architecture is required fig 2 presents the layered architecture the architecture is divided into five layers the top layer namely the presentation layer is responsible for the look and feel of the application the application layer is responsible for the application logic the application logic can be grouped into four major groups model setup model postprocessing geo processing and spatiotemporal processing the foundation layer provides reusable libraries and modeling framework this layer contains core data structure definitions and provides functionalities to access manipulate and visualize data from the underlying sources the ihms layer contains executable programs of the ihm or ihms which are standalone executable files for the numerical computation of the model or models currently vhf accommodates heiflow but it can be extended to accommodate other ihms by coding corresponding model libraries the foundation layer can accept new customized libraries without affecting the remaining parts of the architecture in the architecture the bottom layer includes gis data files i e raster and vector files the relation database that stores point time series and model input output files storage of the point time series in the relation database follows a standard observational data template called observations data model odm odm provides a generic database template to store hydrologic observations and metadata about the data values and has been widely used within the hydrology community horsburgh and reeder 2014 huang and tian 2013 peckham and goodall 2013 in the odm database the controlled vocabularies method is used to alleviate the semantic heterogeneity problem 2 4 system implementation similar to its precursor gsflow heiflow was coded using fortran and c languages and was compiled into a standalone executional file vhf was developed using the c language based on microsoft net framework version 4 5 vhf communicates with heiflow through input output files and the information of model running e g running progress and any warnings or exceptions during the running the net framework has several advantages for developing vhf first the managed extensibility framework mef embedded in net provides a mechanism through which developers can easily develop lightweight extensible applications it also allows developers to encapsulate code easily and avoid fragile hard dependencies the mef also facilitates implementation of the mvvm pattern the dotspatial library was employed to build the gis framework dotspatial was selected because it supports a wide range of data formats provides comprehensive geo processing functionalities and has an active developer community to provide state of the art software developments furthermore because dotspatial is fully open source and is completely written in the c language dotspatial gis capabilities can be readily incorporated into vhf dotspatial has a simple plugin mechanism with which new functionalities can be added without modifying the core source code vhf extends this mechanism by enabling users to add additional functionalities related to ihm i e access to and manipulation of model packages model input output etc the gui of vhf see fig 3 has a similar style as many popular desktop gis applications the main window contains a ribbon style main menu a map view a legend a project explorer a toolbox and other controls that can be displayed in a tabbed interface with movable dockable panels the map view and 3d view are the main visualization elements vhf uses projects to organize different models when a project is opened the model structure appears in the project explorer and the gis layers are displayed in the legend as shown in fig 3 the structure of an ihm is presented as a hierarchy of submodels packages modules and parameters variables the parameters variables can be further grouped into different categories users can navigate through the hierarchy to see a graphical spatial distribution in the map view or the numerical values of a parameter in the table view the project explorer is the major means to manage an ihm including adding or removing a package many commands can be executed on the packages by right click menus in the project explorer fig 4 illustrates several important tools in vhf fig 4a displays the dem in a three dimensional space using the 3d view tool fig 4b presents the odm database manager which provides an interface to manage records in the odm database i e to explore or edit records import data from external sources such as excel spreadsheets and export records following use specified file formats users can also check the imported time series through plot tools in vhf if any error values are identified users can directly modify the values in the interface fig 4c presents the spatial toolbox which provides a collection of tools to develop and manage vector raster data and to perform various spatial analyses the appearance of the toolbox follows the style of arcgis making it friendly to users who are familiar with arcgis fig 4d presents the model toolbox which is a unique feature of vhf the model toolbox provides a set of tools to perform analyses and processes on the multivariable space time data cube table 1 lists some common tools in the model toolbox vhf can also accept user developed tools to extend the model toolbox capabilities by leveraging the mef 3 system application 3 1 application case this section presents an example of setting up a new ihm and running it in vhf the study area is the heihe river basin hrb the second largest endorheic river basin in china the modeling domain see fig 3 covers the middle and lower hrb plus a portion of the badain jaran desert the domain has an area of 90 589 km2 in our previous work tian et al 2015b a gsflow model was established and calibrated for the same domain using several individual tools including arcgis modelmuse matlab and excel the previous modeling process was extremely complicated and time consuming first no integrated platform is available to prepare the required input files in a single environment and a set of third party tools must be used considerable time was spent on data conversion and transfer between individual tools second the sizes of the input and output files for long term large scale simulations are extremely large for example the modflow cell by cell cbc output file generated by a 13 year run can be up to 100 gigabytes to efficiently manage and visualize such large datasets we designed a strategy to flexibly load and manage values from input output files in vhf input output files of ihms typically contain multiple variables e g the modflow cbc output file may contain as many as 10 variables our strategy allows loading values of one user specified variable at a time from the target file rather than loading values of all variables simultaneously the loaded values are represented by a data cube users are able to manipulate the data cube with great flexibility e g deleting it or releasing memories occupied by a specified dimension of the data cube the data cube can be visualized either by the 2d map view tool or the 3d view tool users are able to visualize any data cube of interest through the tools in this application case a new heiflow model that allows time variant land use inputs was built in vhf table 2 summarizes the data used in this case study which are grouped into four categories the first category is used for model setup and initial parameterization including the dem stream network land use soil type hydrogeology map and borehole data in particular three land cover datasets for the years 2000 2007 and 2011 were collected liu et al 2002 hu et al 2015 as shown in table 3 considerable land use changes occurred during the period from 2000 to 2011 the areas of farmland urban water body and forest expanded significantly the total areas of urban water body and forest increased by 47 86 19 92 and 28 49 respectively the expansion of water body area was mainly due to the recovery of the east juyan lake the terminal lake of the heihe river the total area of farmlands expanded from 5422 km2 in 2000 to 6320 km2 in 2011 representing a 16 56 increase the second category is driving force data we used the same datasets as in our previous study tian et al 2015b the third category is hydrological observations for model calibration and validation including daily streamflow observations at four gaging stations and groundwater level observations at 47 monitoring wells all these observations were imported into the odm database the fourth category is independent information for further analyses of the modeling results e g crosscheck and process interpretation in this case an et dataset monthly et in 2000 2012 with 1 km spatial resolution produced by the etwatch model wu et al 2012 and a leaf area index lai dataset every 8 days in 2000 2012 with 1 km spatial resolution produced by a canopy bidirectional reflectance distribution function brdf model liao et al 2013 were considered these two datasets were originally encoded in the tif format and were converted to the data cube format in vhf all the data used in this study were obtained from the heihe program data management center http www heihedata org unless otherwise noted 3 2 overall workflow the workflow of establishing an ihm and running it in vhf is illustrated in fig 5 the first step fig 5a is to create a new project the user needs to define the starting and ending time points of the entire simulation specify the number of stress periods and then define the starting and ending time points of each stress period if time variant land uses are considered the user also needs to specify the time points to load new land use data the second step fig 5b involves configuring the modeling domain the system has tools for automated watershed delineation stream definition and geometric parameter calculations which makes this step easy the next step fig 5c is to construct the surface model in which the main task is to parameterize hrus initially vfh sets default values for all hru parameters based on the prms technical document markstrom et al 2015 the user can further modify the parameter values based on the geospatial data of the study area such as dem land use land cover soil maps and others a set of spatial analysis tools is provided in vfh to aid users in adjusting parameter values the fourth step fig 5d is to build the subsurface model vhf uses a conceptual model approach to help the user complete aquifer parameterization define sources sinks and set boundary conditions the conceptual model approach used in vhf is similar to groundwater modeling system gms software aquaveo 2017 it produces a high level representation of the groundwater model using common gis layers i e points polylines and polygons cascade parameters can also be calculated in this step after the construction of surface and subsurface models the user can generate driving forces including meteorological data and irrigation data fig 5e at this point all the model input files are ready the next step fig 5f is optional to perform model calibration and validation the user needs to collect observations of key hydrological variables such as river flow rates at gaging stations and groundwater levels at monitoring wells the user can import the collected point time series into the odm database using the tool shown in fig 4b raster time series from independent remote sensing products or other modeling results can also be imported into vhf the imported data are stored using the data cube and can be used to crosscheck the model results once all the required and optional data files are prepared the user can run the model view the model running state and check model outputs in real time fig 5g finally data visualization and analysis can be performed on the model outputs via the model toolbox fig 5h 3 3 model setup and running 3 3 1 domain decomposition vhf has an automated watershed delineation tool to generate subwatersheds and stream networks based on the dem fig 6 a presents the dialog of the tool it was developed using the analysis using digital elevation models taudem library yildirim et al 2015 once the delineation is completed all the generated layers i e watershed boundary subwatershed boundaries and stream network can be displayed in the map view a delineated watershed is illustrated in fig 6 note that vhf can also import these layers from outside and skip the delineation step after the subwatershed delineation the user needs to further discretize both surface and subsurface domains into basic computing units heiflow adopts a uniform grid for both surface and subsurface discretization such that hrus in the surface domain have the same rectangular shapes as the respective finite difference grid cells in the subsurface domain once the user specifies the size of the grid cell i e width and height and the number of subsurface layers the discretization can be automatically completed in addition topographic and geometry characteristics e g elevation latitude longitude of the centroid and area of each grid cell in the top layer can also be automatically computed the generated grid is stored in a polygon shapefile and its attribute table stores index information related to the row column and cell the grid shapefile only stores active grid cells within the model boundary and the cells outside of the model boundary are omitted 3 3 2 surface model construction after the delineation the hrus are created and parameterization of the hrus is necessary the prms hru involves greater than 100 parameters some parameters are spatially distributed and can be specified based on gis data layers such as the dem land use land cover and soil data layers vhf uses a lookup table and gis overlay operations to automatically assign parameter values to hrus for parameters that are not distributed the user can directly input modify the parameter values in a table editor the parameter values are stored in a parameter file in the modular modeling system mms format leavesley et al 1996 the procedure of assigning hru parameter values based on gis data layers is illustrated in fig 7 using the soil module soilzone module as an example after loading a digital soil map the user can select the parameters to assign values in the coverage setup dialog then a lookup table that relates coverage classification to parameter values is defined in the lookup table dialog finally the lookup table is used to assign parameter values to each hru in an automated manner 3 3 3 time variant land use vhf can accommodate the input of time variant land use to heiflow heiflow can reload surface model parameters at user specified time points without interrupting the model run in vhf a set of surface model parameter files corresponding to land uses in different time periods can be generated in advance based on the method introduced in the previous section the user can then specify the time points for heiflow to load the parameter files as shown in fig 8 three parameter files corresponding to the land uses in 2000 2006 2007 2010 and 2011 2012 were supplied to the model 3 3 4 subsurface model construction the key operations of subsurface model construction include selecting packages developing the conceptual model and generating input files first the user needs to select the packages to be used in the model simulations currently the modflow packages supported by vhf include bas6 dis lpf upw gmg pcg nwt uzf1 sfr2 ghb1 chd1 lak3 wel1 hfb1 gage and oc some of the packages are required for all simulations and some are optional for example one of the flow model packages and one of the solver packages must be selected whereas both the boundary condition and observations packages are optional inputs for each package are contained in a separate file vhf aids users in generating the input files through a conceptual model approach this approach uses gis functionalities to develop a conceptual model of the modeling domain the conceptual model contains a set of shapefiles that define parameter zones as well as local sources sinks such as wells streams wells and general head flow boundaries once appropriate packages are chosen for a simulation the parameters and all other data required by the packages are defined at the conceptual model level then the conceptual model is used to assign parameter and input values to the packages in an automated manner fig 9 presents an example of assigning parameter values to the lpf package the first step is to create a polygon shapefile to define hydrogeological zones the zoning was based on the hydrogeology map of the study area as shown in fig 9 the modeling domain is divided into 75 hydrogeological zones for the top layer each zone has a unique id the lookup table in fig 9 allows the user to enter parameter values e g hydraulic conductivities specific storage and specific yield for each zone then cell by cell value assignment is automatically performed prms uses the concept of cascade to route overland flow and interflow among hrus streams and lakes the calculation of cascade requires information contained in the input files of dis sfr2 and lak3 packages with the generation of those input files vhf is able to calculate the cascade parameters in an automated manner using an embedded tool called the cascade routing tool crt henson et al 2013 3 3 5 preparation of driving forces and calibration datasets heiflow requires daily meteorological and irrigation data as driving forces the meteorological parameters include precipitation temperature maximum minimum and average air pressure relative humidity wind speed and direction and sunshine hours the irrigation data include daily flow volumes of surface water diversion and groundwater pumping heiflow requires time series of meteorological data for each hru if spatially distributed data are available e g data generated by a regional climate model or derived from radar datasets the user can prepare the input files in vhf through resampling if the data are only available at a limited number of hrus e g data recorded at weather stations vhf can perform data interpolation and extrapolation to prepare the input files using the inverse distance weighting idw method or the gradient plus inverse distance weighting gidw method nalder and wein 1998 fig 10 presents the spatial distributions of annual average precipitations over the modeling domain using two different data sources the precipitation data in fig 10a were derived from a regional climate model xiong and yan 2013 namely the regional integrated environmental model system riems 2 0 calibrated for hrb the precipitation data in fig 10b were generated based on the 19 weather stations indicated in the map using the gidw interpolation method the spatial patterns of the two datasets are quite different irrigation is explicitly represented in heiflow irrigation practices mainly include water withdrawal from sources e g rivers lakes and wells water conveyance and distribution of water in the field tian et al 2015a users are required to prepare an input file that contains information of diverting surface water from stream reaches to their linked hrus the spatial linkage between a stream reach and its linked hrus can be defined using the spatial analysis tool provided by vhf the user must specify the daily volume of diverted water from each stream reach the user should also determine the pumping rate at each well fig 11 presents the spatial distribution of annual average water volume diverted from the midstream of the heihe river warmer colors indicate larger diverted water volumes in practice the formats of original driving forces and calibration data sets are quite different the current version of vhf has utilities to convert a variety of commonly used data formats e g excel netcdf and tif into what heiflow accepts to prepare the calibration datasets the user can either use the odm database manager fig 4b or conversion tools in the model toolbox the conversion tools enable the user to convert spatially distributed data e g et encoded in netcdf or popular raster formats e g tif into the data cube format of vhf all other analysis provided by the model toolbox can be performed on the converted data 3 3 6 model running once all necessary input files are ready a model run can be initiated a window will synchronously present the running information see fig 12 the real time budget percent discrepancy is plotted in a chart and running information e g simulation time period execution time and iteration statistics for each time step is displayed in the textbox below the chart any errors or warnings will also appear in the textbox and the user can abort the simulation at any time during the simulation the user can analyze the model outputs at any time before the simulation is complete for example real time water budget results can be monitored this functionality is extremely useful when a model simulation is time consuming e g hours or even days to finish because it enables the user to identify problems and start a new simulation in a timely manner 4 analysis of the application results 4 1 comparison of modeling and observed results vhf can compare simulated time series at any user specified location against the respective observations if the point observations are ready in the odm database fig 13 presents the streamflow comparison at the zhengyixia gaging station after loading the outputs generated by the sfr2 package simulated streamflow in any reach can be plotted in the same manner by selecting a segment id and a corresponding reach id the sfr2 package divides streams into reaches and segments a stream reach is a section of a stream overlaying a particular modflow finite difference cell whereas a segment is a sequence of reaches with shared properties after selecting a gaging station name and choosing streamflow as the variable to display the observed streamflow will be automatically fetched from the database and plotted in the figure the goodness of fit measures in terms of mean squared error mse root mean squared error rmse correlation efficient correlation and nash sutcliffe coefficient karlsson et al can be calculated and presented in a table in fig 13 the mse rmse correlation and nse are 59 096 7 687 0 951 and 0 904 respectively indicating a good match this comparison functionality greatly facilitates model calibration and validation 4 2 detailed water budget analysis a useful feature of heiflow is the calculation of overall water budget and water budgets of vertical conceptual model zones including the surface zone soil zone unsaturated zone and saturated zone the water balance of the entire region can be expressed as follows 1 p sw i n gw i n et sw o u t gw o u t δ s where p denotes precipitation sw i n and gw i n represent the boundary surface water inflow and groundwater inflow respectively et is actual evapotranspiration sw o u t and gw o u t represent surface water outflow and groundwater outflow across the model boundary respectively and δ s is the total storage change et can be further decomposed as follows 2 et et s f et s et u z et s a t et l a k et r i v et c a n a l where et s f is the evapotranspiration from the surface zone including evaporation of intercepted precipitation evaporation from impervious areas and snow sublimation et s et u z and et s a t are subsurface evapotranspiration components of the soil zone unsaturated zone and saturated zone respectively and et r i v et l a k and et c a n a l represent the evapotranspiration from streams lakes and canals respectively the total storage change δ s can be expressed as follows 3 δ s δ s s f δ s l a k δ s r i v δ s c a n a l δ s s δ s u z δ s s a t where δ s s f is land surface storage which comprises snowpack intercepted precipitation and impervious surface storage δ s l a k δ s r i v and δ s c a n a l are storage changes in lakes streams and canals respectively and δ s s δ s u z and δ s s a t are storage changes in the soil zone unsaturated zone and saturated zone respectively the water balance of all hrus including surface zone and soil zone can be expressed as follows 4 p div pr g e s a t et s f et s si r i v si l a k dr r i v dr l a k ur s δ s s f δ s s where div is diversion from streams pr is pumpage from the saturated zone g e s a t is groundwater discharge from the saturated zone to the soil zone si r i v and si l a k are interflow to streams and lakes respectively dr r i v and dr l a k are dunnian and hortonian runoff to streams and lakes respectively and ur s is percolation beneath the soil zone the water balance of the unsaturated zone can be expressed as follows 5 ur s et u z gr u z δ s u z where et u z is evapotranspiration from the unsaturated zone and gr u z is groundwater recharge from the unsaturated zone to the saturated zone the water balance of the saturated zone can be expressed as follows 6 gr u z sg s a t gw i n et s a t g e s a t gs s a t gw o u t p r δ s s a t where sg s a t is groundwater recharge from streams et s a t is evapotranspiration from the saturated zone and gs s a t is groundwater discharge to streams vhf provides a tool to perform detailed water budget analyses following eqs 1 6 as shown in fig 14 a all the budget terms are organized in a tree like hierarchy left column for example the terms associated with the basin water budget are grouped into three categories 1 the input terms include precipitation boundary surface water and groundwater inflows 2 the output terms include evapotranspiration surface water and groundwater outflows and 3 the storage change terms include individual storages in different zones the user can navigate through the hierarchy to see the temporal variation in each budget component over the simulation period for example the chart view in fig 14a presents the seasonal variation in the cumulative storage change the budget analysis tool is able to compute the water balance of the entire modeling domain for user specified time periods fig 14b presents the 12 year average water budget of the entire domain in the application case it can be seen that local precipitation is the largest input item approximately 67 followed by surface inflow from the upper hrb and groundwater inflow both groundwater outflow and surface outflow are zero which indicates that all the water inputs eventually become et and storage change the water budget can also be analyzed zone by zone vertically fig 14c graphically presents the water balance in vertical conceptual model zones such graphs are very informative because the values of all major fluxes and state variables as well as their relationships with the water balances are clearly illustrated this graphical presentation helps the user easily understand the complicated hydrological processes and quickly identify any budget errors vhf enables users to explore details of the sw gw interaction for example fig 14d displays the results of stream aquifer water exchange calculated by the sfr package positive values indicate stream leakage to the aquifer whereas negative values indicate groundwater discharge to streams the spatial variability of the water exchange along the main heihe river is clearly demonstrated three segments with distinctive exchange patterns can be identified the first 35 km segment extending from yingluoxia to the 312 bridge is a losing segment the second segment extending from the 312 bridge to zhengyixia is a gaining segment below zhengyixia the river again loses water to the aquifer other stream related variables e g stream head and groundwater head can also be visualized in a similar manner 4 3 spatio temporal analysis of ecohydrological processes the model toolbox of vhf enables users to analyze spatiotemporal features of ecohydrological processes from different angles in the following examples precipitation p potential et pet et soil moisture sm and lai are considered and the data period is from 2001 to 2012 precipitation is a key model input extracted from a regional climate model xiong and yan 2013 pet et and sm are model outputs and lai uses a remote sensing based product liao et al 2013 the lai data have an eight day time interval and all other data have a daily temporal resolution to analyze long term trends all the data were summarized at an annual interval in the following analyses the summarization can be readily completed using the aggregate tool in the model toolbox of vhf the aggregate tool can summarize time series of targeted variables for each grid cell with regard to a given time interval e g days weeks months and years 4 3 1 spatial pattern analysis the spatial distribution of annual p lai and et were first derived using the temporal mean tool of vhf then a trend analysis was performed on the three variables using the trend tool the trend tool employs a linear regression method to calculate the change rate of a variable in each grid cell based on the following equation stow et al 2003 zhang et al 2016 7 t r e n d n t 1 n t y t t 1 n t t 1 n y t n t 1 n t 2 t 1 n t 2 where n is the number of time steps and y t is the value of a selected variable at time t if t r e n d 0 the variable exhibits an increasing tendency and vice versa fig 15 a b and 15c display the average annual p lai and et respectively and fig 15d e and 15f present their change rates between 2001 and 2012 the average annual precipitation ranges from 30 to 373 mm yr fig 15a and high values occur in the southeast part of the modeling domain where the elevation is relatively high the heterogeneity of the precipitation change is presented in fig 15d although most of the domain has an increasing tendency the southeastern corner where the elevation is greater than 2700 m exhibits a strong decreasing tendency the irrigated farmlands in the middle hrb exhibit high lai values whereas the vast vacant lands and desert areas outside of the oases have near zero lai values fig 15b fig 15e demonstrates that lai has increased in the midstream farmlands and the ejina delta near the end of the heihe river the latter reflects the positive impact of the environmental flow regulation in the middle hrb on the vegetation recovery in the lower hrb tian et al 2015b the spatial distribution of et fig 15c is more complicated because it is significantly influenced by meteorological conditions human activities and sw gw interaction the highest et values approximately 1200 mm yr are observed over reservoirs and lakes farmlands and places where groundwater discharges to the surface e g wetlands and springs also have high annual et values in the gobi desert with sparse vegetation the et is mainly controlled by precipitation the change in et is also complicated fig 15f in areas without irrigation the trend is similar to that of the precipitation in the irrigated farmlands the et exhibits an increasing trend the most notable increase in et is discerned at the terminal lake east juyan lake because the lake has been recovering from dried up conditions in the 1990s due to environmental flow regulation 4 3 2 temporal variability analysis annual values of p lai and et were first calculated for the entire modeling domain using the spatial mean tool and then calculated for three subbasins including the middle hrb mhrb the lower hrb lhrb and the western part of the badain jaran desert bjd using the zonal statistics tool the boundaries of the three subbasins are the same as those in our previous studies tian et al 2015b the annual precipitation ranges from 5 70 billion m3 yr to 13 33 billion m3 yr during 2001 2012 fig 16 a for the entire domain with a slightly decreasing tendency nonetheless the change varies in different subbasins fig 16d for example mhrb exhibits a significant increase whereas bjd demonstrates a significant decrease during this period for lai an increasing trend is observed for the entire domain fig 16b and the increase mainly reflects the notable vegetation recovery in the lhrb fig 16e spatial details of the lai variation in the mhrb and the ejina delta in the lhrb are further displayed in fig 17 a and b respectively although the mhrb exhibits an overall increasing lai trend an area in gaotai county experienced an lai decrease fig 17a this area has many wetlands fed by direct groundwater discharge due to rapidly increasing groundwater pumping during the data period the discharge significantly declined which affected the plant growth in the wetlands in contrast the increase in lai in the ejina delta fig 17b is largely due to the recovery of groundwater as a direct outcome of the environmental flow regulation in the mhrb as discussed in previous studies tian et al 2015b wu et al 2015 2016 under the existing water allocation regulation the recovery of the groundwater in the lhrb occurs at the cost of groundwater depletion in the mhrb because groundwater pumping in the mhrb has not been well regulated in addition the increase in annual et is notable throughout the entire domain fig 16c and each subbasin fig 16f 4 3 3 correlation analysis the model toolbox in vhf also provides a tool named correlation to calculate the correlation coefficients of targeted variables in each grid cell fig 18 provides an example of et pet correlation at an annual time scale the spatial pattern demonstrated in fig 18 reflects the underlying processes for instance the et pet correlation is significantly positive in farmlands because farmlands are well irrigated and the annual et is energy limited rather than water limited in contrast the et pet correlation is significantly negative in the gobi deserts and many vacant lands because the annual et there is water limited 4 4 impacts of land use change as introduced in section 3 3 3 vhf can handle changing land use in a continuous simulation without any manual intervention this section exemplifies the importance of this functionality in the original model run three periods of land use including 2000 2007 and 2011 were used for comparison an additional model run with the fixed land use of 2000 was performed the two model runs led to the same simulation results for the period 2000 2006 but different results thereafter i e 2007 2012 for example fig 19 maps the difference in average annual et between the two models positive differences indicate that the time variant land use led to more et than the fixed land use and negative differences indicate the opposite the positive differences are mainly due to changes from vacant land gobi desert areas to farmlands grasslands these changes commonly occurred at the edges of artificial oases where farmlands expanded rapidly or along the western boundary of badain jaran desert where wetlands have been recovering in recent years the total amount of the positive differences is 0 262 billion m3 yr which is a significant number with regard to the total water budget in contrast the negative differences are mainly associated with some unchanged farmland hrus or hrus that changed from farmland grassland to vacant land because the irrigation of these hrus has been reduced over time the total amount of the negative differences is 0 205 billion m3 yr which is also a significant number this et example indicates that ignoring significant land use changes in the integrated modeling will introduce large modeling uncertainty 5 conclusions this study developed a comprehensive graphical data processing and modeling system for integrated hydrological simulation named visual heiflow vhf vhf provides a gis based environment to establish and run integrated hydrological models and to visualize and analyze the model inputs outputs its distinctive features in comparison with existing hydrological modeling systems include the following first vhf uses a generic multivariable space time data cube model which enables the system to efficiently handle huge time series datasets over large spatial domains second vhf streamlines the entire integrated modeling procedure from data preparation at the very beginning to visualization and analysis of modeling results in a uniform environment third vhf allows updating of the land use input at user specified time points without manual intervention thereby enabling the ihm to directly simulate the hydrological effects of changing land use the case study in the heihe river basin effectively demonstrated the applicability and versatility of vhf for integrated sw gw modeling in large river basins the study also demonstrated that the visualization and spatial temporal analysis in vfh can greatly facilitate process understanding and support management decision making although the current version of vhf was specially developed for heiflow and gsflow it can be easily extended to accommodate other ihms as increasing hydrologic and geoscience data become available on the internet future work should improve vhf s capability of retrieving online data from the internet via standard web services software availability name visual heiflow program language c developers dr yong tian tiany sustc edu cn and dr yi zheng zhengy sustc edu cn school of environmental science technology southern university of science technology shenzhen 518055 china availability https github com deephydro visual heiflow acknowledgements this work was supported by the national natural science foundation of china no 41501024 no 41622111 no 91225301 no 91425303 additional support was provided by the southern university of science and technology no g01296001 and the shenzhen municipal science and technology innovation committee jcyj20160530190547253 if not collected by the authors or acknowledged in the text the data used in this study were provided by the heihe program data management center http www heihedata org 
26352,in comparison to a local scale flood risk analysis modeling flood losses and risks at the river basin scale is challenging particularly in mountainous watersheds extreme precipitation can be distributed spatially and temporally with remarkable variability depending on the topography of the river basin and the topological characteristics of the river network certain rainfall patterns can lead to a synchronization of the flood peaks between tributaries and the main river thus these complex interactions can lead to high variability in flood losses in addition flood inundation modeling at the river basin scale is computationally resource intensive and the simulation of multiple scenarios is not always feasible in this study we present an approach for reducing complexity in flood risk modeling at the river basin scale we developed a surrogate model for flood loss analysis in the river basin by decomposing the river system into a number of subsystems a relationship between flood magnitude and flood losses is computed for each floodplain in the river basin by means of a flood inundation and flood loss model at sub meter resolution this surrogate model for flood loss estimation can be coupled with a hydrological hydraulic model cascade allowing to compute a high number of flood scenarios for the whole river system the application of this model to a complex mountain river basin showed that the surrogate model approach leads to a reliable and computationally fast analysis of flood losses in a set of probable maximum precipitation scenarios hence this approach offers new possibilities for stress test analyses and monte carlo simulations in the analysis of system behavior under different system loads graphical abstract image 1 keywords model coupling surrogate model flood loss estimation complex river system river basin scale 1 introduction floods are one of the most damaging natural hazards accounting for a majority of all economic losses from natural events worldwide unisdr 2015 managing flood risk requires knowledge about hazardous processes and the impact of floods although flood risk management practice is rapidly changing the primary approach at present is the prevention of floods by means of constructing flood defense works such as lateral dams along rivers flood protection measures are typically designed on a local basis and the most optimized solution in terms of cost benefit analysis mechler 2016 shreve and kelman 2014 the insurance of flood risks is also part of flood risk management practices both the design of flood prevention measures and portfolio risk analyses require sound knowledge of flood hazards within a particular area burke et al 2016 the complex processes occurring in river basins that lead to flooding can be simulated with a cascade of dedicated models biancamaria et al 2009 felder et al 2017 wagner et al 2016 thus atmospheric hydrological flood inundation and flood loss models are run subsequently on the basis of precipitation scenarios with a certain probability recently remarkable progress was made for developing model chains from atmospheric to hydrological and hydraulic models either on global scale sampson et al 2015 continental scale e g trigg et al 2016 or river basin scale lian et al 2007 biancamaria et al 2009 paiva et al 2013 laganier et al 2014 falter et al 2015 nguyen et al 2016 felder et al 2017 however the coupling of atmospheric hydrological and hydraulic models mostly ends with the hydraulic model the extension of a model cascade with flood impact models has been rare to date thus only a small number of studies extend the model chains towards a coupled component model from rainfall to flood losses examples of full model chains from rainfall to flood losses are shown by alfieri et al 2016a ward et al 2013 2015 at global scale by alfieri et al 2016b at continental scale by falter et al 2014 falter et al 2015 qiu et al 2017 schumann et al 2013 van dyck and willems 2013 at large river basin scale and by mcmillan and brasington 2008 foudi et al 2015 koivumäki et al 2010 at regional and local scale in most cases of risk analysis a cascading modeling approach is followed the complexity of the processes triggering floods is determined by spatio temporal patterns in precipitation emmanuel et al 2015 by the geomorphic characteristics of the sub catchments of the river basin and by the synchronization of the flood peaks between the tributaries and the main river channel pattison et al 2014 particularly in mountainous catchments with a high topographical complexity the storm track and the precipitation pattern are influenced by the mountain ranges thus the relative timing of peak discharges in river confluences as a consequence of the spatio temporal distribution of the rainfall pattern have to be addressed emmanuel et al 2016 zoccatelli et al 2011 furthermore in mountainous areas the runoff is also determined by the 0 c isothermal altitude and thus by the share of areas with snowfall rather than rainfall zeimetz et al 2017 hence an integrated modeling approach and the coupling of specific simulation models is needed to assess the processes leading to floods in river basins with a complex river topology in addition if the impacts of floods have to be assessed the simulation models have to be extended with impact models feasible solutions for impact modeling address the interactions between natural and social technological systems and include integrated modeling approaches kelly et al 2013 laniak et al 2013 welsh et al 2013 coupled natural and human systems liu et al 2007 o connell and o donnell 2014 or coupled component models strasser et al 2014 in the case of flood impact modeling there is a lack of computationally efficient flood loss models that can be coupled with hydrologic models and used in wider areas at a higher spatial resolution however the availability of data needed for flood risk analysis at the river basin scale is constantly improving and with it the level of detail is rising consequently this non linearly increases the required computing power in many cases probabilistic approaches are required to simulate a high number of flood scenarios e g in monte carlo simulations here a model chain from atmosphere to rainfall runoff flood inundation and flood losses reaches its limits due to the lengthy amount of computing time necessary in addition to the computationally demanding inundation models the flood loss models require computational resources too if targeted at single object scale but applied in a whole river basin therefore the study design of flood risk analysis at the river basin scale has always required a trade off between the level of detail spatial resolution and the size of the study area fewtrell et al 2008 savage et al 2016 usually with an increasing size of the study area the spatial resolution decreases savage et al 2015 thus there is a gap in methods available for representing a river basin system at a high spatial resolution while contemporaneously maintaining the ability to study the complex interactions between the physical processes and the impact on the values at risk however there are other approaches for dealing with computational demands in integrated environmental models than the variation of the model s spatial resolution such approaches include metamodeling strategies the use of model emulators and surrogate models metamodels model emulators response surface modeling and surrogate modeling are often used as synonyms ratto et al 2012 razavi et al 2012a the principal idea behind surrogate models is emulate and to replace the complex simulation model requiring high computational resources with a simplified and fast to run model castelletti et al 2012 yazdi and salehi neyshabouri 2014 a surrogate model can be derived by simplifying the process based model structure or by generalizing the studied system s behavior with a low order approximation of a set of outcomes of a model experiment with the complex model castelletti et al 2012 dynamic emulation modeling aims at preserving the dynamic nature of the original process based model and is thus preferably used for reducing complexity castelletti et al 2012 surrogate models are often used in applications that require a large number of model runs e g in sensitivity analysis in scenario analysis and in optimization in flood management applications surrogate models have been used for reservoir operation castro gama et al 2014 tsoukalas and makropoulos 2015 water resources management tsoukalas et al 2016 and for reducing the complexity in hydraulic simulations gama et al 2014 meert et al 2016 wolfs et al 2015 a review of surrogate modeling in hydrology is given by razavi et al 2012b nevertheless saint geours et al 2014 and marrel et al 2011 stated that the development of surrogate models with spatially distributed inputs and outputs is still an open research question this also applies to object based flood loss modeling where a 2d inundation model computes flow depths for each affected building and the loss model computes the damages on the basis of the flow depths a vulnerability function and the building value hence the question arises if a surrogate modeling approach is suitable to represent the inherent complexities of flood processes that lead to flood losses within a river basin specifically we aim to assess whether the surrogate modeling approach is able to represent the flood processes and their impacts at the river basin scale with a spatial resolution at street level thus the main aim of this work is to develop a surrogate model for flood loss analysis and to evaluate its applicability in the context of a model experiment with multiple scenarios covering different spatiotemporal patterns of rainfall over a river basin with a complex topography within this context the hypothesis is that a river system can be divided into subsystems which are connected within a topological river network wolfs et al 2015 the reaction of the whole system to a flood scenario can then be deduced from the reactions and interactions of the subsystems thereby we aim at contributing to the discussion about the use of surrogate models in model simplification crout et al 2009 van nes et al 2005 and in flood risk analyses wolfs et al 2015 wolfs and willems 2013 2 methods the main goal of flood risk analysis at the river basin scale is to analyze the potential consequences of a selected precipitation scenario or a set of scenarios this is done by a model cascade of rainfall runoff models with 2d hydraulic models producing the flow depths for the flood loss models here we propose a different method where the last two models are substituted the 2d hydraulic model and the flood loss model are replaced by a 1d hydraulic model with a surrogate model for flood loss computation nested into the 1d hydraulic model this requires two main steps first the surrogate model has to be developed then the surrogate model is introduced into the model chain with reduced complexity we tested this method on the aare river basin located upstream of bern switzerland this chapter is organized as follows first the case study and the definition of the system under consideration are described in detail second the development of the surrogate model is described the interrelation between the two main steps is shown in fig 1 third we describe the model evaluation procedure the methods chapter is concluded with a description of the setup and the application of the model chain for flood loss analyses 2 1 system definition and system delimitation the study area is the watershed of the aare river located upstream of bern switzerland see fig 2 the river basin has an area of approximately 3000 km2 and thus is defined as a mesoscale catchment the river network consists of 26 tributary catchments sub catchments with confluence into the floodplains of the main valley the main valley is divided into seven floodplains these are the floodplains of the river reaches hasliaare between meiringen and lake brienz 1 the coastal areas of lake brienz 2 the flood plain of the city interlaken and the river lütschine 3 the coastal areas of lake thun 4 the floodplain of the city thun 5 the river aare between thun and bern 6 and the tributary gürbe between burgistein and belp 7 the flooding processes in the aare river basin are dominated by both riverine and lake flooding the hasliaare floodplain is dominated mainly by riverine flooding however the delta of the hasliaare floodplain is also affected by flooding from lake brienz the lateral shorelines of lake brienz and lake thun are exposed to lake flooding only in contrast the city of interlaken is exposed to four different flooding processes in the eastern part this floodplain is exposed to lake flooding from lake brienz the western part is exposed to lake flooding from lake thun a high water level in lake brienz leads to a high discharge in the aare river between the two lakes consequently the central part of the city of interlaken is exposed to riverine flooding from the southern boundary of the floodplain the tributary lütschine river flows into lake brienz with the occurrence of riverine flooding possible the city of thun is exposed to both lake flooding from lake thun and riverine flooding from the aare river at high lake levels the floodplain of the aare river between thun and bern and the floodplain of the gürbe river are exposed to riverine flooding the discharge of the aare river downstream of thun is dominated mainly by the outflow from lake thun and secondarily by its tributaries transport and deposition of sediment and woody debris were not considered in this analysis the physical processes in the river system considered here are principally defined as flood processes leading to losses at buildings the main driver for the amount of losses due to flooding is the flood magnitude i e peak discharge and lake level with the related flow depths at the location of the exposed buildings thus we assume here that there is a relevant relationship between the flood magnitude and the values at risk in addition to the flood magnitude this relationship also depends upon the hydromorphic characteristics of the floodplain i e how the building stock is topographically and topologically located within or outside the flooded areas and the characteristics of the building stock economic values and vulnerabilities this relationship can also be named as the exposure footprint of a floodplain rougier et al 2013 this approach was described by hubbard et al 2014 for an urban area exposed to flooding here this approach is extended to a number of floodplains each floodplain is defined as a subsystem of the whole river basin the input of the upper boundary condition of a subsystem is the inflow of water the magnitude of the boundary condition is defined by the peak discharge in a river reach in the case of riverine flooding and by the lake level in case of lake flooding the fluxes flood flows between the subsystems are modelled with the hydrodynamic model basement in 1d basechain vetsch et al 2017 fig 3 shows the spatial setup of the river system and the topology between the subsystems 2 2 development of the surrogate model the surrogate model is built in three steps first for each subsystem i e floodplain the range of system loads at the upper boundary condition were defined on the basis of an observed discharge time series typical flood hydrographs i e a synthetic design hydrograph were derived using the guidelines proposed by serinaldi and grimaldi 2011 for each river gauging station in the study area observed hydrographs were normalized in terms of event duration and peak discharge the resulting dimensionless event hydrographs were superimposed and centered around the peak position a two parametric gamma distribution function was fitted to represent the typical shape of the event hydrographs as described by nadarajah 2007 and rai et al 2009 this resulted in a synthetic unit hydrograph that represents a typical hydrograph shape of flood events in the corresponding catchment the synthetic unit hydrograph was scaled to various peak discharges whereas an empirical peak volume ratio was used to determine the corresponding event duration recently developed techniques for the determination of flood type specific synthetic design hydrographs as for example proposed by brunner et al 2017 were not considered in this study the procedure was applied to generate synthetic design hydrographs for a continuous series of peak discharges for each of the floodplains affected by riverine flooding hasliaare river lütschine river aare river between thun and bern gürbe river the synthetic design hydrographs were used as upper boundary conditions for the 2d inundation model of each floodplain in the second step we developed a flood inundation model in 2d for each floodplain we used the flood inundation model basement in 2d baseplane to represent the water fluxes through the river systems vetsch et al 2017 it is a numerical model solving the shallow water equations on the basis of an irregular mesh the mesh was generated on the basis of a digital terrain model dtm of the year 2015 with a spatial resolution of 0 5 m and a maximum error of 0 2 m in the z axis orientation in the river courses the dtm was corrected on the basis of topographical surveys of the riverbed these data were delivered by the federal office for the environment foen the heights and location of the lateral dams were surveyed by dgps together all data sources result in a high resolution terrain model in the flood models the roughness coefficients in the river channels were calibrated with existing stage discharge relationships the roughness coefficients in the floodplains were estimated based on literature the floodplains are delimited by the lateral dimensions of the floodplains i e the confining hillslopes the upper system boundaries are the main river courses flowing into the floodplains the lower system boundary is determined by the lakes or other topographic or geomorphologic constraints delimiting the floodplains the 2d hydrodynamic model provides the basis for the flood loss model in this study we focus only on structural damages to buildings i e residential public and industrial buildings damages to mobile assets building content movables and infrastructure are not considered here the loss model consists of a dataset of buildings with attributes and a set of vulnerability functions each building is represented by a polygon and classified by type functionality construction period volume reconstruction costs altitude level of ground floor and number of residents the dataset of the values at risk was elaborated on the basis of the swissbuildings3d dataset of the federal office for topography swisstopo based on the approaches of fuchs et al 2017 röthlisberger et al 2017 and röthlisberger et al 2016 the reconstruction values of the buildings were calculated on the basis of the volume derived by the lidar surface and terrain models and the mean prices per cubic meter and building function svkg sek svit 2012 accordingly to the practice in switzerland the flood intensity maps flow depths resulting from the hydrodynamic models lead to the calculation of the object specific vulnerability and therefore to the estimation of object specific losses fuchs et al 2012 zischg et al 2013 vulnerability functions provide a degree of loss on the basis of the flow depth at the location of the house the value ranges from 0 no damage to 1 total loss this degree of loss is subsequently multiplied by the specific reconstruction value of the building currently three vulnerability functions are considered in the damage calculation procedure we used the functions of hydrotec 2001 jonkman et al 2008 and dutta et al 2003 shown in fig 4 the multiplication of the reconstruction value of the house with the degree of loss leads to the flood loss for a specific exposed object e g a single house the sum of all losses in the floodplain enters into the flood peak flood loss relationship of the floodplain we modelled the inundation for each floodplain and specific set of synthetic hydrographs this results in a number of simulations ranging from the river discharge capacity to a worst case flood each model run was overlain with the building dataset and the degree of loss was calculated for each single building on the basis of the flow depth at the building as well as the resulting loss thus for each synthetic hydrograph a sum of flood losses in the floodplain is computed furthermore the number of exposed buildings and the number of exposed residents are summarized generally speaking this follows the dynamic emulation modeling approach of castelletti et al 2012 with peak discharge flood magnitude the resulting flood losses and exposed buildings residents increases for each floodplain the shape of this relationship between flood magnitude and flood losses is calculated these floodplain specific curves are the basis of the meta model or surrogate model for further analyses the surrogate model can then be used to extend coupled hydrological hydraulic model chains by nesting it into the 1d hydrodynamic model 2 3 model evaluation the complexity of the model chain requires a validation of the surrogate model and of the surrogate model coupled with the hydrologic hydraulic model in addition the 2d inundation model used for the elaboration of the surrogate model has also to be validated separately thus the coupled hydrologic hydraulic model the 2d inundation model and the surrogate model were validated separately and in the coupled version first the coupled hydrologic hydraulic model 1d was validated against the observed discharge at the catchment outlet in the validation period from 2011 to 2014 for this we computed the nash sutcliffe efficiency nse nash and sutcliffe 1970 and the kling gupta efficiency kge gupta et al 2009 kling et al 2012 second the 2d inundation models used to elaborate the surrogate model were validated with post event data of the floods in may 1999 and august 2005 table 1 the main purpose of the 2d inundation model is to predict the number of affected buildings and to predict the flow depths at the buildings thus a validation of this model should weight the populated areas higher than the areas without values at risk stephens et al 2014 as the surrogate model gives the number of exposed buildings and the flood losses as outputs we adapted a validation approach proposed by zischg et al 2018 that explicitly focuses on the validity of the flood models in populated areas they proposed to adapt binary performance measures to be used with insurance claims for validating flood models hence we validated the model performance with the model fit measure f bennett et al 2013 eq 1 also defined as critical success index csi or flood area index fai this measure can be computed by either considering the predicted and observed flooded areas or the number of affected and not affected buildings if based on the flooded areas this performance measure requires a comparison of the spatial pattern of the observed and the modelled wet and dry areas if the populated areas should be weighted higher this performance measure can be computed by overlaying the map of the observed flood extent with the dataset of the buildings the buildings within the observed flood extent represent the reference observation dataset subsequently these buildings are compared with the buildings located in the modelled flood extent buildings correctly predicted as inundated count as hits buildings predicted as dry by the model and observed as inundated are counted as misses buildings predicted as wet by the model but observed as dry are defined as false alarms correct negatives are buildings that are predicted and observed as dry outside of observed flood extent the validation of the 2d inundation model of the floodplains of interlaken and thun is described in zischg et al 2018 1 f hits hits false alarms misses third the surrogate model was coupled with the hydrologic hydraulic model chain and validated with event documentations from three past flood events based on the number of affected buildings we counted the number of buildings that are located within the areas that were reportedly flooded during the flood events in may 1999 august 2005 and october 2011 respectively the characteristics of these reference flood events are summarized in table 1 subsequently we computed the number of affected buildings in these three flood events with the surrogate model for the flood event of 1999 we used the observed discharges for calculating the number of affected buildings with the surrogate model in contrast we used both the observed and modelled discharges of the hydrologic hydraulic model chain to calculate the number of affected buildings during the flood events of 2005 and 2011 consequently we compared the modelled number of affected buildings with the observed ones fourth we analyzed the relative error of the surrogate model depending on the range of peak discharges from the river conveyance to the probable maximum flood we selected different intervals of the synthetic hydrographs used for computing the surrogate model in the aare river reach we used intervals of 100 m3 s to compute the surrogate model while in the other floodplains we used intervals of 50 m3 s except in the gürbe river reach in this floodplain we used intervals of 5 m3 s to estimate the interpolation error we doubled the intervals of the peak discharges for deriving the surrogate model and compared the interpolated value of the coarser surrogate model with the values of the original surrogate model the interpolation error is represented here by the root mean square error rsme however the surrogate models of the lakes are based on a continuous simulation and thus we did not analyze the sensitivity of these models to an increase of the interval fifth we modelled one out of the 150 model runs with the full 2d simulation model we selected the model run with the highest peak discharge at the river basin outlet at bern corresponding to the probable maximum flood this model run was used as a benchmark to evaluate the performance of the surrogate model for this scenario a validation of the loss module was not possible as corresponding loss data are protected by privacy regulations of the corresponding cantonal insurance company however the predicted flood losses where validated in another case study using the same model setup with observed loss data delivered by the cantonal insurance company for buildings zischg et al 2018 in the river reach of the engelberger aa river in the canton of nidwalden the flood event of august 2005 led to losses of around 22 million swiss francs the vulnerability function of jonkman et al 2008 underestimated the losses 26 of documented losses whereas the vulnerability function of hydrotec 2001 overestimated the losses by a factor of 2 7 and the vulnerability function of dutta et al 2003 by a factor of 2 1 thus we assume that the three different vulnerability functions should quantify a range of possible outcomes and the most reliable loss estimation lays in between different outcomes 2 4 computing the system behavior and the flood losses during probable maximum precipitation scenarios we tested the applicability of the surrogate model with a set of extreme flood scenarios based on the probable maximum precipitation pmp the pmp is often used for the analysis of residual risks and furthermore for identifying the probable maximum flood pmf in a river basin the pmp in the study area was estimated after the guidelines of wmo world meteorological organization 2009 to consider the spatio temporal patterns of precipitation in the river basin the same amount of areal precipitation in the pmp scenario 300 mm in 3 days was distributed in different spatio temporal patterns across the entire river basin in a monte carlo simulation framework after felder and weingartner 2016 in a first step a random temporal distribution of the total precipitation for the chosen duration was generated in a second step the temporal pattern of the rainfall was distributed spatially in three meteorological regions and in five sub catchments within each meteorological region the sub catchments and the meteorological regions were defined to consider the relatively independent behavior of specific parts of the catchment e g lowlands and mountainous regions in terms of precipitation amount and intensity a set of plausibility criteria evaluates the physical reliability of the randomly generated rainfall pattern for further details we refer to felder and weingartner 2016 from a set of physically valid 106 iterations we extracted 150 scenarios that lead to the highest discharge at the catchment outlet these rainfall scenarios were fed subsequently into the hydrological model felder et al 2017 the rainfall scenarios were modelled in 15 sub catchments with the hydrological model prevah viviroli et al 2009 the discharge at the outlets of the sub catchments was routed through the river system with the hydraulic model basement in 1d vetsch et al 2017 the hydrodynamic model is based on the st venant equations and computes the water fluxes in 1d this model allows for simulation of the weirs at the outlets of the lakes within the river network and thus is able to simulate lake levels the 1d hydrodynamic model was calibrated by empirically adjusting the friction coefficients in the river channels with particular regard to the water surface elevation in the main channel at peak discharge the setup of this system is described in detail by felder et al 2017 and felder and weingartner 2017 the surrogate model described in chapter 2 2 is nested into this 1d hydraulic model the 1d hydraulic model provides the upper boundary conditions for the single floodplain models the peak discharge is then extracted from the modelled inflow hydrographs and used for interpolating the flood losses from the surrogate models of each floodplain the losses of the single sub models are then summed up for each precipitation scenario we computed the number of exposed buildings and residents and the flood losses for 150 scenarios out of 106 simulations these scenarios had the highest discharge at the outlet of the river basin in bern thus this can be assumed as a set of extreme floods the scenario with the highest discharge at bern was modelled with the 2d simulation model as a reference run in the 2d simulation the tributaries flowing into the floodplains from the lateral boundaries are considered as well as in the loss model the same scenario was then simulated with the surrogate model finally the reliability of the surrogate model was evaluated by comparing it with the reference model run 3 results 3 1 surrogate model the primary result of the first part is the flood magnitude flood exposure relationship for each floodplain in fig 5 these relationships are presented for riverine floods here the sensitivity of the floodplain against peak discharges is shown in terms of the number of exposed buildings and people the figure shows that the floodplain of the lütschine river reach is the sub model with the highest sensitivity to an increasing peak discharge the lake thun sub model is that which has the highest sensitivity against a rising lake level fig 6 it is shown that the exposure of residents is increasing on average by 38 residents per cm of rising lake level regarding flood losses the flood magnitude flood loss relationships exhibit shapes similar to that of the flood magnitude flood exposure relationships in contrast to the exposed buildings and residents the figures for the losses show the uncertainty in flood loss estimation in relation to the vulnerability functions the vulnerability function of jonkman et al 2008 results in remarkably low losses figs 7 and 8 fig 8 shows that the floodplain of thun is the subsystem with the highest sensitivity to increasing flood magnitudes 3 2 model evaluation the model evaluation showed that both the single modules and the model chain can be used reliably in this study the coupled hydrologic hydraulic model has a nse value of 0 85 and a kge value of 0 85 in the validation period 2011 2014 the 2d inundation model was validated with the flood event in august 2005 based on the observed discharges and flooded areas of the flood event in august 2005 the 2d flood model of the aare and gürbe river reaches exhibit a model fit of 0 62 the model of the floodplain in thun has a model fit of 0 61 and the model of the floodplain in interlaken has a model fit of 0 68 with consideration of the dam break in the lütschine river in the hasliaare river reach a dam break occurred and thus the observed flooded areas are remarkably higher than the modelled ones in contrast to the lütschine river reach this dam break was not modelled in the validation run and thus no validation was possible for this river reach and this flood event calculating the model fit on the basis of the modelled discharges model chain of the coupled hydrological hydraulic model based on precipitation as the model input and whole study area gives a model fit of 0 46 when considering flooded areas in the validation metric and a model fit of 0 49 when considering the number of exposed buildings respectively the output of the surrogate model in terms of number of exposed buildings was compared with the observed number of affected buildings however in the corresponding simulation the dam breaks that occurred in the hasliaare river reach and in the floodplain of the lütschine river during the flood event in 2005 were not considered and thus the surrogate model underestimates the number of exposed buildings the surrogate model nested into the full model chain predicted 1643 affected buildings while 2366 buildings were actually located in the flooded areas of the 2005 event table 2 in contrast when run with the observed discharges of the flood event in may 1999 the surrogate model predicts 995 affected buildings while 1059 buildings were actually located in the flooded areas this corresponds to an underestimation of 6 however the surrogate model neglects a dam break in the aare river reach during the flood event in 1999 and thus underestimates the exposed buildings in this area for the 2011 flood event the surrogate model predicts 132 and 38 affected buildings with observed and modelled discharges respectively at river basin scale the full 2d model nearly predicts the exact number of buildings affected by the 2011 flood however when looking at the detail there is a slight underestimation in the hasliaare river reach and a slight overestimation in the lütschine river reach the hydrological model underestimates the peak discharges during this rain on snow flood event rössler et al 2014 and thus the surrogate model underestimates the number of affected buildings when implemented into the full model chain a comparison of the presented surrogate models with coarsened surrogate models shows that the number of simulations for elaborating the surrogate model and thus the intervals between the considered flood magnitudes is relevant for the robustness the submodels have an rmse of 54 buildings in the aare river reach 16 in the hasliaare river reach 28 in the lütschine river reach and 7 in the gürbe river reach for the 2005 flood event the rsme lays in the order of 48 of the exposed buildings in the aare river reach and of 0 5 3 9 in the other river reaches in terms of flood losses the rmse is 43 5 1 5 1 8 and 1 4 million swiss francs respectively while the rmse is highly relevant for the aare river reach it is less relevant for the other river reaches the surrogate model of the aare river reach is already based on wide intervals of the peak discharges and thus a coarsening of the intervals leads to remarkable model errors in contrast narrow intervals increase the robustness this is especially relevant for peak discharges around the river conveyance capacity the benchmark test with the selected scenario run in full 2d mode shows the applicability of the surrogate model in the case of extreme floods the surrogate model predicts 3294 exposed buildings and 15413 residents whereas the full 2d simulation predicts 3720 exposed buildings and 17261 residents thus the losses are underestimated in the surrogate model in comparison to a full 2d simulation the simplified model underestimates the number of exposed buildings and the number of exposed residents by 11 and the computed losses by 13 23 depending on the vulnerability function the deviation can be explained by the flooding of smaller lateral tributaries which the reference model considers in contrast to the surrogate model these smaller tributaries did not lead to flooding in the validation runs in the hasliaare river reach the reference model run simulates flooding that is mainly due to the tributaries thus in this river reach the surrogate model does not consider the flooding of more than 200 buildings in the aare river and gürbe river reaches the surrogate model underestimates the exposure of 188 and 275 buildings respectively for the same reason 3 3 model application the combination of the single surrogate models was used in a monte carlo framework for modeling flood losses of probable maximum precipitation scenarios this results in a high number of outcomes rather than a single value in a deterministic framework fig 9 the number of exposed buildings range from 2181 to 3661 depending on the precipitation scenario with a median of 2768 thereby a minimum of 8569 and a maximum of 16175 residents are exposed as a result the median of the exposed residents is 11079 however the histogram of the losses fig 9 shows a double peak this double peak is a consequence of the different vulnerability functions while two vulnerability functions dutta et al 2003 hydrotec 2001 have relatively similar shapes the third vulnerability function jonkman et al 2008 shows remarkable differences to the others up to flow depths of 3 m the left peak in the histogram at the right of fig 9 shows the losses calculated with the vulnerability function of jonkman et al 2008 the right peak in the histogram shows the losses of the other two vulnerability functions consequently the losses range from 129 to 1499 million swiss francs with a median of 782 million swiss francs the loss footprint of the floodplains allows us to understand which precipitation pattern leads to the highest losses high losses are associated with a high level of lake thun and a high discharge in the lütschine river 4 discussion the presented meta modeling approach in a river basin is a combination of surrogate models the main benefit of this approach is that it enables analysis of the behavior of a complex river system under varying system loads the basis of the model is a 1d hydrodynamic routing model vetsch et al 2017 that routes the inflow fluxes from the sub catchments provided by the hydrological model through the connected floodplains the flood loss sub models are nested into this hydrodynamic routing model similar to alarcon et al 2014 mani et al 2014 and bermúdez et al 2017 except in the form of surrogate models since the 1d hydrodynamic routing model is remarkably 2000 4000 times faster than the 2d flood inundation model with a high spatial resolution the combination of the 1d hydrodynamic model with the surrogate for flood loss computation offers a high potential in scenario based flood risk analyses and in other applications that demand low computational costs this is in line with the conclusions of wolfs and willems 2013 and wolfs et al 2015 because the meta model is derived from a flood inundation model in a very high spatial resolution accuracy at the sub meter resolution the high spatial accuracy can be brought to the river basin scale in a computationally efficient framework hence the presented model can be used in monte carlo simulations targeting flood loss analyses as shown in the example of probable maximum precipitation scenarios if the number of scenarios to be simulated remarkably exceeds the number of synthetic hydrographs required for building the surrogate model the simplified model is able to reduce the computational costs however the presented surrogate model has still notable uncertainties in comparison to a full 2d simulation it introduces an interpolation error this error depends on the intervals of flood magnitudes that are as reference simulations needed for elaborating the meta model this is especially relevant for flood events with a high frequency but it can be solved by increasing the number of simulations with a magnitude slightly below and above river conveyance capacity furthermore the surrogate model represents the errors of the 2d model and the loss model a crucial factor is the spatial representation and attribution of the buildings uncertainties in the building dataset are directly relevant for the prediction capability of the surrogate model both errors either caused by the model simplification or by the 2d inundation and flood loss models contribute to the total error while the first is more easily to consider by increasing the number of simulations in the pre processing and elaboration of the surrogate model the errors in the inundation and flood loss models are in many cases difficult to detect and to quantify because of lacking documentation of historic flood events and their impacts last but not least the surrogate model depends on reliable predictions of peak discharges and thus it heavily depends on the reliability of the coupled hydrologic hydraulic model a comparison of the loss estimations that are based either on observed or modelled discharges showed that the uncertainty in the prediction of the peak discharges is still the one of the most relevant contributions to the overall uncertainty in general the surrogate models show the effects of an increase in river discharge on the flood exposure nevertheless the surrogate models do not consider the smaller tributaries yet the reference run shows that in the study area the lateral tributaries play a relevant role in causing flood losses and producing the peak discharge in the main river reach in other cases than those presented the lateral tributaries may be a less significant driver for flood losses than the main river reach with the consideration of more tributaries the system could potentially be better represented by surrogate models in principle the presented approach can be extended with consideration of the tributaries however the problem of duple exposures arises i e buildings that are affected by both the main river and a tributary should not be counted twofold this remains to be addressed it could be solved by developing spatially distributed surrogate models e g meta models that show the relationship between the peak discharge of the main river or the nearby tributary and the flow depths for each single building in such a simulation duple exposure of buildings from the main river and the tributary can be identified and considered however the level of complexity increases and with it the required pre processing work needs to be considered consequently one has to ask for the practicability and efficiency of the approach crout et al 2009 wolfs et al 2015 furthermore discharge time series which are needed for the elaboration of the tributaries synthetic design hydrographs are often not available moreover at the confluence of two river reaches the synchronization of the peak discharges in both rivers plays a determinant role in flood loss estimation neal et al 2013 thus the surrogate modeling approach must be extended by considering multiple scenarios that depend upon each other another approach to overcome this critical issue is to scrupulously define the validity of the model predictions by a rigorously dedicated spatial delimitation of the study area this can be done either by bounding the system to the floodplains in the valley bottoms only by inserting the flood hydrographs directly into the main river rather than in the lateral border of the floodplains or by restricting the data containing the buildings explicitly to the ones the flood prediction should be valid for the loss computation model was not validated on the basis of loss data in the study area for this purpose another study area had to be chosen where reliable data about flood losses was available however the vulnerability functions used in this work can easily be exchanged with other ones as presented by jongman et al 2012 or merz et al 2013 the uncertainty inherent in the chosen vulnerability function has to be estimated merz et al 2004 merz and thieken 2009 wagenaar et al 2016 as this appears to be one of the most sensitive factors in flood loss estimations moel and aerts 2011 furthermore the transferability of vulnerability functions from one region to another is questionable cammerer et al 2013 but is out of the scope of this study the results of the scenario runs show a high variability in the resulting numbers of exposed buildings and residents as well as flood losses the high variability is in line with the findings of sampson et al 2014 furthermore it must be mentioned that the floodplains in the case study do not show a very high sensitivity to the volume of a flood event in terms of flood loss estimation high flood volumes are represented in this study by high lake levels in cases where the volume of a flood is a remarkable factor for the amount of flood losses the presented approach has to be extended with different forms of synthetic hydrographs other points are not discussed here such as the propagation of the uncertainties in the model cascade framework as discussed by mcmillan and brasington 2008 and rodríguez rincón et al 2015 this as well as the questions regarding the limitations of the use of surrogate models must be analyzed next 5 conclusions with the development and application of a surrogate model we present an approach for reducing complexity in flood risk modeling at the river basin scale without losing the ability to study the complex interactions between the physical processes and the impacts on the values at risk we can verify our hypothesis of decomposing the river system into a number of subsystems and deriving the reaction of the whole system to a rainfall scenario by modeling the behavior of the subsystems in the form of relationships between flood magnitude and flood exposure losses for each subsystem the presented approach is a feasible way to overcome the trade off between the spatial resolution of the inundation model and the accuracy of flood loss prediction we have shown that the use of a surrogate model can bridge different scales by maintaining a high spatial resolution while simultaneously allowing the simulation of a high number of flood scenarios this approach offers new possibilities for stress test analyses and monte carlo simulations demanding low computational resources in order to analyze the system behavior under different system loads it has been shown that the surrogate model approach leads to a reliable and computationally fast analysis of flood losses in a set of probable maximum precipitation scenarios in a river basin thus the approach may be implemented in coupled component models in portfolio risk assessments and in the identification of the hot spots in a river basin furthermore the presented approach may offer a high potential to couple it with real time discharge forecast systems thus this approach may be a basis for making a step forward from short term discharge forecast towards short term loss forecasts in addition the sensitivity analyses of the subsystem may also provide a basis for an inverse modeling approach that searches for the spatio temporal precipitation pattern and leads to the worst case scenario losses software availability the hydraulic model basement is available at http www basement ethz ch acknowledgments the authors acknowledge the federal office for environment foen for providing the data of river cross sections the federal office for topography swisstopo for providing the buildings dataset the federal office for statistics fos for providing the residential statistics and the canton of bern for providing the lidar data and the event documentation data this work was financially supported by the mobiliar lab for natural risks 
26352,in comparison to a local scale flood risk analysis modeling flood losses and risks at the river basin scale is challenging particularly in mountainous watersheds extreme precipitation can be distributed spatially and temporally with remarkable variability depending on the topography of the river basin and the topological characteristics of the river network certain rainfall patterns can lead to a synchronization of the flood peaks between tributaries and the main river thus these complex interactions can lead to high variability in flood losses in addition flood inundation modeling at the river basin scale is computationally resource intensive and the simulation of multiple scenarios is not always feasible in this study we present an approach for reducing complexity in flood risk modeling at the river basin scale we developed a surrogate model for flood loss analysis in the river basin by decomposing the river system into a number of subsystems a relationship between flood magnitude and flood losses is computed for each floodplain in the river basin by means of a flood inundation and flood loss model at sub meter resolution this surrogate model for flood loss estimation can be coupled with a hydrological hydraulic model cascade allowing to compute a high number of flood scenarios for the whole river system the application of this model to a complex mountain river basin showed that the surrogate model approach leads to a reliable and computationally fast analysis of flood losses in a set of probable maximum precipitation scenarios hence this approach offers new possibilities for stress test analyses and monte carlo simulations in the analysis of system behavior under different system loads graphical abstract image 1 keywords model coupling surrogate model flood loss estimation complex river system river basin scale 1 introduction floods are one of the most damaging natural hazards accounting for a majority of all economic losses from natural events worldwide unisdr 2015 managing flood risk requires knowledge about hazardous processes and the impact of floods although flood risk management practice is rapidly changing the primary approach at present is the prevention of floods by means of constructing flood defense works such as lateral dams along rivers flood protection measures are typically designed on a local basis and the most optimized solution in terms of cost benefit analysis mechler 2016 shreve and kelman 2014 the insurance of flood risks is also part of flood risk management practices both the design of flood prevention measures and portfolio risk analyses require sound knowledge of flood hazards within a particular area burke et al 2016 the complex processes occurring in river basins that lead to flooding can be simulated with a cascade of dedicated models biancamaria et al 2009 felder et al 2017 wagner et al 2016 thus atmospheric hydrological flood inundation and flood loss models are run subsequently on the basis of precipitation scenarios with a certain probability recently remarkable progress was made for developing model chains from atmospheric to hydrological and hydraulic models either on global scale sampson et al 2015 continental scale e g trigg et al 2016 or river basin scale lian et al 2007 biancamaria et al 2009 paiva et al 2013 laganier et al 2014 falter et al 2015 nguyen et al 2016 felder et al 2017 however the coupling of atmospheric hydrological and hydraulic models mostly ends with the hydraulic model the extension of a model cascade with flood impact models has been rare to date thus only a small number of studies extend the model chains towards a coupled component model from rainfall to flood losses examples of full model chains from rainfall to flood losses are shown by alfieri et al 2016a ward et al 2013 2015 at global scale by alfieri et al 2016b at continental scale by falter et al 2014 falter et al 2015 qiu et al 2017 schumann et al 2013 van dyck and willems 2013 at large river basin scale and by mcmillan and brasington 2008 foudi et al 2015 koivumäki et al 2010 at regional and local scale in most cases of risk analysis a cascading modeling approach is followed the complexity of the processes triggering floods is determined by spatio temporal patterns in precipitation emmanuel et al 2015 by the geomorphic characteristics of the sub catchments of the river basin and by the synchronization of the flood peaks between the tributaries and the main river channel pattison et al 2014 particularly in mountainous catchments with a high topographical complexity the storm track and the precipitation pattern are influenced by the mountain ranges thus the relative timing of peak discharges in river confluences as a consequence of the spatio temporal distribution of the rainfall pattern have to be addressed emmanuel et al 2016 zoccatelli et al 2011 furthermore in mountainous areas the runoff is also determined by the 0 c isothermal altitude and thus by the share of areas with snowfall rather than rainfall zeimetz et al 2017 hence an integrated modeling approach and the coupling of specific simulation models is needed to assess the processes leading to floods in river basins with a complex river topology in addition if the impacts of floods have to be assessed the simulation models have to be extended with impact models feasible solutions for impact modeling address the interactions between natural and social technological systems and include integrated modeling approaches kelly et al 2013 laniak et al 2013 welsh et al 2013 coupled natural and human systems liu et al 2007 o connell and o donnell 2014 or coupled component models strasser et al 2014 in the case of flood impact modeling there is a lack of computationally efficient flood loss models that can be coupled with hydrologic models and used in wider areas at a higher spatial resolution however the availability of data needed for flood risk analysis at the river basin scale is constantly improving and with it the level of detail is rising consequently this non linearly increases the required computing power in many cases probabilistic approaches are required to simulate a high number of flood scenarios e g in monte carlo simulations here a model chain from atmosphere to rainfall runoff flood inundation and flood losses reaches its limits due to the lengthy amount of computing time necessary in addition to the computationally demanding inundation models the flood loss models require computational resources too if targeted at single object scale but applied in a whole river basin therefore the study design of flood risk analysis at the river basin scale has always required a trade off between the level of detail spatial resolution and the size of the study area fewtrell et al 2008 savage et al 2016 usually with an increasing size of the study area the spatial resolution decreases savage et al 2015 thus there is a gap in methods available for representing a river basin system at a high spatial resolution while contemporaneously maintaining the ability to study the complex interactions between the physical processes and the impact on the values at risk however there are other approaches for dealing with computational demands in integrated environmental models than the variation of the model s spatial resolution such approaches include metamodeling strategies the use of model emulators and surrogate models metamodels model emulators response surface modeling and surrogate modeling are often used as synonyms ratto et al 2012 razavi et al 2012a the principal idea behind surrogate models is emulate and to replace the complex simulation model requiring high computational resources with a simplified and fast to run model castelletti et al 2012 yazdi and salehi neyshabouri 2014 a surrogate model can be derived by simplifying the process based model structure or by generalizing the studied system s behavior with a low order approximation of a set of outcomes of a model experiment with the complex model castelletti et al 2012 dynamic emulation modeling aims at preserving the dynamic nature of the original process based model and is thus preferably used for reducing complexity castelletti et al 2012 surrogate models are often used in applications that require a large number of model runs e g in sensitivity analysis in scenario analysis and in optimization in flood management applications surrogate models have been used for reservoir operation castro gama et al 2014 tsoukalas and makropoulos 2015 water resources management tsoukalas et al 2016 and for reducing the complexity in hydraulic simulations gama et al 2014 meert et al 2016 wolfs et al 2015 a review of surrogate modeling in hydrology is given by razavi et al 2012b nevertheless saint geours et al 2014 and marrel et al 2011 stated that the development of surrogate models with spatially distributed inputs and outputs is still an open research question this also applies to object based flood loss modeling where a 2d inundation model computes flow depths for each affected building and the loss model computes the damages on the basis of the flow depths a vulnerability function and the building value hence the question arises if a surrogate modeling approach is suitable to represent the inherent complexities of flood processes that lead to flood losses within a river basin specifically we aim to assess whether the surrogate modeling approach is able to represent the flood processes and their impacts at the river basin scale with a spatial resolution at street level thus the main aim of this work is to develop a surrogate model for flood loss analysis and to evaluate its applicability in the context of a model experiment with multiple scenarios covering different spatiotemporal patterns of rainfall over a river basin with a complex topography within this context the hypothesis is that a river system can be divided into subsystems which are connected within a topological river network wolfs et al 2015 the reaction of the whole system to a flood scenario can then be deduced from the reactions and interactions of the subsystems thereby we aim at contributing to the discussion about the use of surrogate models in model simplification crout et al 2009 van nes et al 2005 and in flood risk analyses wolfs et al 2015 wolfs and willems 2013 2 methods the main goal of flood risk analysis at the river basin scale is to analyze the potential consequences of a selected precipitation scenario or a set of scenarios this is done by a model cascade of rainfall runoff models with 2d hydraulic models producing the flow depths for the flood loss models here we propose a different method where the last two models are substituted the 2d hydraulic model and the flood loss model are replaced by a 1d hydraulic model with a surrogate model for flood loss computation nested into the 1d hydraulic model this requires two main steps first the surrogate model has to be developed then the surrogate model is introduced into the model chain with reduced complexity we tested this method on the aare river basin located upstream of bern switzerland this chapter is organized as follows first the case study and the definition of the system under consideration are described in detail second the development of the surrogate model is described the interrelation between the two main steps is shown in fig 1 third we describe the model evaluation procedure the methods chapter is concluded with a description of the setup and the application of the model chain for flood loss analyses 2 1 system definition and system delimitation the study area is the watershed of the aare river located upstream of bern switzerland see fig 2 the river basin has an area of approximately 3000 km2 and thus is defined as a mesoscale catchment the river network consists of 26 tributary catchments sub catchments with confluence into the floodplains of the main valley the main valley is divided into seven floodplains these are the floodplains of the river reaches hasliaare between meiringen and lake brienz 1 the coastal areas of lake brienz 2 the flood plain of the city interlaken and the river lütschine 3 the coastal areas of lake thun 4 the floodplain of the city thun 5 the river aare between thun and bern 6 and the tributary gürbe between burgistein and belp 7 the flooding processes in the aare river basin are dominated by both riverine and lake flooding the hasliaare floodplain is dominated mainly by riverine flooding however the delta of the hasliaare floodplain is also affected by flooding from lake brienz the lateral shorelines of lake brienz and lake thun are exposed to lake flooding only in contrast the city of interlaken is exposed to four different flooding processes in the eastern part this floodplain is exposed to lake flooding from lake brienz the western part is exposed to lake flooding from lake thun a high water level in lake brienz leads to a high discharge in the aare river between the two lakes consequently the central part of the city of interlaken is exposed to riverine flooding from the southern boundary of the floodplain the tributary lütschine river flows into lake brienz with the occurrence of riverine flooding possible the city of thun is exposed to both lake flooding from lake thun and riverine flooding from the aare river at high lake levels the floodplain of the aare river between thun and bern and the floodplain of the gürbe river are exposed to riverine flooding the discharge of the aare river downstream of thun is dominated mainly by the outflow from lake thun and secondarily by its tributaries transport and deposition of sediment and woody debris were not considered in this analysis the physical processes in the river system considered here are principally defined as flood processes leading to losses at buildings the main driver for the amount of losses due to flooding is the flood magnitude i e peak discharge and lake level with the related flow depths at the location of the exposed buildings thus we assume here that there is a relevant relationship between the flood magnitude and the values at risk in addition to the flood magnitude this relationship also depends upon the hydromorphic characteristics of the floodplain i e how the building stock is topographically and topologically located within or outside the flooded areas and the characteristics of the building stock economic values and vulnerabilities this relationship can also be named as the exposure footprint of a floodplain rougier et al 2013 this approach was described by hubbard et al 2014 for an urban area exposed to flooding here this approach is extended to a number of floodplains each floodplain is defined as a subsystem of the whole river basin the input of the upper boundary condition of a subsystem is the inflow of water the magnitude of the boundary condition is defined by the peak discharge in a river reach in the case of riverine flooding and by the lake level in case of lake flooding the fluxes flood flows between the subsystems are modelled with the hydrodynamic model basement in 1d basechain vetsch et al 2017 fig 3 shows the spatial setup of the river system and the topology between the subsystems 2 2 development of the surrogate model the surrogate model is built in three steps first for each subsystem i e floodplain the range of system loads at the upper boundary condition were defined on the basis of an observed discharge time series typical flood hydrographs i e a synthetic design hydrograph were derived using the guidelines proposed by serinaldi and grimaldi 2011 for each river gauging station in the study area observed hydrographs were normalized in terms of event duration and peak discharge the resulting dimensionless event hydrographs were superimposed and centered around the peak position a two parametric gamma distribution function was fitted to represent the typical shape of the event hydrographs as described by nadarajah 2007 and rai et al 2009 this resulted in a synthetic unit hydrograph that represents a typical hydrograph shape of flood events in the corresponding catchment the synthetic unit hydrograph was scaled to various peak discharges whereas an empirical peak volume ratio was used to determine the corresponding event duration recently developed techniques for the determination of flood type specific synthetic design hydrographs as for example proposed by brunner et al 2017 were not considered in this study the procedure was applied to generate synthetic design hydrographs for a continuous series of peak discharges for each of the floodplains affected by riverine flooding hasliaare river lütschine river aare river between thun and bern gürbe river the synthetic design hydrographs were used as upper boundary conditions for the 2d inundation model of each floodplain in the second step we developed a flood inundation model in 2d for each floodplain we used the flood inundation model basement in 2d baseplane to represent the water fluxes through the river systems vetsch et al 2017 it is a numerical model solving the shallow water equations on the basis of an irregular mesh the mesh was generated on the basis of a digital terrain model dtm of the year 2015 with a spatial resolution of 0 5 m and a maximum error of 0 2 m in the z axis orientation in the river courses the dtm was corrected on the basis of topographical surveys of the riverbed these data were delivered by the federal office for the environment foen the heights and location of the lateral dams were surveyed by dgps together all data sources result in a high resolution terrain model in the flood models the roughness coefficients in the river channels were calibrated with existing stage discharge relationships the roughness coefficients in the floodplains were estimated based on literature the floodplains are delimited by the lateral dimensions of the floodplains i e the confining hillslopes the upper system boundaries are the main river courses flowing into the floodplains the lower system boundary is determined by the lakes or other topographic or geomorphologic constraints delimiting the floodplains the 2d hydrodynamic model provides the basis for the flood loss model in this study we focus only on structural damages to buildings i e residential public and industrial buildings damages to mobile assets building content movables and infrastructure are not considered here the loss model consists of a dataset of buildings with attributes and a set of vulnerability functions each building is represented by a polygon and classified by type functionality construction period volume reconstruction costs altitude level of ground floor and number of residents the dataset of the values at risk was elaborated on the basis of the swissbuildings3d dataset of the federal office for topography swisstopo based on the approaches of fuchs et al 2017 röthlisberger et al 2017 and röthlisberger et al 2016 the reconstruction values of the buildings were calculated on the basis of the volume derived by the lidar surface and terrain models and the mean prices per cubic meter and building function svkg sek svit 2012 accordingly to the practice in switzerland the flood intensity maps flow depths resulting from the hydrodynamic models lead to the calculation of the object specific vulnerability and therefore to the estimation of object specific losses fuchs et al 2012 zischg et al 2013 vulnerability functions provide a degree of loss on the basis of the flow depth at the location of the house the value ranges from 0 no damage to 1 total loss this degree of loss is subsequently multiplied by the specific reconstruction value of the building currently three vulnerability functions are considered in the damage calculation procedure we used the functions of hydrotec 2001 jonkman et al 2008 and dutta et al 2003 shown in fig 4 the multiplication of the reconstruction value of the house with the degree of loss leads to the flood loss for a specific exposed object e g a single house the sum of all losses in the floodplain enters into the flood peak flood loss relationship of the floodplain we modelled the inundation for each floodplain and specific set of synthetic hydrographs this results in a number of simulations ranging from the river discharge capacity to a worst case flood each model run was overlain with the building dataset and the degree of loss was calculated for each single building on the basis of the flow depth at the building as well as the resulting loss thus for each synthetic hydrograph a sum of flood losses in the floodplain is computed furthermore the number of exposed buildings and the number of exposed residents are summarized generally speaking this follows the dynamic emulation modeling approach of castelletti et al 2012 with peak discharge flood magnitude the resulting flood losses and exposed buildings residents increases for each floodplain the shape of this relationship between flood magnitude and flood losses is calculated these floodplain specific curves are the basis of the meta model or surrogate model for further analyses the surrogate model can then be used to extend coupled hydrological hydraulic model chains by nesting it into the 1d hydrodynamic model 2 3 model evaluation the complexity of the model chain requires a validation of the surrogate model and of the surrogate model coupled with the hydrologic hydraulic model in addition the 2d inundation model used for the elaboration of the surrogate model has also to be validated separately thus the coupled hydrologic hydraulic model the 2d inundation model and the surrogate model were validated separately and in the coupled version first the coupled hydrologic hydraulic model 1d was validated against the observed discharge at the catchment outlet in the validation period from 2011 to 2014 for this we computed the nash sutcliffe efficiency nse nash and sutcliffe 1970 and the kling gupta efficiency kge gupta et al 2009 kling et al 2012 second the 2d inundation models used to elaborate the surrogate model were validated with post event data of the floods in may 1999 and august 2005 table 1 the main purpose of the 2d inundation model is to predict the number of affected buildings and to predict the flow depths at the buildings thus a validation of this model should weight the populated areas higher than the areas without values at risk stephens et al 2014 as the surrogate model gives the number of exposed buildings and the flood losses as outputs we adapted a validation approach proposed by zischg et al 2018 that explicitly focuses on the validity of the flood models in populated areas they proposed to adapt binary performance measures to be used with insurance claims for validating flood models hence we validated the model performance with the model fit measure f bennett et al 2013 eq 1 also defined as critical success index csi or flood area index fai this measure can be computed by either considering the predicted and observed flooded areas or the number of affected and not affected buildings if based on the flooded areas this performance measure requires a comparison of the spatial pattern of the observed and the modelled wet and dry areas if the populated areas should be weighted higher this performance measure can be computed by overlaying the map of the observed flood extent with the dataset of the buildings the buildings within the observed flood extent represent the reference observation dataset subsequently these buildings are compared with the buildings located in the modelled flood extent buildings correctly predicted as inundated count as hits buildings predicted as dry by the model and observed as inundated are counted as misses buildings predicted as wet by the model but observed as dry are defined as false alarms correct negatives are buildings that are predicted and observed as dry outside of observed flood extent the validation of the 2d inundation model of the floodplains of interlaken and thun is described in zischg et al 2018 1 f hits hits false alarms misses third the surrogate model was coupled with the hydrologic hydraulic model chain and validated with event documentations from three past flood events based on the number of affected buildings we counted the number of buildings that are located within the areas that were reportedly flooded during the flood events in may 1999 august 2005 and october 2011 respectively the characteristics of these reference flood events are summarized in table 1 subsequently we computed the number of affected buildings in these three flood events with the surrogate model for the flood event of 1999 we used the observed discharges for calculating the number of affected buildings with the surrogate model in contrast we used both the observed and modelled discharges of the hydrologic hydraulic model chain to calculate the number of affected buildings during the flood events of 2005 and 2011 consequently we compared the modelled number of affected buildings with the observed ones fourth we analyzed the relative error of the surrogate model depending on the range of peak discharges from the river conveyance to the probable maximum flood we selected different intervals of the synthetic hydrographs used for computing the surrogate model in the aare river reach we used intervals of 100 m3 s to compute the surrogate model while in the other floodplains we used intervals of 50 m3 s except in the gürbe river reach in this floodplain we used intervals of 5 m3 s to estimate the interpolation error we doubled the intervals of the peak discharges for deriving the surrogate model and compared the interpolated value of the coarser surrogate model with the values of the original surrogate model the interpolation error is represented here by the root mean square error rsme however the surrogate models of the lakes are based on a continuous simulation and thus we did not analyze the sensitivity of these models to an increase of the interval fifth we modelled one out of the 150 model runs with the full 2d simulation model we selected the model run with the highest peak discharge at the river basin outlet at bern corresponding to the probable maximum flood this model run was used as a benchmark to evaluate the performance of the surrogate model for this scenario a validation of the loss module was not possible as corresponding loss data are protected by privacy regulations of the corresponding cantonal insurance company however the predicted flood losses where validated in another case study using the same model setup with observed loss data delivered by the cantonal insurance company for buildings zischg et al 2018 in the river reach of the engelberger aa river in the canton of nidwalden the flood event of august 2005 led to losses of around 22 million swiss francs the vulnerability function of jonkman et al 2008 underestimated the losses 26 of documented losses whereas the vulnerability function of hydrotec 2001 overestimated the losses by a factor of 2 7 and the vulnerability function of dutta et al 2003 by a factor of 2 1 thus we assume that the three different vulnerability functions should quantify a range of possible outcomes and the most reliable loss estimation lays in between different outcomes 2 4 computing the system behavior and the flood losses during probable maximum precipitation scenarios we tested the applicability of the surrogate model with a set of extreme flood scenarios based on the probable maximum precipitation pmp the pmp is often used for the analysis of residual risks and furthermore for identifying the probable maximum flood pmf in a river basin the pmp in the study area was estimated after the guidelines of wmo world meteorological organization 2009 to consider the spatio temporal patterns of precipitation in the river basin the same amount of areal precipitation in the pmp scenario 300 mm in 3 days was distributed in different spatio temporal patterns across the entire river basin in a monte carlo simulation framework after felder and weingartner 2016 in a first step a random temporal distribution of the total precipitation for the chosen duration was generated in a second step the temporal pattern of the rainfall was distributed spatially in three meteorological regions and in five sub catchments within each meteorological region the sub catchments and the meteorological regions were defined to consider the relatively independent behavior of specific parts of the catchment e g lowlands and mountainous regions in terms of precipitation amount and intensity a set of plausibility criteria evaluates the physical reliability of the randomly generated rainfall pattern for further details we refer to felder and weingartner 2016 from a set of physically valid 106 iterations we extracted 150 scenarios that lead to the highest discharge at the catchment outlet these rainfall scenarios were fed subsequently into the hydrological model felder et al 2017 the rainfall scenarios were modelled in 15 sub catchments with the hydrological model prevah viviroli et al 2009 the discharge at the outlets of the sub catchments was routed through the river system with the hydraulic model basement in 1d vetsch et al 2017 the hydrodynamic model is based on the st venant equations and computes the water fluxes in 1d this model allows for simulation of the weirs at the outlets of the lakes within the river network and thus is able to simulate lake levels the 1d hydrodynamic model was calibrated by empirically adjusting the friction coefficients in the river channels with particular regard to the water surface elevation in the main channel at peak discharge the setup of this system is described in detail by felder et al 2017 and felder and weingartner 2017 the surrogate model described in chapter 2 2 is nested into this 1d hydraulic model the 1d hydraulic model provides the upper boundary conditions for the single floodplain models the peak discharge is then extracted from the modelled inflow hydrographs and used for interpolating the flood losses from the surrogate models of each floodplain the losses of the single sub models are then summed up for each precipitation scenario we computed the number of exposed buildings and residents and the flood losses for 150 scenarios out of 106 simulations these scenarios had the highest discharge at the outlet of the river basin in bern thus this can be assumed as a set of extreme floods the scenario with the highest discharge at bern was modelled with the 2d simulation model as a reference run in the 2d simulation the tributaries flowing into the floodplains from the lateral boundaries are considered as well as in the loss model the same scenario was then simulated with the surrogate model finally the reliability of the surrogate model was evaluated by comparing it with the reference model run 3 results 3 1 surrogate model the primary result of the first part is the flood magnitude flood exposure relationship for each floodplain in fig 5 these relationships are presented for riverine floods here the sensitivity of the floodplain against peak discharges is shown in terms of the number of exposed buildings and people the figure shows that the floodplain of the lütschine river reach is the sub model with the highest sensitivity to an increasing peak discharge the lake thun sub model is that which has the highest sensitivity against a rising lake level fig 6 it is shown that the exposure of residents is increasing on average by 38 residents per cm of rising lake level regarding flood losses the flood magnitude flood loss relationships exhibit shapes similar to that of the flood magnitude flood exposure relationships in contrast to the exposed buildings and residents the figures for the losses show the uncertainty in flood loss estimation in relation to the vulnerability functions the vulnerability function of jonkman et al 2008 results in remarkably low losses figs 7 and 8 fig 8 shows that the floodplain of thun is the subsystem with the highest sensitivity to increasing flood magnitudes 3 2 model evaluation the model evaluation showed that both the single modules and the model chain can be used reliably in this study the coupled hydrologic hydraulic model has a nse value of 0 85 and a kge value of 0 85 in the validation period 2011 2014 the 2d inundation model was validated with the flood event in august 2005 based on the observed discharges and flooded areas of the flood event in august 2005 the 2d flood model of the aare and gürbe river reaches exhibit a model fit of 0 62 the model of the floodplain in thun has a model fit of 0 61 and the model of the floodplain in interlaken has a model fit of 0 68 with consideration of the dam break in the lütschine river in the hasliaare river reach a dam break occurred and thus the observed flooded areas are remarkably higher than the modelled ones in contrast to the lütschine river reach this dam break was not modelled in the validation run and thus no validation was possible for this river reach and this flood event calculating the model fit on the basis of the modelled discharges model chain of the coupled hydrological hydraulic model based on precipitation as the model input and whole study area gives a model fit of 0 46 when considering flooded areas in the validation metric and a model fit of 0 49 when considering the number of exposed buildings respectively the output of the surrogate model in terms of number of exposed buildings was compared with the observed number of affected buildings however in the corresponding simulation the dam breaks that occurred in the hasliaare river reach and in the floodplain of the lütschine river during the flood event in 2005 were not considered and thus the surrogate model underestimates the number of exposed buildings the surrogate model nested into the full model chain predicted 1643 affected buildings while 2366 buildings were actually located in the flooded areas of the 2005 event table 2 in contrast when run with the observed discharges of the flood event in may 1999 the surrogate model predicts 995 affected buildings while 1059 buildings were actually located in the flooded areas this corresponds to an underestimation of 6 however the surrogate model neglects a dam break in the aare river reach during the flood event in 1999 and thus underestimates the exposed buildings in this area for the 2011 flood event the surrogate model predicts 132 and 38 affected buildings with observed and modelled discharges respectively at river basin scale the full 2d model nearly predicts the exact number of buildings affected by the 2011 flood however when looking at the detail there is a slight underestimation in the hasliaare river reach and a slight overestimation in the lütschine river reach the hydrological model underestimates the peak discharges during this rain on snow flood event rössler et al 2014 and thus the surrogate model underestimates the number of affected buildings when implemented into the full model chain a comparison of the presented surrogate models with coarsened surrogate models shows that the number of simulations for elaborating the surrogate model and thus the intervals between the considered flood magnitudes is relevant for the robustness the submodels have an rmse of 54 buildings in the aare river reach 16 in the hasliaare river reach 28 in the lütschine river reach and 7 in the gürbe river reach for the 2005 flood event the rsme lays in the order of 48 of the exposed buildings in the aare river reach and of 0 5 3 9 in the other river reaches in terms of flood losses the rmse is 43 5 1 5 1 8 and 1 4 million swiss francs respectively while the rmse is highly relevant for the aare river reach it is less relevant for the other river reaches the surrogate model of the aare river reach is already based on wide intervals of the peak discharges and thus a coarsening of the intervals leads to remarkable model errors in contrast narrow intervals increase the robustness this is especially relevant for peak discharges around the river conveyance capacity the benchmark test with the selected scenario run in full 2d mode shows the applicability of the surrogate model in the case of extreme floods the surrogate model predicts 3294 exposed buildings and 15413 residents whereas the full 2d simulation predicts 3720 exposed buildings and 17261 residents thus the losses are underestimated in the surrogate model in comparison to a full 2d simulation the simplified model underestimates the number of exposed buildings and the number of exposed residents by 11 and the computed losses by 13 23 depending on the vulnerability function the deviation can be explained by the flooding of smaller lateral tributaries which the reference model considers in contrast to the surrogate model these smaller tributaries did not lead to flooding in the validation runs in the hasliaare river reach the reference model run simulates flooding that is mainly due to the tributaries thus in this river reach the surrogate model does not consider the flooding of more than 200 buildings in the aare river and gürbe river reaches the surrogate model underestimates the exposure of 188 and 275 buildings respectively for the same reason 3 3 model application the combination of the single surrogate models was used in a monte carlo framework for modeling flood losses of probable maximum precipitation scenarios this results in a high number of outcomes rather than a single value in a deterministic framework fig 9 the number of exposed buildings range from 2181 to 3661 depending on the precipitation scenario with a median of 2768 thereby a minimum of 8569 and a maximum of 16175 residents are exposed as a result the median of the exposed residents is 11079 however the histogram of the losses fig 9 shows a double peak this double peak is a consequence of the different vulnerability functions while two vulnerability functions dutta et al 2003 hydrotec 2001 have relatively similar shapes the third vulnerability function jonkman et al 2008 shows remarkable differences to the others up to flow depths of 3 m the left peak in the histogram at the right of fig 9 shows the losses calculated with the vulnerability function of jonkman et al 2008 the right peak in the histogram shows the losses of the other two vulnerability functions consequently the losses range from 129 to 1499 million swiss francs with a median of 782 million swiss francs the loss footprint of the floodplains allows us to understand which precipitation pattern leads to the highest losses high losses are associated with a high level of lake thun and a high discharge in the lütschine river 4 discussion the presented meta modeling approach in a river basin is a combination of surrogate models the main benefit of this approach is that it enables analysis of the behavior of a complex river system under varying system loads the basis of the model is a 1d hydrodynamic routing model vetsch et al 2017 that routes the inflow fluxes from the sub catchments provided by the hydrological model through the connected floodplains the flood loss sub models are nested into this hydrodynamic routing model similar to alarcon et al 2014 mani et al 2014 and bermúdez et al 2017 except in the form of surrogate models since the 1d hydrodynamic routing model is remarkably 2000 4000 times faster than the 2d flood inundation model with a high spatial resolution the combination of the 1d hydrodynamic model with the surrogate for flood loss computation offers a high potential in scenario based flood risk analyses and in other applications that demand low computational costs this is in line with the conclusions of wolfs and willems 2013 and wolfs et al 2015 because the meta model is derived from a flood inundation model in a very high spatial resolution accuracy at the sub meter resolution the high spatial accuracy can be brought to the river basin scale in a computationally efficient framework hence the presented model can be used in monte carlo simulations targeting flood loss analyses as shown in the example of probable maximum precipitation scenarios if the number of scenarios to be simulated remarkably exceeds the number of synthetic hydrographs required for building the surrogate model the simplified model is able to reduce the computational costs however the presented surrogate model has still notable uncertainties in comparison to a full 2d simulation it introduces an interpolation error this error depends on the intervals of flood magnitudes that are as reference simulations needed for elaborating the meta model this is especially relevant for flood events with a high frequency but it can be solved by increasing the number of simulations with a magnitude slightly below and above river conveyance capacity furthermore the surrogate model represents the errors of the 2d model and the loss model a crucial factor is the spatial representation and attribution of the buildings uncertainties in the building dataset are directly relevant for the prediction capability of the surrogate model both errors either caused by the model simplification or by the 2d inundation and flood loss models contribute to the total error while the first is more easily to consider by increasing the number of simulations in the pre processing and elaboration of the surrogate model the errors in the inundation and flood loss models are in many cases difficult to detect and to quantify because of lacking documentation of historic flood events and their impacts last but not least the surrogate model depends on reliable predictions of peak discharges and thus it heavily depends on the reliability of the coupled hydrologic hydraulic model a comparison of the loss estimations that are based either on observed or modelled discharges showed that the uncertainty in the prediction of the peak discharges is still the one of the most relevant contributions to the overall uncertainty in general the surrogate models show the effects of an increase in river discharge on the flood exposure nevertheless the surrogate models do not consider the smaller tributaries yet the reference run shows that in the study area the lateral tributaries play a relevant role in causing flood losses and producing the peak discharge in the main river reach in other cases than those presented the lateral tributaries may be a less significant driver for flood losses than the main river reach with the consideration of more tributaries the system could potentially be better represented by surrogate models in principle the presented approach can be extended with consideration of the tributaries however the problem of duple exposures arises i e buildings that are affected by both the main river and a tributary should not be counted twofold this remains to be addressed it could be solved by developing spatially distributed surrogate models e g meta models that show the relationship between the peak discharge of the main river or the nearby tributary and the flow depths for each single building in such a simulation duple exposure of buildings from the main river and the tributary can be identified and considered however the level of complexity increases and with it the required pre processing work needs to be considered consequently one has to ask for the practicability and efficiency of the approach crout et al 2009 wolfs et al 2015 furthermore discharge time series which are needed for the elaboration of the tributaries synthetic design hydrographs are often not available moreover at the confluence of two river reaches the synchronization of the peak discharges in both rivers plays a determinant role in flood loss estimation neal et al 2013 thus the surrogate modeling approach must be extended by considering multiple scenarios that depend upon each other another approach to overcome this critical issue is to scrupulously define the validity of the model predictions by a rigorously dedicated spatial delimitation of the study area this can be done either by bounding the system to the floodplains in the valley bottoms only by inserting the flood hydrographs directly into the main river rather than in the lateral border of the floodplains or by restricting the data containing the buildings explicitly to the ones the flood prediction should be valid for the loss computation model was not validated on the basis of loss data in the study area for this purpose another study area had to be chosen where reliable data about flood losses was available however the vulnerability functions used in this work can easily be exchanged with other ones as presented by jongman et al 2012 or merz et al 2013 the uncertainty inherent in the chosen vulnerability function has to be estimated merz et al 2004 merz and thieken 2009 wagenaar et al 2016 as this appears to be one of the most sensitive factors in flood loss estimations moel and aerts 2011 furthermore the transferability of vulnerability functions from one region to another is questionable cammerer et al 2013 but is out of the scope of this study the results of the scenario runs show a high variability in the resulting numbers of exposed buildings and residents as well as flood losses the high variability is in line with the findings of sampson et al 2014 furthermore it must be mentioned that the floodplains in the case study do not show a very high sensitivity to the volume of a flood event in terms of flood loss estimation high flood volumes are represented in this study by high lake levels in cases where the volume of a flood is a remarkable factor for the amount of flood losses the presented approach has to be extended with different forms of synthetic hydrographs other points are not discussed here such as the propagation of the uncertainties in the model cascade framework as discussed by mcmillan and brasington 2008 and rodríguez rincón et al 2015 this as well as the questions regarding the limitations of the use of surrogate models must be analyzed next 5 conclusions with the development and application of a surrogate model we present an approach for reducing complexity in flood risk modeling at the river basin scale without losing the ability to study the complex interactions between the physical processes and the impacts on the values at risk we can verify our hypothesis of decomposing the river system into a number of subsystems and deriving the reaction of the whole system to a rainfall scenario by modeling the behavior of the subsystems in the form of relationships between flood magnitude and flood exposure losses for each subsystem the presented approach is a feasible way to overcome the trade off between the spatial resolution of the inundation model and the accuracy of flood loss prediction we have shown that the use of a surrogate model can bridge different scales by maintaining a high spatial resolution while simultaneously allowing the simulation of a high number of flood scenarios this approach offers new possibilities for stress test analyses and monte carlo simulations demanding low computational resources in order to analyze the system behavior under different system loads it has been shown that the surrogate model approach leads to a reliable and computationally fast analysis of flood losses in a set of probable maximum precipitation scenarios in a river basin thus the approach may be implemented in coupled component models in portfolio risk assessments and in the identification of the hot spots in a river basin furthermore the presented approach may offer a high potential to couple it with real time discharge forecast systems thus this approach may be a basis for making a step forward from short term discharge forecast towards short term loss forecasts in addition the sensitivity analyses of the subsystem may also provide a basis for an inverse modeling approach that searches for the spatio temporal precipitation pattern and leads to the worst case scenario losses software availability the hydraulic model basement is available at http www basement ethz ch acknowledgments the authors acknowledge the federal office for environment foen for providing the data of river cross sections the federal office for topography swisstopo for providing the buildings dataset the federal office for statistics fos for providing the residential statistics and the canton of bern for providing the lidar data and the event documentation data this work was financially supported by the mobiliar lab for natural risks 
26353,high resolution meteorological data are necessary to understand and predict climate driven impacts on the structure and function of terrestrial ecosystems however the spatial resolution of climate reanalysis data and climate model outputs is often too coarse for studies at local landscape scales additionally climate model projections usually contain important biases requiring the application of statistical corrections here we present meteoland an r package that integrates several tools to facilitate the estimation of daily weather over landscapes both under current and future conditions the package contains functions 1 to interpolate daily weather including topographic effects and 2 to correct the biases of a given weather series e g climate model outputs we illustrate and validate the functions of the package using weather station data from catalonia ne spain re analysis data and climate model outputs for a specific county we conclude with a discussion of current limitations and potential improvements of the package keywords bias correction climate change drought stress statistical downscaling regional climate model weather interpolation abbreviations cdf cumulative distribution function gcm global climate model mae mean average error map mean annual precipitation rcm regional climate model 1 introduction regional climate driven impact studies such as those in forest ecological and hydrological research rely increasingly on spatially and temporally high resolution climatic data for example evaluating the impact of climate on functions and services of forest ecosystems using process based models often requires daily meteorology as input e g davi et al 2006 de cáceres et al 2015 granier et al 2007 ruffault et al 2013 however employing spatially coarse climatic projections drawn from global databases in impact studies can lead to substantial errors e g patsiou et al 2014 randin et al 2009 even relatively fine grained i e 1 km climate products hijmans et al 2005 do not always rely on all the available surface station data as a consequence the surface meteorology of areas with complex terrains may be inadequately described leading to irrelevant inferences of ecological patterns at local scale bedia et al 2013 the tasks needed to obtain data at fine temporal daily and spatial 1 km resolutions are different for observed climate than for climate projections and may depend on the size of the target area and the available data daily meteorological data for a single target location may be obtained from weather sensors installed on site historical data series for the target location may be obtained by comparing the estimates of the recently installed on site sensors with those of a nearby weather station having longer records or with daily reanalysis data extracted from global or continental databases dee et al 2011 hofer et al 2010 turco et al 2014 weedon et al 2011 for a large topographically heterogeneous area however installing sensors may be too expensive and fine grained weather estimates may be obtained by spatial interpolation of records from weather station networks goovaerts 2000 li and heap 2014 wagner et al 2012 different r functions and packages already exist to access daily weather station data iannone 2015 or to calculate radiation in a complex topography brenning 2008 and broad range of options exist to interpolate weather variables using either deterministic approaches such as splines or stochastic methods such as kriging gräler et al 2016 one of the current limitations from the perspective of climate impact modeler however is the lack of platforms providing an integrated interface from which conduct easy and fast spatial interpolation for a given temporal period and a set of relevant weather variables simultaneously regarding climate projections different methodologies have been developed to bridge the gap between the spatial resolution of global circulation models gcms and local observations sunyer et al 2015 winkler et al 2011 one approach to obtain climate change predictions at reasonably fine resolution is through dynamical downscaling which consists on running a high resolution regional climate model rcm driven by gcm outputs or reanalysis data within a limited domain rcms provide to a certain degree physically consistent output at regional scale and allow nowadays reaching spatial resolutions varying from 10 to 50 km jacob et al 2014 kotlarski et al 2014 although rcms provide additional spatial detail compared to gcms rcm resolution may still be too coarse to satisfy the needs of landscape or local scale studies kendon et al 2012 winkler et al 2011 moreover rcm climate projections can have considerable biases including those inherited from gcm forcing that preclude their widespread and direct use glotter et al 2014 ruffault et al 2014 thus generally rcms cannot be directly used as input for impact models unless some form of statistical correction calibration is performed christensen et al 2008 kotlarski et al 2014 turco et al 2013 another approach to reach the local scale is statistical downscaling which is based on empirical relationships established between larger scale variables e g mean sea level pressure and local records in the region of interest e g precipitation in a number of stations benestad et al 2008 the application of statistical techniques on gcm or rcm outputs is also meant to address model biases and requires calibrating transfer functions where local weather data are compared with model outputs for a reference period maraun et al 2017b 2010 winkler et al 2011 during the last decade downscaling has become a strategic topic in international and national climate programs and as a result substantial improvements in statistical and dynamical downscaling have been obtained and are currently under development kotlarski et al 2014 maraun et al 2017a there exist data repositories and tools for accessing gcm and rcm outputs palma 2017 and several r packages specifically designed to perform and evaluate statistical downscaling in its different forms bedia et al 2015 benestad et al 2017 gudmundsson 2016 hanel et al 2017 however generally there is still a gap between climate data providers and impact user needs regarding the choice and parameterization of downscaling procedures potentially limiting the possibility to estimate the climate driven impact uncertainties harris et al 2014 rössler et al 2017 one solution to this problem would be to involve downscaling specialists in every climate driven impact study maraun et al 2017b another perhaps more strategic solution is to promote the development of interdisciplinary collaborative tools that guide decisions of impact researchers by providing access to a selection of robust and sound downscaling and correction procedures here we present meteoland an r package created to facilitate the estimation of observed and future daily weather at any position over complex terrains specifically the package allows the user a to interpolate daily weather records including topographic corrections and the calculation of incident solar radiation and potential evapotranspiration b to statistically correct the biases of weather series such as those given by climate models the main advantage of meteoland over existing r packages is that it covers the full chain of climate data manipulation e g fig 1 helping users to obtain the suitable input needed for ecological impact studies in an easy and integrated way while it currently implements a few among all the options available to perform these tasks it is our intention to continue developing the package as a collaborative project between climate researchers and climate impact modelers in section 2 we describe how daily meteorological data is represented in the package and briefly explain how the interpolation and correction routines work detailed information can be found in the package itself after that section 3 we illustrate the main functions of the package 3 1 the calibration and validation of interpolation routines for the region of catalonia ne spain 3 2 the interpolation of daily weather at 1ha resolution across solsonès a county within catalonia 3 3 the examination of the effect of correcting climate model predictions across solsonès using either coarse resolution re analysis data 0 5 50 km or fine resolution interpolated data 1 km as reference and 3 4 an examination of the corresponding effects in an impact model focused on simulating soil water balance and tree drought stress within forest stands we finish section 4 by briefly discussing limitations of the package and potential improvements 2 package description 2 1 data structures and main functions r package meteoland deals with the estimation of the following daily surface weather variables units in parentheses mean maximum and minimum temperature c precipitation mm l m 2 mean maximum and minimum relative humidity incident solar radiation mj m 2 wind speed m s 1 and wind direction degrees from north at 2 m height building on data structures of the sp package meteoland works with three kinds of spatial structures points grid pixels and full grids class spatialpointsmeteorology is suited to store temporal meteorology series at specific locations it extends spatialpoints in that it includes one data frame for each point where rows correspond to dates and weather variables are in columns each of these data frames is stored in the disk in the form of a separate txt or rds file and point coordinates are stored in an additional metadata file classes spatialpixelsmeteorology and spatialgridmeteorology are suited for storing and mapping daily meteorology over a partial or a full spatial grid respectively these classes extend spatialpixels and spatialgrid objects respectively and contain one data frame for each date where rows are grid cells and columns are weather variables spatialpixelsmeteorology and spatialgridmeteorology objects are stored in the disk using the netcdf format using the ncdf4 r package pierce 2017 functions interpolationpoints interpolationpixels and interpolationgrid perform the spatial interpolation of weather on a set of target points a subset of grid cells or a full spatial grid respectively similarly correctionpoints allows correcting the meteorological series of a set of points calculations can be done either having all data stored in memory or by reading writing meteorological data every time it is needed a feature that becomes critical when processing a large number of points or grid cells table 1 lists these and other functions that are included in meteoland 2 2 spatial interpolation a large number of methods have been proposed for the interpolation of daily meteorology particularly for precipitation daly et al 2002 dodson and marks 1997 goovaerts 2000 liston and elder 2006 thornton et al 1997 and several studies have evaluated and compared their performance dirks et al 1998 li and heap 2014 tabios iii and salas 1985 wagner et al 2012 zhang and srinivasan 2009 generally speaking interpolation schemes that use covariates e g elevation are preferable wagner et al 2012 stochastic methods such are kriging are usually more accurate than simpler deterministic approaches such as inverse distance weighting tabios iii and salas 1985 however differences may be small and other criteria such as ease of parameterization and computational efficiency are also important to drive methodological choices dirks et al 1998 thornton et al 1997 package meteoland implements with a few modifications the daily weather interpolation and estimation algorithms that underpin the u s daymet dataset https daymet ornl gov thornton et al 1997 thornton and running 1999 this approach similar to inverse distance weighting interpolates weather variables using truncated gaussian filters which consist in defining spatial weights w r at radial distance r from a target point p using 1 w r e α r r p 2 e α if r r p 0 otherwise where r p is the truncation distance and α is a kernel shape parameter for each target point the spatial convolution of this filter with a set of weather stations results in a vector of weights r p is automatically adjusted so that it is smaller in data rich regions and is increased in data poor regions the estimation of r p is based on n the average number of observations to be included for each target point and is done using the following algorithm thornton et al 1997 1 a user specified value is used to initialize r p 2 interpolation weights w i are calculated eq 1 for all i 1 n stations and the local station density is calculated as 2 d p i 1 n w i w r p 2 π where w is the average weight over the untruncated region of the filter calculated as 3 w 1 e α α e α 3 a new r p value is calculated as a function of n and d p as 4 r p n d p π where n 2n for the first i 1 iterations and n n for the final iteration 4 the new r p value is substituted in step 2 and steps 2 4 are iterated a specified number of times i the final r p value is used to calculate the final interpolation weights in meteoland estimation of r p is done once for each target point variable and day interpolation of temperature includes a correction for the effects of elevation more specifically a weighted least squares regression is used to assess the relationship between temperature differences and elevation differences in weather station data and this relationship is applied to elevation differences between weather stations and the target point thornton et al 1997 interpolation of relative humidity is done after transforming it to dew point temperature liston and elder 2006 no correction for elevation is performed during interpolation but elevation effects arise when back transforming dew point temperature to relative humidity interpolation of daily precipitation is complicated by the need to predict both precipitation occurrence and conditioned on this precipitation amount thornton et al 1997 defined a binomial predictor of spatial precipitation occurrence as a function of the weighted occurrence at surrounding weather stations conditional on precipitation occurrence the interpolation routine predicts precipitation amount where weighted least squares regression is also used to account for elevation effects interpolation of wind is performed in three different ways depending on the information available if only wind speed data is available the spatial interpolation with gaussian weights is used on wind scalars as described above if weather station data includes wind direction a polar average is calculated using gaussian weights finally if static wind fields are also available e g forthofer et al 2014 the interpolation routine first finds for each weather station the wind field that best matches the observed vector then the wind vectors extracted from the selected wind fields are averaged as before details of the interpolation of each variable can be found in the r package vignette daily potential solar radiation r pot is estimated from solar constant solar declination latitude aspect and slope according to garnier and ohmura 1968 instantaneous potential radiation r pot s is calculated using 5 t 1 sin φ cos h cos a sin z x sin h sin a sin z x cos φ cos h cos z x t 2 cos φ cos a cos z x sin φ cos z x r p o t s i 0 t 1 cos δ t 2 sin δ where ϕ is latitude h is the hour angle measured from solar noon a is aspect measured from north z x is the angle of slope δ is solar declination and i 0 is the solar constant daily potential radiation is obtained by integrating r pot s between sunrise and sunset incident solar radiation r g the amount of solar radiation reaching a surface after accounting for the atmosphere is then calculated as thornton and running 1999 6 r g r p o t t t m a x t f m a x where t t max is the daily average i e between sunrise and sunset maximum cloud free total transmittance and t f max is the proportion of t t max realized on a given day i e cloud correction t t max and t f max are calculated using the interpolated estimates of temperature relative humidity and precipitation thornton and running 1999 the package provides additional functions to divide daily incident radiation into sub daily instantaneous estimates of direct and diffuse radiation spitters et al 1986 2 3 statistical correction generally speaking the statistical correction of a weather data series using a more accurate series is necessary when the more accurate series does not cover the period of interest and the less accurate one does in principle both series can be at the same spatial scale but most often the less accurate series will be at broader scale when used for downscaling coarse scale reanalysis data statistical corrections can be broadly referred to as empirical statistical downscaling benestad et al 2008 when applied to post processing climate model outputs this kind of downscaling usually implies an attempt to correct model biases and is often termed empirical statistical downscaling and error correction themeβl et al 2012 although there are many ways to perform statistical corrections meteoland currently offers three options unbiasing consists in subtracting from the series to be corrected the average difference between the two series for the reference period déqué 2007 let x i be the value of the variable of the more accurate e g local series for a given day i and u i the corresponding value for the less accurate series e g climate model output the bias θ is the average difference over all n days of the reference period 7 θ i n u i x i n i n u i n i n x i n the bias calculated in the reference period is then subtracted from the value of u for any day of the period of interest scaling a slope is calculated by regressing u on x through the origin i e zero intercept using data of the reference period the slope can then be used as scaling factor to multiply the values of u for any day of the period of interest this method should be preferred to unbiasing for non negative variables e g wind speed empirical quantile mapping due to its distributional properties neither multiplicative or additive factors are appropriate for daily precipitation gudmundsson et al 2012 ruffault et al 2014 in this case it has been recommended to compare the empirical cumulative distribution function cdf of the two series for the reference period gudmundsson et al 2012 themeβl et al 2011 the empirical cdfs of x and u for the reference period are approximated using tables of empirical percentiles and this mapping is used to correct values of u for the period of interest 8 c d e c d f x 1 e c d f u u d where ecdf x and ecdf u are the empirical cdfs of x and u respectively values between the percentiles are approximated using linear interpolation meteoland performs quantile mapping by calling functions from r package qmap gudmundsson 2016 for each target location to be processed the correction routine first determines which is the nearest climate model cell and extracts its weather data series for the reference period and the period of interest statistical corrections are done for each of the twelve months separately to account for seasonal variation of distributional differences ruffault et al 2014 although users can choose their preferred correction method for each variable meteoland has default approaches the unbiasing method is used to correct values of mean temperature and radiation whereas precipitation values are by default corrected using empirical quantile mapping including a correction for the frequency of wet days to correct minimum respectively maximum temperature values scaling is applied to the difference between minimum resp maximum temperature and mean temperature scaling is also applied to wind speed mean relative humidity is first transformed to specific humidity the unbiasing method is applied to this variable and the result is back transformed to mean minimum and maximum relative humidity using the previously corrected series of mean maximum and minimum temperature respectively 3 examples of application 3 1 calibration and validation of weather interpolation in catalonia we compiled year 2001 meteorology data of 506 stations within and around the region of catalonia ne of spain fig 2 from the catalan meteorological service and the spanish meteorological agency tomas burguera et al 2016 interpolation parameters α and n were calibrated for minimum temperature maximum temperature dew point temperature and precipitation using function interpolation calibration we determined for each variable the pair of parameter values corresponding to a minimum mean absolute error mae for minimum and maximum temperature mae was smaller when both n and α increased simultaneously and optimum values were n opt tmax n opt tmin 60 and α opt tmax α opt tmin 9 5 table a1 and fig a1 in appendix a optimum interpolation of dew point temperature implied weights estimated using fewer points n opt tdew 10 α opt tdew 2 5 we calibrated interpolation parameters for precipitation in two steps using cost functions that accounted for errors in the spatial and temporal patterns first we determined the parameter combination for interpolation occurrence weights resulting in a minimum value of the geometric average between the mae in the frequency of rainy days and the mae in the frequency of rainy stations optimum values corresponded to n opt pocc 5 and α opt pocc 10 meaning that the occurrence of precipitation in a small set of stations close to the target point was often sufficient to predict precipitation events given these optimum parameters for occurrence we then determined the parameter combination for interpolation precipitation amount weights resulting in a minimum value of the geometric average between the mae in the average daily precipitation and the mae in the average station precipitation the precipitation patterns of a larger area n opt pam 60 and α opt pam 1 were needed to ensure the reliability of regression coefficients fig a2 using the optimum interpolation parameters and function interpolation cv we conducted an evaluation of the prediction errors for all meteorological variables except wind by leave one out cross validation as performance statistics we used mae bias and pearson s correlation coefficient maes for daily predicted vs observed minimum and maximum temperatures were 1 5 c and 1 4 c respectively bias for minimum temperature was negligible 0 00029 c but a small positive bias was found for maximum daily temperature 0 12 c observed and predicted distributions of both variables were very similar except at the extremes fig a3 predictions of temperature range had slightly higher errors and the predicted distribution underrepresented extreme values table 2 fig a3 of all station days with precipitation data the interpolation procedure predicted 19 6 wet days very close to observed 19 5 wet days success rate for occurrence of precipitation was 87 4 92 1 for dry days and 68 0 for wet days mae for annual precipitation across stations was 67 7 mm with errors being larger for stations with larger annual precipitation fig a4 observed and predicted distributions of precipitation values were very similar for values up to 150 mm and the predicted frequency of daily wet stations closely matched the observed frequency fig a4 mae for relative humidity was 6 9 and bias was 0 04 with predicted distribution underrepresenting low humidity values fig a5 cross validation maes for radiation was 2 5 mj m 2 and bias was 0 02 mj m 2 with very low and very high radiation values being slightly underrepresented fig a 6 3 2 weather interpolation over solsonès county we used function interpolationpixels to interpolate daily meteorology for year 2001 and over 100 073 cells of 1 ha covering solsonès 1001 km2 a county in the central part of catalonia fig 1 the county has complex topography including an elevation gradient from south to north fig 3 first row using a buffer of 30 km around the county 63 weather stations had temperature data 79 stations had precipitation data 29 stations had relative humidity data and only 19 stations had wind speed data fig 3 shows the result of estimating meteorology for a particular day 2001 03 01 second and third row and the 2001 annual statistics last row influence of elevation was clear on interpolated daily patterns for temperature and relative humidity estimated radiation was strongly determined by aspect and slope but was also influenced by precipitation events with no precipitation areas receiving more radiation wind speed had the simplest spatial structure because wind data was scarce after averaging daily maps mean annual temperature mat and mean annual precipitation map also reflected topographic influences although tessellation of rainfall patterns aroused following the location of stations 3 3 downscaling correcting climate model predictions for the solsonès county we now illustrate the usage of meteoland for downscaling and correcting climate model predictions over solsonès county daily weather predictions of the cnrm cerfacs cnrm cm5 global model downscaled to europe at 11 km resolution by the cclm4 8 17 regional climate model rcm were obtained for the historical period 1976 2005 and the projection period 2006 2100 under representative concentration pathway rcp 8 5 from the eu cordex project kotlarski et al 2014 available at the earth system grid federation http esgf llnl gov we used the function extractnetcdf to subset the rcm predictions corresponding to the solsonès county to obtain reference historical series on each of 998 1 km grid cells over solsonès we used either a era interim daily re analysis data 1979 2005 at 0 125 resolution original grid at 0 5 resolution dee et al 2011 available at https software ecmwf int or b interpolated weather data 1976 2005 at 1 km resolution obtained using interpolationpoints and weather station data from the previous examples we used function correctionpoints errors with interpolated data as reference to evaluate the errors of rcm predictions before and after statistical correction before correction there was a negative bias in mean temperature and radiation and a positive bias in mean relative humidity fig 4 correction methods were kept to variable defaults and errors after correction were determined by cross validation i e by removing 1 month at a time from the historical period correction of rcm predictions nearly eliminated biases in the previous three variables but small negative biases remained in relative humidity for high elevation areas fig 4 mae values were also substantially lower after correction being residual mae larger at high elevations before correction mean annual precipitation map predicted by the rcm was substantially larger than observed average 280 mm and the proportion of days with precipitation were also over predicted average 55 probably caused by drizzle effect fig 4 errors in map and in the proportion of days with precipitation were strongly reduced after correction we used function correctionpoints to correct rcm projections 2006 2100 on each 1 km grid cell using either reanalysis data or interpolated data as historical reference as before default correction methods were applied large differences existed in patterns of mean annual temperature mat between rcm outputs reanalysis data and interpolated data for the historical period and these differences persisted after correction for the projected period fig 5 a however temperature correction did not substantially alter the 4 c mat increase predicted for rcp 8 5 this was not the case for mean annual precipitation map not only the uncorrected rcm outputs generally overestimated map compared to reanalysis and interpolated data but also the projected decrease in map was milder after corrections an average 160 mm reduction for uncorrected rcm data vs 107 mm and 116 mm for correction with reanalysis data and interpolated data respectively fig 5b this difference in the predicted map trend is the result of the non linear transformation of trends obtained by empirical quantile mapping maraun et al 2017b 3 4 evaluation of the effect of climate change corrections on forest drought we applied the water balance model of de cáceres et al 2015 to 573 forest plots in the solsonès county to illustrate how the predicted impacts of climate change on forest drought may be affected by the correction and downscaling of rcm outputs the purpose of the model is to predict daily variations in soil water content and assess drought stress for woody plants within forest stands forest plot data was obtained from the third spanish forest inventory villanueva 2004 for simplicity forest structure and composition was assumed constant during simulations and soil was assumed to be 1 m deep divided into two layers 0 30 cm 30 100 cm parameter values for species e g leaf area canopy storage capacity light extinction coefficient and soil attributes e g texture rock fragment content macroporosity were set as detailed in de cáceres et al 2015 for each forest plot we simulated daily water balance for two 20 yr periods within the 21st century 2006 2025 and 2081 2100 and using three meteorological inputs a the uncorrected climate series of the corresponding rcm cell b the corrected climate series obtained using correctionpoints and era interim data 0 125 resolution as reference and c the corrected climate series obtained using correctionpoints and interpolated weather station data 1 km resolution as reference for all plants in a forest plot the water balance model produces daily drought stress values scaled between 0 and 1 and calculated as the one complement of whole plant relative conductance as indication of annual drought duration in each plot we calculated the maximum number of consecutive days where the average drought stress of the plot was larger than 0 5 de cáceres et al 2015 mouillot et al 2002 values of annual drought duration were averaged for each plot simulated period and meteorological input regardless of the simulated period the number of drought days ndd was often small when rcm projections were not corrected fig 6 while this result contrasts with the strong reduction in map predicted by uncorrected rcm projections fig 5b it may be explained by historical biases of uncorrected rcm data positive bias in map and relative humidity negative bias in radiation and temperature fig 4 these biases led the water balance model to unrealistically underestimate drought stress in the target area for both periods corrections conducted using reanalysis data led to a much higher frequency of drought days compared to the uncorrected data fig 6 nevertheless they were still not satisfactory because of the strong water deficits predicted in northwest mountainous areas under current and future conditions resulting from a poor representation of altitudinal precipitation gradients fig 5b when applying corrections with fine resolution interpolated data increases in ndd between the two simulated periods 2006 2025 and 2081 2100 were found to be largest in plots of the western areas southern areas did not obtain such increases despite having a drier climate because of plots having low values of leaf area fig 6 northwest mountain forests were not predicted to be strongly affected by drought impacts despite their high leaf area and the predicted decrease in precipitation indicating that precipitation by the end of the century was still sufficient to avoid forest drought stress 4 remarks limitations and potential improvements in our opinion the greatest value of the package presented here is that it offers an integrated and easy to use platform to perform the full chain of tasks required to obtain consistent daily weather data at fine spatial resolution for current and future conditions e g fig 1 with the aim to reduce the burden on the user side the package offers a limited set of statistical options with default parameterizations while we initially designed meteoland to provide suitable inputs for modelling climate change impacts on forests we believe it may be useful for any kind of research studies requiring fine grained climatic data e g agronomy hydrology ecology forestry calibration and especially validation of interpolation routines should be conducted for each weather station data set and target period similarly an evaluation of residual errors is recommended for the historical simulations of every climate model before using the package to correct its projections but see maraun et al 2017b despite its advantages we are aware that the algorithms currently implemented in meteoland may not be optimal for all situations and future work should be directed to improve current methods or even complement them with others as the philosophy of meteoland is to discharge the end user from performing extensive literature reviews and comparisons of available methods we plan on undergoing these tasks ourselves and when necessary implement new routines or wrappers to call functions of other packages in the case of bias correction the largest disadvantage of the methods currently offered is that they ignore the dependency between weather variables although this issue may be less critical when correcting high resolution climate model outputs hempel et al 2013 moreover the advantages disadvantages of alternative forms of bias correction including the role of cross validation are still being discussed ehret et al 2012 gobiet et al 2015 maraun et al 2017b themeβl et al 2012 while climatologists usually prefer having as many options as possible for method comparison and testing we envision meteoland as a platform where an informed subset of all the available methods is offered to the climate driven impact researcher this implies that the development of the package should involve collaborative work between climatologists and climate impact researchers who should be guided to fast and robust methods but not necessarily the most accurate or sophisticated ones the package should offer several choices only when the alternative to be advised depends on factors that the end user is able to easily identify besides revising the methods currently offered for interpolation and statistical correction other aspects of the package could be improved to facilitate climate driven impact studies the calculation of insolation could be improved by taking into account shading from nearby topographic elements for example by calling functions of package rsaga brenning 2008 the data structures of meteoland have been designed to store daily data but they could be generalized to store data at other temporal scales e g sub daily the package currently offers the possibility to access public weather data from the spanish national meteorological agency but similar routines could be added to access observations forecasts or reanalysis data from other providers if the necessary programming interfaces are available iannone 2015 the package could also be improved by incorporating weather generation tools cordano and eccel 2016 which would allow considering the uncertainty in the daily series obtained after downscaling correction of climate model outputs finally although the package is already quite fast most routines are implemented in c computation speed could be increased through parallelization author s contributions mdc conceived the design of the package and implemented most functions nms vg and ac contributed their code to specific functions and package features all authors participated in the design and interpretation of illustrative examples and contributed to manuscript writing software availability package meteoland is freely available at cran https cran r project org package meteoland and detailed information on how to use it can be found at the package website http vegmod ctfc cat meteolandweb development versions can be found at http github com miquelcaceres meteoland acknowledgements we thank the agencia estatal de meteorología aemet and servei meteorològic de catalunya smc for providing daily weather station data authors are grateful to marc stéphanon sergio vicente serrano and three anonymous reviewers for insightful comments on previous versions of this manuscript this work was supported by the era net foresterra project informed pcin 2014 050 by project cgl2014 59742 c2 2 r spanish ministry of economy and competitiveness by a spanish ramon y cajal fellowship to m d c ryc 2012 11109 and by a spanish juan de la cierva fellowship to m t ijci 2015 26953 appendix a supplementary data the following are the supplementary data related to this article decaceres et al ems decaceres et al ems data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 08 003 
26353,high resolution meteorological data are necessary to understand and predict climate driven impacts on the structure and function of terrestrial ecosystems however the spatial resolution of climate reanalysis data and climate model outputs is often too coarse for studies at local landscape scales additionally climate model projections usually contain important biases requiring the application of statistical corrections here we present meteoland an r package that integrates several tools to facilitate the estimation of daily weather over landscapes both under current and future conditions the package contains functions 1 to interpolate daily weather including topographic effects and 2 to correct the biases of a given weather series e g climate model outputs we illustrate and validate the functions of the package using weather station data from catalonia ne spain re analysis data and climate model outputs for a specific county we conclude with a discussion of current limitations and potential improvements of the package keywords bias correction climate change drought stress statistical downscaling regional climate model weather interpolation abbreviations cdf cumulative distribution function gcm global climate model mae mean average error map mean annual precipitation rcm regional climate model 1 introduction regional climate driven impact studies such as those in forest ecological and hydrological research rely increasingly on spatially and temporally high resolution climatic data for example evaluating the impact of climate on functions and services of forest ecosystems using process based models often requires daily meteorology as input e g davi et al 2006 de cáceres et al 2015 granier et al 2007 ruffault et al 2013 however employing spatially coarse climatic projections drawn from global databases in impact studies can lead to substantial errors e g patsiou et al 2014 randin et al 2009 even relatively fine grained i e 1 km climate products hijmans et al 2005 do not always rely on all the available surface station data as a consequence the surface meteorology of areas with complex terrains may be inadequately described leading to irrelevant inferences of ecological patterns at local scale bedia et al 2013 the tasks needed to obtain data at fine temporal daily and spatial 1 km resolutions are different for observed climate than for climate projections and may depend on the size of the target area and the available data daily meteorological data for a single target location may be obtained from weather sensors installed on site historical data series for the target location may be obtained by comparing the estimates of the recently installed on site sensors with those of a nearby weather station having longer records or with daily reanalysis data extracted from global or continental databases dee et al 2011 hofer et al 2010 turco et al 2014 weedon et al 2011 for a large topographically heterogeneous area however installing sensors may be too expensive and fine grained weather estimates may be obtained by spatial interpolation of records from weather station networks goovaerts 2000 li and heap 2014 wagner et al 2012 different r functions and packages already exist to access daily weather station data iannone 2015 or to calculate radiation in a complex topography brenning 2008 and broad range of options exist to interpolate weather variables using either deterministic approaches such as splines or stochastic methods such as kriging gräler et al 2016 one of the current limitations from the perspective of climate impact modeler however is the lack of platforms providing an integrated interface from which conduct easy and fast spatial interpolation for a given temporal period and a set of relevant weather variables simultaneously regarding climate projections different methodologies have been developed to bridge the gap between the spatial resolution of global circulation models gcms and local observations sunyer et al 2015 winkler et al 2011 one approach to obtain climate change predictions at reasonably fine resolution is through dynamical downscaling which consists on running a high resolution regional climate model rcm driven by gcm outputs or reanalysis data within a limited domain rcms provide to a certain degree physically consistent output at regional scale and allow nowadays reaching spatial resolutions varying from 10 to 50 km jacob et al 2014 kotlarski et al 2014 although rcms provide additional spatial detail compared to gcms rcm resolution may still be too coarse to satisfy the needs of landscape or local scale studies kendon et al 2012 winkler et al 2011 moreover rcm climate projections can have considerable biases including those inherited from gcm forcing that preclude their widespread and direct use glotter et al 2014 ruffault et al 2014 thus generally rcms cannot be directly used as input for impact models unless some form of statistical correction calibration is performed christensen et al 2008 kotlarski et al 2014 turco et al 2013 another approach to reach the local scale is statistical downscaling which is based on empirical relationships established between larger scale variables e g mean sea level pressure and local records in the region of interest e g precipitation in a number of stations benestad et al 2008 the application of statistical techniques on gcm or rcm outputs is also meant to address model biases and requires calibrating transfer functions where local weather data are compared with model outputs for a reference period maraun et al 2017b 2010 winkler et al 2011 during the last decade downscaling has become a strategic topic in international and national climate programs and as a result substantial improvements in statistical and dynamical downscaling have been obtained and are currently under development kotlarski et al 2014 maraun et al 2017a there exist data repositories and tools for accessing gcm and rcm outputs palma 2017 and several r packages specifically designed to perform and evaluate statistical downscaling in its different forms bedia et al 2015 benestad et al 2017 gudmundsson 2016 hanel et al 2017 however generally there is still a gap between climate data providers and impact user needs regarding the choice and parameterization of downscaling procedures potentially limiting the possibility to estimate the climate driven impact uncertainties harris et al 2014 rössler et al 2017 one solution to this problem would be to involve downscaling specialists in every climate driven impact study maraun et al 2017b another perhaps more strategic solution is to promote the development of interdisciplinary collaborative tools that guide decisions of impact researchers by providing access to a selection of robust and sound downscaling and correction procedures here we present meteoland an r package created to facilitate the estimation of observed and future daily weather at any position over complex terrains specifically the package allows the user a to interpolate daily weather records including topographic corrections and the calculation of incident solar radiation and potential evapotranspiration b to statistically correct the biases of weather series such as those given by climate models the main advantage of meteoland over existing r packages is that it covers the full chain of climate data manipulation e g fig 1 helping users to obtain the suitable input needed for ecological impact studies in an easy and integrated way while it currently implements a few among all the options available to perform these tasks it is our intention to continue developing the package as a collaborative project between climate researchers and climate impact modelers in section 2 we describe how daily meteorological data is represented in the package and briefly explain how the interpolation and correction routines work detailed information can be found in the package itself after that section 3 we illustrate the main functions of the package 3 1 the calibration and validation of interpolation routines for the region of catalonia ne spain 3 2 the interpolation of daily weather at 1ha resolution across solsonès a county within catalonia 3 3 the examination of the effect of correcting climate model predictions across solsonès using either coarse resolution re analysis data 0 5 50 km or fine resolution interpolated data 1 km as reference and 3 4 an examination of the corresponding effects in an impact model focused on simulating soil water balance and tree drought stress within forest stands we finish section 4 by briefly discussing limitations of the package and potential improvements 2 package description 2 1 data structures and main functions r package meteoland deals with the estimation of the following daily surface weather variables units in parentheses mean maximum and minimum temperature c precipitation mm l m 2 mean maximum and minimum relative humidity incident solar radiation mj m 2 wind speed m s 1 and wind direction degrees from north at 2 m height building on data structures of the sp package meteoland works with three kinds of spatial structures points grid pixels and full grids class spatialpointsmeteorology is suited to store temporal meteorology series at specific locations it extends spatialpoints in that it includes one data frame for each point where rows correspond to dates and weather variables are in columns each of these data frames is stored in the disk in the form of a separate txt or rds file and point coordinates are stored in an additional metadata file classes spatialpixelsmeteorology and spatialgridmeteorology are suited for storing and mapping daily meteorology over a partial or a full spatial grid respectively these classes extend spatialpixels and spatialgrid objects respectively and contain one data frame for each date where rows are grid cells and columns are weather variables spatialpixelsmeteorology and spatialgridmeteorology objects are stored in the disk using the netcdf format using the ncdf4 r package pierce 2017 functions interpolationpoints interpolationpixels and interpolationgrid perform the spatial interpolation of weather on a set of target points a subset of grid cells or a full spatial grid respectively similarly correctionpoints allows correcting the meteorological series of a set of points calculations can be done either having all data stored in memory or by reading writing meteorological data every time it is needed a feature that becomes critical when processing a large number of points or grid cells table 1 lists these and other functions that are included in meteoland 2 2 spatial interpolation a large number of methods have been proposed for the interpolation of daily meteorology particularly for precipitation daly et al 2002 dodson and marks 1997 goovaerts 2000 liston and elder 2006 thornton et al 1997 and several studies have evaluated and compared their performance dirks et al 1998 li and heap 2014 tabios iii and salas 1985 wagner et al 2012 zhang and srinivasan 2009 generally speaking interpolation schemes that use covariates e g elevation are preferable wagner et al 2012 stochastic methods such are kriging are usually more accurate than simpler deterministic approaches such as inverse distance weighting tabios iii and salas 1985 however differences may be small and other criteria such as ease of parameterization and computational efficiency are also important to drive methodological choices dirks et al 1998 thornton et al 1997 package meteoland implements with a few modifications the daily weather interpolation and estimation algorithms that underpin the u s daymet dataset https daymet ornl gov thornton et al 1997 thornton and running 1999 this approach similar to inverse distance weighting interpolates weather variables using truncated gaussian filters which consist in defining spatial weights w r at radial distance r from a target point p using 1 w r e α r r p 2 e α if r r p 0 otherwise where r p is the truncation distance and α is a kernel shape parameter for each target point the spatial convolution of this filter with a set of weather stations results in a vector of weights r p is automatically adjusted so that it is smaller in data rich regions and is increased in data poor regions the estimation of r p is based on n the average number of observations to be included for each target point and is done using the following algorithm thornton et al 1997 1 a user specified value is used to initialize r p 2 interpolation weights w i are calculated eq 1 for all i 1 n stations and the local station density is calculated as 2 d p i 1 n w i w r p 2 π where w is the average weight over the untruncated region of the filter calculated as 3 w 1 e α α e α 3 a new r p value is calculated as a function of n and d p as 4 r p n d p π where n 2n for the first i 1 iterations and n n for the final iteration 4 the new r p value is substituted in step 2 and steps 2 4 are iterated a specified number of times i the final r p value is used to calculate the final interpolation weights in meteoland estimation of r p is done once for each target point variable and day interpolation of temperature includes a correction for the effects of elevation more specifically a weighted least squares regression is used to assess the relationship between temperature differences and elevation differences in weather station data and this relationship is applied to elevation differences between weather stations and the target point thornton et al 1997 interpolation of relative humidity is done after transforming it to dew point temperature liston and elder 2006 no correction for elevation is performed during interpolation but elevation effects arise when back transforming dew point temperature to relative humidity interpolation of daily precipitation is complicated by the need to predict both precipitation occurrence and conditioned on this precipitation amount thornton et al 1997 defined a binomial predictor of spatial precipitation occurrence as a function of the weighted occurrence at surrounding weather stations conditional on precipitation occurrence the interpolation routine predicts precipitation amount where weighted least squares regression is also used to account for elevation effects interpolation of wind is performed in three different ways depending on the information available if only wind speed data is available the spatial interpolation with gaussian weights is used on wind scalars as described above if weather station data includes wind direction a polar average is calculated using gaussian weights finally if static wind fields are also available e g forthofer et al 2014 the interpolation routine first finds for each weather station the wind field that best matches the observed vector then the wind vectors extracted from the selected wind fields are averaged as before details of the interpolation of each variable can be found in the r package vignette daily potential solar radiation r pot is estimated from solar constant solar declination latitude aspect and slope according to garnier and ohmura 1968 instantaneous potential radiation r pot s is calculated using 5 t 1 sin φ cos h cos a sin z x sin h sin a sin z x cos φ cos h cos z x t 2 cos φ cos a cos z x sin φ cos z x r p o t s i 0 t 1 cos δ t 2 sin δ where ϕ is latitude h is the hour angle measured from solar noon a is aspect measured from north z x is the angle of slope δ is solar declination and i 0 is the solar constant daily potential radiation is obtained by integrating r pot s between sunrise and sunset incident solar radiation r g the amount of solar radiation reaching a surface after accounting for the atmosphere is then calculated as thornton and running 1999 6 r g r p o t t t m a x t f m a x where t t max is the daily average i e between sunrise and sunset maximum cloud free total transmittance and t f max is the proportion of t t max realized on a given day i e cloud correction t t max and t f max are calculated using the interpolated estimates of temperature relative humidity and precipitation thornton and running 1999 the package provides additional functions to divide daily incident radiation into sub daily instantaneous estimates of direct and diffuse radiation spitters et al 1986 2 3 statistical correction generally speaking the statistical correction of a weather data series using a more accurate series is necessary when the more accurate series does not cover the period of interest and the less accurate one does in principle both series can be at the same spatial scale but most often the less accurate series will be at broader scale when used for downscaling coarse scale reanalysis data statistical corrections can be broadly referred to as empirical statistical downscaling benestad et al 2008 when applied to post processing climate model outputs this kind of downscaling usually implies an attempt to correct model biases and is often termed empirical statistical downscaling and error correction themeβl et al 2012 although there are many ways to perform statistical corrections meteoland currently offers three options unbiasing consists in subtracting from the series to be corrected the average difference between the two series for the reference period déqué 2007 let x i be the value of the variable of the more accurate e g local series for a given day i and u i the corresponding value for the less accurate series e g climate model output the bias θ is the average difference over all n days of the reference period 7 θ i n u i x i n i n u i n i n x i n the bias calculated in the reference period is then subtracted from the value of u for any day of the period of interest scaling a slope is calculated by regressing u on x through the origin i e zero intercept using data of the reference period the slope can then be used as scaling factor to multiply the values of u for any day of the period of interest this method should be preferred to unbiasing for non negative variables e g wind speed empirical quantile mapping due to its distributional properties neither multiplicative or additive factors are appropriate for daily precipitation gudmundsson et al 2012 ruffault et al 2014 in this case it has been recommended to compare the empirical cumulative distribution function cdf of the two series for the reference period gudmundsson et al 2012 themeβl et al 2011 the empirical cdfs of x and u for the reference period are approximated using tables of empirical percentiles and this mapping is used to correct values of u for the period of interest 8 c d e c d f x 1 e c d f u u d where ecdf x and ecdf u are the empirical cdfs of x and u respectively values between the percentiles are approximated using linear interpolation meteoland performs quantile mapping by calling functions from r package qmap gudmundsson 2016 for each target location to be processed the correction routine first determines which is the nearest climate model cell and extracts its weather data series for the reference period and the period of interest statistical corrections are done for each of the twelve months separately to account for seasonal variation of distributional differences ruffault et al 2014 although users can choose their preferred correction method for each variable meteoland has default approaches the unbiasing method is used to correct values of mean temperature and radiation whereas precipitation values are by default corrected using empirical quantile mapping including a correction for the frequency of wet days to correct minimum respectively maximum temperature values scaling is applied to the difference between minimum resp maximum temperature and mean temperature scaling is also applied to wind speed mean relative humidity is first transformed to specific humidity the unbiasing method is applied to this variable and the result is back transformed to mean minimum and maximum relative humidity using the previously corrected series of mean maximum and minimum temperature respectively 3 examples of application 3 1 calibration and validation of weather interpolation in catalonia we compiled year 2001 meteorology data of 506 stations within and around the region of catalonia ne of spain fig 2 from the catalan meteorological service and the spanish meteorological agency tomas burguera et al 2016 interpolation parameters α and n were calibrated for minimum temperature maximum temperature dew point temperature and precipitation using function interpolation calibration we determined for each variable the pair of parameter values corresponding to a minimum mean absolute error mae for minimum and maximum temperature mae was smaller when both n and α increased simultaneously and optimum values were n opt tmax n opt tmin 60 and α opt tmax α opt tmin 9 5 table a1 and fig a1 in appendix a optimum interpolation of dew point temperature implied weights estimated using fewer points n opt tdew 10 α opt tdew 2 5 we calibrated interpolation parameters for precipitation in two steps using cost functions that accounted for errors in the spatial and temporal patterns first we determined the parameter combination for interpolation occurrence weights resulting in a minimum value of the geometric average between the mae in the frequency of rainy days and the mae in the frequency of rainy stations optimum values corresponded to n opt pocc 5 and α opt pocc 10 meaning that the occurrence of precipitation in a small set of stations close to the target point was often sufficient to predict precipitation events given these optimum parameters for occurrence we then determined the parameter combination for interpolation precipitation amount weights resulting in a minimum value of the geometric average between the mae in the average daily precipitation and the mae in the average station precipitation the precipitation patterns of a larger area n opt pam 60 and α opt pam 1 were needed to ensure the reliability of regression coefficients fig a2 using the optimum interpolation parameters and function interpolation cv we conducted an evaluation of the prediction errors for all meteorological variables except wind by leave one out cross validation as performance statistics we used mae bias and pearson s correlation coefficient maes for daily predicted vs observed minimum and maximum temperatures were 1 5 c and 1 4 c respectively bias for minimum temperature was negligible 0 00029 c but a small positive bias was found for maximum daily temperature 0 12 c observed and predicted distributions of both variables were very similar except at the extremes fig a3 predictions of temperature range had slightly higher errors and the predicted distribution underrepresented extreme values table 2 fig a3 of all station days with precipitation data the interpolation procedure predicted 19 6 wet days very close to observed 19 5 wet days success rate for occurrence of precipitation was 87 4 92 1 for dry days and 68 0 for wet days mae for annual precipitation across stations was 67 7 mm with errors being larger for stations with larger annual precipitation fig a4 observed and predicted distributions of precipitation values were very similar for values up to 150 mm and the predicted frequency of daily wet stations closely matched the observed frequency fig a4 mae for relative humidity was 6 9 and bias was 0 04 with predicted distribution underrepresenting low humidity values fig a5 cross validation maes for radiation was 2 5 mj m 2 and bias was 0 02 mj m 2 with very low and very high radiation values being slightly underrepresented fig a 6 3 2 weather interpolation over solsonès county we used function interpolationpixels to interpolate daily meteorology for year 2001 and over 100 073 cells of 1 ha covering solsonès 1001 km2 a county in the central part of catalonia fig 1 the county has complex topography including an elevation gradient from south to north fig 3 first row using a buffer of 30 km around the county 63 weather stations had temperature data 79 stations had precipitation data 29 stations had relative humidity data and only 19 stations had wind speed data fig 3 shows the result of estimating meteorology for a particular day 2001 03 01 second and third row and the 2001 annual statistics last row influence of elevation was clear on interpolated daily patterns for temperature and relative humidity estimated radiation was strongly determined by aspect and slope but was also influenced by precipitation events with no precipitation areas receiving more radiation wind speed had the simplest spatial structure because wind data was scarce after averaging daily maps mean annual temperature mat and mean annual precipitation map also reflected topographic influences although tessellation of rainfall patterns aroused following the location of stations 3 3 downscaling correcting climate model predictions for the solsonès county we now illustrate the usage of meteoland for downscaling and correcting climate model predictions over solsonès county daily weather predictions of the cnrm cerfacs cnrm cm5 global model downscaled to europe at 11 km resolution by the cclm4 8 17 regional climate model rcm were obtained for the historical period 1976 2005 and the projection period 2006 2100 under representative concentration pathway rcp 8 5 from the eu cordex project kotlarski et al 2014 available at the earth system grid federation http esgf llnl gov we used the function extractnetcdf to subset the rcm predictions corresponding to the solsonès county to obtain reference historical series on each of 998 1 km grid cells over solsonès we used either a era interim daily re analysis data 1979 2005 at 0 125 resolution original grid at 0 5 resolution dee et al 2011 available at https software ecmwf int or b interpolated weather data 1976 2005 at 1 km resolution obtained using interpolationpoints and weather station data from the previous examples we used function correctionpoints errors with interpolated data as reference to evaluate the errors of rcm predictions before and after statistical correction before correction there was a negative bias in mean temperature and radiation and a positive bias in mean relative humidity fig 4 correction methods were kept to variable defaults and errors after correction were determined by cross validation i e by removing 1 month at a time from the historical period correction of rcm predictions nearly eliminated biases in the previous three variables but small negative biases remained in relative humidity for high elevation areas fig 4 mae values were also substantially lower after correction being residual mae larger at high elevations before correction mean annual precipitation map predicted by the rcm was substantially larger than observed average 280 mm and the proportion of days with precipitation were also over predicted average 55 probably caused by drizzle effect fig 4 errors in map and in the proportion of days with precipitation were strongly reduced after correction we used function correctionpoints to correct rcm projections 2006 2100 on each 1 km grid cell using either reanalysis data or interpolated data as historical reference as before default correction methods were applied large differences existed in patterns of mean annual temperature mat between rcm outputs reanalysis data and interpolated data for the historical period and these differences persisted after correction for the projected period fig 5 a however temperature correction did not substantially alter the 4 c mat increase predicted for rcp 8 5 this was not the case for mean annual precipitation map not only the uncorrected rcm outputs generally overestimated map compared to reanalysis and interpolated data but also the projected decrease in map was milder after corrections an average 160 mm reduction for uncorrected rcm data vs 107 mm and 116 mm for correction with reanalysis data and interpolated data respectively fig 5b this difference in the predicted map trend is the result of the non linear transformation of trends obtained by empirical quantile mapping maraun et al 2017b 3 4 evaluation of the effect of climate change corrections on forest drought we applied the water balance model of de cáceres et al 2015 to 573 forest plots in the solsonès county to illustrate how the predicted impacts of climate change on forest drought may be affected by the correction and downscaling of rcm outputs the purpose of the model is to predict daily variations in soil water content and assess drought stress for woody plants within forest stands forest plot data was obtained from the third spanish forest inventory villanueva 2004 for simplicity forest structure and composition was assumed constant during simulations and soil was assumed to be 1 m deep divided into two layers 0 30 cm 30 100 cm parameter values for species e g leaf area canopy storage capacity light extinction coefficient and soil attributes e g texture rock fragment content macroporosity were set as detailed in de cáceres et al 2015 for each forest plot we simulated daily water balance for two 20 yr periods within the 21st century 2006 2025 and 2081 2100 and using three meteorological inputs a the uncorrected climate series of the corresponding rcm cell b the corrected climate series obtained using correctionpoints and era interim data 0 125 resolution as reference and c the corrected climate series obtained using correctionpoints and interpolated weather station data 1 km resolution as reference for all plants in a forest plot the water balance model produces daily drought stress values scaled between 0 and 1 and calculated as the one complement of whole plant relative conductance as indication of annual drought duration in each plot we calculated the maximum number of consecutive days where the average drought stress of the plot was larger than 0 5 de cáceres et al 2015 mouillot et al 2002 values of annual drought duration were averaged for each plot simulated period and meteorological input regardless of the simulated period the number of drought days ndd was often small when rcm projections were not corrected fig 6 while this result contrasts with the strong reduction in map predicted by uncorrected rcm projections fig 5b it may be explained by historical biases of uncorrected rcm data positive bias in map and relative humidity negative bias in radiation and temperature fig 4 these biases led the water balance model to unrealistically underestimate drought stress in the target area for both periods corrections conducted using reanalysis data led to a much higher frequency of drought days compared to the uncorrected data fig 6 nevertheless they were still not satisfactory because of the strong water deficits predicted in northwest mountainous areas under current and future conditions resulting from a poor representation of altitudinal precipitation gradients fig 5b when applying corrections with fine resolution interpolated data increases in ndd between the two simulated periods 2006 2025 and 2081 2100 were found to be largest in plots of the western areas southern areas did not obtain such increases despite having a drier climate because of plots having low values of leaf area fig 6 northwest mountain forests were not predicted to be strongly affected by drought impacts despite their high leaf area and the predicted decrease in precipitation indicating that precipitation by the end of the century was still sufficient to avoid forest drought stress 4 remarks limitations and potential improvements in our opinion the greatest value of the package presented here is that it offers an integrated and easy to use platform to perform the full chain of tasks required to obtain consistent daily weather data at fine spatial resolution for current and future conditions e g fig 1 with the aim to reduce the burden on the user side the package offers a limited set of statistical options with default parameterizations while we initially designed meteoland to provide suitable inputs for modelling climate change impacts on forests we believe it may be useful for any kind of research studies requiring fine grained climatic data e g agronomy hydrology ecology forestry calibration and especially validation of interpolation routines should be conducted for each weather station data set and target period similarly an evaluation of residual errors is recommended for the historical simulations of every climate model before using the package to correct its projections but see maraun et al 2017b despite its advantages we are aware that the algorithms currently implemented in meteoland may not be optimal for all situations and future work should be directed to improve current methods or even complement them with others as the philosophy of meteoland is to discharge the end user from performing extensive literature reviews and comparisons of available methods we plan on undergoing these tasks ourselves and when necessary implement new routines or wrappers to call functions of other packages in the case of bias correction the largest disadvantage of the methods currently offered is that they ignore the dependency between weather variables although this issue may be less critical when correcting high resolution climate model outputs hempel et al 2013 moreover the advantages disadvantages of alternative forms of bias correction including the role of cross validation are still being discussed ehret et al 2012 gobiet et al 2015 maraun et al 2017b themeβl et al 2012 while climatologists usually prefer having as many options as possible for method comparison and testing we envision meteoland as a platform where an informed subset of all the available methods is offered to the climate driven impact researcher this implies that the development of the package should involve collaborative work between climatologists and climate impact researchers who should be guided to fast and robust methods but not necessarily the most accurate or sophisticated ones the package should offer several choices only when the alternative to be advised depends on factors that the end user is able to easily identify besides revising the methods currently offered for interpolation and statistical correction other aspects of the package could be improved to facilitate climate driven impact studies the calculation of insolation could be improved by taking into account shading from nearby topographic elements for example by calling functions of package rsaga brenning 2008 the data structures of meteoland have been designed to store daily data but they could be generalized to store data at other temporal scales e g sub daily the package currently offers the possibility to access public weather data from the spanish national meteorological agency but similar routines could be added to access observations forecasts or reanalysis data from other providers if the necessary programming interfaces are available iannone 2015 the package could also be improved by incorporating weather generation tools cordano and eccel 2016 which would allow considering the uncertainty in the daily series obtained after downscaling correction of climate model outputs finally although the package is already quite fast most routines are implemented in c computation speed could be increased through parallelization author s contributions mdc conceived the design of the package and implemented most functions nms vg and ac contributed their code to specific functions and package features all authors participated in the design and interpretation of illustrative examples and contributed to manuscript writing software availability package meteoland is freely available at cran https cran r project org package meteoland and detailed information on how to use it can be found at the package website http vegmod ctfc cat meteolandweb development versions can be found at http github com miquelcaceres meteoland acknowledgements we thank the agencia estatal de meteorología aemet and servei meteorològic de catalunya smc for providing daily weather station data authors are grateful to marc stéphanon sergio vicente serrano and three anonymous reviewers for insightful comments on previous versions of this manuscript this work was supported by the era net foresterra project informed pcin 2014 050 by project cgl2014 59742 c2 2 r spanish ministry of economy and competitiveness by a spanish ramon y cajal fellowship to m d c ryc 2012 11109 and by a spanish juan de la cierva fellowship to m t ijci 2015 26953 appendix a supplementary data the following are the supplementary data related to this article decaceres et al ems decaceres et al ems data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 08 003 
26354,land use change models generally include neighbourhood rules to capture the spatial dynamics between different land uses that drive land use changes introducing many parameters that require calibration we present a process specific semi automatic method for calibrating neighbourhood rules that utilises discursive knowledge and empirical analysis to reduce the complexity of the calibration problem and efficiently calibrates the remaining interactions with consideration of locational agreement and landscape pattern structure objectives the approach and software for implementing it are tested on four case studies of major european cities with different physical characteristics and rates of urban growth exploring preferences for different objectives the approach outperformed benchmark models for both calibration and validation when a balanced objective preference was used this research demonstrates the utility of process specific calibration methods and highlights how process knowledge can be integrated with automatic calibration to make it more efficient keywords cellular automata land use model calibration complexity reduction semi automatic calibration automatic parameter tuning software availability name of software empirical neighbourhood rule calibrator enc developer charles p newland contact address the university of adelaide adelaide sa 5005 contact email charles p newland adelaide edu au software requirements current implementation is linked with metronamica land use model http www metronamica nl through manipulation of xml input files and calls to a command line interface use with metronamica is recommended although code may be adjusted for linking with other land use models first year available 2017 program language python 3 program size 20 mb availability and cost open source software downloadable from https github com ursidean enc py3 release git 1 introduction land use change models are used to understand the wide ranging impacts of land use changes including the impact on the rate of greenhouse gas emissions li et al 2017 pogson et al 2016 the balance of agricultural production with ecosystem preservation connor et al 2015 van delden et al 2010 and the influence of land use policy on urban growth berberoglu et al 2016 chaudhuri and clarke 2013 land use changes are caused by many different mutually influential bio physical and socio economic drivers lambin et al 2001 wang et al 2011 which must be captured effectively by land use change models to generate realistic output to model land use changes effectively cellular automata ca were proposed to replicate land use as a dynamic spatial system tobler 1979 this required relaxing the conventional implementation of ca couclelis 1985 to replicate fractal patterns of land use changes consistent with urban evolution white and engelen 1993a doing so led to the development of multiple land use cellular automata luca models which have proliferated due to their simplicity flexibility and intuitiveness santé et al 2010 an important aspect of modelling land use changes is the consideration of spatial and temporal dynamics between different land uses van vliet et al 2013b applications of ca to land use modelling consider the composition of the neighbourhood a strict ca only considers the neighbourhood composition of the geometrically closest set of cells for the consideration of spatial dynamics implemented in numerous luca models li et al 2013 wu 2002 including the popular generic modelling framework sleuth clarke et al 1997 silva and clarke 2002 however other luca models most notably those derived from white and engelen 1993b that use a transition potential as the mechanism for the allocation of land use changes use an extended neighbourhood i e the set of cells within a certain cellular radius for a more detailed consideration of spatial dynamics which requires the use of neighbourhood rules that characterise the influence different land use classes exert on each other relative to proximity while the use of neighbourhood rules allows for a more detailed replication of the spatial dynamics that exist between different land use classes it also increases the number of model parameters significantly often by hundreds blecic et al 2015 garcía et al 2013 luca model application requires appropriate values of these parameters to be determined by means of calibration which involves the initial setting of parameter values the iterative adjustment of these values based on comparison of the model output with observations and the selection of a final parameter set for application to a specific case study for long term scenario analysis newland et al 2018 the difficulty of the calibration processes is generally a function of the dimensionality of the parameter space which is a function of the number and possible ranges parameters can take during the iterative adjustment process neighbourhood rules are the primary contributor to this high parameter dimensionality and hence are often the main calibration parameters of these types of luca models engelen and white 2008 the conventional method of calibration is to manually adjust the neighbourhood rule parameters garcía et al 2013 van delden et al 2012 which is a knowledge and time intensive process given the complexity of the calibration problem the parameter dimensionality is often implicitly reduced when using such an approach based on the modeller s knowledge of the spatial dynamics driving land use changes in a region white et al 1997 and the common forms of neighbourhood rules hagoort et al 2008 additionally metrics can be used to objectively evaluate calibration performance for luca models there are two distinct types of metrics that measure different aspects of calibration performance newland et al 2018 locational agreement metrics the match of pixels between simulated output and data and landscape pattern structure metrics the difference between simulated and observed patterns of land use that infer the realism of land use change processes objective measures of luca model performance have facilitated the development of automatic calibration methods where parameters are iteratively adjusted automatically to improve luca model performance as quantified by the metrics used there are two general approaches to automatically calibrate neighbourhood rules the first approach uses a population based optimisation algorithm blecic et al 2015 garcía et al 2013 newland et al 2018 where a population of solutions is generated i e a number of different sets of neighbourhood rules and adjusted based on some operators to improve the objective performance of the solutions over a number of iterations optimisation approaches are effective at generating multiple possible model parameterisations but are computationally intensive often requiring parallel computing resources for practical implementation blecic et al 2015 newland et al 2018 such approaches also lack transparency which can result in parameters that conflict with process understanding despite producing a model with objectively good performance the alternative automatic calibration approach is efficiency focussed targeted to the spatial dynamic processes in the transition potential model aiming to generate a single set of calibrated neighbourhood rules that is consistent with discursive knowledge for a limited computational budget straatman et al 2004 maas et al 2005 van vliet et al 2013b achievable by a desktop pc as opposed to a supercomputer given the limited availability of supercomputing resources such approaches designated as process specific are valuable as a practical means of automatic calibration despite a specific focus on neighbourhood rules previous process specific methods have not fully utilised discursive knowledge to generate neighbourhood rules consistent with process knowledge van vliet et al 2013b and do not necessarily focus on the most important spatial interactions during calibration straatman et al 2004 previous methods have also only used a single metric of performance not considering the implementation of multiple metrics to capture the two aspects of calibration performance previously discussed and how the competing objectives locational agreement and landscape pattern structure impact on the resulting model performance given the shortcomings of current process specific calibration methods outlined above while maintaining their relative computational efficiency and transparency compared with population based optimisation algorithms this research proposes a semi automatic process specific calibration method the aims of this paper are i to develop a calibration method that utilises process knowledge about meaningful interactions to facilitate efficient automatic calibration that allows for the consideration of both aspects of luca model performance for a limited computational budget ii to investigate the utility of the proposed approach through application to several case studies and iii to obtain insight into the choice of objectives and the impact of the preference on the resultant calibrated model the remainder of this paper is organised as follows to illustrate the complexity of the calibration problem background information about neighbourhood rules is presented in section 2 section 3 presents the proposed approach for calibration and section 4 presents the implementation and computational testing regime used to evaluate it the results are presented and discussed in section 5 and the conclusions of this work are presented in section 6 2 complexity of the calibration problem this section provides explicit details of the complexity of calibrating neighbourhood rules by defining the dimensionality of the neighbourhood rule parameter space for transition potential luca models to do this consider the transition potential land use allocation mechanism illustrated in fig 1 which is described in detail below a transition potential based luca model can be defined as a dynamic process providing a sequence of land use maps x 1 x 2 x t given an initial land use map x o each land use map is defined specifically as the set of land use classes for each cell 1 x t x c t a c c where x c t is the land use class of cell c at time t a is the set of all land use classes and c is the set of all cells for most applications the set a can be partitioned as 2 a a a a p a s where a a is the set of active land use classes i e the classes that are actively allocated by the model generally urban classes such as residential and commercial land use a p is the set of passive classes i e classes that have low transition costs such as natural areas and are allocated after the actively modelled classes and a s is the set of static classes i e classes that occupy a fixed location throughout the simulation such as water bodies or airports the set c can be partitioned following the same convention 3 c c a t c p t c s where c a t is the set of cells containing active land use classes at time t c p t is the set of cells containing passive land use classes at time t and c s is the set of cells containing static classes which do not vary with time the evolution dynamics of the land use is given by the functional relationship 4 x t 1 f θ t n x t v t δ where θ t are parametric maps governing the effect of processes such as soil quality and zoning that influence land use changes which are independent of the land use map n x t is the set of neighbourhood influence values that is dependent on the current state of the land use map v t is a stochastic perturbation term included to capture the system uncertainty and δ is the land use demand assuming the luca model is constrained for each active land use class each of θ t n x t and v t are given for each cell in the land use map for each actively modelled land use class for each time step t i e each year the functional relationship in equation 4 describes how land use is allocated to every cell in the map based on the transition potential which is the potential for each specific cell to support each type of active or passive land use class for each active class the cells with the highest transition potential are selected until the demand is met after which passive land uses are allocated to the remaining cells as these have no specified demand a general form for determining transition potential is 5 t p c i t θ c i t n c i t v t where tp c i t is the potential for cell c to support land use i at time t and n c i t is the neighbourhood potential for cell c to support land use i at time t which is a function of the composition of the cellular neighbourhood of c with each cell in the neighbourhood exerting some influence based on its class and distance from c this has the general form 6 n c i t c d c w i x c t d c c where d c is the set of cells in the neighbourhood of cell c d c c is the linear distance between the cells c and c and w i j d is a neighbourhood weighting parameter that expresses the influence that a cell of land use type j exerts on the potential for land use class i at a linear distance d between the two cells note that in equation 6 the subscripts of the general form of the weight term w i j d are given by j x c t and d d c c neighbourhood rules define the weight values capturing distinct aspects of spatial dynamics based on the type of relationship being described and the distance being considered van vliet et al 2013b this is illustrated in fig 2 showing how weights are defined at different distances for different spatial dynamics it is important to note that the weight values have no physical meaning but derive meaning relative to each other in total the set of neighbourhood interactions of class j on class i at distance d symbolised by i j d are defined by the set 7 p i j d i a a j a d 0 1 d m a x where p is the set of all interactions a a is the set of actively allocated land use classes a is the set of all land use classes and d max is the maximum distance that defines the size of the neighbourhood interactions can be categorised as either self influencing when i and j are equal e g describing the influence of a residential class to attract more residential development or across class interactions when i and j are unequal e g describing the influence of a residential class to attract industrial development all interactions can also be described as either a point influence at distance zero describing the influence of a cell on itself or a neighbourhood tail influence for distances of one or greater describing the remote influence of a class on a cell given these categorisations there are four distinct types of spatial dynamics examples of which are given in fig 2 which are captured by neighbourhood interactions van vliet et al 2013b outlined below 1 inertia points i i 0 p i a a these describe the persistence of a specific land use class i to remain in its present location 2 conversion points i j 0 p i a a j a j i these describe the potential for a transition from one land use class j to a different land use class i at its present location 3 self influence tails i i d p i a a d 1 these describe the influence a land use class i exerts on the same type of land use i that is in the neighbourhood of a cell and 4 cross influence tails i j d p i a a j a j i d 1 these describe the influence a land use class j exerts on a different type of land use i that is in the neighbourhood of a cell due to the four distinct types of interaction the full set of interactions can be partitioned as 8 p p i p p c p p s t p c t where p ip is the set of inertia point interactions p cp is the set of conversion point interactions p st is the set of self influence tail interactions and p ct is the set of cross influence tail interactions the total number of interactions considered is generally given by 9 p a a a each tail interaction can be described by the weight at distance 1 multiplied by a function that describes the decaying neighbourhood tail interaction that is 10 w i j d w i j 1 u d f o r d 1 where u d is the functional form of the neighbourhood rule tail shape the tail shape u d is a pre specified function e g linear exponential starting at u 1 and decaying as d increases resulting in a diminishing effect of a land use class on another with increasing distance as each neighbourhood influence can feature four parameters as shown in fig 2 e g a point weight at distance 0 and a piecewise linear relationship of tail weights for distance 1 a point of inflection and a point where influence is set to zero and as luca models can typically have 20 land use classes where 10 are actively modelled there are minimally 800 parameters that require calibration hence the dimensionality of the neighbourhood rule parameter space is typically very high it is for this reason that neighbourhood rules are the main focus of the calibration of luca models engelen and white 2008 and why the calibration of neighbourhood rules is complex the high dimensionality makes it difficult to know which parameters to adjust to achieve an improved model calibration this problem is further exacerbated by the limited calibration data available as typically there exists only an initial data land use map x ˆ 0 and one or two other data land use maps x ˆ a and x ˆ b for some a and b 1 the high parametric dimensionality also introduces potential issues with equifinality van vliet et al 2016 where the same calibration performance can be achieved by different sets of neighbourhood rules despite the rules not necessarily being consistent with process knowledge 3 proposed approach a conceptual outline of the proposed approach for the semi automatic calibration of neighbourhood rules is shown in fig 3 the approach has been developed with two primary aims the first is the reduction of the dimensionality of the neighbourhood rule parameter space to mitigate the parametric dimensionality issues outlined in section 2 by identifying the key land use interactions for consideration symbolised by the set p this is based on automating and formalising the process of interaction elimination see section 3 1 that is common in manual calibration methods the proposed approach is conducted in an objective transparent and repeatable manner and also allows for the manual inclusion or elimination of interactions the second aim is the computationally efficient calibration of the remaining neighbourhood rule weighting parameters an initial set of neighbourhood rule weighting parameters w initial is generated during the parameter categorisation stage see section 3 2 by introducing a set of meta parameters θ cp θ st and θ ct which express the inter type importance of each interaction type relative to the inertia point interactions e g how important conversion points are compared to inertia points and by limiting the parameters possible values to a finite set of possible categorised values allocating each parameter included for calibration within each interaction group to a category value based on an empirical analysis the neighbourhood rule weighting parameters are subsequently refined in the coarse adjustment stage see section 3 3 to generate w coarse by tuning the meta parameters that control the categorised neighbourhood weighting parameters next during the fine adjustment stage see section 3 4 the neighbourhood rule weighting parameters are individually tuned to generate a set of calibrated weighting parameters w final advantages of the proposed approach are that i calibration is performed efficiently with a minimal number of model simulations i e a computational budget achievable by a desktop pc as opposed to a supercomputer ii the two key aspects of luca model performance are considered i e locational agreement and landscape pattern structure iii the neighbourhood rules generated at the conclusion of each stage are consistent with process understanding and iv manual intervention is possible to include or exclude interactions and to set neighbourhood weighting parameters at any point the remainder of this section presents the details of each stage 3 1 interaction elimination 3 1 1 overview the interaction elimination stage reduces parameter dimensionality by eliminating certain interactions to simplify the subsequent calibration problem reducing the total set p to the smaller set p of important interactions that are identified to be driving land use changes in the region fig 3 the interaction elimination is based on an analysis of the available data making the process objective and repeatable the primary objective of this stage is to determine the set of interactions p composed of 11 p p i p p c p p s t p c t where the subsets p i p p c p p s t and p c t contain only the meaningful neighbourhood interactions typically all inertia point and self influence tail interactions are considered meaningful due to all land uses exhibiting some tendency for persistence and some tendency for self influence in the neighbourhood this means that p ip is equal to p ip and p st is equal to p st and that the reductions in p are achieved in the reduction of conversion point and cross influence tail interactions p cp and p ct the interaction elimination process is summarised in fig 4 where an empirical analysis and significance testing are used for identifying the meaningful interactions in p cp and p ct this is based on an empirical evaluation of the transitions that occur in the calibration data x ˆ 0 and x ˆ a as these data provide the main behaviour the land use model is attempting to capture van vliet et al 2013b the empirical analysis and significance testing are conducted to determine if there is a statistically significant representation of a given land use class for transitions to each active land use class this analysis is undertaken either at the location of the transitioned cells informing the inclusion of a conversion point or in the neighbourhood of the transitioned cells informing the inclusion of a cross influence tail hence the meaningful interactions are such that 12 p c p i j 0 p c p t 0 i j 0 x ˆ t 0 and z 0 i j 0 x ˆ z 0 p c t i j 1 p c t t 1 i j 1 x ˆ t 1 and z 1 i j 0 x ˆ z 1 where t d i j d x ˆ is the empirically derived measure for the representation of land use class j in the neighbourhood of cells at distance d that transitioned to class i see fig 4 and t d is the associated threshold value z d i j d x ˆ is the measure of statistical significance for the representation of land use class j in the neighbourhood of cells at distance d that transitioned to class i and z d is the associated significance level for the interaction elimination the user specifies an empirical analysis threshold for conversion points t 0 and cross influence tails t 1 and a significance threshold for conversion points z 0 and cross influence tails z 1 empirical analysis section 3 1 2 is used to determine the values of t 0 and t 1 and significance testing section 3 1 3 to determine the values of z 0 and z 1 interactions that do not exceed both thresholds i e t 0 and z 0 for a conversion point or t 1 and z 1 for a cross influence tail are eliminated from subsequent calibration the proposed interaction elimination is designed to extract the meaningful interactions based on the available data which is likely limited to a pair of data maps the method is inherently limited by the available data and can only capture what the available data indicate i e intermediate transitions cannot be considered if there are no data to show these which must be considered during application as this has the potential to impact calibration performance blecic et al 2015 the user must therefore consider the expected amount of change over the period considered when setting the thresholds the empirical analysis measures and significance test used for the elimination are described in the next two sections it should be noted that the elimination uses measures to capture the independent influence of individual classes on an active class as this is consistent with the model structure i e neighbourhood rules only capture individual influence of one land use on another so for example because the model uses a neighbourhood rule describing the influence of land use class j on the allocation of class i a measure relating the presence of land use class j in the neighbourhood of class i is used for evaluation higher order combined spatial influences are not considered as these are not sufficiently captured by the transition potential model with such effects only accounted for by the summation of the individual influences mentioned see equation 6 the proposed interaction elimination is conducted based on the available data but given the potential limitations of the data the user has the ability to either include rules that are not identified as statistically significant or to reject rules that are identified as significant but inconsistent with process understanding 3 1 2 empirical analysis measures the evaluation of conversion points is based on a form of the confusion matrix congalton 1991 referred to as a contingency table see table 1 for an example generated by logging the land use class transitions for each cell for x ˆ 0 to x ˆ a see fig 4 in table 1 η i j is the total number of cells that are class i in x ˆ 0 and have transitioned to class j in x ˆ a the value η i i the case where i is equal to j shows the total number of cells that did not change land use class between x ˆ 0 and x ˆ a which provides information about the level of inertia using the contingency table the inertia rate which quantifies the tendency of a land use class to persist can be derived as 13 i r i η i i m 1 n η i m using the contingency table the conversion rate which quantifies the tendency of transitions to a land use class as a function of all transitions to that class can also be derived as follows 14 c r i j η i j m 1 n η m j η i i for the proposed approach the threshold value for the inclusion of a conversion point t 0 is a user defined conversion rate the corresponding empirical measure t 0 for each conversion point is calculated from the contingency table the evaluation of cross influence tails is performed using the enrichment factor of transitioning cells van vliet et al 2013b which expresses the over or under representation of a particular land use class in the neighbourhood of cells that transitioned to a certain class at a certain distance relative to the representation of the neighbourhood class in the entire landscape fig 4 15 e f i j d l o g 10 r i j d n j n where e f i j d is the enrichment factor for the presence of land use class j at distance d in the neighbourhood of cells that transitioned to class i r i j d is the average relative representation of land use class j in the neighbourhood at distance d of cells that transitioned to land use class i n j is the total number of cells of land use class j in the data map x ˆ 0 and n is the total number of cells in the data map x ˆ 0 to assist with interpretation the enrichment factor is log scaled so that values greater than 0 indicate over representation and values less than 0 indicate under representation the threshold function t 1 for the inclusion of a cross influence tail is a user defined log scaled enrichment factor value the corresponding empirical measure t 1 for each cross influence rule is the calculated log scaled enrichment factor it should be noted that the information from the contingency table specifically the conversion rate is used for the analysis of conversion points as opposed to the enrichment factor values at distance zero because the contingency table is more effective at capturing the different conversions in the data though the two measures use the same information the enrichment factor at distance zero can be derived from the information in the contingency table see appendix a the enrichment factor can be less effective at capturing the conversions occurring for example many conversions may be occurring from a certain class to another but the enrichment factor may not suggest over representation because there is a large representation of the class in the landscape large n i erroneously indicating that a conversion point is not required because there is a large representation of the class in the landscape a common example of this are conversions from large relatively passive land use classes such as natural vegetation because they occupy a large area of the landscape and facilitate many conversions to different classes which are meaningful to include 3 1 3 significance test the mann whitney u test mwu test is applied to determine whether the representation of land use class j in transitions to land use class i is statistically significant fig 4 the mwu test requires compiling ranked data sets shown in fig 5 for the neighbourhood at distance one for i the percentage of cells of land use class j in the neighbourhood at distance d for cells transitioning to class i compared to ii the percentage of cells of land use class j at distance d for all cells the mwu test is used to assess whether there is significant variation for data sets i and ii this is formally stated as testing whether a sample drawn from one distribution will be equally likely to be greater or less than a sample drawn from the other this is the null hypothesis h 0 where the alternative hypothesis h a is that the first aforementioned sample will be more likely to be either greater or less than the second for the proposed approach h 0 is that the ranks of the respective relative composition tallies for the two data sets i and ii are statistically equivalent this is shown in fig 5 for case a as the distributions appear fairly similar alternatively h a is that the ranks of the respective relative composition tallies for the two data sets are systematically higher or lower this is shown in fig 5 for case b as the distributions for the transition data set possess a significantly larger percentage of higher cell counts i e 15 of transition cells have 8 cells of class j at distance d equal to 1 as compared to only 2 of all cells have this number at the same distance the mwu test requires the u test statistic to be calculated which requires the two distribution tallies to be combined and ranked from low to high corder and foreman 2014 with the ranked order determined the u test statistic is calculated for the ranks of the distribution of a specific land use class j in the neighbourhood at a distance d of cells that transitioned to land use class i 16 u i j d n i n n i n i 1 2 r s i j d where u i j d is the u test statistic for the presence of land use class j at a distance d for cells that transitioned to land use class i n i is the number of newly allocated cells of class i n is the total number of cells in the landscape and rs i j d is the sum of the ranks of the distribution of the relative composition for the presence of land use class j at a distance d for cells that transitioned to land use class i the u test statistic is also calculated for the ranks of the relative composition of a specific land use class j at a distance d for all cells 17 u c j d n i n n n 1 2 r s c j d where u c j d is the u test statistic for the presence of land use class j at a distance d of all cells c and rs c j d is the sum of the ranks of the distribution of the relative composition for the presence of land use class j at a distance d for all cells c the final u test statistic used is the minimum of the two that are calculated for this proposed application the samples are sufficiently large that a normal approximation can be used for the determination of significance which corresponds to z d in equation 12 18 z i j d min u i j d u c j d x u s u where z d is the z score approximation for the presence of land use class j at a distance d of cells that transitioned to land use class i x u is the mean of the data and s u is the standard deviation of the data the calculated z d must be greater than the user defined z d for the corresponding conversion point or cross influence tail to be included 3 2 parameter categorisation and initialisation the parameter categorisation and initialisation stage further reduces the dimensionality of the calibration problem to facilitate initialising the luca model with a set of neighbourhood rule weighting parameter values for subsequent coarse calibration fig 3 by using a method to efficiently generate neighbourhood rules that are consistent with process understanding the categorisation process achieves this across two steps discussed in detail below the first step is the introduction of a set of meta parameters θ c p 0 θ s t 0 and θ c t 0 which describe the inter type importance of each interaction type p c p p s t and p c t with respect to the inertia point interaction type p i p the meta parameters quantify how important each interaction type is relative to the inertia point interaction type which is assumed to be the dominant process e g the meta parameter θ c p expresses how important the p c p interactions are compared to the p i p interactions for example high values of θ c p mean conversion points exhibit similar influence to inertia points which will likely cause more conversions the meta parameters range from zero to one and are set by the user note that a value of 1 places the interactions at the same level of importance as the inertia point interactions recommended ranges are between 0 and 0 1 for all meta parameters the meta parameters are subsequently tuned in the coarse adjustment stage section 3 3 the second step is the categorisation of the parameters included for calibration within each interaction type this achieves a discretisation of the corresponding weighted influence values w i j d where rather than taking values from a continuum they are restricted to a set of finite values this categorisation establishes a hierarchy of the intra type importance of each interaction for each type of interaction e g how important interaction i j 0 is compared to i k 0 for both i j 0 i k 0 p c p categorisation is based on an empirical analysis of the available data a result of the proposed categorisation method is that the weighting parameters for the interactions p i p p c p p s t and p c t are given by the following representations 19 w i i 0 w i i 0 w i j 0 θ c p 0 w i j 0 w i i d θ s t 0 w i i 1 u d w i j d θ c t 0 w i j 1 u d where θ c p 0 θ s t 0 and θ c t 0 are the meta parameters that are user defined and take values between 0 and 1 and w i j d are the normalised weighting parameters that take values from the discretised parameter space w i j d 1 w i j d k where k is the number of discrete levels so for example if an interaction type has three levels low medium and high the weights take values from the set range w i j d 1 w i j d 2 w i j d 3 the functional dependence of the neighbourhood potential for the proposed meta parameterisation and categorisation is summarised in appendix b the parameter categorisation is based on an empirical evaluation of the available data a point or tail for each interaction type is assigned as follows 20 w i j d w i j d 1 if t k i j d x ˆ i k 1 w i j d 2 if i k 1 t k i j d x ˆ i k 2 w i j d k if i k k 1 t k i j d x ˆ where t k is the empirically determined threshold value for interaction type k i e ip cp st ct d is either 0 or 1 defining either a point or a tail and i k n is the upper threshold value for importance category n different for each interaction type where higher n means higher importance the categorisation of the different interaction types uses the measures previously derived in section 3 1 2 shown in table 2 the p i p and p c p interactions are categorised using measures derived from the contingency table the inertia rate and conversion rate respectively the p s t and p c t interactions are categorised using the enrichment factor values at distance d equal to one thresholds i k for the different groups are set by the user hence given that each interaction type uses the same number of levels the calibration complexity is reduced so that the luca model neighbourhood rules are characterised by three meta parameters and weighting parameter values for each discrete level across all interaction types the model can be initialised by specifying these values and the required thresholds to generate an initial parameter estimate w i n i t i a l 3 3 coarse parameter adjustment the coarse parameter adjustment stage calibrates the neighbourhood weighting parameters at a coarse level to improve luca model performance by tuning the meta parameters that control the categorised neighbourhood weighting parameters introduced in section 3 2 fig 3 the coarse parameter adjustment facilitates the initialisation of the fine parameter adjustment process at a starting point within the parameter space that results in objectively good performance and is consistent with process knowledge model performance is measured by comparing the simulated output with the data using at least two metrics as discussed in section 1 one to quantify locational agreement and another to quantify landscape pattern structure model performance is improved by generating a set of parameters w c p c o a r s e w s t c o a r s e and w c t c o a r s e through the coarse adjustment of the meta parameters θ c p θ s t and θ c t within this stage the parametric representation equation 19 is retained but the meta parameters are varied about the initial settings by using the neighbourhood weighting parameterisation from equation 19 the weights can be expressed as functions of the meta parameters such that 21 w c p w c p θ c p w s t w s t θ i t w c t w c t θ c t using this formulation the initial parameter set is given by 22 w c p 0 w c p θ c p 0 w s t 0 w s t θ s t 0 w c t 0 w c t θ c t 0 as outlined above the coarse level parameter adjustment is performed by adjusting the meta parameters to optimise the objectives of calibration performance formally the goal is to identify a set of meta parameters θ θ c p θ s t θ c t that result in a non dominated calibration performance i e improved performance in one objective cannot be achieved without detrimental performance in the other objective to the bi objective problem 23 max f 1 w θ x ˆ a min f 2 w θ x ˆ a where f 1 corresponds to the locational agreement objective between the calibration data map x ˆ a and the luca model simulation map x a which is a function of w θ and f 2 corresponds to the landscape pattern structure objective measured as the error between the calibration data map x ˆ a pattern and the luca model simulation map x a pattern which is a function of w θ as opposed to determining every possible combination of meta parameter values to identify those that optimise the objective an approximate and more computationally efficient approach is implemented as shown in fig 6 in this approach one meta parameter is sampled at a time within each stage allowing for an explicit consideration of the non dominated solutions subject to the variation of the single meta parameter under consideration within this process user input is required to select the best meta parameter value from the set of solutions this is an important step in the process as it allows for the user to make a judgement as to which meta parameter value is best based on additional subjective criteria e g user experience and or consideration of landscape features not characterised by the objective metrics the selected meta parameter value as shown in fig 6 is then used within the subsequent stages where the other meta parameters are sampled one at a time the final set of meta parameters θ is obtained at the conclusion of the coarse adjustment stage 3 4 fine parameter adjustment while the coarse parameter adjustment stage enables the whole parameter space to be explored to identify promising regions the values individual parameters can take is restricted to a relatively coarse grid as only values of the meta parameters are adjusted the purpose of the fine parameter adjustment stage is to remove this restriction to enable the values of individual parameters that maximise the objectives functions in the vicinity of those from the coarse adjustment stage to be identified fig 3 consequently the fine parameter adjustment stage is initialised using the output of the coarse adjustment stage as this corresponds to a good starting point in the parameter space that has objectively good performance and agreement with process understanding as with the coarse level adjustment locational agreement and landscape pattern structure metrics are used to objectively assess calibration performance as mentioned above the fine parameter adjustment stage considers the neighbourhood weighting parameters on an individual level rather than the meta parameters as was the case in the coarse parameter adjustment stage iteratively refining each to optimise the performance metrics used within the available number of iterations computational budget this is achieved using a line search algorithm as shown in fig 7 because this is an automatic search method that efficiently converges on a parameter value that locally optimises the objectives the specific neighbourhood weighting parameter w i j d corresponding to one of the four interactions groups is assumed to lie within a range for which a certain value optimises the performance metrics used the line search is conducted using the golden section search algorithm kiefer 1953 the over arching structure of the sequential line search algorithm is as follows the working parameter set w is initialised to the values obtained at the conclusion of the coarse calibration as shown then the algorithm loops through the interaction types in order of importance as identified by the user based on case study characteristics generally inertia points followed by self influence tails conversion points and cross influence tails for each type the algorithm sequentially loops through all weighting parameters searching between a set of user defined minimum and maximum neighbourhood weighting parameter values w i j d m i n and w i j d m a x to determine the refined value w i j d f i n e that maximises the specified objective f fine discussed below subject to all other parameters being held constant the working parameter set is updated when the refined variable value is obtained and the procedure continues through the loop until the computational budget is exhausted returning the final set w f i n a l composed of the refined neighbourhood weighting parameters the line search uses a single objective f fine to characterise the calibration performance of the proposed parameter set as discussed previously multiple objectives are required to characterise the performance of luca models so f fine is taken as the weighted sum of the objectives 24 f f i n e w x ˆ i 1 m a i f i w x ˆ where f i is the ith objective a i is the user defined weight given to the ith objective all weight values sum to one and m is the number of objectives as f fine is the sum over both locational agreement and landscape pattern structure error each metric must be transformed into a maximisation objective e g reverse the sign of the metric of minimisation objectives additionally to mitigate potential issues with variable ranges of the metrics the objectives are scaled to values between 0 and 1 using metric ranges as the normalising upper and lower bounds that are defined by the user it is important that the metrics used are appropriately balanced to achieve a trade off between the performance objectives to ensure robust calibration and to prevent over calibration the proposed approach assumes a model structure sufficiently general to capture the major land use change processes in a region this assumption allows for a focus on efficient calibration where all objectives can be combined using a single weighted sum however this does limit the ability to fully explore the trade off between the objectives which can obscure more fundamental modelling issues a notable example would be if a balance between locational agreement and landscape pattern structure cannot be easily achieved white et al 2015 where the calibrated model performs well in either metric but not both this is important to consider as it might suggest more fundamental structural model issues as the case where the luca model does not fully capture major land use change processes identifying and addressing such structural problems are not a primary focus of this work but this is an important issue consider that warrants future research 4 application of proposed approach the utility of the proposed semi automatic calibration method for neighbourhood rules is evaluated using the following computational testing regime the approach is applied to calibrate the neighbourhood rules of four case studies in europe comparing the output obtained based on the preference for different objectives and evaluating the performance against neutral models of landscape change that are used to generate benchmark metric values for calibration performance hagen zanker and lajoie 2008 4 1 land use model the luca model metronamica is used to model land use changes metronamica is a generic constrained ca modelling framework van delden and hurkens 2011 that has numerous global applications wickramasuriya et al 2009 rutledge et al 2008 van delden et al 2011 metronamica is derived from the transition potential model developed by white and engelen 1993b and hence includes neighbourhood rules in the determination of land use changes metronamica follows the convention for the calculation of transition potential equation 4 where the additional processes considered via parametric maps are given by 25 θ c i a c i s c i z c i where a c i is the influence of accessibility s c i is the influence of suitability and z c i is the influence of zoning the specific parameterisation of each component is given in the metronamica documentation riks 2015 4 2 case studies the four case studies used for testing include berlin germany budapest hungary lisbon portugal and madrid spain and their surrounding regions shown in fig 8 these case studies are selected to enable the approach to be tested under a range of different conditions given the variation in physical characteristics e g coastal or inland high or low elevation and rates of urban growth e g low for berlin moderate for budapest and high for lisbon and madrid see appendix c for contingency tables of these locations the corine land use data set haines young et al 2006 is used to generate the land use maps for the case studies each case study uses 15 land use classes eight of which are actively modelled which are reclassified from the 48 corine level 3 land use classes with a 250 m resolution covering a region of 10 000 km2 400 by 400 cells centrally located on the city centre the calibration period for each case study is 1990 2000 and the validation period is 2000 2006 each case study includes major road data for accessibility and elevation and slope data for suitability 4 3 implementation of proposed methodology for each of the case studies the empirical neighbourhood rule calibrator enc software developed as part of this work see software availability section for details is applied as each case study uses the same number of total and active land use classes each is implemented with the same number of possible neighbourhood interactions and possible neighbourhood weighting parameters 4 3 1 interaction elimination to conduct the interaction elimination stage the thresholds presented in table 3 are used for the empirical neighbourhood analysis only conversion points and cross influence tails are eliminated for this proposed implementation because all eight active classes for each case study are expected to exhibit inertia and self influence a conversion rate of 2 5 is used as a minimum threshold as this prevented infrequent and erroneous conversions present in the data such as conversions from fresh water to residential land use from being included in the subsequent calibration to demonstrate the most straight forward application possible the interaction elimination is applied to only find attractive cross influence tails and does not consider repulsive cross influence tails log scaled enrichment factor values at distance one that indicated over representation i e greater than 0 and hence hint at a possible attractive influence between different land uses are used for the empirical analysis of cross influence tails a minimum z limit of 1 96 the 95 confidence limit is used as the statistical significance level for both conversion points and cross influence tails the results of the parameter elimination are summarised in fig 9 which shows the reduction per case study for each interaction type as shown the implemented parameter elimination for the conversion points and cross influence tails resulted in a substantial reduction in the number of interactions considered for each case study the interactions that are included were generally consistent with process knowledge for example it is expected that attractive influences exist between the urban classes residential industry commerce and recreation areas given the limited number of urban classes however there are also examples such as conversion points from agricultural land uses to urban land uses which represent a case where the land use is desirable for urban land uses because it acts as a supply of vacant land such examples highlight the potential benefits of making the calibration procedure semi automatic as discursive knowledge can identify such parameters to remove them from subsequent calibration or ensure these influences are minimised by the calibration procedure 4 3 2 parameter categorisation to conduct the parameter categorisation stage three possible categories low medium and high are used for each case study this allowed for sufficient differentiation of weighting parameter values for each interaction type the threshold values for the categories used for each type of interaction are the same for each case study and are presented in table 4 interactions not exceeding the medium threshold are graded as low the values used are based on knowledge of the case studies and some trial and error analysis to ensure sufficient category variation across the case studies at this stage a neighbourhood shape function that describes the distance decay of the neighbourhood influence is used to aggregate self influence and cross influence tail neighbourhood weighting parameters to make the calibration as efficient as possible an aggregation strategy is used that aggregates tails to a set of key points white et al 1997 including an influence value at distance one an influence value at distance two that is ten percent of the influence at distance one and a point at distance five where influence is set to zero using this strategy each self influence and cross influence tail is represented by a single weighting parameter at distance one reducing the complexity of the calibration problem to generate the initial neighbourhood weighting parameters the inertia point parameters are set such that high inertia is assumed to have double the influence of medium inertia and medium inertia is assumed to have double the influence of low inertia using values of 1 000 500 and 250 for high medium and low inertia respectively the initial meta parameters used are presented in table 5 the values used are based on the case studies which are mostly characterised by persistence resulting in the highest values for the self influence tail meta parameter and lower values for the conversion point and cross influence tail meta parameters 4 3 3 coarse parameter adjustment the coarse parameter adjustment stage is conducted using the ranges and step sizes for each meta parameter given in table 5 the order of the meta parameter sampling is θ st θ cp and θ ct ordered by the impact i e value range on the output metrics obtained the ranges are determined based on discursive knowledge and some trial and error analysis with values outside the ranges given in table 5 resulting in poor performance in both objectives to evaluate locational agreement two variations of cohen s kappa are used fuzzy kappa fk developed by hagen zanker 2009 and fuzzy kappa simulation fks developed by van vliet et al 2013a as both metrics are derivatives of cohen s kappa they measure the observed agreement between two categorical data sets corrected for the agreement expected from random allocation of the given class sizes with fuzziness allowing for the consideration of partial agreement of location and class the major distinction between the two metrics is that fk considers the entire land use map for the calculation of agreement whereas fks only considers the transitioned cells both metrics are used because the variation in the measurement of agreement can impact the results obtained newland et al 2018 to measure landscape pattern structure the error of the simulated and observed clumpiness for the actively modelled land use classes is used clumpiness is a measure of the proportional deviation of the proportion of like adjacencies from that expected under a spatially random distribution for a specific land use class mcgarigal 2014 as clumpiness is measured at the class level it requires aggregation to a single value previous calibration methods that used the average class level clumpiness newland et al 2018 found this is not an ideal aggregation strategy because it can over emphasize relatively minor land use classes that occupy a relatively small amount of the total landscape hence the absolute area weighted clumpiness error awce of the actively modelled land use classes is used as this provides a better landscape centric perspective by emphasizing the classes that occupy the greatest area within the region for the calibration three combinations of metrics are used shown in table 6 each combination includes a metric for locational agreement and landscape pattern structure fk and fks are included together in set three to provide further insight into the relationships between these two objectives the use of different metrics also allows for an exploration of the influence this has on the resultant model output during the coarse adjustment stage two types of behaviour are observed in the objective space which dictates the selection of the meta parameter value these are shown in fig 10 the first type of behaviour is convergent shown in plot a for the fks and awce values for the range of θ st values sampled for the madrid case study as shown there is a meta parameter value that produces both the best fks and awce value which makes selecting the meta parameter value straightforward the second type of behaviour observed is a trade off see newland et al 2018 shown in plot b for the fks and awce values for the range of θ st values sampled for the berlin case study as shown improved performance in fks results in reduced performance in awce as the error increases in this case selection requires further interpretation from the user as the selection of a meta parameter value requires a preference for a certain trade off between objectives which impacts the fine parameter adjustment starting position and hence the resultant final output 4 3 4 fine parameter adjustment the fine parameter adjustment stage of calibration uses the combinations of metrics listed in table 6 the resultant output obtained from the fine parameter adjustment stage is heavily influenced by the weight and ranges used for each metric allowing the user to preference a certain objective an example is shown in fig 11 for the trajectory of the metrics whilst conducting the fine parameter adjustment for the lisbon case study using the objectives fk and awce as shown the method is quite sensitive to the preference for certain metrics and given the same starting point a preference for either locational agreement landscape pattern structure or a balance between the two has a large impact on both the trajectory of the fine calibration and the final metric values obtained given the possible variation that can be achieved three tests are conducted for each set of metrics for each case study one test that purely focuses on improving locational agreement la one test that purely focuses on improving landscape pattern structure lps and one test that balances improvements in both objectives the weightings used for each test are summarised in table 7 as shown total preference for a specific objective is achieved by weighting the other objective s as zero essentially making the problem single objective this is to highlight potential issues with using a single objective as this will likely result in over calibration it should be noted that fk and fks are evenly weighted for the case where both are used and there is preference for locational agreement or a balanced preference as noted in section 3 4 the metrics are scaled to between 0 and 1 by using practical ranges that are defined by the user the selected ranges are partially informed by the results of the meta parameter analysis but some trial and error is required particularly in the cases with a balanced preference to achieve the best possible objectives for different case studies 4 4 computational tests to evaluate the utility of the proposed approach as objectively as possible the calibration and validation performance obtained using the approach is compared with that achieved using two benchmark performance models that replicate common urban growth strategies these models include the random constraint match rcm neutral model riks 2011 which generates reference maps characterised by a speckled distribution of small clusters of each land use class and the growing clusters gc neutral model van vliet et al 2013b which generates reference maps characterised by agglomerated distributions of large clusters of each land use class generated using the default neighbourhood rule settings in metronamica if the performance of the calibrated luca model exceeds that of the benchmark models the luca model can be considered to have captured the processes driving land use change correctly hagen zanker and lajoie 2008 for this research ten reference maps are generated for each case study for both the calibration and validation period and reference metrics calculated for each reference map to evaluate the output 50 model replicates with different luca model random seeds are run with the set of neighbourhood weighting parameters obtained at the end of the proposed calibration approach for the calibration 1990 2000 and validation 2000 2006 periods for each simulated output map a number of performance metrics are calculated first the average metrics of the simulated output are compared with the average benchmark metrics to determine whether the benchmarks are outperformed in this case this corresponds to the average fk or fks values for the simulated output being greater than the average benchmark values and the average awce value being less than the average benchmark values the metric values obtained are then compared with the corresponding metrics for the benchmark models using welch s t test of statistical significance welch 1947 a method of comparing two independent samples with varying size and variance that obey parametric assumptions at the 95 confidence limit to determine whether the average of the output metrics is significantly better or worse than the benchmark metrics obtained from a statistical perspective 5 results and discussion this section presents the results of the application of the computational tests outlined in section 4 4 the results demonstrate how to determine a final calibrated model for each case study based on the different configurations tested to do this first an objective evaluation is conducted for the calibration and validation periods in comparison to the benchmark models the performance is summarised in figs 12 and 13 respectively with green colouring indicating that performance of the models calibrated with the proposed approach is statistically significantly better at the 95 confidence limit than that of both benchmark models yellow that model performance is superior to one benchmark and inferior to the other and red that model performance is inferior to both benchmarks in figs 12 and 13 the average metric value across the 50 model replicates is shown as there tended to be limited spread across the different replicates see box plots in appendix d configurations that outperform all benchmarks are then evaluated for their physical plausibility first the simulated output maps are compared with the data using visual interpretation at this stage if multiple configurations have resulted in superior performance a solution is selected that produces the simulated output most consistent with process knowledge the parameters of this solution are then evaluated against discursive knowledge to determine if the calibrated model is consistent with expectation 5 1 objective evaluation 5 1 1 calibration performance the performance of the different configurations applied to the case studies for the calibration period is summarised in fig 12 as shown the performance is generally significantly better than the benchmarks for the calibration period irrespective of the metrics used and the preference for a particular objective reflected by a majority of the cells being green this highlights the overall robustness of the proposed approach a detailed discussion of the impact of the choice of metrics and preference on the robustness of the proposed approach is given in the subsequent sections the performance across all three metrics fk fks and awce is particularly good when locational agreement and landscape pattern structure are balanced as there are only two cases where both benchmarks are not exceeded significantly fig 12 these results suggest that the proposed approach can identify parameter combinations that result in modelled land use dynamics that successfully balance locational agreement and landscape pattern structure when locational agreement is favoured during the fine adjustment stage values of fk and fks are generally higher than when landscape pattern structure is favoured or a balanced approach is used as shown in fig 12 however this comes at the expense of landscape pattern structure accuracy the performance of which deteriorates i e values of awce are increased to the point where values are only significantly better than all benchmark values in half the results this indicates that it is difficult to achieve results that satisfy both locational agreement and landscape pattern structure when only locational agreement is considered during the fine adjustment process and that a balanced approach is needed to achieve acceptable performance for both locational agreement and landscape patter structure see section 5 1 1 which is also consistent with the findings of newland et al 2018 values of fks are significantly better than all benchmarks for all case studies and objective function combinations when locational agreement is favoured during the fine adjustment process however somewhat surprisingly this is not the case when fk is used as the performance metric as can be seen from fig 12 when fks and awce are used as objectives fk values are only significantly better than all benchmarks for the madrid case study and for berlin and budapest are significantly worse than both benchmarks this likely occurs because the use of fks as the sole objective places an over emphasis on capturing the transitions that are present in the data with insufficient weight given to inertia this is especially important for cases where there is low or moderate growth such as berlin and budapest however in cases with high growth such as madrid this impact is less pronounced when there is a preference for landscape pattern structure during the fine adjustment process values of awce are superior i e less to those obtained when there is a preference for a different objective as shown in fig 12 however the singular focus on landscape pattern structure has a detrimental impact on the metrics used to measure locational agreement this is most noticeable for the fk values where both benchmarks are only exceeded in three cases all for the same case study budapest 5 1 2 validation performance the general performance of the different configurations applied to the different case studies for the validation period is summarised in fig 13 as shown the results for the validation period generally follow the same trends as for the calibration period in that a balanced preference between locational agreement and landscape pattern structure results in the best overall results while solely favouring locational agreement or landscape pattern structure results in a deterioration in performance of the other objective to the point that it is much less likely that corresponding benchmarks are exceeded specifically when a balanced approach is used in conjunction with either fk and awce or fk fks and awce as objectives validation performance is significantly better than that of all benchmarks for all metrics for three of the four case studies considered as well as outperforming both benchmarks in most cases for the other case study and only performing significantly worse than both benchmarks in one out of twenty four cases i e fk for budapest when a balanced approach is used in conjunction with fk and awce as objectives this suggests that the proposed approach is capable of generating results for both calibration and validation periods that perform significantly better at the 95 confidence level over 50 replicates for all three performance metrics in almost all experiments provided both locational agreement and landscape pattern structure are appropriately balanced during the fine adjustment process and fks is not used as the sole objective quantifying locational agreement while the validation performance of the models calibrated is significantly better than that of both benchmarks when an appropriate balance of objectives and preferences is used validation performance is generally worse than calibration performance as can be expected given that performance is tuned to the calibration data this deterioration in performance is particularly pronounced for the case studies experiencing less growth i e berlin and budapest and when fks is used as the sole objective for locational agreement this can be explained by the fact that fks only focuses on transitioned cells which can result in an over emphasis on capturing the small number of cells that do in fact transition in low growth cases during the calibration period at the expense of capturing inertia 5 2 simulated output evaluation this section presents an evaluation of a simulated output map for a configuration that performs statistically significantly better than the benchmarks for each case study the simulated output maps are compared with the data via visual inspection to determine if the resultant simulated output is sufficiently realistic further verifying the quality of the calibrated model overall the results for the four case studies demonstrated that the proposed approach produced realistic output maps provided an appropriate balance between locational agreement and landscape pattern structure is achieved as shown in figs 14 17 and discussed below fig 14 shows the similarity between the simulated output and the data for the berlin case study resulting from a balanced preference with fk and awce as the objectives the simulated output appears fairly similar to the data however there is some variation between the two maps with the larger amount of residential red cells allocated in the western region of the simulated output map as shown in fig 14c also the simulated map for industry commerce fig 14d tends to show slightly larger clusters in the simulated output compared to the data the similarity between the simulated output and the corresponding data for the budapest case study is shown in fig 15 when a balanced preference is used with fk fks and awce as the objectives in the budapest case study moderate land use change has taken place over the calibration period and the results obtained show the model captured the inertia well for the changes that did occur the model did not always allocate the transitions to the correct locations but the size of the simulated clusters resembles the data this is shown in fig 15 as there is good agreement between the patterns and locations of the urban classes residential red and industry commerce purple as shown in fig 15c and d as well as recreation areas brown this is also true for the major agricultural class arable land yellow there is some variation in the amount of interspersion of the classes natural areas light green and forest dark green with the simulated output producing more clumped areas though this is quite minor for the lisbon case study the simulated output for a balanced preference with fk fks and awce as the objectives is shown in fig 16 as shown the general behaviour of the simulated output is sufficiently consistent with the data though there are some differences in the newly allocated land use classes the simulated residential locations tend to fill in space in the urban core creating larger clusters than in the data this is shown in fig 16c as the red cells tend to be at the edge of the existing residential area whereas the blue tends to be within the existing residential area the simulated cluster size of the industry commerce area resembles the data and while the allocation takes place in similar areas exact matches were not often found as shown in fig 16d also the high degree of interspersion of the different agricultural and natural land use classes has resulted is a noticeable amount of the class pastures light green interspersed amongst a region of arable land dark yellow in the centre of the map for the simulated output that is not present in the data the consistency between the simulated output and corresponding data for the madrid case study is shown in fig 17 for a balanced preference with fk fks and awce as the objectives as shown there is reasonably good agreement for the expansion of the existing central urban region composed of residential red industry commerce purple and recreation areas brown the main variation is that the urban classes in the simulated output appear slightly more clumped than in the data as shown in fig 17c and d there tends to be a more speckled distribution of red cells whereas the blue cells tend to be at the fringes of the existing urban region it is worth highlighting that for the low and moderate growth case studies berlin and budapest respectively there was only one configuration that performed statistically significantly better than the benchmarks meaning only a single solution map required visual inspection and interpretation however for the high growth case studies lisbon and madrid more configurations resulted in superior benchmark performance for these situations visual comparison of the simulated output is important to determine the best performing calibrated model an example of this is presented in fig 18 which shows another configuration for the madrid case using a balanced preference with fk and awce as objectives fig 18b compared with the data fig 18a and the solution that was selected fig 18c as shown there is a large variation in the output obtained most notably the alternate solution fig 18b results in a large clustered formation of the class industry commerce purple in the north region of the map that does not appear in the data and as such is less realistic than the simulated output for the other configuration this highlights the sensitivity of the output to the configuration used and illustrates why visual inspection is beneficial as further interpretation of the output that is not captured with objective assessment alone 5 3 parameter analysis this section presents an analysis of the parameters for the four solutions corresponding to the best objective performance and most realistic simulated output the parameter analysis is used to further verify that the obtained solutions are consistent with process understanding in general the parameters obtained were consistent with expectation because the neighbourhood rules were parameterised to generate shapes that were consistent with process knowledge however there were cases where rules were included as discussed previously in section 4 3 1 that were not necessarily consistent with process knowledge which could be addressed with additional manual intervention in the parameter elimination stage given the variation in the interaction elimination across the case studies and the large number of parameters that were calibrated the parameter analysis is focussed on the parameters that were included in each case study the inertia points and self influence tails the final parameter values for each case study are presented in appendix e a comparison plot of the inertia point parameters obtained for the analysed solutions is presented in fig 19 ordered by class and case study as shown for each case study the classes that exhibit the highest inertia tend to be the urban classes residential industry commerce and recreation areas which is consistent with expectation as such classes have high transition costs the land use class arable land which essentially serves as a vacant class to facilitate transitions exhibits the lowest inertia for each case which is expected as such land uses tend to have the lowest transition costs fig 20 shows a comparison of the self influence tail parameters i e the influence of class i in the neighbourhood of class i for the different solutions for each case study the self influence tails exhibit behaviour that is also consistent with expectation the urban classes exhibit a higher degree of self influence across the different case studies agricultural pastures permanent crops agricultural areas classes exhibit less self influence and the class arable land exhibits virtually no self influence a trend observed across the case studies is that the higher growth cases lisbon and madrid exhibit higher self influence for the class residential than the low and moderate growth cases berlin and budapest this would be expected as higher growth cases will have more newly allocated residential cells which will require a stronger attraction to ensure they are allocated near the existing residential region hence based on the parameter evaluation the calibration method was able to generate parameters that are consistent with expectation 6 summary and conclusions transition potential luca models use neighbourhood rules to replicate the spatial dynamics that drive land use changes in a region neighbourhood rules must be effectively calibrated for model application as these are the main calibration parameters of such models due to their impact on parameter dimensionality this paper presents a semi automatic calibration method that integrates objective analysis with discursive input to facilitate efficient calibration of neighbourhood rules within a limited computational budget achievable using a desktop pc the method first reduces the complexity of the calibration problem and then calibrates the remaining neighbourhood rules in a computationally efficient manner based on a set of metrics that quantify the two key aspects of luca model performance locational agreement and landscape pattern structure the utility of the proposed approach was demonstrated via application to four european case studies with varying physical characteristics and rates of growth for each case study the method was implemented with a focus on a certain objective either locational agreement landscape pattern structure or a balance between the two based on a statistical analysis of the simulated output metrics compared with metrics obtained from two benchmark models and consideration of the simulated output maps and parameters optimal performance required using a balanced objective preference for this research the best performance for the low growth case study berlin was achieved when using a balanced approach between two objectives fuzzy kappa and area weighted clumpiness error for the moderate budapest and high lisbon and madrid growth cases performance was maximised when using a balanced approach between three objectives fuzzy kappa fuzzy kappa simulation and area weighted clumpiness error suggesting that locational agreement is best quantified by balancing between the agreement of transitions and the agreement of the entire land use map this research demonstrates the efficiency that the integration of process knowledge affords process specific calibration methods the results suggest further improvements in efficiency and the quality of the final output could be achieved by utilising additional process knowledge with the elimination or inclusion of certain neighbourhood rules and the input of more complex neighbourhood rule shapes the results also suggest potential improvement could be achieved by adjusting the calibration objectives as discussed in section 5 2 there tended to be over clustering of the resultant output for urban regions hence a different aggregation strategy for combining the class level clumpiness errors may improve the results also additional metrics could be used during the automatic calibration procedure that emphasize different pattern aspects such as the fractal dimension or edge density mcgarigal 2014 which were only captured by visual inspection for the case study application the demonstrated application shows that the proposed approach is a step towards efficient luca model calibration with the potential to facilitate more widespread use of luca models to support scenario and policy analysis the efficiency that is achieved with a process specific method also has the potential to increase the efficiency of more computationally demanding approaches such as optimisation this would further reduce the computational demands of such approaches as has been demonstrated with the use of meta modelling by şalap ayça et al 2018 integrating a process specific method with optimisation would have the added benefit of integrating further process knowledge into such methods potentially improving the applicability of the resulting models acknowledgements the authors wish to acknowledge the financial support from the bushfire and natural hazards cooperative research centre made available by the commonwealth of australia through the cooperative research centre program appendix a derivation of empirical measures this appendix provides a detailed derivation of the empirical measures used in the interaction elimination and parameter categorisation and initialisation sections of the proposed approach the link between data from the contingency table and the enrichment factor at distance zero is also shown the generation of the contingency table requires an analysis of the state of each cell in two data maps a1 s i j x ˆ 0 x ˆ a c c x ˆ 0 c i x ˆ a c j where s i j logs the state of the land use class in the cell c of interest at the start and end of the calibration period populating the contingency table requires evaluating the state transition for each possible combination of land use classes for the entire land use map a2 η i j s i j where η i j is the total numbers number of cells that are class i in x ˆ 0 and class j in x ˆ a the case where i does not equal j corresponds to a transition from i to j and the case where i is equal to j η i i shows the total number of cells that did not change land use class between x ˆ 0 and x ˆ a with these values determined the contingency table is populated such that table a1 example contingency table populated by logging the land use class in each cell between two maps table a1 x ˆ a a 1 a 2 a n x ˆ 0 a 1 η 1 1 η 1 2 η 1 n a 2 η 2 1 η 2 2 η 2 n a n η n 1 η n 2 η n n from the contingency table the empirical measures inertia rate and conversion rate can be calculated the inertia rate which quantifies the tendency of a particular class to persist is calculated by a3 i r i η i i m 1 n η i m the conversion rate which quantifies the tendency of transitions to a particular class as a function of all transitions to that class is calculated by a4 c r i j η i j m 1 n η m j η i i to calculate the enrichment factor requires evaluating the composition of the neighbourhoods of cells that transitioned the size of the neighbourhood is defined by a radius d max the maximum allowable distance between two cells for inclusion in the neighbourhood the neighbourhood is divided into a set of discrete unit distance rings i e distances of 1 2 3 based on the nearest unit distance between the neighbourhood cell and the central cell this is shown in fig a1 for a maximum cellular distance of three cells cells are allocated to the nearest unit distance neighbourhood ring so the neighbourhood of distance 0 comprises 1 cell at the location of the transition distance 1 comprises a total of 8 cells at distances of 1 and 1 41 cell lengths from the centre the neighbourhood of distance 2 comprises a total of 12 cells at distances 2 and 2 24 cell lengths from the centre and the neighbourhood of distance 3 comprises a total of 8 cells at distances 2 83 and 3 cell lengths from the centre fig a1 example delineation of a neighbourhood of maximum distance three cell into a set of unit distance rings for counting neighbourhood composition fig a1 following the subdivision of the neighbourhood into discrete unit distance rings the number of cells of each land use class in each unit distance ring in x 0 is tallied a5 n i j d c d d c x 0 c j where n i j d is the number of cells of land use class j in the neighbourhood of distance d of a cell that transitioned to land use class i and d d c is the set of cells in the neighbourhood of cell c at distance d this tally of absolute neighbourhood representation of a land use class at a certain distance from cells that transitioned to a certain land use class is then converted to a relative representation expressing the absolute representation as the percentage of the maximum number of possible cells in the neighbourhood e g for distance one 1 cell in the neighbourhood is equivalent to a relative representation of 0 125 2 cells in the neighbourhood is equivalent to a relative representation of 0 250 a6 r i j d n i j d n d where r i j d is the relative neighbourhood representation of the neighbourhood of distance d occupied by land use j for cells that transitioned to land use class i and n d is the total number of cells in the neighbourhood of distance d from this the average relative representation is computed for each cell that transitioned to a particular class a7 r i j d 1 c i i c i r i j d where r i j d is the average relative representation of land use class j in the neighbourhood at distance d of cells that transitioned to land use class i and c i is the set of cells that transitioned to land use class i the enrichment factor is then computed by a8 e f i j d l o g 10 r i j d n j n where ef i j d is the enrichment factor for the presence of land use class j in the neighbourhood at a distance d of cells that transitioned to land use class i n j is the number of cells of the neighbourhood class j in the land use map x ˆ 0 and n is the total number of cells in the land use map for ease of interpretation enrichment factor values are log scaled by a factor of ten with negative values indicating under representation and positive values indicating over representation it is possible to determine the enrichment factor value at distance 0 by using the contingency table as the average relative representation can be calculated from the contingency table by a9 r i j 0 η j i m 1 n η m i η i i hence it is possible to derive the enrichment factor value at distance zero which provides information about conversions using the contingency table however this highlights why the conversion rate can be more effective at capturing the different conversions that are occurring in the data for example many conversions may be occurring from a certain class to another but the enrichment factor may not suggest over representation indicating that a conversion point is not required because there is a large representation of the class in the landscape a common example of this are conversions from large relatively passive land use classes such as natural vegetation because they occupy a large area of the landscape and facilitate many conversions to different classes which are meaningful to include appendix b functional dependency of parameter categorisation scheme this appendix details how the proposed meta parameterisation and categorisation detailed in section 3 2 control the relative influence of the different interactions types and the functional dependence of the neighbourhood potential of the normalised weighting parameters within each interaction type the proposed approach introduces three meta parameters θ cp θ st θ ct that describe the inter type importance of each type of interaction p c p p s t and p c t with respect to the inertia point interaction type p i p and a quantisation scheme that describes the intra type importance within each interaction type the impact of this parameter representation on the neighbourhood potential is given by b1 n c i t n c i t i p θ c p n c i t c p θ s t n c i t s t θ c t n c i t c t where n c i t k is the neighbourhood potential of interaction type k for cell c to support land use type i at time t given the implemented parameterisation the individual interaction type terms can be defined as below for the inertia points b2 n c i t i p w i i 0 i f x c t i 0 o t h e r w i s e for the conversion points b3 n c i t c p w i x c t 0 0 i f x c t i o t h e r w i s e for the self influence tails b4 n c i t s t c d c i x c t p i t w i i d c c for the cross influence tails b5 n c i t c t c d c i x c t p c t w i x c t d c c appendix c contingency tables this appendix contains contingency tables for the data maps for all case studies for the calibration 1990 2000 and validation 2000 2006 periods table c1 berlin contingency table 1990 2000 table c1 luc luc map 2000 nat arl per pas agr res i c rec for r r por air m d fre mar tot map 1990 nat 2440 0 0 13 0 6 7 0 188 0 0 0 21 1 0 2676 arl 8 51812 148 1482 27 601 379 127 198 0 0 0 31 5 0 54818 per 0 1285 327 31 0 28 0 19 7 0 0 0 0 0 0 1697 pas 13 773 0 12937 32 23 5 3 39 0 0 0 0 18 0 13843 agr 0 45 0 151 1931 14 8 0 34 0 0 0 9 3 0 2195 res 4 4 0 3 0 17206 5 15 16 0 0 0 2 0 0 17255 i c 0 0 0 2 0 0 2033 0 0 0 0 0 0 0 0 2035 rec 0 0 0 0 0 9 0 1572 0 0 0 0 0 0 0 1581 for 111 8 0 4 9 58 30 0 58301 0 0 1 65 17 0 58604 r r 0 0 0 0 0 0 0 0 0 201 0 0 0 0 0 201 por 0 0 0 0 0 0 0 0 0 0 6 0 0 0 0 6 air 12 0 0 0 0 0 0 0 4 0 0 559 0 0 0 575 m d 20 5 0 0 0 0 0 0 8 0 0 0 282 5 0 320 fre 5 0 0 0 0 0 0 0 0 0 0 0 0 4189 0 4194 mar 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 tot 2613 53932 475 14623 1999 17945 2467 1736 58795 201 6 560 410 4238 0 160000 table c2 berlin contingency table 2000 2006 table c2 luc luc map 2006 nat arl per pas agr res i c rec for r r por air m d fre mar tot map 2000 nat 2258 7 0 78 96 0 2 11 150 4 0 0 3 4 0 2613 arl 22 52786 40 440 267 127 40 98 58 12 0 5 32 5 0 53932 per 0 41 433 0 0 1 0 0 0 0 0 0 0 0 0 475 pas 17 270 0 14293 24 4 6 2 5 0 0 0 0 2 0 14623 agr 28 2 0 2 1943 0 3 6 0 5 3 0 7 0 0 1999 res 15 14 0 34 0 17732 102 25 0 8 1 0 14 0 0 17945 i c 19 15 0 7 0 39 2347 6 2 11 0 0 21 0 0 2467 rec 0 0 0 0 0 19 0 1717 0 0 0 0 0 0 0 1736 for 80 2 0 0 8 31 35 13 58594 8 0 3 20 1 0 58795 r r 0 0 0 0 0 21 8 0 0 172 0 0 0 0 0 201 por 0 0 0 0 0 0 0 0 0 0 6 0 0 0 0 6 air 52 0 0 30 0 8 4 18 0 0 0 448 0 0 0 560 m d 5 1 0 10 12 5 21 9 7 0 0 0 338 2 0 410 fre 0 1 0 4 0 0 0 0 6 0 0 0 0 4227 0 4238 mar 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 tot 2496 53139 473 14898 2350 17987 2568 1905 58822 220 10 456 435 4241 0 160000 table c3 budapest contingency table 1990 2000 table c3 luc luc map 2000 nat arl per pas agr res i c rec for r r por air m d fre mar tot map 1990 nat 7226 31 0 37 0 5 0 0 2522 0 0 0 6 9 0 9836 arl 202 77735 329 1256 303 107 102 43 258 61 0 3 85 112 0 80596 per 60 356 3460 51 37 6 12 0 13 7 0 0 0 0 0 4002 pas 100 465 3 8265 21 25 22 7 17 0 0 0 2 6 0 8933 agr 55 36 6 73 9522 22 6 0 36 0 0 0 2 2 0 9760 res 6 4 0 16 0 13547 16 11 1 3 0 0 3 7 0 13614 i c 1 0 0 0 0 0 2213 1 0 1 0 0 1 0 0 2217 rec 4 0 0 0 0 3 3 2242 2 0 0 0 0 0 0 2254 for 1503 10 0 15 15 0 0 0 23426 5 0 0 4 5 0 24983 r r 0 0 0 0 0 0 0 0 0 271 0 0 0 0 0 271 por 0 0 0 0 0 0 0 0 0 0 58 0 0 0 0 58 air 0 0 0 0 0 0 0 0 0 0 0 415 0 0 0 415 m d 6 0 0 14 1 0 0 0 0 0 0 0 189 0 0 210 fre 0 0 0 2 4 0 0 3 0 0 0 0 0 2842 0 2851 mar 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 tot 9163 78637 3798 9729 9903 13715 2374 2307 26275 348 58 418 292 2983 0 160000 table c4 budapest contingency table 2000 2006 table c4 luc luc map 2006 nat arl per pas agr res i c rec for r r por air m d fre mar tot map 2000 nat 7001 134 20 316 129 20 3 1 1488 0 0 0 10 41 0 9163 arl 351 76176 598 318 223 195 105 19 425 13 0 1 42 171 0 78637 per 11 220 3435 14 86 12 2 3 15 0 0 0 0 0 0 3798 pas 228 335 32 8825 86 51 54 35 61 0 4 0 8 10 0 9729 agr 96 1546 32 308 7221 362 80 81 166 0 0 0 1 10 0 9903 res 0 15 0 5 31 13609 44 3 3 3 0 0 0 2 0 13715 i c 0 23 0 15 0 25 2294 13 2 2 0 0 0 0 0 2374 rec 1 5 0 0 0 89 8 2194 1 0 0 0 0 9 0 2307 for 914 36 13 4 34 3 8 5 25251 0 0 0 0 7 0 26275 r r 0 1 0 0 3 0 12 0 0 332 0 0 0 0 0 348 por 0 0 0 0 0 0 7 0 0 0 51 0 0 0 0 58 air 0 2 0 0 0 0 0 0 0 0 0 416 0 0 0 418 m d 0 9 0 13 0 9 2 6 0 0 0 0 236 17 0 292 fre 24 2 0 0 11 0 1 2 2 0 0 0 3 2938 0 2983 mar 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 tot 8626 78504 4130 9818 7824 14375 2620 2362 27414 350 55 417 300 3205 0 160000 table c5 lisbon contingency table 1990 2000 table c5 luc luc map 2000 nat arl per pas agr res i c rec for r r por air m d fre mar tot map 1990 nat 5884 111 43 1 99 232 97 83 2591 31 0 0 74 7 1 9254 arl 236 15304 88 186 154 259 105 0 263 9 0 7 19 11 0 16641 per 5 312 9558 0 32 34 20 8 17 0 0 0 8 0 0 9994 pas 14 1649 0 392 6 2 10 0 2 0 0 0 0 4 2 2081 agr 196 366 262 0 30873 1094 241 29 271 44 0 0 43 0 0 33419 res 0 0 0 0 0 5372 27 0 0 7 0 2 2 0 0 5410 i c 0 0 0 0 0 15 870 11 0 5 0 0 0 0 0 901 rec 0 0 0 0 0 0 4 397 0 4 0 0 0 0 0 405 for 3794 563 70 0 335 132 120 69 32769 24 0 0 65 5 5 37951 r r 0 0 0 0 0 0 0 0 0 21 0 0 0 0 0 21 por 0 0 0 0 0 0 0 8 0 0 103 0 0 0 0 111 air 0 0 0 0 0 0 0 0 0 0 0 223 0 0 0 223 m d 20 0 4 0 1 2 4 0 2 0 0 0 251 0 0 284 fre 53 1 0 0 0 0 0 0 0 0 0 0 0 829 0 883 mar 2 0 0 0 0 2 16 4 0 0 10 0 0 0 42388 42422 tot 10204 18306 10025 579 31500 7144 1514 609 35915 145 113 232 462 856 42396 160000 table c6 lisbon contingency table 2000 2006 table c6 luc luc map 2006 nat arl per pas agr res i c rec for r r por air m d fre mar tot map 2000 nat 6900 119 10 20 335 194 116 73 2327 30 0 0 53 25 2 10204 arl 212 16604 20 775 380 167 42 0 91 11 0 0 3 1 0 18306 per 33 111 9272 7 385 179 18 1 16 0 0 0 0 3 0 10025 pas 4 197 0 352 25 0 0 0 0 0 0 0 0 1 0 579 agr 374 216 71 6 28463 1560 97 39 629 23 0 0 17 0 5 31500 res 11 2 7 0 33 7006 42 25 7 8 0 0 2 0 1 7144 i c 19 0 4 1 17 17 1435 12 0 1 1 0 5 0 2 1514 rec 0 0 1 0 1 11 8 579 0 0 0 9 0 0 0 609 for 3660 83 18 0 256 87 33 72 31659 26 0 0 18 2 1 35915 r r 0 0 0 0 1 1 0 0 0 143 0 0 0 0 0 145 por 0 0 0 0 0 0 1 0 0 0 112 0 0 0 0 113 air 0 0 0 0 0 2 0 0 0 0 0 230 0 0 0 232 m d 12 11 0 0 13 32 14 0 6 2 0 0 372 0 0 462 fre 0 4 0 0 0 0 0 0 0 0 0 0 0 852 0 856 mar 1 15 0 0 1 2 1 5 0 0 0 0 0 0 42371 42396 tot 11226 17362 9403 1161 29910 9258 1807 806 34735 244 113 239 470 884 42382 160000 table c7 madrid contingency table 1990 2000 table c7 luc luc map 2000 nat arl per pas agr res i c rec for r r por air m d fre mar tot map 1990 nat 46821 268 3 0 50 1051 316 164 357 31 0 12 161 20 0 49254 arl 1540 55265 30 0 383 1492 1059 121 32 70 0 92 351 26 0 60461 per 50 18 4278 0 0 29 3 0 0 0 0 0 18 5 0 4401 pas 13 0 0 624 0 23 5 0 0 0 0 0 0 0 0 665 agr 191 34 15 0 20761 207 45 22 6 5 0 0 101 0 0 21387 res 1 4 0 0 6 7653 85 31 0 1 0 0 0 0 0 7781 i c 3 8 0 0 0 15 1152 5 0 0 0 0 0 0 0 1183 rec 0 0 0 0 0 37 0 656 0 0 0 0 0 0 0 693 for 55 16 0 0 8 21 0 11 12164 0 0 0 0 7 0 12282 r r 0 0 0 0 0 0 0 0 0 145 0 0 0 0 0 145 por 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 air 0 0 0 0 0 0 4 0 0 0 0 494 0 0 0 498 m d 33 6 0 0 0 29 26 39 0 0 0 0 368 2 0 503 fre 24 4 0 0 0 0 0 0 0 0 0 0 0 719 0 747 mar 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 tot 48731 55623 4326 624 21208 10557 2695 1049 12559 252 0 598 999 779 0 160000 table c8 madrid contingency table 2000 2006 table c8 luc luc map 2006 nat arl per pas agr res i c rec for r r por air m d fre mar tot map 2000 nat 47105 386 17 68 152 451 90 34 166 107 0 6 68 81 0 48731 arl 191 52871 35 0 249 865 442 50 5 518 0 186 202 9 0 55623 per 17 25 4149 0 45 38 5 0 2 11 0 0 10 24 0 4326 pas 13 0 0 586 0 17 0 0 2 4 0 0 0 2 0 624 agr 27 58 39 336 20499 109 43 11 17 30 0 0 39 0 0 21208 res 116 55 0 0 55 9931 136 86 21 62 0 90 2 3 0 10557 i c 16 23 0 0 0 287 2300 30 0 28 0 11 0 0 0 2695 rec 19 4 0 0 4 47 13 953 1 1 0 0 7 0 0 1049 for 143 4 0 0 16 37 3 6 12324 9 0 15 2 0 0 12559 r r 0 0 0 0 0 148 9 0 0 95 0 0 0 0 0 252 por 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 air 6 44 0 0 0 6 4 0 0 0 0 538 0 0 0 598 m d 88 78 0 11 25 71 71 41 0 11 0 13 590 0 0 999 fre 0 1 0 4 0 0 0 0 0 0 0 0 10 764 0 779 mar 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 tot 47741 53549 4240 1005 21045 12007 3116 1211 12538 876 0 859 930 883 0 160000 appendix d boxplots of simulated replicates this appendix contains boxplots to show the spread of the results for the different configurations of the proposed approach applied to the different case studies fig d1 boxplot of simulated replicates for different configurations for berlin case study for calibration and validation periods fig d1 fig d2 boxplot of simulated replicates for different configurations for budapest case study for calibration and validation periods fig d2 fig d3 boxplot of simulated replicates for different configurations for lisbon case study for calibration and validation periods fig d3 fig d4 boxplot of simulated replicates for different configurations for madrid case study for calibration and validation periods fig d4 appendix e final model parameterisations this appendix contains the final recommend values for the neighbourhood rules for each case study obtained at the conclusion of the application of the calibration method table e1 final neighbourhood rule parameter values berlin table e1 from to distance 0 1 2 5 influence natural areas arable land 0 00 0 00 0 00 0 00 natural areas permanent crops 0 00 0 00 0 00 0 00 natural areas pastures 0 00 0 00 0 00 0 00 natural areas agricultural areas 0 00 0 00 0 00 0 00 natural areas residential 0 00 0 00 0 00 0 00 natural areas industry commerce 0 00 0 00 0 00 0 00 natural areas recreation areas 0 00 0 00 0 00 0 00 natural areas forest 7 20 5 57 0 56 0 00 arable land arable land 404 99 2 94 0 29 0 00 arable land permanent crops 51 97 2 94 0 29 0 00 arable land pastures 45 08 1 32 0 13 0 00 arable land agricultural areas 66 56 6 39 0 64 0 00 arable land residential 26 55 1 50 0 15 0 00 arable land industry commerce 62 11 9 33 0 93 0 00 arable land recreation areas 69 50 1 82 0 18 0 00 arable land forest 25 00 0 00 0 00 0 00 permanent crops arable land 70 32 0 81 0 08 0 00 permanent crops permanent crops 317 63 35 57 3 56 0 00 permanent crops pastures 0 00 15 91 1 59 0 00 permanent crops agricultural areas 0 00 0 00 0 00 0 00 permanent crops residential 94 43 23 92 2 39 0 00 permanent crops industry commerce 0 00 0 00 0 00 0 00 permanent crops recreation areas 25 23 3 00 0 30 0 00 permanent crops forest 0 00 0 00 0 00 0 00 pastures arable land 25 00 1 32 0 13 0 00 pastures permanent crops 0 00 0 00 0 00 0 00 pastures pastures 500 00 38 20 3 82 0 00 pastures agricultural areas 25 00 3 00 0 30 0 00 pastures residential 12 50 0 00 0 00 0 00 pastures industry commerce 0 00 0 00 0 00 0 00 pastures recreation areas 0 00 0 00 0 00 0 00 pastures forest 99 50 0 00 0 00 0 00 agricultural areas arable land 0 00 6 39 0 64 0 00 agricultural areas permanent crops 0 00 1 50 0 15 0 00 agricultural areas pastures 11 65 3 00 0 30 0 00 agricultural areas agricultural areas 443 02 10 64 1 06 0 00 agricultural areas residential 0 00 16 22 1 62 0 00 agricultural areas industry commerce 0 00 1 50 0 15 0 00 agricultural areas recreation areas 0 00 0 00 0 00 0 00 agricultural areas forest 99 50 4 26 0 43 0 00 residential arable land 0 00 0 00 0 00 0 00 residential permanent crops 0 00 0 00 0 00 0 00 residential pastures 0 00 0 00 0 00 0 00 residential agricultural areas 0 00 0 00 0 00 0 00 residential residential 1000 00 17 22 1 72 0 00 residential industry commerce 0 00 0 00 0 00 0 00 residential recreation areas 12 50 0 00 0 00 0 00 residential forest 12 50 0 00 0 00 0 00 industry commerce arable land 0 00 0 00 0 00 0 00 industry commerce permanent crops 0 00 0 00 0 00 0 00 industry commerce pastures 0 00 0 00 0 00 0 00 industry commerce agricultural areas 0 00 0 00 0 00 0 00 industry commerce residential 0 00 0 00 0 00 0 00 industry commerce industry commerce 1000 00 84 91 8 49 0 00 industry commerce recreation areas 0 00 0 00 0 00 0 00 industry commerce forest 0 00 0 00 0 00 0 00 recreation areas arable land 0 00 0 00 0 00 0 00 recreation areas permanent crops 0 00 0 00 0 00 0 00 recreation areas pastures 0 00 0 00 0 00 0 00 recreation areas agricultural areas 0 00 0 00 0 00 0 00 recreation areas residential 0 00 38 20 3 82 0 00 recreation areas industry commerce 0 00 0 00 0 00 0 00 recreation areas recreation areas 1000 00 84 60 8 46 0 00 recreation areas forest 0 00 0 00 0 00 0 00 forest arable land 0 00 0 00 0 00 0 00 forest permanent crops 0 00 0 00 0 00 0 00 forest pastures 0 00 0 00 0 00 0 00 forest agricultural areas 25 00 0 00 0 00 0 00 forest residential 12 50 0 00 0 00 0 00 forest industry commerce 12 50 0 00 0 00 0 00 forest recreation areas 0 00 0 00 0 00 0 00 forest forest 996 23 9 83 0 98 0 00 road rail arable land 0 00 0 00 0 00 0 00 road rail permanent crops 0 00 0 00 0 00 0 00 road rail pastures 0 00 0 00 0 00 0 00 road rail agricultural areas 0 00 0 00 0 00 0 00 road rail residential 0 00 0 00 0 00 0 00 road rail industry commerce 0 00 0 00 0 00 0 00 road rail recreation areas 0 00 0 00 0 00 0 00 road rail forest 0 00 0 00 0 00 0 00 port area arable land 0 00 0 00 0 00 0 00 port area permanent crops 0 00 0 00 0 00 0 00 port area pastures 0 00 0 00 0 00 0 00 port area agricultural areas 0 00 0 00 0 00 0 00 port area residential 0 00 0 00 0 00 0 00 port area industry commerce 0 00 0 00 0 00 0 00 port area recreation areas 0 00 0 00 0 00 0 00 port area forest 0 00 0 00 0 00 0 00 airports arable land 0 00 0 00 0 00 0 00 airports permanent crops 0 00 27 86 2 79 0 00 airports pastures 0 00 0 00 0 00 0 00 airports agricultural areas 0 00 0 00 0 00 0 00 airports residential 0 00 0 00 0 00 0 00 airports industry commerce 0 00 25 74 2 57 0 00 airports recreation areas 0 00 0 00 0 00 0 00 airports forest 0 00 0 00 0 00 0 00 mine dump sites arable land 0 00 0 00 0 00 0 00 mine dump sites permanent crops 0 00 0 00 0 00 0 00 mine dump sites pastures 0 00 0 00 0 00 0 00 mine dump sites agricultural areas 0 00 0 00 0 00 0 00 mine dump sites residential 0 00 0 00 0 00 0 00 mine dump sites industry commerce 0 00 0 00 0 00 0 00 mine dump sites recreation areas 0 00 0 00 0 00 0 00 mine dump sites forest 0 00 33 44 3 34 0 00 fresh water arable land 0 00 0 00 0 00 0 00 fresh water permanent crops 0 00 0 00 0 00 0 00 fresh water pastures 0 00 0 00 0 00 0 00 fresh water agricultural areas 0 00 9 83 0 98 0 00 fresh water residential 0 00 0 00 0 00 0 00 fresh water industry commerce 0 00 0 00 0 00 0 00 fresh water recreation areas 0 00 0 00 0 00 0 00 fresh water forest 0 00 0 00 0 00 0 00 marine water arable land 0 00 0 00 0 00 0 00 marine water permanent crops 0 00 0 00 0 00 0 00 marine water pastures 0 00 0 00 0 00 0 00 marine water agricultural areas 0 00 0 00 0 00 0 00 marine water residential 0 00 0 00 0 00 0 00 marine water industry commerce 0 00 0 00 0 00 0 00 marine water recreation areas 0 00 0 00 0 00 0 00 marine water forest 0 00 0 00 0 00 0 00 table e2 final neighbourhood rule parameter values budapest table e2 from to distance 0 1 2 5 influence natural areas arable land 8 13 0 00 0 00 0 00 natural areas permanent crops 0 00 0 00 0 00 0 00 natural areas pastures 8 13 0 00 0 00 0 00 natural areas agricultural areas 0 00 0 00 0 00 0 00 natural areas residential 8 13 0 00 0 00 0 00 natural areas industry commerce 0 00 0 00 0 00 0 00 natural areas recreation areas 0 00 0 00 0 00 0 00 natural areas forest 32 50 3 00 0 30 0 00 arable land arable land 250 00 5 00 0 50 0 00 arable land permanent crops 32 50 1 50 0 15 0 00 arable land pastures 32 50 1 50 0 15 0 00 arable land agricultural areas 32 50 0 00 0 00 0 00 arable land residential 32 50 0 00 0 00 0 00 arable land industry commerce 32 50 0 00 0 00 0 00 arable land recreation areas 32 50 0 00 0 00 0 00 arable land forest 8 13 0 00 0 00 0 00 permanent crops arable land 16 25 6 00 0 60 0 00 permanent crops permanent crops 800 89 10 00 1 00 0 00 permanent crops pastures 8 13 1 50 0 15 0 00 permanent crops agricultural areas 8 13 1 50 0 15 0 00 permanent crops residential 8 13 0 00 0 00 0 00 permanent crops industry commerce 8 13 1 50 0 15 0 00 permanent crops recreation areas 0 00 0 00 0 00 0 00 permanent crops forest 0 00 0 00 0 00 0 00 pastures arable land 32 50 3 00 0 30 0 00 pastures permanent crops 0 00 0 00 0 00 0 00 pastures pastures 500 00 5 00 0 50 0 00 pastures agricultural areas 8 13 0 00 0 00 0 00 pastures residential 16 25 0 00 0 00 0 00 pastures industry commerce 16 25 0 00 0 00 0 00 pastures recreation areas 16 25 0 00 0 00 0 00 pastures forest 0 00 0 00 0 00 0 00 agricultural areas arable land 8 13 0 00 0 00 0 00 agricultural areas permanent crops 0 00 1 50 0 15 0 00 agricultural areas pastures 8 13 0 00 0 00 0 00 agricultural areas agricultural areas 1000 00 42 45 4 25 0 00 agricultural areas residential 16 25 1 50 0 15 0 00 agricultural areas industry commerce 8 13 0 00 0 00 0 00 agricultural areas recreation areas 0 00 0 00 0 00 0 00 agricultural areas forest 0 00 0 00 0 00 0 00 residential arable land 0 00 0 00 0 00 0 00 residential permanent crops 0 00 0 00 0 00 0 00 residential pastures 0 00 0 00 0 00 0 00 residential agricultural areas 0 00 0 00 0 00 0 00 residential residential 1000 00 19 35 1 93 0 00 residential industry commerce 8 13 1 50 0 15 0 00 residential recreation areas 16 25 1 50 0 15 0 00 residential forest 0 00 0 00 0 00 0 00 industry commerce arable land 0 00 0 00 0 00 0 00 industry commerce permanent crops 0 00 0 00 0 00 0 00 industry commerce pastures 0 00 0 00 0 00 0 00 industry commerce agricultural areas 0 00 0 00 0 00 0 00 industry commerce residential 0 00 0 00 0 00 0 00 industry commerce industry commerce 1000 00 46 71 4 67 0 00 industry commerce recreation areas 0 00 1 50 0 15 0 00 industry commerce forest 0 00 0 00 0 00 0 00 recreation areas arable land 0 00 0 00 0 00 0 00 recreation areas permanent crops 0 00 0 00 0 00 0 00 recreation areas pastures 0 00 1 50 0 15 0 00 recreation areas agricultural areas 0 00 0 00 0 00 0 00 recreation areas residential 0 00 1 50 0 15 0 00 recreation areas industry commerce 0 00 1 50 0 15 0 00 recreation areas recreation areas 1000 00 43 27 4 33 0 00 recreation areas forest 0 00 0 00 0 00 0 00 forest arable land 0 00 0 00 0 00 0 00 forest permanent crops 0 00 0 00 0 00 0 00 forest pastures 0 00 0 00 0 00 0 00 forest agricultural areas 8 13 0 00 0 00 0 00 forest residential 0 00 0 00 0 00 0 00 forest industry commerce 0 00 0 00 0 00 0 00 forest recreation areas 0 00 0 00 0 00 0 00 forest forest 500 00 10 33 1 03 0 00 road rail arable land 0 00 0 00 0 00 0 00 road rail permanent crops 0 00 0 00 0 00 0 00 road rail pastures 0 00 0 00 0 00 0 00 road rail agricultural areas 0 00 0 00 0 00 0 00 road rail residential 0 00 0 00 0 00 0 00 road rail industry commerce 0 00 0 00 0 00 0 00 road rail recreation areas 0 00 0 00 0 00 0 00 road rail forest 0 00 0 00 0 00 0 00 port area arable land 0 00 0 00 0 00 0 00 port area permanent crops 0 00 0 00 0 00 0 00 port area pastures 0 00 0 00 0 00 0 00 port area agricultural areas 0 00 0 00 0 00 0 00 port area residential 0 00 0 00 0 00 0 00 port area industry commerce 0 00 0 00 0 00 0 00 port area recreation areas 0 00 0 00 0 00 0 00 port area forest 0 00 0 00 0 00 0 00 airports arable land 0 00 0 00 0 00 0 00 airports permanent crops 0 00 0 00 0 00 0 00 airports pastures 0 00 0 00 0 00 0 00 airports agricultural areas 0 00 0 00 0 00 0 00 airports residential 0 00 0 00 0 00 0 00 airports industry commerce 0 00 0 00 0 00 0 00 airports recreation areas 0 00 0 00 0 00 0 00 airports forest 0 00 0 00 0 00 0 00 mine dump sites arable land 0 00 0 00 0 00 0 00 mine dump sites permanent crops 0 00 1 50 0 15 0 00 mine dump sites pastures 0 00 3 00 0 30 0 00 mine dump sites agricultural areas 0 00 0 00 0 00 0 00 mine dump sites residential 0 00 0 00 0 00 0 00 mine dump sites industry commerce 0 00 0 00 0 00 0 00 mine dump sites recreation areas 0 00 0 00 0 00 0 00 mine dump sites forest 0 00 0 00 0 00 0 00 fresh water arable land 0 00 0 00 0 00 0 00 fresh water permanent crops 0 00 0 00 0 00 0 00 fresh water pastures 0 00 0 00 0 00 0 00 fresh water agricultural areas 0 00 1 50 0 15 0 00 fresh water residential 0 00 0 00 0 00 0 00 fresh water industry commerce 0 00 0 00 0 00 0 00 fresh water recreation areas 8 13 3 00 0 30 0 00 fresh water forest 0 00 0 00 0 00 0 00 marine water arable land 0 00 0 00 0 00 0 00 marine water permanent crops 0 00 0 00 0 00 0 00 marine water pastures 0 00 0 00 0 00 0 00 marine water agricultural areas 0 00 0 00 0 00 0 00 marine water residential 0 00 0 00 0 00 0 00 marine water industry commerce 0 00 0 00 0 00 0 00 marine water recreation areas 0 00 0 00 0 00 0 00 marine water forest 0 00 0 00 0 00 0 00 table e3 final neighbourhood rule parameter values lisbon table e3 from to distance 0 1 2 5 influence natural areas arable land 8 51 0 00 0 00 0 00 natural areas permanent crops 10 00 1 00 0 10 0 00 natural areas pastures 0 00 0 00 0 00 0 00 natural areas agricultural areas 20 00 1 00 0 10 0 00 natural areas residential 28 68 9 33 0 93 0 00 natural areas industry commerce 18 34 1 00 0 10 0 00 natural areas recreation areas 20 00 2 00 0 20 0 00 natural areas forest 39 51 4 00 0 40 0 00 arable land arable land 500 00 3 75 0 38 0 00 arable land permanent crops 20 00 1 00 0 10 0 00 arable land pastures 40 00 2 00 0 20 0 00 arable land agricultural areas 20 00 1 00 0 10 0 00 arable land residential 36 38 0 00 0 00 0 00 arable land industry commerce 26 55 1 63 0 16 0 00 arable land recreation areas 0 00 0 00 0 00 0 00 arable land forest 10 00 0 00 0 00 0 00 permanent crops arable land 20 00 1 00 0 10 0 00 permanent crops permanent crops 1000 00 6 25 0 63 0 00 permanent crops pastures 0 00 0 00 0 00 0 00 permanent crops agricultural areas 10 00 0 00 0 00 0 00 permanent crops residential 0 00 0 00 0 00 0 00 permanent crops industry commerce 99 50 0 00 0 00 0 00 permanent crops recreation areas 10 00 0 00 0 00 0 00 permanent crops forest 0 00 0 00 0 00 0 00 pastures arable land 38 20 4 00 0 40 0 00 pastures permanent crops 0 00 0 00 0 00 0 00 pastures pastures 851 11 1 32 0 13 0 00 pastures agricultural areas 0 00 0 00 0 00 0 00 pastures residential 0 00 0 00 0 00 0 00 pastures industry commerce 0 00 0 00 0 00 0 00 pastures recreation areas 0 00 0 00 0 00 0 00 pastures forest 0 00 0 00 0 00 0 00 agricultural areas arable land 14 59 0 00 0 00 0 00 agricultural areas permanent crops 40 00 1 00 0 10 0 00 agricultural areas pastures 0 00 0 00 0 00 0 00 agricultural areas agricultural areas 275 83 0 50 0 05 0 00 agricultural areas residential 47 21 27 86 2 79 0 00 agricultural areas industry commerce 58 86 3 75 0 38 0 00 agricultural areas recreation areas 20 00 0 00 0 00 0 00 agricultural areas forest 10 00 0 00 0 00 0 00 residential arable land 0 00 0 00 0 00 0 00 residential permanent crops 0 00 0 00 0 00 0 00 residential pastures 0 00 0 00 0 00 0 00 residential agricultural areas 0 00 1 00 0 10 0 00 residential residential 1000 00 62 31 6 23 0 00 residential industry commerce 10 00 27 05 2 71 0 00 residential recreation areas 0 00 1 00 0 10 0 00 residential forest 0 00 0 00 0 00 0 00 industry commerce arable land 0 00 0 00 0 00 0 00 industry commerce permanent crops 0 00 0 00 0 00 0 00 industry commerce pastures 0 00 0 00 0 00 0 00 industry commerce agricultural areas 0 00 0 00 0 00 0 00 industry commerce residential 0 00 83 28 8 33 0 00 industry commerce industry commerce 1000 00 85 41 8 54 0 00 industry commerce recreation areas 10 00 2 00 0 20 0 00 industry commerce forest 0 00 0 00 0 00 0 00 recreation areas arable land 0 00 0 00 0 00 0 00 recreation areas permanent crops 0 00 0 00 0 00 0 00 recreation areas pastures 0 00 0 00 0 00 0 00 recreation areas agricultural areas 0 00 0 00 0 00 0 00 recreation areas residential 0 00 40 33 4 03 0 00 recreation areas industry commerce 0 00 38 20 3 82 0 00 recreation areas recreation areas 1000 00 85 41 8 54 0 00 recreation areas forest 0 00 0 00 0 00 0 00 forest arable land 23 10 0 00 0 00 0 00 forest permanent crops 61 80 0 00 0 00 0 00 forest pastures 0 00 0 00 0 00 0 00 forest agricultural areas 98 68 1 00 0 10 0 00 forest residential 43 27 0 00 0 00 0 00 forest industry commerce 20 00 0 00 0 00 0 00 forest recreation areas 61 30 6 89 0 69 0 00 forest forest 250 00 7 20 0 72 0 00 road rail arable land 0 00 0 00 0 00 0 00 road rail permanent crops 0 00 0 00 0 00 0 00 road rail pastures 0 00 0 00 0 00 0 00 road rail agricultural areas 0 00 0 00 0 00 0 00 road rail residential 0 00 72 95 7 29 0 00 road rail industry commerce 0 00 0 00 0 00 0 00 road rail recreation areas 0 00 2 00 0 20 0 00 road rail forest 0 00 0 00 0 00 0 00 port area arable land 0 00 0 00 0 00 0 00 port area permanent crops 0 00 0 00 0 00 0 00 port area pastures 0 00 0 00 0 00 0 00 port area agricultural areas 0 00 0 00 0 00 0 00 port area residential 0 00 0 00 0 00 0 00 port area industry commerce 0 00 0 00 0 00 0 00 port area recreation areas 10 00 76 39 7 64 0 00 port area forest 0 00 0 00 0 00 0 00 airports arable land 0 00 0 00 0 00 0 00 airports permanent crops 0 00 0 00 0 00 0 00 airports pastures 0 00 0 00 0 00 0 00 airports agricultural areas 0 00 0 00 0 00 0 00 airports residential 0 00 0 00 0 00 0 00 airports industry commerce 0 00 0 00 0 00 0 00 airports recreation areas 0 00 0 00 0 00 0 00 airports forest 0 00 0 00 0 00 0 00 mine dump sites arable land 0 00 0 00 0 00 0 00 mine dump sites permanent crops 0 00 0 00 0 00 0 00 mine dump sites pastures 0 00 0 00 0 00 0 00 mine dump sites agricultural areas 0 00 18 85 1 88 0 00 mine dump sites residential 0 00 1 00 0 10 0 00 mine dump sites industry commerce 0 00 1 00 0 10 0 00 mine dump sites recreation areas 0 00 2 00 0 20 0 00 mine dump sites forest 0 00 0 00 0 00 0 00 fresh water arable land 0 00 0 00 0 00 0 00 fresh water permanent crops 0 00 0 00 0 00 0 00 fresh water pastures 0 00 80 15 8 01 0 00 fresh water agricultural areas 0 00 0 00 0 00 0 00 fresh water residential 0 00 0 00 0 00 0 00 fresh water industry commerce 0 00 0 00 0 00 0 00 fresh water recreation areas 0 00 0 00 0 00 0 00 fresh water forest 0 00 0 00 0 00 0 00 marine water arable land 0 00 0 00 0 00 0 00 marine water permanent crops 0 00 0 00 0 00 0 00 marine water pastures 0 00 0 00 0 00 0 00 marine water agricultural areas 0 00 0 00 0 00 0 00 marine water residential 0 00 0 00 0 00 0 00 marine water industry commerce 0 00 0 00 0 00 0 00 marine water recreation areas 0 00 0 00 0 00 0 00 marine water forest 0 00 0 00 0 00 0 00 table e4 final neighbourhood rule parameter values madrid table e4 from to distance 0 1 2 5 influence natural areas arable land 15 40 2 75 0 28 0 00 natural areas permanent crops 60 49 0 00 0 00 0 00 natural areas pastures 0 00 0 00 0 00 0 00 natural areas agricultural areas 20 00 0 00 0 00 0 00 natural areas residential 20 00 0 00 0 00 0 00 natural areas industry commerce 32 12 0 00 0 00 0 00 natural areas recreation areas 19 85 2 75 0 28 0 00 natural areas forest 33 13 2 13 0 21 0 00 arable land arable land 250 00 7 70 0 77 0 00 arable land permanent crops 40 00 2 75 0 28 0 00 arable land pastures 0 00 0 00 0 00 0 00 arable land agricultural areas 28 37 2 75 0 28 0 00 arable land residential 52 48 2 75 0 28 0 00 arable land industry commerce 69 50 2 13 0 21 0 00 arable land recreation areas 38 70 0 00 0 00 0 00 arable land forest 81 46 0 00 0 00 0 00 permanent crops arable land 10 00 0 00 0 00 0 00 permanent crops permanent crops 572 17 21 48 2 15 0 00 permanent crops pastures 0 00 0 00 0 00 0 00 permanent crops agricultural areas 0 00 0 00 0 00 0 00 permanent crops residential 0 00 0 00 0 00 0 00 permanent crops industry commerce 0 00 0 00 0 00 0 00 permanent crops recreation areas 0 00 0 00 0 00 0 00 permanent crops forest 0 00 0 00 0 00 0 00 pastures arable land 0 00 0 00 0 00 0 00 pastures permanent crops 0 00 0 00 0 00 0 00 pastures pastures 735 59 10 00 1 00 0 00 pastures agricultural areas 0 00 0 00 0 00 0 00 pastures residential 0 00 2 75 0 28 0 00 pastures industry commerce 0 00 0 00 0 00 0 00 pastures recreation areas 0 00 0 00 0 00 0 00 pastures forest 0 00 0 00 0 00 0 00 agricultural areas arable land 10 00 0 00 0 00 0 00 agricultural areas permanent crops 20 00 8 51 0 85 0 00 agricultural areas pastures 0 00 0 00 0 00 0 00 agricultural areas agricultural areas 353 33 9 33 0 93 0 00 agricultural areas residential 42 96 0 00 0 00 0 00 agricultural areas industry commerce 52 28 0 00 0 00 0 00 agricultural areas recreation areas 63 12 0 00 0 00 0 00 agricultural areas forest 0 00 0 00 0 00 0 00 residential arable land 0 00 0 00 0 00 0 00 residential permanent crops 0 00 0 00 0 00 0 00 residential pastures 0 00 0 00 0 00 0 00 residential agricultural areas 0 00 0 00 0 00 0 00 residential residential 1000 00 61 30 6 13 0 00 residential industry commerce 10 00 35 57 3 56 0 00 residential recreation areas 10 00 2 75 0 28 0 00 residential forest 0 00 0 00 0 00 0 00 industry commerce arable land 0 00 2 75 0 28 0 00 industry commerce permanent crops 0 00 0 00 0 00 0 00 industry commerce pastures 0 00 0 00 0 00 0 00 industry commerce agricultural areas 0 00 0 00 0 00 0 00 industry commerce residential 0 00 3 44 0 34 0 00 industry commerce industry commerce 1000 00 55 73 5 57 0 00 industry commerce recreation areas 0 00 10 33 1 03 0 00 industry commerce forest 0 00 0 00 0 00 0 00 recreation areas arable land 0 00 0 00 0 00 0 00 recreation areas permanent crops 0 00 0 00 0 00 0 00 recreation areas pastures 0 00 0 00 0 00 0 00 recreation areas agricultural areas 0 00 0 00 0 00 0 00 recreation areas residential 0 00 5 57 0 56 0 00 recreation areas industry commerce 0 00 0 00 0 00 0 00 recreation areas recreation areas 500 00 20 16 2 02 0 00 recreation areas forest 0 00 0 00 0 00 0 00 forest arable land 10 00 0 00 0 00 0 00 forest permanent crops 0 00 0 00 0 00 0 00 forest pastures 0 00 0 00 0 00 0 00 forest agricultural areas 0 00 0 00 0 00 0 00 forest residential 0 00 0 00 0 00 0 00 forest industry commerce 0 00 0 00 0 00 0 00 forest recreation areas 10 00 0 00 0 00 0 00 forest forest 1000 00 5 57 0 56 0 00 road rail arable land 0 00 0 00 0 00 0 00 road rail permanent crops 0 00 0 00 0 00 0 00 road rail pastures 0 00 0 00 0 00 0 00 road rail agricultural areas 0 00 0 00 0 00 0 00 road rail residential 0 00 0 00 0 00 0 00 road rail industry commerce 0 00 68 19 6 82 0 00 road rail recreation areas 0 00 0 00 0 00 0 00 road rail forest 0 00 0 00 0 00 0 00 port area arable land 0 00 0 00 0 00 0 00 port area permanent crops 0 00 0 00 0 00 0 00 port area pastures 0 00 0 00 0 00 0 00 port area agricultural areas 0 00 0 00 0 00 0 00 port area residential 0 00 0 00 0 00 0 00 port area industry commerce 0 00 0 00 0 00 0 00 port area recreation areas 0 00 0 00 0 00 0 00 port area forest 0 00 0 00 0 00 0 00 airports arable land 0 00 0 00 0 00 0 00 airports permanent crops 0 00 0 00 0 00 0 00 airports pastures 0 00 0 00 0 00 0 00 airports agricultural areas 0 00 0 00 0 00 0 00 airports residential 0 00 0 00 0 00 0 00 airports industry commerce 0 00 19 35 1 93 0 00 airports recreation areas 0 00 0 00 0 00 0 00 airports forest 0 00 0 00 0 00 0 00 mine dump sites arable land 0 00 0 00 0 00 0 00 mine dump sites permanent crops 0 00 31 31 3 13 0 00 mine dump sites pastures 0 00 0 00 0 00 0 00 mine dump sites agricultural areas 0 00 0 00 0 00 0 00 mine dump sites residential 0 00 51 47 5 15 0 00 mine dump sites industry commerce 0 00 11 15 1 11 0 00 mine dump sites recreation areas 10 00 11 15 1 11 0 00 mine dump sites forest 0 00 0 00 0 00 0 00 fresh water arable land 0 00 0 00 0 00 0 00 fresh water permanent crops 0 00 0 00 0 00 0 00 fresh water pastures 0 00 0 00 0 00 0 00 fresh water agricultural areas 0 00 0 00 0 00 0 00 fresh water residential 0 00 0 00 0 00 0 00 fresh water industry commerce 0 00 0 00 0 00 0 00 fresh water recreation areas 0 00 0 00 0 00 0 00 fresh water forest 0 00 0 00 0 00 0 00 marine water arable land 0 00 0 00 0 00 0 00 marine water permanent crops 0 00 0 00 0 00 0 00 marine water pastures 0 00 0 00 0 00 0 00 marine water agricultural areas 0 00 0 00 0 00 0 00 marine water residential 0 00 0 00 0 00 0 00 marine water industry commerce 0 00 0 00 0 00 0 00 marine water recreation areas 0 00 0 00 0 00 0 00 marine water forest 0 00 0 00 0 00 0 00 
26354,land use change models generally include neighbourhood rules to capture the spatial dynamics between different land uses that drive land use changes introducing many parameters that require calibration we present a process specific semi automatic method for calibrating neighbourhood rules that utilises discursive knowledge and empirical analysis to reduce the complexity of the calibration problem and efficiently calibrates the remaining interactions with consideration of locational agreement and landscape pattern structure objectives the approach and software for implementing it are tested on four case studies of major european cities with different physical characteristics and rates of urban growth exploring preferences for different objectives the approach outperformed benchmark models for both calibration and validation when a balanced objective preference was used this research demonstrates the utility of process specific calibration methods and highlights how process knowledge can be integrated with automatic calibration to make it more efficient keywords cellular automata land use model calibration complexity reduction semi automatic calibration automatic parameter tuning software availability name of software empirical neighbourhood rule calibrator enc developer charles p newland contact address the university of adelaide adelaide sa 5005 contact email charles p newland adelaide edu au software requirements current implementation is linked with metronamica land use model http www metronamica nl through manipulation of xml input files and calls to a command line interface use with metronamica is recommended although code may be adjusted for linking with other land use models first year available 2017 program language python 3 program size 20 mb availability and cost open source software downloadable from https github com ursidean enc py3 release git 1 introduction land use change models are used to understand the wide ranging impacts of land use changes including the impact on the rate of greenhouse gas emissions li et al 2017 pogson et al 2016 the balance of agricultural production with ecosystem preservation connor et al 2015 van delden et al 2010 and the influence of land use policy on urban growth berberoglu et al 2016 chaudhuri and clarke 2013 land use changes are caused by many different mutually influential bio physical and socio economic drivers lambin et al 2001 wang et al 2011 which must be captured effectively by land use change models to generate realistic output to model land use changes effectively cellular automata ca were proposed to replicate land use as a dynamic spatial system tobler 1979 this required relaxing the conventional implementation of ca couclelis 1985 to replicate fractal patterns of land use changes consistent with urban evolution white and engelen 1993a doing so led to the development of multiple land use cellular automata luca models which have proliferated due to their simplicity flexibility and intuitiveness santé et al 2010 an important aspect of modelling land use changes is the consideration of spatial and temporal dynamics between different land uses van vliet et al 2013b applications of ca to land use modelling consider the composition of the neighbourhood a strict ca only considers the neighbourhood composition of the geometrically closest set of cells for the consideration of spatial dynamics implemented in numerous luca models li et al 2013 wu 2002 including the popular generic modelling framework sleuth clarke et al 1997 silva and clarke 2002 however other luca models most notably those derived from white and engelen 1993b that use a transition potential as the mechanism for the allocation of land use changes use an extended neighbourhood i e the set of cells within a certain cellular radius for a more detailed consideration of spatial dynamics which requires the use of neighbourhood rules that characterise the influence different land use classes exert on each other relative to proximity while the use of neighbourhood rules allows for a more detailed replication of the spatial dynamics that exist between different land use classes it also increases the number of model parameters significantly often by hundreds blecic et al 2015 garcía et al 2013 luca model application requires appropriate values of these parameters to be determined by means of calibration which involves the initial setting of parameter values the iterative adjustment of these values based on comparison of the model output with observations and the selection of a final parameter set for application to a specific case study for long term scenario analysis newland et al 2018 the difficulty of the calibration processes is generally a function of the dimensionality of the parameter space which is a function of the number and possible ranges parameters can take during the iterative adjustment process neighbourhood rules are the primary contributor to this high parameter dimensionality and hence are often the main calibration parameters of these types of luca models engelen and white 2008 the conventional method of calibration is to manually adjust the neighbourhood rule parameters garcía et al 2013 van delden et al 2012 which is a knowledge and time intensive process given the complexity of the calibration problem the parameter dimensionality is often implicitly reduced when using such an approach based on the modeller s knowledge of the spatial dynamics driving land use changes in a region white et al 1997 and the common forms of neighbourhood rules hagoort et al 2008 additionally metrics can be used to objectively evaluate calibration performance for luca models there are two distinct types of metrics that measure different aspects of calibration performance newland et al 2018 locational agreement metrics the match of pixels between simulated output and data and landscape pattern structure metrics the difference between simulated and observed patterns of land use that infer the realism of land use change processes objective measures of luca model performance have facilitated the development of automatic calibration methods where parameters are iteratively adjusted automatically to improve luca model performance as quantified by the metrics used there are two general approaches to automatically calibrate neighbourhood rules the first approach uses a population based optimisation algorithm blecic et al 2015 garcía et al 2013 newland et al 2018 where a population of solutions is generated i e a number of different sets of neighbourhood rules and adjusted based on some operators to improve the objective performance of the solutions over a number of iterations optimisation approaches are effective at generating multiple possible model parameterisations but are computationally intensive often requiring parallel computing resources for practical implementation blecic et al 2015 newland et al 2018 such approaches also lack transparency which can result in parameters that conflict with process understanding despite producing a model with objectively good performance the alternative automatic calibration approach is efficiency focussed targeted to the spatial dynamic processes in the transition potential model aiming to generate a single set of calibrated neighbourhood rules that is consistent with discursive knowledge for a limited computational budget straatman et al 2004 maas et al 2005 van vliet et al 2013b achievable by a desktop pc as opposed to a supercomputer given the limited availability of supercomputing resources such approaches designated as process specific are valuable as a practical means of automatic calibration despite a specific focus on neighbourhood rules previous process specific methods have not fully utilised discursive knowledge to generate neighbourhood rules consistent with process knowledge van vliet et al 2013b and do not necessarily focus on the most important spatial interactions during calibration straatman et al 2004 previous methods have also only used a single metric of performance not considering the implementation of multiple metrics to capture the two aspects of calibration performance previously discussed and how the competing objectives locational agreement and landscape pattern structure impact on the resulting model performance given the shortcomings of current process specific calibration methods outlined above while maintaining their relative computational efficiency and transparency compared with population based optimisation algorithms this research proposes a semi automatic process specific calibration method the aims of this paper are i to develop a calibration method that utilises process knowledge about meaningful interactions to facilitate efficient automatic calibration that allows for the consideration of both aspects of luca model performance for a limited computational budget ii to investigate the utility of the proposed approach through application to several case studies and iii to obtain insight into the choice of objectives and the impact of the preference on the resultant calibrated model the remainder of this paper is organised as follows to illustrate the complexity of the calibration problem background information about neighbourhood rules is presented in section 2 section 3 presents the proposed approach for calibration and section 4 presents the implementation and computational testing regime used to evaluate it the results are presented and discussed in section 5 and the conclusions of this work are presented in section 6 2 complexity of the calibration problem this section provides explicit details of the complexity of calibrating neighbourhood rules by defining the dimensionality of the neighbourhood rule parameter space for transition potential luca models to do this consider the transition potential land use allocation mechanism illustrated in fig 1 which is described in detail below a transition potential based luca model can be defined as a dynamic process providing a sequence of land use maps x 1 x 2 x t given an initial land use map x o each land use map is defined specifically as the set of land use classes for each cell 1 x t x c t a c c where x c t is the land use class of cell c at time t a is the set of all land use classes and c is the set of all cells for most applications the set a can be partitioned as 2 a a a a p a s where a a is the set of active land use classes i e the classes that are actively allocated by the model generally urban classes such as residential and commercial land use a p is the set of passive classes i e classes that have low transition costs such as natural areas and are allocated after the actively modelled classes and a s is the set of static classes i e classes that occupy a fixed location throughout the simulation such as water bodies or airports the set c can be partitioned following the same convention 3 c c a t c p t c s where c a t is the set of cells containing active land use classes at time t c p t is the set of cells containing passive land use classes at time t and c s is the set of cells containing static classes which do not vary with time the evolution dynamics of the land use is given by the functional relationship 4 x t 1 f θ t n x t v t δ where θ t are parametric maps governing the effect of processes such as soil quality and zoning that influence land use changes which are independent of the land use map n x t is the set of neighbourhood influence values that is dependent on the current state of the land use map v t is a stochastic perturbation term included to capture the system uncertainty and δ is the land use demand assuming the luca model is constrained for each active land use class each of θ t n x t and v t are given for each cell in the land use map for each actively modelled land use class for each time step t i e each year the functional relationship in equation 4 describes how land use is allocated to every cell in the map based on the transition potential which is the potential for each specific cell to support each type of active or passive land use class for each active class the cells with the highest transition potential are selected until the demand is met after which passive land uses are allocated to the remaining cells as these have no specified demand a general form for determining transition potential is 5 t p c i t θ c i t n c i t v t where tp c i t is the potential for cell c to support land use i at time t and n c i t is the neighbourhood potential for cell c to support land use i at time t which is a function of the composition of the cellular neighbourhood of c with each cell in the neighbourhood exerting some influence based on its class and distance from c this has the general form 6 n c i t c d c w i x c t d c c where d c is the set of cells in the neighbourhood of cell c d c c is the linear distance between the cells c and c and w i j d is a neighbourhood weighting parameter that expresses the influence that a cell of land use type j exerts on the potential for land use class i at a linear distance d between the two cells note that in equation 6 the subscripts of the general form of the weight term w i j d are given by j x c t and d d c c neighbourhood rules define the weight values capturing distinct aspects of spatial dynamics based on the type of relationship being described and the distance being considered van vliet et al 2013b this is illustrated in fig 2 showing how weights are defined at different distances for different spatial dynamics it is important to note that the weight values have no physical meaning but derive meaning relative to each other in total the set of neighbourhood interactions of class j on class i at distance d symbolised by i j d are defined by the set 7 p i j d i a a j a d 0 1 d m a x where p is the set of all interactions a a is the set of actively allocated land use classes a is the set of all land use classes and d max is the maximum distance that defines the size of the neighbourhood interactions can be categorised as either self influencing when i and j are equal e g describing the influence of a residential class to attract more residential development or across class interactions when i and j are unequal e g describing the influence of a residential class to attract industrial development all interactions can also be described as either a point influence at distance zero describing the influence of a cell on itself or a neighbourhood tail influence for distances of one or greater describing the remote influence of a class on a cell given these categorisations there are four distinct types of spatial dynamics examples of which are given in fig 2 which are captured by neighbourhood interactions van vliet et al 2013b outlined below 1 inertia points i i 0 p i a a these describe the persistence of a specific land use class i to remain in its present location 2 conversion points i j 0 p i a a j a j i these describe the potential for a transition from one land use class j to a different land use class i at its present location 3 self influence tails i i d p i a a d 1 these describe the influence a land use class i exerts on the same type of land use i that is in the neighbourhood of a cell and 4 cross influence tails i j d p i a a j a j i d 1 these describe the influence a land use class j exerts on a different type of land use i that is in the neighbourhood of a cell due to the four distinct types of interaction the full set of interactions can be partitioned as 8 p p i p p c p p s t p c t where p ip is the set of inertia point interactions p cp is the set of conversion point interactions p st is the set of self influence tail interactions and p ct is the set of cross influence tail interactions the total number of interactions considered is generally given by 9 p a a a each tail interaction can be described by the weight at distance 1 multiplied by a function that describes the decaying neighbourhood tail interaction that is 10 w i j d w i j 1 u d f o r d 1 where u d is the functional form of the neighbourhood rule tail shape the tail shape u d is a pre specified function e g linear exponential starting at u 1 and decaying as d increases resulting in a diminishing effect of a land use class on another with increasing distance as each neighbourhood influence can feature four parameters as shown in fig 2 e g a point weight at distance 0 and a piecewise linear relationship of tail weights for distance 1 a point of inflection and a point where influence is set to zero and as luca models can typically have 20 land use classes where 10 are actively modelled there are minimally 800 parameters that require calibration hence the dimensionality of the neighbourhood rule parameter space is typically very high it is for this reason that neighbourhood rules are the main focus of the calibration of luca models engelen and white 2008 and why the calibration of neighbourhood rules is complex the high dimensionality makes it difficult to know which parameters to adjust to achieve an improved model calibration this problem is further exacerbated by the limited calibration data available as typically there exists only an initial data land use map x ˆ 0 and one or two other data land use maps x ˆ a and x ˆ b for some a and b 1 the high parametric dimensionality also introduces potential issues with equifinality van vliet et al 2016 where the same calibration performance can be achieved by different sets of neighbourhood rules despite the rules not necessarily being consistent with process knowledge 3 proposed approach a conceptual outline of the proposed approach for the semi automatic calibration of neighbourhood rules is shown in fig 3 the approach has been developed with two primary aims the first is the reduction of the dimensionality of the neighbourhood rule parameter space to mitigate the parametric dimensionality issues outlined in section 2 by identifying the key land use interactions for consideration symbolised by the set p this is based on automating and formalising the process of interaction elimination see section 3 1 that is common in manual calibration methods the proposed approach is conducted in an objective transparent and repeatable manner and also allows for the manual inclusion or elimination of interactions the second aim is the computationally efficient calibration of the remaining neighbourhood rule weighting parameters an initial set of neighbourhood rule weighting parameters w initial is generated during the parameter categorisation stage see section 3 2 by introducing a set of meta parameters θ cp θ st and θ ct which express the inter type importance of each interaction type relative to the inertia point interactions e g how important conversion points are compared to inertia points and by limiting the parameters possible values to a finite set of possible categorised values allocating each parameter included for calibration within each interaction group to a category value based on an empirical analysis the neighbourhood rule weighting parameters are subsequently refined in the coarse adjustment stage see section 3 3 to generate w coarse by tuning the meta parameters that control the categorised neighbourhood weighting parameters next during the fine adjustment stage see section 3 4 the neighbourhood rule weighting parameters are individually tuned to generate a set of calibrated weighting parameters w final advantages of the proposed approach are that i calibration is performed efficiently with a minimal number of model simulations i e a computational budget achievable by a desktop pc as opposed to a supercomputer ii the two key aspects of luca model performance are considered i e locational agreement and landscape pattern structure iii the neighbourhood rules generated at the conclusion of each stage are consistent with process understanding and iv manual intervention is possible to include or exclude interactions and to set neighbourhood weighting parameters at any point the remainder of this section presents the details of each stage 3 1 interaction elimination 3 1 1 overview the interaction elimination stage reduces parameter dimensionality by eliminating certain interactions to simplify the subsequent calibration problem reducing the total set p to the smaller set p of important interactions that are identified to be driving land use changes in the region fig 3 the interaction elimination is based on an analysis of the available data making the process objective and repeatable the primary objective of this stage is to determine the set of interactions p composed of 11 p p i p p c p p s t p c t where the subsets p i p p c p p s t and p c t contain only the meaningful neighbourhood interactions typically all inertia point and self influence tail interactions are considered meaningful due to all land uses exhibiting some tendency for persistence and some tendency for self influence in the neighbourhood this means that p ip is equal to p ip and p st is equal to p st and that the reductions in p are achieved in the reduction of conversion point and cross influence tail interactions p cp and p ct the interaction elimination process is summarised in fig 4 where an empirical analysis and significance testing are used for identifying the meaningful interactions in p cp and p ct this is based on an empirical evaluation of the transitions that occur in the calibration data x ˆ 0 and x ˆ a as these data provide the main behaviour the land use model is attempting to capture van vliet et al 2013b the empirical analysis and significance testing are conducted to determine if there is a statistically significant representation of a given land use class for transitions to each active land use class this analysis is undertaken either at the location of the transitioned cells informing the inclusion of a conversion point or in the neighbourhood of the transitioned cells informing the inclusion of a cross influence tail hence the meaningful interactions are such that 12 p c p i j 0 p c p t 0 i j 0 x ˆ t 0 and z 0 i j 0 x ˆ z 0 p c t i j 1 p c t t 1 i j 1 x ˆ t 1 and z 1 i j 0 x ˆ z 1 where t d i j d x ˆ is the empirically derived measure for the representation of land use class j in the neighbourhood of cells at distance d that transitioned to class i see fig 4 and t d is the associated threshold value z d i j d x ˆ is the measure of statistical significance for the representation of land use class j in the neighbourhood of cells at distance d that transitioned to class i and z d is the associated significance level for the interaction elimination the user specifies an empirical analysis threshold for conversion points t 0 and cross influence tails t 1 and a significance threshold for conversion points z 0 and cross influence tails z 1 empirical analysis section 3 1 2 is used to determine the values of t 0 and t 1 and significance testing section 3 1 3 to determine the values of z 0 and z 1 interactions that do not exceed both thresholds i e t 0 and z 0 for a conversion point or t 1 and z 1 for a cross influence tail are eliminated from subsequent calibration the proposed interaction elimination is designed to extract the meaningful interactions based on the available data which is likely limited to a pair of data maps the method is inherently limited by the available data and can only capture what the available data indicate i e intermediate transitions cannot be considered if there are no data to show these which must be considered during application as this has the potential to impact calibration performance blecic et al 2015 the user must therefore consider the expected amount of change over the period considered when setting the thresholds the empirical analysis measures and significance test used for the elimination are described in the next two sections it should be noted that the elimination uses measures to capture the independent influence of individual classes on an active class as this is consistent with the model structure i e neighbourhood rules only capture individual influence of one land use on another so for example because the model uses a neighbourhood rule describing the influence of land use class j on the allocation of class i a measure relating the presence of land use class j in the neighbourhood of class i is used for evaluation higher order combined spatial influences are not considered as these are not sufficiently captured by the transition potential model with such effects only accounted for by the summation of the individual influences mentioned see equation 6 the proposed interaction elimination is conducted based on the available data but given the potential limitations of the data the user has the ability to either include rules that are not identified as statistically significant or to reject rules that are identified as significant but inconsistent with process understanding 3 1 2 empirical analysis measures the evaluation of conversion points is based on a form of the confusion matrix congalton 1991 referred to as a contingency table see table 1 for an example generated by logging the land use class transitions for each cell for x ˆ 0 to x ˆ a see fig 4 in table 1 η i j is the total number of cells that are class i in x ˆ 0 and have transitioned to class j in x ˆ a the value η i i the case where i is equal to j shows the total number of cells that did not change land use class between x ˆ 0 and x ˆ a which provides information about the level of inertia using the contingency table the inertia rate which quantifies the tendency of a land use class to persist can be derived as 13 i r i η i i m 1 n η i m using the contingency table the conversion rate which quantifies the tendency of transitions to a land use class as a function of all transitions to that class can also be derived as follows 14 c r i j η i j m 1 n η m j η i i for the proposed approach the threshold value for the inclusion of a conversion point t 0 is a user defined conversion rate the corresponding empirical measure t 0 for each conversion point is calculated from the contingency table the evaluation of cross influence tails is performed using the enrichment factor of transitioning cells van vliet et al 2013b which expresses the over or under representation of a particular land use class in the neighbourhood of cells that transitioned to a certain class at a certain distance relative to the representation of the neighbourhood class in the entire landscape fig 4 15 e f i j d l o g 10 r i j d n j n where e f i j d is the enrichment factor for the presence of land use class j at distance d in the neighbourhood of cells that transitioned to class i r i j d is the average relative representation of land use class j in the neighbourhood at distance d of cells that transitioned to land use class i n j is the total number of cells of land use class j in the data map x ˆ 0 and n is the total number of cells in the data map x ˆ 0 to assist with interpretation the enrichment factor is log scaled so that values greater than 0 indicate over representation and values less than 0 indicate under representation the threshold function t 1 for the inclusion of a cross influence tail is a user defined log scaled enrichment factor value the corresponding empirical measure t 1 for each cross influence rule is the calculated log scaled enrichment factor it should be noted that the information from the contingency table specifically the conversion rate is used for the analysis of conversion points as opposed to the enrichment factor values at distance zero because the contingency table is more effective at capturing the different conversions in the data though the two measures use the same information the enrichment factor at distance zero can be derived from the information in the contingency table see appendix a the enrichment factor can be less effective at capturing the conversions occurring for example many conversions may be occurring from a certain class to another but the enrichment factor may not suggest over representation because there is a large representation of the class in the landscape large n i erroneously indicating that a conversion point is not required because there is a large representation of the class in the landscape a common example of this are conversions from large relatively passive land use classes such as natural vegetation because they occupy a large area of the landscape and facilitate many conversions to different classes which are meaningful to include 3 1 3 significance test the mann whitney u test mwu test is applied to determine whether the representation of land use class j in transitions to land use class i is statistically significant fig 4 the mwu test requires compiling ranked data sets shown in fig 5 for the neighbourhood at distance one for i the percentage of cells of land use class j in the neighbourhood at distance d for cells transitioning to class i compared to ii the percentage of cells of land use class j at distance d for all cells the mwu test is used to assess whether there is significant variation for data sets i and ii this is formally stated as testing whether a sample drawn from one distribution will be equally likely to be greater or less than a sample drawn from the other this is the null hypothesis h 0 where the alternative hypothesis h a is that the first aforementioned sample will be more likely to be either greater or less than the second for the proposed approach h 0 is that the ranks of the respective relative composition tallies for the two data sets i and ii are statistically equivalent this is shown in fig 5 for case a as the distributions appear fairly similar alternatively h a is that the ranks of the respective relative composition tallies for the two data sets are systematically higher or lower this is shown in fig 5 for case b as the distributions for the transition data set possess a significantly larger percentage of higher cell counts i e 15 of transition cells have 8 cells of class j at distance d equal to 1 as compared to only 2 of all cells have this number at the same distance the mwu test requires the u test statistic to be calculated which requires the two distribution tallies to be combined and ranked from low to high corder and foreman 2014 with the ranked order determined the u test statistic is calculated for the ranks of the distribution of a specific land use class j in the neighbourhood at a distance d of cells that transitioned to land use class i 16 u i j d n i n n i n i 1 2 r s i j d where u i j d is the u test statistic for the presence of land use class j at a distance d for cells that transitioned to land use class i n i is the number of newly allocated cells of class i n is the total number of cells in the landscape and rs i j d is the sum of the ranks of the distribution of the relative composition for the presence of land use class j at a distance d for cells that transitioned to land use class i the u test statistic is also calculated for the ranks of the relative composition of a specific land use class j at a distance d for all cells 17 u c j d n i n n n 1 2 r s c j d where u c j d is the u test statistic for the presence of land use class j at a distance d of all cells c and rs c j d is the sum of the ranks of the distribution of the relative composition for the presence of land use class j at a distance d for all cells c the final u test statistic used is the minimum of the two that are calculated for this proposed application the samples are sufficiently large that a normal approximation can be used for the determination of significance which corresponds to z d in equation 12 18 z i j d min u i j d u c j d x u s u where z d is the z score approximation for the presence of land use class j at a distance d of cells that transitioned to land use class i x u is the mean of the data and s u is the standard deviation of the data the calculated z d must be greater than the user defined z d for the corresponding conversion point or cross influence tail to be included 3 2 parameter categorisation and initialisation the parameter categorisation and initialisation stage further reduces the dimensionality of the calibration problem to facilitate initialising the luca model with a set of neighbourhood rule weighting parameter values for subsequent coarse calibration fig 3 by using a method to efficiently generate neighbourhood rules that are consistent with process understanding the categorisation process achieves this across two steps discussed in detail below the first step is the introduction of a set of meta parameters θ c p 0 θ s t 0 and θ c t 0 which describe the inter type importance of each interaction type p c p p s t and p c t with respect to the inertia point interaction type p i p the meta parameters quantify how important each interaction type is relative to the inertia point interaction type which is assumed to be the dominant process e g the meta parameter θ c p expresses how important the p c p interactions are compared to the p i p interactions for example high values of θ c p mean conversion points exhibit similar influence to inertia points which will likely cause more conversions the meta parameters range from zero to one and are set by the user note that a value of 1 places the interactions at the same level of importance as the inertia point interactions recommended ranges are between 0 and 0 1 for all meta parameters the meta parameters are subsequently tuned in the coarse adjustment stage section 3 3 the second step is the categorisation of the parameters included for calibration within each interaction type this achieves a discretisation of the corresponding weighted influence values w i j d where rather than taking values from a continuum they are restricted to a set of finite values this categorisation establishes a hierarchy of the intra type importance of each interaction for each type of interaction e g how important interaction i j 0 is compared to i k 0 for both i j 0 i k 0 p c p categorisation is based on an empirical analysis of the available data a result of the proposed categorisation method is that the weighting parameters for the interactions p i p p c p p s t and p c t are given by the following representations 19 w i i 0 w i i 0 w i j 0 θ c p 0 w i j 0 w i i d θ s t 0 w i i 1 u d w i j d θ c t 0 w i j 1 u d where θ c p 0 θ s t 0 and θ c t 0 are the meta parameters that are user defined and take values between 0 and 1 and w i j d are the normalised weighting parameters that take values from the discretised parameter space w i j d 1 w i j d k where k is the number of discrete levels so for example if an interaction type has three levels low medium and high the weights take values from the set range w i j d 1 w i j d 2 w i j d 3 the functional dependence of the neighbourhood potential for the proposed meta parameterisation and categorisation is summarised in appendix b the parameter categorisation is based on an empirical evaluation of the available data a point or tail for each interaction type is assigned as follows 20 w i j d w i j d 1 if t k i j d x ˆ i k 1 w i j d 2 if i k 1 t k i j d x ˆ i k 2 w i j d k if i k k 1 t k i j d x ˆ where t k is the empirically determined threshold value for interaction type k i e ip cp st ct d is either 0 or 1 defining either a point or a tail and i k n is the upper threshold value for importance category n different for each interaction type where higher n means higher importance the categorisation of the different interaction types uses the measures previously derived in section 3 1 2 shown in table 2 the p i p and p c p interactions are categorised using measures derived from the contingency table the inertia rate and conversion rate respectively the p s t and p c t interactions are categorised using the enrichment factor values at distance d equal to one thresholds i k for the different groups are set by the user hence given that each interaction type uses the same number of levels the calibration complexity is reduced so that the luca model neighbourhood rules are characterised by three meta parameters and weighting parameter values for each discrete level across all interaction types the model can be initialised by specifying these values and the required thresholds to generate an initial parameter estimate w i n i t i a l 3 3 coarse parameter adjustment the coarse parameter adjustment stage calibrates the neighbourhood weighting parameters at a coarse level to improve luca model performance by tuning the meta parameters that control the categorised neighbourhood weighting parameters introduced in section 3 2 fig 3 the coarse parameter adjustment facilitates the initialisation of the fine parameter adjustment process at a starting point within the parameter space that results in objectively good performance and is consistent with process knowledge model performance is measured by comparing the simulated output with the data using at least two metrics as discussed in section 1 one to quantify locational agreement and another to quantify landscape pattern structure model performance is improved by generating a set of parameters w c p c o a r s e w s t c o a r s e and w c t c o a r s e through the coarse adjustment of the meta parameters θ c p θ s t and θ c t within this stage the parametric representation equation 19 is retained but the meta parameters are varied about the initial settings by using the neighbourhood weighting parameterisation from equation 19 the weights can be expressed as functions of the meta parameters such that 21 w c p w c p θ c p w s t w s t θ i t w c t w c t θ c t using this formulation the initial parameter set is given by 22 w c p 0 w c p θ c p 0 w s t 0 w s t θ s t 0 w c t 0 w c t θ c t 0 as outlined above the coarse level parameter adjustment is performed by adjusting the meta parameters to optimise the objectives of calibration performance formally the goal is to identify a set of meta parameters θ θ c p θ s t θ c t that result in a non dominated calibration performance i e improved performance in one objective cannot be achieved without detrimental performance in the other objective to the bi objective problem 23 max f 1 w θ x ˆ a min f 2 w θ x ˆ a where f 1 corresponds to the locational agreement objective between the calibration data map x ˆ a and the luca model simulation map x a which is a function of w θ and f 2 corresponds to the landscape pattern structure objective measured as the error between the calibration data map x ˆ a pattern and the luca model simulation map x a pattern which is a function of w θ as opposed to determining every possible combination of meta parameter values to identify those that optimise the objective an approximate and more computationally efficient approach is implemented as shown in fig 6 in this approach one meta parameter is sampled at a time within each stage allowing for an explicit consideration of the non dominated solutions subject to the variation of the single meta parameter under consideration within this process user input is required to select the best meta parameter value from the set of solutions this is an important step in the process as it allows for the user to make a judgement as to which meta parameter value is best based on additional subjective criteria e g user experience and or consideration of landscape features not characterised by the objective metrics the selected meta parameter value as shown in fig 6 is then used within the subsequent stages where the other meta parameters are sampled one at a time the final set of meta parameters θ is obtained at the conclusion of the coarse adjustment stage 3 4 fine parameter adjustment while the coarse parameter adjustment stage enables the whole parameter space to be explored to identify promising regions the values individual parameters can take is restricted to a relatively coarse grid as only values of the meta parameters are adjusted the purpose of the fine parameter adjustment stage is to remove this restriction to enable the values of individual parameters that maximise the objectives functions in the vicinity of those from the coarse adjustment stage to be identified fig 3 consequently the fine parameter adjustment stage is initialised using the output of the coarse adjustment stage as this corresponds to a good starting point in the parameter space that has objectively good performance and agreement with process understanding as with the coarse level adjustment locational agreement and landscape pattern structure metrics are used to objectively assess calibration performance as mentioned above the fine parameter adjustment stage considers the neighbourhood weighting parameters on an individual level rather than the meta parameters as was the case in the coarse parameter adjustment stage iteratively refining each to optimise the performance metrics used within the available number of iterations computational budget this is achieved using a line search algorithm as shown in fig 7 because this is an automatic search method that efficiently converges on a parameter value that locally optimises the objectives the specific neighbourhood weighting parameter w i j d corresponding to one of the four interactions groups is assumed to lie within a range for which a certain value optimises the performance metrics used the line search is conducted using the golden section search algorithm kiefer 1953 the over arching structure of the sequential line search algorithm is as follows the working parameter set w is initialised to the values obtained at the conclusion of the coarse calibration as shown then the algorithm loops through the interaction types in order of importance as identified by the user based on case study characteristics generally inertia points followed by self influence tails conversion points and cross influence tails for each type the algorithm sequentially loops through all weighting parameters searching between a set of user defined minimum and maximum neighbourhood weighting parameter values w i j d m i n and w i j d m a x to determine the refined value w i j d f i n e that maximises the specified objective f fine discussed below subject to all other parameters being held constant the working parameter set is updated when the refined variable value is obtained and the procedure continues through the loop until the computational budget is exhausted returning the final set w f i n a l composed of the refined neighbourhood weighting parameters the line search uses a single objective f fine to characterise the calibration performance of the proposed parameter set as discussed previously multiple objectives are required to characterise the performance of luca models so f fine is taken as the weighted sum of the objectives 24 f f i n e w x ˆ i 1 m a i f i w x ˆ where f i is the ith objective a i is the user defined weight given to the ith objective all weight values sum to one and m is the number of objectives as f fine is the sum over both locational agreement and landscape pattern structure error each metric must be transformed into a maximisation objective e g reverse the sign of the metric of minimisation objectives additionally to mitigate potential issues with variable ranges of the metrics the objectives are scaled to values between 0 and 1 using metric ranges as the normalising upper and lower bounds that are defined by the user it is important that the metrics used are appropriately balanced to achieve a trade off between the performance objectives to ensure robust calibration and to prevent over calibration the proposed approach assumes a model structure sufficiently general to capture the major land use change processes in a region this assumption allows for a focus on efficient calibration where all objectives can be combined using a single weighted sum however this does limit the ability to fully explore the trade off between the objectives which can obscure more fundamental modelling issues a notable example would be if a balance between locational agreement and landscape pattern structure cannot be easily achieved white et al 2015 where the calibrated model performs well in either metric but not both this is important to consider as it might suggest more fundamental structural model issues as the case where the luca model does not fully capture major land use change processes identifying and addressing such structural problems are not a primary focus of this work but this is an important issue consider that warrants future research 4 application of proposed approach the utility of the proposed semi automatic calibration method for neighbourhood rules is evaluated using the following computational testing regime the approach is applied to calibrate the neighbourhood rules of four case studies in europe comparing the output obtained based on the preference for different objectives and evaluating the performance against neutral models of landscape change that are used to generate benchmark metric values for calibration performance hagen zanker and lajoie 2008 4 1 land use model the luca model metronamica is used to model land use changes metronamica is a generic constrained ca modelling framework van delden and hurkens 2011 that has numerous global applications wickramasuriya et al 2009 rutledge et al 2008 van delden et al 2011 metronamica is derived from the transition potential model developed by white and engelen 1993b and hence includes neighbourhood rules in the determination of land use changes metronamica follows the convention for the calculation of transition potential equation 4 where the additional processes considered via parametric maps are given by 25 θ c i a c i s c i z c i where a c i is the influence of accessibility s c i is the influence of suitability and z c i is the influence of zoning the specific parameterisation of each component is given in the metronamica documentation riks 2015 4 2 case studies the four case studies used for testing include berlin germany budapest hungary lisbon portugal and madrid spain and their surrounding regions shown in fig 8 these case studies are selected to enable the approach to be tested under a range of different conditions given the variation in physical characteristics e g coastal or inland high or low elevation and rates of urban growth e g low for berlin moderate for budapest and high for lisbon and madrid see appendix c for contingency tables of these locations the corine land use data set haines young et al 2006 is used to generate the land use maps for the case studies each case study uses 15 land use classes eight of which are actively modelled which are reclassified from the 48 corine level 3 land use classes with a 250 m resolution covering a region of 10 000 km2 400 by 400 cells centrally located on the city centre the calibration period for each case study is 1990 2000 and the validation period is 2000 2006 each case study includes major road data for accessibility and elevation and slope data for suitability 4 3 implementation of proposed methodology for each of the case studies the empirical neighbourhood rule calibrator enc software developed as part of this work see software availability section for details is applied as each case study uses the same number of total and active land use classes each is implemented with the same number of possible neighbourhood interactions and possible neighbourhood weighting parameters 4 3 1 interaction elimination to conduct the interaction elimination stage the thresholds presented in table 3 are used for the empirical neighbourhood analysis only conversion points and cross influence tails are eliminated for this proposed implementation because all eight active classes for each case study are expected to exhibit inertia and self influence a conversion rate of 2 5 is used as a minimum threshold as this prevented infrequent and erroneous conversions present in the data such as conversions from fresh water to residential land use from being included in the subsequent calibration to demonstrate the most straight forward application possible the interaction elimination is applied to only find attractive cross influence tails and does not consider repulsive cross influence tails log scaled enrichment factor values at distance one that indicated over representation i e greater than 0 and hence hint at a possible attractive influence between different land uses are used for the empirical analysis of cross influence tails a minimum z limit of 1 96 the 95 confidence limit is used as the statistical significance level for both conversion points and cross influence tails the results of the parameter elimination are summarised in fig 9 which shows the reduction per case study for each interaction type as shown the implemented parameter elimination for the conversion points and cross influence tails resulted in a substantial reduction in the number of interactions considered for each case study the interactions that are included were generally consistent with process knowledge for example it is expected that attractive influences exist between the urban classes residential industry commerce and recreation areas given the limited number of urban classes however there are also examples such as conversion points from agricultural land uses to urban land uses which represent a case where the land use is desirable for urban land uses because it acts as a supply of vacant land such examples highlight the potential benefits of making the calibration procedure semi automatic as discursive knowledge can identify such parameters to remove them from subsequent calibration or ensure these influences are minimised by the calibration procedure 4 3 2 parameter categorisation to conduct the parameter categorisation stage three possible categories low medium and high are used for each case study this allowed for sufficient differentiation of weighting parameter values for each interaction type the threshold values for the categories used for each type of interaction are the same for each case study and are presented in table 4 interactions not exceeding the medium threshold are graded as low the values used are based on knowledge of the case studies and some trial and error analysis to ensure sufficient category variation across the case studies at this stage a neighbourhood shape function that describes the distance decay of the neighbourhood influence is used to aggregate self influence and cross influence tail neighbourhood weighting parameters to make the calibration as efficient as possible an aggregation strategy is used that aggregates tails to a set of key points white et al 1997 including an influence value at distance one an influence value at distance two that is ten percent of the influence at distance one and a point at distance five where influence is set to zero using this strategy each self influence and cross influence tail is represented by a single weighting parameter at distance one reducing the complexity of the calibration problem to generate the initial neighbourhood weighting parameters the inertia point parameters are set such that high inertia is assumed to have double the influence of medium inertia and medium inertia is assumed to have double the influence of low inertia using values of 1 000 500 and 250 for high medium and low inertia respectively the initial meta parameters used are presented in table 5 the values used are based on the case studies which are mostly characterised by persistence resulting in the highest values for the self influence tail meta parameter and lower values for the conversion point and cross influence tail meta parameters 4 3 3 coarse parameter adjustment the coarse parameter adjustment stage is conducted using the ranges and step sizes for each meta parameter given in table 5 the order of the meta parameter sampling is θ st θ cp and θ ct ordered by the impact i e value range on the output metrics obtained the ranges are determined based on discursive knowledge and some trial and error analysis with values outside the ranges given in table 5 resulting in poor performance in both objectives to evaluate locational agreement two variations of cohen s kappa are used fuzzy kappa fk developed by hagen zanker 2009 and fuzzy kappa simulation fks developed by van vliet et al 2013a as both metrics are derivatives of cohen s kappa they measure the observed agreement between two categorical data sets corrected for the agreement expected from random allocation of the given class sizes with fuzziness allowing for the consideration of partial agreement of location and class the major distinction between the two metrics is that fk considers the entire land use map for the calculation of agreement whereas fks only considers the transitioned cells both metrics are used because the variation in the measurement of agreement can impact the results obtained newland et al 2018 to measure landscape pattern structure the error of the simulated and observed clumpiness for the actively modelled land use classes is used clumpiness is a measure of the proportional deviation of the proportion of like adjacencies from that expected under a spatially random distribution for a specific land use class mcgarigal 2014 as clumpiness is measured at the class level it requires aggregation to a single value previous calibration methods that used the average class level clumpiness newland et al 2018 found this is not an ideal aggregation strategy because it can over emphasize relatively minor land use classes that occupy a relatively small amount of the total landscape hence the absolute area weighted clumpiness error awce of the actively modelled land use classes is used as this provides a better landscape centric perspective by emphasizing the classes that occupy the greatest area within the region for the calibration three combinations of metrics are used shown in table 6 each combination includes a metric for locational agreement and landscape pattern structure fk and fks are included together in set three to provide further insight into the relationships between these two objectives the use of different metrics also allows for an exploration of the influence this has on the resultant model output during the coarse adjustment stage two types of behaviour are observed in the objective space which dictates the selection of the meta parameter value these are shown in fig 10 the first type of behaviour is convergent shown in plot a for the fks and awce values for the range of θ st values sampled for the madrid case study as shown there is a meta parameter value that produces both the best fks and awce value which makes selecting the meta parameter value straightforward the second type of behaviour observed is a trade off see newland et al 2018 shown in plot b for the fks and awce values for the range of θ st values sampled for the berlin case study as shown improved performance in fks results in reduced performance in awce as the error increases in this case selection requires further interpretation from the user as the selection of a meta parameter value requires a preference for a certain trade off between objectives which impacts the fine parameter adjustment starting position and hence the resultant final output 4 3 4 fine parameter adjustment the fine parameter adjustment stage of calibration uses the combinations of metrics listed in table 6 the resultant output obtained from the fine parameter adjustment stage is heavily influenced by the weight and ranges used for each metric allowing the user to preference a certain objective an example is shown in fig 11 for the trajectory of the metrics whilst conducting the fine parameter adjustment for the lisbon case study using the objectives fk and awce as shown the method is quite sensitive to the preference for certain metrics and given the same starting point a preference for either locational agreement landscape pattern structure or a balance between the two has a large impact on both the trajectory of the fine calibration and the final metric values obtained given the possible variation that can be achieved three tests are conducted for each set of metrics for each case study one test that purely focuses on improving locational agreement la one test that purely focuses on improving landscape pattern structure lps and one test that balances improvements in both objectives the weightings used for each test are summarised in table 7 as shown total preference for a specific objective is achieved by weighting the other objective s as zero essentially making the problem single objective this is to highlight potential issues with using a single objective as this will likely result in over calibration it should be noted that fk and fks are evenly weighted for the case where both are used and there is preference for locational agreement or a balanced preference as noted in section 3 4 the metrics are scaled to between 0 and 1 by using practical ranges that are defined by the user the selected ranges are partially informed by the results of the meta parameter analysis but some trial and error is required particularly in the cases with a balanced preference to achieve the best possible objectives for different case studies 4 4 computational tests to evaluate the utility of the proposed approach as objectively as possible the calibration and validation performance obtained using the approach is compared with that achieved using two benchmark performance models that replicate common urban growth strategies these models include the random constraint match rcm neutral model riks 2011 which generates reference maps characterised by a speckled distribution of small clusters of each land use class and the growing clusters gc neutral model van vliet et al 2013b which generates reference maps characterised by agglomerated distributions of large clusters of each land use class generated using the default neighbourhood rule settings in metronamica if the performance of the calibrated luca model exceeds that of the benchmark models the luca model can be considered to have captured the processes driving land use change correctly hagen zanker and lajoie 2008 for this research ten reference maps are generated for each case study for both the calibration and validation period and reference metrics calculated for each reference map to evaluate the output 50 model replicates with different luca model random seeds are run with the set of neighbourhood weighting parameters obtained at the end of the proposed calibration approach for the calibration 1990 2000 and validation 2000 2006 periods for each simulated output map a number of performance metrics are calculated first the average metrics of the simulated output are compared with the average benchmark metrics to determine whether the benchmarks are outperformed in this case this corresponds to the average fk or fks values for the simulated output being greater than the average benchmark values and the average awce value being less than the average benchmark values the metric values obtained are then compared with the corresponding metrics for the benchmark models using welch s t test of statistical significance welch 1947 a method of comparing two independent samples with varying size and variance that obey parametric assumptions at the 95 confidence limit to determine whether the average of the output metrics is significantly better or worse than the benchmark metrics obtained from a statistical perspective 5 results and discussion this section presents the results of the application of the computational tests outlined in section 4 4 the results demonstrate how to determine a final calibrated model for each case study based on the different configurations tested to do this first an objective evaluation is conducted for the calibration and validation periods in comparison to the benchmark models the performance is summarised in figs 12 and 13 respectively with green colouring indicating that performance of the models calibrated with the proposed approach is statistically significantly better at the 95 confidence limit than that of both benchmark models yellow that model performance is superior to one benchmark and inferior to the other and red that model performance is inferior to both benchmarks in figs 12 and 13 the average metric value across the 50 model replicates is shown as there tended to be limited spread across the different replicates see box plots in appendix d configurations that outperform all benchmarks are then evaluated for their physical plausibility first the simulated output maps are compared with the data using visual interpretation at this stage if multiple configurations have resulted in superior performance a solution is selected that produces the simulated output most consistent with process knowledge the parameters of this solution are then evaluated against discursive knowledge to determine if the calibrated model is consistent with expectation 5 1 objective evaluation 5 1 1 calibration performance the performance of the different configurations applied to the case studies for the calibration period is summarised in fig 12 as shown the performance is generally significantly better than the benchmarks for the calibration period irrespective of the metrics used and the preference for a particular objective reflected by a majority of the cells being green this highlights the overall robustness of the proposed approach a detailed discussion of the impact of the choice of metrics and preference on the robustness of the proposed approach is given in the subsequent sections the performance across all three metrics fk fks and awce is particularly good when locational agreement and landscape pattern structure are balanced as there are only two cases where both benchmarks are not exceeded significantly fig 12 these results suggest that the proposed approach can identify parameter combinations that result in modelled land use dynamics that successfully balance locational agreement and landscape pattern structure when locational agreement is favoured during the fine adjustment stage values of fk and fks are generally higher than when landscape pattern structure is favoured or a balanced approach is used as shown in fig 12 however this comes at the expense of landscape pattern structure accuracy the performance of which deteriorates i e values of awce are increased to the point where values are only significantly better than all benchmark values in half the results this indicates that it is difficult to achieve results that satisfy both locational agreement and landscape pattern structure when only locational agreement is considered during the fine adjustment process and that a balanced approach is needed to achieve acceptable performance for both locational agreement and landscape patter structure see section 5 1 1 which is also consistent with the findings of newland et al 2018 values of fks are significantly better than all benchmarks for all case studies and objective function combinations when locational agreement is favoured during the fine adjustment process however somewhat surprisingly this is not the case when fk is used as the performance metric as can be seen from fig 12 when fks and awce are used as objectives fk values are only significantly better than all benchmarks for the madrid case study and for berlin and budapest are significantly worse than both benchmarks this likely occurs because the use of fks as the sole objective places an over emphasis on capturing the transitions that are present in the data with insufficient weight given to inertia this is especially important for cases where there is low or moderate growth such as berlin and budapest however in cases with high growth such as madrid this impact is less pronounced when there is a preference for landscape pattern structure during the fine adjustment process values of awce are superior i e less to those obtained when there is a preference for a different objective as shown in fig 12 however the singular focus on landscape pattern structure has a detrimental impact on the metrics used to measure locational agreement this is most noticeable for the fk values where both benchmarks are only exceeded in three cases all for the same case study budapest 5 1 2 validation performance the general performance of the different configurations applied to the different case studies for the validation period is summarised in fig 13 as shown the results for the validation period generally follow the same trends as for the calibration period in that a balanced preference between locational agreement and landscape pattern structure results in the best overall results while solely favouring locational agreement or landscape pattern structure results in a deterioration in performance of the other objective to the point that it is much less likely that corresponding benchmarks are exceeded specifically when a balanced approach is used in conjunction with either fk and awce or fk fks and awce as objectives validation performance is significantly better than that of all benchmarks for all metrics for three of the four case studies considered as well as outperforming both benchmarks in most cases for the other case study and only performing significantly worse than both benchmarks in one out of twenty four cases i e fk for budapest when a balanced approach is used in conjunction with fk and awce as objectives this suggests that the proposed approach is capable of generating results for both calibration and validation periods that perform significantly better at the 95 confidence level over 50 replicates for all three performance metrics in almost all experiments provided both locational agreement and landscape pattern structure are appropriately balanced during the fine adjustment process and fks is not used as the sole objective quantifying locational agreement while the validation performance of the models calibrated is significantly better than that of both benchmarks when an appropriate balance of objectives and preferences is used validation performance is generally worse than calibration performance as can be expected given that performance is tuned to the calibration data this deterioration in performance is particularly pronounced for the case studies experiencing less growth i e berlin and budapest and when fks is used as the sole objective for locational agreement this can be explained by the fact that fks only focuses on transitioned cells which can result in an over emphasis on capturing the small number of cells that do in fact transition in low growth cases during the calibration period at the expense of capturing inertia 5 2 simulated output evaluation this section presents an evaluation of a simulated output map for a configuration that performs statistically significantly better than the benchmarks for each case study the simulated output maps are compared with the data via visual inspection to determine if the resultant simulated output is sufficiently realistic further verifying the quality of the calibrated model overall the results for the four case studies demonstrated that the proposed approach produced realistic output maps provided an appropriate balance between locational agreement and landscape pattern structure is achieved as shown in figs 14 17 and discussed below fig 14 shows the similarity between the simulated output and the data for the berlin case study resulting from a balanced preference with fk and awce as the objectives the simulated output appears fairly similar to the data however there is some variation between the two maps with the larger amount of residential red cells allocated in the western region of the simulated output map as shown in fig 14c also the simulated map for industry commerce fig 14d tends to show slightly larger clusters in the simulated output compared to the data the similarity between the simulated output and the corresponding data for the budapest case study is shown in fig 15 when a balanced preference is used with fk fks and awce as the objectives in the budapest case study moderate land use change has taken place over the calibration period and the results obtained show the model captured the inertia well for the changes that did occur the model did not always allocate the transitions to the correct locations but the size of the simulated clusters resembles the data this is shown in fig 15 as there is good agreement between the patterns and locations of the urban classes residential red and industry commerce purple as shown in fig 15c and d as well as recreation areas brown this is also true for the major agricultural class arable land yellow there is some variation in the amount of interspersion of the classes natural areas light green and forest dark green with the simulated output producing more clumped areas though this is quite minor for the lisbon case study the simulated output for a balanced preference with fk fks and awce as the objectives is shown in fig 16 as shown the general behaviour of the simulated output is sufficiently consistent with the data though there are some differences in the newly allocated land use classes the simulated residential locations tend to fill in space in the urban core creating larger clusters than in the data this is shown in fig 16c as the red cells tend to be at the edge of the existing residential area whereas the blue tends to be within the existing residential area the simulated cluster size of the industry commerce area resembles the data and while the allocation takes place in similar areas exact matches were not often found as shown in fig 16d also the high degree of interspersion of the different agricultural and natural land use classes has resulted is a noticeable amount of the class pastures light green interspersed amongst a region of arable land dark yellow in the centre of the map for the simulated output that is not present in the data the consistency between the simulated output and corresponding data for the madrid case study is shown in fig 17 for a balanced preference with fk fks and awce as the objectives as shown there is reasonably good agreement for the expansion of the existing central urban region composed of residential red industry commerce purple and recreation areas brown the main variation is that the urban classes in the simulated output appear slightly more clumped than in the data as shown in fig 17c and d there tends to be a more speckled distribution of red cells whereas the blue cells tend to be at the fringes of the existing urban region it is worth highlighting that for the low and moderate growth case studies berlin and budapest respectively there was only one configuration that performed statistically significantly better than the benchmarks meaning only a single solution map required visual inspection and interpretation however for the high growth case studies lisbon and madrid more configurations resulted in superior benchmark performance for these situations visual comparison of the simulated output is important to determine the best performing calibrated model an example of this is presented in fig 18 which shows another configuration for the madrid case using a balanced preference with fk and awce as objectives fig 18b compared with the data fig 18a and the solution that was selected fig 18c as shown there is a large variation in the output obtained most notably the alternate solution fig 18b results in a large clustered formation of the class industry commerce purple in the north region of the map that does not appear in the data and as such is less realistic than the simulated output for the other configuration this highlights the sensitivity of the output to the configuration used and illustrates why visual inspection is beneficial as further interpretation of the output that is not captured with objective assessment alone 5 3 parameter analysis this section presents an analysis of the parameters for the four solutions corresponding to the best objective performance and most realistic simulated output the parameter analysis is used to further verify that the obtained solutions are consistent with process understanding in general the parameters obtained were consistent with expectation because the neighbourhood rules were parameterised to generate shapes that were consistent with process knowledge however there were cases where rules were included as discussed previously in section 4 3 1 that were not necessarily consistent with process knowledge which could be addressed with additional manual intervention in the parameter elimination stage given the variation in the interaction elimination across the case studies and the large number of parameters that were calibrated the parameter analysis is focussed on the parameters that were included in each case study the inertia points and self influence tails the final parameter values for each case study are presented in appendix e a comparison plot of the inertia point parameters obtained for the analysed solutions is presented in fig 19 ordered by class and case study as shown for each case study the classes that exhibit the highest inertia tend to be the urban classes residential industry commerce and recreation areas which is consistent with expectation as such classes have high transition costs the land use class arable land which essentially serves as a vacant class to facilitate transitions exhibits the lowest inertia for each case which is expected as such land uses tend to have the lowest transition costs fig 20 shows a comparison of the self influence tail parameters i e the influence of class i in the neighbourhood of class i for the different solutions for each case study the self influence tails exhibit behaviour that is also consistent with expectation the urban classes exhibit a higher degree of self influence across the different case studies agricultural pastures permanent crops agricultural areas classes exhibit less self influence and the class arable land exhibits virtually no self influence a trend observed across the case studies is that the higher growth cases lisbon and madrid exhibit higher self influence for the class residential than the low and moderate growth cases berlin and budapest this would be expected as higher growth cases will have more newly allocated residential cells which will require a stronger attraction to ensure they are allocated near the existing residential region hence based on the parameter evaluation the calibration method was able to generate parameters that are consistent with expectation 6 summary and conclusions transition potential luca models use neighbourhood rules to replicate the spatial dynamics that drive land use changes in a region neighbourhood rules must be effectively calibrated for model application as these are the main calibration parameters of such models due to their impact on parameter dimensionality this paper presents a semi automatic calibration method that integrates objective analysis with discursive input to facilitate efficient calibration of neighbourhood rules within a limited computational budget achievable using a desktop pc the method first reduces the complexity of the calibration problem and then calibrates the remaining neighbourhood rules in a computationally efficient manner based on a set of metrics that quantify the two key aspects of luca model performance locational agreement and landscape pattern structure the utility of the proposed approach was demonstrated via application to four european case studies with varying physical characteristics and rates of growth for each case study the method was implemented with a focus on a certain objective either locational agreement landscape pattern structure or a balance between the two based on a statistical analysis of the simulated output metrics compared with metrics obtained from two benchmark models and consideration of the simulated output maps and parameters optimal performance required using a balanced objective preference for this research the best performance for the low growth case study berlin was achieved when using a balanced approach between two objectives fuzzy kappa and area weighted clumpiness error for the moderate budapest and high lisbon and madrid growth cases performance was maximised when using a balanced approach between three objectives fuzzy kappa fuzzy kappa simulation and area weighted clumpiness error suggesting that locational agreement is best quantified by balancing between the agreement of transitions and the agreement of the entire land use map this research demonstrates the efficiency that the integration of process knowledge affords process specific calibration methods the results suggest further improvements in efficiency and the quality of the final output could be achieved by utilising additional process knowledge with the elimination or inclusion of certain neighbourhood rules and the input of more complex neighbourhood rule shapes the results also suggest potential improvement could be achieved by adjusting the calibration objectives as discussed in section 5 2 there tended to be over clustering of the resultant output for urban regions hence a different aggregation strategy for combining the class level clumpiness errors may improve the results also additional metrics could be used during the automatic calibration procedure that emphasize different pattern aspects such as the fractal dimension or edge density mcgarigal 2014 which were only captured by visual inspection for the case study application the demonstrated application shows that the proposed approach is a step towards efficient luca model calibration with the potential to facilitate more widespread use of luca models to support scenario and policy analysis the efficiency that is achieved with a process specific method also has the potential to increase the efficiency of more computationally demanding approaches such as optimisation this would further reduce the computational demands of such approaches as has been demonstrated with the use of meta modelling by şalap ayça et al 2018 integrating a process specific method with optimisation would have the added benefit of integrating further process knowledge into such methods potentially improving the applicability of the resulting models acknowledgements the authors wish to acknowledge the financial support from the bushfire and natural hazards cooperative research centre made available by the commonwealth of australia through the cooperative research centre program appendix a derivation of empirical measures this appendix provides a detailed derivation of the empirical measures used in the interaction elimination and parameter categorisation and initialisation sections of the proposed approach the link between data from the contingency table and the enrichment factor at distance zero is also shown the generation of the contingency table requires an analysis of the state of each cell in two data maps a1 s i j x ˆ 0 x ˆ a c c x ˆ 0 c i x ˆ a c j where s i j logs the state of the land use class in the cell c of interest at the start and end of the calibration period populating the contingency table requires evaluating the state transition for each possible combination of land use classes for the entire land use map a2 η i j s i j where η i j is the total numbers number of cells that are class i in x ˆ 0 and class j in x ˆ a the case where i does not equal j corresponds to a transition from i to j and the case where i is equal to j η i i shows the total number of cells that did not change land use class between x ˆ 0 and x ˆ a with these values determined the contingency table is populated such that table a1 example contingency table populated by logging the land use class in each cell between two maps table a1 x ˆ a a 1 a 2 a n x ˆ 0 a 1 η 1 1 η 1 2 η 1 n a 2 η 2 1 η 2 2 η 2 n a n η n 1 η n 2 η n n from the contingency table the empirical measures inertia rate and conversion rate can be calculated the inertia rate which quantifies the tendency of a particular class to persist is calculated by a3 i r i η i i m 1 n η i m the conversion rate which quantifies the tendency of transitions to a particular class as a function of all transitions to that class is calculated by a4 c r i j η i j m 1 n η m j η i i to calculate the enrichment factor requires evaluating the composition of the neighbourhoods of cells that transitioned the size of the neighbourhood is defined by a radius d max the maximum allowable distance between two cells for inclusion in the neighbourhood the neighbourhood is divided into a set of discrete unit distance rings i e distances of 1 2 3 based on the nearest unit distance between the neighbourhood cell and the central cell this is shown in fig a1 for a maximum cellular distance of three cells cells are allocated to the nearest unit distance neighbourhood ring so the neighbourhood of distance 0 comprises 1 cell at the location of the transition distance 1 comprises a total of 8 cells at distances of 1 and 1 41 cell lengths from the centre the neighbourhood of distance 2 comprises a total of 12 cells at distances 2 and 2 24 cell lengths from the centre and the neighbourhood of distance 3 comprises a total of 8 cells at distances 2 83 and 3 cell lengths from the centre fig a1 example delineation of a neighbourhood of maximum distance three cell into a set of unit distance rings for counting neighbourhood composition fig a1 following the subdivision of the neighbourhood into discrete unit distance rings the number of cells of each land use class in each unit distance ring in x 0 is tallied a5 n i j d c d d c x 0 c j where n i j d is the number of cells of land use class j in the neighbourhood of distance d of a cell that transitioned to land use class i and d d c is the set of cells in the neighbourhood of cell c at distance d this tally of absolute neighbourhood representation of a land use class at a certain distance from cells that transitioned to a certain land use class is then converted to a relative representation expressing the absolute representation as the percentage of the maximum number of possible cells in the neighbourhood e g for distance one 1 cell in the neighbourhood is equivalent to a relative representation of 0 125 2 cells in the neighbourhood is equivalent to a relative representation of 0 250 a6 r i j d n i j d n d where r i j d is the relative neighbourhood representation of the neighbourhood of distance d occupied by land use j for cells that transitioned to land use class i and n d is the total number of cells in the neighbourhood of distance d from this the average relative representation is computed for each cell that transitioned to a particular class a7 r i j d 1 c i i c i r i j d where r i j d is the average relative representation of land use class j in the neighbourhood at distance d of cells that transitioned to land use class i and c i is the set of cells that transitioned to land use class i the enrichment factor is then computed by a8 e f i j d l o g 10 r i j d n j n where ef i j d is the enrichment factor for the presence of land use class j in the neighbourhood at a distance d of cells that transitioned to land use class i n j is the number of cells of the neighbourhood class j in the land use map x ˆ 0 and n is the total number of cells in the land use map for ease of interpretation enrichment factor values are log scaled by a factor of ten with negative values indicating under representation and positive values indicating over representation it is possible to determine the enrichment factor value at distance 0 by using the contingency table as the average relative representation can be calculated from the contingency table by a9 r i j 0 η j i m 1 n η m i η i i hence it is possible to derive the enrichment factor value at distance zero which provides information about conversions using the contingency table however this highlights why the conversion rate can be more effective at capturing the different conversions that are occurring in the data for example many conversions may be occurring from a certain class to another but the enrichment factor may not suggest over representation indicating that a conversion point is not required because there is a large representation of the class in the landscape a common example of this are conversions from large relatively passive land use classes such as natural vegetation because they occupy a large area of the landscape and facilitate many conversions to different classes which are meaningful to include appendix b functional dependency of parameter categorisation scheme this appendix details how the proposed meta parameterisation and categorisation detailed in section 3 2 control the relative influence of the different interactions types and the functional dependence of the neighbourhood potential of the normalised weighting parameters within each interaction type the proposed approach introduces three meta parameters θ cp θ st θ ct that describe the inter type importance of each type of interaction p c p p s t and p c t with respect to the inertia point interaction type p i p and a quantisation scheme that describes the intra type importance within each interaction type the impact of this parameter representation on the neighbourhood potential is given by b1 n c i t n c i t i p θ c p n c i t c p θ s t n c i t s t θ c t n c i t c t where n c i t k is the neighbourhood potential of interaction type k for cell c to support land use type i at time t given the implemented parameterisation the individual interaction type terms can be defined as below for the inertia points b2 n c i t i p w i i 0 i f x c t i 0 o t h e r w i s e for the conversion points b3 n c i t c p w i x c t 0 0 i f x c t i o t h e r w i s e for the self influence tails b4 n c i t s t c d c i x c t p i t w i i d c c for the cross influence tails b5 n c i t c t c d c i x c t p c t w i x c t d c c appendix c contingency tables this appendix contains contingency tables for the data maps for all case studies for the calibration 1990 2000 and validation 2000 2006 periods table c1 berlin contingency table 1990 2000 table c1 luc luc map 2000 nat arl per pas agr res i c rec for r r por air m d fre mar tot map 1990 nat 2440 0 0 13 0 6 7 0 188 0 0 0 21 1 0 2676 arl 8 51812 148 1482 27 601 379 127 198 0 0 0 31 5 0 54818 per 0 1285 327 31 0 28 0 19 7 0 0 0 0 0 0 1697 pas 13 773 0 12937 32 23 5 3 39 0 0 0 0 18 0 13843 agr 0 45 0 151 1931 14 8 0 34 0 0 0 9 3 0 2195 res 4 4 0 3 0 17206 5 15 16 0 0 0 2 0 0 17255 i c 0 0 0 2 0 0 2033 0 0 0 0 0 0 0 0 2035 rec 0 0 0 0 0 9 0 1572 0 0 0 0 0 0 0 1581 for 111 8 0 4 9 58 30 0 58301 0 0 1 65 17 0 58604 r r 0 0 0 0 0 0 0 0 0 201 0 0 0 0 0 201 por 0 0 0 0 0 0 0 0 0 0 6 0 0 0 0 6 air 12 0 0 0 0 0 0 0 4 0 0 559 0 0 0 575 m d 20 5 0 0 0 0 0 0 8 0 0 0 282 5 0 320 fre 5 0 0 0 0 0 0 0 0 0 0 0 0 4189 0 4194 mar 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 tot 2613 53932 475 14623 1999 17945 2467 1736 58795 201 6 560 410 4238 0 160000 table c2 berlin contingency table 2000 2006 table c2 luc luc map 2006 nat arl per pas agr res i c rec for r r por air m d fre mar tot map 2000 nat 2258 7 0 78 96 0 2 11 150 4 0 0 3 4 0 2613 arl 22 52786 40 440 267 127 40 98 58 12 0 5 32 5 0 53932 per 0 41 433 0 0 1 0 0 0 0 0 0 0 0 0 475 pas 17 270 0 14293 24 4 6 2 5 0 0 0 0 2 0 14623 agr 28 2 0 2 1943 0 3 6 0 5 3 0 7 0 0 1999 res 15 14 0 34 0 17732 102 25 0 8 1 0 14 0 0 17945 i c 19 15 0 7 0 39 2347 6 2 11 0 0 21 0 0 2467 rec 0 0 0 0 0 19 0 1717 0 0 0 0 0 0 0 1736 for 80 2 0 0 8 31 35 13 58594 8 0 3 20 1 0 58795 r r 0 0 0 0 0 21 8 0 0 172 0 0 0 0 0 201 por 0 0 0 0 0 0 0 0 0 0 6 0 0 0 0 6 air 52 0 0 30 0 8 4 18 0 0 0 448 0 0 0 560 m d 5 1 0 10 12 5 21 9 7 0 0 0 338 2 0 410 fre 0 1 0 4 0 0 0 0 6 0 0 0 0 4227 0 4238 mar 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 tot 2496 53139 473 14898 2350 17987 2568 1905 58822 220 10 456 435 4241 0 160000 table c3 budapest contingency table 1990 2000 table c3 luc luc map 2000 nat arl per pas agr res i c rec for r r por air m d fre mar tot map 1990 nat 7226 31 0 37 0 5 0 0 2522 0 0 0 6 9 0 9836 arl 202 77735 329 1256 303 107 102 43 258 61 0 3 85 112 0 80596 per 60 356 3460 51 37 6 12 0 13 7 0 0 0 0 0 4002 pas 100 465 3 8265 21 25 22 7 17 0 0 0 2 6 0 8933 agr 55 36 6 73 9522 22 6 0 36 0 0 0 2 2 0 9760 res 6 4 0 16 0 13547 16 11 1 3 0 0 3 7 0 13614 i c 1 0 0 0 0 0 2213 1 0 1 0 0 1 0 0 2217 rec 4 0 0 0 0 3 3 2242 2 0 0 0 0 0 0 2254 for 1503 10 0 15 15 0 0 0 23426 5 0 0 4 5 0 24983 r r 0 0 0 0 0 0 0 0 0 271 0 0 0 0 0 271 por 0 0 0 0 0 0 0 0 0 0 58 0 0 0 0 58 air 0 0 0 0 0 0 0 0 0 0 0 415 0 0 0 415 m d 6 0 0 14 1 0 0 0 0 0 0 0 189 0 0 210 fre 0 0 0 2 4 0 0 3 0 0 0 0 0 2842 0 2851 mar 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 tot 9163 78637 3798 9729 9903 13715 2374 2307 26275 348 58 418 292 2983 0 160000 table c4 budapest contingency table 2000 2006 table c4 luc luc map 2006 nat arl per pas agr res i c rec for r r por air m d fre mar tot map 2000 nat 7001 134 20 316 129 20 3 1 1488 0 0 0 10 41 0 9163 arl 351 76176 598 318 223 195 105 19 425 13 0 1 42 171 0 78637 per 11 220 3435 14 86 12 2 3 15 0 0 0 0 0 0 3798 pas 228 335 32 8825 86 51 54 35 61 0 4 0 8 10 0 9729 agr 96 1546 32 308 7221 362 80 81 166 0 0 0 1 10 0 9903 res 0 15 0 5 31 13609 44 3 3 3 0 0 0 2 0 13715 i c 0 23 0 15 0 25 2294 13 2 2 0 0 0 0 0 2374 rec 1 5 0 0 0 89 8 2194 1 0 0 0 0 9 0 2307 for 914 36 13 4 34 3 8 5 25251 0 0 0 0 7 0 26275 r r 0 1 0 0 3 0 12 0 0 332 0 0 0 0 0 348 por 0 0 0 0 0 0 7 0 0 0 51 0 0 0 0 58 air 0 2 0 0 0 0 0 0 0 0 0 416 0 0 0 418 m d 0 9 0 13 0 9 2 6 0 0 0 0 236 17 0 292 fre 24 2 0 0 11 0 1 2 2 0 0 0 3 2938 0 2983 mar 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 tot 8626 78504 4130 9818 7824 14375 2620 2362 27414 350 55 417 300 3205 0 160000 table c5 lisbon contingency table 1990 2000 table c5 luc luc map 2000 nat arl per pas agr res i c rec for r r por air m d fre mar tot map 1990 nat 5884 111 43 1 99 232 97 83 2591 31 0 0 74 7 1 9254 arl 236 15304 88 186 154 259 105 0 263 9 0 7 19 11 0 16641 per 5 312 9558 0 32 34 20 8 17 0 0 0 8 0 0 9994 pas 14 1649 0 392 6 2 10 0 2 0 0 0 0 4 2 2081 agr 196 366 262 0 30873 1094 241 29 271 44 0 0 43 0 0 33419 res 0 0 0 0 0 5372 27 0 0 7 0 2 2 0 0 5410 i c 0 0 0 0 0 15 870 11 0 5 0 0 0 0 0 901 rec 0 0 0 0 0 0 4 397 0 4 0 0 0 0 0 405 for 3794 563 70 0 335 132 120 69 32769 24 0 0 65 5 5 37951 r r 0 0 0 0 0 0 0 0 0 21 0 0 0 0 0 21 por 0 0 0 0 0 0 0 8 0 0 103 0 0 0 0 111 air 0 0 0 0 0 0 0 0 0 0 0 223 0 0 0 223 m d 20 0 4 0 1 2 4 0 2 0 0 0 251 0 0 284 fre 53 1 0 0 0 0 0 0 0 0 0 0 0 829 0 883 mar 2 0 0 0 0 2 16 4 0 0 10 0 0 0 42388 42422 tot 10204 18306 10025 579 31500 7144 1514 609 35915 145 113 232 462 856 42396 160000 table c6 lisbon contingency table 2000 2006 table c6 luc luc map 2006 nat arl per pas agr res i c rec for r r por air m d fre mar tot map 2000 nat 6900 119 10 20 335 194 116 73 2327 30 0 0 53 25 2 10204 arl 212 16604 20 775 380 167 42 0 91 11 0 0 3 1 0 18306 per 33 111 9272 7 385 179 18 1 16 0 0 0 0 3 0 10025 pas 4 197 0 352 25 0 0 0 0 0 0 0 0 1 0 579 agr 374 216 71 6 28463 1560 97 39 629 23 0 0 17 0 5 31500 res 11 2 7 0 33 7006 42 25 7 8 0 0 2 0 1 7144 i c 19 0 4 1 17 17 1435 12 0 1 1 0 5 0 2 1514 rec 0 0 1 0 1 11 8 579 0 0 0 9 0 0 0 609 for 3660 83 18 0 256 87 33 72 31659 26 0 0 18 2 1 35915 r r 0 0 0 0 1 1 0 0 0 143 0 0 0 0 0 145 por 0 0 0 0 0 0 1 0 0 0 112 0 0 0 0 113 air 0 0 0 0 0 2 0 0 0 0 0 230 0 0 0 232 m d 12 11 0 0 13 32 14 0 6 2 0 0 372 0 0 462 fre 0 4 0 0 0 0 0 0 0 0 0 0 0 852 0 856 mar 1 15 0 0 1 2 1 5 0 0 0 0 0 0 42371 42396 tot 11226 17362 9403 1161 29910 9258 1807 806 34735 244 113 239 470 884 42382 160000 table c7 madrid contingency table 1990 2000 table c7 luc luc map 2000 nat arl per pas agr res i c rec for r r por air m d fre mar tot map 1990 nat 46821 268 3 0 50 1051 316 164 357 31 0 12 161 20 0 49254 arl 1540 55265 30 0 383 1492 1059 121 32 70 0 92 351 26 0 60461 per 50 18 4278 0 0 29 3 0 0 0 0 0 18 5 0 4401 pas 13 0 0 624 0 23 5 0 0 0 0 0 0 0 0 665 agr 191 34 15 0 20761 207 45 22 6 5 0 0 101 0 0 21387 res 1 4 0 0 6 7653 85 31 0 1 0 0 0 0 0 7781 i c 3 8 0 0 0 15 1152 5 0 0 0 0 0 0 0 1183 rec 0 0 0 0 0 37 0 656 0 0 0 0 0 0 0 693 for 55 16 0 0 8 21 0 11 12164 0 0 0 0 7 0 12282 r r 0 0 0 0 0 0 0 0 0 145 0 0 0 0 0 145 por 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 air 0 0 0 0 0 0 4 0 0 0 0 494 0 0 0 498 m d 33 6 0 0 0 29 26 39 0 0 0 0 368 2 0 503 fre 24 4 0 0 0 0 0 0 0 0 0 0 0 719 0 747 mar 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 tot 48731 55623 4326 624 21208 10557 2695 1049 12559 252 0 598 999 779 0 160000 table c8 madrid contingency table 2000 2006 table c8 luc luc map 2006 nat arl per pas agr res i c rec for r r por air m d fre mar tot map 2000 nat 47105 386 17 68 152 451 90 34 166 107 0 6 68 81 0 48731 arl 191 52871 35 0 249 865 442 50 5 518 0 186 202 9 0 55623 per 17 25 4149 0 45 38 5 0 2 11 0 0 10 24 0 4326 pas 13 0 0 586 0 17 0 0 2 4 0 0 0 2 0 624 agr 27 58 39 336 20499 109 43 11 17 30 0 0 39 0 0 21208 res 116 55 0 0 55 9931 136 86 21 62 0 90 2 3 0 10557 i c 16 23 0 0 0 287 2300 30 0 28 0 11 0 0 0 2695 rec 19 4 0 0 4 47 13 953 1 1 0 0 7 0 0 1049 for 143 4 0 0 16 37 3 6 12324 9 0 15 2 0 0 12559 r r 0 0 0 0 0 148 9 0 0 95 0 0 0 0 0 252 por 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 air 6 44 0 0 0 6 4 0 0 0 0 538 0 0 0 598 m d 88 78 0 11 25 71 71 41 0 11 0 13 590 0 0 999 fre 0 1 0 4 0 0 0 0 0 0 0 0 10 764 0 779 mar 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 tot 47741 53549 4240 1005 21045 12007 3116 1211 12538 876 0 859 930 883 0 160000 appendix d boxplots of simulated replicates this appendix contains boxplots to show the spread of the results for the different configurations of the proposed approach applied to the different case studies fig d1 boxplot of simulated replicates for different configurations for berlin case study for calibration and validation periods fig d1 fig d2 boxplot of simulated replicates for different configurations for budapest case study for calibration and validation periods fig d2 fig d3 boxplot of simulated replicates for different configurations for lisbon case study for calibration and validation periods fig d3 fig d4 boxplot of simulated replicates for different configurations for madrid case study for calibration and validation periods fig d4 appendix e final model parameterisations this appendix contains the final recommend values for the neighbourhood rules for each case study obtained at the conclusion of the application of the calibration method table e1 final neighbourhood rule parameter values berlin table e1 from to distance 0 1 2 5 influence natural areas arable land 0 00 0 00 0 00 0 00 natural areas permanent crops 0 00 0 00 0 00 0 00 natural areas pastures 0 00 0 00 0 00 0 00 natural areas agricultural areas 0 00 0 00 0 00 0 00 natural areas residential 0 00 0 00 0 00 0 00 natural areas industry commerce 0 00 0 00 0 00 0 00 natural areas recreation areas 0 00 0 00 0 00 0 00 natural areas forest 7 20 5 57 0 56 0 00 arable land arable land 404 99 2 94 0 29 0 00 arable land permanent crops 51 97 2 94 0 29 0 00 arable land pastures 45 08 1 32 0 13 0 00 arable land agricultural areas 66 56 6 39 0 64 0 00 arable land residential 26 55 1 50 0 15 0 00 arable land industry commerce 62 11 9 33 0 93 0 00 arable land recreation areas 69 50 1 82 0 18 0 00 arable land forest 25 00 0 00 0 00 0 00 permanent crops arable land 70 32 0 81 0 08 0 00 permanent crops permanent crops 317 63 35 57 3 56 0 00 permanent crops pastures 0 00 15 91 1 59 0 00 permanent crops agricultural areas 0 00 0 00 0 00 0 00 permanent crops residential 94 43 23 92 2 39 0 00 permanent crops industry commerce 0 00 0 00 0 00 0 00 permanent crops recreation areas 25 23 3 00 0 30 0 00 permanent crops forest 0 00 0 00 0 00 0 00 pastures arable land 25 00 1 32 0 13 0 00 pastures permanent crops 0 00 0 00 0 00 0 00 pastures pastures 500 00 38 20 3 82 0 00 pastures agricultural areas 25 00 3 00 0 30 0 00 pastures residential 12 50 0 00 0 00 0 00 pastures industry commerce 0 00 0 00 0 00 0 00 pastures recreation areas 0 00 0 00 0 00 0 00 pastures forest 99 50 0 00 0 00 0 00 agricultural areas arable land 0 00 6 39 0 64 0 00 agricultural areas permanent crops 0 00 1 50 0 15 0 00 agricultural areas pastures 11 65 3 00 0 30 0 00 agricultural areas agricultural areas 443 02 10 64 1 06 0 00 agricultural areas residential 0 00 16 22 1 62 0 00 agricultural areas industry commerce 0 00 1 50 0 15 0 00 agricultural areas recreation areas 0 00 0 00 0 00 0 00 agricultural areas forest 99 50 4 26 0 43 0 00 residential arable land 0 00 0 00 0 00 0 00 residential permanent crops 0 00 0 00 0 00 0 00 residential pastures 0 00 0 00 0 00 0 00 residential agricultural areas 0 00 0 00 0 00 0 00 residential residential 1000 00 17 22 1 72 0 00 residential industry commerce 0 00 0 00 0 00 0 00 residential recreation areas 12 50 0 00 0 00 0 00 residential forest 12 50 0 00 0 00 0 00 industry commerce arable land 0 00 0 00 0 00 0 00 industry commerce permanent crops 0 00 0 00 0 00 0 00 industry commerce pastures 0 00 0 00 0 00 0 00 industry commerce agricultural areas 0 00 0 00 0 00 0 00 industry commerce residential 0 00 0 00 0 00 0 00 industry commerce industry commerce 1000 00 84 91 8 49 0 00 industry commerce recreation areas 0 00 0 00 0 00 0 00 industry commerce forest 0 00 0 00 0 00 0 00 recreation areas arable land 0 00 0 00 0 00 0 00 recreation areas permanent crops 0 00 0 00 0 00 0 00 recreation areas pastures 0 00 0 00 0 00 0 00 recreation areas agricultural areas 0 00 0 00 0 00 0 00 recreation areas residential 0 00 38 20 3 82 0 00 recreation areas industry commerce 0 00 0 00 0 00 0 00 recreation areas recreation areas 1000 00 84 60 8 46 0 00 recreation areas forest 0 00 0 00 0 00 0 00 forest arable land 0 00 0 00 0 00 0 00 forest permanent crops 0 00 0 00 0 00 0 00 forest pastures 0 00 0 00 0 00 0 00 forest agricultural areas 25 00 0 00 0 00 0 00 forest residential 12 50 0 00 0 00 0 00 forest industry commerce 12 50 0 00 0 00 0 00 forest recreation areas 0 00 0 00 0 00 0 00 forest forest 996 23 9 83 0 98 0 00 road rail arable land 0 00 0 00 0 00 0 00 road rail permanent crops 0 00 0 00 0 00 0 00 road rail pastures 0 00 0 00 0 00 0 00 road rail agricultural areas 0 00 0 00 0 00 0 00 road rail residential 0 00 0 00 0 00 0 00 road rail industry commerce 0 00 0 00 0 00 0 00 road rail recreation areas 0 00 0 00 0 00 0 00 road rail forest 0 00 0 00 0 00 0 00 port area arable land 0 00 0 00 0 00 0 00 port area permanent crops 0 00 0 00 0 00 0 00 port area pastures 0 00 0 00 0 00 0 00 port area agricultural areas 0 00 0 00 0 00 0 00 port area residential 0 00 0 00 0 00 0 00 port area industry commerce 0 00 0 00 0 00 0 00 port area recreation areas 0 00 0 00 0 00 0 00 port area forest 0 00 0 00 0 00 0 00 airports arable land 0 00 0 00 0 00 0 00 airports permanent crops 0 00 27 86 2 79 0 00 airports pastures 0 00 0 00 0 00 0 00 airports agricultural areas 0 00 0 00 0 00 0 00 airports residential 0 00 0 00 0 00 0 00 airports industry commerce 0 00 25 74 2 57 0 00 airports recreation areas 0 00 0 00 0 00 0 00 airports forest 0 00 0 00 0 00 0 00 mine dump sites arable land 0 00 0 00 0 00 0 00 mine dump sites permanent crops 0 00 0 00 0 00 0 00 mine dump sites pastures 0 00 0 00 0 00 0 00 mine dump sites agricultural areas 0 00 0 00 0 00 0 00 mine dump sites residential 0 00 0 00 0 00 0 00 mine dump sites industry commerce 0 00 0 00 0 00 0 00 mine dump sites recreation areas 0 00 0 00 0 00 0 00 mine dump sites forest 0 00 33 44 3 34 0 00 fresh water arable land 0 00 0 00 0 00 0 00 fresh water permanent crops 0 00 0 00 0 00 0 00 fresh water pastures 0 00 0 00 0 00 0 00 fresh water agricultural areas 0 00 9 83 0 98 0 00 fresh water residential 0 00 0 00 0 00 0 00 fresh water industry commerce 0 00 0 00 0 00 0 00 fresh water recreation areas 0 00 0 00 0 00 0 00 fresh water forest 0 00 0 00 0 00 0 00 marine water arable land 0 00 0 00 0 00 0 00 marine water permanent crops 0 00 0 00 0 00 0 00 marine water pastures 0 00 0 00 0 00 0 00 marine water agricultural areas 0 00 0 00 0 00 0 00 marine water residential 0 00 0 00 0 00 0 00 marine water industry commerce 0 00 0 00 0 00 0 00 marine water recreation areas 0 00 0 00 0 00 0 00 marine water forest 0 00 0 00 0 00 0 00 table e2 final neighbourhood rule parameter values budapest table e2 from to distance 0 1 2 5 influence natural areas arable land 8 13 0 00 0 00 0 00 natural areas permanent crops 0 00 0 00 0 00 0 00 natural areas pastures 8 13 0 00 0 00 0 00 natural areas agricultural areas 0 00 0 00 0 00 0 00 natural areas residential 8 13 0 00 0 00 0 00 natural areas industry commerce 0 00 0 00 0 00 0 00 natural areas recreation areas 0 00 0 00 0 00 0 00 natural areas forest 32 50 3 00 0 30 0 00 arable land arable land 250 00 5 00 0 50 0 00 arable land permanent crops 32 50 1 50 0 15 0 00 arable land pastures 32 50 1 50 0 15 0 00 arable land agricultural areas 32 50 0 00 0 00 0 00 arable land residential 32 50 0 00 0 00 0 00 arable land industry commerce 32 50 0 00 0 00 0 00 arable land recreation areas 32 50 0 00 0 00 0 00 arable land forest 8 13 0 00 0 00 0 00 permanent crops arable land 16 25 6 00 0 60 0 00 permanent crops permanent crops 800 89 10 00 1 00 0 00 permanent crops pastures 8 13 1 50 0 15 0 00 permanent crops agricultural areas 8 13 1 50 0 15 0 00 permanent crops residential 8 13 0 00 0 00 0 00 permanent crops industry commerce 8 13 1 50 0 15 0 00 permanent crops recreation areas 0 00 0 00 0 00 0 00 permanent crops forest 0 00 0 00 0 00 0 00 pastures arable land 32 50 3 00 0 30 0 00 pastures permanent crops 0 00 0 00 0 00 0 00 pastures pastures 500 00 5 00 0 50 0 00 pastures agricultural areas 8 13 0 00 0 00 0 00 pastures residential 16 25 0 00 0 00 0 00 pastures industry commerce 16 25 0 00 0 00 0 00 pastures recreation areas 16 25 0 00 0 00 0 00 pastures forest 0 00 0 00 0 00 0 00 agricultural areas arable land 8 13 0 00 0 00 0 00 agricultural areas permanent crops 0 00 1 50 0 15 0 00 agricultural areas pastures 8 13 0 00 0 00 0 00 agricultural areas agricultural areas 1000 00 42 45 4 25 0 00 agricultural areas residential 16 25 1 50 0 15 0 00 agricultural areas industry commerce 8 13 0 00 0 00 0 00 agricultural areas recreation areas 0 00 0 00 0 00 0 00 agricultural areas forest 0 00 0 00 0 00 0 00 residential arable land 0 00 0 00 0 00 0 00 residential permanent crops 0 00 0 00 0 00 0 00 residential pastures 0 00 0 00 0 00 0 00 residential agricultural areas 0 00 0 00 0 00 0 00 residential residential 1000 00 19 35 1 93 0 00 residential industry commerce 8 13 1 50 0 15 0 00 residential recreation areas 16 25 1 50 0 15 0 00 residential forest 0 00 0 00 0 00 0 00 industry commerce arable land 0 00 0 00 0 00 0 00 industry commerce permanent crops 0 00 0 00 0 00 0 00 industry commerce pastures 0 00 0 00 0 00 0 00 industry commerce agricultural areas 0 00 0 00 0 00 0 00 industry commerce residential 0 00 0 00 0 00 0 00 industry commerce industry commerce 1000 00 46 71 4 67 0 00 industry commerce recreation areas 0 00 1 50 0 15 0 00 industry commerce forest 0 00 0 00 0 00 0 00 recreation areas arable land 0 00 0 00 0 00 0 00 recreation areas permanent crops 0 00 0 00 0 00 0 00 recreation areas pastures 0 00 1 50 0 15 0 00 recreation areas agricultural areas 0 00 0 00 0 00 0 00 recreation areas residential 0 00 1 50 0 15 0 00 recreation areas industry commerce 0 00 1 50 0 15 0 00 recreation areas recreation areas 1000 00 43 27 4 33 0 00 recreation areas forest 0 00 0 00 0 00 0 00 forest arable land 0 00 0 00 0 00 0 00 forest permanent crops 0 00 0 00 0 00 0 00 forest pastures 0 00 0 00 0 00 0 00 forest agricultural areas 8 13 0 00 0 00 0 00 forest residential 0 00 0 00 0 00 0 00 forest industry commerce 0 00 0 00 0 00 0 00 forest recreation areas 0 00 0 00 0 00 0 00 forest forest 500 00 10 33 1 03 0 00 road rail arable land 0 00 0 00 0 00 0 00 road rail permanent crops 0 00 0 00 0 00 0 00 road rail pastures 0 00 0 00 0 00 0 00 road rail agricultural areas 0 00 0 00 0 00 0 00 road rail residential 0 00 0 00 0 00 0 00 road rail industry commerce 0 00 0 00 0 00 0 00 road rail recreation areas 0 00 0 00 0 00 0 00 road rail forest 0 00 0 00 0 00 0 00 port area arable land 0 00 0 00 0 00 0 00 port area permanent crops 0 00 0 00 0 00 0 00 port area pastures 0 00 0 00 0 00 0 00 port area agricultural areas 0 00 0 00 0 00 0 00 port area residential 0 00 0 00 0 00 0 00 port area industry commerce 0 00 0 00 0 00 0 00 port area recreation areas 0 00 0 00 0 00 0 00 port area forest 0 00 0 00 0 00 0 00 airports arable land 0 00 0 00 0 00 0 00 airports permanent crops 0 00 0 00 0 00 0 00 airports pastures 0 00 0 00 0 00 0 00 airports agricultural areas 0 00 0 00 0 00 0 00 airports residential 0 00 0 00 0 00 0 00 airports industry commerce 0 00 0 00 0 00 0 00 airports recreation areas 0 00 0 00 0 00 0 00 airports forest 0 00 0 00 0 00 0 00 mine dump sites arable land 0 00 0 00 0 00 0 00 mine dump sites permanent crops 0 00 1 50 0 15 0 00 mine dump sites pastures 0 00 3 00 0 30 0 00 mine dump sites agricultural areas 0 00 0 00 0 00 0 00 mine dump sites residential 0 00 0 00 0 00 0 00 mine dump sites industry commerce 0 00 0 00 0 00 0 00 mine dump sites recreation areas 0 00 0 00 0 00 0 00 mine dump sites forest 0 00 0 00 0 00 0 00 fresh water arable land 0 00 0 00 0 00 0 00 fresh water permanent crops 0 00 0 00 0 00 0 00 fresh water pastures 0 00 0 00 0 00 0 00 fresh water agricultural areas 0 00 1 50 0 15 0 00 fresh water residential 0 00 0 00 0 00 0 00 fresh water industry commerce 0 00 0 00 0 00 0 00 fresh water recreation areas 8 13 3 00 0 30 0 00 fresh water forest 0 00 0 00 0 00 0 00 marine water arable land 0 00 0 00 0 00 0 00 marine water permanent crops 0 00 0 00 0 00 0 00 marine water pastures 0 00 0 00 0 00 0 00 marine water agricultural areas 0 00 0 00 0 00 0 00 marine water residential 0 00 0 00 0 00 0 00 marine water industry commerce 0 00 0 00 0 00 0 00 marine water recreation areas 0 00 0 00 0 00 0 00 marine water forest 0 00 0 00 0 00 0 00 table e3 final neighbourhood rule parameter values lisbon table e3 from to distance 0 1 2 5 influence natural areas arable land 8 51 0 00 0 00 0 00 natural areas permanent crops 10 00 1 00 0 10 0 00 natural areas pastures 0 00 0 00 0 00 0 00 natural areas agricultural areas 20 00 1 00 0 10 0 00 natural areas residential 28 68 9 33 0 93 0 00 natural areas industry commerce 18 34 1 00 0 10 0 00 natural areas recreation areas 20 00 2 00 0 20 0 00 natural areas forest 39 51 4 00 0 40 0 00 arable land arable land 500 00 3 75 0 38 0 00 arable land permanent crops 20 00 1 00 0 10 0 00 arable land pastures 40 00 2 00 0 20 0 00 arable land agricultural areas 20 00 1 00 0 10 0 00 arable land residential 36 38 0 00 0 00 0 00 arable land industry commerce 26 55 1 63 0 16 0 00 arable land recreation areas 0 00 0 00 0 00 0 00 arable land forest 10 00 0 00 0 00 0 00 permanent crops arable land 20 00 1 00 0 10 0 00 permanent crops permanent crops 1000 00 6 25 0 63 0 00 permanent crops pastures 0 00 0 00 0 00 0 00 permanent crops agricultural areas 10 00 0 00 0 00 0 00 permanent crops residential 0 00 0 00 0 00 0 00 permanent crops industry commerce 99 50 0 00 0 00 0 00 permanent crops recreation areas 10 00 0 00 0 00 0 00 permanent crops forest 0 00 0 00 0 00 0 00 pastures arable land 38 20 4 00 0 40 0 00 pastures permanent crops 0 00 0 00 0 00 0 00 pastures pastures 851 11 1 32 0 13 0 00 pastures agricultural areas 0 00 0 00 0 00 0 00 pastures residential 0 00 0 00 0 00 0 00 pastures industry commerce 0 00 0 00 0 00 0 00 pastures recreation areas 0 00 0 00 0 00 0 00 pastures forest 0 00 0 00 0 00 0 00 agricultural areas arable land 14 59 0 00 0 00 0 00 agricultural areas permanent crops 40 00 1 00 0 10 0 00 agricultural areas pastures 0 00 0 00 0 00 0 00 agricultural areas agricultural areas 275 83 0 50 0 05 0 00 agricultural areas residential 47 21 27 86 2 79 0 00 agricultural areas industry commerce 58 86 3 75 0 38 0 00 agricultural areas recreation areas 20 00 0 00 0 00 0 00 agricultural areas forest 10 00 0 00 0 00 0 00 residential arable land 0 00 0 00 0 00 0 00 residential permanent crops 0 00 0 00 0 00 0 00 residential pastures 0 00 0 00 0 00 0 00 residential agricultural areas 0 00 1 00 0 10 0 00 residential residential 1000 00 62 31 6 23 0 00 residential industry commerce 10 00 27 05 2 71 0 00 residential recreation areas 0 00 1 00 0 10 0 00 residential forest 0 00 0 00 0 00 0 00 industry commerce arable land 0 00 0 00 0 00 0 00 industry commerce permanent crops 0 00 0 00 0 00 0 00 industry commerce pastures 0 00 0 00 0 00 0 00 industry commerce agricultural areas 0 00 0 00 0 00 0 00 industry commerce residential 0 00 83 28 8 33 0 00 industry commerce industry commerce 1000 00 85 41 8 54 0 00 industry commerce recreation areas 10 00 2 00 0 20 0 00 industry commerce forest 0 00 0 00 0 00 0 00 recreation areas arable land 0 00 0 00 0 00 0 00 recreation areas permanent crops 0 00 0 00 0 00 0 00 recreation areas pastures 0 00 0 00 0 00 0 00 recreation areas agricultural areas 0 00 0 00 0 00 0 00 recreation areas residential 0 00 40 33 4 03 0 00 recreation areas industry commerce 0 00 38 20 3 82 0 00 recreation areas recreation areas 1000 00 85 41 8 54 0 00 recreation areas forest 0 00 0 00 0 00 0 00 forest arable land 23 10 0 00 0 00 0 00 forest permanent crops 61 80 0 00 0 00 0 00 forest pastures 0 00 0 00 0 00 0 00 forest agricultural areas 98 68 1 00 0 10 0 00 forest residential 43 27 0 00 0 00 0 00 forest industry commerce 20 00 0 00 0 00 0 00 forest recreation areas 61 30 6 89 0 69 0 00 forest forest 250 00 7 20 0 72 0 00 road rail arable land 0 00 0 00 0 00 0 00 road rail permanent crops 0 00 0 00 0 00 0 00 road rail pastures 0 00 0 00 0 00 0 00 road rail agricultural areas 0 00 0 00 0 00 0 00 road rail residential 0 00 72 95 7 29 0 00 road rail industry commerce 0 00 0 00 0 00 0 00 road rail recreation areas 0 00 2 00 0 20 0 00 road rail forest 0 00 0 00 0 00 0 00 port area arable land 0 00 0 00 0 00 0 00 port area permanent crops 0 00 0 00 0 00 0 00 port area pastures 0 00 0 00 0 00 0 00 port area agricultural areas 0 00 0 00 0 00 0 00 port area residential 0 00 0 00 0 00 0 00 port area industry commerce 0 00 0 00 0 00 0 00 port area recreation areas 10 00 76 39 7 64 0 00 port area forest 0 00 0 00 0 00 0 00 airports arable land 0 00 0 00 0 00 0 00 airports permanent crops 0 00 0 00 0 00 0 00 airports pastures 0 00 0 00 0 00 0 00 airports agricultural areas 0 00 0 00 0 00 0 00 airports residential 0 00 0 00 0 00 0 00 airports industry commerce 0 00 0 00 0 00 0 00 airports recreation areas 0 00 0 00 0 00 0 00 airports forest 0 00 0 00 0 00 0 00 mine dump sites arable land 0 00 0 00 0 00 0 00 mine dump sites permanent crops 0 00 0 00 0 00 0 00 mine dump sites pastures 0 00 0 00 0 00 0 00 mine dump sites agricultural areas 0 00 18 85 1 88 0 00 mine dump sites residential 0 00 1 00 0 10 0 00 mine dump sites industry commerce 0 00 1 00 0 10 0 00 mine dump sites recreation areas 0 00 2 00 0 20 0 00 mine dump sites forest 0 00 0 00 0 00 0 00 fresh water arable land 0 00 0 00 0 00 0 00 fresh water permanent crops 0 00 0 00 0 00 0 00 fresh water pastures 0 00 80 15 8 01 0 00 fresh water agricultural areas 0 00 0 00 0 00 0 00 fresh water residential 0 00 0 00 0 00 0 00 fresh water industry commerce 0 00 0 00 0 00 0 00 fresh water recreation areas 0 00 0 00 0 00 0 00 fresh water forest 0 00 0 00 0 00 0 00 marine water arable land 0 00 0 00 0 00 0 00 marine water permanent crops 0 00 0 00 0 00 0 00 marine water pastures 0 00 0 00 0 00 0 00 marine water agricultural areas 0 00 0 00 0 00 0 00 marine water residential 0 00 0 00 0 00 0 00 marine water industry commerce 0 00 0 00 0 00 0 00 marine water recreation areas 0 00 0 00 0 00 0 00 marine water forest 0 00 0 00 0 00 0 00 table e4 final neighbourhood rule parameter values madrid table e4 from to distance 0 1 2 5 influence natural areas arable land 15 40 2 75 0 28 0 00 natural areas permanent crops 60 49 0 00 0 00 0 00 natural areas pastures 0 00 0 00 0 00 0 00 natural areas agricultural areas 20 00 0 00 0 00 0 00 natural areas residential 20 00 0 00 0 00 0 00 natural areas industry commerce 32 12 0 00 0 00 0 00 natural areas recreation areas 19 85 2 75 0 28 0 00 natural areas forest 33 13 2 13 0 21 0 00 arable land arable land 250 00 7 70 0 77 0 00 arable land permanent crops 40 00 2 75 0 28 0 00 arable land pastures 0 00 0 00 0 00 0 00 arable land agricultural areas 28 37 2 75 0 28 0 00 arable land residential 52 48 2 75 0 28 0 00 arable land industry commerce 69 50 2 13 0 21 0 00 arable land recreation areas 38 70 0 00 0 00 0 00 arable land forest 81 46 0 00 0 00 0 00 permanent crops arable land 10 00 0 00 0 00 0 00 permanent crops permanent crops 572 17 21 48 2 15 0 00 permanent crops pastures 0 00 0 00 0 00 0 00 permanent crops agricultural areas 0 00 0 00 0 00 0 00 permanent crops residential 0 00 0 00 0 00 0 00 permanent crops industry commerce 0 00 0 00 0 00 0 00 permanent crops recreation areas 0 00 0 00 0 00 0 00 permanent crops forest 0 00 0 00 0 00 0 00 pastures arable land 0 00 0 00 0 00 0 00 pastures permanent crops 0 00 0 00 0 00 0 00 pastures pastures 735 59 10 00 1 00 0 00 pastures agricultural areas 0 00 0 00 0 00 0 00 pastures residential 0 00 2 75 0 28 0 00 pastures industry commerce 0 00 0 00 0 00 0 00 pastures recreation areas 0 00 0 00 0 00 0 00 pastures forest 0 00 0 00 0 00 0 00 agricultural areas arable land 10 00 0 00 0 00 0 00 agricultural areas permanent crops 20 00 8 51 0 85 0 00 agricultural areas pastures 0 00 0 00 0 00 0 00 agricultural areas agricultural areas 353 33 9 33 0 93 0 00 agricultural areas residential 42 96 0 00 0 00 0 00 agricultural areas industry commerce 52 28 0 00 0 00 0 00 agricultural areas recreation areas 63 12 0 00 0 00 0 00 agricultural areas forest 0 00 0 00 0 00 0 00 residential arable land 0 00 0 00 0 00 0 00 residential permanent crops 0 00 0 00 0 00 0 00 residential pastures 0 00 0 00 0 00 0 00 residential agricultural areas 0 00 0 00 0 00 0 00 residential residential 1000 00 61 30 6 13 0 00 residential industry commerce 10 00 35 57 3 56 0 00 residential recreation areas 10 00 2 75 0 28 0 00 residential forest 0 00 0 00 0 00 0 00 industry commerce arable land 0 00 2 75 0 28 0 00 industry commerce permanent crops 0 00 0 00 0 00 0 00 industry commerce pastures 0 00 0 00 0 00 0 00 industry commerce agricultural areas 0 00 0 00 0 00 0 00 industry commerce residential 0 00 3 44 0 34 0 00 industry commerce industry commerce 1000 00 55 73 5 57 0 00 industry commerce recreation areas 0 00 10 33 1 03 0 00 industry commerce forest 0 00 0 00 0 00 0 00 recreation areas arable land 0 00 0 00 0 00 0 00 recreation areas permanent crops 0 00 0 00 0 00 0 00 recreation areas pastures 0 00 0 00 0 00 0 00 recreation areas agricultural areas 0 00 0 00 0 00 0 00 recreation areas residential 0 00 5 57 0 56 0 00 recreation areas industry commerce 0 00 0 00 0 00 0 00 recreation areas recreation areas 500 00 20 16 2 02 0 00 recreation areas forest 0 00 0 00 0 00 0 00 forest arable land 10 00 0 00 0 00 0 00 forest permanent crops 0 00 0 00 0 00 0 00 forest pastures 0 00 0 00 0 00 0 00 forest agricultural areas 0 00 0 00 0 00 0 00 forest residential 0 00 0 00 0 00 0 00 forest industry commerce 0 00 0 00 0 00 0 00 forest recreation areas 10 00 0 00 0 00 0 00 forest forest 1000 00 5 57 0 56 0 00 road rail arable land 0 00 0 00 0 00 0 00 road rail permanent crops 0 00 0 00 0 00 0 00 road rail pastures 0 00 0 00 0 00 0 00 road rail agricultural areas 0 00 0 00 0 00 0 00 road rail residential 0 00 0 00 0 00 0 00 road rail industry commerce 0 00 68 19 6 82 0 00 road rail recreation areas 0 00 0 00 0 00 0 00 road rail forest 0 00 0 00 0 00 0 00 port area arable land 0 00 0 00 0 00 0 00 port area permanent crops 0 00 0 00 0 00 0 00 port area pastures 0 00 0 00 0 00 0 00 port area agricultural areas 0 00 0 00 0 00 0 00 port area residential 0 00 0 00 0 00 0 00 port area industry commerce 0 00 0 00 0 00 0 00 port area recreation areas 0 00 0 00 0 00 0 00 port area forest 0 00 0 00 0 00 0 00 airports arable land 0 00 0 00 0 00 0 00 airports permanent crops 0 00 0 00 0 00 0 00 airports pastures 0 00 0 00 0 00 0 00 airports agricultural areas 0 00 0 00 0 00 0 00 airports residential 0 00 0 00 0 00 0 00 airports industry commerce 0 00 19 35 1 93 0 00 airports recreation areas 0 00 0 00 0 00 0 00 airports forest 0 00 0 00 0 00 0 00 mine dump sites arable land 0 00 0 00 0 00 0 00 mine dump sites permanent crops 0 00 31 31 3 13 0 00 mine dump sites pastures 0 00 0 00 0 00 0 00 mine dump sites agricultural areas 0 00 0 00 0 00 0 00 mine dump sites residential 0 00 51 47 5 15 0 00 mine dump sites industry commerce 0 00 11 15 1 11 0 00 mine dump sites recreation areas 10 00 11 15 1 11 0 00 mine dump sites forest 0 00 0 00 0 00 0 00 fresh water arable land 0 00 0 00 0 00 0 00 fresh water permanent crops 0 00 0 00 0 00 0 00 fresh water pastures 0 00 0 00 0 00 0 00 fresh water agricultural areas 0 00 0 00 0 00 0 00 fresh water residential 0 00 0 00 0 00 0 00 fresh water industry commerce 0 00 0 00 0 00 0 00 fresh water recreation areas 0 00 0 00 0 00 0 00 fresh water forest 0 00 0 00 0 00 0 00 marine water arable land 0 00 0 00 0 00 0 00 marine water permanent crops 0 00 0 00 0 00 0 00 marine water pastures 0 00 0 00 0 00 0 00 marine water agricultural areas 0 00 0 00 0 00 0 00 marine water residential 0 00 0 00 0 00 0 00 marine water industry commerce 0 00 0 00 0 00 0 00 marine water recreation areas 0 00 0 00 0 00 0 00 marine water forest 0 00 0 00 0 00 0 00 
